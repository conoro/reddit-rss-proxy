<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-12T00:29:31+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1otjeax</id>
    <title>Am i in danger?</title>
    <updated>2025-11-10T17:06:02+00:00</updated>
    <author>
      <name>/u/Minecraft-tlauncher</name>
      <uri>https://old.reddit.com/user/Minecraft-tlauncher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otjeax/am_i_in_danger/"&gt; &lt;img alt="Am i in danger?" src="https://preview.redd.it/e8oqcp7eog0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53d46605b5594f04b3972493e2d6d4154b02f46e" title="Am i in danger?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tinyllama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Minecraft-tlauncher"&gt; /u/Minecraft-tlauncher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e8oqcp7eog0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otjeax/am_i_in_danger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otjeax/am_i_in_danger/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T17:06:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1osrrxi</id>
    <title>CPU on self host ollama 1000%</title>
    <updated>2025-11-09T19:08:45+00:00</updated>
    <author>
      <name>/u/Super-Professor519</name>
      <uri>https://old.reddit.com/user/Super-Professor519</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I host a ollama app with gemma3:4b model in a server with 16gb ram. I use caddy as reserve proxy to the ollama port. What I send a request it takes 20+ seconds to respond. &lt;/p&gt; &lt;p&gt;Note I use the /chat endpoint with 2 messages one for system and one for user.&lt;/p&gt; &lt;p&gt;I set the OLLAMA_KEEP_ALLIVE to 86400 so it never sleeps.&lt;/p&gt; &lt;p&gt;How can I speed up the respond time? Any idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Super-Professor519"&gt; /u/Super-Professor519 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1osrrxi/cpu_on_self_host_ollama_1000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1osrrxi/cpu_on_self_host_ollama_1000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1osrrxi/cpu_on_self_host_ollama_1000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-09T19:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ospgkw</id>
    <title>GPT 5 for Computer Use agents</title>
    <updated>2025-11-09T17:39:02+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ospgkw/gpt_5_for_computer_use_agents/"&gt; &lt;img alt="GPT 5 for Computer Use agents" src="https://external-preview.redd.it/OHA3ZWVjMW5wOTBnMRG6-oWujtVtwWnIPYQYAphLJPDZc9z94p-KY-4O-UR8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff9163fdb86010d07646c010c115ee643423195b" title="GPT 5 for Computer Use agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same tasks, same grounding model we just swapped GPT 4o with GPT 5 as the thinking model.&lt;/p&gt; &lt;p&gt;Left = 4o, right = 5.&lt;/p&gt; &lt;p&gt;Watch GPT 5 pull through.&lt;/p&gt; &lt;p&gt;Grounding model: Salesforce GTA1-7B&lt;/p&gt; &lt;p&gt;Action space: CUA Cloud Instances (macOS/Linux/Windows)&lt;/p&gt; &lt;p&gt;The task is: &amp;quot;Navigate to {random_url} and play the game until you reach a score of 5/5‚Äù....each task is set up by having claude generate a random app from a predefined list of prompts (multiple choice trivia, form filling, or color matching)&amp;quot;&lt;/p&gt; &lt;p&gt;Try it yourself here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs : &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agent"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agent&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.gg/cua-ai"&gt;https://discord.gg/cua-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ojia28enp90g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ospgkw/gpt_5_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ospgkw/gpt_5_for_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-09T17:39:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1otbs2d</id>
    <title>Which model is better to create notes from sample?</title>
    <updated>2025-11-10T11:53:43+00:00</updated>
    <author>
      <name>/u/Adventurous-Hunter98</name>
      <uri>https://old.reddit.com/user/Adventurous-Hunter98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I need to create lots of notes from a sample note with a note that has list of data.&lt;/p&gt; &lt;p&gt;Which model achieves to do this?&lt;/p&gt; &lt;p&gt;For example; note sample has&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Title:&lt;br /&gt; Date:&lt;br /&gt; Description:&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;and I have a list of these datas in a note like below&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;title1, date1, description1&lt;/p&gt; &lt;p&gt;title2, date2, description2&lt;/p&gt; &lt;p&gt;title3, date3, description3 ...&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Hunter98"&gt; /u/Adventurous-Hunter98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otbs2d/which_model_is_better_to_create_notes_from_sample/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otbs2d/which_model_is_better_to_create_notes_from_sample/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otbs2d/which_model_is_better_to_create_notes_from_sample/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T11:53:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1otnogr</id>
    <title>HOW DO I DO THIS</title>
    <updated>2025-11-10T19:40:08+00:00</updated>
    <author>
      <name>/u/Minecraft-tlauncher</name>
      <uri>https://old.reddit.com/user/Minecraft-tlauncher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otnogr/how_do_i_do_this/"&gt; &lt;img alt="HOW DO I DO THIS" src="https://external-preview.redd.it/0vxFDxo53p-qtsifbP0aBQuFAgn8-6d3UDzff95MmEM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4d823b7229949e6d21ed9ed57ad324369ffb1a7" title="HOW DO I DO THIS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im guessing this can be done with any local ai, in ollama aswell probably&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Minecraft-tlauncher"&gt; /u/Minecraft-tlauncher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/WP5_XJY_P0Q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otnogr/how_do_i_do_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otnogr/how_do_i_do_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T19:40:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1otgf7i</id>
    <title>Ollama not finishing thoughts/replies.</title>
    <updated>2025-11-10T15:16:09+00:00</updated>
    <author>
      <name>/u/New-Maintenance2371</name>
      <uri>https://old.reddit.com/user/New-Maintenance2371</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm new to Ollama. I've been using Ollama gpt-oss:120b-cloud for two days, primarily for assistance in programming. The last time I tried writing it a request, it thinks for 5-7 seconds and then it stops. It doesn't finish what it thinks and sometimes when it does, it doesn't finish the sentence when replying, it simply cuts out. I've decided to wait for a week to let it cool down but the issue persists. I did not run out of my Hourly/Weekly usages. The problem is still present and it's frustrating. Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New-Maintenance2371"&gt; /u/New-Maintenance2371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otgf7i/ollama_not_finishing_thoughtsreplies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otgf7i/ollama_not_finishing_thoughtsreplies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otgf7i/ollama_not_finishing_thoughtsreplies/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T15:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1otgiyg</id>
    <title>Wiredigg now integrates Ollama for AI-powered network analysis + new packet visualization engine!</title>
    <updated>2025-11-10T15:20:13+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otgiyg/wiredigg_now_integrates_ollama_for_aipowered/"&gt; &lt;img alt="Wiredigg now integrates Ollama for AI-powered network analysis + new packet visualization engine!" src="https://external-preview.redd.it/ii2qXwOt9q-3JoNEi3AZTjVgmUTUDvnX5B70rzCS6xA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c066983a82b9b18a5d4a22fc39abfcfd82be25a0" title="Wiredigg now integrates Ollama for AI-powered network analysis + new packet visualization engine!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Zrufy/wiredigg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otgiyg/wiredigg_now_integrates_ollama_for_aipowered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otgiyg/wiredigg_now_integrates_ollama_for_aipowered/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T15:20:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1otgj9y</id>
    <title>PolyMCP ‚Äî Giving LLM Agents Real Multi-Tool Intelligence</title>
    <updated>2025-11-10T15:20:35+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otgj9y/polymcp_giving_llm_agents_real_multitool/"&gt; &lt;img alt="PolyMCP ‚Äî Giving LLM Agents Real Multi-Tool Intelligence" src="https://external-preview.redd.it/Msm-QajuVHOOiFNkJqYJNVPFdKfyURY_aL6fbzgG9Vc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1c84385768301e5c01a6756534af82406a9b572" title="PolyMCP ‚Äî Giving LLM Agents Real Multi-Tool Intelligence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/Polymcp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otgj9y/polymcp_giving_llm_agents_real_multitool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otgj9y/polymcp_giving_llm_agents_real_multitool/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T15:20:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1otgjmm</id>
    <title>You don‚Äôt need the biggest model: how LLM-Use helps humans solve complex problems</title>
    <updated>2025-11-10T15:20:59+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otgjmm/you_dont_need_the_biggest_model_how_llmuse_helps/"&gt; &lt;img alt="You don‚Äôt need the biggest model: how LLM-Use helps humans solve complex problems" src="https://external-preview.redd.it/RRYPD6RWnoPPKoEXXdxyjsRQ413b2PstPkKrMO7uaos.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c17d5626dd71809e0d938b9fafdadf8a3bf4450e" title="You don‚Äôt need the biggest model: how LLM-Use helps humans solve complex problems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/llm-use-agentic"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otgjmm/you_dont_need_the_biggest_model_how_llmuse_helps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otgjmm/you_dont_need_the_biggest_model_how_llmuse_helps/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T15:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1otlpy9</id>
    <title>Mimir - OSS memory bank and file indexer + MCP http server ++ under MIT license.</title>
    <updated>2025-11-10T18:29:33+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/GithubCopilot/comments/1otlo8c/mimir_oss_memory_bank_and_file_indexer_mcp_http/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otlpy9/mimir_oss_memory_bank_and_file_indexer_mcp_http/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otlpy9/mimir_oss_memory_bank_and_file_indexer_mcp_http/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T18:29:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot3pp8</id>
    <title>Granite 4 micro-h doing great on my older pc</title>
    <updated>2025-11-10T03:47:18+00:00</updated>
    <author>
      <name>/u/newz2000</name>
      <uri>https://old.reddit.com/user/newz2000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"&gt; &lt;img alt="Granite 4 micro-h doing great on my older pc" src="https://a.thumbs.redditmedia.com/bbtxjoQlGAnti-gN1X_64JqMNQSAnCRZif4YFLrMqj0.jpg" title="Granite 4 micro-h doing great on my older pc" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/nars9rownc0g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2d27ce0136bd0fc08a26e32900ff809de4c39dd"&gt;https://preview.redd.it/nars9rownc0g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2d27ce0136bd0fc08a26e32900ff809de4c39dd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My older 7th Gen i5 gaming computer has been repurposed into my local llm workhorse for most of this year. I use it to automate tasks. In this example, extract key dates and information from an email producing the results in JSON format. &lt;/p&gt; &lt;p&gt;I have been using Qwen 3 and Gemma 3 and I'd say if I want to have a conversation, Qwen 3:8b is my favorite. But it's not good at instruction following. Gemma 3:4b really does great all around and is very quick on this computer. But for instruction following, Granite 4 micro-h is tough to beat.&lt;/p&gt; &lt;p&gt;I have not yet tested it with tool calling, but this is something I want to do and is what made me check out Granite.&lt;/p&gt; &lt;p&gt;Since you can kinda see my prompt through the translucent window, I'll save you the effort and put it in here. &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;You are an assistant that extracts litigation-relevant structured data from incoming court notices that arrive via email or plaintext.&lt;/p&gt; &lt;p&gt;Read the following email or document VERY CAREFULLY.&lt;/p&gt; &lt;p&gt;Then output ONLY JSON.&lt;/p&gt; &lt;p&gt;Do not summarize.&lt;/p&gt; &lt;p&gt;Do not infer beyond what is explicitly written.&lt;/p&gt; &lt;p&gt;If a field cannot be determined, return null ‚Äî do NOT guess.&lt;/p&gt; &lt;p&gt;You must return ONLY this exact JSON structure (no explanation):&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;case_name&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;case_number&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;court&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;hearing_date&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;hearing_time&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;presiding_judge&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;filing_date_of_order&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;required_filing_deadlines&amp;quot;: [],&lt;/p&gt; &lt;p&gt;&amp;quot;parties_involved&amp;quot;: [],&lt;/p&gt; &lt;p&gt;&amp;quot;topic_or_subject_matter&amp;quot;: &amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;Return ONLY a JSON object with EXACTLY these keys:&lt;/p&gt; &lt;p&gt;case_name, case_number, court, hearing_date, hearing_time, presiding_judge,&lt;/p&gt; &lt;p&gt;filing_date_of_order, required_filing_deadlines, parties_involved, topic_or_subject_matter.&lt;/p&gt; &lt;p&gt;If a value is unknown, set null. Do NOT add any other keys or sections.&lt;/p&gt; &lt;p&gt;Dates = YYYY-MM-DD. Times = 24-hour local-to-court (e.g., 13:30).&lt;/p&gt; &lt;p&gt;Include ONLY human names in `parties_involved` (no emails). Remove HTML entities.&lt;/p&gt; &lt;p&gt;Rules:&lt;/p&gt; &lt;p&gt;hearing_date and hearing_time must be extracted if a hearing is set.&lt;/p&gt; &lt;p&gt;required_filing_deadlines must list ONLY dates that represent ‚Äúsomething is due‚Äù by ‚Äúa date certain.‚Äù&lt;/p&gt; &lt;p&gt;parties_involved should list all names referenced as parties, attorneys, or counsel receiving service.&lt;/p&gt; &lt;p&gt;The topic_or_subject_matter is a single short clause describing WHAT the order is about (motion type, hearing type, etc).&lt;/p&gt; &lt;p&gt;Dates must be formatted YYYY-MM-DD.&lt;/p&gt; &lt;p&gt;Times must be formatted 24 hour format, local to the court when stated (CST ‚Üí convert to 24h).&lt;/p&gt; &lt;p&gt;DO NOT return anything not inside the JSON block.&lt;/p&gt; &lt;p&gt;EMAIL:&lt;/p&gt; &lt;p&gt;‚Ä¶&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newz2000"&gt; /u/newz2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T03:47:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou02ob</id>
    <title>Please reply ASAP I really need your thoughts on this</title>
    <updated>2025-11-11T04:25:05+00:00</updated>
    <author>
      <name>/u/Acceptable-Buyer-184</name>
      <uri>https://old.reddit.com/user/Acceptable-Buyer-184</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can I run ollama qwen3-vl: 8b on my local computer? it will be use for ai generation from materials like summary, quiz, and flashcards? Or should i stick to 4b?&lt;/p&gt; &lt;p&gt;PC Specs:&lt;br /&gt; Ryzen 7 5700x&lt;br /&gt; RTX 5060 8GB&lt;br /&gt; 16GB RAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable-Buyer-184"&gt; /u/Acceptable-Buyer-184 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou02ob/please_reply_asap_i_really_need_your_thoughts_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou02ob/please_reply_asap_i_really_need_your_thoughts_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ou02ob/please_reply_asap_i_really_need_your_thoughts_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T04:25:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot3u7f</id>
    <title>Speculative decoding: Faster inference for local LLMs over the network?</title>
    <updated>2025-11-10T03:53:54+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ot3u7f/speculative_decoding_faster_inference_for_local/"&gt; &lt;img alt="Speculative decoding: Faster inference for local LLMs over the network?" src="https://preview.redd.it/70p6li1poc0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6daaaeff166a74d20a883ce88ad7a5a9b3feaf6" title="Speculative decoding: Faster inference for local LLMs over the network?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am gearing up for a big release to add support for speculative decoding for LLMs and looking for early feedback.&lt;/p&gt; &lt;p&gt;First a bit of context, speculative decoding is a technique whereby a draft model (usually a smaller LLM) is engaged to produce tokens and the candidate set produced is verified by a target model (usually a larger model). The set of candidate tokens produced by a draft model must be verifiable via logits by the target model. While tokens produced are serial, verification can happen in parallel which can lead to significant improvements in speed.&lt;/p&gt; &lt;p&gt;This is what OpenAI uses to accelerate the speed of its responses especially in cases where outputs can be guaranteed to come from the same distribution, where:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;propose(x, k) ‚Üí œÑ # Draft model proposes k tokens based on context x verify(x, œÑ) ‚Üí m # Target verifies œÑ, returns accepted count m continue_from(x) # If diverged, resume from x with target model &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So I am thinking of adding support to &lt;a href="https://github.com/katanemo/archgw"&gt;arch&lt;/a&gt; (a models-native sidecar proxy for agents). And the developer experience could be something along the following lines:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;POST /v1/chat/completions { &amp;quot;model&amp;quot;: &amp;quot;target:gpt-large@2025-06&amp;quot;, &amp;quot;speculative&amp;quot;: { &amp;quot;draft_model&amp;quot;: &amp;quot;draft:small@v3&amp;quot;, &amp;quot;max_draft_window&amp;quot;: 8, &amp;quot;min_accept_run&amp;quot;: 2, &amp;quot;verify_logprobs&amp;quot;: false }, &amp;quot;messages&amp;quot;: [...], &amp;quot;stream&amp;quot;: true } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here the max_draft_window is the number of tokens to verify, the max_accept_run tells us after how many failed verifications should we give up and just send all the remaining traffic to the target model etc. Of course this work assumes a low RTT between the target and draft model so that speculative decoding is faster without compromising quality.&lt;/p&gt; &lt;p&gt;Question: how would you feel about this functionality? Could you see it being useful for your LLM-based applications? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/70p6li1poc0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ot3u7f/speculative_decoding_faster_inference_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ot3u7f/speculative_decoding_faster_inference_for_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T03:53:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ota5ou</id>
    <title>GLM-4.6-REAP any good for coding? Min VRAM+RAM?</title>
    <updated>2025-11-10T10:17:23+00:00</updated>
    <author>
      <name>/u/WaitformeBumblebee</name>
      <uri>https://old.reddit.com/user/WaitformeBumblebee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using mostly QWEN3 variants (&amp;lt;20GB) for python coding tasks. Would 16GB VRAM + 64GB RAM be able to &amp;quot;run&amp;quot; (I don't mind waiting some minutes if the answer is much better) 72GB model like &lt;a href="https://ollama.com/MichelRosselli/GLM-4.6-REAP-218B-A32B-FP8-mixed-AutoRound"&gt;https://ollama.com/MichelRosselli/GLM-4.6-REAP-218B-A32B-FP8-mixed-AutoRound&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and how good is it? Been hearing high praise for GLM-4.5-AIR, but don't want to download &amp;gt;70GB for nothing. Perhaps I'd be better of with GLM-4.5-Air:Q2_K at 45GB ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WaitformeBumblebee"&gt; /u/WaitformeBumblebee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ota5ou/glm46reap_any_good_for_coding_min_vramram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ota5ou/glm46reap_any_good_for_coding_min_vramram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ota5ou/glm46reap_any_good_for_coding_min_vramram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T10:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1otzvk3</id>
    <title>Built a Terminal-Based AI Chatbot Powered by Ollama Cloud</title>
    <updated>2025-11-11T04:15:17+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I‚Äôve been experimenting with &lt;strong&gt;Ollama&lt;/strong&gt; and just released a &lt;strong&gt;terminal-based AI Chatbot&lt;/strong&gt; that runs completely on your local machine ‚Äî &lt;strong&gt;no browser, no cloud dependency&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;üí¨ I‚Äôd love your help testing it out and sharing feedback (UX, response flow, model handling, or new feature ideas).&lt;/p&gt; &lt;p&gt;üì¶ &lt;strong&gt;Download the latest release:&lt;/strong&gt;&lt;br /&gt; üëâ &lt;a href="https://github.com/yasniy97/aichatbot/releases/tag/ollama"&gt;Releases URL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what the Ollama community thinks ‚Äî how it performs on your setup, any compatibility notes, or prompts you find interesting! üôå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otzvk3/built_a_terminalbased_ai_chatbot_powered_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otzvk3/built_a_terminalbased_ai_chatbot_powered_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otzvk3/built_a_terminalbased_ai_chatbot_powered_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T04:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1otbtp2</id>
    <title>Ollama-powered open source single-stock analysis tool with Python, including ratios/news analysis/LSTM forecast</title>
    <updated>2025-11-10T11:56:09+00:00</updated>
    <author>
      <name>/u/gruquilla</name>
      <uri>https://old.reddit.com/user/gruquilla</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otbtp2/ollamapowered_open_source_singlestock_analysis/"&gt; &lt;img alt="Ollama-powered open source single-stock analysis tool with Python, including ratios/news analysis/LSTM forecast" src="https://preview.redd.it/9hn8znm45f0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22404009b637970ab1e3ab5561cb65759c99e1c7" title="Ollama-powered open source single-stock analysis tool with Python, including ratios/news analysis/LSTM forecast" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good morning everyone,&lt;/p&gt; &lt;p&gt;I am currently a MSc Fintech student at Aston University (Birmingham, UK) and Audencia Business School (Nantes, France). Alongside my studies, I've started to develop a few personal Python projects.&lt;/p&gt; &lt;p&gt;My first big open-source project: A single-stock analysis tool that uses both market and financial statements informations. It also integrates news sentiment analysis (FinBert and Pygooglenews), as well as LSTM forecast for the stock price. You can also enable Ollama to get information complements using a local LLM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What my project (FinAPy) does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prologue: Ticker input collection and essential functions and data: &lt;em&gt;In this part, the program gets in input a ticker from the user, and asks wether or not he wants to enable the AI analysis. Then, it generates a short summary about the company fetching information from Yahoo Finance, so the user has something to read while the next step proceeds. It also fetches the main financial metrics and computes additional ones.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Step 1: Events and news fetching: &lt;em&gt;This part fetches stock events from Yahoo Finance and news from Google RSS feed. It also generates a sentiment analysis about the articles fetched using FinBERT.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 2: Forecast using Machine Learning LSTM: &lt;em&gt;This part creates a baseline scenario from a LSTM forecast. The forecast covers 60 days and is trained from 100 last values of close/ high/low prices. It is a quantiative model only. An optimistic and pessimistic scenario are then created by tweaking the main baseline to give a window of prediction. They do not integrate macroeconomic factors, specific metric variations nor Monte Carlo simulations for the moment.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 3: Market data restitution: &lt;em&gt;This part is dedicated to restitute graphically the previously computed data. It also computes CFA classical metrics (histogram of returns, skewness, kurtosis) and their explanation. The part concludes with an Ollama AI commentary of the analysis.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 4: Financial statement analysis: &lt;em&gt;This part is dedicated to the generation of the main ratios from the financial statements of the last 3 years of the company. Each part concludes with an Ollama AI commentary on the ratios. The analysis includes an overview of the variation, and highlights in color wether the change is positive or negative. Each ratio is commented so you can understand what they represent/ how they are calculated. The ratios include:&lt;/em&gt; &lt;ul&gt; &lt;li&gt;Profitability ratios: Profit margin, ROA, ROCE, ROE,...&lt;/li&gt; &lt;li&gt;Asset related ratios: Asset turnover, working capital.&lt;/li&gt; &lt;li&gt;Liquidity ratios: Current ratio, quick ratio, cash ratio.&lt;/li&gt; &lt;li&gt;Solvency ratios: debt to assets, debt to capital, financial leverage, coverage ratios,...&lt;/li&gt; &lt;li&gt;Operational ratios (cashflow related): CFI/ CFF/ CFO ratios, cash return on assets,...&lt;/li&gt; &lt;li&gt;Bankrupcy and financial health scores: Altman Z-score/ Ohlson O-score.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Appendix: Financial statements: &lt;em&gt;A summary of the financial statements scaled for better readability in case you want to push the manual analysis further.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Target audience:&lt;/strong&gt; Students, researchers,... For educational and research purpose only. However, it illustrates how local LLMs could be integrated into industry practices and workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Comparison:&lt;/strong&gt; The project enables both a market and statement analysis perspective, and showcases how a local LLM can run in a financial context while showing to which extent it can bring something to analysts.&lt;/p&gt; &lt;p&gt;At this point, I'm considering starting to work on industry metrics (for comparability of ratios) and portfolio construction. Thank you in advance for your insights, I‚Äôm keen to refine this further with input from the community!&lt;/p&gt; &lt;p&gt;The repository: &lt;a href="https://github.com/gruquilla/FinAPy"&gt;gruquilla/FinAPy: Single-stock analysis using Python and local machine learning/ AI tools (Ollama, LSTM).&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gruquilla"&gt; /u/gruquilla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9hn8znm45f0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otbtp2/ollamapowered_open_source_singlestock_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otbtp2/ollamapowered_open_source_singlestock_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T11:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou2hzw</id>
    <title>Smarter Agents, Fewer Integrations: How PolyMCP Is Changing Multi-Tool AI Workflows</title>
    <updated>2025-11-11T06:42:26+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ou2hzw/smarter_agents_fewer_integrations_how_polymcp_is/"&gt; &lt;img alt="Smarter Agents, Fewer Integrations: How PolyMCP Is Changing Multi-Tool AI Workflows" src="https://external-preview.redd.it/4WW_W4p9ydeJXDr9T_Om3KiQJYlfiVcevEHUJWwfFd4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f1c5ae24902278d025d41c1773334a9c3d6de5f" title="Smarter Agents, Fewer Integrations: How PolyMCP Is Changing Multi-Tool AI Workflows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/Polymcp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou2hzw/smarter_agents_fewer_integrations_how_polymcp_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ou2hzw/smarter_agents_fewer_integrations_how_polymcp_is/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T06:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1otbtrh</id>
    <title>Built a local chat UI for Ollama - thought I'd share</title>
    <updated>2025-11-10T11:56:14+00:00</updated>
    <author>
      <name>/u/neiellcare</name>
      <uri>https://old.reddit.com/user/neiellcare</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a web interface for Ollama that stores everything locally. No external servers, all conversations stay on your machine.&lt;/p&gt; &lt;p&gt;Main features: - Memory system so the AI remembers context between chats - Upload documents (PDFs, Word files) for the AI to reference - Web search integration when you need current information - Works with vision models like LLaVA - Live preview for code the AI generates&lt;/p&gt; &lt;p&gt;Everything runs in Docker or you can run it locally with Node. It uses React and TypeScript, stores data in IndexedDB in your browser.&lt;/p&gt; &lt;p&gt;I built it because I wanted something privacy-focused that also had RAG and conversation memory in one place. Works well for me, figured others might find it useful.&lt;/p&gt; &lt;p&gt;It's open source if anyone wants to check it out or contribute. Happy to answer questions about how it works.&lt;/p&gt; &lt;p&gt;Look for &lt;code&gt;Symchat&lt;/code&gt; in Github.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neiellcare"&gt; /u/neiellcare &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otbtrh/built_a_local_chat_ui_for_ollama_thought_id_share/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otbtrh/built_a_local_chat_ui_for_ollama_thought_id_share/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otbtrh/built_a_local_chat_ui_for_ollama_thought_id_share/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T11:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou7kuh</id>
    <title>Sheet / Data Analyst Tools, Partial Functionality Achieved</title>
    <updated>2025-11-11T11:58:31+00:00</updated>
    <author>
      <name>/u/eworker8888</name>
      <uri>https://old.reddit.com/user/eworker8888</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eworker8888"&gt; /u/eworker8888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ou71yg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou7kuh/sheet_data_analyst_tools_partial_functionality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ou7kuh/sheet_data_analyst_tools_partial_functionality/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T11:58:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1otsq4j</id>
    <title>RAG. Embedding model. What do u prefer ?</title>
    <updated>2025-11-10T22:52:33+00:00</updated>
    <author>
      <name>/u/apolorotov</name>
      <uri>https://old.reddit.com/user/apolorotov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm doing some research on real-world RAG setups and I‚Äôm curious which embedding models people actually use in production (or serious side projects).&lt;/p&gt; &lt;p&gt;There are dozens of options now ‚Äî OpenAI text-embedding-3, BGE-M3, Voyage, Cohere, Qwen3, local MiniLM, etc. But despite all the talk about ‚Äúdomain-specific embeddings‚Äù, I almost never see anyone training or fine-tuning their own.&lt;/p&gt; &lt;p&gt;So I‚Äôd love to hear from you: 1. Which embedding model(s) are you using, and for what kind of data/tasks? 2. Have you ever tried to fine-tune your own? Why or why not?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apolorotov"&gt; /u/apolorotov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otsq4j/rag_embedding_model_what_do_u_prefer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otsq4j/rag_embedding_model_what_do_u_prefer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otsq4j/rag_embedding_model_what_do_u_prefer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T22:52:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou6ami</id>
    <title>Enterpise prices license</title>
    <updated>2025-11-11T10:43:43+00:00</updated>
    <author>
      <name>/u/LowTerrible9453</name>
      <uri>https://old.reddit.com/user/LowTerrible9453</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, we're working on an on-premise AI solution for an enterprise client and planning to integrate Ollama with a Web UI. Could you please share the pricing details or licensing requirements for enterprise use? Or if theres any consultant or business professionals who can support us let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LowTerrible9453"&gt; /u/LowTerrible9453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou6ami/enterpise_prices_license/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou6ami/enterpise_prices_license/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ou6ami/enterpise_prices_license/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T10:43:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou1x4c</id>
    <title>Built a CLI tool to reuse AI instructions for specific tasks</title>
    <updated>2025-11-11T06:07:04+00:00</updated>
    <author>
      <name>/u/Revolutionary-Judge9</name>
      <uri>https://old.reddit.com/user/Revolutionary-Judge9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ou1x4c/built_a_cli_tool_to_reuse_ai_instructions_for/"&gt; &lt;img alt="Built a CLI tool to reuse AI instructions for specific tasks" src="https://external-preview.redd.it/bGQxbG05aDRpazBnMUXN_YYfWjInCTvbB8XSnnOq8UmtvCL2MyTJG86DAUjb.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bcf65b87e5e6aff456c7e4867e6af1c06677a893" title="Built a CLI tool to reuse AI instructions for specific tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Before creating Askimo, I used to keep my prompt templates in notes - things like &amp;quot;summarize this document concisely&amp;quot; or &amp;quot;write documentation that lists only a class‚Äôs responsibilities, not its implementation details.&amp;quot;&lt;br /&gt; Every time, I‚Äôd copy and paste those instructions into chat to get the same kind of result.&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://github.com/haiphucnguyen/askimo"&gt;Askimo&lt;/a&gt;, a CLI tool that lets you turn those instructions into reusable ‚Äúrecipes.‚Äù&lt;br /&gt; Now I can just run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;askimo -r summarize &amp;lt;file_path&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;User can write their own recipe to instruct the AI response&lt;/p&gt; &lt;p&gt;The CLI tool can also run in interactive mode, just like a regular chat program. It works locally with Ollama, and can also connect to other LLM providers.&lt;/p&gt; &lt;p&gt;If you have the problems like mine, this tool is worth for checkout and you know why I made it :D. &lt;/p&gt; &lt;p&gt;Github Repo: &lt;a href="https://github.com/haiphucnguyen/askimo"&gt;https://github.com/haiphucnguyen/askimo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Revolutionary-Judge9"&gt; /u/Revolutionary-Judge9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q315f9h4ik0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou1x4c/built_a_cli_tool_to_reuse_ai_instructions_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ou1x4c/built_a_cli_tool_to_reuse_ai_instructions_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T06:07:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oubxxp</id>
    <title>model using 100% GPU in idle, is this normal?</title>
    <updated>2025-11-11T15:12:28+00:00</updated>
    <author>
      <name>/u/iMaexx_Backup</name>
      <uri>https://old.reddit.com/user/iMaexx_Backup</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oubxxp/model_using_100_gpu_in_idle_is_this_normal/"&gt; &lt;img alt="model using 100% GPU in idle, is this normal?" src="https://a.thumbs.redditmedia.com/LXyPa4BX87i_btgRKtZWhbozYyuaVqZLJEe8a0REts4.jpg" title="model using 100% GPU in idle, is this normal?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I'm new to this, so I apologize if the question is stupid. &lt;/p&gt; &lt;p&gt;I installed Ollama and Rocm on my Fedora distro, downloaded the latest gpt-oss:20b model and started it. &lt;/p&gt; &lt;p&gt;I noted that, as soon as I start the model, my GPU is and stays 100% utilized until I stop the model, even though I'm taking no inputs at all and the model is just idling. &lt;/p&gt; &lt;p&gt;Is this behavior normal? Shouldn't the utilization only go up after I did an input and the LLM is generating the response?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/olo09xm38n0g1.png?width=1168&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5a2966dad16985173674566b5bfe363312610cd"&gt;llm Idle&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/33oej42z8n0g1.png?width=1172&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d83596b58f27d4df12c1d8ea28d5d1df189b8ca9"&gt;llm off&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iMaexx_Backup"&gt; /u/iMaexx_Backup &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oubxxp/model_using_100_gpu_in_idle_is_this_normal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oubxxp/model_using_100_gpu_in_idle_is_this_normal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oubxxp/model_using_100_gpu_in_idle_is_this_normal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T15:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1otixju</id>
    <title>Ollama working well on the vs code</title>
    <updated>2025-11-10T16:49:23+00:00</updated>
    <author>
      <name>/u/Dry_Shower287</name>
      <uri>https://old.reddit.com/user/Dry_Shower287</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otixju/ollama_working_well_on_the_vs_code/"&gt; &lt;img alt="Ollama working well on the vs code" src="https://preview.redd.it/fotbt1hplg0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aff4a58a24ebef77a98f660883ac714cad18fdef" title="Ollama working well on the vs code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Shower287"&gt; /u/Dry_Shower287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fotbt1hplg0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otixju/ollama_working_well_on_the_vs_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otixju/ollama_working_well_on_the_vs_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T16:49:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1otwqev</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-11-11T01:44:51+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Note Management&lt;/li&gt; &lt;li&gt;Multi Collaborative Notebooks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otwqev/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otwqev/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otwqev/open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T01:44:51+00:00</published>
  </entry>
</feed>
