<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-24T02:39:34+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1raw6z8</id>
    <title>qwen3-coder-next on desktop</title>
    <updated>2026-02-21T16:58:15+00:00</updated>
    <author>
      <name>/u/BobcatLegitimate1497</name>
      <uri>https://old.reddit.com/user/BobcatLegitimate1497</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"&gt; &lt;img alt="qwen3-coder-next on desktop" src="https://preview.redd.it/2t8c4wugnvkg1.png?width=140&amp;amp;height=112&amp;amp;auto=webp&amp;amp;s=b4c92b7d5d4f3747b4fa2068f7da52d3878b28fa" title="qwen3-coder-next on desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Old computer with low-end GPUs (RTX 4060 Ti 16 Gb and 1660 6 Gb)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2t8c4wugnvkg1.png?width=1202&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f426423c92b99569186820894b7fa3a3557b5d82"&gt;https://preview.redd.it/2t8c4wugnvkg1.png?width=1202&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f426423c92b99569186820894b7fa3a3557b5d82&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CPU half-loaded, GPU loaded about 20%. Are there any ways to optimize it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BobcatLegitimate1497"&gt; /u/BobcatLegitimate1497 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T16:58:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb5t3c</id>
    <title>I have a substantial codebase that I want to analyse and build a proof-of-concept around for demonstration purposes</title>
    <updated>2026-02-21T23:22:44+00:00</updated>
    <author>
      <name>/u/eufemiapiccio77</name>
      <uri>https://old.reddit.com/user/eufemiapiccio77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;which local LLM options would allow me to work without the usage restrictions imposed by mainstream hosted providers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eufemiapiccio77"&gt; /u/eufemiapiccio77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb5t3c/i_have_a_substantial_codebase_that_i_want_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb5t3c/i_have_a_substantial_codebase_that_i_want_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb5t3c/i_have_a_substantial_codebase_that_i_want_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T23:22:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1rarsuo</id>
    <title>TIFU/PSA: didn‚Äôt check which GPU ollama was using and was stuck wondering why so slow</title>
    <updated>2026-02-21T13:59:43+00:00</updated>
    <author>
      <name>/u/IAmANobodyAMA</name>
      <uri>https://old.reddit.com/user/IAmANobodyAMA</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure where to tell this story (megathread?) but it made me laugh and has a teachable moment so I thought I would share.&lt;/p&gt; &lt;p&gt;TL;DR: I was running ollama on an old GPU by mistake ü§¶‚Äç‚ôÇÔ∏è&lt;/p&gt; &lt;p&gt;I have two local machines running ollama. My gaming rig has a 5070ti 16gb but isn‚Äôt always on, and my ‚Äúdedicated‚Äù unraid server **had** a 3060ti 8gb that was going to be my AI workhorse.&lt;/p&gt; &lt;p&gt;I chose models that would kick ass on the 5070 when online and more conservative models for the 3060 otherwise.&lt;/p&gt; &lt;p&gt;This is all still experimental/for learning so this setup is fines for me ‚Ä¶ except the unraid server was painfully slow. Took me way too long to figure out that‚Äôs because it was hitting an old GTX 1650 4gb card!! I forgot I swapped out the cards because I was going to build a gaming rig for my kid with the 3060.&lt;/p&gt; &lt;p&gt;I spent way too long researching models and trying to figure out why my ‚Äú3060‚Äù was offloading over 50% of qwen3:4b to my CPU. Since this is hosted on unraid I was convinced that another service (plex?) was using my GPU without permission. Nope, I‚Äôm just a doofus.&lt;/p&gt; &lt;p&gt;It wasn‚Äôt until running `nvidia-smi` in terminal that I realized my error.&lt;/p&gt; &lt;p&gt;Anyways, hope this makes someone chuckle as much as me. Anyone else have some fun ‚Äúdoh‚Äù moments?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IAmANobodyAMA"&gt; /u/IAmANobodyAMA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T13:59:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb8q6p</id>
    <title>Your Model Doesn't Know What It's About to Do. Mine Does. Introducing The Cradle. (V1.0)</title>
    <updated>2026-02-22T01:33:25+00:00</updated>
    <author>
      <name>/u/BiscottiDisastrous19</name>
      <uri>https://old.reddit.com/user/BiscottiDisastrous19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rb8q6p/your_model_doesnt_know_what_its_about_to_do_mine/"&gt; &lt;img alt="Your Model Doesn't Know What It's About to Do. Mine Does. Introducing The Cradle. (V1.0)" src="https://preview.redd.it/jeeinozfmwkg1.png?width=140&amp;amp;height=58&amp;amp;auto=webp&amp;amp;s=0bb84b45e3850cd11ca230d83904e5ff34b94573" title="Your Model Doesn't Know What It's About to Do. Mine Does. Introducing The Cradle. (V1.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BiscottiDisastrous19"&gt; /u/BiscottiDisastrous19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/u_BiscottiDisastrous19/comments/1rb12vw/your_model_doesnt_know_what_its_about_to_do_mine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb8q6p/your_model_doesnt_know_what_its_about_to_do_mine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb8q6p/your_model_doesnt_know_what_its_about_to_do_mine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T01:33:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb1wzf</id>
    <title>RX 9060 XT 16GB</title>
    <updated>2026-02-21T20:41:26+00:00</updated>
    <author>
      <name>/u/ButterscotchTop4598</name>
      <uri>https://old.reddit.com/user/ButterscotchTop4598</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Leute,&lt;/p&gt; &lt;p&gt;Gibt es irgendwo eine Anleitung f√ºr die Verwendung der oben genannten Grafikkarte unter Olama? Leider finde ich dazu nichts. Zudem w√ºrde mich interessieren welches Modell ihr f√ºr die Grafikkarte empfehlen k√∂nnt.&lt;/p&gt; &lt;p&gt;Vielen Dank f√ºr eure Hilfe!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ButterscotchTop4598"&gt; /u/ButterscotchTop4598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb1wzf/rx_9060_xt_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb1wzf/rx_9060_xt_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb1wzf/rx_9060_xt_16gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T20:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb8w1n</id>
    <title>NPU for local models?</title>
    <updated>2026-02-22T01:40:59+00:00</updated>
    <author>
      <name>/u/MemeGLS</name>
      <uri>https://old.reddit.com/user/MemeGLS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm running Qwen 3 on my laptop which has an NPU but I have no idea on how to use it (I was hoping that I could use a bigger model if I‚Äôm able to find a way to use the NPU).&lt;/p&gt; &lt;p&gt;Thanks a lot &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MemeGLS"&gt; /u/MemeGLS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb8w1n/npu_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb8w1n/npu_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb8w1n/npu_for_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T01:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbha7b</id>
    <title>Unable to pull model from ollama</title>
    <updated>2026-02-22T09:12:51+00:00</updated>
    <author>
      <name>/u/badasssravikumae</name>
      <uri>https://old.reddit.com/user/badasssravikumae</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to pull models from ollama but I am unable to do &lt;/p&gt; &lt;p&gt;I did ollama serve&lt;br /&gt; I deleted the cache and checked if the models is available and tried pulling the model even though I see this error: &lt;/p&gt; &lt;p&gt;pulling manifest &lt;/p&gt; &lt;p&gt;Error: pull model manifest: file does not exist&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badasssravikumae"&gt; /u/badasssravikumae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T09:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbdq0m</id>
    <title>Agent architectures for SLMs</title>
    <updated>2026-02-22T05:43:51+00:00</updated>
    <author>
      <name>/u/PangolinPossible7674</name>
      <uri>https://old.reddit.com/user/PangolinPossible7674</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;What kind of agent architectures are generally used with Small Language Models? In the past, I had tried ReAct with some 8B param models, and they failed. Recently, I have been trying out tool calling models via Ollama. Even with function calling, Qwen 3 8B appears to somewhat work, but some other 8B models don't seem to be so great.&lt;/p&gt; &lt;p&gt;Therefore, I was wondering what SLM-agent gas worked for the others. Does verbose docstrings for tools affect performance with SLMs? Alternatively, what smallest model size generally allows diverse tool usage in a reliable way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PangolinPossible7674"&gt; /u/PangolinPossible7674 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T05:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc0r8o</id>
    <title>Got $800 of credits on a cloud platform (for GPU usage). Anyone here that's into AI training and inference and could make use of it?</title>
    <updated>2026-02-22T23:09:21+00:00</updated>
    <author>
      <name>/u/DocumentFun9077</name>
      <uri>https://old.reddit.com/user/DocumentFun9077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have around 800 bucks worth of GPU usage credits on one of the major platform, those can be used specifically for GPU and clusters. So if any individual or hobbyist or anyone out here is training models or inference, or anything else, please contact! (not free btw, but selling at way less price)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocumentFun9077"&gt; /u/DocumentFun9077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0r8o/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0r8o/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc0r8o/got_800_of_credits_on_a_cloud_platform_for_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T23:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc0sqn</id>
    <title>Got $800 of credits on a cloud platform (for GPU usage). Anyone here that's into AI training and inference and could make use of it?</title>
    <updated>2026-02-22T23:11:05+00:00</updated>
    <author>
      <name>/u/DocumentFun9077</name>
      <uri>https://old.reddit.com/user/DocumentFun9077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have around 800 bucks worth of GPU usage credits on one of the major platform, those can be used specifically for GPU and clusters. So if any individual or hobbyist or anyone out here is training models or inference, or anything else, please contact! (not free btw, but selling at way less price)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocumentFun9077"&gt; /u/DocumentFun9077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0sqn/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0sqn/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc0sqn/got_800_of_credits_on_a_cloud_platform_for_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T23:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1rajqj6</id>
    <title>15,000+ tok/s on ChatJimmy: Is the "Model-on-Silicon" era finally starting?</title>
    <updated>2026-02-21T06:19:08+00:00</updated>
    <author>
      <name>/u/Significant-Topic433</name>
      <uri>https://old.reddit.com/user/Significant-Topic433</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"&gt; &lt;img alt="15,000+ tok/s on ChatJimmy: Is the &amp;quot;Model-on-Silicon&amp;quot; era finally starting?" src="https://preview.redd.it/bq69s0n5jskg1.jpg?width=140&amp;amp;height=66&amp;amp;auto=webp&amp;amp;s=fa3f690c9b529f18075dc6e27d8b984f7fcc4fcd" title="15,000+ tok/s on ChatJimmy: Is the &amp;quot;Model-on-Silicon&amp;quot; era finally starting?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôve been discussing local inference for years, but chatjimmy.ai just moved the goalposts. They are hitting 15,414 tokens per second using what they call &amp;quot;mask ROM recall fabric&amp;quot;‚Äîbasically etching the model weights directly into the silicon logic.&lt;/p&gt; &lt;p&gt;‚ÄãThis is a massive shift from our current setups. We‚Äôre used to general-purpose compute, but this is a dedicated ASIC. No HBM, no VRAM bottlenecks, just raw, hardcoded inference.&lt;/p&gt; &lt;p&gt;‚Äã I just invested in two Gigabyte AI TOP ATOM units (the ones based on the NVIDIA Spark / Grace Blackwell architecture). They are absolute beasts for training and fine-tuning with 128GB of unified memory, but seeing a dedicated chip do 15k tok/s makes me wonder: &lt;/p&gt; &lt;p&gt;‚ÄãDid I make the right call with the AI TOP Spark units for local dev, or are we going to see these specialized ASIC cards hit the market soon and make general-purpose desktop AI look like dial-up?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant-Topic433"&gt; /u/Significant-Topic433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rajqj6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T06:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbt4di</id>
    <title>Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation</title>
    <updated>2026-02-22T18:17:48+00:00</updated>
    <author>
      <name>/u/dorbeats</name>
      <uri>https://old.reddit.com/user/dorbeats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"&gt; &lt;img alt="Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation" src="https://preview.redd.it/vj71i95883lg1.jpg?width=140&amp;amp;height=140&amp;amp;auto=webp&amp;amp;s=253fed1c03e9709c1d506fd05f1a1a3300ef8eda" title="Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this feasible on modest hardware?&lt;/p&gt; &lt;p&gt;Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.boomspot.com/llama-3-1-70b-on-rtx-3090-bypassing-cpu-for-ai-innovation"&gt;https://www.boomspot.com/llama-3-1-70b-on-rtx-3090-bypassing-cpu-for-ai-innovation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vj71i95883lg1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c0859a6b570010f0108ab4369475820f191095ed"&gt;https://preview.redd.it/vj71i95883lg1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c0859a6b570010f0108ab4369475820f191095ed&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dorbeats"&gt; /u/dorbeats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T18:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc3fkr</id>
    <title>Model requires more system memory (eventhough I have enough)</title>
    <updated>2026-02-23T01:03:23+00:00</updated>
    <author>
      <name>/u/gfejer</name>
      <uri>https://old.reddit.com/user/gfejer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have two Tesla P40s passed through as vGPU profiles to a Ubuntu 24.04 VM. As an example I can run gpt-oss just fine and the GPUs get recognized by the system, but when it comes to running Llama 3.3 which is a 43GB model (my 2*24GB VRAM should be enough right?) gives me an error that I don‚Äôt have enough system memory.&lt;/p&gt; &lt;p&gt;I am guessing that for some reason it tries to run the model on the CPU, but I don‚Äôt understand why‚Ä¶ Are there any possible fixes for this issue?&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gfejer"&gt; /u/gfejer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:03:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbq3dy</id>
    <title>Ollama for Dummies</title>
    <updated>2026-02-22T16:24:37+00:00</updated>
    <author>
      <name>/u/catbutchie</name>
      <uri>https://old.reddit.com/user/catbutchie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone needs to write a book. Right now my mentor is ChatGPT. There are so many parameters i just tell it my issue and it tells me what to change. Some small tweaks are significant performance adjustment. I‚Äôm new obviously. I‚Äôd like to know why I‚Äôm doing what I‚Äôm doing so I can be more in control. I‚Äôm using silly tavern and ollama for self hosted chat. Suggestions needed. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/catbutchie"&gt; /u/catbutchie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T16:24:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcaxo7</id>
    <title>Qwen3VL:30b-Q6-Thinking</title>
    <updated>2026-02-23T07:23:02+00:00</updated>
    <author>
      <name>/u/CheekyMonkeee</name>
      <uri>https://old.reddit.com/user/CheekyMonkeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been trying for hours to make a Modelfile that will actually make this work in Ollama. I have the ggufs for both the base model and the mmproj together in a folder and the create command finishes successfully. I get the directory and the blobs and the chat accepts images, but I get a 500 error when I prompt. Downloaded both ggufs from huggingface.&lt;/p&gt; &lt;p&gt;At this point, I don‚Äôt even care if it‚Äôs the best model for my use case anymore. I‚Äôm fairly sure it‚Äôs not a memory issue as I‚Äôm on a 5090 with 64GB of system RAM, and it will run the Q4 32b non-thinking model that I downloaded straight from Ollama (even though that spills over into RAM).&lt;/p&gt; &lt;p&gt;I‚Äôve just never had to do the modelfile thing with a download from huggingface before (still fairly new to this) and I don‚Äôt want to give up. I have to sleep, but any advice would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CheekyMonkeee"&gt; /u/CheekyMonkeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcaxo7/qwen3vl30bq6thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcaxo7/qwen3vl30bq6thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcaxo7/qwen3vl30bq6thinking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T07:23:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc7fd1</id>
    <title>AMD 5550xt Macbook Pro 2019</title>
    <updated>2026-02-23T04:11:48+00:00</updated>
    <author>
      <name>/u/tubaraodogroove</name>
      <uri>https://old.reddit.com/user/tubaraodogroove</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm planning to purchase this MacBook second-hand just to run some models that my 2017 laptop can't handle.&lt;/p&gt; &lt;p&gt;However, based on what I found during my research, Ollama doesn‚Äôt recognize this graphics card as a usable GPU. Can someone explain whether this is a good deal for running Ollama, or if it simply won‚Äôt work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tubaraodogroove"&gt; /u/tubaraodogroove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc7fd1/amd_5550xt_macbook_pro_2019/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc7fd1/amd_5550xt_macbook_pro_2019/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc7fd1/amd_5550xt_macbook_pro_2019/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T04:11:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rccbn8</id>
    <title>Built a honeypot token library for AI agents ‚Äî detects prompt injection the moment it succeeds</title>
    <updated>2026-02-23T08:49:56+00:00</updated>
    <author>
      <name>/u/Responsible-Yak-9657</name>
      <uri>https://old.reddit.com/user/Responsible-Yak-9657</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rccbn8/built_a_honeypot_token_library_for_ai_agents/"&gt; &lt;img alt="Built a honeypot token library for AI agents ‚Äî detects prompt injection the moment it succeeds" src="https://external-preview.redd.it/y-WYMevcIN_5-ZHiVkm0emVpTgZqx82u0L69rZGk6FM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18cf100549c9b843e934f38bccc2bb7dc338f724" title="Built a honeypot token library for AI agents ‚Äî detects prompt injection the moment it succeeds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible-Yak-9657"&gt; /u/Responsible-Yak-9657 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/aiagents/comments/1rby96z/built_a_honeypot_token_library_for_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rccbn8/built_a_honeypot_token_library_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rccbn8/built_a_honeypot_token_library_for_ai_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T08:49:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcgy0h</id>
    <title>Help Setting up - nanobot?</title>
    <updated>2026-02-23T13:07:26+00:00</updated>
    <author>
      <name>/u/lawfulcrispy</name>
      <uri>https://old.reddit.com/user/lawfulcrispy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lawfulcrispy"&gt; /u/lawfulcrispy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1rcgxpv/help_setting_up_nanobot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcgy0h/help_setting_up_nanobot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcgy0h/help_setting_up_nanobot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T13:07:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc4l9j</id>
    <title>16GB VRAM for mode agent</title>
    <updated>2026-02-23T01:56:50+00:00</updated>
    <author>
      <name>/u/ColdTransition5828</name>
      <uri>https://old.reddit.com/user/ColdTransition5828</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was hoping to enjoy something similar to Cursor on my PC. I even bought what was supposed to be a mid-range card. But the results are disappointing.&lt;/p&gt; &lt;p&gt;After studying it, I realized I'm missing the core agent and a better Ollama model that accepts tools. But honestly, I'm bored. What do you recommend I do to get the most out of local models with my 16GB of VRAM?&lt;/p&gt; &lt;p&gt;I mostly do full-track coding and Java.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ColdTransition5828"&gt; /u/ColdTransition5828 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:56:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcurz4</id>
    <title>Building a service and PWA for Ollama (and other models) with SQLite RAG and artifacts. Is this project interesting to the community?</title>
    <updated>2026-02-23T21:37:21+00:00</updated>
    <author>
      <name>/u/pokemondodo</name>
      <uri>https://old.reddit.com/user/pokemondodo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! For almost a year, I‚Äôve been working on a project that serves as a smart, functional, and secure UI for LLM models. There are many ready-made solutions, but most often they require complex Docker setups or writing configurations. Projects with a simpler launch but similar functionality are usually paid.&lt;/p&gt; &lt;p&gt;My solution works in the browser or via a PWA application. Absolutely all computations happen on the user's device. The project has no server at all; hosting with SSL is only needed to organize the PWA application.&lt;/p&gt; &lt;p&gt;In general, the project will work even without internet if the models are deployed locally and the PWA application is already downloaded.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical points I focused on:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Client-side data storage&lt;/strong&gt; ‚Äî a SQLite database is used. Once created, you can place it anywhere; the browser will only ask for permission to write to the file. Everything is stored in the database: chats, messages, embeddings, system settings, artifacts. You can change databases whenever you want.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic module&lt;/strong&gt; ‚Äî through triggers, the application extracts any important facts about the user. Name, other people, allergies, city of residence, favorite games ‚Äî anything. Everything is stored in the selected database as a text fact and an embedding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Heuristic module&lt;/strong&gt; ‚Äî the project has a mascot that displays its emotions in the form of stickers under each message. This can be turned off in the settings. Besides this, the assistant has its own mood. Through a mathematical expression, its final behavior is calculated based on variables: general mood, level of sarcasm, level of humor. This doesn't affect the quality of the answer or tasks, but it affects human perception ‚Äî answers can be dry, restrained, or sarcastic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Artifacts&lt;/strong&gt; ‚Äî the project has a library of documents and an application. There are 4 types of artifacts in total: games, applications, documents, analytical documents. You can ask to generate a document by prompt or by feeding information through a file attachment.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Working with files&lt;/strong&gt; ‚Äî PDF, DOCX, XLSX, TXT, and any files that can be interpreted as text or code are accepted. Nothing goes anywhere beyond your device; text is extracted at the moment of the request by the application or the browser tab.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt; ‚Äî the database itself you work in is not encrypted, only the password. This is done so that you don't lose access to your documents and chats even without using the project. But in the project, connecting any database or entering settings happens only through a password.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Operating modes&lt;/strong&gt; ‚Äî there are three modes: Kids, Teens, and Adult. I should probably write a whole separate post for this. Briefly: the kids' mode is protected from adult and dangerous topics, and the assistant will not give answers to homework, only tell and help with what and how to solve.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compatibility&lt;/strong&gt; ‚Äî it will work not only with local models but also through cloud APIs. It's not as secure, of course, but you can buy tokens in Gemini or a plan in OpenRouter ‚Äî everything will work perfectly. There are three connection providers in total: Gemini, OpenRouter, Custom (Ollama, etc.). You can change providers on the fly in any chat; the assistant's behavior will only change due to the power of the model itself.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why I‚Äôm writing this:&lt;/strong&gt; This is a completely free project. It started as a tool for personal needs to check semantics in another project.&lt;/p&gt; &lt;p&gt;The project has no ads, subscriptions, price plans, or anywhere you need to enter your card details.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would the community be interested in a full technical review with a demonstration of functionality?&lt;/strong&gt; Or is this too niche a topic for such a sub?&lt;/p&gt; &lt;p&gt;If a demonstration is needed, won't I get banned for advertising or promotion? And what aspects do you want me to reveal in more detail ‚Äî general demonstration or a deep dive into code and architecture?&lt;/p&gt; &lt;p&gt;I'll be glad to hear your opinion and am ready to answer any questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pokemondodo"&gt; /u/pokemondodo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcurz4/building_a_service_and_pwa_for_ollama_and_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcurz4/building_a_service_and_pwa_for_ollama_and_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcurz4/building_a_service_and_pwa_for_ollama_and_other/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T21:37:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcv5by</id>
    <title>I turned MCP tools into standard CLI commands to solve context pollution</title>
    <updated>2026-02-23T21:51:03+00:00</updated>
    <author>
      <name>/u/_pdp_</name>
      <uri>https://old.reddit.com/user/_pdp_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;One of the biggest issues with MCP is context pollution. Loading a single service might be fine, but when you have 10 or 100 of them, you're spending most of your valuable context on tool definitions.&lt;/p&gt; &lt;p&gt;The usual solution is to use an MCP gateway that exposes a single generic function. Unfortunately, this doesn't work well because with a single function, the context of how and when to use each tool is completely lost.&lt;/p&gt; &lt;p&gt;What I've found is that the best approach is usually the Unix way. So what if, instead of loading MCP tools into the context, you make them available as standard CLIs? Now you can write your own SKILL.md and be happy.&lt;/p&gt; &lt;p&gt;This is what MCPShim does. It starts a background daemon that keeps all your MCPs nicely organized. It supports all authentication types (including OAuth, even when you don't have a publicly exposed HTTP server). Most importantly, you can now call into any of these MCPs and tools like standard commands. MCPShim even automatically generates bashrc aliases and bash completion to make things super easy.&lt;/p&gt; &lt;p&gt;As an added benefit, if you develop your own agents you no longer need to bolt on an MCP library and handle everything manually. You can focus on building a lean, high-quality AI agent and leave the MCP work to system processes.&lt;/p&gt; &lt;p&gt;The link to the open-source project is in the comments below.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_pdp_"&gt; /u/_pdp_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcv5by/i_turned_mcp_tools_into_standard_cli_commands_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcv5by/i_turned_mcp_tools_into_standard_cli_commands_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcv5by/i_turned_mcp_tools_into_standard_cli_commands_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T21:51:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc8xvy</id>
    <title>spend more time downloading models than actually using them</title>
    <updated>2026-02-23T05:29:41+00:00</updated>
    <author>
      <name>/u/Sharp-Mouse9049</name>
      <uri>https://old.reddit.com/user/Sharp-Mouse9049</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;qwen 2.5 dropped and suddenly mixtral is dead to me. downloaded the 72b. ran it once. went back to 7b cause i dont actually need 72b for anything i do&lt;/p&gt; &lt;p&gt;got like 200gb of models sitting on my drive. couldnt tell you the difference between half of them without checking the folder names&lt;/p&gt; &lt;p&gt;every week theres a new one thats supposedly better and i gotta have it. run some vibes check. wow this one feels smarter. back to doing the same three things i always do&lt;/p&gt; &lt;p&gt;its like im collecting pokemon but the pokemon just sit there&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sharp-Mouse9049"&gt; /u/Sharp-Mouse9049 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc8xvy/spend_more_time_downloading_models_than_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc8xvy/spend_more_time_downloading_models_than_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc8xvy/spend_more_time_downloading_models_than_actually/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T05:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcf94q</id>
    <title>What GPU do you use?</title>
    <updated>2026-02-23T11:43:10+00:00</updated>
    <author>
      <name>/u/Keensworth</name>
      <uri>https://old.reddit.com/user/Keensworth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've recently started using Ollama with an old GPU I had laying around.&lt;/p&gt; &lt;p&gt;Problem is that my GTX 1660S only got 6Gb VRAM and you can't do much with that.&lt;/p&gt; &lt;p&gt;I can run Mistral 7B Instruct but he sucks.&lt;/p&gt; &lt;p&gt;What hardware are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Keensworth"&gt; /u/Keensworth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T11:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc3srb</id>
    <title>I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review</title>
    <updated>2026-02-23T01:19:58+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"&gt; &lt;img alt="I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review" src="https://external-preview.redd.it/anBvOHE4ZWxiNWxnMYmmpi9UU3yP9yrC87ePDCyv5Mn4iZk_AUHCQZq2TOQ_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=452fe5fc9cf576221ea71aff1d15b07c8fa36f35" title="I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all ‚Äî been experimenting with running a local AI agent using ClawBot + Ollama and wanted to share what actually happened.&lt;/p&gt; &lt;p&gt;Link to full tutorial: &lt;a href="https://www.youtube.com/watch?v=FxyQkj95VXs"&gt;https://www.youtube.com/watch?v=FxyQkj95VXs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yes, ClawBot + Ollama works on Mac. Does it work well? Depends on what you mean by &amp;quot;work&amp;quot;&lt;/li&gt; &lt;li&gt;With an 8B model, agentic tasks are limited. Basic Q&amp;amp;A? Fine. Anything complex? It'll humble you real quick&lt;/li&gt; &lt;li&gt;Should you expect ChatGPT-level speed? Absolutely not. Go make a coffee while you wait üòÖ&lt;/li&gt; &lt;li&gt;Is it worth it for learning the stack and experimenting locally for free? Honestly yes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What worked&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Setup is cleaner than expected - VS Code, JSON config, localhost dashboard, done. I have no luck setting up ollama using their onboarding. So...I went straight to config file.&lt;/li&gt; &lt;li&gt;Ollama model switching is straightforward once you understand the config structure&lt;/li&gt; &lt;li&gt;Great for understanding how local AI agent setups actually work under the hood&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What didn't&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Speed is rough on anything under 32GB RAM&lt;/li&gt; &lt;li&gt;8B models hit their ceiling fast on multi-step reasoning and real agentic workflows. Keep context window low.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1ee663elb5lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcxt3m</id>
    <title>How are you monitoring your Ollama calls/usage?</title>
    <updated>2026-02-23T23:32:55+00:00</updated>
    <author>
      <name>/u/gkarthi280</name>
      <uri>https://old.reddit.com/user/gkarthi280</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"&gt; &lt;img alt="How are you monitoring your Ollama calls/usage?" src="https://preview.redd.it/b8gxcch9xblg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=ced5ee353224c286bd1ab54c4819b57ff238fad4" title="How are you monitoring your Ollama calls/usage?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Ollama in my LLM applications and wanted some feedback on what type of metrics people here would find useful to track in an app that eventually would go into prod. I used OpenTelemetry to instrument my app by following this&lt;a href="https://signoz.io/docs/ollama-monitoring/"&gt; Ollama observability guide&lt;/a&gt; and was able to create this dashboard.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b8gxcch9xblg1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc90458a61e2e80c8ed5e283edc3e914ccbddcd6"&gt;Ollama dashboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It tracks things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;token usage&lt;/li&gt; &lt;li&gt;error rate&lt;/li&gt; &lt;li&gt;number of requests&lt;/li&gt; &lt;li&gt;latency&lt;/li&gt; &lt;li&gt;LLM provider and model &amp;amp; token distribution&lt;/li&gt; &lt;li&gt;logs and errors&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Are there any important metrics that you would want to keep track of in prod for monitoring your Ollama usage that aren't included here? And have you guys found any other ways to monitor these llm calls made through ollama?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1r8j5ob"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gkarthi280"&gt; /u/gkarthi280 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T23:32:55+00:00</published>
  </entry>
</feed>
