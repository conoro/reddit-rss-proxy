<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-11T07:37:17+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qzrwlb</id>
    <title>Recommend model for openclaw clawdbot running locally on old laptop 4gb vram 16g ram asus</title>
    <updated>2026-02-09T02:21:02+00:00</updated>
    <author>
      <name>/u/Flux-of-Time</name>
      <uri>https://old.reddit.com/user/Flux-of-Time</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flux-of-Time"&gt; /u/Flux-of-Time &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzrwlb/recommend_model_for_openclaw_clawdbot_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzrwlb/recommend_model_for_openclaw_clawdbot_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzrwlb/recommend_model_for_openclaw_clawdbot_running/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T02:21:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0jo10</id>
    <title>HoopyClaw: Run your OpenClaw agent 24/7 in the cloud without buying dedicated hardware</title>
    <updated>2026-02-09T23:02:22+00:00</updated>
    <author>
      <name>/u/ResourcePuzzled2387</name>
      <uri>https://old.reddit.com/user/ResourcePuzzled2387</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone&lt;/p&gt; &lt;p&gt;If you've been following the OpenClaw hype (and it's well deserved ‚Äî the project is incredible), you know the biggest barrier to entry: &lt;strong&gt;you need dedicated hardware running 24/7&lt;/strong&gt;. Most people are buying Mac Minis, repurposing old laptops, or spinning up VPS instances and configuring everything manually.&lt;/p&gt; &lt;p&gt;We're building&lt;a href="https://hoopyclaw.com"&gt; &lt;strong&gt;HoopyClaw&lt;/strong&gt;&lt;/a&gt; to solve exactly that ‚Äî a managed cloud platform where you can deploy OpenClaw or nanobot agents in seconds, without touching a terminal or managing infrastructure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create an account&lt;/li&gt; &lt;li&gt;Choose your engine ‚Äî &lt;strong&gt;OpenClaw&lt;/strong&gt; (full power: deep reasoning, browser control, skills, persistent memory) or &lt;strong&gt;nanobot&lt;/strong&gt; (ultra-lightweight, ~4,000 lines of code, perfect for simple assistants)&lt;/li&gt; &lt;li&gt;Connect your platforms ‚Äî Discord, Telegram, WhatsApp, Slack, Email, and more&lt;/li&gt; &lt;li&gt;Deploy ‚Äî your agent is live and running 24/7&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why we're building this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;OpenClaw is amazing, but self-hosting isn't for everyone. Beyond the hassle of keeping a machine running 24/7, there's a real &lt;strong&gt;security concern&lt;/strong&gt;: running an AI agent with full system access on your personal computer means exposing your files, credentials, and private data. If something goes wrong ‚Äî a misconfigured skill, an unexpected command ‚Äî it's &lt;em&gt;your&lt;/em&gt; machine at risk.&lt;/p&gt; &lt;p&gt;With HoopyClaw, &lt;strong&gt;your agent runs in an isolated cloud instance&lt;/strong&gt;, completely separated from your personal environment. Your data stays safe, your machine stays yours, and your agent gets its own sandboxed space to operate. All on European infrastructure, fully GDPR compliant.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What you get:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Always-on cloud deployment&lt;/strong&gt; ‚Äî no Mac Mini sitting in your closet&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Isolated instances&lt;/strong&gt; ‚Äî your agent never touches your personal machine or data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;9+ platforms&lt;/strong&gt; connected out of the box&lt;/li&gt; &lt;li&gt;&lt;strong&gt;European infrastructure&lt;/strong&gt;, encrypted and GDPR compliant&lt;/li&gt; &lt;li&gt;&lt;strong&gt;One-click setup&lt;/strong&gt; ‚Äî no CLI, no DevOps knowledge required&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Monitor &amp;amp; optimize&lt;/strong&gt; your agent with real conversation data&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Pricing:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Nano plan&lt;/strong&gt; (nanobot engine): ‚Ç¨39/mo&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Claw plan&lt;/strong&gt; (OpenClaw engine): ‚Ç¨59/mo&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom/Enterprise&lt;/strong&gt;: contact us&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear your feedback ‚Äî especially from people who tried self-hosting OpenClaw and found it painful, or from those who wanted to try it but didn't have the hardware.&lt;/p&gt; &lt;p&gt;üîó&lt;a href="https://hoopyclaw.com"&gt; https://hoopyclaw.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResourcePuzzled2387"&gt; /u/ResourcePuzzled2387 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0jo10/hoopyclaw_run_your_openclaw_agent_247_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0jo10/hoopyclaw_run_your_openclaw_agent_247_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0jo10/hoopyclaw_run_your_openclaw_agent_247_in_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T23:02:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzh0op</id>
    <title>I created a small AI Agent</title>
    <updated>2026-02-08T18:43:20+00:00</updated>
    <author>
      <name>/u/Rough_Philosopher877</name>
      <uri>https://old.reddit.com/user/Rough_Philosopher877</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys.. I know it's not so big thing.. just for fun I created a Small AI Agent:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/tysonchamp/Small-AI-Agent"&gt;https://github.com/tysonchamp/Small-AI-Agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love the feedback of the community.. and any suggestions of new ideas.&lt;/p&gt; &lt;p&gt;I created this for my day to day activities.. such as setup reminders, take notes, monitor all my client's website (if they are all ok or not).. monitor all my servers, connecting it to my custom erp for due invoice fetching, project management etc &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rough_Philosopher877"&gt; /u/Rough_Philosopher877 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T18:43:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzrd8g</id>
    <title>DaveLovable is an open-source, AI-powered web UI/UX development platform</title>
    <updated>2026-02-09T01:54:32+00:00</updated>
    <author>
      <name>/u/LeadingFun1849</name>
      <uri>https://old.reddit.com/user/LeadingFun1849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on this project for a while.&lt;/p&gt; &lt;p&gt;DaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch. It combines cutting-edge AI orchestration with browser-based execution to offer the most advanced open-source alternative for rapid frontend prototyping.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/davidmonterocrespo24/DaveLovable"&gt;https://github.com/davidmonterocrespo24/DaveLovable&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeadingFun1849"&gt; /u/LeadingFun1849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T01:54:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0b835</id>
    <title>Free Strix Halo performance!</title>
    <updated>2026-02-09T17:55:04+00:00</updated>
    <author>
      <name>/u/Potential_Block4598</name>
      <uri>https://old.reddit.com/user/Potential_Block4598</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Potential_Block4598"&gt; /u/Potential_Block4598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1r0b7p8/free_strix_halo_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0b835/free_strix_halo_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0b835/free_strix_halo_performance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T17:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r03gd0</id>
    <title>Ollama and claude code</title>
    <updated>2026-02-09T12:54:47+00:00</updated>
    <author>
      <name>/u/Separate_Coconut_592</name>
      <uri>https://old.reddit.com/user/Separate_Coconut_592</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, i dont know what im doing wrong but my models can't write to local files. Regardless of model . Am i doing something wrong. Currently on fedora 42. Gemini CLI is fine , claude code is fine just local models cant do it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Separate_Coconut_592"&gt; /u/Separate_Coconut_592 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r03gd0/ollama_and_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r03gd0/ollama_and_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r03gd0/ollama_and_claude_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T12:54:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzzd5x</id>
    <title>Qwen 3 coder next for R coding (academic)</title>
    <updated>2026-02-09T09:01:29+00:00</updated>
    <author>
      <name>/u/Bahaal_1981</name>
      <uri>https://old.reddit.com/user/Bahaal_1981</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am an academic. I have claude via work and it excels at R coding and building Shiny apps with little prompting (Opus 4.5 but Sonnet does fairly well also). This is both for teaching / research. But I also want local models (for various reasons, privacy, reproducibility, etc). I have ollama with cohere / Mistral Large / phi reasoning, running on an M4 Max with 128 gb ram. Reading up I think qwen coder next might do better:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/qwen3-coder-next"&gt;https://ollama.com/library/qwen3-coder-next&lt;/a&gt; --&amp;gt; 85GB model -- additional settings needed?&lt;/p&gt; &lt;p&gt;I also looked for Kimi but could only find the cloud version. Any advice? Many thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bahaal_1981"&gt; /u/Bahaal_1981 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T09:01:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0p7b2</id>
    <title>Model reccomendations</title>
    <updated>2026-02-10T03:00:15+00:00</updated>
    <author>
      <name>/u/No-Mortgage4154</name>
      <uri>https://old.reddit.com/user/No-Mortgage4154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so I recently found out about ollama and was wondering about like what models are good to use for coding and other stuff&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mortgage4154"&gt; /u/No-Mortgage4154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0p7b2/model_reccomendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0p7b2/model_reccomendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0p7b2/model_reccomendations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T03:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0pu1y</id>
    <title>Need suggestion</title>
    <updated>2026-02-10T03:28:38+00:00</updated>
    <author>
      <name>/u/harneetsingh17</name>
      <uri>https://old.reddit.com/user/harneetsingh17</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/harneetsingh17"&gt; /u/harneetsingh17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalAIServers/comments/1r0phxl/need_suggestion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0pu1y/need_suggestion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0pu1y/need_suggestion/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T03:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1r07fwp</id>
    <title>Izwi - A local audio inference engine written in Rust</title>
    <updated>2026-02-09T15:39:06+00:00</updated>
    <author>
      <name>/u/zinyando</name>
      <uri>https://old.reddit.com/user/zinyando</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r07fwp/izwi_a_local_audio_inference_engine_written_in/"&gt; &lt;img alt="Izwi - A local audio inference engine written in Rust" src="https://external-preview.redd.it/eXviKCvfn4aFzZdhc6p4q23Sa8NTTdYj4kpCRejt2Io.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82e5ee56fd8e2f07ab6f70585c28ac58010947bd" title="Izwi - A local audio inference engine written in Rust" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been building Izwi, a fully local audio inference stack for speech workflows. No cloud APIs, no data leaving your machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's inside:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text-to-speech &amp;amp; speech recognition (ASR)&lt;/li&gt; &lt;li&gt;Voice cloning &amp;amp; voice design&lt;/li&gt; &lt;li&gt;Chat/audio-chat models&lt;/li&gt; &lt;li&gt;OpenAI-compatible API (&lt;code&gt;/v1&lt;/code&gt; routes)&lt;/li&gt; &lt;li&gt;Apple Silicon acceleration (Metal)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Stack:&lt;/strong&gt; Rust backend (Candle/MLX), React/Vite UI, CLI-first workflow.&lt;/p&gt; &lt;p&gt;Everything runs locally. Pull models from Hugging Face, benchmark throughput, or just &lt;code&gt;izwi tts &amp;quot;Hello world&amp;quot;&lt;/code&gt; and go.&lt;/p&gt; &lt;p&gt;Apache 2.0, actively developed. Would love feedback from anyone working on local ML in Rust!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/agentem-ai/izwi"&gt;https://github.com/agentem-ai/izwi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zinyando"&gt; /u/zinyando &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/agentem-ai/izwi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r07fwp/izwi_a_local_audio_inference_engine_written_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r07fwp/izwi_a_local_audio_inference_engine_written_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T15:39:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0u6mb</id>
    <title>Optimization</title>
    <updated>2026-02-10T07:15:18+00:00</updated>
    <author>
      <name>/u/Particular_Clue_893</name>
      <uri>https://old.reddit.com/user/Particular_Clue_893</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rig Specs: GPU nvidia 5090&lt;/p&gt; &lt;p&gt;CPU 9800x3d&lt;/p&gt; &lt;p&gt;32gb DDR5 ram&lt;/p&gt; &lt;p&gt;Edit:Windows 10&lt;/p&gt; &lt;p&gt;Using Ollama to LLM Claude Code by utilizing the Qwen3coder:30b and qlm4.7-flash.&lt;/p&gt; &lt;p&gt;I haven‚Äôt gotten too far into this whole process yet, have just been playing with qwen coder to help me build a financial literacy app, and getting around the first learning curve with optimizing my system.&lt;/p&gt; &lt;p&gt;I‚Äôm having trouble with 2 things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I cannot for the life of me see my tokens per second, the reason I‚Äôm trying to get this to show up is so I can benchmark my system so I can tell what runs more efficient.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If someone could please help me out that would be much appreciated. Do I need to switch off Ollama? I‚Äôve seen some people say LM is the most user friendly. Any advice helps, most importantly I just want to be able to benchmark my system in my terminal so I can figure out what system I want to run.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Particular_Clue_893"&gt; /u/Particular_Clue_893 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0u6mb/optimization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0u6mb/optimization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0u6mb/optimization/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T07:15:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0d432</id>
    <title>Local AI for small company</title>
    <updated>2026-02-09T19:02:02+00:00</updated>
    <author>
      <name>/u/LiteLive</name>
      <uri>https://old.reddit.com/user/LiteLive</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I‚Äòm looking into options to get local AI running for my company.&lt;/p&gt; &lt;p&gt;We do technical consulting and love to use AI for skimming through technical documents and pinpointing information down.&lt;/p&gt; &lt;p&gt;We are burning through Tokens and I‚Äòm trying to save us some money but having local AI would actually allow us to use it on sensible data. Not all our customers allow cloud based AI assistance, because even when the providers say they don‚Äôt train / store data, we cannot be certain.&lt;/p&gt; &lt;p&gt;What do we want to do?&lt;/p&gt; &lt;p&gt;I envision a paperless-ngx instance where we can upload a shitton of unsorted / unknown data. We have a solid promt&lt;/p&gt; &lt;p&gt;That categorizes the data and indexes the files. Allocates it to the right customer / project. And makes it accessible, searchable and tags them according to our need.&lt;/p&gt; &lt;p&gt;Right now we use cloud providers to do this, but as I mentioned before we are burning through tokens. Especially in the beginning of projects when we digitalize a wheelbarrow full of hard copies folders.&lt;/p&gt; &lt;p&gt;My colleague said we should just buy a Mac mini and use that as an Ollama host, but I hate Apple with a passion (while writing this on an iPhone‚Ä¶).&lt;/p&gt; &lt;p&gt;I was looking at the Minisforum MS-S1 Max, hardware looks promising. I want to run Proxmox PVE 9 on it, then pass the GPU to the LXC where Ollama will reside.&lt;/p&gt; &lt;p&gt;Is this a viable path? &lt;/p&gt; &lt;p&gt;My calculation is, if we spent 500‚Ç¨ on Tokens per month, and we can save half of that with this device, it would basically pay itself off within a year. And looking back at the last 12 months, I can see a steady increase in tokens for us. While enabling us to also process highly sensible data with AI.&lt;/p&gt; &lt;p&gt;What models can I realistically run on this hardware? I was thinking something like Llama4:Maverik will probably work for us.&lt;/p&gt; &lt;p&gt;Would you guys maybe recommend a different model for our ‚Äûbackground‚Äú usecase? Are there other ways to streamline our workflow maybe?&lt;/p&gt; &lt;p&gt;To be fair I don‚Äôt want to get rid of all cloud AI, as I fully understand that their models will always be more sophisticated and faster.&lt;/p&gt; &lt;p&gt;Looking forward for you comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiteLive"&gt; /u/LiteLive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0d432/local_ai_for_small_company/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0d432/local_ai_for_small_company/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0d432/local_ai_for_small_company/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T19:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0n1sg</id>
    <title>We won a hackathon with this project using Ollama. But is it actually useful?</title>
    <updated>2026-02-10T01:25:10+00:00</updated>
    <author>
      <name>/u/BriefAd2120</name>
      <uri>https://old.reddit.com/user/BriefAd2120</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: I built a 3d memory layer to visualize your chats with a custom MCP server to inject relevant context, Looking for feedback!&lt;/p&gt; &lt;p&gt;Cortex turns raw chat history into reusable context using hybrid retrieval (about 65% keyword, 35% semantic), local summaries with Qwen 2.5 8B, and auto system prompts so setup goes from minutes to seconds.&lt;/p&gt; &lt;p&gt;It also runs through a custom MCP server with search + fetch tools, so external LLMs like Claude can pull the right memory at inference time.&lt;/p&gt; &lt;p&gt;And because scrolling is pain, I added a 3D brain-style map built with UMAP, K-Means, and Three.js so you can explore conversations like a network instead of a timeline.&lt;/p&gt; &lt;p&gt;We won the hackathon with it, but I want a reality check: is this actually useful, or just a cool demo?&lt;/p&gt; &lt;p&gt;YouTube demo: &lt;a href="https://www.youtube.com/watch?v=SC_lDydnCF4"&gt;https://www.youtube.com/watch?v=SC_lDydnCF4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LinkedIn post: &lt;a href="https://www.linkedin.com/feed/update/urn:li:activity:7426518101162205184/"&gt;https://www.linkedin.com/feed/update/urn:li:activity:7426518101162205184/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github Link (pls star itü•∫): &lt;a href="https://github.com/Vibhor7-7/Cortex-CxC"&gt;https://github.com/Vibhor7-7/Cortex-CxC&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BriefAd2120"&gt; /u/BriefAd2120 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T01:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0q55x</id>
    <title>any good models?</title>
    <updated>2026-02-10T03:43:05+00:00</updated>
    <author>
      <name>/u/No-Mortgage4154</name>
      <uri>https://old.reddit.com/user/No-Mortgage4154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i recently found out about ollama and how its like a local ai and was wondering what are some good models out there my pc specs are: ryzen 7 7800x3d, 4070ti super nvidia, and ddr5 32gb ram.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mortgage4154"&gt; /u/No-Mortgage4154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0q55x/any_good_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0q55x/any_good_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0q55x/any_good_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T03:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0x5bs</id>
    <title>Is Ollama capable of doing all of that on a gaming Omen laptop (image object recognition, storywriting, TTS, image &amp; audio generation)?</title>
    <updated>2026-02-10T10:23:17+00:00</updated>
    <author>
      <name>/u/d_test_2030</name>
      <uri>https://old.reddit.com/user/d_test_2030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I was wondering if I can run all of this on a standard gaming laptop. Ideally it should be able to work in German.&lt;br /&gt; Also if you could help me find the right model (which will work smoothly on a laptop), I would be very thankful.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;- Detecting objects from a still image:&lt;/strong&gt; I assume LLava is the best model for that?&lt;br /&gt; &lt;strong&gt;- Writing short creative stories&lt;/strong&gt; (just a few sentences based on keywords provided): LLama or Gemma?&lt;br /&gt; - I assume &lt;strong&gt;image &amp;amp; audio generation&lt;/strong&gt; isn't the forte of Ollama, do you have other open source recommendations that work on Omen?&lt;br /&gt; - For &lt;strong&gt;TTS&lt;/strong&gt; I could also use a python library or does Ollama come with good German options?&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/d_test_2030"&gt; /u/d_test_2030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0x5bs/is_ollama_capable_of_doing_all_of_that_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0x5bs/is_ollama_capable_of_doing_all_of_that_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0x5bs/is_ollama_capable_of_doing_all_of_that_on_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T10:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r18ge5</id>
    <title>title: openclaw telegram works for /help and new session but freezes when the agent actually responds</title>
    <updated>2026-02-10T18:16:50+00:00</updated>
    <author>
      <name>/u/nurdthug</name>
      <uri>https://old.reddit.com/user/nurdthug</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nurdthug"&gt; /u/nurdthug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1r18fmb/title_openclaw_telegram_works_for_help_and_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r18ge5/title_openclaw_telegram_works_for_help_and_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r18ge5/title_openclaw_telegram_works_for_help_and_new/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T18:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1acup</id>
    <title>PolyMCP Major Update: New Website, New Inspector UX, Installable Desktop App, and skills.sh-First Workflow</title>
    <updated>2026-02-10T19:23:51+00:00</updated>
    <author>
      <name>/u/Just_Vugg_PolyMCP</name>
      <uri>https://old.reddit.com/user/Just_Vugg_PolyMCP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r1acup/polymcp_major_update_new_website_new_inspector_ux/"&gt; &lt;img alt="PolyMCP Major Update: New Website, New Inspector UX, Installable Desktop App, and skills.sh-First Workflow" src="https://external-preview.redd.it/TTmX4QB47ieCB6lScf1c5zSQ8KES1TshcAp18basPSg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4529bf6eedf476025a82561414f9218da9edf0d1" title="PolyMCP Major Update: New Website, New Inspector UX, Installable Desktop App, and skills.sh-First Workflow" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Vugg_PolyMCP"&gt; /u/Just_Vugg_PolyMCP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/poly-mcp/PolyMCP"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1acup/polymcp_major_update_new_website_new_inspector_ux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1acup/polymcp_major_update_new_website_new_inspector_ux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T19:23:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0vhy0</id>
    <title>What's the fastest-response model to run on AMD (no-GPU) machines ?</title>
    <updated>2026-02-10T08:39:10+00:00</updated>
    <author>
      <name>/u/mohamedheiba</name>
      <uri>https://old.reddit.com/user/mohamedheiba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey I'm running Ollama on Kubernetes. Help me choose the best model for text summarization and writing documentation based on code please.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hetzner &lt;a href="https://www.hetzner.com/dedicated-rootserver/ax102/"&gt;AX102&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ryzen 7950X3D processor with 16 vCPU&lt;/li&gt; &lt;li&gt;96MB 3D V-Cache&lt;/li&gt; &lt;li&gt;192 GB DDR5 RAM.&lt;/li&gt; &lt;li&gt;No GPU.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Use case:&lt;/strong&gt; AI agent (OpenClaw) orchestrated via n8n, heavy on tool calling / function calling. Needs 40K+ context window. Not doing chat ‚Äî it's purely agentic workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I've tried so far:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;qwen3:32b&lt;/code&gt; (dense) ‚Äî painfully slow on CPU, unusable&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3:30b-a3b-q8_0&lt;/code&gt; (MoE) ‚Äî much faster, works well, decent tool calling&lt;/li&gt; &lt;li&gt;&lt;code&gt;gpt-oss:20b&lt;/code&gt; (MoE, MXFP4) ‚Äî noticeably faster than Qwen3-30B, lightest memory footprint (~12-16GB). Impressed so far.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Now considering:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPT-OSS 20B&lt;/strong&gt; ‚Äî 21B/3.6B active, MXFP4 native, ~12-16GB RAM. Lightest option. Built-in tool calling. Concerned about the harmony format playing nice with n8n.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7-Flash&lt;/strong&gt; ‚Äî 30B/3B active, 128K context, best SWE-bench scores. Saw reports of Ollama template issues ‚Äî is that fixed?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sticking with Qwen3-30B-A3B&lt;/strong&gt; but Q4_K_M &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I haven't tried any of them yet with OpenClaw or n8n.&lt;/p&gt; &lt;p&gt;What are your recommendations ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mohamedheiba"&gt; /u/mohamedheiba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T08:39:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r18kd0</id>
    <title>Open-Source Apple Silicon Local LLM Benchmarking Software. Would love some feedback!</title>
    <updated>2026-02-10T18:20:38+00:00</updated>
    <author>
      <name>/u/peppaz</name>
      <uri>https://old.reddit.com/user/peppaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r18kd0/opensource_apple_silicon_local_llm_benchmarking/"&gt; &lt;img alt="Open-Source Apple Silicon Local LLM Benchmarking Software. Would love some feedback!" src="https://external-preview.redd.it/--hFrBiIzmgKy0LazyUbsxh-mezAHM_pi4pFCex-GiU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25fef397f6029bcc19c322f5a7f3635232833d0e" title="Open-Source Apple Silicon Local LLM Benchmarking Software. Would love some feedback!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peppaz"&gt; /u/peppaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Cyberpunk69420/anubis-oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r18kd0/opensource_apple_silicon_local_llm_benchmarking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r18kd0/opensource_apple_silicon_local_llm_benchmarking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T18:20:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1r11i8r</id>
    <title>Best model for Figma MCP server</title>
    <updated>2026-02-10T14:00:42+00:00</updated>
    <author>
      <name>/u/commandermd</name>
      <uri>https://old.reddit.com/user/commandermd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to design in Figma using a local LLM. The idea came after reading about using Claude to design in Figma in the browser. &lt;a href="https://cianfrani.dev/posts/a-better-figma-mcp/"&gt;https://cianfrani.dev/posts/a-better-figma-mcp/&lt;/a&gt; I love the concept but want to try to run locally. Nothing local could beat Claude with my setup. What would get close? I'm thinking &lt;a href="https://ollama.com/library/qwen3-vl:8b"&gt;qwen3-vl:8b&lt;/a&gt;. What should I look for in a model besides vision capabilities? Are there others that would work better? &lt;/p&gt; &lt;p&gt;Specs &amp;amp; Settings&lt;/p&gt; &lt;p&gt;AMD 5600G&lt;br /&gt; 64 GB DDR4&lt;br /&gt; 5060 TI 16GB&lt;br /&gt; Chrome Devtools CDP&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/commandermd"&gt; /u/commandermd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r11i8r/best_model_for_figma_mcp_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r11i8r/best_model_for_figma_mcp_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r11i8r/best_model_for_figma_mcp_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T14:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1bstj</id>
    <title>Preprocessing and prompt formatting with multimodal models</title>
    <updated>2026-02-10T20:15:21+00:00</updated>
    <author>
      <name>/u/AdaObvlada</name>
      <uri>https://old.reddit.com/user/AdaObvlada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have some coding experiences but am still pretty new to AI. So far I managed to set up a few local inferences, but I struggled with understanding the right preprocessing and more important prompt message formatting.&lt;/p&gt; &lt;p&gt;Example: &lt;a href="https://huggingface.co/dam2452/Qwen3-VL-Embedding-8B-GGUF"&gt;https://huggingface.co/dam2452/Qwen3-VL-Embedding-8B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HTTP payload example used by author:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;content&amp;quot;: &amp;quot;Your text or image data here&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But looking at the prompt construction in the helper functions for the original model here (line 250): &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-Embedding-8B/blob/main/scripts/qwen3_vl_embedding.py"&gt;https://huggingface.co/Qwen/Qwen3-VL-Embedding-8B/blob/main/scripts/qwen3_vl_embedding.py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I see, for example, for &lt;code&gt;image_content&lt;/code&gt; that it appends it as instance of PIL.Image&lt;br /&gt; &lt;code&gt;'type': 'image', 'image': image_content&lt;/code&gt; or first downloads it if it was passed as URL.&lt;/p&gt; &lt;p&gt;What exactly is author of the GGUF model expecting me to input then at &lt;code&gt;&amp;quot;content&amp;quot;: &amp;quot;Your text or image data here&amp;quot;&lt;/code&gt; Am I supposed think of passing image data as passing a string of RGB pixel information? The original model also expects min and max pixel metadata that is entirely missing from the other ones prompt.&lt;/p&gt; &lt;p&gt;I didn't check how it does the video but I expect it just grabs out selective frames.&lt;/p&gt; &lt;p&gt;Does it even matter as long as the prompt is consistent across embedding and later query encoding?&lt;/p&gt; &lt;p&gt;Thanks for all the tips.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdaObvlada"&gt; /u/AdaObvlada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1bstj/preprocessing_and_prompt_formatting_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1bstj/preprocessing_and_prompt_formatting_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1bstj/preprocessing_and_prompt_formatting_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T20:15:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1lefu</id>
    <title>Run AI agents locally. Let them call real tools.</title>
    <updated>2026-02-11T02:44:31+00:00</updated>
    <author>
      <name>/u/Consistent_One7493</name>
      <uri>https://old.reddit.com/user/Consistent_One7493</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r1lefu/run_ai_agents_locally_let_them_call_real_tools/"&gt; &lt;img alt="Run AI agents locally. Let them call real tools." src="https://external-preview.redd.it/eHNwbGZxeW0zc2lnMQz3hyL4cHe66E3IbaJazQzlilqsNGq__S9OW5ByLHZI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d9faac7aff8658b29b74ee6448535a892700879" title="Run AI agents locally. Let them call real tools." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built &lt;strong&gt;OnsetLab&lt;/strong&gt;, an open-source framework for local, tool-calling AI agents using small language models and simple MCP connections.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Build once, run anywhere. Your models, your tools, your machine.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/riyanshibohra/OnsetLab"&gt;https://github.com/riyanshibohra/OnsetLab&lt;/a&gt;&lt;br /&gt; (if you find it useful, a ‚≠ê on the repo is always awesome!)&lt;/p&gt; &lt;p&gt;Happy to hear feedback from folks building agents or working with local LLMs!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_One7493"&gt; /u/Consistent_One7493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qv21bpym3sig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1lefu/run_ai_agents_locally_let_them_call_real_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1lefu/run_ai_agents_locally_let_them_call_real_tools/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T02:44:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r132zl</id>
    <title>My Journey Building an AI Agent Orchestrator</title>
    <updated>2026-02-10T15:02:13+00:00</updated>
    <author>
      <name>/u/PuzzleheadedFail3131</name>
      <uri>https://old.reddit.com/user/PuzzleheadedFail3131</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;# üéÆ 88% Success Rate with qwen2.5-coder:7b on RTX 3060 Ti - My Journey Building an AI Agent Orchestrator **TL;DR:** Built a tiered AI agent system where Ollama handles 88% of tasks for FREE, with automatic escalation to Claude for complex work. Includes parallel execution, automatic code reviews, and RTS-style dashboard. ## Why This Matters for After months of testing, I've proven that **local models can handle real production workloads** with the right architecture. Here's the breakdown: ### The Setup - **Hardware:** RTX 3060 Ti (8GB VRAM) - **Model:** qwen2.5-coder:7b (4.7GB) - **Temperature:** 0 (critical for tool calling!) - **Context Management:** 3s rest between tasks + 8s every 5 tasks ### The Results (40-Task Stress Test) - **C1-C8 tasks: 100% success** (20/20) - **C9 tasks: 80% success** (LeetCode medium, class implementations) - **Overall: 88% success** (35/40 tasks) - **Average execution: 0.88 seconds** ### What Works ‚úÖ File I/O operations ‚úÖ Algorithm implementations (merge sort, binary search) ‚úÖ Class implementations (Stack, RPN Calculator) ‚úÖ LeetCode Medium (LRU Cache!) ‚úÖ Data structure operations ### The Secret Sauce **1. Temperature 0** This was the game-changer. T=0.7 ‚Üí model outputs code directly. T=0 ‚Üí reliable tool calling. **2. Rest Between Tasks** Context pollution is real! Without rest: 85% success. With rest: 100% success (C1-C8). **3. Agent Persona (&amp;quot;CodeX-7&amp;quot;)** Gave the model an elite agent identity with mission examples. Completion rates jumped significantly. Agents need personality! **4. Stay in VRAM** Tested 14B model ‚Üí CPU offload ‚Üí 40% pass rate 7B model fully in VRAM ‚Üí 88-100% pass rate **5. Smart Escalation** Tasks that fail escalate to Claude automatically. Best of both worlds. ### The Architecture ``` Task Queue ‚Üí Complexity Router ‚Üí Resource Pool ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚Üì ‚Üì ‚Üì Ollama Haiku Sonnet (C1-6) (C7-8) (C9-10) FREE! $0.003 $0.01 ‚Üì ‚Üì ‚Üì Automatic Code Reviews (Haiku every 5th, Opus every 10th) ``` ### Cost Comparison (10-task batch) - **All Claude Opus:** ~$15 - **Tiered (mostly Ollama):** ~$1.50 - **Savings:** 90% ### GitHub https://github.com/mrdushidush/agent-battle-command-center Full Docker setup, just needs Ollama + optional Claude API for fallback. ## Questions for the Community 1. **Has anyone else tested qwen2.5-coder:7b for production?** How do your results compare? 2. **What's your sweet spot for VRAM vs model size?** 3. **Agent personas - placebo or real?** My tests suggest real improvement but could be confirmation bias. 4. **Other models?** Considering DeepSeek Coder v2 next. --- **Stack:** TypeScript, Python, FastAPI, CrewAI, Ollama, Docker **Status:** Production ready, all tests passing Let me know if you want me to share the full prompt engineering approach or stress test methodology! &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PuzzleheadedFail3131"&gt; /u/PuzzleheadedFail3131 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T15:02:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1cz0l</id>
    <title>How much autonomy do you give your AI?</title>
    <updated>2026-02-10T20:58:13+00:00</updated>
    <author>
      <name>/u/Xthebuilder</name>
      <uri>https://old.reddit.com/user/Xthebuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i use AI in a few of my workflows. I noticed the more time i spent with tools the more I learn their limitations. I also realize that to more you do something unconsciously , the harder it is to bring it to the forefront to explain it. The relevance to all that is i wonder how much autonomy is okay and how much is not enough generally when using AI in workflows. I feel like there is a spectrum and every application of AI varies. So whats your workflow in general and how much autonomy does your AI have in it? Do you watch it like a hawk or only start checking if things don't look or feel right kinda like a sense.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xthebuilder"&gt; /u/Xthebuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1cz0l/how_much_autonomy_do_you_give_your_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1cz0l/how_much_autonomy_do_you_give_your_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1cz0l/how_much_autonomy_do_you_give_your_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T20:58:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1474m</id>
    <title>A Ralph Loop for Reading: Beating GPT 5.2 with a 4k Context Window &amp; Ollama</title>
    <updated>2026-02-10T15:43:46+00:00</updated>
    <author>
      <name>/u/smhanov</name>
      <uri>https://old.reddit.com/user/smhanov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r1474m/a_ralph_loop_for_reading_beating_gpt_52_with_a_4k/"&gt; &lt;img alt="A Ralph Loop for Reading: Beating GPT 5.2 with a 4k Context Window &amp;amp; Ollama" src="https://external-preview.redd.it/PcLeb8yolVzvJxYoO6wR-sH415-VkF75d4IUzvwdPfo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b997b5b30b859d417fafb336d277eb90311962a3" title="A Ralph Loop for Reading: Beating GPT 5.2 with a 4k Context Window &amp;amp; Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLM costs are getting out of hand. I made an AI agent for research that works well using only qwen3:4b with Ollama. It's all about managing the context window smartly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smhanov"&gt; /u/smhanov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://stevehanov.ca/blog/a-ralph-loop-for-reading-beating-gpt-52-with-a-4k-context-window-and-4-gpus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1474m/a_ralph_loop_for_reading_beating_gpt_52_with_a_4k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1474m/a_ralph_loop_for_reading_beating_gpt_52_with_a_4k/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T15:43:46+00:00</published>
  </entry>
</feed>
