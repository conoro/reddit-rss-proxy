<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-15T23:36:26+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pkfv0i</id>
    <title>In OllaMan, using the Qwen3-Next model</title>
    <updated>2025-12-12T02:08:46+00:00</updated>
    <author>
      <name>/u/ComfyTightwad</name>
      <uri>https://old.reddit.com/user/ComfyTightwad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pkfv0i/in_ollaman_using_the_qwen3next_model/"&gt; &lt;img alt="In OllaMan, using the Qwen3-Next model" src="https://external-preview.redd.it/emRicXR0ZG9sbzZnMdW2ajc4pd0IkwIxB2RsxHZaHcmU2fwx0RdQoBvo_mLe.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb5c8330f13bf10d85a8aafb9df92bf8714eddbc" title="In OllaMan, using the Qwen3-Next model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComfyTightwad"&gt; /u/ComfyTightwad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cflbcgdolo6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkfv0i/in_ollaman_using_the_qwen3next_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkfv0i/in_ollaman_using_the_qwen3next_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T02:08:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl6mgg</id>
    <title>Crypto Bot</title>
    <updated>2025-12-12T23:34:50+00:00</updated>
    <author>
      <name>/u/Fantastic_Active9334</name>
      <uri>https://old.reddit.com/user/Fantastic_Active9334</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pl6mgg/crypto_bot/"&gt; &lt;img alt="Crypto Bot" src="https://external-preview.redd.it/cSi8uSxKAQ292UK5DvDBWv7Qq0EtCv5xhgeNsqgPq-o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63389a644987b7a2e8e73505741fa7c1e6f15a73" title="Crypto Bot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;TLDR;&lt;/p&gt; &lt;p&gt;I wrote an open-source crypto trading bot. It actively manages positions, trades and activity in the market checking recent news and determining if it should wait for a better time to trade in market or act now.&lt;/p&gt; &lt;p&gt;‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî&lt;/p&gt; &lt;p&gt;It‚Äôs thinking logic is dictated by an LLM, it uses tavily search for browsing the web and integrates directly with Alpacas API to manage a portfolio actively. It checks periodically by determining the next best time to check the news and portfolio -&amp;gt; gives a probability score, based on determined sentiment as well as a brief summary of how it views the market before taking another step with the tools it‚Äôs been provided. Currently prompt has been predefined for solana and a claude model is the default but this can be changed easily simply by switching whether it be an open-source llm on Ollama or a closed-source model like Claude.&lt;/p&gt; &lt;p&gt;sqlite is used for state management, and it can be deployed using docker or purely locally. &lt;/p&gt; &lt;p&gt;Code is complete free to use if you have any ideas on how to improve and make it better - just message me or create a PR&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic_Active9334"&gt; /u/Fantastic_Active9334 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/GB153/AlpacaAgent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pl6mgg/crypto_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pl6mgg/crypto_bot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T23:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkn0j5</id>
    <title>Europe's devstral-small-2, available in the ollama library, looks promising</title>
    <updated>2025-12-12T08:40:23+00:00</updated>
    <author>
      <name>/u/New_Cranberry_6451</name>
      <uri>https://old.reddit.com/user/New_Cranberry_6451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share with you that yesterday I tested the devstral-small-2 model and it surprised me for good. With 24B params, it runs at 2,5 tokens per sec on my 8Gb VRAM laptop on Windows. (GPT-OSS, with 20B, runs at 20 tokens per sec, don't know how they do it...).&lt;/p&gt; &lt;p&gt;Despite this substantial performance difference, the quality of the answers is very high in my opinion, obtaining great results with simple prompts, and working great on instruction processing and system prompt following.&lt;/p&gt; &lt;p&gt;I am very happy, give it a try and tell me what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Cranberry_6451"&gt; /u/New_Cranberry_6451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkn0j5/europes_devstralsmall2_available_in_the_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkn0j5/europes_devstralsmall2_available_in_the_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkn0j5/europes_devstralsmall2_available_in_the_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T08:40:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkycev</id>
    <title>üöÄ New: Olmo 3.1 Think 32B &amp; Olmo 3.1 Instruct 32B</title>
    <updated>2025-12-12T17:50:44+00:00</updated>
    <author>
      <name>/u/Glittering-Fish3178</name>
      <uri>https://old.reddit.com/user/Glittering-Fish3178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pkycev/new_olmo_31_think_32b_olmo_31_instruct_32b/"&gt; &lt;img alt="üöÄ New: Olmo 3.1 Think 32B &amp;amp; Olmo 3.1 Instruct 32B" src="https://preview.redd.it/37012o2u5t6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6b659c4d8abf3b542adbe99af324e1df0c0bbcc" title="üöÄ New: Olmo 3.1 Think 32B &amp;amp; Olmo 3.1 Instruct 32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Fish3178"&gt; /u/Glittering-Fish3178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/37012o2u5t6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkycev/new_olmo_31_think_32b_olmo_31_instruct_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkycev/new_olmo_31_think_32b_olmo_31_instruct_32b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T17:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkvopv</id>
    <title>AI Agent from scratch: Django + Ollama + Pydantic AI - A Step-by-Step Guide</title>
    <updated>2025-12-12T16:07:00+00:00</updated>
    <author>
      <name>/u/tom-mart</name>
      <uri>https://old.reddit.com/user/tom-mart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone, &lt;/p&gt; &lt;p&gt;I just published Part 2 of the article series, which dives deep into creating a multi-layered memory system.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The agent has:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Short-term memory&lt;/strong&gt; for the current chat (with auto-pruning).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-term memory&lt;/strong&gt; using &lt;code&gt;pgvector&lt;/code&gt; to find relevant info from past conversations (RAG).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Summarization&lt;/strong&gt; to create condensed memories of old chats.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structured Memory&lt;/strong&gt; using tools to save/retrieve data from a Django model (I used a fitness tracker as an example).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Django &amp;amp; Django Ninja&lt;/li&gt; &lt;li&gt;Ollama (to run models like Llama 3 or Gemma locally)&lt;/li&gt; &lt;li&gt;Pydantic AI (for agent logic and tools)&lt;/li&gt; &lt;li&gt;PostgreSQL + &lt;code&gt;pgvector&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's a step-by-step guide meant to be easy to follow. I tried to explain the &amp;quot;why&amp;quot; behind the design, not just the &amp;quot;how.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;You can read the full article here:&lt;/strong&gt; &lt;a href="https://medium.com/@tom.mart/build-self-hosted-ai-agent-with-ollama-pydantic-ai-and-django-ninja-65214a3afb35"&gt;https://medium.com/@tom.mart/build-self-hosted-ai-agent-with-ollama-pydantic-ai-and-django-ninja-65214a3afb35&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The full code is on GitHub if you just want to browse. Happy to answer any questions!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/tom-mart/ai-agent"&gt;https://github.com/tom-mart/ai-agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tom-mart"&gt; /u/tom-mart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkvopv/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkvopv/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkvopv/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T16:07:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkzlxq</id>
    <title>I turned my computer into a war room. Quorum: A CLI for local model debates (Ollama zero-config)</title>
    <updated>2025-12-12T18:40:13+00:00</updated>
    <author>
      <name>/u/C12H16N2HPO4</name>
      <uri>https://old.reddit.com/user/C12H16N2HPO4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone.&lt;/p&gt; &lt;p&gt;I got tired of manually copy-pasting prompts between local &lt;strong&gt;Llama 4&lt;/strong&gt; and Mistral to verify facts, so I built &lt;strong&gt;Quorum&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It‚Äôs a CLI tool that orchestrates debates between 2‚Äì6 models. You can mix and match‚Äîfor example, have your local &lt;strong&gt;Llama 4&lt;/strong&gt; argue against &lt;strong&gt;GPT-5.2&lt;/strong&gt;, or run a fully offline debate.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features for this sub:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ollama Auto-discovery:&lt;/strong&gt; It detects your local models automatically. No config files or YAML hell.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;7 Debate Methods:&lt;/strong&gt; Includes &amp;quot;Oxford Debate&amp;quot; (For/Against), &amp;quot;Devil's Advocate&amp;quot;, and &amp;quot;Delphi&amp;quot; (consensus building).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Privacy:&lt;/strong&gt; Local-first. Your data stays on your rig unless you explicitly add an API model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Heads-up:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;VRAM Warning:&lt;/strong&gt; Running multiple simultaneous &lt;strong&gt;405B or 70B&lt;/strong&gt; models will eat your VRAM for breakfast. Make sure your hardware can handle the concurrency.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; It‚Äôs BSL 1.1. It‚Äôs free for personal/internal use, but stops cloud corps from reselling it as a SaaS. Just wanted to be upfront about that.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/Detrol/quorum-cli"&gt;https://github.com/Detrol/quorum-cli&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Install:&lt;/strong&gt; &lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/Detrol/quorum-cli.git"&gt;&lt;code&gt;https://github.com/Detrol/quorum-cli.git&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if the auto-discovery works on your specific setup!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/C12H16N2HPO4"&gt; /u/C12H16N2HPO4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkzlxq/i_turned_my_computer_into_a_war_room_quorum_a_cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkzlxq/i_turned_my_computer_into_a_war_room_quorum_a_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkzlxq/i_turned_my_computer_into_a_war_room_quorum_a_cli/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T18:40:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1plmm5h</id>
    <title>ollama on mac m1 has bugÔºå i dont know how to run?</title>
    <updated>2025-12-13T14:27:05+00:00</updated>
    <author>
      <name>/u/Worldly-Badger-937</name>
      <uri>https://old.reddit.com/user/Worldly-Badger-937</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Error: 500 Internal Server Error: model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details&lt;/p&gt; &lt;p&gt;(base) sun2022@sun2022deMacBook-Pro ~ % ollama run qwen2.5:7b&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; Send a message (/? for help)Ôºåi try to use deepseek and qwenÔºå and they are all not to be run.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worldly-Badger-937"&gt; /u/Worldly-Badger-937 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1plmm5h/ollama_on_mac_m1_has_bug_i_dont_know_how_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1plmm5h/ollama_on_mac_m1_has_bug_i_dont_know_how_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1plmm5h/ollama_on_mac_m1_has_bug_i_dont_know_how_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-13T14:27:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pld98r</id>
    <title>HTML BASED UI for Ollama Models and Other Local Models. Because I Respect Privacy.</title>
    <updated>2025-12-13T05:05:12+00:00</updated>
    <author>
      <name>/u/Cummanaati</name>
      <uri>https://old.reddit.com/user/Cummanaati</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TBH, I used AI Vibecoding to make this Entire UI but atleast it is useful and not complicated to setup and it doesn't need a dedicated server or anything like that. Atleast this is not a random ai slop though. I made this for people to utilize offline models at ease and that's all. Hope y'all like it and i would apprecitae if u star my github repository. &lt;/p&gt; &lt;p&gt;Note: As a Privacy Enthusiast myself, there is no telemetry other than the google fonts lol, there's no ads or nothing related to monetization. I made this app out of passion and boredom ofcourse lmao.&lt;/p&gt; &lt;p&gt;Adiyos gang : )&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/one-man-studios/Shinzo-UI"&gt;https://github.com/one-man-studios/Shinzo-UI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cummanaati"&gt; /u/Cummanaati &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pld98r/html_based_ui_for_ollama_models_and_other_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pld98r/html_based_ui_for_ollama_models_and_other_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pld98r/html_based_ui_for_ollama_models_and_other_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-13T05:05:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1plv5xv</id>
    <title>A Brief Primer on Embeddings - Intuition, History &amp; Their Role in LLMs</title>
    <updated>2025-12-13T20:28:25+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1plv5xv/a_brief_primer_on_embeddings_intuition_history/"&gt; &lt;img alt="A Brief Primer on Embeddings - Intuition, History &amp;amp; Their Role in LLMs" src="https://external-preview.redd.it/wDzZFd-8OO5EoBmlfyxqfNu6wZlvgh0Jd9Ml6gS39fY.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=975e62061a08bdadf708e9ca8fa0c0fe8e4d0c93" title="A Brief Primer on Embeddings - Intuition, History &amp;amp; Their Role in LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/Cv5kSs2Jcu4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1plv5xv/a_brief_primer_on_embeddings_intuition_history/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1plv5xv/a_brief_primer_on_embeddings_intuition_history/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-13T20:28:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1plgvss</id>
    <title>I stopped using the Prompt Engineering manual. Quick guide to setting up a Local RAG with Python and Ollama (Code included)</title>
    <updated>2025-12-13T08:49:59+00:00</updated>
    <author>
      <name>/u/jokiruiz</name>
      <uri>https://old.reddit.com/user/jokiruiz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd been frustrated for a while with the context limitations of ChatGPT and the privacy issues. I started investigating and realized that traditional Prompt Engineering is a workaround. The real solution is RAG (Retrieval-Augmented Generation).&lt;/p&gt; &lt;p&gt;I've put together a simple Python script (less than 30 lines) to chat with my PDF documents/websites using Ollama (Llama 3) and LangChain. It all runs locally and is free.&lt;/p&gt; &lt;p&gt;The Stack: Python + LangChain Llama (Inference Engine) ChromaDB (Vector Database)&lt;/p&gt; &lt;p&gt;If you're interested in seeing a step-by-step explanation and how to install everything from scratch, I've uploaded a visual tutorial here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/sj1yzbXVXM0?si=oZnmflpHWqoCBnjr"&gt;https://youtu.be/sj1yzbXVXM0?si=oZnmflpHWqoCBnjr&lt;/a&gt; I've also uploaded the Gist to GitHub: &lt;a href="https://gist.github.com/JoaquinRuiz/e92bbf50be2dffd078b57febb3d961b2"&gt;https://gist.github.com/JoaquinRuiz/e92bbf50be2dffd078b57febb3d961b2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is anyone else tinkering with Llama 3 locally? How's the performance for you?&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jokiruiz"&gt; /u/jokiruiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1plgvss/i_stopped_using_the_prompt_engineering_manual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1plgvss/i_stopped_using_the_prompt_engineering_manual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1plgvss/i_stopped_using_the_prompt_engineering_manual/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-13T08:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1plbnq1</id>
    <title>Ollama now supports Mistral AI‚Äôs Devstral 2 models</title>
    <updated>2025-12-13T03:39:40+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1plbnq1/ollama_now_supports_mistral_ais_devstral_2_models/"&gt; &lt;img alt="Ollama now supports Mistral AI‚Äôs Devstral 2 models" src="https://preview.redd.it/6rvjqky86w6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a03bcaf3abf5411f898c4e4855e36735885785b" title="Ollama now supports Mistral AI‚Äôs Devstral 2 models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6rvjqky86w6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1plbnq1/ollama_now_supports_mistral_ais_devstral_2_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1plbnq1/ollama_now_supports_mistral_ais_devstral_2_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-13T03:39:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmamuf</id>
    <title>When do you think a LLM with a really high context window is most likely to be released (not talking about the ones with 1 million, I mean like &gt;30 million and they have to be public, and actually usable, not the mess that Llama Scout was)</title>
    <updated>2025-12-14T10:06:07+00:00</updated>
    <author>
      <name>/u/Fast_Engine_7038</name>
      <uri>https://old.reddit.com/user/Fast_Engine_7038</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pmamuf/when_do_you_think_a_llm_with_a_really_high/"&gt; &lt;img alt="When do you think a LLM with a really high context window is most likely to be released (not talking about the ones with 1 million, I mean like &amp;gt;30 million and they have to be public, and actually usable, not the mess that Llama Scout was)" src="https://a.thumbs.redditmedia.com/0WpkYn_Yt6BRDPsbmE0g43bVIfJCE0m_gNacK6ywIZ4.jpg" title="When do you think a LLM with a really high context window is most likely to be released (not talking about the ones with 1 million, I mean like &amp;gt;30 million and they have to be public, and actually usable, not the mess that Llama Scout was)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pm16ltxq857g1.png?width=1727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51c9c817861ee4210935faa97c768a5a4e0c306f"&gt;https://preview.redd.it/pm16ltxq857g1.png?width=1727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51c9c817861ee4210935faa97c768a5a4e0c306f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fast_Engine_7038"&gt; /u/Fast_Engine_7038 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmamuf/when_do_you_think_a_llm_with_a_really_high/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmamuf/when_do_you_think_a_llm_with_a_really_high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pmamuf/when_do_you_think_a_llm_with_a_really_high/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-14T10:06:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm5mg9</id>
    <title>Odd Ollama behavior on Strix Halo</title>
    <updated>2025-12-14T04:52:46+00:00</updated>
    <author>
      <name>/u/arlaneenalra</name>
      <uri>https://old.reddit.com/user/arlaneenalra</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed that anytime Ollama gets to or exceeds somewhere around 25~32k tokens in context the model I'm using starts repeating itself over and over again. It doesn't seem to matter what model I'm using or what the context is set to (I'm usually using 131072, roughly 128k and models that are know to handle that context size like ministral-3:14b, llama3.3:70b, qwen3:32b, qwen3-next:80b, etc. It doesn't seem to matter which one I use.&lt;/p&gt; &lt;p&gt;Any suggestions on what to try?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arlaneenalra"&gt; /u/arlaneenalra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pm5mg9/odd_ollama_behavior_on_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pm5mg9/odd_ollama_behavior_on_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pm5mg9/odd_ollama_behavior_on_strix_halo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-14T04:52:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmj9t8</id>
    <title>Running create a simple MCP server and use it with Ollama + Open-webui</title>
    <updated>2025-12-14T17:12:28+00:00</updated>
    <author>
      <name>/u/rukey3001</name>
      <uri>https://old.reddit.com/user/rukey3001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pmj9t8/running_create_a_simple_mcp_server_and_use_it/"&gt; &lt;img alt="Running create a simple MCP server and use it with Ollama + Open-webui" src="https://external-preview.redd.it/JVrpLDa-Ze0i82LCyTE7mglI1wMGrTfB0UfpLzIari8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e9ecd5e36a57c6fc78178352fa95e8b7f6aa88c" title="Running create a simple MCP server and use it with Ollama + Open-webui" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rukey3001"&gt; /u/rukey3001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/l-Q2bayDydw?si=JL8yYZSMhenwPosc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmj9t8/running_create_a_simple_mcp_server_and_use_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pmj9t8/running_create_a_simple_mcp_server_and_use_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-14T17:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmf50q</id>
    <title>Noob trying to figure out how to automate personal finance analysis using ollama models</title>
    <updated>2025-12-14T14:19:14+00:00</updated>
    <author>
      <name>/u/InternationalSail400</name>
      <uri>https://old.reddit.com/user/InternationalSail400</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am just starting out with ollama and AI in general and have decided the best way to learn about it is to use it for an everyday task. I have years of manual excel sheets with all my expense and income transactions similar to a check book ledger which I wanted to see if I get a local ollama model to analyze provide trends and historical analysis, etc. with one day when i get more advanced potentially feeding it IRS tax rules and having it help me understand tax credits and deductions i should be leveraging and turn into a tax advisor.&lt;/p&gt; &lt;p&gt;At this point I am starting small and asking a model to take a sample of my data and just provide a summary of total income, expenses, and then break down my expenses by category.&lt;/p&gt; &lt;p&gt;The data is organized in excel files and has been recorded based on my personal way of recording transactions rather than any official accounting ledger methodology of recording transactions.&lt;/p&gt; &lt;p&gt;I installed ollama and leveraged the native ollama UI and feed it a small sample of data in a csv and txt format and played around with multiple models (gemma2, deepseek, etc.). Nothing was able to parse the data and come up with accurate total income and expenses.&lt;/p&gt; &lt;p&gt;I tried to prompt the models with the goal and provided context of what each column header means and tried to explain the basic calculation formula as well as trying to provide feedback as far as where it was going wrong. A lot of times it wasn‚Äôt able to associate a value in a cell with the correct column header to understand if it should be treated as income or expenses.&lt;/p&gt; &lt;p&gt;What am i doing wrong? Do I need to clean the sample data for a model to interrupt? Does ollama models not like csvs? Should i be using command prompt terminal rather than ollama interface? Is there a particular model?&lt;/p&gt; &lt;p&gt;Any tips or advice would be appreciated.&lt;/p&gt; &lt;p&gt;I have an HP 255 G10 16GB AMD Ryzen 5 processor laptop for context.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalSail400"&gt; /u/InternationalSail400 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmf50q/noob_trying_to_figure_out_how_to_automate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmf50q/noob_trying_to_figure_out_how_to_automate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pmf50q/noob_trying_to_figure_out_how_to_automate/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-14T14:19:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmui12</id>
    <title>Has anyone tried creating an Ollama Modelfile for a cloud model?</title>
    <updated>2025-12-15T01:11:02+00:00</updated>
    <author>
      <name>/u/Vivid-Competition-20</name>
      <uri>https://old.reddit.com/user/Vivid-Competition-20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wondering if it is possible to give a cloud model a custom System Prompt without using the Ollama API that way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vivid-Competition-20"&gt; /u/Vivid-Competition-20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmui12/has_anyone_tried_creating_an_ollama_modelfile_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmui12/has_anyone_tried_creating_an_ollama_modelfile_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pmui12/has_anyone_tried_creating_an_ollama_modelfile_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-15T01:11:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmc6pt</id>
    <title>üéÖ Built a Santa Tracker powered by Ollama + Llama 3.2 (100% local, privacy-first)</title>
    <updated>2025-12-14T11:44:40+00:00</updated>
    <author>
      <name>/u/VegetableSense</name>
      <uri>https://old.reddit.com/user/VegetableSense</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;With Xmas around the corner, I built a fun Santa Tracker app that's powered entirely by &lt;strong&gt;local AI&lt;/strong&gt; using Ollama and Llama 3.2. No cloud APIs, no data collection - everything runs on your machine!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xdnhwq3s647g1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a9baf1de38ee213394ee229479244ce49975f022"&gt;https://preview.redd.it/xdnhwq3s647g1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a9baf1de38ee213394ee229479244ce49975f022&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/majtpt3s647g1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75b7933a932f48ff203b4a427b326f5b6e65b19b"&gt;https://preview.redd.it/majtpt3s647g1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75b7933a932f48ff203b4a427b326f5b6e65b19b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tracks Santa's journey around the world on Christmas Eve&lt;/li&gt; &lt;li&gt;Calculates distance from YOUR location (with consent - location never leaves your browser)&lt;/li&gt; &lt;li&gt;Generates personalized messages from Santa using Llama 3.2&lt;/li&gt; &lt;li&gt;Beautiful animations with twinkling stars and Santa's sleigh&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama + Llama 3.2 for AI message generation&lt;/li&gt; &lt;li&gt;Python server as a CORS proxy&lt;/li&gt; &lt;li&gt;React (via CDN, no build step)&lt;/li&gt; &lt;li&gt;Browser Geolocation API (opt-in only)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Privacy features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;100% local processing&lt;/li&gt; &lt;li&gt;No external API calls&lt;/li&gt; &lt;li&gt;Location data never stored or transmitted&lt;/li&gt; &lt;li&gt;Everything runs on localhost&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The setup is super simple - just &lt;code&gt;ollama serve&lt;/code&gt;, &lt;code&gt;python3&lt;/code&gt; &lt;a href="http://server.py"&gt;&lt;code&gt;server.py&lt;/code&gt;&lt;/a&gt;, and you're tracking Santa with AI-powered messages!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/sukanto-m/santa-local-ai"&gt;https://github.com/sukanto-m/santa-local-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your feedback or suggestions for improvements! üéÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VegetableSense"&gt; /u/VegetableSense &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmc6pt/built_a_santa_tracker_powered_by_ollama_llama_32/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmc6pt/built_a_santa_tracker_powered_by_ollama_llama_32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pmc6pt/built_a_santa_tracker_powered_by_ollama_llama_32/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-14T11:44:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmm1jv</id>
    <title>How Do You Add Models?</title>
    <updated>2025-12-14T19:04:23+00:00</updated>
    <author>
      <name>/u/Head-Investigator540</name>
      <uri>https://old.reddit.com/user/Head-Investigator540</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a very beginner question but how do you add models? When I open up ollama on my computer, in the lower right I see that drop down that lets me toggle through a few models. But it's a preset list and only a few. How do I add more models that I can download?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Head-Investigator540"&gt; /u/Head-Investigator540 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmm1jv/how_do_you_add_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmm1jv/how_do_you_add_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pmm1jv/how_do_you_add_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-14T19:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmmo9u</id>
    <title>Nanocoder Hits the OpenRouter leaderboard for the first time üéâüî•</title>
    <updated>2025-12-14T19:29:51+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pmmo9u/nanocoder_hits_the_openrouter_leaderboard_for_the/"&gt; &lt;img alt="Nanocoder Hits the OpenRouter leaderboard for the first time üéâüî•" src="https://preview.redd.it/ekubekfa187g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bdd98bd0b48bc09909f72f76c17474338cf77c7d" title="Nanocoder Hits the OpenRouter leaderboard for the first time üéâüî•" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ekubekfa187g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmmo9u/nanocoder_hits_the_openrouter_leaderboard_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pmmo9u/nanocoder_hits_the_openrouter_leaderboard_for_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-14T19:29:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmctp5</id>
    <title>ü§Ø Why is 120B GPT-OSS ~13x Faster than 70B DeepSeek R1 on my AMD Radeon Pro GPU (ROCm/Ollama)?</title>
    <updated>2025-12-14T12:22:16+00:00</updated>
    <author>
      <name>/u/Comfortable-Fudge233</name>
      <uri>https://old.reddit.com/user/Comfortable-Fudge233</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've run into a confusing performance bottleneck with two large models in Ollama, and I'm hoping the AMD/ROCm experts here might have some insight.&lt;/p&gt; &lt;p&gt;I'm running on powerful hardware, but the performance difference between these two models is night and day, which seems counter-intuitive given the model sizes.&lt;/p&gt; &lt;h1&gt;üñ•Ô∏è My System Specs:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; AMD Radeon AI Pro R9700 (32GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD Ryzen 9 9950X&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 64GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OS/Software:&lt;/strong&gt; Ubuntu 24/Ollama (latest) / ROCm (latest)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;1. The Fast Model: gpt-oss:120b&lt;/h1&gt; &lt;p&gt;Despite being the larger model, the performance is very fast and responsive.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚ùØ ollama run gpt-oss:120b --verbose &amp;gt;&amp;gt;&amp;gt; Hello ... eval count: 32 token(s) eval duration: 1.630745435s **eval rate: 19.62 tokens/s** &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;2. The Slow Model: deepseek-r1:70b-llama-distill-q8_0&lt;/h1&gt; &lt;p&gt;This model is smaller (70B vs 120B) and is using a highly quantized Q8_0, but it is &lt;em&gt;extremely&lt;/em&gt; slow.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚ùØ ollama run deepseek-r1:70b-llama-distill-q8_0 --verbose &amp;gt;&amp;gt;&amp;gt; hi ... eval count: 110 token(s) eval duration: 1m12.408170734s **eval rate: 1.52 tokens/s** &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;üìä Summary of Difference:&lt;/h1&gt; &lt;p&gt;The 70B DeepSeek model is achieving only &lt;strong&gt;1.52 tokens/s&lt;/strong&gt;, while the 120B GPT-OSS model hits &lt;strong&gt;19.62 tokens/s&lt;/strong&gt;. That's a &lt;strong&gt;~13x performance gap&lt;/strong&gt;! The prompt evaluation rate is also drastically slower for DeepSeek (15.12 t/s vs 84.40 t/s).&lt;/p&gt; &lt;h1&gt;ü§î My Question: Why is DeepSeek R1 so much slower?&lt;/h1&gt; &lt;p&gt;My hypothesis is that this is likely an issue with &lt;strong&gt;ROCm/GPU-specific kernel optimization&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is the specific &lt;code&gt;llama-distill-q8_0&lt;/code&gt; GGUF format for DeepSeek not properly optimized for the RDNA architecture on my Radeon Pro R9700?&lt;/li&gt; &lt;li&gt;Are the low-level kernels that power the DeepSeek architecture in Ollama/ROCm simply less efficient than the ones used by &lt;code&gt;gpt-oss&lt;/code&gt;?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone else on an &lt;strong&gt;AMD GPU with ROCm&lt;/strong&gt; seen similar performance differences, especially with the DeepSeek R1 models? Any tips on a better quantization or an alternative DeepSeek format to try? Or any suggestions on best alternative faster models?&lt;/p&gt; &lt;p&gt;Thanks for the help! I've attached screenshots of the full output.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Fudge233"&gt; /u/Comfortable-Fudge233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmctp5/why_is_120b_gptoss_13x_faster_than_70b_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmctp5/why_is_120b_gptoss_13x_faster_than_70b_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pmctp5/why_is_120b_gptoss_13x_faster_than_70b_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-14T12:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn4plw</id>
    <title>AI-based document renaming for paperless-ngx (Ollama supported)</title>
    <updated>2025-12-15T11:05:30+00:00</updated>
    <author>
      <name>/u/dolce04</name>
      <uri>https://old.reddit.com/user/dolce04</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dolce04"&gt; /u/dolce04 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Paperlessngx/comments/1pn4pak/aibased_document_renaming_for_paperlessngx_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pn4plw/aibased_document_renaming_for_paperlessngx_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pn4plw/aibased_document_renaming_for_paperlessngx_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-15T11:05:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pngfaw</id>
    <title>What's the proper way to ban eos token in Ollama so that it doesn't restrict response length?</title>
    <updated>2025-12-15T19:22:45+00:00</updated>
    <author>
      <name>/u/Head-Investigator540</name>
      <uri>https://old.reddit.com/user/Head-Investigator540</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Head-Investigator540"&gt; /u/Head-Investigator540 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pngfaw/whats_the_proper_way_to_ban_eos_token_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pngfaw/whats_the_proper_way_to_ban_eos_token_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pngfaw/whats_the_proper_way_to_ban_eos_token_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-15T19:22:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pndbz0</id>
    <title>Ollama Pull alternative?</title>
    <updated>2025-12-15T17:26:42+00:00</updated>
    <author>
      <name>/u/DriftTony</name>
      <uri>https://old.reddit.com/user/DriftTony</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could someone tell me what the 'right' way to use this software is?&lt;/p&gt; &lt;p&gt;As I understand it, it is really cool to try out different models, and Ollama seems cool to use. But the ollama pull system is really, really bad (download going from 80% back to 30% etc etc, I do have a slow bandwidth, but I am able to use wget without any problems). And Perhaps you can tell me why it is so different from a wget command (that works flawlessly for when I download models for Comfyui).&lt;/p&gt; &lt;p&gt;I use ollama pull only because I am having trouble finding the knowledge to actually create a proper modelfile myself, especially for the VLM models (for instance a gguf with a mmproj, they never seem to work).&lt;/p&gt; &lt;p&gt;Is it time for me to just find other software all together?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DriftTony"&gt; /u/DriftTony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pndbz0/ollama_pull_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pndbz0/ollama_pull_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pndbz0/ollama_pull_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-15T17:26:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmz4zw</id>
    <title>Bro I just got rickrolled by Mistral-Nemo</title>
    <updated>2025-12-15T05:06:19+00:00</updated>
    <author>
      <name>/u/Fido_27</name>
      <uri>https://old.reddit.com/user/Fido_27</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pmz4zw/bro_i_just_got_rickrolled_by_mistralnemo/"&gt; &lt;img alt="Bro I just got rickrolled by Mistral-Nemo" src="https://preview.redd.it/yco3fvp9va7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=612ee73fa05b6a0a40d33be8f53a4ea81b9b01c6" title="Bro I just got rickrolled by Mistral-Nemo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I already gave it access to open urls on my tv, hours ago. right now i was just testing the chat. I asked it to list 20 fruits. then 20 vegetables, and this guy just rickrolled me on my tv.&lt;/p&gt; &lt;p&gt;(Look at the input box below, and I swear I did not give it any input to rickroll me, you can see it)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fido_27"&gt; /u/Fido_27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yco3fvp9va7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmz4zw/bro_i_just_got_rickrolled_by_mistralnemo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pmz4zw/bro_i_just_got_rickrolled_by_mistralnemo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-15T05:06:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pni9e4</id>
    <title>can I try ollama with a macbook air m3?</title>
    <updated>2025-12-15T20:34:41+00:00</updated>
    <author>
      <name>/u/Lost_Foot_6301</name>
      <uri>https://old.reddit.com/user/Lost_Foot_6301</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;simple question so I will delete once there is an answer, it has 16gb ram. &lt;/p&gt; &lt;p&gt;I just want to do basic intro stuff with ollama to learn about it, is a macbook powerful enough to try toying around with it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lost_Foot_6301"&gt; /u/Lost_Foot_6301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pni9e4/can_i_try_ollama_with_a_macbook_air_m3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pni9e4/can_i_try_ollama_with_a_macbook_air_m3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pni9e4/can_i_try_ollama_with_a_macbook_air_m3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-15T20:34:41+00:00</published>
  </entry>
</feed>
