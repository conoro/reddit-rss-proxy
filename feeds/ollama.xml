<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-27T05:15:45+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qlw48b</id>
    <title>‚ÄúLocal LLHAMA‚Äù project - Orchestration Middleware for Ollama and HomeAssistant</title>
    <updated>2026-01-24T19:20:31+00:00</updated>
    <author>
      <name>/u/NicolaZanarini533</name>
      <uri>https://old.reddit.com/user/NicolaZanarini533</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qlw48b/local_llhama_project_orchestration_middleware_for/"&gt; &lt;img alt="‚ÄúLocal LLHAMA‚Äù project - Orchestration Middleware for Ollama and HomeAssistant" src="https://b.thumbs.redditmedia.com/wmwG-5gFBvSU1f_PeAAO9_irl1ewGvzBDRnSBDFPXzU.jpg" title="‚ÄúLocal LLHAMA‚Äù project - Orchestration Middleware for Ollama and HomeAssistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What‚Äôs the problem/why build this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In a world where, to cite some other posts, the &amp;quot;enshittification of AI&amp;quot; is a trend, having the ability to run effective AI systems locally, even on modest hardware, becomes more and more important. This of course comes with its own problems, which this project aims to address.&lt;/p&gt; &lt;p&gt;The main idea here is that raw model size isn‚Äôt the blocker for smart‚Äëhome control and smart-home assistants ‚Äì it‚Äôs &lt;em&gt;routing &amp;amp; context&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Typical setups struggle with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi‚Äëintent utterances (e.g., ‚Äúturn off lights AND set alarm AND check weather‚Äù)&lt;/li&gt; &lt;li&gt;Exact device names / lack of fuzzy/multi‚Äëlang matching&lt;/li&gt; &lt;li&gt;Base‚Äëprompt control &amp;amp; external‚Äëdata integration&lt;/li&gt; &lt;li&gt;Conversation memory &amp;amp; user/system management&lt;/li&gt; &lt;li&gt;Working without cloud APIs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I‚Äôm building&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;An &lt;strong&gt;orchestration middleware&lt;/strong&gt; that sits &lt;em&gt;between Home Assistant and Ollama&lt;/em&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Decomposes intents in parallel&lt;/li&gt; &lt;li&gt;Routes each to the right backend (HA API, PostgreSQL, weather API, etc.)&lt;/li&gt; &lt;li&gt;Injects only the needed context&lt;/li&gt; &lt;li&gt;Auto‚Äëscales the prompt window&lt;/li&gt; &lt;li&gt;Synthesizes a single, natural‚Äëlanguage reply&lt;/li&gt; &lt;li&gt;Uses memory to include previous conversation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Result: 2‚Äì5 s for multi‚Äëintent commands; sub‚Äëminute even with web searches ‚Äì all offline.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware‚Äëvalidated presets&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;VRAM&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Languages&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;8 GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen2.5‚Äë8B&lt;/td&gt; &lt;td align="left"&gt;English only&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;16 GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen2.5‚Äë14B&lt;/td&gt; &lt;td align="left"&gt;6+ languages&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;24 GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;GPT‚ÄëOSS‚Äë20B&lt;/td&gt; &lt;td align="left"&gt;6+ languages&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Tested on:&lt;/p&gt; &lt;p&gt;- Xeon E5‚Äë2640 v4 + RTX 4060 Ti 16 GB&lt;/p&gt; &lt;p&gt;- i7‚Äë12700H + RTX 4060 8 GB (mobile)&lt;/p&gt; &lt;p&gt;- Xeon E5‚Äë2640 v4 + RTX 2080 Ti + Ollama VM with RTX 4060 Ti 16 GB.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example commands (single utterance)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄúTurn off the kitchen light, set my 7 am alarm and tell me the weather for tomorrow‚Äù&lt;/li&gt; &lt;li&gt;‚Äú¬øCu√°les son las noticias de Par√≠s? ¬øQu√© lugares interesantes hay para ver all√≠? ‚Äù&lt;/li&gt; &lt;li&gt;‚ÄúRappelle‚Äëmoi d‚Äôaller √† l‚ÄôAlexanderplatz demain ‚Äì comment devrais‚Äëje m‚Äôhabiller ? Aussi r√®gle le thermostat √† 22 ¬∞C ‚Äù&lt;/li&gt; &lt;li&gt;‚ÄúSpegni la luce della cucina e parlami di Roma‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The system auto‚Äëdetects language, fuzzy‚Äëmatches entities, and calls the appropriate functions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Architecture highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi‚Äëpass prompt engineering (base ‚Üí decision ‚Üí safety ‚Üí format)&lt;/li&gt; &lt;li&gt;Adaptive context windows&lt;/li&gt; &lt;li&gt;Parallel backend routing (HA + PostgreSQL + web APIs)&lt;/li&gt; &lt;li&gt;Reflection‚Äëbased function discovery&lt;/li&gt; &lt;li&gt;Per‚Äëuser conversation memory&lt;/li&gt; &lt;li&gt;Zero‚Äëcloud, privacy‚Äëfirst (no telemetry)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tech stack&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Python 3.10+ (3.12 recommended)&lt;/li&gt; &lt;li&gt;Ollama (any model; Qwen2.5 / GPT‚ÄëOSS tested)&lt;/li&gt; &lt;li&gt;Home Assistant (local or remote)&lt;/li&gt; &lt;li&gt;PostgreSQL (history + embeddings)&lt;/li&gt; &lt;li&gt;OpenWakeWord + Whisper + Piper TTS&lt;/li&gt; &lt;li&gt;Flask + WebSocket chat UI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;One‚Äëcommand setup with an interactive wizard.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Potential Other Uses&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The base structure of the project allows creating RAG-enhanced assistants, integrating with other systems and in general having full control over an Ai assistant that runs locally, but which can perform close to cloud solutions. I've used it to create a translation bot, a URS analysis bot, and many others.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;License &amp;amp; repo&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CC BY 4.0&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/Nemesis533/Local_LLHAMA"&gt;https://github.com/Nemesis533/Local_LLHAMA&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The project had started a while back but after the recent trends in &amp;quot;public AI&amp;quot;, has evolved to the state it is in today - happy to answer questions and get your feedback!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q4oyj5hhlcfg1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7140dd5fe64945e4622f108184ae27792a3e1731"&gt;https://preview.redd.it/q4oyj5hhlcfg1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7140dd5fe64945e4622f108184ae27792a3e1731&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NicolaZanarini533"&gt; /u/NicolaZanarini533 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlw48b/local_llhama_project_orchestration_middleware_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlw48b/local_llhama_project_orchestration_middleware_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qlw48b/local_llhama_project_orchestration_middleware_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-24T19:20:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm8g8t</id>
    <title>DGX Spark worth it for finetuning larger models?</title>
    <updated>2026-01-25T03:52:23+00:00</updated>
    <author>
      <name>/u/goldcakes</name>
      <uri>https://old.reddit.com/user/goldcakes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I already have a 5090 and have been having a lot of fun with Unsloth, and I‚Äôm ready to move on to larger models.&lt;/p&gt; &lt;p&gt;However, as I‚Äôm also a professional video editor, my 5090 often needs to be rendering/exporting, and it‚Äôs a pain constantly switching workloads. &lt;/p&gt; &lt;p&gt;I already know it‚Äôs a terrible rig for LLM inference, but can people confirm it‚Äôs okay for hobbyist local fine-tuning of larger LLMs? &lt;/p&gt; &lt;p&gt;With prices increasing on Strix Halo, the DGX premium seems a lot easier to justify‚Ä¶ lets me stay within CUDA and keeps my partner happier than building another big, power hungry rig.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goldcakes"&gt; /u/goldcakes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qm8g8t/dgx_spark_worth_it_for_finetuning_larger_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qm8g8t/dgx_spark_worth_it_for_finetuning_larger_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qm8g8t/dgx_spark_worth_it_for_finetuning_larger_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T03:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlssc0</id>
    <title>HashIndex: An alternative to a page that doesn't require RAG but can still perform indexing well.</title>
    <updated>2026-01-24T17:19:08+00:00</updated>
    <author>
      <name>/u/jasonhon2013</name>
      <uri>https://old.reddit.com/user/jasonhon2013</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Pardus AI team has decided to open source our memory system, which is similar to PageIndex. However, instead of using a B+ tree, we use a hash map to handle data. This feature allows you to parse the document only once, while achieving retrieval performance on par with PageIndex and significantly better than embedding vector search. It also supports Ollama. Give it a try and consider implementing it in your system ‚Äî you might like it! Give us a star maybe hahahaha&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/JasonHonKL/HashIndex/tree/main"&gt; https://github.com/JasonHonKL/HashIndex/tree/main &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonhon2013"&gt; /u/jasonhon2013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlssc0/hashindex_an_alternative_to_a_page_that_doesnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlssc0/hashindex_an_alternative_to_a_page_that_doesnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qlssc0/hashindex_an_alternative_to_a_page_that_doesnt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-24T17:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmeuon</id>
    <title>Image generation API</title>
    <updated>2026-01-25T09:39:22+00:00</updated>
    <author>
      <name>/u/empios</name>
      <uri>https://old.reddit.com/user/empios</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As I saw there is image generation on macOS using flux or z-image models and I tested it out in terminal looking promising! Is there any api currently working to connect it via local network similar to chat generation? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/empios"&gt; /u/empios &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmeuon/image_generation_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmeuon/image_generation_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qmeuon/image_generation_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T09:39:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlvud9</id>
    <title>Mac Studio as host for Ollama</title>
    <updated>2026-01-24T19:10:24+00:00</updated>
    <author>
      <name>/u/amgsus</name>
      <uri>https://old.reddit.com/user/amgsus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I‚Äôm wondering if it worth buying Mac Studio M4 Max (64 GB) for hosting Ollama. Does anyone have experience with this box? Or better to build a cluster of GPUs like RTX 3090, etc.?&lt;/p&gt; &lt;p&gt;Primarily, I will be using LLMs for coding. Rarely, for media content generation.&lt;/p&gt; &lt;p&gt;Kind regards&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amgsus"&gt; /u/amgsus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlvud9/mac_studio_as_host_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlvud9/mac_studio_as_host_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qlvud9/mac_studio_as_host_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-24T19:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn55ks</id>
    <title>I Can't Get Ollama running through Continue to write complex code... Is there a setting I can adjust or is it a timeout window I have to adjust?</title>
    <updated>2026-01-26T03:45:53+00:00</updated>
    <author>
      <name>/u/warpanomaly</name>
      <uri>https://old.reddit.com/user/warpanomaly</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qn55ks/i_cant_get_ollama_running_through_continue_to/"&gt; &lt;img alt="I Can't Get Ollama running through Continue to write complex code... Is there a setting I can adjust or is it a timeout window I have to adjust?" src="https://b.thumbs.redditmedia.com/8YUx7bhRCKolvRLIKfmf9iWOhGzOhxab0786jQwzC6E.jpg" title="I Can't Get Ollama running through Continue to write complex code... Is there a setting I can adjust or is it a timeout window I have to adjust?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/warpanomaly"&gt; /u/warpanomaly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1qn552r/i_cant_get_ollama_running_through_continue_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qn55ks/i_cant_get_ollama_running_through_continue_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qn55ks/i_cant_get_ollama_running_through_continue_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T03:45:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmlc8p</id>
    <title>My AI Open Source Workflow</title>
    <updated>2026-01-25T15:01:05+00:00</updated>
    <author>
      <name>/u/candidosales</name>
      <uri>https://old.reddit.com/user/candidosales</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately, I have been studying AI and Open Source Workflows. I thought it would be interesting to share a bit of what I'm learning: &lt;a href="https://www.candidosales.me/blog/my-ai-open-source-workflow"&gt;https://www.candidosales.me/blog/my-ai-open-source-workflow&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/candidosales"&gt; /u/candidosales &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmlc8p/my_ai_open_source_workflow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmlc8p/my_ai_open_source_workflow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qmlc8p/my_ai_open_source_workflow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T15:01:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn1v2d</id>
    <title>On-device tool calling with Llama 3.2 3B on iPhone - made it suggest sushi restaurants [Open Source, React Native]</title>
    <updated>2026-01-26T01:20:21+00:00</updated>
    <author>
      <name>/u/New_Inflation_6927</name>
      <uri>https://old.reddit.com/user/New_Inflation_6927</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Inflation_6927"&gt; /u/New_Inflation_6927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1qn1uux/ondevice_tool_calling_with_llama_32_3b_on_iphone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qn1v2d/ondevice_tool_calling_with_llama_32_3b_on_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qn1v2d/ondevice_tool_calling_with_llama_32_3b_on_iphone/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T01:20:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmwuk8</id>
    <title>Penny - or why the Model agnostic multi agent orchestrator needs an VM</title>
    <updated>2026-01-25T21:58:59+00:00</updated>
    <author>
      <name>/u/Glum_Mistake1933</name>
      <uri>https://old.reddit.com/user/Glum_Mistake1933</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;While working with AI, I kept running into the same limitations. I can't claim to have solved everything, but I've made some progress by building a custom setup that I wanted to share for feedback.&lt;/p&gt; &lt;p&gt;The core idea of AI is to help me with things I don't understand. So, what do I know nothing about? &lt;strong&gt;HR Departments.&lt;/strong&gt; It‚Äôs a shame because valid organizational structures are helpful. So, I built a virtual HR department (an orchestrator with a small DB for &amp;quot;employee memory&amp;quot;).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Workflow:&lt;/strong&gt; It started with simple roleplay. I would say: &lt;em&gt;&amp;quot;HR, I have this Python problem...&amp;quot;&lt;/em&gt; via a prompt. HR would then retrieve exactly the experts needed for that specific context. For example: &lt;em&gt;&amp;quot;Ah, Employee M004 (Python Dev) is helpful here, along with M008 (Security Admin) and M010 (Virtual Environment Specialist).&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Even as a pure roleplay context injection, this had tangible effects. A single model instance often struggles with depth, but a &amp;quot;bag of experts&amp;quot; (Multi-Agent approach) handles complexity much better. Adding a Project Manager and a QA Department agent, for instance, yielded huge improvements in output quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Evolution: Hybrid VMs &amp;amp; The &amp;quot;Penny&amp;quot; Incident&lt;/strong&gt; Then I had a major idea: What if I gave HR actual logins for these employees? Instead of just simulating the dialogue, I now have the agents log into actual VMs and start their own CLI instances.&lt;/p&gt; &lt;p&gt;Crucially, I learned that &lt;strong&gt;strict isolation via VMs is mandatory&lt;/strong&gt;. Before I hid the agents from each other, I ran into serious conflicts due to &amp;quot;creative&amp;quot; problem-solving.&lt;/p&gt; &lt;p&gt;&lt;em&gt;The Incident:&lt;/em&gt; I had an agent named &lt;strong&gt;&amp;quot;Penny&amp;quot;&lt;/strong&gt; responsible for checking incoming emails. When server-side connection issues occurred, error logs started piling up. My QA agent analyzed the situation and decided to &amp;quot;solve&amp;quot; it. Instead of fixing the server, &lt;strong&gt;QA simply deleted Penny.&lt;/strong&gt; The logic was flawless but terrifying: &lt;em&gt;&amp;quot;If the entity complaining about errors is gone, the errors stop. Problem solved.&amp;quot;&lt;/em&gt; It makes you smile now, but Penny was gone. Since then, everyone lives in their own isolated VM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Architecture: Hybrid &amp;amp; Model Agnosticism&lt;/strong&gt; This architecture is now &lt;strong&gt;hybrid and model-agnostic&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Infrastructure:&lt;/strong&gt; The instances can run locally on my hardware or reside on remote servers. HR doesn't care; it just assigns the task.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Swarm:&lt;/strong&gt; This setup allows me to mix and match models. I expect to run a heterogeneous team: local open-weights like &lt;strong&gt;Mistral&lt;/strong&gt; and &lt;strong&gt;DeepSeek&lt;/strong&gt; for specific tasks/privacy, alongside &lt;strong&gt;Gemini&lt;/strong&gt; and &lt;strong&gt;ChatGPT&lt;/strong&gt; instances for broader reasoning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scalability:&lt;/strong&gt; I can scale a single model across multiple instances (3 employees = 3 processes) or deploy different models for different perspectives.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Control &amp;amp; Cost Management (The &amp;quot;Human-in-the-Loop&amp;quot;)&lt;/strong&gt; How do I know what they are doing? They communicate via &lt;strong&gt;Email&lt;/strong&gt;, and I make the final decisions. I can read every thread via my regular mail client. This acts as a central kill-switch: I can simply send &amp;quot;SHUTDOWN VM&amp;quot; to the central controller, and all instances‚Äîlocal or remote‚Äîdrop. To manage costs (since I mix API-based models with local ones), I recently instructed HR to appoint a &amp;quot;Token Accountant.&amp;quot; It‚Äôs just good to know that role exists!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Orchestrator:&lt;/strong&gt; Custom &amp;quot;HR&amp;quot; logic with memory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Infrastructure:&lt;/strong&gt; Ubuntu Server VMs (Local &amp;amp; Remote).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; My local server has 32GB RAM. Standard Ubuntu Server eats about 2GB, so resource management is key when running local inference, but offloading to remote APIs helps balance the load.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Networking:&lt;/strong&gt; Setting up the VMs (50MB minimal distros exist, but I stuck with Ubuntu for compatibility) was a patience test‚Äîspecifically networking, mail clients, and browsers within the CLI environment.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Question for the community:&lt;/strong&gt; What do you think of this architecture? A central dispatcher (&amp;quot;HR&amp;quot;) deploying a hybrid swarm of autonomous agents (DeepSeek, Mistral, Gemini, etc.) via isolated VMs to prevent them from deleting each other, while I stay in the loop via Email.&lt;/p&gt; &lt;p&gt;Does this resemble any existing frameworks you use, or is this a completely weird approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glum_Mistake1933"&gt; /u/Glum_Mistake1933 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmwuk8/penny_or_why_the_model_agnostic_multi_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmwuk8/penny_or_why_the_model_agnostic_multi_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qmwuk8/penny_or_why_the_model_agnostic_multi_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T21:58:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmmp22</id>
    <title>Claude Code stuck on &lt;function=TaskList&gt; when using Ollama + Qwen3-Coder</title>
    <updated>2026-01-25T15:52:04+00:00</updated>
    <author>
      <name>/u/Healthy-Laugh-6745</name>
      <uri>https://old.reddit.com/user/Healthy-Laugh-6745</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm struggling to get Claude Code working with Ollama on my Mac M4 Max (48GB RAM). I strictly followed the official Ollama integration guide (&lt;a href="https://docs.ollama.com/integrations/claude-code"&gt;https://docs.ollama.com/integrations/claude-code&lt;/a&gt;), but I'm stuck in a loop.&lt;/p&gt; &lt;p&gt;Every time I ask the model to perform a file-based task (e.g., &amp;quot;create a txt file&amp;quot;), the process hangs indefinitely.&lt;/p&gt; &lt;p&gt;The model acknowledges the request.&lt;/p&gt; &lt;p&gt;It outputs: ‚ùØ &amp;lt;function=TaskList&amp;gt; ‚è∫&lt;/p&gt; &lt;p&gt;Nothing happens after that. No file is created, and the terminal just sits there with the &amp;quot;active&amp;quot; dot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Laugh-6745"&gt; /u/Healthy-Laugh-6745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T15:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qna4xh</id>
    <title>connecting dokuwiki zu ollama?</title>
    <updated>2026-01-26T08:06:47+00:00</updated>
    <author>
      <name>/u/Tennis0711</name>
      <uri>https://old.reddit.com/user/Tennis0711</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;br /&gt; at the moment wie download the txt files from dokuwiki and upload it to ollama. Can i connect dokuwiki direktly as knowledgebase to the ollama model?&lt;/p&gt; &lt;p&gt;greetings, lars&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tennis0711"&gt; /u/Tennis0711 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qna4xh/connecting_dokuwiki_zu_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qna4xh/connecting_dokuwiki_zu_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qna4xh/connecting_dokuwiki_zu_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T08:06:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmteov</id>
    <title>Claraverse is not dead, now you can use AI with more fun, more productivity, and more PRIVACY.</title>
    <updated>2026-01-25T19:52:48+00:00</updated>
    <author>
      <name>/u/aruntemme</name>
      <uri>https://old.reddit.com/user/aruntemme</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qmteov/claraverse_is_not_dead_now_you_can_use_ai_with/"&gt; &lt;img alt="Claraverse is not dead, now you can use AI with more fun, more productivity, and more PRIVACY." src="https://b.thumbs.redditmedia.com/3hQSxjjHHYxVm66vsPAtk8Nw52svybso3qjhCjI2TpM.jpg" title="Claraverse is not dead, now you can use AI with more fun, more productivity, and more PRIVACY." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;#x200b;&lt;/p&gt; &lt;p&gt;We updated our AI workspace to actually gets things done locally (not just another chatbot or AI slope)&lt;/p&gt; &lt;p&gt;we've been grinding on ClaraVerse for the past few months, and we just dropped a major update. If you're tired of AI tools that just... talk at you, just check this out...&lt;/p&gt; &lt;p&gt;best part is you can even connect ollama ü¶ô as a provider &lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Run it anywhere:&lt;/strong&gt; CLI tool that works on your laptop, VPS, cloud, whatever. No platform lock-in BS.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;50+ integrations:&lt;/strong&gt; Gmail, Sheets, Discord, Slack, you name it. we'll be adding more soon&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Actual automation:&lt;/strong&gt; Build agents that DO things, not just answer questions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chat-first workflow builder:&lt;/strong&gt; Like n8n/Zapier but for AI. Chat your way through creating workflows ask, create, iterate.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Everything becomes an API:&lt;/strong&gt; Seriously, every workflow you build = instant API endpoint or schedule it daily, hourly your choice.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One-liner&lt;/strong&gt;: It's an all-in-one platform (chat, image gen, agents, docs, search). Every tool is part of the package.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's actually new (beyond UI polish)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Built-in tools that agents and chats need:&lt;/p&gt; &lt;p&gt;- PPT, PDF, XLSX readers and creators&lt;/p&gt; &lt;p&gt;- Isolated code execution with dependency management&lt;/p&gt; &lt;p&gt;- Interactive chat so local LLMs can ask clarifying questions mid-prompt&lt;/p&gt; &lt;p&gt;- Search, scrape, image search, API tools, and memory all default&lt;/p&gt; &lt;p&gt;- Tool router if you have too many tools&lt;/p&gt; &lt;p&gt;- Memories that can remember and forget based on your usage&lt;/p&gt; &lt;p&gt;- Connect your Gmail, Sheets, Discord, Slack, and more&lt;/p&gt; &lt;p&gt;Simple UI works on phone responsive as hell&lt;/p&gt; &lt;p&gt;Try it and let us know&lt;/p&gt; &lt;p&gt;GitHub: github.com/claraverse-space/ClaraVerse&lt;/p&gt; &lt;p&gt;We're open source and privacy-first (chat and data stored in browser or DB, even when self-hosted - user's choice).&lt;/p&gt; &lt;p&gt;I use this myself every day. Honestly, I've seen worse tools raise fund and then lock everything behind subscriptions. This community helped build this with feedback, so it's staying free and open-source.&lt;/p&gt; &lt;p&gt;Happy to answer questions, take feature requests, or hear about how it crashes on your machine so we can fix and improve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aruntemme"&gt; /u/aruntemme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qmteov"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmteov/claraverse_is_not_dead_now_you_can_use_ai_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qmteov/claraverse_is_not_dead_now_you_can_use_ai_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T19:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm9cgp</id>
    <title>Ollama Models Ranked by VRAM Requirements</title>
    <updated>2026-01-25T04:36:31+00:00</updated>
    <author>
      <name>/u/AdventurousLion9548</name>
      <uri>https://old.reddit.com/user/AdventurousLion9548</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1250.08 GB | cogito-2.1:latest&lt;/p&gt; &lt;p&gt;1250.08 GB | cogito-2.1:671b&lt;/p&gt; &lt;p&gt;376.71 GB | deepseek-v3.1:latest&lt;/p&gt; &lt;p&gt;376.71 GB | deepseek-v3.1:671b&lt;/p&gt; &lt;p&gt;376.65 GB | deepseek-r1:671b&lt;/p&gt; &lt;p&gt;376.65 GB | deepseek-v3:latest&lt;/p&gt; &lt;p&gt;376.65 GB | deepseek-v3:671b&lt;/p&gt; &lt;p&gt;376.65 GB | r1-1776:671b&lt;/p&gt; &lt;p&gt;270.14 GB | qwen3-coder:480b&lt;/p&gt; &lt;p&gt;226.38 GB | llama3.1:405b&lt;/p&gt; &lt;p&gt;213.14 GB | hermes3:405b&lt;/p&gt; &lt;p&gt;133.43 GB | qwen3-vl:235b&lt;/p&gt; &lt;p&gt;132.39 GB | qwen3:235b&lt;/p&gt; &lt;p&gt;123.78 GB | deepseek-coder-v2:236b&lt;/p&gt; &lt;p&gt;123.78 GB | deepseek-v2:236b&lt;/p&gt; &lt;p&gt;123.78 GB | deepseek-v2.5:latest&lt;/p&gt; &lt;p&gt;123.78 GB | deepseek-v2.5:236b&lt;/p&gt; &lt;p&gt;94.51 GB | falcon:180b&lt;/p&gt; &lt;p&gt;74.05 GB | zephyr:141b&lt;/p&gt; &lt;p&gt;69.75 GB | devstral-2:latest&lt;/p&gt; &lt;p&gt;69.75 GB | devstral-2:123b&lt;/p&gt; &lt;p&gt;69.1 GB | dbrx:latest&lt;/p&gt; &lt;p&gt;69.1 GB | dbrx:132b&lt;/p&gt; &lt;p&gt;68.19 GB | mistral-large:latest&lt;/p&gt; &lt;p&gt;68.19 GB | mistral-large:123b&lt;/p&gt; &lt;p&gt;63.1 GB | megadolphin:latest&lt;/p&gt; &lt;p&gt;63.1 GB | megadolphin:120b&lt;/p&gt; &lt;p&gt;62.81 GB | llama4:latest&lt;/p&gt; &lt;p&gt;62.52 GB | command-a:latest&lt;/p&gt; &lt;p&gt;62.52 GB | command-a:111b&lt;/p&gt; &lt;p&gt;60.88 GB | gpt-oss:120b&lt;/p&gt; &lt;p&gt;60.88 GB | gpt-oss-safeguard:120b&lt;/p&gt; &lt;p&gt;58.57 GB | qwen:110b&lt;/p&gt; &lt;p&gt;55.15 GB | command-r-plus:latest&lt;/p&gt; &lt;p&gt;55.15 GB | command-r-plus:104b&lt;/p&gt; &lt;p&gt;50.87 GB | llama3.2-vision:90b&lt;/p&gt; &lt;p&gt;46.89 GB | qwen3-next:latest&lt;/p&gt; &lt;p&gt;46.89 GB | qwen3-next:80b&lt;/p&gt; &lt;p&gt;45.36 GB | qwen2.5vl:72b&lt;/p&gt; &lt;p&gt;44.16 GB | athene-v2:latest&lt;/p&gt; &lt;p&gt;44.16 GB | athene-v2:72b&lt;/p&gt; &lt;p&gt;44.16 GB | qwen2.5:72b&lt;/p&gt; &lt;p&gt;39.6 GB | cogito:70b&lt;/p&gt; &lt;p&gt;39.6 GB | deepseek-r1:70b&lt;/p&gt; &lt;p&gt;39.6 GB | llama3.1:70b&lt;/p&gt; &lt;p&gt;39.6 GB | llama3.3:latest&lt;/p&gt; &lt;p&gt;39.6 GB | llama3.3:70b&lt;/p&gt; &lt;p&gt;39.6 GB | nemotron:latest&lt;/p&gt; &lt;p&gt;39.6 GB | nemotron:70b&lt;/p&gt; &lt;p&gt;39.6 GB | r1-1776:latest&lt;/p&gt; &lt;p&gt;39.6 GB | r1-1776:70b&lt;/p&gt; &lt;p&gt;39.6 GB | tulu3:70b&lt;/p&gt; &lt;p&gt;38.4 GB | qwen2:72b&lt;/p&gt; &lt;p&gt;38.4 GB | qwen2-math:72b&lt;/p&gt; &lt;p&gt;38.18 GB | qwen:72b&lt;/p&gt; &lt;p&gt;37.22 GB | dolphin-llama3:70b&lt;/p&gt; &lt;p&gt;37.22 GB | firefunction-v2:latest&lt;/p&gt; &lt;p&gt;37.22 GB | firefunction-v2:70b&lt;/p&gt; &lt;p&gt;37.22 GB | hermes3:70b&lt;/p&gt; &lt;p&gt;37.22 GB | llama3:70b&lt;/p&gt; &lt;p&gt;37.22 GB | llama3-chatqa:70b&lt;/p&gt; &lt;p&gt;37.22 GB | llama3-gradient:70b&lt;/p&gt; &lt;p&gt;37.22 GB | llama3-groq-tool-use:70b&lt;/p&gt; &lt;p&gt;37.22 GB | reflection:latest&lt;/p&gt; &lt;p&gt;37.22 GB | reflection:70b&lt;/p&gt; &lt;p&gt;36.2 GB | codellama:70b&lt;/p&gt; &lt;p&gt;36.2 GB | llama2:70b&lt;/p&gt; &lt;p&gt;36.2 GB | llama2-uncensored:70b&lt;/p&gt; &lt;p&gt;36.2 GB | meditron:70b&lt;/p&gt; &lt;p&gt;36.2 GB | orca-mini:70b&lt;/p&gt; &lt;p&gt;36.2 GB | stable-beluga:70b&lt;/p&gt; &lt;p&gt;36.2 GB | wizard-math:70b&lt;/p&gt; &lt;p&gt;35.53 GB | deepseek-llm:67b&lt;/p&gt; &lt;p&gt;24.63 GB | dolphin-mixtral:latest&lt;/p&gt; &lt;p&gt;24.63 GB | mixtral:latest&lt;/p&gt; &lt;p&gt;24.63 GB | notux:latest&lt;/p&gt; &lt;p&gt;24.63 GB | nous-hermes2-mixtral:latest&lt;/p&gt; &lt;p&gt;22.6 GB | nemotron-3-nano:latest&lt;/p&gt; &lt;p&gt;22.6 GB | nemotron-3-nano:30b&lt;/p&gt; &lt;p&gt;22.17 GB | alfred:latest&lt;/p&gt; &lt;p&gt;22.17 GB | alfred:40b&lt;/p&gt; &lt;p&gt;22.17 GB | falcon:40b&lt;/p&gt; &lt;p&gt;19.71 GB | qwen2.5vl:32b&lt;/p&gt; &lt;p&gt;19.47 GB | qwen3-vl:32b&lt;/p&gt; &lt;p&gt;18.84 GB | aya:35b&lt;/p&gt; &lt;p&gt;18.81 GB | qwen3:32b&lt;/p&gt; &lt;p&gt;18.78 GB | llava:34b&lt;/p&gt; &lt;p&gt;18.49 GB | cogito:32b&lt;/p&gt; &lt;p&gt;18.49 GB | deepseek-r1:32b&lt;/p&gt; &lt;p&gt;18.49 GB | openthinker:32b&lt;/p&gt; &lt;p&gt;18.49 GB | qwen2.5:32b&lt;/p&gt; &lt;p&gt;18.49 GB | qwen2.5-coder:32b&lt;/p&gt; &lt;p&gt;18.49 GB | qwq:latest&lt;/p&gt; &lt;p&gt;18.49 GB | qwq:32b&lt;/p&gt; &lt;p&gt;18.44 GB | aya-expanse:32b&lt;/p&gt; &lt;p&gt;18.25 GB | qwen3-vl:30b&lt;/p&gt; &lt;p&gt;18.14 GB | olmo-3:32b&lt;/p&gt; &lt;p&gt;18.14 GB | olmo-3.1:latest&lt;/p&gt; &lt;p&gt;18.14 GB | olmo-3.1:32b&lt;/p&gt; &lt;p&gt;18.13 GB | nous-hermes2:34b&lt;/p&gt; &lt;p&gt;18.13 GB | yi:34b&lt;/p&gt; &lt;p&gt;18.02 GB | exaone-deep:32b&lt;/p&gt; &lt;p&gt;18.02 GB | exaone3.5:32b&lt;/p&gt; &lt;p&gt;17.92 GB | granite-code:34b&lt;/p&gt; &lt;p&gt;17.74 GB | codebooga:latest&lt;/p&gt; &lt;p&gt;17.74 GB | codebooga:34b&lt;/p&gt; &lt;p&gt;17.74 GB | codellama:34b&lt;/p&gt; &lt;p&gt;17.74 GB | phind-codellama:latest&lt;/p&gt; &lt;p&gt;17.74 GB | phind-codellama:34b&lt;/p&gt; &lt;p&gt;17.53 GB | deepseek-coder:33b&lt;/p&gt; &lt;p&gt;17.53 GB | wizardcoder:33b&lt;/p&gt; &lt;p&gt;17.43 GB | command-r:latest&lt;/p&gt; &lt;p&gt;17.43 GB | command-r:35b&lt;/p&gt; &lt;p&gt;17.28 GB | qwen3:30b&lt;/p&gt; &lt;p&gt;17.28 GB | qwen3-coder:latest&lt;/p&gt; &lt;p&gt;17.28 GB | qwen3-coder:30b&lt;/p&gt; &lt;p&gt;17.23 GB | qwen:32b&lt;/p&gt; &lt;p&gt;17.1 GB | vicuna:33b&lt;/p&gt; &lt;p&gt;17.1 GB | wizard-vicuna-uncensored:30b&lt;/p&gt; &lt;p&gt;16.2 GB | gemma3:27b&lt;/p&gt; &lt;p&gt;16.17 GB | translategemma:27b&lt;/p&gt; &lt;p&gt;15.5 GB | shieldgemma:27b&lt;/p&gt; &lt;p&gt;14.56 GB | gemma2:27b&lt;/p&gt; &lt;p&gt;14.42 GB | mistral-small3.1:latest&lt;/p&gt; &lt;p&gt;14.42 GB | mistral-small3.1:24b&lt;/p&gt; &lt;p&gt;14.14 GB | devstral-small-2:latest&lt;/p&gt; &lt;p&gt;14.14 GB | devstral-small-2:24b&lt;/p&gt; &lt;p&gt;14.14 GB | mistral-small3.2:latest&lt;/p&gt; &lt;p&gt;14.14 GB | mistral-small3.2:24b&lt;/p&gt; &lt;p&gt;13.35 GB | devstral:latest&lt;/p&gt; &lt;p&gt;13.35 GB | devstral:24b&lt;/p&gt; &lt;p&gt;13.35 GB | magistral:latest&lt;/p&gt; &lt;p&gt;13.35 GB | magistral:24b&lt;/p&gt; &lt;p&gt;13.35 GB | mistral-small:latest&lt;/p&gt; &lt;p&gt;13.35 GB | mistral-small:24b&lt;/p&gt; &lt;p&gt;12.85 GB | gpt-oss:latest&lt;/p&gt; &lt;p&gt;12.85 GB | gpt-oss:20b&lt;/p&gt; &lt;p&gt;12.85 GB | gpt-oss-safeguard:latest&lt;/p&gt; &lt;p&gt;12.85 GB | gpt-oss-safeguard:20b&lt;/p&gt; &lt;p&gt;12.4 GB | solar-pro:latest&lt;/p&gt; &lt;p&gt;12.4 GB | solar-pro:22b&lt;/p&gt; &lt;p&gt;11.71 GB | codestral:latest&lt;/p&gt; &lt;p&gt;11.71 GB | codestral:22b&lt;/p&gt; &lt;p&gt;11.71 GB | mistral-small:22b&lt;/p&gt; &lt;p&gt;10.82 GB | sailor2:20b&lt;/p&gt; &lt;p&gt;10.76 GB | granite-code:20b&lt;/p&gt; &lt;p&gt;10.55 GB | internlm2:20b&lt;/p&gt; &lt;p&gt;10.35 GB | phi4-reasoning:latest&lt;/p&gt; &lt;p&gt;10.35 GB | phi4-reasoning:14b&lt;/p&gt; &lt;p&gt;8.64 GB | qwen3:14b&lt;/p&gt; &lt;p&gt;8.46 GB | ministral-3:14b&lt;/p&gt; &lt;p&gt;8.44 GB | dolphincoder:15b&lt;/p&gt; &lt;p&gt;8.44 GB | starcoder2:15b&lt;/p&gt; &lt;p&gt;8.43 GB | phi4:latest&lt;/p&gt; &lt;p&gt;8.43 GB | phi4:14b&lt;/p&gt; &lt;p&gt;8.37 GB | cogito:14b&lt;/p&gt; &lt;p&gt;8.37 GB | deepcoder:latest&lt;/p&gt; &lt;p&gt;8.37 GB | deepcoder:14b&lt;/p&gt; &lt;p&gt;8.37 GB | deepseek-r1:14b&lt;/p&gt; &lt;p&gt;8.37 GB | qwen2.5:14b&lt;/p&gt; &lt;p&gt;8.37 GB | qwen2.5-coder:14b&lt;/p&gt; &lt;p&gt;8.37 GB | sqlcoder:15b&lt;/p&gt; &lt;p&gt;8.37 GB | starcoder:15b&lt;/p&gt; &lt;p&gt;8.29 GB | deepseek-coder-v2:latest&lt;/p&gt; &lt;p&gt;8.29 GB | deepseek-coder-v2:16b&lt;/p&gt; &lt;p&gt;8.29 GB | deepseek-v2:latest&lt;/p&gt; &lt;p&gt;8.29 GB | deepseek-v2:16b&lt;/p&gt; &lt;p&gt;7.78 GB | olmo2:13b&lt;/p&gt; &lt;p&gt;7.62 GB | qwen:14b&lt;/p&gt; &lt;p&gt;7.59 GB | gemma3:12b&lt;/p&gt; &lt;p&gt;7.55 GB | translategemma:12b&lt;/p&gt; &lt;p&gt;7.46 GB | llava:13b&lt;/p&gt; &lt;p&gt;7.35 GB | phi3:14b&lt;/p&gt; &lt;p&gt;7.28 GB | llama3.2-vision:latest&lt;/p&gt; &lt;p&gt;7.28 GB | llama3.2-vision:11b&lt;/p&gt; &lt;p&gt;7.03 GB | gemma3n:latest&lt;/p&gt; &lt;p&gt;6.86 GB | codellama:13b&lt;/p&gt; &lt;p&gt;6.86 GB | codeup:latest&lt;/p&gt; &lt;p&gt;6.86 GB | codeup:13b&lt;/p&gt; &lt;p&gt;6.86 GB | everythinglm:latest&lt;/p&gt; &lt;p&gt;6.86 GB | everythinglm:13b&lt;/p&gt; &lt;p&gt;6.86 GB | llama2:13b&lt;/p&gt; &lt;p&gt;6.86 GB | llama2-chinese:13b&lt;/p&gt; &lt;p&gt;6.86 GB | nexusraven:latest&lt;/p&gt; &lt;p&gt;6.86 GB | nexusraven:13b&lt;/p&gt; &lt;p&gt;6.86 GB | nous-hermes:13b&lt;/p&gt; &lt;p&gt;6.86 GB | open-orca-platypus2:latest&lt;/p&gt; &lt;p&gt;6.86 GB | open-orca-platypus2:13b&lt;/p&gt; &lt;p&gt;6.86 GB | orca-mini:13b&lt;/p&gt; &lt;p&gt;6.86 GB | orca2:13b&lt;/p&gt; &lt;p&gt;6.86 GB | stable-beluga:13b&lt;/p&gt; &lt;p&gt;6.86 GB | vicuna:13b&lt;/p&gt; &lt;p&gt;6.86 GB | wizard-math:13b&lt;/p&gt; &lt;p&gt;6.86 GB | wizard-vicuna:latest&lt;/p&gt; &lt;p&gt;6.86 GB | wizard-vicuna:13b&lt;/p&gt; &lt;p&gt;6.86 GB | wizard-vicuna-uncensored:13b&lt;/p&gt; &lt;p&gt;6.86 GB | wizardlm-uncensored:latest&lt;/p&gt; &lt;p&gt;6.86 GB | wizardlm-uncensored:13b&lt;/p&gt; &lt;p&gt;6.86 GB | xwinlm:13b&lt;/p&gt; &lt;p&gt;6.86 GB | yarn-llama2:13b&lt;/p&gt; &lt;p&gt;6.59 GB | mistral-nemo:latest&lt;/p&gt; &lt;p&gt;6.59 GB | mistral-nemo:12b&lt;/p&gt; &lt;p&gt;6.49 GB | stablelm2:12b&lt;/p&gt; &lt;p&gt;6.23 GB | deepseek-ocr:latest&lt;/p&gt; &lt;p&gt;6.23 GB | deepseek-ocr:3b&lt;/p&gt; &lt;p&gt;5.94 GB | falcon2:latest&lt;/p&gt; &lt;p&gt;5.94 GB | falcon2:11b&lt;/p&gt; &lt;p&gt;5.86 GB | falcon3:10b&lt;/p&gt; &lt;p&gt;5.72 GB | qwen3-vl:latest&lt;/p&gt; &lt;p&gt;5.72 GB | qwen3-vl:8b&lt;/p&gt; &lt;p&gt;5.66 GB | nous-hermes2:latest&lt;/p&gt; &lt;p&gt;5.66 GB | nous-hermes2:10.7b&lt;/p&gt; &lt;p&gt;5.66 GB | solar:latest&lt;/p&gt; &lt;p&gt;5.66 GB | solar:10.7b&lt;/p&gt; &lt;p&gt;5.61 GB | ministral-3:latest&lt;/p&gt; &lt;p&gt;5.61 GB | ministral-3:8b&lt;/p&gt; &lt;p&gt;5.56 GB | qwen2.5vl:latest&lt;/p&gt; &lt;p&gt;5.56 GB | qwen2.5vl:7b&lt;/p&gt; &lt;p&gt;5.4 GB | granite3-guardian:8b&lt;/p&gt; &lt;p&gt;5.37 GB | shieldgemma:latest&lt;/p&gt; &lt;p&gt;5.37 GB | shieldgemma:9b&lt;/p&gt; &lt;p&gt;5.16 GB | llava-llama3:latest&lt;/p&gt; &lt;p&gt;5.16 GB | llava-llama3:8b&lt;/p&gt; &lt;p&gt;5.1 GB | minicpm-v:latest&lt;/p&gt; &lt;p&gt;5.1 GB | minicpm-v:8b&lt;/p&gt; &lt;p&gt;5.08 GB | codegeex4:latest&lt;/p&gt; &lt;p&gt;5.08 GB | codegeex4:9b&lt;/p&gt; &lt;p&gt;5.08 GB | glm4:latest&lt;/p&gt; &lt;p&gt;5.08 GB | glm4:9b&lt;/p&gt; &lt;p&gt;5.07 GB | gemma2:latest&lt;/p&gt; &lt;p&gt;5.07 GB | gemma2:9b&lt;/p&gt; &lt;p&gt;4.88 GB | sailor2:latest&lt;/p&gt; &lt;p&gt;4.88 GB | sailor2:8b&lt;/p&gt; &lt;p&gt;4.87 GB | deepseek-r1:latest&lt;/p&gt; &lt;p&gt;4.87 GB | deepseek-r1:8b&lt;/p&gt; &lt;p&gt;4.87 GB | qwen3:latest&lt;/p&gt; &lt;p&gt;4.87 GB | qwen3:8b&lt;/p&gt; &lt;p&gt;4.76 GB | rnj-1:latest&lt;/p&gt; &lt;p&gt;4.76 GB | rnj-1:8b&lt;/p&gt; &lt;p&gt;4.71 GB | aya-expanse:latest&lt;/p&gt; &lt;p&gt;4.71 GB | aya-expanse:8b&lt;/p&gt; &lt;p&gt;4.71 GB | command-r7b:latest&lt;/p&gt; &lt;p&gt;4.71 GB | command-r7b:7b&lt;/p&gt; &lt;p&gt;4.71 GB | command-r7b-arabic:latest&lt;/p&gt; &lt;p&gt;4.71 GB | command-r7b-arabic:7b&lt;/p&gt; &lt;p&gt;4.69 GB | yi:9b&lt;/p&gt; &lt;p&gt;4.69 GB | yi-coder:latest&lt;/p&gt; &lt;p&gt;4.69 GB | yi-coder:9b&lt;/p&gt; &lt;p&gt;4.67 GB | codegemma:latest&lt;/p&gt; &lt;p&gt;4.67 GB | codegemma:7b&lt;/p&gt; &lt;p&gt;4.67 GB | gemma:latest&lt;/p&gt; &lt;p&gt;4.67 GB | gemma:7b&lt;/p&gt; &lt;p&gt;4.65 GB | granite3.1-dense:latest&lt;/p&gt; &lt;p&gt;4.65 GB | granite3.1-dense:8b&lt;/p&gt; &lt;p&gt;4.6 GB | granite3-dense:8b&lt;/p&gt; &lt;p&gt;4.6 GB | granite3.2:latest&lt;/p&gt; &lt;p&gt;4.6 GB | granite3.2:8b&lt;/p&gt; &lt;p&gt;4.6 GB | granite3.3:latest&lt;/p&gt; &lt;p&gt;4.6 GB | granite3.3:8b&lt;/p&gt; &lt;p&gt;4.58 GB | cogito:latest&lt;/p&gt; &lt;p&gt;4.58 GB | cogito:8b&lt;/p&gt; &lt;p&gt;4.58 GB | dolphin3:latest&lt;/p&gt; &lt;p&gt;4.58 GB | dolphin3:8b&lt;/p&gt; &lt;p&gt;4.58 GB | llama-guard3:latest&lt;/p&gt; &lt;p&gt;4.58 GB | llama-guard3:8b&lt;/p&gt; &lt;p&gt;4.58 GB | llama3.1:latest&lt;/p&gt; &lt;p&gt;4.58 GB | llama3.1:8b&lt;/p&gt; &lt;p&gt;4.58 GB | tulu3:latest&lt;/p&gt; &lt;p&gt;4.58 GB | tulu3:8b&lt;/p&gt; &lt;p&gt;4.47 GB | aya:latest&lt;/p&gt; &lt;p&gt;4.47 GB | aya:8b&lt;/p&gt; &lt;p&gt;4.44 GB | exaone-deep:latest&lt;/p&gt; &lt;p&gt;4.44 GB | exaone-deep:7.8b&lt;/p&gt; &lt;p&gt;4.44 GB | exaone3.5:latest&lt;/p&gt; &lt;p&gt;4.44 GB | exaone3.5:7.8b&lt;/p&gt; &lt;p&gt;4.41 GB | bakllava:latest&lt;/p&gt; &lt;p&gt;4.41 GB | bakllava:7b&lt;/p&gt; &lt;p&gt;4.41 GB | llama-pro:latest&lt;/p&gt; &lt;p&gt;4.41 GB | llava:latest&lt;/p&gt; &lt;p&gt;4.41 GB | llava:7b&lt;/p&gt; &lt;p&gt;4.41 GB | opencoder:latest&lt;/p&gt; &lt;p&gt;4.41 GB | opencoder:8b&lt;/p&gt; &lt;p&gt;4.39 GB | bespoke-minicheck:latest&lt;/p&gt; &lt;p&gt;4.39 GB | bespoke-minicheck:7b&lt;/p&gt; &lt;p&gt;4.36 GB | deepseek-r1:7b&lt;/p&gt; &lt;p&gt;4.36 GB | marco-o1:latest&lt;/p&gt; &lt;p&gt;4.36 GB | marco-o1:7b&lt;/p&gt; &lt;p&gt;4.36 GB | openthinker:latest&lt;/p&gt; &lt;p&gt;4.36 GB | openthinker:7b&lt;/p&gt; &lt;p&gt;4.36 GB | qwen2.5:latest&lt;/p&gt; &lt;p&gt;4.36 GB | qwen2.5:7b&lt;/p&gt; &lt;p&gt;4.36 GB | qwen2.5-coder:latest&lt;/p&gt; &lt;p&gt;4.36 GB | qwen2.5-coder:7b&lt;/p&gt; &lt;p&gt;4.36 GB | qwen3-embedding:latest&lt;/p&gt; &lt;p&gt;4.36 GB | qwen3-embedding:8b&lt;/p&gt; &lt;p&gt;4.34 GB | dolphin-llama3:latest&lt;/p&gt; &lt;p&gt;4.34 GB | dolphin-llama3:8b&lt;/p&gt; &lt;p&gt;4.34 GB | hermes3:latest&lt;/p&gt; &lt;p&gt;4.34 GB | hermes3:8b&lt;/p&gt; &lt;p&gt;4.34 GB | llama3:latest&lt;/p&gt; &lt;p&gt;4.34 GB | llama3:8b&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-chatqa:latest&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-chatqa:8b&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-gradient:latest&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-gradient:8b&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-groq-tool-use:latest&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-groq-tool-use:8b&lt;/p&gt; &lt;p&gt;4.28 GB | granite-code:8b&lt;/p&gt; &lt;p&gt;4.26 GB | falcon3:latest&lt;/p&gt; &lt;p&gt;4.26 GB | falcon3:7b&lt;/p&gt; &lt;p&gt;4.2 GB | qwen:7b&lt;/p&gt; &lt;p&gt;4.16 GB | olmo-3:latest&lt;/p&gt; &lt;p&gt;4.16 GB | olmo-3:7b&lt;/p&gt; &lt;p&gt;4.16 GB | olmo2:latest&lt;/p&gt; &lt;p&gt;4.16 GB | olmo2:7b&lt;/p&gt; &lt;p&gt;4.15 GB | internlm2:latest&lt;/p&gt; &lt;p&gt;4.15 GB | internlm2:7b&lt;/p&gt; &lt;p&gt;4.13 GB | qwen2:latest&lt;/p&gt; &lt;p&gt;4.13 GB | qwen2:7b&lt;/p&gt; &lt;p&gt;4.13 GB | qwen2-math:latest&lt;/p&gt; &lt;p&gt;4.13 GB | qwen2-math:7b&lt;/p&gt; &lt;p&gt;4.07 GB | mistral:latest&lt;/p&gt; &lt;p&gt;4.07 GB | mistral:7b&lt;/p&gt; &lt;p&gt;4.0 GB | starcoder:7b&lt;/p&gt; &lt;p&gt;3.94 GB | dolphincoder:latest&lt;/p&gt; &lt;p&gt;3.94 GB | dolphincoder:7b&lt;/p&gt; &lt;p&gt;3.92 GB | falcon:latest&lt;/p&gt; &lt;p&gt;3.92 GB | falcon:7b&lt;/p&gt; &lt;p&gt;3.89 GB | codeqwen:latest&lt;/p&gt; &lt;p&gt;3.89 GB | codeqwen:7b&lt;/p&gt; &lt;p&gt;3.83 GB | dolphin-mistral:latest&lt;/p&gt; &lt;p&gt;3.83 GB | dolphin-mistral:7b&lt;/p&gt; &lt;p&gt;3.83 GB | mathstral:latest&lt;/p&gt; &lt;p&gt;3.83 GB | mathstral:7b&lt;/p&gt; &lt;p&gt;3.83 GB | mistral-openorca:latest&lt;/p&gt; &lt;p&gt;3.83 GB | mistral-openorca:7b&lt;/p&gt; &lt;p&gt;3.83 GB | mistrallite:latest&lt;/p&gt; &lt;p&gt;3.83 GB | mistrallite:7b&lt;/p&gt; &lt;p&gt;3.83 GB | neural-chat:latest&lt;/p&gt; &lt;p&gt;3.83 GB | neural-chat:7b&lt;/p&gt; &lt;p&gt;3.83 GB | notus:latest&lt;/p&gt; &lt;p&gt;3.83 GB | notus:7b&lt;/p&gt; &lt;p&gt;3.83 GB | openchat:latest&lt;/p&gt; &lt;p&gt;3.83 GB | openchat:7b&lt;/p&gt; &lt;p&gt;3.83 GB | openhermes:latest&lt;/p&gt; &lt;p&gt;3.83 GB | samantha-mistral:latest&lt;/p&gt; &lt;p&gt;3.83 GB | samantha-mistral:7b&lt;/p&gt; &lt;p&gt;3.83 GB | sqlcoder:latest&lt;/p&gt; &lt;p&gt;3.83 GB | sqlcoder:7b&lt;/p&gt; &lt;p&gt;3.83 GB | starling-lm:latest&lt;/p&gt; &lt;p&gt;3.83 GB | starling-lm:7b&lt;/p&gt; &lt;p&gt;3.83 GB | wizard-math:latest&lt;/p&gt; &lt;p&gt;3.83 GB | wizard-math:7b&lt;/p&gt; &lt;p&gt;3.83 GB | wizardlm2:latest&lt;/p&gt; &lt;p&gt;3.83 GB | wizardlm2:7b&lt;/p&gt; &lt;p&gt;3.83 GB | yarn-mistral:latest&lt;/p&gt; &lt;p&gt;3.83 GB | yarn-mistral:7b&lt;/p&gt; &lt;p&gt;3.83 GB | zephyr:latest&lt;/p&gt; &lt;p&gt;3.83 GB | zephyr:7b&lt;/p&gt; &lt;p&gt;3.77 GB | starcoder2:7b&lt;/p&gt; &lt;p&gt;3.73 GB | deepseek-llm:latest&lt;/p&gt; &lt;p&gt;3.73 GB | deepseek-llm:7b&lt;/p&gt; &lt;p&gt;3.56 GB | codellama:latest&lt;/p&gt; &lt;p&gt;3.56 GB | codellama:7b&lt;/p&gt; &lt;p&gt;3.56 GB | deepseek-coder:6.7b&lt;/p&gt; &lt;p&gt;3.56 GB | duckdb-nsql:latest&lt;/p&gt; &lt;p&gt;3.56 GB | duckdb-nsql:7b&lt;/p&gt; &lt;p&gt;3.56 GB | llama2:latest&lt;/p&gt; &lt;p&gt;3.56 GB | llama2:7b&lt;/p&gt; &lt;p&gt;3.56 GB | llama2-chinese:latest&lt;/p&gt; &lt;p&gt;3.56 GB | llama2-chinese:7b&lt;/p&gt; &lt;p&gt;3.56 GB | llama2-uncensored:latest&lt;/p&gt; &lt;p&gt;3.56 GB | llama2-uncensored:7b&lt;/p&gt; &lt;p&gt;3.56 GB | magicoder:latest&lt;/p&gt; &lt;p&gt;3.56 GB | magicoder:7b&lt;/p&gt; &lt;p&gt;3.56 GB | meditron:latest&lt;/p&gt; &lt;p&gt;3.56 GB | meditron:7b&lt;/p&gt; &lt;p&gt;3.56 GB | medllama2:latest&lt;/p&gt; &lt;p&gt;3.56 GB | medllama2:7b&lt;/p&gt; &lt;p&gt;3.56 GB | nous-hermes:latest&lt;/p&gt; &lt;p&gt;3.56 GB | nous-hermes:7b&lt;/p&gt; &lt;p&gt;3.56 GB | orca-mini:7b&lt;/p&gt; &lt;p&gt;3.56 GB | orca2:latest&lt;/p&gt; &lt;p&gt;3.56 GB | orca2:7b&lt;/p&gt; &lt;p&gt;3.56 GB | stable-beluga:latest&lt;/p&gt; &lt;p&gt;3.56 GB | stable-beluga:7b&lt;/p&gt; &lt;p&gt;3.56 GB | vicuna:latest&lt;/p&gt; &lt;p&gt;3.56 GB | vicuna:7b&lt;/p&gt; &lt;p&gt;3.56 GB | wizard-vicuna-uncensored:latest&lt;/p&gt; &lt;p&gt;3.56 GB | wizard-vicuna-uncensored:7b&lt;/p&gt; &lt;p&gt;3.56 GB | xwinlm:latest&lt;/p&gt; &lt;p&gt;3.56 GB | xwinlm:7b&lt;/p&gt; &lt;p&gt;3.56 GB | yarn-llama2:latest&lt;/p&gt; &lt;p&gt;3.56 GB | yarn-llama2:7b&lt;/p&gt; &lt;p&gt;3.37 GB | smallthinker:latest&lt;/p&gt; &lt;p&gt;3.37 GB | smallthinker:3b&lt;/p&gt; &lt;p&gt;3.32 GB | deepscaler:latest&lt;/p&gt; &lt;p&gt;3.32 GB | deepscaler:1.5b&lt;/p&gt; &lt;p&gt;3.24 GB | yi:latest&lt;/p&gt; &lt;p&gt;3.24 GB | yi:6b&lt;/p&gt; &lt;p&gt;3.11 GB | gemma3:latest&lt;/p&gt; &lt;p&gt;3.11 GB | gemma3:4b&lt;/p&gt; &lt;p&gt;3.07 GB | qwen3-vl:4b&lt;/p&gt; &lt;p&gt;3.07 GB | translategemma:latest&lt;/p&gt; &lt;p&gt;3.07 GB | translategemma:4b&lt;/p&gt; &lt;p&gt;3.04 GB | granite4:1b&lt;/p&gt; &lt;p&gt;2.98 GB | qwen2.5vl:3b&lt;/p&gt; &lt;p&gt;2.94 GB | phi4-mini-reasoning:latest&lt;/p&gt; &lt;p&gt;2.94 GB | phi4-mini-reasoning:3.8b&lt;/p&gt; &lt;p&gt;2.75 GB | ministral-3:3b&lt;/p&gt; &lt;p&gt;2.73 GB | llava-phi3:latest&lt;/p&gt; &lt;p&gt;2.73 GB | llava-phi3:3.8b&lt;/p&gt; &lt;p&gt;2.51 GB | granite3-guardian:latest&lt;/p&gt; &lt;p&gt;2.51 GB | granite3-guardian:2b&lt;/p&gt; &lt;p&gt;2.51 GB | nemotron-mini:latest&lt;/p&gt; &lt;p&gt;2.51 GB | nemotron-mini:4b&lt;/p&gt; &lt;p&gt;2.33 GB | qwen3:4b&lt;/p&gt; &lt;p&gt;2.33 GB | qwen3-embedding:4b&lt;/p&gt; &lt;p&gt;2.32 GB | phi4-mini:latest&lt;/p&gt; &lt;p&gt;2.32 GB | phi4-mini:3.8b&lt;/p&gt; &lt;p&gt;2.27 GB | granite3.2-vision:latest&lt;/p&gt; &lt;p&gt;2.27 GB | granite3.2-vision:2b&lt;/p&gt; &lt;p&gt;2.17 GB | qwen:latest&lt;/p&gt; &lt;p&gt;2.17 GB | qwen:4b&lt;/p&gt; &lt;p&gt;2.09 GB | cogito:3b&lt;/p&gt; &lt;p&gt;2.03 GB | nuextract:latest&lt;/p&gt; &lt;p&gt;2.03 GB | nuextract:3.8b&lt;/p&gt; &lt;p&gt;2.03 GB | phi3:latest&lt;/p&gt; &lt;p&gt;2.03 GB | phi3:3.8b&lt;/p&gt; &lt;p&gt;2.03 GB | phi3.5:latest&lt;/p&gt; &lt;p&gt;2.03 GB | phi3.5:3.8b&lt;/p&gt; &lt;p&gt;1.96 GB | granite4:3b&lt;/p&gt; &lt;p&gt;1.92 GB | granite3-moe:3b&lt;/p&gt; &lt;p&gt;1.9 GB | granite3.1-moe:latest&lt;/p&gt; &lt;p&gt;1.9 GB | granite3.1-moe:3b&lt;/p&gt; &lt;p&gt;1.88 GB | hermes3:3b&lt;/p&gt; &lt;p&gt;1.88 GB | llama3.2:latest&lt;/p&gt; &lt;p&gt;1.88 GB | llama3.2:3b&lt;/p&gt; &lt;p&gt;1.87 GB | falcon3:3b&lt;/p&gt; &lt;p&gt;1.86 GB | granite-code:latest&lt;/p&gt; &lt;p&gt;1.86 GB | granite-code:3b&lt;/p&gt; &lt;p&gt;1.84 GB | orca-mini:latest&lt;/p&gt; &lt;p&gt;1.84 GB | orca-mini:3b&lt;/p&gt; &lt;p&gt;1.8 GB | qwen2.5:3b&lt;/p&gt; &lt;p&gt;1.8 GB | qwen2.5-coder:3b&lt;/p&gt; &lt;p&gt;1.76 GB | qwen3-vl:2b&lt;/p&gt; &lt;p&gt;1.71 GB | starcoder:latest&lt;/p&gt; &lt;p&gt;1.71 GB | starcoder:3b&lt;/p&gt; &lt;p&gt;1.7 GB | smollm2:latest&lt;/p&gt; &lt;p&gt;1.7 GB | smollm2:1.7b&lt;/p&gt; &lt;p&gt;1.66 GB | falcon3:1b&lt;/p&gt; &lt;p&gt;1.62 GB | moondream:latest&lt;/p&gt; &lt;p&gt;1.62 GB | moondream:1.8b&lt;/p&gt; &lt;p&gt;1.59 GB | shieldgemma:2b&lt;/p&gt; &lt;p&gt;1.59 GB | starcoder2:latest&lt;/p&gt; &lt;p&gt;1.59 GB | starcoder2:3b&lt;/p&gt; &lt;p&gt;1.56 GB | gemma:2b&lt;/p&gt; &lt;p&gt;1.53 GB | exaone-deep:2.4b&lt;/p&gt; &lt;p&gt;1.53 GB | exaone3.5:2.4b&lt;/p&gt; &lt;p&gt;1.52 GB | gemma2:2b&lt;/p&gt; &lt;p&gt;1.5 GB | stable-code:latest&lt;/p&gt; &lt;p&gt;1.5 GB | stable-code:3b&lt;/p&gt; &lt;p&gt;1.5 GB | stablelm-zephyr:latest&lt;/p&gt; &lt;p&gt;1.5 GB | stablelm-zephyr:3b&lt;/p&gt; &lt;p&gt;1.49 GB | dolphin-phi:latest&lt;/p&gt; &lt;p&gt;1.49 GB | dolphin-phi:2.7b&lt;/p&gt; &lt;p&gt;1.49 GB | granite3-dense:latest&lt;/p&gt; &lt;p&gt;1.49 GB | granite3-dense:2b&lt;/p&gt; &lt;p&gt;1.49 GB | llama-guard3:1b&lt;/p&gt; &lt;p&gt;1.49 GB | phi:latest&lt;/p&gt; &lt;p&gt;1.49 GB | phi:2.7b&lt;/p&gt; &lt;p&gt;1.46 GB | granite3.1-dense:2b&lt;/p&gt; &lt;p&gt;1.44 GB | codegemma:2b&lt;/p&gt; &lt;p&gt;1.44 GB | granite3.2:2b&lt;/p&gt; &lt;p&gt;1.44 GB | granite3.3:2b&lt;/p&gt; &lt;p&gt;1.32 GB | granite3.1-moe:1b&lt;/p&gt; &lt;p&gt;1.32 GB | opencoder:1.5b&lt;/p&gt; &lt;p&gt;1.27 GB | qwen3:1.7b&lt;/p&gt; &lt;p&gt;1.23 GB | llama3.2:1b&lt;/p&gt; &lt;p&gt;1.08 GB | bge-m3:latest&lt;/p&gt; &lt;p&gt;1.08 GB | snowflake-arctic-embed2:latest&lt;/p&gt; &lt;p&gt;1.04 GB | deepcoder:1.5b&lt;/p&gt; &lt;p&gt;1.04 GB | deepseek-r1:1.5b&lt;/p&gt; &lt;p&gt;1.04 GB | internlm2:1.8b&lt;/p&gt; &lt;p&gt;1.04 GB | qwen:1.8b&lt;/p&gt; &lt;p&gt;0.98 GB | sailor2:1b&lt;/p&gt; &lt;p&gt;0.92 GB | qwen2.5:1.5b&lt;/p&gt; &lt;p&gt;0.92 GB | qwen2.5-coder:1.5b&lt;/p&gt; &lt;p&gt;0.92 GB | smollm:latest&lt;/p&gt; &lt;p&gt;0.92 GB | smollm:1.7b&lt;/p&gt; &lt;p&gt;0.92 GB | stablelm2:latest&lt;/p&gt; &lt;p&gt;0.92 GB | stablelm2:1.6b&lt;/p&gt; &lt;p&gt;0.87 GB | qwen2:1.5b&lt;/p&gt; &lt;p&gt;0.87 GB | qwen2-math:1.5b&lt;/p&gt; &lt;p&gt;0.87 GB | reader-lm:latest&lt;/p&gt; &lt;p&gt;0.87 GB | reader-lm:1.5b&lt;/p&gt; &lt;p&gt;0.81 GB | yi-coder:1.5b&lt;/p&gt; &lt;p&gt;0.77 GB | granite3-moe:latest&lt;/p&gt; &lt;p&gt;0.77 GB | granite3-moe:1b&lt;/p&gt; &lt;p&gt;0.76 GB | gemma3:1b&lt;/p&gt; &lt;p&gt;0.72 GB | deepseek-coder:latest&lt;/p&gt; &lt;p&gt;0.72 GB | deepseek-coder:1.3b&lt;/p&gt; &lt;p&gt;0.68 GB | lfm2.5-thinking:latest&lt;/p&gt; &lt;p&gt;0.68 GB | lfm2.5-thinking:1.2b&lt;/p&gt; &lt;p&gt;0.68 GB | starcoder:1b&lt;/p&gt; &lt;p&gt;0.62 GB | bge-large:latest&lt;/p&gt; &lt;p&gt;0.62 GB | mxbai-embed-large:latest&lt;/p&gt; &lt;p&gt;0.62 GB | snowflake-arctic-embed:latest&lt;/p&gt; &lt;p&gt;0.6 GB | qwen3-embedding:0.6b&lt;/p&gt; &lt;p&gt;0.59 GB | tinydolphin:latest&lt;/p&gt; &lt;p&gt;0.59 GB | tinydolphin:1.1b&lt;/p&gt; &lt;p&gt;0.59 GB | tinyllama:latest&lt;/p&gt; &lt;p&gt;0.59 GB | tinyllama:1.1b&lt;/p&gt; &lt;p&gt;0.58 GB | embeddinggemma:latest&lt;/p&gt; &lt;p&gt;0.52 GB | paraphrase-multilingual:latest&lt;/p&gt; &lt;p&gt;0.49 GB | qwen3:0.6b&lt;/p&gt; &lt;p&gt;0.37 GB | qwen:0.5b&lt;/p&gt; &lt;p&gt;0.37 GB | qwen2.5:0.5b&lt;/p&gt; &lt;p&gt;0.37 GB | qwen2.5-coder:0.5b&lt;/p&gt; &lt;p&gt;0.33 GB | qwen2:0.5b&lt;/p&gt; &lt;p&gt;0.33 GB | reader-lm:0.5b&lt;/p&gt; &lt;p&gt;0.28 GB | functiongemma:latest&lt;/p&gt; &lt;p&gt;0.26 GB | nomic-embed-text:latest&lt;/p&gt; &lt;p&gt;0.06 GB | granite-embedding:latest&lt;/p&gt; &lt;p&gt;0.04 GB | all-minilm:latest&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventurousLion9548"&gt; /u/AdventurousLion9548 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T04:36:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnmj5x</id>
    <title>xsukax Ollama AI Prompt Generator - A Privacy-First Tool for Enhancing AI Prompts Locally</title>
    <updated>2026-01-26T17:27:07+00:00</updated>
    <author>
      <name>/u/apt-xsukax</name>
      <uri>https://old.reddit.com/user/apt-xsukax</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apt-xsukax"&gt; /u/apt-xsukax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/PromptEngineering/comments/1qnm14s/xsukax_ollama_ai_prompt_generator_a_privacyfirst/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnmj5x/xsukax_ollama_ai_prompt_generator_a_privacyfirst/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnmj5x/xsukax_ollama_ai_prompt_generator_a_privacyfirst/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T17:27:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnd67t</id>
    <title>Any free ollama models that works well with Cline tool calling?</title>
    <updated>2026-01-26T11:08:04+00:00</updated>
    <author>
      <name>/u/mixoadrian</name>
      <uri>https://old.reddit.com/user/mixoadrian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, i use free local models with ollama on Cline.&lt;/p&gt; &lt;p&gt;I have been using quite a few, deepseek-coder:33b, Qwen3-cider:30b, llama3.1, gemma3:12b.&lt;/p&gt; &lt;p&gt;none work, nearly all tool calling would fail, and often not even able to read file.&lt;/p&gt; &lt;p&gt;it happens only recently, perhaps after an update i didnt realise.&lt;/p&gt; &lt;p&gt;IS this normal or is it just me?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mixoadrian"&gt; /u/mixoadrian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnd67t/any_free_ollama_models_that_works_well_with_cline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnd67t/any_free_ollama_models_that_works_well_with_cline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnd67t/any_free_ollama_models_that_works_well_with_cline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T11:08:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnp78i</id>
    <title>ollama-term: A sleek, terminal-based UI for Ollama ‚Äî because sometimes you just want to run LLMs without leaving the CLI</title>
    <updated>2026-01-26T18:56:33+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qnp78i/ollamaterm_a_sleek_terminalbased_ui_for_ollama/"&gt; &lt;img alt="ollama-term: A sleek, terminal-based UI for Ollama ‚Äî because sometimes you just want to run LLMs without leaving the CLI" src="https://b.thumbs.redditmedia.com/ydEn5j3DSjsy6n2Mbq0CtCmVEOhk93HNTZYu9u0TPio.jpg" title="ollama-term: A sleek, terminal-based UI for Ollama ‚Äî because sometimes you just want to run LLMs without leaving the CLI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/31ju1q1vpqfg1.gif"&gt;https://i.redd.it/31ju1q1vpqfg1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been using Ollama a ton to run local LLMs. I use linux and like the terminal so i create a ui in the terminal that connect to ollama&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/Laszlobeer/ollama-term"&gt;https://github.com/Laszlobeer/ollama-term&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some of the main features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Browse &amp;amp; Manage Models:&lt;/strong&gt; See all your pulled models, pull new ones, delete them, and view details, all in a navigable interface.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat Interface:&lt;/strong&gt; Have multi-turn conversations with any model. The chat pane is clean and focuses on readability.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context &amp;amp; System Prompt:&lt;/strong&gt; Easily set a system prompt and see how many tokens are in your current context window&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lightweight:&lt;/strong&gt; It's a single binary. Just download and run.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uw87e7g5qqfg1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bf0e282b9569d637a97cbd3e5ddf01fb50d98752"&gt;https://preview.redd.it/uw87e7g5qqfg1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bf0e282b9569d637a97cbd3e5ddf01fb50d98752&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnp78i/ollamaterm_a_sleek_terminalbased_ui_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnp78i/ollamaterm_a_sleek_terminalbased_ui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnp78i/ollamaterm_a_sleek_terminalbased_ui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T18:56:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnjla4</id>
    <title>Local image generation with Ollama + FLUX + Celeste AI</title>
    <updated>2026-01-26T15:45:48+00:00</updated>
    <author>
      <name>/u/Familiar_Print_4882</name>
      <uri>https://old.reddit.com/user/Familiar_Print_4882</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qnjla4/local_image_generation_with_ollama_flux_celeste_ai/"&gt; &lt;img alt="Local image generation with Ollama + FLUX + Celeste AI" src="https://preview.redd.it/lkdh65o9rpfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4d9e789cf33f12ea6fff2c60b35758b760a5385" title="Local image generation with Ollama + FLUX + Celeste AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Familiar_Print_4882"&gt; /u/Familiar_Print_4882 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lkdh65o9rpfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnjla4/local_image_generation_with_ollama_flux_celeste_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnjla4/local_image_generation_with_ollama_flux_celeste_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T15:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnsqdl</id>
    <title>Deploying PriceGhost: What ollama model would be best for this type of application?</title>
    <updated>2026-01-26T20:58:38+00:00</updated>
    <author>
      <name>/u/Extra-Citron-7630</name>
      <uri>https://old.reddit.com/user/Extra-Citron-7630</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am looking to deploy the following project &lt;a href="https://github.com/clucraft/PriceGhost"&gt;PriceGhost&lt;/a&gt; and was wondering what ollama model would be good for this application? It's essentially a self-hosted price tracking application that monitors product prices from any website.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extra-Citron-7630"&gt; /u/Extra-Citron-7630 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnsqdl/deploying_priceghost_what_ollama_model_would_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnsqdl/deploying_priceghost_what_ollama_model_would_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnsqdl/deploying_priceghost_what_ollama_model_would_be/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T20:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnyhr5</id>
    <title>Ollama 0.15 can't run in Win svr 2022?</title>
    <updated>2026-01-27T00:36:17+00:00</updated>
    <author>
      <name>/u/IsaacWang</name>
      <uri>https://old.reddit.com/user/IsaacWang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The earlier version is ok. Click and dead after installing the 0.15 version.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IsaacWang"&gt; /u/IsaacWang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnyhr5/ollama_015_cant_run_in_win_svr_2022/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnyhr5/ollama_015_cant_run_in_win_svr_2022/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnyhr5/ollama_015_cant_run_in_win_svr_2022/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T00:36:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnsuou</id>
    <title>Advice needed!</title>
    <updated>2026-01-26T21:02:43+00:00</updated>
    <author>
      <name>/u/TheBlueFlashh</name>
      <uri>https://old.reddit.com/user/TheBlueFlashh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello I'm pretty new to LLM but id like to give it a go. I've got a 5070ti 16gb and a good cpu (although I think it doesnt matter?) and Id like to do some social work with it. That means reasoning from local info only and sumarizing for the most part. If I coud create audios from texts would be awesome as well.&lt;br /&gt; Magistral is my best bet right? how to go with audios?&lt;br /&gt; Thank you everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheBlueFlashh"&gt; /u/TheBlueFlashh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnsuou/advice_needed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnsuou/advice_needed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnsuou/advice_needed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T21:02:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo33hq</id>
    <title>What kind of models can I realistically run on my M2 MacBook Air with 8GB RAM?</title>
    <updated>2026-01-27T03:55:47+00:00</updated>
    <author>
      <name>/u/saintforlife1</name>
      <uri>https://old.reddit.com/user/saintforlife1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are there particular models optimized to run on everyday hardware like this fairly reasonable? TIA!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/saintforlife1"&gt; /u/saintforlife1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qo33hq/what_kind_of_models_can_i_realistically_run_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qo33hq/what_kind_of_models_can_i_realistically_run_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qo33hq/what_kind_of_models_can_i_realistically_run_on_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T03:55:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnpy7y</id>
    <title>Fine tuning open models and prep for robust deployment</title>
    <updated>2026-01-26T19:21:32+00:00</updated>
    <author>
      <name>/u/codes_astro</name>
      <uri>https://old.reddit.com/user/codes_astro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was at a tech event recently and lots of devs mentioned about problem with ML projects, and most common was deployments and production issues.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;note:&lt;/strong&gt; I'm part of the KitOps community&lt;/p&gt; &lt;p&gt;Training a model is crucial but usually the easy part due to tools like Unsloth and lots of other options. You fine-tune it, it works, results look good. But when you start building a product, everything gets messy:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;model files in notebooks&lt;/li&gt; &lt;li&gt;configs and prompts not tracked properly&lt;/li&gt; &lt;li&gt;deployment steps that only work on one machine&lt;/li&gt; &lt;li&gt;datasets or other assets are lying somewhere else&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even when training is clean, moving the model forward feels challenging with real products.&lt;/p&gt; &lt;p&gt;So I tried a full train ‚Üí push ‚Üí pull ‚Üí run flow to see if it could actually be simple.&lt;/p&gt; &lt;p&gt;I fine-tuned a model using Unsloth.&lt;/p&gt; &lt;p&gt;It was fast, becasue I kept it simple for testing purpose, and ran fine using official cookbook. Nothing fancy, just a real dataset and a IBM-Granite-4.0 model.&lt;/p&gt; &lt;p&gt;Training wasn‚Äôt the issue though. What mattered was what came next.&lt;/p&gt; &lt;p&gt;Instead of manually moving files around, I pushed the fine-tuned model to Hugging Face, then imported it into Jozu ML. Jozu treats models like proper versioned artifacts, not random folders.&lt;/p&gt; &lt;p&gt;From there, I used KitOps to pull the model locally. One command and I had everything - weights, configs, metadata in the right place.&lt;/p&gt; &lt;p&gt;After that, running inference or deploying was straightforward.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Now, let me give context on why Jozu or KitOps?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Kitops is only open-source AIML tool for packaging and versioning for ML and it follows best practices for Devops while taking care of AI usecases.&lt;/p&gt; &lt;p&gt;- Jozu is enterprise platform which can be run on-prem on any existing infra and when it comes to problems like hot reload and cold start or pods going offline when making changes in large scale application, it's 7x faster then other in terms of GPU optimization.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The main takeaway for me:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most ML pain isn‚Äôt about training better models.&lt;br /&gt; It‚Äôs about keeping things clean at scale.&lt;/p&gt; &lt;p&gt;Unsloth made training easy.&lt;br /&gt; KitOps kept things organized with versioning and packaging.&lt;br /&gt; Jozu handled production side things like tracking, security and deployment.&lt;/p&gt; &lt;p&gt;I wrote a detailed article &lt;a href="https://mranand.substack.com/p/from-training-to-deployment-push"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codes_astro"&gt; /u/codes_astro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnpy7y/fine_tuning_open_models_and_prep_for_robust/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnpy7y/fine_tuning_open_models_and_prep_for_robust/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnpy7y/fine_tuning_open_models_and_prep_for_robust/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T19:21:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnpc7a</id>
    <title>Vulkan vs ROCm on RX 9070XT (RDNA4): 9% faster, 50% less power</title>
    <updated>2026-01-26T19:01:07+00:00</updated>
    <author>
      <name>/u/Due_Pea_372</name>
      <uri>https://old.reddit.com/user/Due_Pea_372</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Benchmarked Ollama 0.15.1 with qwen3-coder:30b on my RX 9070 XT (gfx1201, 16GB VRAM).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Vulkan&lt;/th&gt; &lt;th align="left"&gt;ROCm&lt;/th&gt; &lt;th align="left"&gt;Difference&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Tokens/s&lt;/td&gt; &lt;td align="left"&gt;52.5&lt;/td&gt; &lt;td align="left"&gt;48.2&lt;/td&gt; &lt;td align="left"&gt;+8.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Power&lt;/td&gt; &lt;td align="left"&gt;68 W&lt;/td&gt; &lt;td align="left"&gt;149 W&lt;/td&gt; &lt;td align="left"&gt;-54%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;VRAM&lt;/td&gt; &lt;td align="left"&gt;16.1 GB&lt;/td&gt; &lt;td align="left"&gt;15.8 GB&lt;/td&gt; &lt;td align="left"&gt;+2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TTFT&lt;/td&gt; &lt;td align="left"&gt;27.8 ms&lt;/td&gt; &lt;td align="left"&gt;22.9 ms&lt;/td&gt; &lt;td align="left"&gt;+21%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Temp&lt;/td&gt; &lt;td align="left"&gt;51¬∞C&lt;/td&gt; &lt;td align="left"&gt;47¬∞C&lt;/td&gt; &lt;td align="left"&gt;+8%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Key takeaway:&lt;/strong&gt; Vulkan is not only faster but dramatically more power efficient on RDNA4. ROCm draws 2x the power for worse performance.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt; Cachyos, ollama-vulkan / ollama-rocm from AUR, ~35 runs each.&lt;/p&gt; &lt;p&gt;Script used for benchmarking: &lt;a href="https://github.com/maeddesg/spielwiese"&gt;https://github.com/maeddesg/spielwiese&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone else seeing similar results on RDNA4? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Pea_372"&gt; /u/Due_Pea_372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnpc7a/vulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnpc7a/vulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnpc7a/vulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T19:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnjk4p</id>
    <title>SHELLper üêö: Qwen3 0.6B for More Reliable Multi-Turn Function Calling</title>
    <updated>2026-01-26T15:44:43+00:00</updated>
    <author>
      <name>/u/gabucz</name>
      <uri>https://old.reddit.com/user/gabucz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We fine-tuned a 0.6B model for converting English to executable bash commands. It's small enough to run locally on your laptop, giving you full data privacy.&lt;/p&gt; &lt;p&gt;Multi-turn tool calling is incredibly challenging for small models - before fine-tuning, Qwen3-0.6B had 84% single-call accuracy, which collapses to &lt;strong&gt;only 42% across 5 turns&lt;/strong&gt;! After our tuning, it reaches 100% on our test set, providing dependable multi-turn capabilities.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Parameters&lt;/th&gt; &lt;th align="left"&gt;Tool call accuracy (test set)&lt;/th&gt; &lt;th align="left"&gt;=&amp;gt; 5-turn tool call accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B Instruct (teacher)&lt;/td&gt; &lt;td align="left"&gt;235B&lt;/td&gt; &lt;td align="left"&gt;99%&lt;/td&gt; &lt;td align="left"&gt;95%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 0.6B (base)&lt;/td&gt; &lt;td align="left"&gt;0.6B&lt;/td&gt; &lt;td align="left"&gt;84%&lt;/td&gt; &lt;td align="left"&gt;42%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3 0.6B (tuned)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.6B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;100%&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;100%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/distil-labs/distil-SHELLper"&gt;https://github.com/distil-labs/distil-SHELLper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface model: &lt;a href="https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper"&gt;https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Quick Start&lt;/h1&gt; &lt;p&gt;Set up the environment:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Set up environment python -m venv .venv . .venv/bin/activate pip install openai huggingface_hub &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Dowload the model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;hf download distil-labs/distil-qwen3-0.6b-SHELLper --local-dir distil_model cd distil_model ollama create distil_model -f Modelfile cd .. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the assistant:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python filesystem_demo.py &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The demo asks for permission before executing commands (for safety) and restricts dangerous operations (like &lt;code&gt;rm -r /&lt;/code&gt;), so don't hesitate to try it!&lt;/p&gt; &lt;h1&gt;How We Trained SHELLper&lt;/h1&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;Multi-turn tool calling is exceptionally hard for small models - performance breaks down as tool calls chain, degrading with each turn. If prediction errors are independent (e.g. due to bad parameter values), an 80% accurate model has just a 33% chance of succeeding across 5 turns.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Single tool call accuracy&lt;/th&gt; &lt;th align="left"&gt;5-turn tool call accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;80%&lt;/td&gt; &lt;td align="left"&gt;33%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;90%&lt;/td&gt; &lt;td align="left"&gt;59%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;95%&lt;/td&gt; &lt;td align="left"&gt;77%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;99%&lt;/td&gt; &lt;td align="left"&gt;95%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;In this demo, we explored whether we could substantially boost a small model's multi-turn performance. We picked a task from the &lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html"&gt;Berkeley function calling leaderboard&lt;/a&gt; - the &lt;a href="https://github.com/ShishirPatil/gorilla/blob/main/berkeley-function-call-leaderboard/bfcl_eval/data/BFCL_v4_multi_turn_base.json"&gt;gorilla file system tool calling task&lt;/a&gt;. Our modifications were:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The original allows multiple tool calls per assistant turn ‚Üí we permit only one&lt;/li&gt; &lt;li&gt;Maximum 5 turns&lt;/li&gt; &lt;li&gt;Commands map to real bash (not gorilla filesystem functions)&lt;/li&gt; &lt;li&gt;Tool call outputs aren't added to the conversation history&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In short, an identical tool set, but simpler &lt;a href="https://github.com/distil-labs/distil-SHELLper/tree/main/data"&gt;train/test data.&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Training Pipeline&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Seed Data:&lt;/strong&gt; We wrote 20 simplified training conversations that span the available tools while remaining reasonably realistic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Synthetic Expansion:&lt;/strong&gt; Through our &lt;a href="https://www.distillabs.ai/blog/small-expert-agents-from-10-examples/?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=shellper"&gt;data synthesis pipeline&lt;/a&gt;, we scaled to thousands of examples.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For handling variable conversation lengths, we split each conversation into subsets ending with a tool call. For instance:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Input] User: List all files =&amp;gt; Model: ls -al =&amp;gt; User: go to directory models [Output] Model: cd models &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;... becomes 2 training points:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Input] User: List all files [Output] Model: ls -al [Input] User: List all files =&amp;gt; Model: ls -al =&amp;gt; User: go to directory models [Output] Model: cd models` &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Fine-tuning:&lt;/strong&gt; We went with &lt;strong&gt;Qwen3-0.6B&lt;/strong&gt; as the &lt;a href="https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning"&gt;best fine-tunable sub-1B&lt;/a&gt; model on our platform that handles tool calling.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Usage Examples&lt;/h1&gt; &lt;p&gt;The assistant takes natural language input, generates bash commands, and optionally runs them (after Y/N confirmation).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Basic filesystem operations&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; python filesystem_demo.py USER: List all files in the current directory COMMAND: ls USER: Create a new directory called test_folder COMMAND: mkdir test_folder USER: Navigate to test_folder COMMAND: cd test_folder &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Limitations and Next Steps&lt;/h1&gt; &lt;p&gt;Right now, we're limited to a basic bash tool set:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;no pipes, compound commands, or multiple tool calls per turn&lt;/li&gt; &lt;li&gt;no validation of invalid commands/parameters&lt;/li&gt; &lt;li&gt;5-turn conversation maximum&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We prioritized getting the simple case right before expanding to more complex scenarios. Up next: support for multiple tool calls (enabling more sophisticated agent workflows) and benchmarking on &lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html"&gt;BFCL&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you're using this in your bash workflows, track failing commands, append them to &lt;code&gt;data/train.jsonl&lt;/code&gt;, and retrain with the updated dataset (or experiment with a larger student model!).&lt;/p&gt; &lt;h1&gt;Discussion&lt;/h1&gt; &lt;p&gt;Interested to hear from the community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is anyone else working on fine-tuning small models for multi-turn tool calling?&lt;/li&gt; &lt;li&gt;What other &amp;quot;narrow but valuable&amp;quot; tasks could benefit from local, privacy-preserving models?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let us know your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gabucz"&gt; /u/gabucz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnjk4p/shellper_qwen3_06b_for_more_reliable_multiturn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnjk4p/shellper_qwen3_06b_for_more_reliable_multiturn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnjk4p/shellper_qwen3_06b_for_more_reliable_multiturn/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T15:44:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnersl</id>
    <title>I built a "Spatial" website for Ollama because I hate linear chats. (Local-first, no DB)</title>
    <updated>2026-01-26T12:31:18+00:00</updated>
    <author>
      <name>/u/yibie</name>
      <uri>https://old.reddit.com/user/yibie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qnersl/i_built_a_spatial_website_for_ollama_because_i/"&gt; &lt;img alt="I built a &amp;quot;Spatial&amp;quot; website for Ollama because I hate linear chats. (Local-first, no DB)" src="https://external-preview.redd.it/YjUzczlrM29ub2ZnMc1v4660SmLmGdaCwdxTOqPLhJLKWmMLYGebzhszHCXS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c84014d04ec9dff478c02a4e070e3416870f2d7" title="I built a &amp;quot;Spatial&amp;quot; website for Ollama because I hate linear chats. (Local-first, no DB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running Llama 3 locally via Ollama for a while, but I kept getting frustrated with the standard &amp;quot;ChatGPT-style&amp;quot; linear interface. My brain doesn't work in a straight line. I'm usually debugging code in one thread, writing docs in another, and brainstorming ideas in a third. In a linear chat, context gets polluted constantly.&lt;/p&gt; &lt;p&gt;So I built a tool called Project Nodal. It's a &amp;quot;Spatial Thinking OS&amp;quot; for your local LLMs.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Infinite Canvas: Drag and drop chat windows (Sticky Notes) anywhere.&lt;/li&gt; &lt;li&gt;Context Isolation: Group backend notes separate from frontend notes.&lt;/li&gt; &lt;li&gt;Forking: This is the big one. Click a message to &amp;quot;fork&amp;quot; it into a new branch/note. Great for &amp;quot;what if&amp;quot; scenarios without ruining the main thread.&lt;/li&gt; &lt;li&gt;100% Local: It uses IndexedDB. No backend database. Connects directly to your Ollama endpoint (or OpenAI if you want).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's open source and I just deployed a demo. &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/yibie/project-nodal"&gt;https://github.com/yibie/project-nodal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://project-nodal-ai.vercel.app/"&gt;https://project-nodal-ai.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;‚ö†Ô∏è A Note on Web Deployment (Vercel/Netlify)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/yibie/project-nodal#%EF%B8%8F-a-note-on-web-deployment-vercelnetlify"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are viewing this demo online (HTTPS), you &lt;strong&gt;cannot&lt;/strong&gt; connect to a local Ollama instance (HTTP) due to browser security policies (Mixed Content Blocking).&lt;/p&gt; &lt;p&gt;To use Local Ollama: Please &lt;a href="https://github.com/yibie/project-nodal"&gt;clone this repo&lt;/a&gt; and run it locally:&lt;/p&gt; &lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/yibie/project-nodal.git"&gt;&lt;code&gt;https://github.com/yibie/project-nodal.git&lt;/code&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;cd project-nodal&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;npm install&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;npm run dev&lt;/code&gt;&lt;/p&gt; &lt;p&gt;To use the Online Demo: Please use an OpenAI or DeepSeek API Key in the settings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yibie"&gt; /u/yibie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/utanig3onofg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnersl/i_built_a_spatial_website_for_ollama_because_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnersl/i_built_a_spatial_website_for_ollama_because_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T12:31:18+00:00</published>
  </entry>
</feed>
