<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-25T05:36:11+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1nnd7n3</id>
    <title>Use your local models to investigate leaks &amp; government docs</title>
    <updated>2025-09-22T04:37:39+00:00</updated>
    <author>
      <name>/u/New_Pomegranate_1060</name>
      <uri>https://old.reddit.com/user/New_Pomegranate_1060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;After a lot of tinkering, I’ve finally released a project I’ve been working on: TruthSeeker.&lt;/p&gt; &lt;p&gt;It’s a tool designed to make it easier to search, parse, and analyze government documents and leaks. Think of it as a way to cut through the noise and surface the signal in huge, messy datasets.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;p&gt;Pulls in documents (FOIA releases, leaks, etc.) Indexes them for fast keyword + context search Helps spot connections and recurring themes&lt;/p&gt; &lt;p&gt;Why I built it: I was tired of watching people drop big document dumps online, only for them to disappear into the void because no one had the time or tools to dig through them properly. This project is my attempt to fix that.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/RawdodReverend/TruthSeeker"&gt;https://github.com/RawdodReverend/TruthSeeker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Videos: &lt;a href="https://www.tiktok.com/@rawdogreverend"&gt;https://www.tiktok.com/@rawdogreverend&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’d love any feedback, feature requests, or just thoughts on whether you’d find this useful. If you try it out and break it, let me know. I want to improve it fast.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Pomegranate_1060"&gt; /u/New_Pomegranate_1060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnd7n3/use_your_local_models_to_investigate_leaks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnd7n3/use_your_local_models_to_investigate_leaks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnd7n3/use_your_local_models_to_investigate_leaks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T04:37:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nny46f</id>
    <title>Ollama cloud and privacy</title>
    <updated>2025-09-22T20:46:47+00:00</updated>
    <author>
      <name>/u/cyuhat</name>
      <uri>https://old.reddit.com/user/cyuhat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I am intrested in the ollama cloud feature but as someone concerned with data privacy I struggle to find all the information I need. Mainly I can't find answer for the following questions: 1. I live in Europe. I know that USA have the USA Patriot Act and the Cloud act which basically give the governement access to any data hosted by US servers in place or abroad. Ollama cloud does not store any log or data in their server, but is it possible then that requests get intercepted? 2. I know Ollama is close to OpenAI and I wanted to ask to whom the datacenter belong to.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cyuhat"&gt; /u/cyuhat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nny46f/ollama_cloud_and_privacy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nny46f/ollama_cloud_and_privacy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nny46f/ollama_cloud_and_privacy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T20:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1no6mfa</id>
    <title>We made a new AI interface that is compatible with Ollama</title>
    <updated>2025-09-23T03:13:15+00:00</updated>
    <author>
      <name>/u/GermainCampman</name>
      <uri>https://old.reddit.com/user/GermainCampman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please check us out if you want a local AI interface that rivals and even surpasses chatGPT in some ways!&lt;/p&gt; &lt;p&gt;magelab.ai &lt;/p&gt; &lt;ul&gt; &lt;li&gt;no vendor lock in&lt;/li&gt; &lt;li&gt;compatible with Ollama&lt;/li&gt; &lt;li&gt;powerful out of box experience &lt;/li&gt; &lt;li&gt;full speech integration &lt;/li&gt; &lt;li&gt;transparent use of AI by design&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GermainCampman"&gt; /u/GermainCampman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no6mfa/we_made_a_new_ai_interface_that_is_compatible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no6mfa/we_made_a_new_ai_interface_that_is_compatible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1no6mfa/we_made_a_new_ai_interface_that_is_compatible/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T03:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnktup</id>
    <title>How does Ollama run gpt-oss?</title>
    <updated>2025-09-22T12:15:32+00:00</updated>
    <author>
      <name>/u/AirCigar</name>
      <uri>https://old.reddit.com/user/AirCigar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi.&lt;/p&gt; &lt;p&gt;As far as I understand, running gpt-oss with native mxfp4 quantization requires Hopper architecture and newer. However, I've seen people run people run it on Ada Lovelace GPUs such as RTX 4090. What does Ollama do to support mxfp4? I couldn't find any documentation.&lt;/p&gt; &lt;p&gt;Transformers workaround is dequantization, according to &lt;a href="https://github.com/huggingface/transformers/pull/39940"&gt;https://github.com/huggingface/transformers/pull/39940&lt;/a&gt;, does Ollama do something similar?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AirCigar"&gt; /u/AirCigar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnktup/how_does_ollama_run_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnktup/how_does_ollama_run_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnktup/how_does_ollama_run_gptoss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T12:15:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nolw3a</id>
    <title>Qoder plans at 50% off !!</title>
    <updated>2025-09-23T16:21:25+00:00</updated>
    <author>
      <name>/u/Training-Surround228</name>
      <uri>https://old.reddit.com/user/Training-Surround228</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nolw3a/qoder_plans_at_50_off/"&gt; &lt;img alt="Qoder plans at 50% off !!" src="https://preview.redd.it/5yaiypjpwxqf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32363581516df275873b8abd689133f84f910bfe" title="Qoder plans at 50% off !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found it to perform reasonably well during free trial. &lt;/p&gt; &lt;p&gt;Wanted to get community feedback before subscribing. &lt;/p&gt; &lt;p&gt;I already have Trae subscription which went to shit earlier, but last few days have been good (perhaps Sonnet 4 APi bugs resolved) . Will adding this be worth it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Training-Surround228"&gt; /u/Training-Surround228 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5yaiypjpwxqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nolw3a/qoder_plans_at_50_off/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nolw3a/qoder_plans_at_50_off/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T16:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1no9yio</id>
    <title>ADAM - Your Agile Digital Asisstant</title>
    <updated>2025-09-23T06:21:45+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1no9yio/adam_your_agile_digital_asisstant/"&gt; &lt;img alt="ADAM - Your Agile Digital Asisstant" src="https://external-preview.redd.it/MHVvbmI4OHJ4dXFmMX7JxH1xO5NXSruR7HvEl3I_E7WAJ5O94MjagrWhHLIm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b86cb7a5b6b40d530c6f0ae24fff106e65d6f5da" title="ADAM - Your Agile Digital Asisstant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;take a sneak peak at &lt;a href="https://adam-showcase.vercel.app"&gt;ADAM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Post in your prompts for ADAM to response below. This will also be part of my stress testing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/twvnn98rxuqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no9yio/adam_your_agile_digital_asisstant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1no9yio/adam_your_agile_digital_asisstant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T06:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1no7ab4</id>
    <title>Qwen3-Omni coming soon?</title>
    <updated>2025-09-23T03:47:23+00:00</updated>
    <author>
      <name>/u/veryhasselglad</name>
      <uri>https://old.reddit.com/user/veryhasselglad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyway to test this with ollama right now from hf?&lt;br /&gt; Will ollama make their own tweaks before release?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/veryhasselglad"&gt; /u/veryhasselglad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no7ab4/qwen3omni_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no7ab4/qwen3omni_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1no7ab4/qwen3omni_coming_soon/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T03:47:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nob7hx</id>
    <title>Calling through the API causes the model to be crazy. Anybody else experiencing this?</title>
    <updated>2025-09-23T07:43:20+00:00</updated>
    <author>
      <name>/u/businessAlcoholCream</name>
      <uri>https://old.reddit.com/user/businessAlcoholCream</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use gemma3:4b-it-qat for this project and it has been working for almost 3 months now but I noticed starting yesterday, the model went crazy.&lt;/p&gt; &lt;p&gt;The project is a simple python script that takes in information from &lt;a href="http://vlr.gg"&gt;vlr.gg&lt;/a&gt;, process it, and then pass it to the model. I made sure that it runs on startup too. I use it to be updated on what is happening to teams I like. With the information collected, I process it to prompts like these&lt;/p&gt; &lt;p&gt;&amp;quot;Team X is about to face team Y in z days&amp;quot;&lt;br /&gt; &amp;quot;Team X previous match against team W resulted to a score of 2:0&amp;quot;&lt;br /&gt; &amp;quot;Team A has no upcoming match&amp;quot;&lt;br /&gt; &amp;quot;Team B has no upcoming match&amp;quot;&lt;/p&gt; &lt;p&gt;After giving all the necessary prompts as the user, I give the model one final prompt along the lines of&lt;/p&gt; &lt;p&gt;&amp;quot;With those information, create a single paragraph summary to keep me updated on what is happening in VCT&amp;quot;&lt;/p&gt; &lt;p&gt;It worked well before and I would get results like&lt;/p&gt; &lt;p&gt;&amp;quot;Here is your summary for the day. Team X is about to face team Y in z days. In their previous match, they won against team W with a score of 2:0&amp;quot;&lt;/p&gt; &lt;p&gt;But starting yesterday, I get results like&lt;/p&gt; &lt;p&gt;&amp;quot;I'm&lt;/p&gt; &lt;p&gt;Okay, I want to be&lt;/p&gt; &lt;p&gt;I want a report&lt;/p&gt; &lt;p&gt;report.&lt;/p&gt; &lt;p&gt;Do not&lt;/p&gt; &lt;p&gt;Do&lt;/p&gt; &lt;p&gt;I don't.&lt;/p&gt; &lt;p&gt;&amp;quot;&lt;/p&gt; &lt;p&gt;and&lt;/p&gt; &lt;p&gt;&amp;quot; to&lt;/p&gt; &lt;p&gt;The only&lt;/p&gt; &lt;p&gt;to deliver&lt;/p&gt; &lt;p&gt;It's.&lt;/p&gt; &lt;p&gt;the.&lt;/p&gt; &lt;p&gt;to deliver&lt;/p&gt; &lt;p&gt;to.&lt;/p&gt; &lt;p&gt;a&lt;/p&gt; &lt;p&gt;It's&lt;/p&gt; &lt;p&gt;to&lt;/p&gt; &lt;p&gt;I&lt;/p&gt; &lt;p&gt;The summary&lt;/p&gt; &lt;p&gt;to&lt;/p&gt; &lt;p&gt;to be&lt;/p&gt; &lt;p&gt;&amp;quot;&lt;/p&gt; &lt;p&gt;I tested the model through ollama run and it responds normally. Anyone else experiencing this problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/businessAlcoholCream"&gt; /u/businessAlcoholCream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nob7hx/calling_through_the_api_causes_the_model_to_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nob7hx/calling_through_the_api_causes_the_model_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nob7hx/calling_through_the_api_causes_the_model_to_be/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T07:43:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnwbhn</id>
    <title>Best open uncensored model for writing short stories?</title>
    <updated>2025-09-22T19:38:41+00:00</updated>
    <author>
      <name>/u/MassiveBoner911_3</name>
      <uri>https://old.reddit.com/user/MassiveBoner911_3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this has been asked before but the post was a few months old; figured id ask again since models come out faster every week.&lt;/p&gt; &lt;p&gt;Whats everyone using for their creative writing? Id like an open uncensored model thats great with creative and generating ideas. &lt;/p&gt; &lt;p&gt;I like writing dark / gory slasher horror. &lt;/p&gt; &lt;p&gt;OpenAI immediately tells me to “fuck off”. Gemini goes “absolutely not” Grok goes “here is all the things”….but id like to try others.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MassiveBoner911_3"&gt; /u/MassiveBoner911_3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnwbhn/best_open_uncensored_model_for_writing_short/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnwbhn/best_open_uncensored_model_for_writing_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnwbhn/best_open_uncensored_model_for_writing_short/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T19:38:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1no94pr</id>
    <title>I’ve been using old Xeon boxes (especially dual-socket setups) with heaps of RAM, and wanted to put together some thoughts + research that backs up why that setup is still quite viable.</title>
    <updated>2025-09-23T05:31:29+00:00</updated>
    <author>
      <name>/u/AggravatingGiraffe46</name>
      <uri>https://old.reddit.com/user/AggravatingGiraffe46</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;What makes old Xeons + lots of RAM still powerful&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Memory-heavy workloads:&lt;/strong&gt; Applications like in-memory databases, caching (Redis / Memcached), big Spark jobs, or large virtual machine setups benefit heavily from having &lt;em&gt;physical memory&lt;/em&gt; over disk or even SSD bottlenecks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parallelism over clock speed:&lt;/strong&gt; Xeons with many cores/threads, even if older, can still outperform modern CPUs in tasks where you can spread work well. If single-thread isn’t super critical, you get a lot of value.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Price/performance + amortization:&lt;/strong&gt; Used Xeon gear + cheap server RAM (especially ECC/registered) can deliver fractions of the cost of modern CPUs with relatively modest performance loss for many use-cases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reliability / durability:&lt;/strong&gt; Server parts are built for sustained loads, often with better cooling, ECC memory, etc., so done right the maintenance cost can be low.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here are some studies &amp;amp; posts that support various claims about using a lot of RAM, memory behavior, and what kinds of workloads benefit:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Source&lt;/th&gt; &lt;th align="left"&gt;What it shows / relevance&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;A Study of Virtual Memory Usage and Implications for Big-Memory Systems&lt;/em&gt; (UW, 2013)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://homes.cs.washington.edu/%7Eluisceze/publications/vmstudy-uwtr2013.pdf?utm_source=chatgpt.com"&gt;Homes at the University of Washington&lt;/a&gt;Examines how modern server + client applications make heavy use of RAM; shows that servers often have hundreds of GBs of physical memory and that “big-memory” usage is growing.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;The Case for RAMClouds: Scalable High-Performance Storage Entirely in DRAM&lt;/em&gt; (Ousterhout et al., PDF)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.cs.princeton.edu/courses/archive/spring13/cos598C/ramcloud.pdf?utm_source=chatgpt.com"&gt;Princeton CS&lt;/a&gt;Argues that keeping data in RAM (distributed across many machines) yields 100-1000× lower latency / much higher throughput vs disk-based systems. Good support for the idea that if you have big RAM you can do powerful stuff.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;A Comprehensive Memory Analysis of Data Intensive Applications&lt;/em&gt; (GMU, 2018)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://mason.gmu.edu/%7Espudukot/Files/Conferences/Mem_Subsys18.pdf?utm_source=chatgpt.com"&gt;Mason&lt;/a&gt;Shows how big data / Spark / MPI frameworks behave based on memory capacity, number of channels, etc. Points out that some applications benefit greatly from more memory, especially if they are iterative or aggregate large data in memory.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;Revisiting Memory Errors in Large-Scale Production Data Centers&lt;/em&gt; (Facebook / CMU)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://users.ece.cmu.edu/%7Eomutlu/pub/memory-errors-at-facebook_dsn15.pdf?utm_source=chatgpt.com"&gt;Carnegie Mellon University ECE&lt;/a&gt;Deals with reliability of DRAM in server fleets. Relevant if you’re using older RAM / many DIMMs — shows what kinds of error rates and what matters (ECC, controller, channel, DIMM quality).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;My Home Lab Server with 20 cores / 40 threads and 128 GB memory&lt;/em&gt; (blog post)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://louwrentius.com/my-home-lab-server-with-20-cores-40-threads-and-128-gb-memory.html?utm_source=chatgpt.com"&gt;louwrentius.com&lt;/a&gt;Real-world example: an older Xeon E5-2680 v2 machine, with 128 GB RAM, showing how usable performance still is despite age (VMs/containers) and decent multi-core scores.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Tradeoffs / what to watch out for&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Power draw and efficiency&lt;/strong&gt;: Old dual-Xeon boards + many DIMMs = higher idle power and higher heat. If running 24/7, electricity and cooling matter.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Single-thread / per core speed&lt;/strong&gt;: Newer CPUs typically have higher clock speeds, better IPC. For tasks that depend on those (e.g. UI responsiveness, some compiles, gaming), old Xeons may lag.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compatibility &amp;amp; spares&lt;/strong&gt;: Motherboard, ECC RAM, firmware updates, etc., can be harder/cheaper to source.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory reliability&lt;/strong&gt;: As DRAM ages and if ECC isn’t used, error rates go up. Also older DIMMs might be higher failure risk.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggravatingGiraffe46"&gt; /u/AggravatingGiraffe46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no94pr/ive_been_using_old_xeon_boxes_especially/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no94pr/ive_been_using_old_xeon_boxes_especially/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1no94pr/ive_been_using_old_xeon_boxes_especially/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T05:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nomrz9</id>
    <title>Best local models for RTX 4050?</title>
    <updated>2025-09-23T16:54:15+00:00</updated>
    <author>
      <name>/u/Grouchy-Ad1910</name>
      <uri>https://old.reddit.com/user/Grouchy-Ad1910</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grouchy-Ad1910"&gt; /u/Grouchy-Ad1910 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/selfhosted/comments/1nomfpr/best_local_models_for_rtx_4050/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nomrz9/best_local_models_for_rtx_4050/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nomrz9/best_local_models_for_rtx_4050/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T16:54:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nomwnd</id>
    <title>Performance Expectations? [AMD 7840HS / 780M]</title>
    <updated>2025-09-23T16:59:03+00:00</updated>
    <author>
      <name>/u/AftermarketMesomorph</name>
      <uri>https://old.reddit.com/user/AftermarketMesomorph</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: Do these results make sense, or is something misconfigured? The iGPU doesn't seem to give much benefit for me.&lt;/p&gt; &lt;p&gt;edit: Fixed formatting&lt;/p&gt; &lt;p&gt;I'm playing around with ollama on a Minisforum UM780 XTX machine and after some simple prompts, I'm not sure if there is any real benefit to using the iGPU over just the CPU. In fact, there's very little air between the two.&lt;/p&gt; &lt;h2&gt;Host config:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;CPU: 7840HS @ 54W&lt;/li&gt; &lt;li&gt;RAM: 32 GiB DDR5 5600 CL40-40-40-89 (G.SKILL F5-5600S4040A16GX2-RS)&lt;/li&gt; &lt;li&gt;GPU: 780M iGPU&lt;/li&gt; &lt;li&gt;OS: Ubuntu 24.04 LTS&lt;/li&gt; &lt;li&gt;VRAM: Set in BIOS to 16 GiB (max)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The most VRAM that can be set is 16 GiB, leaving 16 GiB for the OS.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# free -h total used free shared buff/cache available Mem: 15Gi 3.1Gi 9.7Gi 161Mi 3.1Gi 12Gi Swap: 8.0Gi 998Mi 7.0Gi &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have installed the latest AMD drivers and used the &lt;code&gt;curl | sh&lt;/code&gt; method to install ollama. In order to enable the iGPU with ROCm, I've run &lt;code&gt;systemctl edit ollama.service&lt;/code&gt; and added the following:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Service] Environment=&amp;quot;HSA_OVERRIDE_GFX_VERSION=11.0.0&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The service was then restarted with &lt;code&gt;systemctl restart ollama.service&lt;/code&gt;. &lt;/p&gt; &lt;p&gt;Disabling the iGPU is accomplished by commenting out the &lt;code&gt;Environment&lt;/code&gt; line and restarting the service.&lt;/p&gt; &lt;h2&gt;Model:&lt;/h2&gt; &lt;p&gt;I'm using &lt;code&gt;qwen3:latest&lt;/code&gt; - No particular reason, other than it fitting into VRAM. &lt;code&gt;qwen3:14b&lt;/code&gt; should fit, but winds up split between CPU and GPU.&lt;/p&gt; &lt;h2&gt;Prompting:&lt;/h2&gt; &lt;p&gt;In both CPU and GPU scenarios, I've issued the prompt from the command line rather than the readline interface. The model is loaded once before issuing prompts to reduce the impact on measurements.&lt;/p&gt; &lt;p&gt;The test is run using this script:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;#!/bin/sh -xe OLLAMA=/usr/local/bin/ollama MODEL=&amp;quot;qwen3:latest&amp;quot; PROMPT=&amp;quot;How much wood would a woodchuck chuck if a woodchuck could chuck wood?&amp;quot; # Pre-load model &amp;quot;${OLLAMA}&amp;quot; stop &amp;quot;${MODEL}&amp;quot; || true &amp;quot;${OLLAMA}&amp;quot; run --verbose --nowordwrap --keepalive 60m &amp;quot;${MODEL}&amp;quot; &amp;quot;&amp;quot; # Run 6 times and record output. The first run will be discarded. for run_num in $( seq 0 5 ); do OUT_FILE=&amp;quot;${PWD}/llm.out.${run_num}&amp;quot; &amp;quot;${OLLAMA}&amp;quot; ps 2&amp;gt;&amp;amp;1 | tee -a &amp;quot;${OUT_FILE}&amp;quot; &amp;quot;${OLLAMA}&amp;quot; run --verbose --nowordwrap --keepalive 60m &amp;quot;${MODEL}&amp;quot; &amp;quot;${PROMPT}&amp;quot; 2&amp;gt;&amp;amp;1 \ | tee -a &amp;quot;${OUT_FILE}&amp;quot; done &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Results:&lt;/h2&gt; &lt;p&gt;Each modality had a single outlier which affected the prompt evaluation rate. The GPU outlier was on the third run while the CPU outlier was on the first. I am not excluding these from the results since they appear to be genuine.&lt;/p&gt; &lt;p&gt;The CPU had an average prompt eval rate of 254.1 tokens/s and median of 294.4. The stddev was 110.899. The min rate was 46.83 token/s and the max was 298 token/s.&lt;/p&gt; &lt;p&gt;The average CPU response eval rate was 10.7 tokens/s, median of 10.6, and a stddev of 0.068. The number of response tokens ranged from 663 - 1263 with a mean of 896, median of 758, and stddev of 273.&lt;/p&gt; &lt;p&gt;The GPU had an average prompt eval rate of 4912.0 tokens/s and median of 5794.7. The stddev was 2597.075. The min rate was 341, max was 6622. The median was 5794 and the stddev was 2597.&lt;/p&gt; &lt;p&gt;The average CPU response eval rate was between 11.66 and 13.03 with an average of 12.6 tokens/s, median of 13.0, and a stddev of 0.590.&lt;/p&gt; &lt;p&gt;For the relatively simple prompt, the GPU gives a ~ 20% improvement for the response. Evaluating the prompt give ~ 2000% but the actual improvement is less than 1 second.&lt;/p&gt; &lt;p&gt;The response rate was only slightly improved by the GPU. 20% is nothing to sneeze at, but not revolutionary...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AftermarketMesomorph"&gt; /u/AftermarketMesomorph &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nomwnd/performance_expectations_amd_7840hs_780m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nomwnd/performance_expectations_amd_7840hs_780m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nomwnd/performance_expectations_amd_7840hs_780m/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T16:59:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1noijqp</id>
    <title>Best PHP Coding Model for 5060ti 16GB/128GB RAM</title>
    <updated>2025-09-23T14:14:28+00:00</updated>
    <author>
      <name>/u/vortec350</name>
      <uri>https://old.reddit.com/user/vortec350</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;sup&gt;that.&lt;/sup&gt; I’ve asked AI and googled and browsed this forum but most people care about JavaScript, not PHP haha. Thank you :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vortec350"&gt; /u/vortec350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1noijqp/best_php_coding_model_for_5060ti_16gb128gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1noijqp/best_php_coding_model_for_5060ti_16gb128gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1noijqp/best_php_coding_model_for_5060ti_16gb128gb_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T14:14:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1noqf65</id>
    <title>How accurate PrivateGPT is with your documents?</title>
    <updated>2025-09-23T19:11:15+00:00</updated>
    <author>
      <name>/u/Ok-Macaroon9817</name>
      <uri>https://old.reddit.com/user/Ok-Macaroon9817</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hello,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm interested in using PrivateGPT to conduct research across a large collection of documents. I’d like to know how accurate it is in practice. Has anyone here used it before and can share their experience?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Macaroon9817"&gt; /u/Ok-Macaroon9817 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1noqf65/how_accurate_privategpt_is_with_your_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1noqf65/how_accurate_privategpt_is_with_your_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1noqf65/how_accurate_privategpt_is_with_your_documents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T19:11:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nozp8t</id>
    <title>Ollama registering 44% CPU usage?</title>
    <updated>2025-09-24T01:44:03+00:00</updated>
    <author>
      <name>/u/IamLuckyy</name>
      <uri>https://old.reddit.com/user/IamLuckyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nozp8t/ollama_registering_44_cpu_usage/"&gt; &lt;img alt="Ollama registering 44% CPU usage?" src="https://b.thumbs.redditmedia.com/Mx4_JK-ld225wdsBzqAINYilnL-KzCWtW0bRfwjdBqU.jpg" title="Ollama registering 44% CPU usage?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/l1ya1zsio0rf1.png?width=875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa0a268034c61ddfdadce72b94d8a5c9581f830a"&gt;https://preview.redd.it/l1ya1zsio0rf1.png?width=875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa0a268034c61ddfdadce72b94d8a5c9581f830a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I used to run the same Mistral-Small3.2:24b model on a bare metal ubuntu server and would get 100% GPU usage (At least thats what I remember). Now I am running it through the Ollama TrueNAS app and it shows 44% CPU yet the model it seems to run the exact same. I thought maybe one of my GPU's was getting mistaked as a CPU since I only gave the app 2 cores and 4gb of ram since I had the two gpus. But when I run nvidia-smi they both show up as the Nvidia P102-100 so I'm not sure if Ollama actually is registering one of my GPU's as a CPU or not. I assume with the app CPU being limited to 2 Cores and 4gb of ram it would run horribly slow if that truly was the case.&lt;/p&gt; &lt;p&gt;FYI if I run gpt-oss:20b its runs perfectly fine and shows up as 100% gpu usage with a 14gb size under the Ollama ps command.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IamLuckyy"&gt; /u/IamLuckyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nozp8t/ollama_registering_44_cpu_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nozp8t/ollama_registering_44_cpu_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nozp8t/ollama_registering_44_cpu_usage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-24T01:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1no2b7d</id>
    <title>Most Dangerous Ollama Agent? Demo + Repo</title>
    <updated>2025-09-22T23:45:15+00:00</updated>
    <author>
      <name>/u/New_Pomegranate_1060</name>
      <uri>https://old.reddit.com/user/New_Pomegranate_1060</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1no2b7d/most_dangerous_ollama_agent_demo_repo/"&gt; &lt;img alt="Most Dangerous Ollama Agent? Demo + Repo" src="https://external-preview.redd.it/YWp4Y29jaDB6c3FmMXt94OWDuAWHQAvniQ96yay80ZGLpddsHnI1O_WJEL_6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=653779aec1642f70f934b334caddfdb4f0fe5ee2" title="Most Dangerous Ollama Agent? Demo + Repo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on an ollama agent I’m calling TermNet and it’s honestly kind of nuts. In the demo video I show it doing a bunch of stuff most agents probably shouldn’t be trusted with. It’s got full terminal access so it can run commands directly on my machine.&lt;/p&gt; &lt;p&gt;It doesn’t stop there. It pulls system info, makes directories and files, writes and executes programs (can do gui) browses the web, and scans my local network. None of it is scripted or staged either. The agent strings everything together on its own and gives me the results in plain language. It’s a strange mix of useful and dangerous, which is why I figured I’d share it here.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/RawdodReverend/TermNet"&gt;https://github.com/RawdodReverend/TermNet&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TikTok: &lt;a href="https://www.tiktok.com/@rawdogreverend"&gt;https://www.tiktok.com/@rawdogreverend&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone decides to try it, I’d highly recommend running it in a VM or sandbox. It has full access to the system, so don’t point it at anything you care about.&lt;/p&gt; &lt;p&gt;Not trying to make this into some big “AI safety” post, just showing off what I’ve been playing with. But after seeing it chain commands and spin up code on the fly, I think it might be one of the more dangerous ollama agents out there right now. Curious what people here think and if anyone else has pushed agents this far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Pomegranate_1060"&gt; /u/New_Pomegranate_1060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ff0w6fm0zsqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no2b7d/most_dangerous_ollama_agent_demo_repo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1no2b7d/most_dangerous_ollama_agent_demo_repo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T23:45:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1np1ubc</id>
    <title>Revolutionary</title>
    <updated>2025-09-24T03:29:08+00:00</updated>
    <author>
      <name>/u/ReviewDazzling9105</name>
      <uri>https://old.reddit.com/user/ReviewDazzling9105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1np1ubc/revolutionary/"&gt; &lt;img alt="Revolutionary" src="https://a.thumbs.redditmedia.com/1j7-hIlhJWAdUYS64t1WL9gfmDUd1jRdbRlRo8H6o18.jpg" title="Revolutionary" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gr2azbhy71rf1.png?width=1648&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c3447be84ceb7ed757b6d0638b485694588f81e"&gt;https://preview.redd.it/gr2azbhy71rf1.png?width=1648&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c3447be84ceb7ed757b6d0638b485694588f81e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Running ollama using openwebui on a pop-os workstation with RTXA2000 I7-7700 with 32gb of ram&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReviewDazzling9105"&gt; /u/ReviewDazzling9105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1np1ubc/revolutionary/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1np1ubc/revolutionary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1np1ubc/revolutionary/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-24T03:29:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nokzx7</id>
    <title>Computer Use on Windows Sandbox</title>
    <updated>2025-09-23T15:47:59+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nokzx7/computer_use_on_windows_sandbox/"&gt; &lt;img alt="Computer Use on Windows Sandbox" src="https://external-preview.redd.it/NGVuamVvaXpxeHFmMYar2P-d3EU8x2ju_uKYrB4yrb0aAUxLp4mH5szJsZ9M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db4b778804fd2edc20bde5e8cf8b999696f4b392" title="Computer Use on Windows Sandbox" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing Windows Sandbox support - run computer-use agents on Windows business apps without VMs or cloud costs.&lt;/p&gt; &lt;p&gt;Your enterprise software runs on Windows, but testing agents required expensive cloud instances. Windows Sandbox changes this - it's Microsoft's built-in lightweight virtualization sitting on every Windows 10/11 machine, ready for instant agent development.&lt;/p&gt; &lt;p&gt;Enterprise customers kept asking for AutoCAD automation, SAP integration, and legacy Windows software support. Traditional VM testing was slow and resource-heavy. Windows Sandbox solves this with disposable, seconds-to-boot Windows environments for safe agent testing.&lt;/p&gt; &lt;p&gt;What you can build: AutoCAD drawing automation, SAP workflow processing, Bloomberg terminal trading bots, manufacturing execution system integration, or any Windows-only enterprise software automation - all tested safely in disposable sandbox environments.&lt;/p&gt; &lt;p&gt;Free with Windows 10/11, boots in seconds, completely disposable. Perfect for development and testing before deploying to Windows cloud instances (coming later this month).&lt;/p&gt; &lt;p&gt;Check out the github here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/windows-sandbox"&gt;https://www.trycua.com/blog/windows-sandbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ibs8dytzqxqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nokzx7/computer_use_on_windows_sandbox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nokzx7/computer_use_on_windows_sandbox/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T15:47:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1npbruw</id>
    <title>iPhone app for voice recording and AI processing</title>
    <updated>2025-09-24T13:12:18+00:00</updated>
    <author>
      <name>/u/Altruistic_Call_3023</name>
      <uri>https://old.reddit.com/user/Altruistic_Call_3023</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic_Call_3023"&gt; /u/Altruistic_Call_3023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1npbrcw/iphone_app_for_voice_recording_and_ai_processing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npbruw/iphone_app_for_voice_recording_and_ai_processing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1npbruw/iphone_app_for_voice_recording_and_ai_processing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-24T13:12:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1npq4kb</id>
    <title>Any recommended small and snappy (but not dumb) models for a budget GPU?</title>
    <updated>2025-09-24T22:31:10+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got an unRAID server with an Intel Arc A380 GPU. So, in order to be able to use my non-NVIDIA GPU, I'm running Intel’s IPEX‑LLM Ollama container and accessing the models through Open WebUI.&lt;/p&gt; &lt;p&gt;I want to know what small and snappy, but not stupid, models you'd recommend for simple tasks? Right now I'm just experimenting, but we'll see how I'd like to expand in the future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npq4kb/any_recommended_small_and_snappy_but_not_dumb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npq4kb/any_recommended_small_and_snappy_but_not_dumb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1npq4kb/any_recommended_small_and_snappy_but_not_dumb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-24T22:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1npqhis</id>
    <title>Qwen3-embedding, how to set dimensionality?</title>
    <updated>2025-09-24T22:46:53+00:00</updated>
    <author>
      <name>/u/vredditt</name>
      <uri>https://old.reddit.com/user/vredditt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All 3 qwen3-embedding models seem to work great. However, I would very much like to compare results with different dimensions other than their respective maximum (1k, 2k, 4k dim respectively for 0.6b, 4b and 8b). &lt;/p&gt; &lt;p&gt;Did anyone succeed in finding the right parameter for that? &amp;quot;dimentions&amp;quot;: 512, as well as &amp;quot;dim&amp;quot;, &amp;quot;emd_dim&amp;quot; or options -&amp;gt; &amp;quot;dimentions&amp;quot; etc. do nothing. I didn't find anything in both, the ollama API reference and the model's description except a textual reference to the fact that setting users dimension is supported (from 32 dim to max).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vredditt"&gt; /u/vredditt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npqhis/qwen3embedding_how_to_set_dimensionality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npqhis/qwen3embedding_how_to_set_dimensionality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1npqhis/qwen3embedding_how_to_set_dimensionality/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-24T22:46:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1npx76n</id>
    <title>analyze a pdf for content and structure/design</title>
    <updated>2025-09-25T04:12:45+00:00</updated>
    <author>
      <name>/u/fttklr</name>
      <uri>https://old.reddit.com/user/fttklr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if it is better to use a LLM with vision capacities or something else like ConfyUI, so I thought to ask here.&lt;/p&gt; &lt;p&gt;I would like to extract from documents (mostly PDF or word); the content of each page. The problem is that I want to get the images and the text, and get the way in which the text is arranged with the images (so the design/structure of each page basically).&lt;/p&gt; &lt;p&gt;The final result is to restore some old documents without actually scan them all and use OCR and then re-create the existing layout and text. So anything that can help me with this task would be really appreciated&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fttklr"&gt; /u/fttklr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npx76n/analyze_a_pdf_for_content_and_structuredesign/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npx76n/analyze_a_pdf_for_content_and_structuredesign/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1npx76n/analyze_a_pdf_for_content_and_structuredesign/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T04:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1npo4jh</id>
    <title>Orchestrate multiple Ollama models to do complex stuff with the automatic Multi-Agent Builder using Observer! (Free and Open Source)</title>
    <updated>2025-09-24T21:08:53+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1npo4jh/orchestrate_multiple_ollama_models_to_do_complex/"&gt; &lt;img alt="Orchestrate multiple Ollama models to do complex stuff with the automatic Multi-Agent Builder using Observer! (Free and Open Source)" src="https://external-preview.redd.it/56SPQExPI827GpA98kpipwMJSEu04uBeqWtJrHoG4nc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5567fa8755a2e8bb9fdc634d1c9e2f0436b0e05a" title="Orchestrate multiple Ollama models to do complex stuff with the automatic Multi-Agent Builder using Observer! (Free and Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR; This new Automatic Multi-Agent creator and editor makes Observer super super powerful. You can create multiple agents automatically and iterate System Prompts to get your local agents working really fast!&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;, &lt;/p&gt; &lt;p&gt;Ever since i started using Ollama i've thought about this exact use case for local models. Using vision + reasoning models to do more advanced things, like guiding you while creating a Google account! &lt;/p&gt; &lt;p&gt;Last time i showed you guys how to create them manually using Observer to solve LeetCode problems on screen, but now the Agent Builder can create them automatically!! And better yet, if a model is hallucinating or not triggering your notifications correctly, you just click one button and the Agent Builder can fix it for you. &lt;/p&gt; &lt;p&gt;This lets you have some agents that do the following: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Monitor &amp;amp; Document&lt;/strong&gt; - One agent describes your screen, another keeps a document of the process.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extract &amp;amp; Solve&lt;/strong&gt; - One agent extracts problems from the screen, another solves them.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Watch &amp;amp; Guide&lt;/strong&gt; - One agent lists out possible buttons or actions, another provides step-by-step guidance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Of course you can still have simple one-agent configs to get notifications when downloads finish, renders complete, something happens on a video game etc. etc. Everything using your local Ollama models! &lt;/p&gt; &lt;p&gt;You can download the app and look at the code right here: &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Or try it out without any install (non-local but easy): &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Thanks to the Ollama team for making this type of App possible! I hope this App makes more people interested in local models and their possible uses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=6zJh8NmCXYw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npo4jh/orchestrate_multiple_ollama_models_to_do_complex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1npo4jh/orchestrate_multiple_ollama_models_to_do_complex/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-24T21:08:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1npwqw0</id>
    <title>local computer vision on webcam</title>
    <updated>2025-09-25T03:48:43+00:00</updated>
    <author>
      <name>/u/faflappy</name>
      <uri>https://old.reddit.com/user/faflappy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1npwqw0/local_computer_vision_on_webcam/"&gt; &lt;img alt="local computer vision on webcam" src="https://external-preview.redd.it/Ypd2kQ-k8wRi8W0sQ8OMojLcTwuapPuL7em0lGxtBK4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64d37f8896fb78027d65fe7720961fbc08448972" title="local computer vision on webcam" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i made a local object detection and identification script that uses yolo, sam, and ollama vlm models. it runs on the webcam with ~30fps on my laptop. &lt;/p&gt; &lt;p&gt;two versions:&lt;br /&gt; 1. YOLO/SAM object detection and tracking with vlm object tagging&lt;/p&gt; &lt;ol&gt; &lt;li&gt;motion detection with vlm descriptions of the entire frame&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;still new to computer vision systems so very open to feedback and advice&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/faflappy"&gt; /u/faflappy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kazumah1/local-detection"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npwqw0/local_computer_vision_on_webcam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1npwqw0/local_computer_vision_on_webcam/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T03:48:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nph8mq</id>
    <title>using ollama&amp;gemini with comfyui</title>
    <updated>2025-09-24T16:44:40+00:00</updated>
    <author>
      <name>/u/Far-Entertainer6755</name>
      <uri>https://old.reddit.com/user/Far-Entertainer6755</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nph8mq/using_ollamagemini_with_comfyui/"&gt; &lt;img alt="using ollama&amp;amp;gemini with comfyui" src="https://external-preview.redd.it/YTdiczBwOTA2NXJmMSzTzzAWBhaeIDAUXYuWZ8SFYObdBmY6Opua2srAH4iY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7eac2af0758ab1ebbccbe4247c0b7b73152d0740" title="using ollama&amp;amp;gemini with comfyui" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;📌 ComfyUI-OllamaGemini – Run Ollama inside ComfyUI&lt;/h1&gt; &lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I’ve put together a &lt;strong&gt;ComfyUI custom node&lt;/strong&gt; that integrates directly with &lt;strong&gt;Ollama&lt;/strong&gt; so you can use your local LLMs inside ComfyUI workflows.&lt;/p&gt; &lt;p&gt;👉 GitHub: &lt;a href="https://github.com/al-swaiti/ComfyUI-OllamaGemini"&gt;ComfyUI-OllamaGemini&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;🔹 Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Use any Ollama model (Llama 3, Mistral, Gemma, etc.) inside ComfyUI&lt;/li&gt; &lt;li&gt;Combine text generation with image and video workflows&lt;/li&gt; &lt;li&gt;Build multimodal pipelines (reasoning → prompts → visuals)&lt;/li&gt; &lt;li&gt;Keep everything &lt;strong&gt;local and private&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔹 Installation&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;cd ComfyUI/custom_nodes git clone https://github.com/al-swaiti/ComfyUI-OllamaGemini.git &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Entertainer6755"&gt; /u/Far-Entertainer6755 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/27ulvq9065rf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nph8mq/using_ollamagemini_with_comfyui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nph8mq/using_ollamagemini_with_comfyui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-24T16:44:40+00:00</published>
  </entry>
</feed>
