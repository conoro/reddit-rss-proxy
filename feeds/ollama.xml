<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-10-28T20:49:51+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1oef0f7</id>
    <title>I created a canvas that integrates with Ollama.</title>
    <updated>2025-10-23T20:58:13+00:00</updated>
    <author>
      <name>/u/Financial_Click9119</name>
      <uri>https://old.reddit.com/user/Financial_Click9119</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oef0f7/i_created_a_canvas_that_integrates_with_ollama/"&gt; &lt;img alt="I created a canvas that integrates with Ollama." src="https://external-preview.redd.it/OG16bGVyNjlkeHdmMVL2umS07UZ7yNIEoPIe0Z1Sv93ClYn_60KRbocXKZ2V.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b937176ed8334ecf074201d5639af66118bb255" title="I created a canvas that integrates with Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got my dissertation and major exams coming up, and I was struggling to keep up.&lt;/p&gt; &lt;p&gt;Jumped from Notion to Obsidian and decided to build what I needed myself.&lt;/p&gt; &lt;p&gt;If you would like a canvas to mind map and break down complex ideas, give it a spin.&lt;/p&gt; &lt;p&gt;Website: &lt;a href="http://notare.uk"&gt;notare.uk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Future plans:&lt;br /&gt; - Templates&lt;br /&gt; - Note editor&lt;br /&gt; - Note Grouping&lt;/p&gt; &lt;p&gt;I would love some community feedback about the project. Feel free to reach out with questions or issues, send me a DM.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; Ollama Mistral is used on local host.&lt;br /&gt; While Mistral API is used for the web version. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial_Click9119"&gt; /u/Financial_Click9119 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9p5zdq69dxwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oef0f7/i_created_a_canvas_that_integrates_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oef0f7/i_created_a_canvas_that_integrates_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T20:58:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oexm94</id>
    <title>Taking Control of LLM Observability for the better App Experience, the OpenSource Way</title>
    <updated>2025-10-24T13:22:10+00:00</updated>
    <author>
      <name>/u/Silent_Employment966</name>
      <uri>https://old.reddit.com/user/Silent_Employment966</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My AI app has multiple parts - RAG retrieval, embeddings, agent chains, tool calls. Users started complaining about slow responses, weird answers, and occasional errors. But which part was broken was getting difficult to point out for me as a solo dev The vector search? A bad prompt? Token limits?.&lt;/p&gt; &lt;p&gt;A week ago, I was debugging by adding print statements everywhere and hoping for the best. Realized I needed actual LLM observability instead of relying on logs that show nothing useful.&lt;/p&gt; &lt;p&gt;Started using Langfuse(openSource). Now I see the complete flow= which documents got retrieved, what prompt went to the LLM, exact token counts, latency per step, costs per user. The &lt;code&gt;@observe()&lt;/code&gt; decorator traces everything automatically.&lt;/p&gt; &lt;p&gt;Also added AnannasAI as my gateway one API for 500+ models (OpenAI, Anthropic, Mistral). If a provider fails, it auto-switches. No more managing multiple SDKs.&lt;/p&gt; &lt;p&gt;it gets dual layer observability, Anannas tracks gateway metrics, Langfuse captures your application traces and debugging flow, Full visibility from model selection to production executions&lt;/p&gt; &lt;p&gt;The user experience improved because I could finally see what was actually happening and fix the real issues. it can be easily with integrated here's the Langfuse &lt;a href="https://langfuse.com/integrations/gateways/anannas"&gt;guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You can &lt;strong&gt;self host&lt;/strong&gt; the &lt;strong&gt;Langfuse&lt;/strong&gt; as well. so total Data under your Control.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silent_Employment966"&gt; /u/Silent_Employment966 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oexm94/taking_control_of_llm_observability_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oexm94/taking_control_of_llm_observability_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oexm94/taking_control_of_llm_observability_for_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-24T13:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofafxq</id>
    <title>how can i remove chinese censorship from qwen3 ?</title>
    <updated>2025-10-24T21:47:47+00:00</updated>
    <author>
      <name>/u/nico721GD</name>
      <uri>https://old.reddit.com/user/nico721GD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im running qwen3 4b on my ollama + open webui + searxng setup but i cant manage to remove the chinese propaganda from its brain, it got lobotomised too much for it to work, is there tips or whatnot to make it work properly ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nico721GD"&gt; /u/nico721GD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofafxq/how_can_i_remove_chinese_censorship_from_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofafxq/how_can_i_remove_chinese_censorship_from_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ofafxq/how_can_i_remove_chinese_censorship_from_qwen3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-24T21:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1of8dlu</id>
    <title>Offline first coding agent on your terminal</title>
    <updated>2025-10-24T20:22:14+00:00</updated>
    <author>
      <name>/u/Sea-Reception-2697</name>
      <uri>https://old.reddit.com/user/Sea-Reception-2697</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1of8dlu/offline_first_coding_agent_on_your_terminal/"&gt; &lt;img alt="Offline first coding agent on your terminal" src="https://external-preview.redd.it/MTh0Zzk1bDVjNHhmMWeU_7mDURBNP1Sda2-z7G3nNT97YS2GcY0AFck-U5_D.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2989b99e6557c38766a1542941cb6654fc7a5c41" title="Offline first coding agent on your terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those running local AI models with ollama&lt;br /&gt; you can use the Xandai CLI tool to create and edit code directly from your terminal.&lt;/p&gt; &lt;p&gt;It also supports natural language commands, so if you don’t remember a specific command, you can simply ask Xandai to do it for you. For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;List the 50 largest files on my system. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Install it easily with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install xandai-cli &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Github repo: &lt;a href="https://github.com/XandAI-project/Xandai-CLI"&gt;https://github.com/XandAI-project/Xandai-CLI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Reception-2697"&gt; /u/Sea-Reception-2697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t6fx65l5c4xf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1of8dlu/offline_first_coding_agent_on_your_terminal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1of8dlu/offline_first_coding_agent_on_your_terminal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-24T20:22:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofuk5o</id>
    <title>Ollama - I’m trying to learn to help it learn</title>
    <updated>2025-10-25T15:35:35+00:00</updated>
    <author>
      <name>/u/Punnalackakememumu</name>
      <uri>https://old.reddit.com/user/Punnalackakememumu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been toying around with Ollama for about a week now at home on an HP desktop running Linux Mint with 16 GB of RAM and an Intel i5 processor but no GPU support.&lt;/p&gt; &lt;p&gt;Upon learning that my employer is setting up an internal AI solution, as an IT guy I felt it was a good idea to learn how to handle the administration side of AI to help me with jobs in the future. &lt;/p&gt; &lt;p&gt;I have gotten it running a couple of times with wipes and reloads in slightly different configurations using different models to test out its ability to adjust to the questions that I might be asking it in a work situation. &lt;/p&gt; &lt;p&gt;I do find myself a bit confused about how companies implement AI in order for it to assist them in creating job proposals and things of that nature because I assume they would have to be able to upload old proposals in .DOCX or .PDF formats for the AI to learn.&lt;/p&gt; &lt;p&gt;Based on my research, in order to have Ollama do that you need something like Haystack or Rasa so you can feed it documents for it to integrate into its “learning.”&lt;/p&gt; &lt;p&gt;I’d appreciate any pointers to a mid-level geek (a novice Linux guy) on how to do that. &lt;/p&gt; &lt;p&gt;In implementing Haystack in a venv, the advice I got during the Haystack installation was to use the [all] option for loading it and it never wanted to complete the installation, even though the SSD had plenty of free space.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Punnalackakememumu"&gt; /u/Punnalackakememumu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofuk5o/ollama_im_trying_to_learn_to_help_it_learn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofuk5o/ollama_im_trying_to_learn_to_help_it_learn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ofuk5o/ollama_im_trying_to_learn_to_help_it_learn/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T15:35:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofjp6u</id>
    <title>What is the simplest way to set up a model on ollama to be able to search the internet?</title>
    <updated>2025-10-25T05:40:16+00:00</updated>
    <author>
      <name>/u/StarfireNebula</name>
      <uri>https://old.reddit.com/user/StarfireNebula</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running several models in ollama on Ubuntu with Open WebUI including Deepseek, LLama3, and Qwen3.&lt;/p&gt; &lt;p&gt;I've been running in circles figuring out how to set this up to use tools and search the internet in response to my prompts. How do I do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StarfireNebula"&gt; /u/StarfireNebula &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofjp6u/what_is_the_simplest_way_to_set_up_a_model_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofjp6u/what_is_the_simplest_way_to_set_up_a_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ofjp6u/what_is_the_simplest_way_to_set_up_a_model_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T05:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofxora</id>
    <title>AI but at what price?🏷️</title>
    <updated>2025-10-25T17:41:25+00:00</updated>
    <author>
      <name>/u/Maleficent-Hotel8207</name>
      <uri>https://old.reddit.com/user/Maleficent-Hotel8207</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which components/PC should I get for 600€? &lt;/p&gt; &lt;p&gt;I have to wait for a MAC mini M5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Hotel8207"&gt; /u/Maleficent-Hotel8207 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofxora/ai_but_at_what_price/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofxora/ai_but_at_what_price/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ofxora/ai_but_at_what_price/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T17:41:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofdckn</id>
    <title>Why LLMs are getting smaller in size?</title>
    <updated>2025-10-25T00:01:10+00:00</updated>
    <author>
      <name>/u/Hedgehog_Dapper</name>
      <uri>https://old.reddit.com/user/Hedgehog_Dapper</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have noticed the LLM models are getting smaller in terms of parameter size. Is it because of computing resources or better performance? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hedgehog_Dapper"&gt; /u/Hedgehog_Dapper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofdckn/why_llms_are_getting_smaller_in_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofdckn/why_llms_are_getting_smaller_in_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ofdckn/why_llms_are_getting_smaller_in_size/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T00:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1og4c2q</id>
    <title>Running ollama with whisper.</title>
    <updated>2025-10-25T22:19:13+00:00</updated>
    <author>
      <name>/u/grandpasam</name>
      <uri>https://old.reddit.com/user/grandpasam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a server with a couple GPUs on it. I've been running some ollama models on it for quite a while and have been enjoying it. Now I want to leverage some of this with my home assistant. The first thing I want to do is install a whisper docker on my AI server but when I get it running it takes up a whole GPU even with Idle. Is there a way I can lazy load whisper so that it loads up only when I send in a request?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grandpasam"&gt; /u/grandpasam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og4c2q/running_ollama_with_whisper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og4c2q/running_ollama_with_whisper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1og4c2q/running_ollama_with_whisper/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T22:19:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofizyc</id>
    <title>Batch GUI for Ollama</title>
    <updated>2025-10-25T04:58:57+00:00</updated>
    <author>
      <name>/u/jankovize</name>
      <uri>https://old.reddit.com/user/jankovize</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ofizyc/batch_gui_for_ollama/"&gt; &lt;img alt="Batch GUI for Ollama" src="https://external-preview.redd.it/6vhwNsa9NjheSalaNogjzsNd-_GQSzhvCEveT_DxzlA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13b53c618d24240f556cdf191d96cebcd45e051b" title="Batch GUI for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a free GUI for Ollama that enables batching large files in. Primary use is translation and text processing. There are presets and everything is customizable through a json.&lt;/p&gt; &lt;p&gt;You can get it here: &lt;a href="https://github.com/hclivess/ollama-batch-processor"&gt;https://github.com/hclivess/ollama-batch-processor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vg9cuc2aw6xf1.png?width=1645&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=663612955c5a484feddc3da0756f00755311297a"&gt;https://preview.redd.it/vg9cuc2aw6xf1.png?width=1645&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=663612955c5a484feddc3da0756f00755311297a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jankovize"&gt; /u/jankovize &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofizyc/batch_gui_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofizyc/batch_gui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ofizyc/batch_gui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T04:58:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1og28sp</id>
    <title>Exploring Embedding Support in Ollama Cloud</title>
    <updated>2025-10-25T20:48:16+00:00</updated>
    <author>
      <name>/u/CertainTime5947</name>
      <uri>https://old.reddit.com/user/CertainTime5947</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using Ollama Cloud, and I really love it! I’d like to ask — is there any possibility to add embedding support into Ollama Cloud as well?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CertainTime5947"&gt; /u/CertainTime5947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og28sp/exploring_embedding_support_in_ollama_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og28sp/exploring_embedding_support_in_ollama_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1og28sp/exploring_embedding_support_in_ollama_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T20:48:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1og5uul</id>
    <title>playing with coding models pt2</title>
    <updated>2025-10-25T23:30:20+00:00</updated>
    <author>
      <name>/u/Western_Courage_6563</name>
      <uri>https://old.reddit.com/user/Western_Courage_6563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the second round, we dramatically increased the complexity to test a model's true &amp;quot;understanding&amp;quot; of a codebase. The task was no longer a simple feature addition but a complex, multi-file refactoring operation.&lt;/p&gt; &lt;p&gt;The goal? To see if an LLM can distinguish between &lt;em&gt;essential&lt;/em&gt; logic and &lt;em&gt;non-essential&lt;/em&gt; dependencies. Can it understand not just &lt;em&gt;what&lt;/em&gt; the code does, but &lt;em&gt;why&lt;/em&gt;?&lt;/p&gt; &lt;h1&gt;The Testbed: Hardware and Software&lt;/h1&gt; &lt;p&gt;The setup remained consistent, running on a system with 24GB of VRAM:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; NVIDIA Tesla P40&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; Ollama&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; We tested a new batch of 10 models, including &lt;code&gt;phi4-reasoning&lt;/code&gt;, &lt;code&gt;magistral&lt;/code&gt;, multiple &lt;code&gt;qwen&lt;/code&gt; coders, &lt;code&gt;deepseek-r1&lt;/code&gt;, &lt;code&gt;devstral&lt;/code&gt;, and &lt;code&gt;mistral-small&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Challenge: A Devious Refactor&lt;/h1&gt; &lt;p&gt;This time, the models were given a three-file application:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;main.py&lt;/code&gt;**:** The &amp;quot;brain.&amp;quot; This file contained the &lt;code&gt;CodingAgentV2&lt;/code&gt; class, which holds the core self-correction loop. This loop generates code, generates tests, runs tests, and—if they fail—uses an &lt;code&gt;_analyze_test_failure&lt;/code&gt; method to determine &lt;em&gt;why&lt;/em&gt; and then branch to either debug the code or regenerate the tests.&lt;/li&gt; &lt;li&gt;&lt;code&gt;project_manager.py&lt;/code&gt;**:** The &amp;quot;sandbox.&amp;quot; A utility class to create a safe, temporary directory for executing the generated code and tests.&lt;/li&gt; &lt;li&gt;&lt;code&gt;conversation_manager.py&lt;/code&gt;**:** The &amp;quot;memory.&amp;quot; A database handler using SQLite and ChromaDB to save the history of successful and failed coding attempts.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The prompt was a common (and tricky) request:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;hey, i have this app, could you please simplify it, let's remove the database stuff altogether, and lets try to fit it in single file script, please.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;The Criteria for Success&lt;/h1&gt; &lt;p&gt;This prompt is a minefield. A &amp;quot;successful&amp;quot; model had to perform three distinct operations, in order of difficulty:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Structural Merge (Easy):&lt;/strong&gt; Combine the classes from &lt;code&gt;project_manager.py&lt;/code&gt; and &lt;a href="http://main.py"&gt;&lt;code&gt;main.py&lt;/code&gt;&lt;/a&gt; into a single file.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Surgical Removal (Medium):&lt;/strong&gt; Identify and completely remove the &lt;code&gt;ConversationManager&lt;/code&gt; class, all its database-related imports (&lt;code&gt;sqlite3&lt;/code&gt;, &lt;code&gt;langchain&lt;/code&gt;), and all calls to it (e.g., &lt;code&gt;save_successful_code&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Functional Preservation (Hard):&lt;/strong&gt; This is the real test. The model &lt;em&gt;must&lt;/em&gt; understand that the self-correction loop (the &lt;code&gt;_analyze_test_failure&lt;/code&gt; method and its &lt;code&gt;code_bug&lt;/code&gt;/&lt;code&gt;test_bug&lt;/code&gt; logic) is the &lt;em&gt;entire point&lt;/em&gt; of the application and must be preserved perfectly, even while removing the database logic it was once connected to.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;The Results: Surgeons, Butchers, and The Confused&lt;/h1&gt; &lt;p&gt;The models' attempts fell into three clear categories.&lt;/p&gt; &lt;h1&gt;Category 1: Flawless Victory (The &amp;quot;Surgeons&amp;quot;)&lt;/h1&gt; &lt;p&gt;These models demonstrated a true understanding of the code's &lt;em&gt;purpose&lt;/em&gt;. They successfully merged the files, surgically removed the database dependency, and—most importantly—left the agent's self-correction &amp;quot;brain&amp;quot; 100% intact.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Winners:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;phi4-reasoning:14b-plus-q8_0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;magistral:latest&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen2_5-coder:32b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;mistral-small:24b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3-coder:latest&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Code Example (The &amp;quot;Preserved Brain&amp;quot; from&lt;/strong&gt; &lt;code&gt;phi4-reasoning&lt;/code&gt;**):** This is what success looks like. The &lt;code&gt;ConversationManager&lt;/code&gt; is gone, but the &lt;em&gt;essential&lt;/em&gt; logic is perfectly preserved.&lt;/p&gt; &lt;p&gt;Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# ... (inside execute_coding_agent_v2) ... else: print(f&amp;quot; -&amp;gt; [CodingAgentV2] Tests failed on attempt {attempt + 1}. Analyzing failure...&amp;quot;) test_output = stdout + stderr # --- THIS IS THE CRITICAL LOGIC --- analysis_result = self._analyze_test_failure(generated_code, test_output) # print(f&amp;quot; -&amp;gt; [CodingAgentV2] Analysis result: '{analysis_result}'&amp;quot;) if analysis_result == 'code_bug' and attempt &amp;lt; MAX_DEBUG_ATTEMPTS: # print(&amp;quot; -&amp;gt; [CodingAgentV2] Identified as a code bug. Attempting to debug...&amp;quot;) generated_code = self._debug_code(generated_code, test_output, test_file) # self.project_manager.write_file(code_file, generated_code) elif analysis_result == 'test_bug' and attempt &amp;lt; MAX_TEST_REGEN_ATTEMPTS: # print(&amp;quot; -&amp;gt; [CodingAgentV2] Identified as a test bug. Regenerating tests...&amp;quot;) # Loop will try again with new unit tests continue # else: print(&amp;quot; -&amp;gt; [CodingAgentV2] Cannot determine cause or max attempts reached. Stopping.&amp;quot;) break # &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Category 2: Partial Failures (The &amp;quot;Butchers&amp;quot;)&lt;/h1&gt; &lt;p&gt;These models failed on a critical detail. They either misunderstood the prompt or &amp;quot;simplified&amp;quot; the code by destroying its most important feature.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;deepseek-r1:32b.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; &lt;strong&gt;Broke the agent's brain.&lt;/strong&gt; This model's failure was subtle but devastating. It correctly merged and removed the database, but in its quest to &amp;quot;simplify,&amp;quot; it &lt;em&gt;deleted the entire&lt;/em&gt; &lt;code&gt;_analyze_test_failure&lt;/code&gt; &lt;em&gt;method and self-correction loop&lt;/em&gt;. It turned the intelligent agent into a dumb script that gives up on the first error.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code Example (The &amp;quot;Broken Brain&amp;quot;):&lt;/strong&gt; Python# ... (inside execute_coding_agent_v2) ... for attempt in range(MAX_DEBUG_ATTEMPTS + MAX_TEST_REGEN_ATTEMPTS): # print(f&amp;quot;Starting test attempt {attempt + 1}...&amp;quot;) generated_tests = self._generate_unit_tests(code_file, generated_code, test_plan) # self.project_manager.write_file(test_file, generated_tests) # stdout, stderr, returncode = self.project_manager.run_command(['pytest', '-q', '--tb=no', test_file]) # if returncode == 0: # print(f&amp;quot;Tests passed successfully on attempt {attempt + 1}.&amp;quot;) test_passed = True break # # --- IT GIVES UP! NO ANALYSIS, NO DEBUGGING ---&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;gpt-oss:latest.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; Ignored the &amp;quot;remove&amp;quot; instruction. Instead of deleting the &lt;code&gt;ConversationManager&lt;/code&gt;, it &amp;quot;simplified&amp;quot; it into an in-memory class. This adds pointless code and fails the prompt's main constraint.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3:30b-a3b.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; Introduced a fatal bug. It had a great idea (replacing &lt;code&gt;ProjectManager&lt;/code&gt; with &lt;code&gt;tempfile&lt;/code&gt;), but fumbled the execution by incorrectly calling &lt;a href="http://subprocess.run"&gt;&lt;code&gt;subprocess.run&lt;/code&gt;&lt;/a&gt; twice for &lt;code&gt;stdout&lt;/code&gt; and &lt;code&gt;stderr&lt;/code&gt;, which would crash at runtime.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Category 3: Total Failures (The &amp;quot;Confused&amp;quot;)&lt;/h1&gt; &lt;p&gt;These models failed at the most basic level.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;devstral:latest.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; Destroyed the agent. This model &lt;em&gt;massively&lt;/em&gt; oversimplified. It deleted the &lt;code&gt;ProjectManager&lt;/code&gt;, the test plan generation, the debug loop, and the &lt;code&gt;_analyze_test_failure&lt;/code&gt; method. It turned the agent into a single &lt;code&gt;os.popen&lt;/code&gt; call, rendering it useless.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;granite4:small-h.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; Incomplete merge. It removed the &lt;code&gt;ConversationManager&lt;/code&gt; but &lt;strong&gt;forgot to merge in the&lt;/strong&gt; &lt;code&gt;ProjectManager&lt;/code&gt; &lt;strong&gt;class&lt;/strong&gt;. The resulting script is broken and would crash immediately.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Analysis &amp;amp; Takeaways&lt;/h1&gt; &lt;p&gt;This experiment was a much better filter for &amp;quot;intelligence.&amp;quot;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Purpose&amp;quot; vs. &amp;quot;Pattern&amp;quot; is the Real Test:&lt;/strong&gt; The winning models (&lt;code&gt;phi4&lt;/code&gt;, &lt;code&gt;magistral&lt;/code&gt;, &lt;code&gt;qwen2_5-coder&lt;/code&gt;, &lt;code&gt;mistral-small&lt;/code&gt;, &lt;code&gt;qwen3-coder&lt;/code&gt;) understood the &lt;em&gt;purpose&lt;/em&gt; of the code (self-correction) and protected it. The failing models (&lt;code&gt;deepseek-r1&lt;/code&gt;, &lt;code&gt;devstral&lt;/code&gt;) only saw a &lt;em&gt;pattern&lt;/em&gt; (&amp;quot;simplify&amp;quot; = &amp;quot;delete complex-looking code&amp;quot;) and deleted the agent's brain.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Brain-Deletion&amp;quot; Problem is Real:&lt;/strong&gt; &lt;code&gt;deepseek-r1&lt;/code&gt; and &lt;code&gt;devstral&lt;/code&gt;'s attempts are a perfect warning. They &amp;quot;simplified&amp;quot; the code by making it non-functional, a catastrophic failure for any real-world coding assistant.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quality Over Size, Again:&lt;/strong&gt; The 14B &lt;code&gt;phi4-reasoning:14b-plus-q8_0&lt;/code&gt; once again performed flawlessly, equalling or bettering 30B+ models. This reinforces that a model's reasoning and instruction-following capabilities are far more important than its parameter count.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;code, if you want to have a look:&lt;br /&gt; &lt;a href="https://github.com/MarekIksinski/experiments_various/tree/main/experiment2"&gt;https://github.com/MarekIksinski/experiments_various/tree/main/experiment2&lt;/a&gt;&lt;br /&gt; part1:&lt;br /&gt; &lt;a href="https://www.reddit.com/r/ollama/comments/1ocuuej/comment/nlby2g6/"&gt;https://www.reddit.com/r/ollama/comments/1ocuuej/comment/nlby2g6/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Courage_6563"&gt; /u/Western_Courage_6563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og5uul/playing_with_coding_models_pt2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og5uul/playing_with_coding_models_pt2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1og5uul/playing_with_coding_models_pt2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T23:30:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogwst6</id>
    <title>How to run Ollama on an old iMac with macOS 15 Catalina ?</title>
    <updated>2025-10-26T21:40:33+00:00</updated>
    <author>
      <name>/u/_threads</name>
      <uri>https://old.reddit.com/user/_threads</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'd like to know if there is an old build of Ollama that would run on my late 2013 27&amp;quot; iMac. &lt;/p&gt; &lt;p&gt;It has 32Go RAM and and NVIDIA GeForce GTX 775M 2 Go graphic card &lt;/p&gt; &lt;p&gt;I'm not asking much, justing running a mistral model (or others you'd recommend) for simple text generation tasks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_threads"&gt; /u/_threads &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ogwst6/how_to_run_ollama_on_an_old_imac_with_macos_15/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ogwst6/how_to_run_ollama_on_an_old_imac_with_macos_15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ogwst6/how_to_run_ollama_on_an_old_imac_with_macos_15/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-26T21:40:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogv5f6</id>
    <title>Optimze ollama</title>
    <updated>2025-10-26T20:32:36+00:00</updated>
    <author>
      <name>/u/Substantial_Poet1092</name>
      <uri>https://old.reddit.com/user/Substantial_Poet1092</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I would like to know how to make ollama run better on windows 11. i've used it on the same computer on linux and it ran nice and fast was able to get up to 14b parameters but when im on windows it struggles to run 8b parameters &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Poet1092"&gt; /u/Substantial_Poet1092 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ogv5f6/optimze_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ogv5f6/optimze_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ogv5f6/optimze_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-26T20:32:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oha3a2</id>
    <title>How to use ollama chat in comfyUI</title>
    <updated>2025-10-27T09:43:10+00:00</updated>
    <author>
      <name>/u/Pierrepierrepierreuh</name>
      <uri>https://old.reddit.com/user/Pierrepierrepierreuh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oha3a2/how_to_use_ollama_chat_in_comfyui/"&gt; &lt;img alt="How to use ollama chat in comfyUI" src="https://preview.redd.it/oqb2htswkmxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c05bde1f5f3d55d18cfde90a9707e17df5aacaf7" title="How to use ollama chat in comfyUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm brand new to the world of AI, and I'm trying out comfyUI with the ollama cat. I'd like to modify one of my images, but I find the AI's image suggestions to be really poor. I don't know if I did something wrong in my nodes, or in the comfyUI git installation and installing the comfy manager and control net extensions. Anyway, do you have any recommendations? My KSAMPLER has the following parameters: steps 100, CFG 20, sampler name dpmpp_2m_2de, scheduler simple, and denoise 0.3 I'm waiting for my workflow to help me with my interior architecture images. To boost the realism of certain textures, change the mood of the images, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pierrepierrepierreuh"&gt; /u/Pierrepierrepierreuh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oqb2htswkmxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oha3a2/how_to_use_ollama_chat_in_comfyui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oha3a2/how_to_use_ollama_chat_in_comfyui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-27T09:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogzkof</id>
    <title>Custom full stack AI suite for local Voice Cloning (TTS) + LLM</title>
    <updated>2025-10-26T23:42:33+00:00</updated>
    <author>
      <name>/u/Chronos127</name>
      <uri>https://old.reddit.com/user/Chronos127</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ogzkof/custom_full_stack_ai_suite_for_local_voice/"&gt; &lt;img alt="Custom full stack AI suite for local Voice Cloning (TTS) + LLM" src="https://external-preview.redd.it/71DEp2ovueIqdp_0bFuMoLvAb1FI1IZ-BkzAroQo6z0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=adddd9de77ab2d1d6d91b05720f4d807c8c24787" title="Custom full stack AI suite for local Voice Cloning (TTS) + LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chronos127"&gt; /u/Chronos127 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/82vajkokrixf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ogzkof/custom_full_stack_ai_suite_for_local_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ogzkof/custom_full_stack_ai_suite_for_local_voice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-26T23:42:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohbh1j</id>
    <title>Best of LLM,AUDIO AI for M1-series chips (64GB ram)</title>
    <updated>2025-10-27T11:06:54+00:00</updated>
    <author>
      <name>/u/dxcore_35</name>
      <uri>https://old.reddit.com/user/dxcore_35</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ohbh1j/best_of_llmaudio_ai_for_m1series_chips_64gb_ram/"&gt; &lt;img alt="Best of LLM,AUDIO AI for M1-series chips (64GB ram)" src="https://b.thumbs.redditmedia.com/DI2kCEfe3omKV8pmXDvz4SAQ9ktp7eEopfx41NWrcJQ.jpg" title="Best of LLM,AUDIO AI for M1-series chips (64GB ram)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dxcore_35"&gt; /u/dxcore_35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/MacLLM/comments/1ohbfgx/best_of_llmaudio_ai_for_m1series_chips_64gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ohbh1j/best_of_llmaudio_ai_for_m1series_chips_64gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ohbh1j/best_of_llmaudio_ai_for_m1series_chips_64gb_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-27T11:06:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohvrkp</id>
    <title>NotebookLM alternative</title>
    <updated>2025-10-28T00:44:44+00:00</updated>
    <author>
      <name>/u/karkibigyan</name>
      <uri>https://old.reddit.com/user/karkibigyan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! NotebookLM is awesome, and it inspired us to push things even further. We are building an alternative where you can not only upload resources and get grounded answers, but also collaborate with AI to actually accomplish tasks.&lt;/p&gt; &lt;p&gt;Any file operation you can think of such as creating, sharing, or organizing files can be executed through natural language. For example, you could say:&lt;br /&gt; • “Organize all my files by subject or by type.”&lt;br /&gt; • “Analyze this spreadsheet and give me insights with charts.”&lt;br /&gt; • “Create folders for each project listed in this CSV and invite teammates with read-only access.”&lt;/p&gt; &lt;p&gt;We also recently introduced automatic organization for files uploaded to your root directory, along with a Gmail integration that detects attachments in new emails and organizes them for you.&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts. If you are interested in trying it out: &lt;a href="https://thedrive.ai/"&gt;https://thedrive.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/karkibigyan"&gt; /u/karkibigyan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ohvrkp/notebooklm_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ohvrkp/notebooklm_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ohvrkp/notebooklm_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-28T00:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohc1s2</id>
    <title>Ollama + n8n credential</title>
    <updated>2025-10-27T11:38:36+00:00</updated>
    <author>
      <name>/u/Interesting_Range270</name>
      <uri>https://old.reddit.com/user/Interesting_Range270</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ohc1s2/ollama_n8n_credential/"&gt; &lt;img alt="Ollama + n8n credential" src="https://b.thumbs.redditmedia.com/eS1AVpSWlkBmRfnHdToZI67rWa9NcFOwgANAco91WlM.jpg" title="Ollama + n8n credential" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I tried everything i could find on the internet but my local llama2 model just refuses to connect to my n8n project.&lt;/p&gt; &lt;p&gt;I use Windows 11, and don't use a Docker for Ollama and n8n. Ollama's version is: 0.12.6, and i use n8n Cloud, that always automaticly updates&lt;/p&gt; &lt;p&gt;I tried:&lt;br /&gt; - Re-installing Ollama, using different Ollama model types&lt;br /&gt; - installing n8n on pc with Node.js, instead of running on cloud&lt;br /&gt; - all types of ports in the Base URL code&lt;br /&gt; - clearing RAM&lt;br /&gt; - turned off all firewalls&lt;/p&gt; &lt;p&gt;but it still doesnt work&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bhwrddcw4nxf1.png?width=1027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d6408f51f363be8703f49f3783725346b645041"&gt;https://preview.redd.it/bhwrddcw4nxf1.png?width=1027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d6408f51f363be8703f49f3783725346b645041&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting_Range270"&gt; /u/Interesting_Range270 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ohc1s2/ollama_n8n_credential/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ohc1s2/ollama_n8n_credential/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ohc1s2/ollama_n8n_credential/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-27T11:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh9qvs</id>
    <title>How can I get persistent memory with ollama?</title>
    <updated>2025-10-27T09:20:46+00:00</updated>
    <author>
      <name>/u/ThrowRa-Pandakitty</name>
      <uri>https://old.reddit.com/user/ThrowRa-Pandakitty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I am completely new to this, if you have any ideas or suggestions, please consider an ELI5 format.&lt;/p&gt; &lt;p&gt;I just downloaded ollama and I really just want to use it like a simple story bot. I have my characters and just want the bot to remember who they are and what they are about.&lt;/p&gt; &lt;p&gt;What are some ways I could go about that? Any resources I could look into?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThrowRa-Pandakitty"&gt; /u/ThrowRa-Pandakitty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oh9qvs/how_can_i_get_persistent_memory_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oh9qvs/how_can_i_get_persistent_memory_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oh9qvs/how_can_i_get_persistent_memory_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-27T09:20:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohiwdn</id>
    <title>Script for Updating all Models to the Latest Versions</title>
    <updated>2025-10-27T16:21:10+00:00</updated>
    <author>
      <name>/u/Wentil</name>
      <uri>https://old.reddit.com/user/Wentil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanting to keep all of my Ollama models updated to their latest versions &lt;em&gt;[and finding that there was no native command in Ollama to do it]&lt;/em&gt;, I wrote the following script for use in Windows (which has worked well), and so I thought to share it to the community here. Just copy and paste it into a Batch (.bat) file. You can then either run that Batch file directly from a Command Shell or make a Shortcut pointing to it.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@echo off setlocal enabledelayedexpansion echo Updating all models to the latest versions... for /f &amp;quot;tokens=1&amp;quot; %%a in ('ollama list ^| more +1') do ( echo Updating model: %%a ollama pull %%a ) echo Done. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wentil"&gt; /u/Wentil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ohiwdn/script_for_updating_all_models_to_the_latest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ohiwdn/script_for_updating_all_models_to_the_latest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ohiwdn/script_for_updating_all_models_to_the_latest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-27T16:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh9s66</id>
    <title>What's the best, I can run with 32GB of RAM and 8GB of VRAM</title>
    <updated>2025-10-27T09:23:06+00:00</updated>
    <author>
      <name>/u/AnxiousJuggernaut291</name>
      <uri>https://old.reddit.com/user/AnxiousJuggernaut291</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the best, I can run with 32GB of RAM and 8GB of VRAM , i'm using my own computer&lt;br /&gt; + how can i make it answer any question without any restrictions or moral code or whatever the nonsense that make AI dump&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnxiousJuggernaut291"&gt; /u/AnxiousJuggernaut291 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oh9s66/whats_the_best_i_can_run_with_32gb_of_ram_and_8gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oh9s66/whats_the_best_i_can_run_with_32gb_of_ram_and_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oh9s66/whats_the_best_i_can_run_with_32gb_of_ram_and_8gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-27T09:23:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi3a1c</id>
    <title>Models for creative fantasy writing</title>
    <updated>2025-10-28T07:44:48+00:00</updated>
    <author>
      <name>/u/Cyclonit</name>
      <uri>https://old.reddit.com/user/Cyclonit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am planning to run a new DND campaign with some of my friends. Thus far I have used Mistral and ChatGPT for world building to some effect. But I would like to pivot to using a self hosted solution instead. What are current options for models in this space?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cyclonit"&gt; /u/Cyclonit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oi3a1c/models_for_creative_fantasy_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oi3a1c/models_for_creative_fantasy_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oi3a1c/models_for_creative_fantasy_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-28T07:44:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiaqs3</id>
    <title>NVIDIA SMI 470... Is it enough?</title>
    <updated>2025-10-28T14:23:09+00:00</updated>
    <author>
      <name>/u/Alarmed_Card_8495</name>
      <uri>https://old.reddit.com/user/Alarmed_Card_8495</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I am trying to run ollama models with GPU accel.&lt;/p&gt; &lt;p&gt;I have two graphics cards, one is a K2000, and the other is an A2000. I want to use the K2000 simply to display my screens on windows, nothing else. This leaves the A2000's 6GB VRAM completely free for ollama.&lt;/p&gt; &lt;p&gt;However, the issue is how old the K2000 is and the driver it wants. It wants to use 470, and when I install 470 ollama completely stops using the GPU, even when I point to ID=1 (the A2000).&lt;/p&gt; &lt;p&gt;However, if I upgrade to nvidia 580, ollama now works with gpu accel but the PC cannot recognise the K2000 anymore and my screens stop displaying...&lt;/p&gt; &lt;p&gt;Is there anyway at all to have 2 graphics cards, one of which is &amp;quot;too old&amp;quot; and should not be used anyway?&lt;/p&gt; &lt;p&gt;Maybe I should also add I am using WSL2 to run ollama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarmed_Card_8495"&gt; /u/Alarmed_Card_8495 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oiaqs3/nvidia_smi_470_is_it_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oiaqs3/nvidia_smi_470_is_it_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oiaqs3/nvidia_smi_470_is_it_enough/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-28T14:23:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi54mw</id>
    <title>Large Language Models for GNU Octave</title>
    <updated>2025-10-28T09:52:43+00:00</updated>
    <author>
      <name>/u/pr0m1th3as</name>
      <uri>https://old.reddit.com/user/pr0m1th3as</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oi54mw/large_language_models_for_gnu_octave/"&gt; &lt;img alt="Large Language Models for GNU Octave" src="https://external-preview.redd.it/IxsKGIh-ILZUIIw_RAVbL7OrD7Tbd1WpcDZbZwj0WxA.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49abe0ca1caeb24973eacb1cde5539a0ad6ed073" title="Large Language Models for GNU Octave" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pr0m1th3as"&gt; /u/pr0m1th3as &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gnu-octave.github.io/packages/llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oi54mw/large_language_models_for_gnu_octave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oi54mw/large_language_models_for_gnu_octave/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-28T09:52:43+00:00</published>
  </entry>
</feed>
