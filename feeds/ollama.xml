<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-07T19:34:19+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pe18c7</id>
    <title>How does Ollama truncate the context when it's too long?</title>
    <updated>2025-12-04T14:21:49+00:00</updated>
    <author>
      <name>/u/AmphibianFrog</name>
      <uri>https://old.reddit.com/user/AmphibianFrog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to understand how prompts are truncated then they exceed the context limit. Let's say I have the num_ctx parameter set to 1000 tokens. I then send in a system prompt, and a whole bunch of user and assistant messages, but in total there are 2000 tokens in the context. What happens?&lt;/p&gt; &lt;p&gt;Specific questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does truncation happen per message or do partial messages get fed into the LLM?&lt;/li&gt; &lt;li&gt;Does the system prompt get truncated? Or is it always passed in even if other messages are removed from the input?&lt;/li&gt; &lt;li&gt;Do the messages get removed in pairs, i.e will it always make sure a user message is first in the prompt?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;My conclusion is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The system prompt is never truncated&lt;/li&gt; &lt;li&gt;Entire messages are dropped from the context, not individual tokens&lt;/li&gt; &lt;li&gt;Ollama will include as many messages as possible, getting rid of the oldest messages first (apart from the system prompt which is always included)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hopefully this helps someone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AmphibianFrog"&gt; /u/AmphibianFrog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T14:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe2xbe</id>
    <title>how close to codex or claude code can you get with ollama and a 4090?</title>
    <updated>2025-12-04T15:30:22+00:00</updated>
    <author>
      <name>/u/reelznfeelz</name>
      <uri>https://old.reddit.com/user/reelznfeelz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to run an agentic code assistant that can use tools on top of ollama without having to spend a whole ton of time on building it yourself? &lt;/p&gt; &lt;p&gt;Somebody here has probably followed or played in that space. &lt;/p&gt; &lt;p&gt;Just thinking/planning for the day when openAI/Claude has to charge what's actually needed to be profitable, which will be like $900/mo for the kind of usage I have. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reelznfeelz"&gt; /u/reelznfeelz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T15:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexhan</id>
    <title>Es Qwen3 el mejor modelo de IA del mundo en general en este momento? Mienten los benchmarks?</title>
    <updated>2025-12-05T15:08:59+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Algunos no nos fiamos de los Benchmarks..nos fiamos mas de lo que vemos por nosotros mismos…&lt;/p&gt; &lt;p&gt;Vosotros que pensais?&lt;/p&gt; &lt;p&gt;Es Qwen el mejor optimizado y el mas preciso mas inteligente y que mejor resultados da en matematicas fisica y programacion ? Siendo un modelo general razonablemente pequeño en comparacion con otros…aunque no he probado el nuevo modelo 3.2 exp de deepseek pero yo con el qwen3 estoy muy muy contento&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexhan/es_qwen3_el_mejor_modelo_de_ia_del_mundo_en/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexhan/es_qwen3_el_mejor_modelo_de_ia_del_mundo_en/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pexhan/es_qwen3_el_mejor_modelo_de_ia_del_mundo_en/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T15:08:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1petnr2</id>
    <title>Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices</title>
    <updated>2025-12-05T12:20:16+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1petnr2/translating_webpages_using_local_model_on_ipad/"&gt; &lt;img alt="Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices" src="https://external-preview.redd.it/dmJremhrcWZvZDVnMVvVURK3C0St0olAiXhHNsqHlBUDFrodjuu0gc-gowgU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=471e58ef80ba2c5d0ce953debe5fc9808e05745f" title="Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Browsers comes with translation tool - but few of them provide legible translations. We are not used to the high quality translation provided by LLMs, and we expect the same experience with webpages translation when browsing.&lt;/p&gt; &lt;p&gt;I am pleased to announce that Vector Space now integrates Webpage translation. Featuring:&lt;/p&gt; &lt;p&gt;- Use a LLM instead of translation APIs&lt;/p&gt; &lt;p&gt;- Works on mobile&lt;/p&gt; &lt;p&gt;- Call local models for unlimited and private, translation&lt;/p&gt; &lt;p&gt;- Perserve HTML structures and visuals&lt;/p&gt; &lt;p&gt;- Connect to OpenAI API for faster transaction (enter your API in the settings)&lt;/p&gt; &lt;p&gt;Result is some very nice translations! Please see the video. It is filmed on a M1 iPad.&lt;/p&gt; &lt;p&gt;Try it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://short.yomer.jp/vector-space"&gt;https://short.yomer.jp/vector-space&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Limitations and next directions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Right now a relative large model (~4B) is needed for preserving HTML tags and improving translation quality. I believe a fine tuned model of a much smaller size can do the trick. With enough people supporting me I can work on it to increase translation speed at least 10x.&lt;/li&gt; &lt;li&gt;Due to Apple restriction on running GPU work in the background, currently only iPad multi tasking is supported on iOS. I believe this is solvable by either looking at Background Tasks framework or move to neural engine.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i3d55jqfod5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1petnr2/translating_webpages_using_local_model_on_ipad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1petnr2/translating_webpages_using_local_model_on_ipad/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T12:20:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1peu215</id>
    <title>A better way to share and talk to long PDFs</title>
    <updated>2025-12-05T12:40:44+00:00</updated>
    <author>
      <name>/u/simplext</name>
      <uri>https://old.reddit.com/user/simplext</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1peu215/a_better_way_to_share_and_talk_to_long_pdfs/"&gt; &lt;img alt="A better way to share and talk to long PDFs" src="https://external-preview.redd.it/OGpvMjQ4ZmNyZDVnMbGTwPFoXGKKHKurfth5fGD5PZl_uCHNFDWTG1GTjAFD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0adbc635fe108608aa908b6ae8ac3dae15871e9" title="A better way to share and talk to long PDFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Imagine going through the long and boring jobs data PDF shared by the BLS. What if they had shared a presentation that you could talk to ? &lt;/p&gt; &lt;p&gt;Visual Book allows you to break down an PDF into slides and then share it with people so they can talk to it.&lt;/p&gt; &lt;p&gt;Visual Book: &lt;a href="https://www.visualbook.app"&gt;https://www.visualbook.app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simplext"&gt; /u/simplext &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3vqnevecrd5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1peu215/a_better_way_to_share_and_talk_to_long_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1peu215/a_better_way_to_share_and_talk_to_long_pdfs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T12:40:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf07j7</id>
    <title>Looking for something to manage API usage</title>
    <updated>2025-12-05T16:54:07+00:00</updated>
    <author>
      <name>/u/tecneeq</name>
      <uri>https://old.reddit.com/user/tecneeq</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for something that allows me to give different amounts of tokens per day or week to different users for different models. Basically some kind of rate limiting.&lt;/p&gt; &lt;p&gt;Does anyone know something like that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tecneeq"&gt; /u/tecneeq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pf07j7/looking_for_something_to_manage_api_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pf07j7/looking_for_something_to_manage_api_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pf07j7/looking_for_something_to_manage_api_usage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T16:54:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pekbzp</id>
    <title>No Uncensored Models?</title>
    <updated>2025-12-05T03:16:26+00:00</updated>
    <author>
      <name>/u/One_Spaceman</name>
      <uri>https://old.reddit.com/user/One_Spaceman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just downloaded Ollama, pulled Dolphin-Ollama3 from &lt;a href="https://www.youtube.com/watch?v=cTxENLLX1ho"&gt;THIS &lt;/a&gt;vid but its completely censored, is there anything that is totally uncensored? idk what to do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One_Spaceman"&gt; /u/One_Spaceman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T03:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfkede</id>
    <title>Y si Elon musk nos regalo MOE y nadie le puede dar las gracias?</title>
    <updated>2025-12-06T08:32:48+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all know who financed with 100 billion dollars the artificial intelligence from the beginning.&lt;br /&gt; We all know that Elon Musk complained that they were hiding from him the results that the AI was giving.&lt;br /&gt; Some of us think that out of envy of being the richest in the world, they want you only to put the money, but the other shareholders and companies did not want &amp;quot;the richest&amp;quot; to know about the advances.&lt;br /&gt; Elon Musk went out... he left voluntarily, right after leaving Musk, the AI explodes suddenly and becomes the most advanced on the planet.. What a coincidence?? While he contributed money the AI does not give results and as soon as I leave, the results appear.&lt;br /&gt; Then he would consider it a scam (They used him to finance it and keep it for themselves)&lt;br /&gt; What Musk financed according to my point of view was the MOE technology (The core of everything)&lt;br /&gt; If that were so, the only way to be able to compete with them, would be taking them to the starting square, and for that he should take away the MOE technology from them, and the best way would be talking with some Chinese engineers, so that they passed it to the Chinese for free, (or to the French of MIXTRAL) he would support them to do it in exchange for committing to release the technology to the world, that way he would put OPENAI to start from zero, and they would start to compete with GROK from zero.&lt;br /&gt; We DO NOT know for sure what has happened, but I know that no one gives away something that is worth billions, neither to the French nor to the Chinese, and I do not believe that they had the capacity to have developed something so valuable.... I believe that here something has happened that no one knows, and I believe that maybe, for having used and scammed Elon Musk, he decided that the technology that he paid out of his pocket was not going to stay with the competition if he wanted to compete with them, and that he looked for a way to give it away to the French of Mixtral or to the Chinese, in order to thus send them all to the starting square of the race and start competing with them with GROK from zero.&lt;br /&gt; The only way to put a company that has a secret worth billions to start from zero, is to release that secret, and that we all have it, that way it stops having value.&lt;br /&gt; I have the feeling that here all this conspiracy that everyone thought of technology thefts and sabotages, no one ever thought that it was the MOE technology, the one that made the race balance and everyone started to compete from the same starting square.&lt;br /&gt; But I have in my head stuck that something has to do with Elon Musk with us having this MOE technology (because think about it well) this is worth millions and they give it to us for free!!!!&lt;br /&gt; Who has paid for it? it was Elon Musk!! HE WAS THE ONE WHO MAINLY FINANCED OPEN AI AT THE BEGINNING TO DEVELOP THIS TECHNOLOGY!!!&lt;br /&gt; In the end the technology fell into our hands, not because he wanted it, but because they tried to scam deceive and use him out of envy of being the richest of all.&lt;br /&gt; How if this were true it cannot be known??? The entire OPENSOURCE community should thank ELON MUSK for what he has done, but since in theory it is something hypothetical that no one knows, we cannot give them...&lt;br /&gt; But I give them anyway, in case he was the one who gave us the MOE technology, because if you think about it well, it is what makes the difference between the AI, The MOE technology is what allows the AI to be used on a large scale.&lt;br /&gt; So nothing.. if hypothetical theory were true... Thanks Elon for being so brave!!!!&lt;/p&gt; &lt;p&gt;Todos sabemos quien financio con 100 mil millones de dolares la inteligencia artificial desde el principio.&lt;/p&gt; &lt;p&gt;Todos sabemos que Elon musk se quejo que le ocultaban los resultados que estaba dando la IA&lt;/p&gt; &lt;p&gt;Algunos pensamos que por envidia de ser el mas rico del mundo , te quieren solo para poner el dinero , pero los otros accionistas y empresas no querian que &amp;quot;el mas rico&amp;quot; supiese de los avances.&lt;/p&gt; &lt;p&gt;Elon musk se fue fuera...abandono voluntariamente , justo despues de abandonar Musk , la IA explota de repente y se vuelve lo mas avanzado en el planeta..Que coincidencia?? MIentras aporto dinero la IA no da resultados y en cuento me voy , aparecen los resultados.&lt;/p&gt; &lt;p&gt;Entonces el lo consideraria una estafa (Lo utilizaron para financiarla y quedarse ellos con ella)&lt;/p&gt; &lt;p&gt;Lo que musk financio segun mi punto de vista fue la tecnologia MOE (El nucleo de todo)&lt;/p&gt; &lt;p&gt;SI ello asi fuese , la unica forma de poder competir con ellos , seria llevandolos a ellos a la casilla de salida , y para ello deberia sacarles la tecnologia MOE , y la mejor forma seria hablando con algunos ingenieros chinos , para que se la pasaron a los chinos de forma gratuita ,(o a los franceses de MIXTRAL) el los apoyaria a hacerlo a cambio de comprometerse a librar la tecnologia al mundo , asi pondria a OPENAI a empezar desde cero , y empezarian a competir con GROK desde cero.&lt;/p&gt; &lt;p&gt;NO sabemos a ciencia cierta que ha pasado , pero yo se que nadie regala algo que vale miles de millones , ni a los franceses ni a los chinos , y no creo que ellos tuviesen la capacidad de haber desarrollado algo tan valioso....yo creo que aqui algo ha pasado que nadie sabe , y creo que a lo mejor , por haber utilizado y estafado a Elon musk , el decidio que la tecnologia que el pago de su bolsillo no se iba a quedar en la competencia si el queria competir con ellos , y que busco la forma de regalarsela a los Franceses de Mixtral o a los chinos , para poder asi mandarlos a todos a la casilla de salida de la carrera y empezar a competir con ellos con GROK desde cero.&lt;/p&gt; &lt;p&gt;La unica forma de poner auna empresa que tiene un secreto que vale miles de millones a empezar de cero , es liberar ese secreto , y que todos lo tengamos , asi deja de tener valor.&lt;/p&gt; &lt;p&gt;Yo tengo la sensacion de que aqui toda esta conspiracion que todo el mundo penso de robos de tecnologias y sabotajes , nunca nadie penso que fue la tecnologia MOE , la que hizo que la carrera se equilibrara y empezaran todos a competir desde la misma casilla de salida.&lt;/p&gt; &lt;p&gt;Pero yo tengo en la cabeza metido que algo tiene que ver Elon musk con nostros tener esta tecnologia MOE (porque pensadlo bien) esto vale millones y nos lo dan dado gratis!!!!&lt;/p&gt; &lt;p&gt;Quien lo ha pagado? fue elon musk!! EL FUE QUIEN PRINCIPALMENTE FINANCIA A OPEN AI AL PRINCIPIO PARA DESARROLLAR ESTA TECNOLOGIA!!!&lt;/p&gt; &lt;p&gt;Al final la tecnologia cayo en nuestras manos , no porque el quisiera , sino porque lo intentaron estafar engañar e utilizar por la envidia de ser el mas rico de todos.&lt;/p&gt; &lt;p&gt;Como esto si fuese cierto no se puede saber??? Toda la comunidad OPENSOURCE le deberia dar las gracias a ELON MUSK por lo que ha echo , pero como en teoria es algo hipotetico que nadie sabe , no se las podemos dar...&lt;/p&gt; &lt;p&gt;Pero yo se las doy igual , por si el fue quien nos regalo la tecnologia MOE , porque si lo pensais bien , es lo que marca la diferencia entre la IA , La tecnologia MOE es lo que permite que la IA pueda ser usada a gran escala.&lt;/p&gt; &lt;p&gt;Asi que nada..si hipotetica teoria fuese cierta...Gracias Elon por ser tan valiente!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfkede/y_si_elon_musk_nos_regalo_moe_y_nadie_le_puede/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfkede/y_si_elon_musk_nos_regalo_moe_y_nadie_le_puede/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfkede/y_si_elon_musk_nos_regalo_moe_y_nadie_le_puede/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T08:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexi8g</id>
    <title>Confused and unsure</title>
    <updated>2025-12-05T15:10:01+00:00</updated>
    <author>
      <name>/u/SirEblingMis</name>
      <uri>https://old.reddit.com/user/SirEblingMis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there.&lt;/p&gt; &lt;p&gt;I've seen lots of different rankings, but I haven't found a good concise resource that explains how I judge a model fitting onto 16gb vram or on 20-24gb M4 pro [mtx on LMstudio? or similar]&lt;/p&gt; &lt;p&gt;I'm genuinely just interested in a solid model to help administrative tasks as I do my Master's degree. I use Overleaf for it's great help with LaTex, and Perplexity for finding papers, teaching myself code or LaTex, etc.&lt;/p&gt; &lt;p&gt;But I want to run this stuff locally, especially since I may sometimes end up working with datasets that are confidential or secure.&lt;/p&gt; &lt;p&gt;Apologies if this post is a repeat or faux pas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SirEblingMis"&gt; /u/SirEblingMis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexi8g/confused_and_unsure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexi8g/confused_and_unsure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pexi8g/confused_and_unsure/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T15:10:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pev1eh</id>
    <title>I can't make Ministral 3 14B to work.</title>
    <updated>2025-12-05T13:27:40+00:00</updated>
    <author>
      <name>/u/No-Acanthisitta9773</name>
      <uri>https://old.reddit.com/user/No-Acanthisitta9773</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pev1eh/i_cant_make_ministral_3_14b_to_work/"&gt; &lt;img alt="I can't make Ministral 3 14B to work." src="https://preview.redd.it/usb606890e5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48964b9a072420216bf48a6c3636ad0d9b29b044" title="I can't make Ministral 3 14B to work." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Ministral-3-14B-Instruct-2512-Q5_K_M &lt;/p&gt; &lt;p&gt;I've tried different kind of modelfile, but it always responds the same nonsense. Ollama is up-to-date. Did you manage to make it work? What was your modelfile like? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Acanthisitta9773"&gt; /u/No-Acanthisitta9773 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/usb606890e5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pev1eh/i_cant_make_ministral_3_14b_to_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pev1eh/i_cant_make_ministral_3_14b_to_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T13:27:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf9grf</id>
    <title>Best model in the 8B range for RAG in 2025</title>
    <updated>2025-12-05T23:00:35+00:00</updated>
    <author>
      <name>/u/Hour-Entertainer-478</name>
      <uri>https://old.reddit.com/user/Hour-Entertainer-478</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hour-Entertainer-478"&gt; /u/Hour-Entertainer-478 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pf92li/best_model_in_the_8b_range_for_rag_in_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pf9grf/best_model_in_the_8b_range_for_rag_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pf9grf/best_model_in_the_8b_range_for_rag_in_2025/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T23:00:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pffgp1</id>
    <title>Noob here, looking for the perfect local LLM for my M3 Macbook Air 24GB RAM</title>
    <updated>2025-12-06T03:45:27+00:00</updated>
    <author>
      <name>/u/sylntnyte</name>
      <uri>https://old.reddit.com/user/sylntnyte</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sylntnyte"&gt; /u/sylntnyte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pff8b5/noob_here_looking_for_the_perfect_local_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pffgp1/noob_here_looking_for_the_perfect_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pffgp1/noob_here_looking_for_the_perfect_local_llm_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T03:45:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfj633</id>
    <title>Llm for log analysis</title>
    <updated>2025-12-06T07:14:35+00:00</updated>
    <author>
      <name>/u/gargento83</name>
      <uri>https://old.reddit.com/user/gargento83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is a good LLM model for security log analysis for cybersecurity?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gargento83"&gt; /u/gargento83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfj633/llm_for_log_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfj633/llm_for_log_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfj633/llm_for_log_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T07:14:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfr31a</id>
    <title>NornicDB - V1 MemoryOS for LLMs - MIT</title>
    <updated>2025-12-06T14:50:57+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1pfr2tj/nornicdb_v1_memoryos_for_llms_mit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfr31a/nornicdb_v1_memoryos_for_llms_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfr31a/nornicdb_v1_memoryos_for_llms_mit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T14:50:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfibo6</id>
    <title>CocoIndex 0.3.1 - Open-Source Data Engine for Dynamic Context Engineering</title>
    <updated>2025-12-06T06:23:30+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I'm back with a new version of &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;CocoIndex&lt;/a&gt; (v0.3.1 - Ollama natively supported), with significant updates since last one. CocoIndex is ultra performant data transformation for AI &amp;amp; Dynamic Context Engineering - Simple to connect to source, and keep the target always fresh for all the heavy AI transformations (and any transformations) with incremental processing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Adaptive Batching&lt;/strong&gt;&lt;br /&gt; Supports automatic, knob-free batching across all functions. In our benchmarks with MiniLM, batching delivered ~5× higher throughput and ~80% lower runtime by amortizing GPU overhead with no manual tuning. I think particular if you have large AI workloads, this can help and is relevant to this sub-reddit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Custom Sources&lt;/strong&gt;&lt;br /&gt; With custom source connector, you can now use it to any external system — APIs, DBs, cloud storage, file systems, and more. CocoIndex handles incremental ingestion, change tracking, and schema alignment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Runtime &amp;amp; Reliability&lt;/strong&gt;&lt;br /&gt; Safer async execution and correct cancellation, Centralized HTTP utility with retries + clear errors, and many others.&lt;/p&gt; &lt;p&gt;You can find the full release notes here: &lt;a href="https://cocoindex.io/blogs/changelog-0310"&gt;https://cocoindex.io/blogs/changelog-0310&lt;/a&gt;&lt;br /&gt; Open source project here : &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;https://github.com/cocoindex-io/cocoindex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Btw, we are also on Github trending in Rust today :) it has Python SDK.&lt;/p&gt; &lt;p&gt;We have been growing so much with feedbacks from this community, thank you so much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfibo6/cocoindex_031_opensource_data_engine_for_dynamic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfibo6/cocoindex_031_opensource_data_engine_for_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfibo6/cocoindex_031_opensource_data_engine_for_dynamic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T06:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg03g5</id>
    <title>Fara-7b on ollama?</title>
    <updated>2025-12-06T21:09:52+00:00</updated>
    <author>
      <name>/u/tracagnotto</name>
      <uri>https://old.reddit.com/user/tracagnotto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; i was trying running Fara-7B from microsoft, they require vllm and it's a nightmare.&lt;/p&gt; &lt;p&gt;I found that it's usable on ollama but I don't really understand how. Ollama is chat, and that model needs to do continuous agentic work. In fact my test was:&lt;br /&gt; ME:&lt;br /&gt; [URL OF SOME BLOG]&lt;/p&gt; &lt;p&gt;what's this blog about&lt;/p&gt; &lt;p&gt;OLLAMA/FARA:&lt;/p&gt; &lt;p&gt;This is a screenshot of the latest news page on Matthew Leer-Bobinson's blog. The URL (url of the blog) indicates it's the second page of the latest news posts. &amp;lt;tool\_call&amp;gt; {&amp;quot;name&amp;quot;: &amp;quot;olution&amp;quot;, &amp;quot;arguments&amp;quot;: {&amp;quot;t&amp;quot;: &amp;quot;info&amp;quot;}} &amp;lt;/tool\_call&amp;gt;&lt;/p&gt; &lt;p&gt;ME:&lt;br /&gt; Examine content pls&lt;/p&gt; &lt;p&gt;OLLAMA/FARA:&lt;br /&gt; I’ll click on the link to open the latest-news page and examine its content. &amp;lt;tool\_call&amp;gt; {&amp;quot;name&amp;quot;: &amp;quot;olution&amp;quot;, &amp;quot;arguments&amp;quot;: {&amp;quot;t&amp;quot;: &amp;quot;left_click&amp;quot;, &amp;quot;coordinate&amp;quot;: [100, 10]}} &amp;lt;/tool\_call&amp;gt;&lt;/p&gt; &lt;p&gt;what am I missing?&lt;br /&gt; Is ollama only a way to run it and then you have to build on it a whole MCP/Agentic stack that takes those result and make actions? lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tracagnotto"&gt; /u/tracagnotto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg03g5/fara7b_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg03g5/fara7b_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pg03g5/fara7b_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T21:09:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg213d</id>
    <title>Usable models and Performance of RTX 2000 Ada 16GB or RTX 4000 20GB?</title>
    <updated>2025-12-06T22:34:06+00:00</updated>
    <author>
      <name>/u/Temporary_Sir</name>
      <uri>https://old.reddit.com/user/Temporary_Sir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm considering picking up an RTX2000 or RTX Pro 4000 card to add to a server. (Ideally directly powered via pci)&lt;/p&gt; &lt;p&gt;Any insights on what performance would be for general usage as well as a bit of coding? &lt;/p&gt; &lt;p&gt;What models would be recommended? Would those even be useful?&lt;/p&gt; &lt;p&gt;Looking forward to your replies&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Sir"&gt; /u/Temporary_Sir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg213d/usable_models_and_performance_of_rtx_2000_ada/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg213d/usable_models_and_performance_of_rtx_2000_ada/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pg213d/usable_models_and_performance_of_rtx_2000_ada/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T22:34:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfo6zh</id>
    <title>Vllama: CLI based framework to run vision models in local or remote gpus(inspired from Ollama)</title>
    <updated>2025-12-06T12:30:17+00:00</updated>
    <author>
      <name>/u/Weekly_Layer_9315</name>
      <uri>https://old.reddit.com/user/Weekly_Layer_9315</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, this is my first post. I have built a simple CLI tool, which can help all to run the llms, vision models like image and video gen, models in the local system and if the system doesn't have the gpu or sufficient ram, they can also run it using kaggle's gpu(which is 30 hrs free for a week).&lt;/p&gt; &lt;p&gt;This is inspired from Ollama, which made downloading llms easy and interacting with it much easy, so I thought of why can't this be made for vision models, so I tried this first on my system, basic image generation is working but not that good, then I thought, why can't we use the Kaggle's GPU to generate videos and images and that can happen directly from the terminal with a single step, so that everyone can use this, so I built this VLLAMA.&lt;/p&gt; &lt;p&gt;In this, currently there are many features, like image, video generation in local and kaggles gpu session; download llms and make it run and also interact with it from anywhere (inspired by ollama) also improved it further by creating a vs code extension VLLAMA, using which you can chat directly from the vs code's chat section, users can chat with the local running llm with just adding &amp;quot;@vllama&amp;quot; at the start of the message and this doesn't use any usage cost and can be used as much as anyone wants, you can check this out at in the vscode extensions.&lt;/p&gt; &lt;p&gt;I want to implement this further so that the companies or anyone with gpu access can download the best llms for their usage and initialize it in their gpu servers, and can directly interact with it from the vscode's chat section and also in further versions, I am planning to implement agentic features so that users can use the local llm to use for code editing, in line suggestions, so that they don't have to pay for premiums and many more.&lt;/p&gt; &lt;p&gt;Currently it also has simple Text-to-Speech, and Speech-to-Text, which I am planning to include in the further versions, using open source audio models and also in further, implement 3D generation models, so that everyone can leverage the use of the open models directly from their terminal, and making the complex process of the using open models easy with just a single command in the terminal.&lt;/p&gt; &lt;p&gt;I have also implemented simple functionalities which can help, like listing the downloaded models and their sizes. Other things available are, basic dataset preprocessing, and training ML models directly with just two commands by just providing it the dataset. This is a basic implementation and want to further improve this so that users with just a dataset can clean and pre-process the data, train the models in their local or using the kaggle's or any free gpu providing services or their own gpus or cloud provided gpus, and can directly deploy the models and can use it any cases.&lt;/p&gt; &lt;p&gt;Currently this are the things it is doing and I want to improve such that everyone can use this for any case of the AI and leveraging the use of open models.&lt;/p&gt; &lt;p&gt;Please checkout the work at: &lt;a href="https://github.com/ManvithGopu13/Vllama"&gt;https://github.com/ManvithGopu13/Vllama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Published version at: &lt;a href="https://pypi.org/project/vllama/"&gt;https://pypi.org/project/vllama/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also the extension: &lt;a href="https://marketplace.visualstudio.com/items?itemName=ManvithGopu.vllama"&gt;https://marketplace.visualstudio.com/items?itemName=ManvithGopu.vllama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would appreciate your time for reading and thankful for everyone who want to contribute and spread a word of it.&lt;/p&gt; &lt;p&gt;Please leave your requests for improvements and any suggestions, ideas, and even roasts or anything in the comments or in the issues, this is well taken and appreciated. Thanks in advance. If you find the project useful, kindly contribute and can star it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weekly_Layer_9315"&gt; /u/Weekly_Layer_9315 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T12:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfiedc</id>
    <title>If it weren't for the Chinese we wouldn't have local AI</title>
    <updated>2025-12-06T06:28:03+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GRACIAS A ELON MUSK!! ESTOY SEGURO!!! Elon el mas trabajador e inteligente del planeta , lo quisieron fastidiar y ahora OpenAi corre riesgo de ir a la quiebra por traicionarlo...todos lo que lo traicionaron por envidia y le ocultaron los resultados de la IA (google Y microsoft) Ahora veremos lo que agutanta OpenAI sin el dinero de musk...&lt;/p&gt; &lt;p&gt;Mientras GROK Y LOS CHINOS avanzan con las cuentas saneadas, con las empresas sin riesgo..bien apalancadas financiadas con seguridad , OPENAI esta temblando de miedo de ir a la quiebra aajjajajajajajaja&lt;/p&gt; &lt;p&gt;Todos sabemos el desastre que pasó cuando un ingeniero chino robó tecnología de Open AI (O eso nos han contado) con todo el lío que causó cuando Deepseek sacó su modelo de IA al mundo Opensource,&lt;/p&gt; &lt;p&gt;Todos sabemos que Open AI se negó a sacar lo mejor de su tecnología...luego cuando MOE technology salió con Qwen e hizo que la bolsa americana cayera por trillones de dólares dándose cuenta que los chips Nvidia quizás no fueran necesarios para la IA.&lt;/p&gt; &lt;p&gt;Sacudió los cimientos de los mercados de valores americanos. No sabemos exactamente qué pasó... si Elon Musk tuvo algo que ver con todo esto al irse de la empresa y se aprovecharon de su financiación al principio... Quizás le ocultaron a Elon Musk los verdaderos resultados que estaba dando la IA para que siguiera financiándolos y él se vengó cuando vio que no había resultados cuando se fue de Open AI y justo después de que Elon Musk se fuera, Open AI dio el gran salto.&lt;/p&gt; &lt;p&gt;Fue demasiada coincidencia...Quizás quien realmente controlaba Open AI (Microsoft), ya que Elon Musk estaba muy ocupado con sus problemas en sus empresas, aprovecharon la oportunidad para apostar y beneficiarse ya que ahora son el accionista mayoritario...ya es cosa tuya saber si Elon Musk estaba sobornando a ingenieros para que le dieran la tecnología MOE y como se negaron hizo un pacto con ellos para dársela a los chinos...y por ahí openai mientras sacaban Grok al mercado...nunca sabremos que pasó en esa telenovela...toda una historia de conspiraciones e hipótesis extrañas...&lt;/p&gt; &lt;p&gt;Lo extraño es que Qwen parece haber adquirido la tecnología que todos creen que está en los modelos de Open AI...y nunca lo sabremos porque los modelos son cerrados y no sabemos qué ingeniería hay realmente detrás. Todo esto pensando... y si no hubiera pasado toda esta historia de robo de tecnología... conspiraciones... etc... caída de la bolsa por el miedo de los inversores a que la burbuja de la IA estallara... fin... si todo eso no hubiera pasado...&lt;/p&gt; &lt;p&gt;Yo creo que la comunidad Opensource no estaría disfrutando de los modelos MOE que tenemos hoy en día... ya que eso conllevó a una feroz competencia y a la salida de modelos para perjudicarse unos a otros... lo que tenemos que ver es que si no hubiera pasado lo de Open AI... creo que no hubieran sacado el GPT-OSS y también creo que nunca volverán a sacar un modelo similar...&lt;/p&gt; &lt;p&gt;Estas empresas son muy avariciosas y quieren todo para ellas y si no fuera por la presión de los chinos, la comunidad Opensource tendría muchos menos modelos capaces y tenemos que estar agradecidos de que QWEN Y DEEPSEEK y otros metieran presión sacando sus modelos...porque los occidentales...si por ellos fuera...se quedarían todo para ellos como hace Google con su buscador...facebook o Amazon con sus algoritmos o Apple Con sus tecnologías...&lt;/p&gt; &lt;p&gt;Normalmente los chinos siempre han sido muy cerrados...pero la competencia por ver quién es ahora la superpotencia que domina el mundo ha traído esos golpes que han sido maravillosos para la comunidad Opensource...&lt;/p&gt; &lt;p&gt;El miedo que tengo es que no se repita algo así y los modelos que saquen de ahora en adelante sean calderilla... las migajas... y lo realmente bueno y las grandes cosas se las queden ellos y lo protejan todo con patentes... y mucha seguridad... ya viste la caída que OpenAi tuvo que ser fundada sin ánimo de lucro para el beneficio de la Humanidad...&lt;/p&gt; &lt;p&gt;Se ha convertido en un negocio que mueve billones con una deuda muy apalancada...y juegan funambulistas colgados de un hilo...Cómo termina todo esto...si todo explota...o siguen sacando modelos...porque los inversores no invierten en el pasado...invierten en el futuro...y si no hay novedades...los occidentales van a estar en problemas...algo que los chinos se financian ellos mismos y son cautos y buscaron la forma de hacer el negocio más rentable requiriendo menos potencia de cálculo...&lt;/p&gt; &lt;p&gt;En resumen, los chinos y asiáticos siempre han destacado en muchas áreas como la electrónica... pero ahora China se está convirtiendo en una Superpotencia mayor que EEUU en muchas áreas y creo que no debemos subestimarlos ni menospreciarlos... OPEN AI va a quebrar completamente creo... y los chinos van a ganar esta batalla... creo que será el principio de la hegemonía de China sobre el mundo...&lt;/p&gt; &lt;p&gt;Son gente trabajadora, eficiente y pacífica...así que realmente apoyo sus modelos y creo que gracias a ellos esta competencia por ver quién gana está beneficiando a la comunidad Opensource de una manera inimaginable que nunca pensé que podríamos tener modelos tan capaces y útiles en nuestras manos para trabajar completamente offline...y gran parte es gracias a los chinos!!!!! Quizás no es gracias a los chinos... sino... a ELON MUSK. Si me equivoco en mis suposiciones OPENAI estaría evolucionando por detrás de todos los demás competidores...ya que serían los mejores...aunque sin la financiación de Elon Musk...todo cambia.&lt;/p&gt; &lt;p&gt;Pronto veremos que ha pasado aquí y que está pasando...porque Elon Musk es una persona brillante y valiente...y una buena persona...y las buenas personas se enfrían...creo que Microsoft pensó que su jugada contra Musk iba a salir bien...pero pronto veremos si OPEN AI quiebra...&lt;/p&gt; &lt;p&gt;Si mis suposiciones son ciertas, Elon Musk nos habría dado a todos la tecnología MOE completamente gratis para que la IA china diera un golpe fuerte a OPENAI para frenarlos y entrar en el top 3 con GROK. Si eso fuera cierto deberíamos agradecerle... pero no se pudo hacer porque nadie sabría que todos los modelos de alta calidad que disfrutamos offline son gracias a ÉL. Al hombre más rico del planeta que nos dio esta tecnología porque algunas personas querían engañarlo!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T06:28:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg5kui</id>
    <title>create model name without the ending :latest</title>
    <updated>2025-12-07T01:18:48+00:00</updated>
    <author>
      <name>/u/Frosty_Chest8025</name>
      <uri>https://old.reddit.com/user/Frosty_Chest8025</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI,&lt;br /&gt; how can i create a copy of gemma3-27b without it having a name gemma3:latest. I need it to be named as gemma3 in the ollama ps list.&lt;/p&gt; &lt;p&gt;ollama create gemma3 -f ollamaproduction&lt;br /&gt; creates model named gemma3:latest. &lt;/p&gt; &lt;p&gt;litellm needs that name without :latest. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frosty_Chest8025"&gt; /u/Frosty_Chest8025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg5kui/create_model_name_without_the_ending_latest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg5kui/create_model_name_without_the_ending_latest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pg5kui/create_model_name_without_the_ending_latest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T01:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgdlw6</id>
    <title>n8n not connecting to Ollama - Locally Hosted</title>
    <updated>2025-12-07T08:43:54+00:00</updated>
    <author>
      <name>/u/zohan_796</name>
      <uri>https://old.reddit.com/user/zohan_796</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I've been using NetworkChuck's videos to get locally hosted AI models and n8n through OpenWebUI.&lt;/p&gt; &lt;p&gt;However, I'm currently having issues with getting my Ollama account on n8n to connect to Ollama. Both are locally hosted using OpenWebUI as per Chuck's videos. I've got the Base URL as &lt;a href="http://localhost:11434"&gt;http://localhost:11434&lt;/a&gt;, which doesn't seem to connect. What do I need to do to allow n8n to link to Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zohan_796"&gt; /u/zohan_796 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgdlw6/n8n_not_connecting_to_ollama_locally_hosted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgdlw6/n8n_not_connecting_to_ollama_locally_hosted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgdlw6/n8n_not_connecting_to_ollama_locally_hosted/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T08:43:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgnjbo</id>
    <title>Aquif 3.5 Max 1205 (42B-A3B)</title>
    <updated>2025-12-07T17:08:31+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pgnjbo/aquif_35_max_1205_42ba3b/"&gt; &lt;img alt="Aquif 3.5 Max 1205 (42B-A3B)" src="https://b.thumbs.redditmedia.com/NEG8P3hqBPVNg2ugj-hAZZUETF03fHVZR0b3FQKZD7g.jpg" title="Aquif 3.5 Max 1205 (42B-A3B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pgnj1q/aquif_35_max_1205_42ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgnjbo/aquif_35_max_1205_42ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgnjbo/aquif_35_max_1205_42ba3b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T17:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgio6a</id>
    <title>GPU acceleration on Ryzen AI 9 HX 370</title>
    <updated>2025-12-07T13:43:34+00:00</updated>
    <author>
      <name>/u/GarauGarau</name>
      <uri>https://old.reddit.com/user/GarauGarau</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm running a machine with the new Ryzen AI 9 HX 370 and 96GB of RAM on Windows 11.&lt;/p&gt; &lt;p&gt;I have a large dataset to process (~320k rows) and I'm trying to determine if it's possible for Ollama to utilize the Radeon 890M iGPU.&lt;/p&gt; &lt;p&gt;Current Status:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: qwen2.5:7b&lt;/li&gt; &lt;li&gt;Setup: Python script sending requests to Ollama with 2 parallel workers.&lt;/li&gt; &lt;li&gt;Performance: I am averaging 8.68s/it (seconds per iteration/row).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been trying to get it to work, but I'm curious if anyone else with this specific chip has actually succeeded in enabling hardware acceleration on Windows, or if we are currently limited to CPU-only inference due to driver/software maturity issues.&lt;/p&gt; &lt;p&gt;I attempted to force GPU usage via PowerShell environment variables using Gemini's advice, but the logs always show &lt;code&gt;inference compute id=cpu&lt;/code&gt; and &lt;code&gt;entering low vram mode&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I have tried the following configurations (restarting the server each time):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Standard Parallelism: $env:OLLAMA_NUM_PARALLEL = &amp;quot;4&amp;quot;&lt;/li&gt; &lt;li&gt;Forcing Vulkan: $env:OLLAMA_VULKAN = &amp;quot;1&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you got it working, determine if it's possible to get Ollama to utilize the Radeon 890M iGPU at this time with stable or preview drivers?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GarauGarau"&gt; /u/GarauGarau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgio6a/gpu_acceleration_on_ryzen_ai_9_hx_370/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgio6a/gpu_acceleration_on_ryzen_ai_9_hx_370/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgio6a/gpu_acceleration_on_ryzen_ai_9_hx_370/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T13:43:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgmbna</id>
    <title>Need opinion/help on my Memory System for LLM</title>
    <updated>2025-12-07T16:19:38+00:00</updated>
    <author>
      <name>/u/I_DiMooo</name>
      <uri>https://old.reddit.com/user/I_DiMooo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! I've been slowly learning and developing a LLM based on the character &lt;strong&gt;Cyn&lt;/strong&gt; from the series &amp;quot;Murder Drones&amp;quot;. My goal is to bring that silly robot to life someday but right now I'm developing her software controlled by an LLM.&lt;/p&gt; &lt;p&gt;I'm currently trying to figure out the &lt;em&gt;(hopefully)&lt;/em&gt; ideal &lt;strong&gt;memory system&lt;/strong&gt; for her. I've been developing this whole project with the help from ChatGPT, we've been brainstorming and we landed on an idea but I want to get some experienced peoples opinions before implementing it.&lt;/p&gt; &lt;p&gt;Cyn currently receives something I call &lt;strong&gt;&amp;quot;State Calls&amp;quot;&lt;/strong&gt; containing various world data and she responds with an array of &lt;strong&gt;&amp;quot;Executable Functions&amp;quot;&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example: {&amp;quot;finalized_speech&amp;quot;: &amp;quot;hi cyn&amp;quot;, &amp;quot;battery&amp;quot;: 80} ---&amp;gt; [&amp;quot;name&amp;quot;: &amp;quot;speak&amp;quot;, &amp;quot;params&amp;quot;: {&amp;quot;text&amp;quot;: &amp;quot;Hello&amp;quot;}]&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So the idea for the &lt;strong&gt;Memory System&lt;/strong&gt; is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;State Calls and Executable Functions are converted into easily readable information (finalized_speech would be: &amp;quot;User said smth&amp;quot;), this gets embedded and stored in recent_memories.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Every State Call will be analyzed and with embedding we will return some memories in &amp;quot;memory&amp;quot; variable within state call.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Every Minute/Hour/etc. a seperate summarizer model will make a minute/hour/etc. summary of the memories. These summary memories will simulate memory decays. We could store them as long-term memories after some point.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That is the base for the system. I am also thinking about making memory types and some memory storing system like cataloging the people she meets and other stuff like that, but right now I just want to land on a base that will make conversations with her have actual continuity, context and meaning.&lt;/p&gt; &lt;p&gt;I'd really appreciate the opinions and possible help with enhancing the idea for the system to make it as stable and lively as possible. If someone wants to help and needs some clarifications I'm happy to answer them!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_DiMooo"&gt; /u/I_DiMooo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgmbna/need_opinionhelp_on_my_memory_system_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgmbna/need_opinionhelp_on_my_memory_system_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgmbna/need_opinionhelp_on_my_memory_system_for_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T16:19:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgl21p</id>
    <title>DevCrew agent swarm for accelerating your software development</title>
    <updated>2025-12-07T15:28:24+00:00</updated>
    <author>
      <name>/u/The_Research_Ninja</name>
      <uri>https://old.reddit.com/user/The_Research_Ninja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Fam. A new version of DevCrew_s1 with 20 agents is available at &lt;a href="https://github.com/GSA-TTS/devCrew_s1"&gt;https://github.com/GSA-TTS/devCrew_s1&lt;/a&gt; . Key agents are: BluePrint writer, ADR/ASR writers, Backend Engineer, Frontend Engineer, Code Reviewer, QA tester, Security Auditor, System Architect, UX/UI designer. DevCrew allows your organization to at least bootstrap any new full-stack software project with design documents, codes, tests, and more. &lt;/p&gt; &lt;p&gt;Imagine giving DevCrew one good software program design document in plain language and the crew gives you back a well documented executable full-stack software program. 🚀 DevCrew_s1 means &amp;quot;Specification 1 of DevCrew&amp;quot; where workflows are rigidly structured while &amp;quot;Specification 2&amp;quot; workflows are more flexible. You may implement and deploy Specification 1 with Claude Code, Amazon Strands, or Crew AI. It would be lovely to see some of us implement DevCrew_s1 locally with Ollama. &lt;/p&gt; &lt;p&gt;My upcoming book about production-grade AI Agent Systems will give you more practical guidance on how to build your own production-grade AI agent teams. I'm also seeking reviewers for the beta version of the book. Any experts from Ollama, nVidia, AWS, etc are welcomed - please DM me for more details. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_Research_Ninja"&gt; /u/The_Research_Ninja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgl21p/devcrew_agent_swarm_for_accelerating_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgl21p/devcrew_agent_swarm_for_accelerating_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgl21p/devcrew_agent_swarm_for_accelerating_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T15:28:24+00:00</published>
  </entry>
</feed>
