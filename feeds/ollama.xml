<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-28T23:06:43+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1puskwn</id>
    <title>Writing custom code to connect to llm api via Ollama and mTLS?</title>
    <updated>2025-12-24T17:27:18+00:00</updated>
    <author>
      <name>/u/Patladjan1738</name>
      <uri>https://old.reddit.com/user/Patladjan1738</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I am pretty new to Ollama and wanted to test it out, but I'm not sure if it can support my use case.&lt;/p&gt; &lt;p&gt;I have my own setup of an LLM API, running on a private server and secured via mTLS, so not just an api key but an api Id, a secret password, and I have to send a certificate and private key file in the payload. &lt;/p&gt; &lt;p&gt;I want to set up tools like langflow and dyad, but they dont seem to easily support all my custom auth code with cert and private key files. &lt;/p&gt; &lt;p&gt;But langflow and dyad do easily connect to Ollama.&lt;/p&gt; &lt;p&gt;Now I am thinking of setting up Ollama as a proxy server, where I can easily connect tools to Ollama, then Ollama can basically run my custom Python code to connect to my private llm server.&lt;/p&gt; &lt;p&gt;Has anyone ever done this with Ollama? Does anyone know if it's possible? What part of the documentation should I look into to kick start my implementation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Patladjan1738"&gt; /u/Patladjan1738 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puskwn/writing_custom_code_to_connect_to_llm_api_via/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puskwn/writing_custom_code_to_connect_to_llm_api_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1puskwn/writing_custom_code_to_connect_to_llm_api_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T17:27:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu6sgl</id>
    <title>I built a native Go runtime to give local Llama 3 "Real Hands" (File System + Browser)</title>
    <updated>2025-12-23T22:17:40+00:00</updated>
    <author>
      <name>/u/AgencySpecific</name>
      <uri>https://old.reddit.com/user/AgencySpecific</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Frustration: Running DeepSeek V3 or Llama 3 locally via Ollama is amazing, but let's be honest: they are &amp;quot;Brains in Jars.&amp;quot;&lt;/p&gt; &lt;p&gt;They can write incredible code, but they can't save it. They can plan research, but they can't browse the docs. I got sick of the &amp;quot;Chat -&amp;gt; Copy Code -&amp;gt; Alt-Tab -&amp;gt; Paste -&amp;gt; Error&amp;quot; loop.&lt;/p&gt; &lt;p&gt;The Project (Runiq): I didn't want another fragile Python wrapper that breaks my venv every week. So I built a standalone MCP Server in Go.&lt;/p&gt; &lt;p&gt;What it actually does:&lt;/p&gt; &lt;p&gt;File System Access: You prompt: &amp;quot;Refactor the ./src folder.&amp;quot; Runiq actually reads the files, sends the context to Ollama, and applies the edits locally.&lt;/p&gt; &lt;p&gt;Stealth Browser: You prompt: &amp;quot;Check the docs at stripe.com.&amp;quot; It spins up a headless browser (bypassing Cloudflare) to give the model real-time context.&lt;/p&gt; &lt;p&gt;The &amp;quot;Air Gap&amp;quot; Firewall: Giving a local model root is scary. Runiq intercepts every write or delete syscall. You get a native OS popup to approve the action. It can't wipe your drive unless you say yes.&lt;/p&gt; &lt;p&gt;Why Go?&lt;/p&gt; &lt;p&gt;Speed: It's instant.&lt;/p&gt; &lt;p&gt;Portability: Single 12MB binary. No pip install, no Docker.&lt;/p&gt; &lt;p&gt;Safety: Memory safe and strictly typed.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/qaysSE/runiq"&gt;https://github.com/qaysSE/runiq&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I built this to turn my local Ollama setup into a fully autonomous agent. Let me know what you think of the architecture.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AgencySpecific"&gt; /u/AgencySpecific &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pu6sgl/i_built_a_native_go_runtime_to_give_local_llama_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pu6sgl/i_built_a_native_go_runtime_to_give_local_llama_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pu6sgl/i_built_a_native_go_runtime_to_give_local_llama_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-23T22:17:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1puzyql</id>
    <title>Meetaugust Scored 100% in USMLE : outperforming OpenAI‚Äôs GPT - 5 and Google MedPaLM 2.</title>
    <updated>2025-12-24T23:19:33+00:00</updated>
    <author>
      <name>/u/vaibhavs8</name>
      <uri>https://old.reddit.com/user/vaibhavs8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1puzyql/meetaugust_scored_100_in_usmle_outperforming/"&gt; &lt;img alt="Meetaugust Scored 100% in USMLE : outperforming OpenAI‚Äôs GPT - 5 and Google MedPaLM 2." src="https://b.thumbs.redditmedia.com/tNks7nCpb3Q-gpPoKucKZ6jijHDMBzY_lgT9GPbdZcY.jpg" title="Meetaugust Scored 100% in USMLE : outperforming OpenAI‚Äôs GPT - 5 and Google MedPaLM 2." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i spent 3 years building Meetaugust and published research on benchmarking health AI accuracy. The goal was simple: make reliable health guidance accessible to anyone.&lt;/p&gt; &lt;p&gt;I know there are a lots of symptom checkers and health apps out there but most are not safe. I wanted something safe and conversational just explain your symptoms naturally and get clear answers.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;p&gt;* Analyzes symptoms through natural conversation (no checkboxes)&lt;/p&gt; &lt;p&gt;* Explains lab reports and prescriptions in simple terms&lt;/p&gt; &lt;p&gt;* Works in multiple languages via WhatsApp also (photos, voice, text)&lt;/p&gt; &lt;p&gt;* Helps determine if something needs urgent attention&lt;/p&gt; &lt;p&gt;* Stores your medical history as a &amp;quot;second brain&amp;quot;&lt;/p&gt; &lt;p&gt;* Available 24/7 for health questions&lt;/p&gt; &lt;p&gt;It won't prescribe medicines it's meant to help you understand your health and know when to see a doctor. We achieved 81.8% diagnostic accuracy in our research testing across 400 clinical cases.&lt;/p&gt; &lt;p&gt;free if anyone wants to try it : &lt;a href="https://www.meetaugust.ai/"&gt;https://www.meetaugust.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs8"&gt; /u/vaibhavs8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1puzyql"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puzyql/meetaugust_scored_100_in_usmle_outperforming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1puzyql/meetaugust_scored_100_in_usmle_outperforming/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T23:19:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvgxr4</id>
    <title>Holiday Promo: Perplexity AI PRO Offer | 95% Cheaper!</title>
    <updated>2025-12-25T16:30:26+00:00</updated>
    <author>
      <name>/u/A2uniquenickname</name>
      <uri>https://old.reddit.com/user/A2uniquenickname</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pvgxr4/holiday_promo_perplexity_ai_pro_offer_95_cheaper/"&gt; &lt;img alt="Holiday Promo: Perplexity AI PRO Offer | 95% Cheaper!" src="https://preview.redd.it/e81patkcnd9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a149e002d4d1aba0004c26bf419b2b4c70699453" title="Holiday Promo: Perplexity AI PRO Offer | 95% Cheaper!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get Perplexity AI PRO (1-Year) ‚Äì at 90% OFF!&lt;/p&gt; &lt;p&gt;Order here: &lt;a href="https://cheapgpts.store/Perplexity"&gt;CHEAPGPT.STORE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Plan: 12 Months&lt;/p&gt; &lt;p&gt;üí≥ Pay with: PayPal or Revolut or your favorite payment method&lt;/p&gt; &lt;p&gt;Reddit reviews: &lt;a href="https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu"&gt;FEEDBACK POST&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TrustPilot: &lt;a href="https://www.trustpilot.com/review/cheapgpt.store"&gt;TrustPilot FEEDBACK&lt;/a&gt; &lt;/p&gt; &lt;p&gt;NEW YEAR BONUS: Apply code PROMO5 for extra discount OFF your order! &lt;/p&gt; &lt;p&gt;BONUS!: Enjoy the AI Powered automated web browser. (Presented by Perplexity) included WITH YOUR PURCHASE!&lt;/p&gt; &lt;p&gt;Trusted and the cheapest! Check all feedbacks before you purchase&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A2uniquenickname"&gt; /u/A2uniquenickname &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e81patkcnd9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pvgxr4/holiday_promo_perplexity_ai_pro_offer_95_cheaper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pvgxr4/holiday_promo_perplexity_ai_pro_offer_95_cheaper/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T16:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pugkbg</id>
    <title>Self Hosted Alternative to NotebookLM</title>
    <updated>2025-12-24T06:28:53+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pugkbg/self_hosted_alternative_to_notebooklm/"&gt; &lt;img alt="Self Hosted Alternative to NotebookLM" src="https://external-preview.redd.it/VQoBiFueOCMY1op6qhV-TxY7TpiBx_VDJmILmMOmfX0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ea68aad29b25cc93508c57524884674c64e162b" title="Self Hosted Alternative to NotebookLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1pugkbg/video/939ag7c3j39g1/player"&gt;https://reddit.com/link/1pugkbg/video/939ag7c3j39g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be one of the open-source alternative to NotebookLM but connected to extra data sources.&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here's a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep Agent with Built-in Tools (knowledge base search, podcast generation, web scraping, link previews, image display)&lt;/li&gt; &lt;li&gt;Note Management (Notion like)&lt;/li&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi Collaborative Chats&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Installation (Self-Host)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Linux/macOS:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 \ -v surfsense-data:/data \ --name surfsense \ --restart unless-stopped \ ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Windows (PowerShell):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 ` -v surfsense-data:/data ` --name surfsense ` --restart unless-stopped ` ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pugkbg/self_hosted_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pugkbg/self_hosted_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pugkbg/self_hosted_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T06:28:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pveom2</id>
    <title>Interesting...</title>
    <updated>2025-12-25T14:39:22+00:00</updated>
    <author>
      <name>/u/EggDroppedSoup</name>
      <uri>https://old.reddit.com/user/EggDroppedSoup</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pveom2/interesting/"&gt; &lt;img alt="Interesting..." src="https://preview.redd.it/v6zdqwxh3d9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0e4fc9a05bc15c73b2018a2cf6767be32fe277b" title="Interesting..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EggDroppedSoup"&gt; /u/EggDroppedSoup &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v6zdqwxh3d9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pveom2/interesting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pveom2/interesting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T14:39:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv9vra</id>
    <title>What models are compatible with the Goose agent?</title>
    <updated>2025-12-25T09:38:39+00:00</updated>
    <author>
      <name>/u/Ok_Imagination_1571</name>
      <uri>https://old.reddit.com/user/Ok_Imagination_1571</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I installed goose-cli 1.18.0 and ollama 0.12.6.&lt;/p&gt; &lt;p&gt;Goose configuration has an option for local ollama provider.&lt;/p&gt; &lt;p&gt;Goose definitely connects to ollama server and shows list of models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;NAME ID SIZE MODIFIED qwen2.5:0.5b a8b0c5157701 397 MB 12 minutes ago deepseek-r1:8b 6995872bfe4c 5.2 GB 11 hours ago phi3:mini 4f2222927938 2.2 GB 14 hours ago There are 3 models on the list but Goose is only able to complete the configuration successfully with Qwen. Is there a criteria to check compatibility of an ollama model upfront with Goose without wasting traffic and time? &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Imagination_1571"&gt; /u/Ok_Imagination_1571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv9vra/what_models_are_compatible_with_the_goose_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv9vra/what_models_are_compatible_with_the_goose_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pv9vra/what_models_are_compatible_with_the_goose_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T09:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pulykd</id>
    <title>Qwen3:4b Too Many Model thoughts to respond to a simple "hi"</title>
    <updated>2025-12-24T12:10:03+00:00</updated>
    <author>
      <name>/u/slow-fast-person</name>
      <uri>https://old.reddit.com/user/slow-fast-person</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pulykd/qwen34b_too_many_model_thoughts_to_respond_to_a/"&gt; &lt;img alt="Qwen3:4b Too Many Model thoughts to respond to a simple &amp;quot;hi&amp;quot;" src="https://preview.redd.it/nfzkw0ex759g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8cdeebcba38e88484dad342114e070ce2c9b6c93" title="Qwen3:4b Too Many Model thoughts to respond to a simple &amp;quot;hi&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is quite hilarious on how the model does not have adaptive chain of thought and puts so much work in something as simple as a &amp;quot;hi&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slow-fast-person"&gt; /u/slow-fast-person &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nfzkw0ex759g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pulykd/qwen34b_too_many_model_thoughts_to_respond_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pulykd/qwen34b_too_many_model_thoughts_to_respond_to_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T12:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvkm0h</id>
    <title>Local LLMs unstable</title>
    <updated>2025-12-25T19:19:25+00:00</updated>
    <author>
      <name>/u/OcelotOk5761</name>
      <uri>https://old.reddit.com/user/OcelotOk5761</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pvkm0h/local_llms_unstable/"&gt; &lt;img alt="Local LLMs unstable" src="https://b.thumbs.redditmedia.com/2yKwQ8y3RFiqDiG8AS4WIn9Y6uYD_O398t8q4oQvGck.jpg" title="Local LLMs unstable" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've been having problems with local LLms recently. I cannot tell if its an ollama issue or specifically an open-webui issue.&lt;/p&gt; &lt;p&gt;Firstly: Some of the models are very buggy, take almost a minute to process and are having problems returning outputs specifically with Qwen3-14B or any 'thinking' model in-fact. they take ages to load (even on GPU) and begin processing. when they do, the model sometimes keeps getting stuck in thinking loops or outright refuses to unload when asked to.&lt;/p&gt; &lt;p&gt;Second: When trying out Qwen3-vl from Ollama even with all the updates and when used in open-webui, the model is outright unusable for me, it either keeps thinking forever or refuses to load, or even refuses to unload making me have to open the terminal to kill with sudo. Rinse and repeat.&lt;/p&gt; &lt;p&gt;Has anyone been having problems recently or is it just me? I am running open-webui through pip (I don't like docker) and it's been very frustrating to use. I really don't know if it's an ollama issue or an open-webui issue.&lt;/p&gt; &lt;p&gt;P.S: I am using Linux (not sure if it's a Linux issue or not)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/30mxo17gff9g1.png?width=494&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0bb43d89fb8b99e213534f9ae1d9e6e42e2fc9e"&gt;https://preview.redd.it/30mxo17gff9g1.png?width=494&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0bb43d89fb8b99e213534f9ae1d9e6e42e2fc9e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nice one man. Idk what to even say. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OcelotOk5761"&gt; /u/OcelotOk5761 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pvkm0h/local_llms_unstable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pvkm0h/local_llms_unstable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pvkm0h/local_llms_unstable/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T19:19:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvfv1h</id>
    <title>Distributed Cognition and Context Control: gait and gaithub</title>
    <updated>2025-12-25T15:39:14+00:00</updated>
    <author>
      <name>/u/automateyournetwork</name>
      <uri>https://old.reddit.com/user/automateyournetwork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pvfv1h/distributed_cognition_and_context_control_gait/"&gt; &lt;img alt="Distributed Cognition and Context Control: gait and gaithub" src="https://external-preview.redd.it/QboS9f2QUI2oU5tnJHS_qXaoU7qrXkIcqyb5tZ65XrE.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=181344eb7ee38bf3d107b3dca75beba6ca77d8c1" title="Distributed Cognition and Context Control: gait and gaithub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the last few weeks, I‚Äôve been building - and just finished demoing - something I think we‚Äôre going to look back on as obvious in hindsight. &lt;/p&gt; &lt;p&gt;Distributed Cognition. Decentralized context control. &lt;/p&gt; &lt;p&gt;GAIT + GaitHub &lt;/p&gt; &lt;p&gt;A Git-like system ‚Äî but not for code. &lt;/p&gt; &lt;p&gt;For AI reasoning, memory, and context. &lt;/p&gt; &lt;p&gt;We‚Äôve spent decades perfecting how we:&lt;br /&gt; ‚Ä¢ version code&lt;br /&gt; ‚Ä¢ review changes&lt;br /&gt; ‚Ä¢ collaborate safely&lt;br /&gt; ‚Ä¢ reproduce results &lt;/p&gt; &lt;p&gt;And yet today, we let LLMs:&lt;br /&gt; ‚Ä¢ make architectural decisions&lt;br /&gt; ‚Ä¢ generate production content&lt;br /&gt; ‚Ä¢ influence real systems&lt;br /&gt; ‚Ä¶with almost no version control at all. &lt;/p&gt; &lt;p&gt;Chat logs aren‚Äôt enough. &lt;/p&gt; &lt;p&gt;Prompt files aren‚Äôt enough. &lt;/p&gt; &lt;p&gt;Screenshots definitely aren‚Äôt enough. &lt;/p&gt; &lt;p&gt;So I built something different. &lt;/p&gt; &lt;p&gt;What GAIT actually versions &lt;/p&gt; &lt;p&gt;GAIT treats AI interactions as first-class, content-addressed objects. &lt;/p&gt; &lt;p&gt;That includes:&lt;br /&gt; ‚Ä¢ user intent&lt;br /&gt; ‚Ä¢ model responses&lt;br /&gt; ‚Ä¢ memory state&lt;br /&gt; ‚Ä¢ branches of reasoning&lt;br /&gt; ‚Ä¢ resumable conversations &lt;/p&gt; &lt;p&gt;Every turn is hashed. Every decision is traceable. Every outcome is reproducible. &lt;/p&gt; &lt;p&gt;If Git solved ‚Äúit worked on my machine,‚Äù &lt;/p&gt; &lt;p&gt;GAIT solves ‚Äúwhy did the AI decide that?‚Äù &lt;/p&gt; &lt;p&gt;The demo (high-level walkthrough) &lt;/p&gt; &lt;p&gt;I recorded a full end-to-end demo showing how this works in practice: &lt;/p&gt; &lt;p&gt;Start in a clean folder ‚Äî no server, no UI &lt;/p&gt; &lt;p&gt;* Initialize GAIT locally&lt;br /&gt; * Run an AI chat session that‚Äôs automatically tracked&lt;br /&gt; * Ask a real, non-trivial technical question&lt;br /&gt; * Inspect the reasoning log&lt;br /&gt; * Resume the conversation later ‚Äî exactly where it left off&lt;br /&gt; * Branch the reasoning into alternate paths&lt;br /&gt; * Verify object integrity and state&lt;br /&gt; * Add a remote (GaitHub)&lt;br /&gt; * Create a remote repo from the CLI&lt;br /&gt; * Authenticate with a simple token&lt;br /&gt; * Push AI reasoning to the cloud&lt;br /&gt; * Fork another repo‚Äôs reasoning&lt;br /&gt; * Open a pull request on ideas, not code&lt;br /&gt; * Merge reasoning deterministically &lt;/p&gt; &lt;p&gt;No magic. No hidden state. No ‚Äútrust me, the model said so.‚Äù &lt;/p&gt; &lt;p&gt;Why this matters (especially for enterprises). AI is no longer a toy. &lt;/p&gt; &lt;p&gt;It‚Äôs:&lt;br /&gt; ‚Ä¢ part of decision pipelines&lt;br /&gt; ‚Ä¢ embedded in workflows&lt;br /&gt; ‚Ä¢ influencing customers, networks, and systems &lt;/p&gt; &lt;p&gt;But we can‚Äôt:&lt;br /&gt; ‚Ä¢ audit it&lt;br /&gt; ‚Ä¢ diff it&lt;br /&gt; ‚Ä¢ reproduce it&lt;br /&gt; ‚Ä¢ roll it back &lt;/p&gt; &lt;p&gt;That‚Äôs not sustainable. &lt;/p&gt; &lt;p&gt;GAIT introduces:&lt;br /&gt; ‚Ä¢ reproducible AI workflows&lt;br /&gt; ‚Ä¢ auditable reasoning history&lt;br /&gt; ‚Ä¢ collaborative cognition&lt;br /&gt; ‚Ä¢ local-first, cloud-optional design &lt;/p&gt; &lt;p&gt;This is infrastructure ‚Äî not a chatbot wrapper. This is not ‚ÄúGitHub for prompts‚Äù. That framing misses the point. &lt;/p&gt; &lt;p&gt;This is Git for cognition. &lt;/p&gt; &lt;p&gt;From:&lt;br /&gt; ‚Ä¢ commits ‚Üí conversations&lt;br /&gt; ‚Ä¢ diffs ‚Üí decisions&lt;br /&gt; ‚Ä¢ branches ‚Üí alternate reasoning&lt;br /&gt; ‚Ä¢ merges ‚Üí shared understanding &lt;/p&gt; &lt;p&gt;I genuinely believe version control for AI reasoning will become as fundamental as version control for source code. &lt;/p&gt; &lt;p&gt;The question isn‚Äôt if. &lt;/p&gt; &lt;p&gt;It‚Äôs who builds it correctly. &lt;/p&gt; &lt;p&gt;I‚Äôm excited to keep pushing this forward ‚Äî openly, transparently, and with the community. &lt;/p&gt; &lt;p&gt;More demos, docs, and real-world use cases coming soon. &lt;/p&gt; &lt;p&gt;If this resonates with you, I‚Äôd love to hear your thoughts üëá&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/automateyournetwork"&gt; /u/automateyournetwork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=0PyFHsYxjbk&amp;amp;si=ugLwYfnV_ETZ_VSR"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pvfv1h/distributed_cognition_and_context_control_gait/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pvfv1h/distributed_cognition_and_context_control_gait/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T15:39:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv33vq</id>
    <title>Llama 3.2 refuses to analyze dark web threat intel. Need uncensored 7B recommendations</title>
    <updated>2025-12-25T02:19:49+00:00</updated>
    <author>
      <name>/u/Loud-Goal190</name>
      <uri>https://old.reddit.com/user/Loud-Goal190</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm crawling onion sites for a defensive threat intel tool, but my local LLM (Llama 3.2) refuses to analyze the raw text due to safety filters. It sees &amp;quot;leak&amp;quot; or &amp;quot;.onion&amp;quot; and shuts down, even with jailbreak prompts. Regex captures emails but misses the context (like company names or data volume). Any recommendations for an uncensored 7B model that handles this well, or should I switch to a BERT model for extraction?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud-Goal190"&gt; /u/Loud-Goal190 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv33vq/llama_32_refuses_to_analyze_dark_web_threat_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv33vq/llama_32_refuses_to_analyze_dark_web_threat_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pv33vq/llama_32_refuses_to_analyze_dark_web_threat_intel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T02:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv71pg</id>
    <title>Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16.</title>
    <updated>2025-12-25T06:23:45+00:00</updated>
    <author>
      <name>/u/Double-Primary-2871</name>
      <uri>https://old.reddit.com/user/Double-Primary-2871</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pv71pg/finetuning_gptoss20b_on_a_ryzen_5950x_because/"&gt; &lt;img alt="Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16." src="https://preview.redd.it/6w9h1554na9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2621fb072eeb16ce8a0f4599cdbbfeb44b9f1c90" title="Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;at 1am.&lt;/p&gt; &lt;p&gt;I am fine-tuning my personal AI, into a gpt-oss-20b model, via LoRA, on a Ryzen 5950x CPU.&lt;/p&gt; &lt;p&gt;I had to pain stakingly deal with massive axolotl errors, venv and python version hell, yaml misconfigs, even fought with my other ai assistant, whom literally told me this couldn't be done on my system.... for hours and hours, for over a week.&lt;/p&gt; &lt;p&gt;Can't fine-tune with my radeon 7900XT because of bf16 kernel issues with ROCm on axolotl. I literally even tried to rent an h100 to help, and ran into serious roadblocks.&lt;/p&gt; &lt;p&gt;So the soultion was for me to convert the mxfp4 (bf16 format) weights back to fp32 and tell axolotl to stop downcasting back fp16.&lt;/p&gt; &lt;p&gt;Sure this will take days to compute all three of the shards, but after days of banging my head against the nearest convenient wall and keyboard, I finally got this s-o-b to work.&lt;/p&gt; &lt;p&gt;üòÅ also hi, new here. just wanted to share my story.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Double-Primary-2871"&gt; /u/Double-Primary-2871 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6w9h1554na9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv71pg/finetuning_gptoss20b_on_a_ryzen_5950x_because/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pv71pg/finetuning_gptoss20b_on_a_ryzen_5950x_because/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T06:23:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv88yv</id>
    <title>I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier models</title>
    <updated>2025-12-25T07:44:21+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pv88yv/i_built_planoa3b_most_efficient_llms_for_agent/"&gt; &lt;img alt="I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier models" src="https://preview.redd.it/059dttgf1b9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=898f1ffead5f76ea7c739cebaf5d8c1a413e3efc" title="I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone ‚Äî I‚Äôm on the Katanemo research team. Today we‚Äôre thrilled to launch &lt;strong&gt;Plano-Orchestrator&lt;/strong&gt;, a new family of LLMs built for fast multi-agent orchestration.&lt;/p&gt; &lt;p&gt;What do these new LLMs do? given a user request and the conversation context, Plano-Orchestrator decides which agent(s) should handle the request and in what sequence. In other words, it acts as the supervisor agent in a multi-agent system. Designed for multi-domain scenarios, it works well across general chat, coding tasks, and long, multi-turn conversations, while staying efficient enough for low-latency production deployments.&lt;/p&gt; &lt;p&gt;Why did we built this? Our applied research is focused on helping teams deliver agents safely and efficiently, with better real-world performance and latency ‚Äî the kind of ‚Äúglue work‚Äù that usually sits outside any single agent‚Äôs core product logic.&lt;/p&gt; &lt;p&gt;Plano-Orchestrator is integrated into Plano, our models-native proxy and dataplane for agents. Hope you enjoy it ‚Äî and we‚Äôd love feedback from anyone building multi-agent systems&lt;/p&gt; &lt;p&gt;Learn more about the LLMs &lt;a href="https://huggingface.co/collections/katanemo/plano-orchestrator"&gt;here&lt;/a&gt;&lt;br /&gt; About our open source project: &lt;a href="https://github.com/katanemo/plano"&gt;https://github.com/katanemo/plano&lt;/a&gt;&lt;br /&gt; And about our research: &lt;a href="https://planoai.dev/research"&gt;https://planoai.dev/research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/059dttgf1b9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv88yv/i_built_planoa3b_most_efficient_llms_for_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pv88yv/i_built_planoa3b_most_efficient_llms_for_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T07:44:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvyead</id>
    <title>Local AI for a game's npc and enemy</title>
    <updated>2025-12-26T07:19:56+00:00</updated>
    <author>
      <name>/u/MeuHorizon</name>
      <uri>https://old.reddit.com/user/MeuHorizon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm a junior game developer, and in Friendslop games, mimics or enemy behaviors fundamentally change the game. I'm thinking of running a local AI within the game for enemy behaviors and for mimics to mimic sounds and dialogue. But if I do this, what will the minimum system requirements be? 5090? Would a local AI that can use at least 2GB VRAM be too dumb?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MeuHorizon"&gt; /u/MeuHorizon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pvyead/local_ai_for_a_games_npc_and_enemy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pvyead/local_ai_for_a_games_npc_and_enemy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pvyead/local_ai_for_a_games_npc_and_enemy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-26T07:19:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw7hbt</id>
    <title>Offline vector DB experiment ‚Äî anyone want to test on their local setup?</title>
    <updated>2025-12-26T15:54:08+00:00</updated>
    <author>
      <name>/u/Serious-Section-5595</name>
      <uri>https://old.reddit.com/user/Serious-Section-5595</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pw7hbt/offline_vector_db_experiment_anyone_want_to_test/"&gt; &lt;img alt="Offline vector DB experiment ‚Äî anyone want to test on their local setup?" src="https://external-preview.redd.it/h_ijte2Ftf_QhAPNCeF0zmImOsXcFk4zfiB954lTuNI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11d4df01f4782cc60aae4d5e976779e127df8a7c" title="Offline vector DB experiment ‚Äî anyone want to test on their local setup?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I‚Äôve been building a small &lt;strong&gt;offline-first vector database&lt;/strong&gt; for local AI workflows. No cloud, no services just files on disk.&lt;/p&gt; &lt;p&gt;I made a universal benchmark script that adjusts dataset size based on your RAM so it doesn‚Äôt nuke laptops (100k vectors did that to me once üòÖ).&lt;/p&gt; &lt;p&gt;If you want to test it locally, here‚Äôs the script:&lt;br /&gt; &lt;a href="https://github.com/Srinivas26k/srvdb/blob/master/universal_benchmark.py"&gt;https://github.com/Srinivas26k/srvdb/blob/master/universal_benchmark.py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any feedback, issues, or benchmark results would help a lot.&lt;/p&gt; &lt;p&gt;Repo stars and contributions are also welcome if you find it useful&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious-Section-5595"&gt; /u/Serious-Section-5595 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pw7hbt/offline_vector_db_experiment_anyone_want_to_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pw7hbt/offline_vector_db_experiment_anyone_want_to_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pw7hbt/offline_vector_db_experiment_anyone_want_to_test/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-26T15:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwbor6</id>
    <title>How to use open source model in Antigravity ?</title>
    <updated>2025-12-26T18:47:30+00:00</updated>
    <author>
      <name>/u/One_Pianist8404</name>
      <uri>https://old.reddit.com/user/One_Pianist8404</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to integrate a self-hosted open-source LLM into Antigravity, is it possible ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One_Pianist8404"&gt; /u/One_Pianist8404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pwbor6/how_to_use_open_source_model_in_antigravity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pwbor6/how_to_use_open_source_model_in_antigravity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pwbor6/how_to_use_open_source_model_in_antigravity/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-26T18:47:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwarn1</id>
    <title>jailbreaks or uncensored models?</title>
    <updated>2025-12-26T18:09:31+00:00</updated>
    <author>
      <name>/u/United_Ad8618</name>
      <uri>https://old.reddit.com/user/United_Ad8618</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a site that has more up to date jailbreaks or uncensored models? All the jailbreaks or uncensored models I've found are for porn essentially, not much for other use cases like security work, and the old jailbreaks don't seem to work on claude anymore&lt;/p&gt; &lt;p&gt;Side note: is it worth using grok for this reason?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United_Ad8618"&gt; /u/United_Ad8618 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pwarn1/jailbreaks_or_uncensored_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pwarn1/jailbreaks_or_uncensored_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pwarn1/jailbreaks_or_uncensored_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-26T18:09:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwyuji</id>
    <title>I built a GraphRAG application to visualize AI knowledge (Runs 100% Local via Ollama OR Fast via Gemini API)</title>
    <updated>2025-12-27T14:17:08+00:00</updated>
    <author>
      <name>/u/Dev-it-with-me</name>
      <uri>https://old.reddit.com/user/Dev-it-with-me</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dev-it-with-me"&gt; /u/Dev-it-with-me &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1pwyu6k/i_built_a_graphrag_application_to_visualize_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pwyuji/i_built_a_graphrag_application_to_visualize_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pwyuji/i_built_a_graphrag_application_to_visualize_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-27T14:17:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwusyo</id>
    <title>best model to run on a 5080 laptop with intel ultra i9 and 64gb of ram on linux mainly for beginner coding?</title>
    <updated>2025-12-27T10:33:09+00:00</updated>
    <author>
      <name>/u/Subject_Swimming6327</name>
      <uri>https://old.reddit.com/user/Subject_Swimming6327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i was suggested mistral and qwen and of course have tried deepseek, just wondering if anyone had any specific suggestions for my setup. im a total beginner. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Subject_Swimming6327"&gt; /u/Subject_Swimming6327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pwusyo/best_model_to_run_on_a_5080_laptop_with_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pwusyo/best_model_to_run_on_a_5080_laptop_with_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pwusyo/best_model_to_run_on_a_5080_laptop_with_intel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-27T10:33:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxi5sp</id>
    <title>AI driven physical product inspection</title>
    <updated>2025-12-28T04:28:50+00:00</updated>
    <author>
      <name>/u/onemorequickchange</name>
      <uri>https://old.reddit.com/user/onemorequickchange</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An order is filled with physical products. Groceries. Products are delivered. A camera captures the products as they are carried on board. What are the challenges woth AI identifying missed products and communicating with vendor to solve rhe issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onemorequickchange"&gt; /u/onemorequickchange &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxi5sp/ai_driven_physical_product_inspection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxi5sp/ai_driven_physical_product_inspection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pxi5sp/ai_driven_physical_product_inspection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T04:28:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxxi8n</id>
    <title>Ollama running on CPU instead of GPU on a Proxmox VM with PCI Bridge</title>
    <updated>2025-12-28T17:49:21+00:00</updated>
    <author>
      <name>/u/Lumaric_</name>
      <uri>https://old.reddit.com/user/Lumaric_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pxxi8n/ollama_running_on_cpu_instead_of_gpu_on_a_proxmox/"&gt; &lt;img alt="Ollama running on CPU instead of GPU on a Proxmox VM with PCI Bridge" src="https://b.thumbs.redditmedia.com/9ttGWiGRO9VQuNJv4Spw_kKfewl9wLi3xkDfcd-VlwM.jpg" title="Ollama running on CPU instead of GPU on a Proxmox VM with PCI Bridge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt; &lt;p&gt;I am looking for help on a specific situation since my configuration is a bit special. I have an computer on the side that i use has a server with Proxmox installed on it. I mainly made with all component of my main PC with special modification. CPU Ryzen 9 5900X, 128Go RAM DDR4 and RX 6700 XT.&lt;/p&gt; &lt;p&gt;I created a Virtual machine with a PCI bridge to the graphic card in the objectif of hosting a self-hosted model, i managed to done it after a lot of work but now the VM correctly detected the graphic and i can see the default terminal interface of debian from an HDMI port.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f4ap5p21dz9g1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92213a024b7b3e2578b02125aa98cb52052455e3"&gt;https://preview.redd.it/f4ap5p21dz9g1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92213a024b7b3e2578b02125aa98cb52052455e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After that a installed ollama and i got the message &amp;quot;AMD GPU ready&amp;quot; indicating that the GPU was correcly detected.&lt;/p&gt; &lt;p&gt;So i took my time to configure everything else like WebUi, but at the moment of running a model, it need 20sec just to respond to a &amp;quot;Bonjour&amp;quot; ( yeah i from France ), i tried different model thinking it was just a model not adapted but same problem.&lt;/p&gt; &lt;p&gt;So i check with ollama ps and i see that all model is running on the CPU :&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5ixofzncfz9g1.png?width=819&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2836d36922f6924244fde627df020601bb19c032"&gt;https://preview.redd.it/5ixofzncfz9g1.png?width=819&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2836d36922f6924244fde627df020601bb19c032&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Does anyone know, if i could have made a misstake during the configuration or if i missing a configuration. I tried to reinstall the AMD Gpu Driver from the link on the Ollama Doc linux page. Shoud i try to use Vulkan ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lumaric_"&gt; /u/Lumaric_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxxi8n/ollama_running_on_cpu_instead_of_gpu_on_a_proxmox/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxxi8n/ollama_running_on_cpu_instead_of_gpu_on_a_proxmox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pxxi8n/ollama_running_on_cpu_instead_of_gpu_on_a_proxmox/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T17:49:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxr2ws</id>
    <title>How to get started with automated workflows?</title>
    <updated>2025-12-28T13:14:38+00:00</updated>
    <author>
      <name>/u/PlastikHateAccount</name>
      <uri>https://old.reddit.com/user/PlastikHateAccount</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there, I'm interested how you guys set up ollama to work on tasks.&lt;/p&gt; &lt;p&gt;The first thing we already tried is having a Python script that calls the company internal Ollama via api with simple tasks in a loop. Imagine pseudocode:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for sourcecode in repository: api-call-to-ollama(&amp;quot;Please do a sourcecode review: &amp;quot; + sourcecode) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We tried multiple tasks like this for &lt;strong&gt;multiple usecases, not just sourcecode reviews&lt;/strong&gt; and the intelligence is quite promising but ofc the context the LLMs have available to solve tasks like that limiting.&lt;/p&gt; &lt;p&gt;So the second idea is to somehow let the LLM make the decision what to include in a prompt. Let's call them &amp;quot;pretasks&amp;quot;.&lt;/p&gt; &lt;p&gt;This pretask could be a prompt saying ¬¥&amp;quot;Write a prompt to an LLM to do a sourcecode review. You can decide to include adjacent PDFs, Jira tickets, pieces of sourcecode by writing &amp;lt;include:filename&amp;gt;&amp;quot; + list-of-available-files-with-descriptions-what-they-are¬¥. The python script would then parse the result of the pretask to collect the relevant files.&lt;/p&gt; &lt;p&gt;Third and finally, at that point we could let the pretask trigger itself even more pretasks. This is where the thing would be almost bootstrapped. But I'm out of ideas how to coordinate this, prevent endless loops etc.&lt;/p&gt; &lt;p&gt;Sorry if my thoughts around this whole topic are a little scattered. I assume the whole world is right now thinking about these kinds of workflows. So I'd like to know where to start reading about it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PlastikHateAccount"&gt; /u/PlastikHateAccount &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxr2ws/how_to_get_started_with_automated_workflows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxr2ws/how_to_get_started_with_automated_workflows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pxr2ws/how_to_get_started_with_automated_workflows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T13:14:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1py50nq</id>
    <title>I updated ollama and now it uses cpu &amp; system ram instead of my gpu</title>
    <updated>2025-12-28T22:49:09+00:00</updated>
    <author>
      <name>/u/NewDildos</name>
      <uri>https://old.reddit.com/user/NewDildos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using a few different models for a while in powershell and without thinking I updated ollama to download a new model. My prompt eval rate went from 2887.53 tokens/s to 8.25 and my eval rate went from 31.91 tokens/s to 4.7 A little over 50s for a 200 word output test. I'm using a 4060ti 16gb and would like to know how to change the settings to run on my gpu again. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewDildos"&gt; /u/NewDildos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1py50nq/i_updated_ollama_and_now_it_uses_cpu_system_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1py50nq/i_updated_ollama_and_now_it_uses_cpu_system_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1py50nq/i_updated_ollama_and_now_it_uses_cpu_system_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T22:49:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxqi11</id>
    <title>Ollama Model which Suits for my System</title>
    <updated>2025-12-28T12:43:52+00:00</updated>
    <author>
      <name>/u/devil__6996</name>
      <uri>https://old.reddit.com/user/devil__6996</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven‚Äôt downloaded these models yet and want to understand real-world experience before pulling them locally.&lt;/p&gt; &lt;p&gt;Hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RTX 4050 (6GB VRAM)&lt;/li&gt; &lt;li&gt;32GB RAM&lt;/li&gt; &lt;li&gt;Ryzen 7 7000 series&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Use case:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vibe coding&lt;/li&gt; &lt;li&gt;Code generation&lt;/li&gt; &lt;li&gt;Building software applications&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;- Web UI via Ollama (Open WebUI or similar)&lt;br /&gt; -For Cybersecurity Code generations etc,,,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devil__6996"&gt; /u/devil__6996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxqi11/ollama_model_which_suits_for_my_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxqi11/ollama_model_which_suits_for_my_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pxqi11/ollama_model_which_suits_for_my_system/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T12:43:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1py1odn</id>
    <title>Old server for local models</title>
    <updated>2025-12-28T20:33:03+00:00</updated>
    <author>
      <name>/u/Jacobmicro</name>
      <uri>https://old.reddit.com/user/Jacobmicro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ended up with an old poweredge r610 with the dual xeon chips and 192gb of ram. Everything is in good working order. Debating go trying to see if I could hack together something to run local models that could automate some of the work I used to pay API keys for with my work. &lt;/p&gt; &lt;p&gt;Anybody ever have any luck using older architecture? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jacobmicro"&gt; /u/Jacobmicro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1py1odn/old_server_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1py1odn/old_server_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1py1odn/old_server_for_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T20:33:03+00:00</published>
  </entry>
</feed>
