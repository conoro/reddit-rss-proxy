<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-30T20:06:55+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1nsy9q2</id>
    <title>Building Real Local AI Agents w/ Braintrust served off Ollama Experiments and Lessons Learned</title>
    <updated>2025-09-28T20:09:23+00:00</updated>
    <author>
      <name>/u/AIForOver50Plus</name>
      <uri>https://old.reddit.com/user/AIForOver50Plus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im using on my local dev rig GPT-OSS:120b served up on Ollama and I wanted to see evals and observability with those local models and frontier models so I ran a few experiments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Experiment Alpha:&lt;/strong&gt; Email Management Agent → lessons on modularity, logging, brittleness.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Experiment Bravo:&lt;/strong&gt; Turning logs into automated evaluations → catching regressions + selective re-runs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Next up:&lt;/strong&gt; model swapping, continuous regression tests, and human-in-the-loop feedback.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This isn’t theory. It’s running code + experiments you can check out here:&lt;br /&gt; 👉 &lt;a href="https://go.fabswill.com/braintrustdeepdive"&gt;https://go.fabswill.com/braintrustdeepdive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’d love feedback from this community — especially on failure modes or additional evals to add. What would &lt;em&gt;you&lt;/em&gt; test next?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIForOver50Plus"&gt; /u/AIForOver50Plus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsy9q2/building_real_local_ai_agents_w_braintrust_served/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsy9q2/building_real_local_ai_agents_w_braintrust_served/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nsy9q2/building_real_local_ai_agents_w_braintrust_served/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T20:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nslgop</id>
    <title>best LLM for reasoning and analysis</title>
    <updated>2025-09-28T11:00:35+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;which is the best model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nslgop/best_llm_for_reasoning_and_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nslgop/best_llm_for_reasoning_and_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nslgop/best_llm_for_reasoning_and_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T11:00:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsfdxs</id>
    <title>ArchGW 🚀 - Use Ollama-based LLMs with Anthropic client (release 0.3.13)</title>
    <updated>2025-09-28T04:38:00+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nsfdxs/archgw_use_ollamabased_llms_with_anthropic_client/"&gt; &lt;img alt="ArchGW 🚀 - Use Ollama-based LLMs with Anthropic client (release 0.3.13)" src="https://preview.redd.it/esrgrn3z3urf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2b09fb4a8a1066f72a29fbc36d10f78c44b73ea" title="ArchGW 🚀 - Use Ollama-based LLMs with Anthropic client (release 0.3.13)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just added support for cross-client streaming &lt;a href="https://github.com/katanemo/archgw"&gt;ArchGW 0.3.13&lt;/a&gt;, which lets you call Ollama compatible models through the Anthropic-clients (via the&lt;code&gt;/v1/messages&lt;/code&gt; API).&lt;/p&gt; &lt;p&gt;With Anthropic becoming popular (and a default) for many developers now this gives them native support for v1/messages for Ollama based models while enabling them to swap models in their agents without changing any client side code or do custom integration work for local models or 3rd party API-based models.&lt;/p&gt; &lt;p&gt;🙏🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/esrgrn3z3urf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsfdxs/archgw_use_ollamabased_llms_with_anthropic_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nsfdxs/archgw_use_ollamabased_llms_with_anthropic_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T04:38:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsojap</id>
    <title>Help with running Ai models with internet connectivity</title>
    <updated>2025-09-28T13:37:02+00:00</updated>
    <author>
      <name>/u/jimminecraftguy</name>
      <uri>https://old.reddit.com/user/jimminecraftguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have successfully installed ollama and open web ui in a Linux server vm on my proxmox server. Everything works nice and im very impressed. Im new to this and Im currently looking for a way for my models to connect and pull info from the internet. Id like it to be like how DeepSeek has an online search function. Im sorry in advanced, im very new to AI and Linux in general&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jimminecraftguy"&gt; /u/jimminecraftguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsojap/help_with_running_ai_models_with_internet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsojap/help_with_running_ai_models_with_internet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nsojap/help_with_running_ai_models_with_internet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T13:37:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1nswk4c</id>
    <title>🚀 Prompt Engineering Contest — Week 1 is LIVE! ✨</title>
    <updated>2025-09-28T19:01:42+00:00</updated>
    <author>
      <name>/u/Comfortable_Device50</name>
      <uri>https://old.reddit.com/user/Comfortable_Device50</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We wanted to create something fun for the community — a place where anyone who enjoys experimenting with AI and prompts can take part, challenge themselves, and learn along the way. That’s why we started the first ever Prompt Engineering Contest on Luna Prompts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://lunaprompts.com/contests"&gt;https://lunaprompts.com/contests&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here’s what you can do:&lt;/p&gt; &lt;p&gt;💡 Write creative prompts&lt;/p&gt; &lt;p&gt;🧩 Solve exciting AI challenges&lt;/p&gt; &lt;p&gt;🎁 Win prizes, certificates, and XP points&lt;/p&gt; &lt;p&gt;It’s simple, fun, and open to everyone. Jump in and be part of the very first contest — let’s make it big together! 🙌&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Device50"&gt; /u/Comfortable_Device50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nswk4c/prompt_engineering_contest_week_1_is_live/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nswk4c/prompt_engineering_contest_week_1_is_live/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nswk4c/prompt_engineering_contest_week_1_is_live/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T19:01:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntayc3</id>
    <title>Hardware for training/finetuning LLMs?</title>
    <updated>2025-09-29T06:37:28+00:00</updated>
    <author>
      <name>/u/Fantastic_Mud_389</name>
      <uri>https://old.reddit.com/user/Fantastic_Mud_389</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am considering getting a GPU of my own to train and finetune LLMs and other AI models, what do you usually use? Both locally and by renting. No way somebody actually has an H100 at their home&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic_Mud_389"&gt; /u/Fantastic_Mud_389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntayc3/hardware_for_trainingfinetuning_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntayc3/hardware_for_trainingfinetuning_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ntayc3/hardware_for_trainingfinetuning_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T06:37:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsyg5x</id>
    <title>What’s the closest I can get to gpt 5 mini performance with a mid tier gpu</title>
    <updated>2025-09-28T20:16:36+00:00</updated>
    <author>
      <name>/u/Practical_Employ4041</name>
      <uri>https://old.reddit.com/user/Practical_Employ4041</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve got a pc with a amd 6800 gpu with 16gb of vram, and I’m trying to get as close to gpt5 mini performance as I can from a locally hosted model. What do you reccomend for my hardware? I’m liking gemma3:12b so far but I’d be interested in what other options are out there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Practical_Employ4041"&gt; /u/Practical_Employ4041 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsyg5x/whats_the_closest_i_can_get_to_gpt_5_mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsyg5x/whats_the_closest_i_can_get_to_gpt_5_mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nsyg5x/whats_the_closest_i_can_get_to_gpt_5_mini/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T20:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntdo6w</id>
    <title>LLM Visualization (by Bycroft / bbycroft.net) — An interactive 3D animation of GPT-style inference: walk through layers, see tensor shapes, attention flows, etc.</title>
    <updated>2025-09-29T09:40:39+00:00</updated>
    <author>
      <name>/u/AggravatingGiraffe46</name>
      <uri>https://old.reddit.com/user/AggravatingGiraffe46</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggravatingGiraffe46"&gt; /u/AggravatingGiraffe46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bbycroft.net/llm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntdo6w/llm_visualization_by_bycroft_bbycroftnet_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ntdo6w/llm_visualization_by_bycroft_bbycroftnet_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T09:40:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntegzo</id>
    <title>LLM Evaluations with different quantizations</title>
    <updated>2025-09-29T10:31:37+00:00</updated>
    <author>
      <name>/u/leki483</name>
      <uri>https://old.reddit.com/user/leki483</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I usually check Artificial Analysis and some LLM arena leaderboards to get a rough idea of the intelligence of open-weight models. However, I have always wondered about the performance of those models after quantization (given that ollama provides all those models in different quantized versions).&lt;/p&gt; &lt;p&gt;Do you know any place where I could find those results in any of the main evals (MMLU-Pro, GPQA, LiveCodeBench, SciCode, HumanEval, Humanity's last exam, etc.)? So that I don't have to evaluate them myself.&lt;/p&gt; &lt;p&gt;Thank you so much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leki483"&gt; /u/leki483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntegzo/llm_evaluations_with_different_quantizations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntegzo/llm_evaluations_with_different_quantizations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ntegzo/llm_evaluations_with_different_quantizations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T10:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt2jhl</id>
    <title>Ollama Desktop</title>
    <updated>2025-09-28T23:10:23+00:00</updated>
    <author>
      <name>/u/bbzzo</name>
      <uri>https://old.reddit.com/user/bbzzo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nt2jhl/ollama_desktop/"&gt; &lt;img alt="Ollama Desktop" src="https://preview.redd.it/ktejon4fmzrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b34778c315d171ebd28722c7c5d522e436cf5a7" title="Ollama Desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I’m an Ollama enthusiast and I use Ollama Desktop for Mac. Recently, there were some updates, and I noticed in the documentation that there are new features. I downloaded the latest version, but they’re not showing up. Does anyone know what I need to do to enable these features? I’ve highlighted what I’m talking about in the image.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bbzzo"&gt; /u/bbzzo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ktejon4fmzrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nt2jhl/ollama_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nt2jhl/ollama_desktop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T23:10:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt5fcr</id>
    <title>How do I get ollama to use the igpu on the AMD AI Max+ 395?</title>
    <updated>2025-09-29T01:29:30+00:00</updated>
    <author>
      <name>/u/StartupTim</name>
      <uri>https://old.reddit.com/user/StartupTim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On debian 13 on a framework desktop (amd ai max+ 395), so I have the trixie-backports firmware-amd-graphics installed as well as the ollama rocm as seen &lt;a href="https://ollama.com/download/ollama-linux-amd64-rocm.tgz"&gt;https://ollama.com/download/ollama-linux-amd64-rocm.tgz&lt;/a&gt; yet when I run ollama it still uses 100% CPU. I can't get it to see the GPU at all.&lt;/p&gt; &lt;p&gt;Any idea on what to do? &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StartupTim"&gt; /u/StartupTim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nt5fcr/how_do_i_get_ollama_to_use_the_igpu_on_the_amd_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nt5fcr/how_do_i_get_ollama_to_use_the_igpu_on_the_amd_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nt5fcr/how_do_i_get_ollama_to_use_the_igpu_on_the_amd_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T01:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nta4sf</id>
    <title>Low memory models</title>
    <updated>2025-09-29T05:45:40+00:00</updated>
    <author>
      <name>/u/Punkygdog</name>
      <uri>https://old.reddit.com/user/Punkygdog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to run ollama on a low resource system. It only has about 8gb of memory available, am I reading correctly that there are very few models that I can get to work in this situation (models that support image analysis)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Punkygdog"&gt; /u/Punkygdog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nta4sf/low_memory_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nta4sf/low_memory_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nta4sf/low_memory_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T05:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntcpam</id>
    <title>Incomplete output from finetuned llama3.1.</title>
    <updated>2025-09-29T08:35:12+00:00</updated>
    <author>
      <name>/u/PurpleCheap1285</name>
      <uri>https://old.reddit.com/user/PurpleCheap1285</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone&lt;/p&gt; &lt;p&gt;I run Ollama with finetuned llama3.1 on 3 PowerShell terminals in parallel. I get correct output on first terminal, but I get incomplete output on 2nd and 3rd terminal. Can someone guide me about this problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PurpleCheap1285"&gt; /u/PurpleCheap1285 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntcpam/incomplete_output_from_finetuned_llama31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntcpam/incomplete_output_from_finetuned_llama31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ntcpam/incomplete_output_from_finetuned_llama31/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T08:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nthjju</id>
    <title>Made a hosted UI for local LLM, originally for docker model runner, can be used with ollama too</title>
    <updated>2025-09-29T13:07:09+00:00</updated>
    <author>
      <name>/u/binuuday</name>
      <uri>https://old.reddit.com/user/binuuday</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nthjju/made_a_hosted_ui_for_local_llm_originally_for/"&gt; &lt;img alt="Made a hosted UI for local LLM, originally for docker model runner, can be used with ollama too" src="https://preview.redd.it/r8dqk7guq3sf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=f5ce95bd2e9fbf898711fb9141cbdd8b371b164e" title="Made a hosted UI for local LLM, originally for docker model runner, can be used with ollama too" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Made a simple online chat ui for docker model runner. But there is a CORS option request failing on docker model runner implemenation (have updated an existing bug)&lt;/p&gt; &lt;p&gt;I know there are so many UI's for docker. But do try this out, if you have time.&lt;/p&gt; &lt;p&gt;&lt;a href="https://binuud.com/staging/aiChat"&gt;https://binuud.com/staging/aiChat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It requires Google Chrome or Firefox to run. Instructions on enabling CORS in the tool itself.&lt;/p&gt; &lt;p&gt;For ollama issue start same using&lt;/p&gt; &lt;p&gt;export OLLAMA_ORIGINS=&amp;quot;&lt;a href="https://binuud.com"&gt;https://binuud.com&lt;/a&gt;&amp;quot;&lt;/p&gt; &lt;p&gt;ollama serve&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/binuuday"&gt; /u/binuuday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r8dqk7guq3sf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nthjju/made_a_hosted_ui_for_local_llm_originally_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nthjju/made_a_hosted_ui_for_local_llm_originally_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T13:07:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu2ewf</id>
    <title>Ollama thinks that it is ChatGPT</title>
    <updated>2025-09-30T03:16:24+00:00</updated>
    <author>
      <name>/u/Silas-SB</name>
      <uri>https://old.reddit.com/user/Silas-SB</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nu2ewf/ollama_thinks_that_it_is_chatgpt/"&gt; &lt;img alt="Ollama thinks that it is ChatGPT" src="https://preview.redd.it/bxckmo9zy7sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97defef8a19e5e202cb60d116c90d9012eb988e7" title="Ollama thinks that it is ChatGPT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think this is because I gave him the personality of an helpful assistant, but I still found that really funny. Does anybody know more about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silas-SB"&gt; /u/Silas-SB &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bxckmo9zy7sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu2ewf/ollama_thinks_that_it_is_chatgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nu2ewf/ollama_thinks_that_it_is_chatgpt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-30T03:16:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntpy87</id>
    <title>Windows ollama using CPU</title>
    <updated>2025-09-29T18:30:43+00:00</updated>
    <author>
      <name>/u/fcnealv</name>
      <uri>https://old.reddit.com/user/fcnealv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using 5060ti 16gb and amd r5 5600x. I pull qwen coder 2.5 14b.. I noticed that my CPU doing the workload? What's the solution to force it to use my gpu&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fcnealv"&gt; /u/fcnealv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntpy87/windows_ollama_using_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntpy87/windows_ollama_using_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ntpy87/windows_ollama_using_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T18:30:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nteept</id>
    <title>Dead-simple example code for MCP with Ollama.</title>
    <updated>2025-09-29T10:27:45+00:00</updated>
    <author>
      <name>/u/kirill_saidov</name>
      <uri>https://old.reddit.com/user/kirill_saidov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nteept/deadsimple_example_code_for_mcp_with_ollama/"&gt; &lt;img alt="Dead-simple example code for MCP with Ollama." src="https://external-preview.redd.it/u58dHZPDQal1dcDvgrTyHK2TMwy2_0EMHtoQwPERqi0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be75b773b0fa0d092b8d75639b98721a816757e5" title="Dead-simple example code for MCP with Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This example shows how to use MCP with Ollama by implementing a super simple MCP client and server in Python.&lt;/p&gt; &lt;p&gt;I made it for people like me who got frustrated with Claude MCP videos and existing &lt;code&gt;mcphosts&lt;/code&gt; that hide all the actual logic. This repo walks through everything step by step so you can see exactly how the pieces fit together.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kirill_saidov"&gt; /u/kirill_saidov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kirillsaidov/ollama-mcp-example"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nteept/deadsimple_example_code_for_mcp_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nteept/deadsimple_example_code_for_mcp_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T10:27:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntne0p</id>
    <title>Why dont it recognize my GPU</title>
    <updated>2025-09-29T16:55:28+00:00</updated>
    <author>
      <name>/u/maybesomenone</name>
      <uri>https://old.reddit.com/user/maybesomenone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ntne0p/why_dont_it_recognize_my_gpu/"&gt; &lt;img alt="Why dont it recognize my GPU" src="https://preview.redd.it/iluqhuiaw4sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40f0f6bc7c84fd70f7bc3b079c91245acd31e989" title="Why dont it recognize my GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why ollama does not recognize my GPU to run the models? what am i doing wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maybesomenone"&gt; /u/maybesomenone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iluqhuiaw4sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntne0p/why_dont_it_recognize_my_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ntne0p/why_dont_it_recognize_my_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T16:55:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu2uc5</id>
    <title>Run ollama behind reverse proxy with a path prefix</title>
    <updated>2025-09-30T03:37:42+00:00</updated>
    <author>
      <name>/u/Wide-Implement-6838</name>
      <uri>https://old.reddit.com/user/Wide-Implement-6838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT: Solved.&lt;/p&gt; &lt;p&gt;Hi, I'm wondering if ollama has any options to have it run behind a reverse proxy with a path prefix (so `domain.tld/ollama` for example).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wide-Implement-6838"&gt; /u/Wide-Implement-6838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu2uc5/run_ollama_behind_reverse_proxy_with_a_path_prefix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu2uc5/run_ollama_behind_reverse_proxy_with_a_path_prefix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nu2uc5/run_ollama_behind_reverse_proxy_with_a_path_prefix/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-30T03:37:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu7kul</id>
    <title>Does Ollama immobilize GPUs / computing resources?</title>
    <updated>2025-09-30T08:19:52+00:00</updated>
    <author>
      <name>/u/Unfair_Resident_5951</name>
      <uri>https://old.reddit.com/user/Unfair_Resident_5951</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! Beginner question here!&lt;/p&gt; &lt;p&gt;I'm considering installing an Ollama instance on my lab's small cluster. However, I'm wondering if Ollama locks the GPUs it uses as long as the HTTP server is running or if we can still use the same GPUs for something else as long as a text generation is not running?&lt;/p&gt; &lt;p&gt;We have only 6 GPUs that we use for a lot of other things so I don't want to degrade performances for other users by running the server non-stop and having to start and stop it every single time makes me feel like maybe just loading the models using HF transformers could be a better solution for my use case.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unfair_Resident_5951"&gt; /u/Unfair_Resident_5951 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu7kul/does_ollama_immobilize_gpus_computing_resources/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu7kul/does_ollama_immobilize_gpus_computing_resources/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nu7kul/does_ollama_immobilize_gpus_computing_resources/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-30T08:19:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu7xbb</id>
    <title>[RELEASE] Doc Builder (MD + PDF) 1.7.3 for Open WebUI</title>
    <updated>2025-09-30T08:43:19+00:00</updated>
    <author>
      <name>/u/Nefhis</name>
      <uri>https://old.reddit.com/user/Nefhis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nu7xbb/release_doc_builder_md_pdf_173_for_open_webui/"&gt; &lt;img alt="[RELEASE] Doc Builder (MD + PDF) 1.7.3 for Open WebUI" src="https://b.thumbs.redditmedia.com/2sg310saEP5IoBJyy2DgQwnJiCk5HqpkE2fBQYXcGOw.jpg" title="[RELEASE] Doc Builder (MD + PDF) 1.7.3 for Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nefhis"&gt; /u/Nefhis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenWebUI/comments/1nu6qqd/release_doc_builder_md_pdf_173_for_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu7xbb/release_doc_builder_md_pdf_173_for_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nu7xbb/release_doc_builder_md_pdf_173_for_open_webui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-30T08:43:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nua25g</id>
    <title>高階AI推理平台建構與測試</title>
    <updated>2025-09-30T10:57:57+00:00</updated>
    <author>
      <name>/u/kuerys</name>
      <uri>https://old.reddit.com/user/kuerys</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;爆機測試&lt;/p&gt; &lt;p&gt;NVIDIA-SMI 580.82.09 Driver Version: 580.82.09 CUDA Version: 13.0&lt;/p&gt; &lt;p&gt;單卡&lt;/p&gt; &lt;p&gt;GPU:RTX 3060 GD6 12G 系統顯 PCI-E 1 (X16)&lt;/p&gt; &lt;p&gt;未分流 未量化 未切片 &lt;/p&gt; &lt;p&gt;gpt-oss:120b 65GB (O) RAM128G 冷啟(8分)到500字長文 15 分鐘內完成，推理穩定，資源分配均衡（CPU 75%、GPU 25%、RAM 50%）&lt;/p&gt; &lt;p&gt;qwen3:235b 142GB (O) RAM128G 冷啟(15分）到500字長文 45 分鐘內完成，推理穩定，資源分配均衡（CPU 98%、GPU 95%、RAM 99%、SRAM 80%）&lt;/p&gt; &lt;p&gt;llama3.1:405b 243GB (O) RAM256G 冷啟(35分）到500字長文 75 分鐘內完成，推理穩定，資源分配均衡（CPU 98%、GPU 95%、RAM 99%、SRAM 99%、SRAM2 20%）&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kuerys"&gt; /u/kuerys &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://copilot.microsoft.com/shares/nnZC7oAAcag8qDeyMAXsH"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nua25g/高階ai推理平台建構與測試/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nua25g/高階ai推理平台建構與測試/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-30T10:57:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nthurw</id>
    <title>I built a private AI Meeting Note Taker that runs 100% offline.</title>
    <updated>2025-09-29T13:20:45+00:00</updated>
    <author>
      <name>/u/Prudent-Meringue845</name>
      <uri>https://old.reddit.com/user/Prudent-Meringue845</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nthurw/i_built_a_private_ai_meeting_note_taker_that_runs/"&gt; &lt;img alt="I built a private AI Meeting Note Taker that runs 100% offline." src="https://external-preview.redd.it/P-TYv9mTeSOOCmZRCbHzYZzYlFIWTdCXucO7FkZ03u0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7db73e296d0a5500ae480c4deb5b0efd99fcd95e" title="I built a private AI Meeting Note Taker that runs 100% offline." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prudent-Meringue845"&gt; /u/Prudent-Meringue845 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/data-science-collective/i-built-an-self-hosted-ai-meeting-note-taker-that-runs-100-offline-heres-how-you-can-too-d110b7ef0b95"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nthurw/i_built_a_private_ai_meeting_note_taker_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nthurw/i_built_a_private_ai_meeting_note_taker_that_runs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T13:20:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nulugn</id>
    <title>Introducing DevCrew_s: Where Human Expertise Meets AI Innovation</title>
    <updated>2025-09-30T18:57:00+00:00</updated>
    <author>
      <name>/u/The_Research_Ninja</name>
      <uri>https://old.reddit.com/user/The_Research_Ninja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Fam. &lt;a href="https://github.com/GSA-TTS/devCrew_s"&gt;DevCrew_s&lt;/a&gt; is an open collection of AI agent specifications and protocols that define how intelligent agents collaborate to solve complex problems. Think of it as blueprints for AI teammates that augment human expertise rather than replace it. You don't need to code to contribute. If you're a domain expert who knows your field inside and out, you can start TODAY by writing your Agent Specification(s) in simple, structured English using &lt;a href="https://github.com/GSA-TTS/devCrew_s/blob/master/docs/templates/AI%20Agent%20Specification%20Template.md"&gt;DevCrew_s templates&lt;/a&gt;. For the technical folks, this is your playground. Every official specification here works immediately—grab Claude Code tonight and watch these agents &lt;a href="https://github.com/GSA-TTS/devCrew_s/tree/master/docs/guides"&gt;come to life&lt;/a&gt;.&lt;br /&gt; &lt;a href="https://github.com/GSA-TTS/devCrew_s"&gt;DevCrew_s&lt;/a&gt; already has 5 official &lt;a href="https://github.com/GSA-TTS/devCrew_s/blob/master/agent-Backend-Engineer-vSEP25.md"&gt;agents&lt;/a&gt; and &lt;a href="https://github.com/GSA-TTS/devCrew_s/tree/master/protocols"&gt;48 protocols&lt;/a&gt; covering most of DevOps, and it's just getting started. Browse what exists, try them out, then add your own expertise to the mix. Whether you fix a typo or design a revolutionary new agent, every &lt;a href="https://github.com/GSA-TTS/devCrew_s/blob/master/CONTRIBUTING.md"&gt;&lt;strong&gt;contribution matters&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_Research_Ninja"&gt; /u/The_Research_Ninja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nulugn/introducing_devcrew_s_where_human_expertise_meets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nulugn/introducing_devcrew_s_where_human_expertise_meets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nulugn/introducing_devcrew_s_where_human_expertise_meets/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-30T18:57:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu4r5t</id>
    <title>How to train a LLM?</title>
    <updated>2025-09-30T05:21:55+00:00</updated>
    <author>
      <name>/u/phoniex7777</name>
      <uri>https://old.reddit.com/user/phoniex7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I want to train (fine-tune) an existing LLM with my own dataset. I’m not trying to train from scratch, just make the model better for my use case.&lt;/p&gt; &lt;p&gt;A few questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;What are the minimum hardware needs (GPU, RAM, storage) if I only have a small dataset?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Can this be done on free cloud services like Colab Free, Kaggle, or Hugging Face Spaces, or do I need to pay for GPUs?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Which model and library would be the easiest for a beginner to start with?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I just want to get some hands-on experience without spending too much money.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phoniex7777"&gt; /u/phoniex7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu4r5t/how_to_train_a_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu4r5t/how_to_train_a_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nu4r5t/how_to_train_a_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-30T05:21:55+00:00</published>
  </entry>
</feed>
