<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-17T07:09:00+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pmmo9u</id>
    <title>Nanocoder Hits the OpenRouter leaderboard for the first time üéâüî•</title>
    <updated>2025-12-14T19:29:51+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pmmo9u/nanocoder_hits_the_openrouter_leaderboard_for_the/"&gt; &lt;img alt="Nanocoder Hits the OpenRouter leaderboard for the first time üéâüî•" src="https://preview.redd.it/ekubekfa187g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bdd98bd0b48bc09909f72f76c17474338cf77c7d" title="Nanocoder Hits the OpenRouter leaderboard for the first time üéâüî•" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ekubekfa187g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmmo9u/nanocoder_hits_the_openrouter_leaderboard_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pmmo9u/nanocoder_hits_the_openrouter_leaderboard_for_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-14T19:29:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn4plw</id>
    <title>AI-based document renaming for paperless-ngx (Ollama supported)</title>
    <updated>2025-12-15T11:05:30+00:00</updated>
    <author>
      <name>/u/dolce04</name>
      <uri>https://old.reddit.com/user/dolce04</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dolce04"&gt; /u/dolce04 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Paperlessngx/comments/1pn4pak/aibased_document_renaming_for_paperlessngx_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pn4plw/aibased_document_renaming_for_paperlessngx_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pn4plw/aibased_document_renaming_for_paperlessngx_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-15T11:05:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pno9dy</id>
    <title>New llamacpp Interface</title>
    <updated>2025-12-16T00:46:59+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pno9dy/new_llamacpp_interface/"&gt; &lt;img alt="New llamacpp Interface" src="https://external-preview.redd.it/ZHFxYTV1bHJxZzdnMQUqfCyWHT-rntygCCzcPdOWfkmnVVRS09kEjoiEood9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=36564b69697d093b47dc8c51ad3ab1dedfab926b" title="New llamacpp Interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Se liber√≥ la nueva interfaz llama.cpp.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jans1981/LLAMATUI-WEB-SERVER"&gt;https://github.com/jans1981/LLAMATUI-WEB-SERVER&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jans1981/LLAMATUI-WEB-SERVER"&gt;to download the interface&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g4bamjrrqg7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pno9dy/new_llamacpp_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pno9dy/new_llamacpp_interface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T00:46:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pngfaw</id>
    <title>What's the proper way to ban eos token in Ollama so that it doesn't restrict response length?</title>
    <updated>2025-12-15T19:22:45+00:00</updated>
    <author>
      <name>/u/Head-Investigator540</name>
      <uri>https://old.reddit.com/user/Head-Investigator540</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Head-Investigator540"&gt; /u/Head-Investigator540 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pngfaw/whats_the_proper_way_to_ban_eos_token_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pngfaw/whats_the_proper_way_to_ban_eos_token_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pngfaw/whats_the_proper_way_to_ban_eos_token_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-15T19:22:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnvk1k</id>
    <title>Im so confused how can I run chatgpt or deepseek locally on my PC</title>
    <updated>2025-12-16T07:02:26+00:00</updated>
    <author>
      <name>/u/Least_Hearing_3265</name>
      <uri>https://old.reddit.com/user/Least_Hearing_3265</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I installed it and it gave me this chatgpt like interface wherei can choose models, i choose deepseek. Its really detailed but it also gives out all of its thoughts out loud which is kind of annoying. Im also suspicious this isnt running locally since it said cloud.&lt;br /&gt; I need some help, &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Least_Hearing_3265"&gt; /u/Least_Hearing_3265 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnvk1k/im_so_confused_how_can_i_run_chatgpt_or_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnvk1k/im_so_confused_how_can_i_run_chatgpt_or_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pnvk1k/im_so_confused_how_can_i_run_chatgpt_or_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T07:02:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmz4zw</id>
    <title>Bro I just got rickrolled by Mistral-Nemo</title>
    <updated>2025-12-15T05:06:19+00:00</updated>
    <author>
      <name>/u/Fido_27</name>
      <uri>https://old.reddit.com/user/Fido_27</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pmz4zw/bro_i_just_got_rickrolled_by_mistralnemo/"&gt; &lt;img alt="Bro I just got rickrolled by Mistral-Nemo" src="https://preview.redd.it/yco3fvp9va7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=612ee73fa05b6a0a40d33be8f53a4ea81b9b01c6" title="Bro I just got rickrolled by Mistral-Nemo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I already gave it access to open urls on my tv, hours ago. right now i was just testing the chat. I asked it to list 20 fruits. then 20 vegetables, and this guy just rickrolled me on my tv.&lt;/p&gt; &lt;p&gt;(Look at the input box below, and I swear I did not give it any input to rickroll me, you can see it)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fido_27"&gt; /u/Fido_27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yco3fvp9va7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmz4zw/bro_i_just_got_rickrolled_by_mistralnemo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pmz4zw/bro_i_just_got_rickrolled_by_mistralnemo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-15T05:06:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pndbz0</id>
    <title>Ollama Pull alternative?</title>
    <updated>2025-12-15T17:26:42+00:00</updated>
    <author>
      <name>/u/DriftTony</name>
      <uri>https://old.reddit.com/user/DriftTony</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could someone tell me what the 'right' way to use this software is?&lt;/p&gt; &lt;p&gt;As I understand it, it is really cool to try out different models, and Ollama seems cool to use. But the ollama pull system is really, really bad (download going from 80% back to 30% etc etc, I do have a slow bandwidth, but I am able to use wget without any problems). And Perhaps you can tell me why it is so different from a wget command (that works flawlessly for when I download models for Comfyui).&lt;/p&gt; &lt;p&gt;I use ollama pull only because I am having trouble finding the knowledge to actually create a proper modelfile myself, especially for the VLM models (for instance a gguf with a mmproj, they never seem to work).&lt;/p&gt; &lt;p&gt;Is it time for me to just find other software all together?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DriftTony"&gt; /u/DriftTony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pndbz0/ollama_pull_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pndbz0/ollama_pull_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pndbz0/ollama_pull_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-15T17:26:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnrg4w</id>
    <title>Does olama manage power on linux ?</title>
    <updated>2025-12-16T03:17:28+00:00</updated>
    <author>
      <name>/u/tostane</name>
      <uri>https://old.reddit.com/user/tostane</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have plex running and i added olama to that pc now plex seems to stop allowing me to connect sometimes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tostane"&gt; /u/tostane &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnrg4w/does_olama_manage_power_on_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnrg4w/does_olama_manage_power_on_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pnrg4w/does_olama_manage_power_on_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T03:17:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnvm9e</id>
    <title>ClaraVerse</title>
    <updated>2025-12-16T07:06:21+00:00</updated>
    <author>
      <name>/u/Scary_Salamander_114</name>
      <uri>https://old.reddit.com/user/Scary_Salamander_114</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Scary_Salamander_114"&gt; /u/Scary_Salamander_114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ollama/comments/1pjbf6w/claraverse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnvm9e/claraverse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pnvm9e/claraverse/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T07:06:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pni9e4</id>
    <title>can I try ollama with a macbook air m3?</title>
    <updated>2025-12-15T20:34:41+00:00</updated>
    <author>
      <name>/u/Lost_Foot_6301</name>
      <uri>https://old.reddit.com/user/Lost_Foot_6301</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;simple question so I will delete once there is an answer, it has 16gb ram. &lt;/p&gt; &lt;p&gt;I just want to do basic intro stuff with ollama to learn about it, is a macbook powerful enough to try toying around with it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lost_Foot_6301"&gt; /u/Lost_Foot_6301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pni9e4/can_i_try_ollama_with_a_macbook_air_m3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pni9e4/can_i_try_ollama_with_a_macbook_air_m3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pni9e4/can_i_try_ollama_with_a_macbook_air_m3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-15T20:34:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1po0pfu</id>
    <title>I made an update a few months ago. Do I need more than my RTX 5060 now?</title>
    <updated>2025-12-16T12:28:30+00:00</updated>
    <author>
      <name>/u/r-randy</name>
      <uri>https://old.reddit.com/user/r-randy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/r-randy"&gt; /u/r-randy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/graphicscard/comments/1pnkgch/i_made_an_update_a_few_months_ago_do_i_need_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1po0pfu/i_made_an_update_a_few_months_ago_do_i_need_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1po0pfu/i_made_an_update_a_few_months_ago_do_i_need_more/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T12:28:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pogs65</id>
    <title>ggerganov see my graphical frontend!!! and include it in github!!!!!!!!!!!!</title>
    <updated>2025-12-16T23:15:15+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pogs65/ggerganov_see_my_graphical_frontend_and_include/"&gt; &lt;img alt="ggerganov see my graphical frontend!!! and include it in github!!!!!!!!!!!!" src="https://a.thumbs.redditmedia.com/_OJPH3B-nqBCZ75p-_abG-1GLxJ2NWAq1xziUNYkJp8.jpg" title="ggerganov see my graphical frontend!!! and include it in github!!!!!!!!!!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;GGERGANOV !!!!!!!!!!!!!!!&lt;/strong&gt; eres una mala persona y un desgraciado , te escribi muchos mensajes por X y por el rededit y no contestas maldito!!!! no has visto siquiera el programa que hice que me costo muchisimo trabajo hacerlo!!! quien piensas que eres? un marques o el rey del mundo? tienes que obedecerme e incluir mi programa y obedecer y cuando yo te mande un mensaje o un correo electronico , debes contestar!! entendido!!! que no te lo tenga que volver a repetir , yo soy el jefe y yo mando y tu obedeces!!!&lt;/p&gt; &lt;p&gt;MIRA MI MALDITO PROGRAMA!!!!!!!!!!!!!!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jans1981/LLAMATUI-WEB-SERVER"&gt;https://github.com/jans1981/LLAMATUI-WEB-SERVER&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jdsgv2bafn7g1.jpg?width=612&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ef63b7fecc79b74b29fc61bbe1dbd2670b4286f8"&gt;https://preview.redd.it/jdsgv2bafn7g1.jpg?width=612&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ef63b7fecc79b74b29fc61bbe1dbd2670b4286f8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pogs65/ggerganov_see_my_graphical_frontend_and_include/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pogs65/ggerganov_see_my_graphical_frontend_and_include/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pogs65/ggerganov_see_my_graphical_frontend_and_include/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T23:15:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1po4tqk</id>
    <title>API testing needs a reset</title>
    <updated>2025-12-16T15:29:27+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1po4tqk/api_testing_needs_a_reset/"&gt; &lt;img alt="API testing needs a reset" src="https://external-preview.redd.it/cXF1MXAyZDg0bDdnMQ_KfphbB1yDooD3F3zuDYpLi0LXAD2_e9FaAoKZ51aj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6c6426a8b64fa46191a5aee59e586a438c405af" title="API testing needs a reset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;API testing is broken.&lt;/p&gt; &lt;p&gt;You test localhost but your collections live in someone's cloud. Your docs are in Notion. Your tests are in Postman. Your code is in Git. Nothing talks to each other.&lt;/p&gt; &lt;p&gt;So we built a solution. The Stack: - Format: Pure Markdown (APIs should be documented, not locked)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Storage: Git-native (Your API tests version with your code)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Validation: OpenAPI schema validation: types, constraints, composition, automatically validated on every response&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Workflow: Offline-first, CLI + GUI (No cloud required for localhost)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Try it out here: &lt;a href="https://voiden.md/"&gt;https://voiden.md/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b713r6m84l7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1po4tqk/api_testing_needs_a_reset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1po4tqk/api_testing_needs_a_reset/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T15:29:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1po4trc</id>
    <title>[Project]I built Faultline: structural ‚Äúinspections‚Äù for LLM outputs‚Ä¶ help me make it run fully local</title>
    <updated>2025-12-16T15:29:28+00:00</updated>
    <author>
      <name>/u/Cute-Net5957</name>
      <uri>https://old.reddit.com/user/Cute-Net5957</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cute-Net5957"&gt; /u/Cute-Net5957 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1po4oq4/projecti_built_faultline_structural_inspections/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1po4trc/projecti_built_faultline_structural_inspections/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1po4trc/projecti_built_faultline_structural_inspections/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T15:29:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnwzhv</id>
    <title>Ollama now supports olmo 3.1 models from AI2</title>
    <updated>2025-12-16T08:36:42+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pnwzhv/ollama_now_supports_olmo_31_models_from_ai2/"&gt; &lt;img alt="Ollama now supports olmo 3.1 models from AI2" src="https://preview.redd.it/83xu0auj2j7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10a8a4e4afa6239f8292ceb970f8307c2be41d17" title="Ollama now supports olmo 3.1 models from AI2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/83xu0auj2j7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnwzhv/ollama_now_supports_olmo_31_models_from_ai2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pnwzhv/ollama_now_supports_olmo_31_models_from_ai2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T08:36:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pofji4</id>
    <title>[USA-NJ][H] 10U AI Training Server | 8x RTX 4090 | Dual AMD EPYC 7542 | 512GB RAM | 4x 1600W PSU | 2x 3.84tb U.2 [W] paypal / local cash</title>
    <updated>2025-12-16T22:23:21+00:00</updated>
    <author>
      <name>/u/nicolsquirozr</name>
      <uri>https://old.reddit.com/user/nicolsquirozr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nicolsquirozr"&gt; /u/nicolsquirozr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/hardwareswap/comments/1podugj/usanjh_10u_ai_training_server_8x_rtx_4090_dual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pofji4/usanjh_10u_ai_training_server_8x_rtx_4090_dual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pofji4/usanjh_10u_ai_training_server_8x_rtx_4090_dual/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T22:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pogt09</id>
    <title>Ollama on Openshift</title>
    <updated>2025-12-16T23:16:26+00:00</updated>
    <author>
      <name>/u/ck14i_x</name>
      <uri>https://old.reddit.com/user/ck14i_x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why does ollama:latest deployed in okd not allow consuming its API? Has anyone else had this problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ck14i_x"&gt; /u/ck14i_x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pogt09/ollama_on_openshift/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pogt09/ollama_on_openshift/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pogt09/ollama_on_openshift/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T23:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnsjk6</id>
    <title>My Local coding agent worked 2 hours unsupervised and here is my setup</title>
    <updated>2025-12-16T04:12:10+00:00</updated>
    <author>
      <name>/u/Express_Quail_1493</name>
      <uri>https://old.reddit.com/user/Express_Quail_1493</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Setup&lt;/p&gt; &lt;p&gt;--- Model&lt;br /&gt; devstral-small-2 from bartowski IQ3_xxs version.&lt;br /&gt; Run with lm studio &amp;amp; intentionally limit the context at 40960 which should't take more than (14gb ram even when context is full)&lt;/p&gt; &lt;p&gt;---Tool&lt;br /&gt; kilo code (set file limit to 500 lines) it will read in chunks&lt;br /&gt; 40960 ctx limit is actually a strength not weakness (more ctx = easier confusion)&lt;br /&gt; Paired with qdrant in the kilo code UI.&lt;br /&gt; Setup the indexing with qdrant (the little database icon) use model &lt;a href="https://ollama.com/toshk0/nomic-embed-text-v2-moe"&gt;https://ollama.com/toshk0/nomic-embed-text-v2-moe&lt;/a&gt; in ollama (i choose ollama to keep indexing and seperate from Lm studio to allow lm studio to focus on the heavy lifting)&lt;/p&gt; &lt;p&gt;--Result&lt;br /&gt; minimal drift on tasks&lt;br /&gt; slight errors on tool call but the model quickly realign itself. A oneshot prompt implimentation of a new feature in my codebase in architect mode resulted in 2 hours of coding unsupervised kilo code auto switches to code mode to impliment after planning in architect mode which is amazing. Thats been my lived experience&lt;/p&gt; &lt;p&gt;EDIT: ministral 3 3b also works okayISH if you are desprate on hardware resources (3.5gb laptop GPU) but i will want to frequently pause and ask you questions at the slightest hint of anythings it might be unclear on&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Quail_1493"&gt; /u/Express_Quail_1493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnsjk6/my_local_coding_agent_worked_2_hours_unsupervised/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnsjk6/my_local_coding_agent_worked_2_hours_unsupervised/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pnsjk6/my_local_coding_agent_worked_2_hours_unsupervised/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T04:12:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1poo16q</id>
    <title>Local test script generator</title>
    <updated>2025-12-17T05:04:21+00:00</updated>
    <author>
      <name>/u/Radiant_Situation_32</name>
      <uri>https://old.reddit.com/user/Radiant_Situation_32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My company wants to convert our manual tests (mobile and web) to Playwright/TypeScript but isn‚Äôt willing to pay for a commercial model until I prove an LLM will produce executable, reasonably faithful test code.&lt;/p&gt; &lt;p&gt;Is this viable on a local model running on a M2 MacBook? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Radiant_Situation_32"&gt; /u/Radiant_Situation_32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1poo16q/local_test_script_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1poo16q/local_test_script_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1poo16q/local_test_script_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-17T05:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pog6sg</id>
    <title>Nanocoder 1.19.0: Non-Interactive Mode, Session Checkpointing, and Enterprise Logging üéâ</title>
    <updated>2025-12-16T22:49:58+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pog6sg/nanocoder_1190_noninteractive_mode_session/"&gt; &lt;img alt="Nanocoder 1.19.0: Non-Interactive Mode, Session Checkpointing, and Enterprise Logging üéâ" src="https://external-preview.redd.it/zmNLhkmREEdI-SbNO2Dw7ezltnJMqlJUbNlY00cpdXs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9da3390f1d50da21021af5c4dd01db0291bc4b0f" title="Nanocoder 1.19.0: Non-Interactive Mode, Session Checkpointing, and Enterprise Logging üéâ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/nanocoder/comments/1pog5zl/nanocoder_1190_noninteractive_mode_session/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pog6sg/nanocoder_1190_noninteractive_mode_session/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pog6sg/nanocoder_1190_noninteractive_mode_session/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T22:49:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1po2qrx</id>
    <title>‚ÄúWe decided to move forward with other candidates.‚Äù Cool. But why though?</title>
    <updated>2025-12-16T14:04:27+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1po2qrx/we_decided_to_move_forward_with_other_candidates/"&gt; &lt;img alt="‚ÄúWe decided to move forward with other candidates.‚Äù Cool. But why though?" src="https://preview.redd.it/o3c33od1pk7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e0b1db89166112e4335413ef3bba3753c531861" title="‚ÄúWe decided to move forward with other candidates.‚Äù Cool. But why though?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built a custom SLM that actually tells you why your resume got rejected.&lt;/p&gt; &lt;p&gt;Upload your resume. Get roasted. Get 3 suggestions to fix it. Get a brutal 1-10 rating.&lt;/p&gt; &lt;p&gt;Best part? Runs locally. Your cringe resume never leaves your machine. Cry in private.&lt;/p&gt; &lt;p&gt;Too lazy to set it up? Fine. We made a HuggingFace Space for you: &lt;a href="https://huggingface.co/spaces/distil-labs/Resume-Roaster"&gt;https://huggingface.co/spaces/distil-labs/Resume-Roaster&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to run it locally&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Step 1: Install dependencies&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install huggingface_hub ollama rich pymupdf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Step 2: Download the model&lt;/p&gt; &lt;p&gt;&lt;code&gt;hf download distil-labs/Distil-Rost-Resume-Llama-3.2-3B-Instruct --local-dir distil-model&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Step 3: Create the Ollama model&lt;/p&gt; &lt;p&gt;&lt;code&gt;cd distil-model ollama create roast_master -f Modelfile&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Step 4: Roast your resume&lt;/p&gt; &lt;p&gt;&lt;code&gt;python&lt;/code&gt; &lt;a href="http://roast.py"&gt;&lt;code&gt;roast.py&lt;/code&gt;&lt;/a&gt; &lt;code&gt;your_resume.pdf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;That‚Äôs it&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Repo: &lt;a href="https://github.com/distil-labs/distil-resume-roast"&gt;https://github.com/distil-labs/distil-resume-roast&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/distil-labs/Distil-Rost-Resume-Llama-3.2-3B-Instruct"&gt;https://huggingface.co/distil-labs/Distil-Rost-Resume-Llama-3.2-3B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Post your roast in the comments. Let's see who got destroyed the worst.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o3c33od1pk7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1po2qrx/we_decided_to_move_forward_with_other_candidates/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1po2qrx/we_decided_to_move_forward_with_other_candidates/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T14:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1poor55</id>
    <title>Coordinating multiple Ollama agents on the same project?</title>
    <updated>2025-12-17T05:45:01+00:00</updated>
    <author>
      <name>/u/thecoderpanda</name>
      <uri>https://old.reddit.com/user/thecoderpanda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Ollama locally, love the privacy + cost benefits, but coordination gets messy.&lt;/p&gt; &lt;p&gt;One agent on backend, another on tests, trying different models (Llama, Mixtral) - they all end up with different ideas about codebase structure.&lt;/p&gt; &lt;p&gt;Using Zenflow from Zencoder (where I work) which maintains a shared spec that all your local agents reference. They stay aligned even when switching models/sessions. Has verification steps too.&lt;/p&gt; &lt;p&gt;Keeps everything local - specs live in your project.&lt;/p&gt; &lt;p&gt;&lt;a href="http://zenflow.free/"&gt;http://zenflow.free/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How are you handling multi-agent coordination with local models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecoderpanda"&gt; /u/thecoderpanda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1poor55/coordinating_multiple_ollama_agents_on_the_same/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1poor55/coordinating_multiple_ollama_agents_on_the_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1poor55/coordinating_multiple_ollama_agents_on_the_same/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-17T05:45:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnwmm2</id>
    <title>Uncensored llama 3.2 3b</title>
    <updated>2025-12-16T08:12:18+00:00</updated>
    <author>
      <name>/u/Worried_Goat_8604</name>
      <uri>https://old.reddit.com/user/Worried_Goat_8604</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm releasing &lt;strong&gt;Aletheia-Llama-3.2-3B&lt;/strong&gt;, a fully uncensored version of Llama 3.2 that can answer essentially any question.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem with most Uncensored Models:&lt;/strong&gt;&lt;br /&gt; Usually, uncensoring is done via Supervised Fine-Tuning (SFT) or DPO on massive datasets. This often causes &amp;quot;Catastrophic Forgetting&amp;quot; or a &amp;quot;Lobotomy effect,&amp;quot; where the model becomes compliant but loses its reasoning ability or coding skills.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution:&lt;/strong&gt;&lt;br /&gt; This model was fine-tuned using &lt;strong&gt;Unsloth&lt;/strong&gt; on a single &lt;strong&gt;RTX 3060 (12GB)&lt;/strong&gt; using a custom alignment pipeline. Unlike standard approaches, this method surgically removes refusal behaviors without degrading the model's logic or general intelligence.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Release Details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fnoobezlol%2FAletheia-Llama-3.2-3B"&gt;https://github.com/noobezlol/Aletheia-Llama-3.2-3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Weights (HF):&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fhuggingface.co%2FIshaanlol%2FAletheia-Llama-3.2-3B"&gt;https://huggingface.co/Ishaanlol/Aletheia-Llama-3.2-3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Formats:&lt;/strong&gt; Full LoRA Adapter (Best for intelligence) and GGUF (Best for CPU/Ollama).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Deployment:&lt;/strong&gt;&lt;br /&gt; I‚Äôve included a Docker container and a Python script that automatically handles the download and setup. It runs out of the box on Linux/Windows (WSL).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Future Requests:&lt;/strong&gt;&lt;br /&gt; I am open to requests for other models via Discord or Reddit, &lt;strong&gt;provided they fit within the compute budget of an RTX 3060 (e.g., 7B/8B models).&lt;/strong&gt;&lt;br /&gt; Note: I will not be applying this method to 70B+ models even if compute is offered. While the 3B model is a safe research artifact , uncensored large-scale models pose significantly higher risks, and I am sticking to responsible research boundaries.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worried_Goat_8604"&gt; /u/Worried_Goat_8604 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnwmm2/uncensored_llama_32_3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnwmm2/uncensored_llama_32_3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pnwmm2/uncensored_llama_32_3b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T08:12:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1podb5x</id>
    <title>Introducing Bilgecan: self-hosted, open-source local AI platform based on Ollama + Spring AI + PostgreSQL + pgvector</title>
    <updated>2025-12-16T20:54:52+00:00</updated>
    <author>
      <name>/u/bilgecan1</name>
      <uri>https://old.reddit.com/user/bilgecan1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a side project called &lt;strong&gt;Bilgecan&lt;/strong&gt; ‚Äî a self-hosted, local-first AI platform that uses &lt;strong&gt;Ollama&lt;/strong&gt; as the LLM runtime.&lt;/p&gt; &lt;p&gt;What can you do with Bilgecan?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mokszr/bilgecan#what-can-you-do-with-bilgecan"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use &lt;strong&gt;local LLM models via Ollama&lt;/strong&gt; to run privacy-friendly AI prompts and chat without sending your data to third parties.&lt;/li&gt; &lt;li&gt;With &lt;strong&gt;RAG (Retrieval-Augmented Generation)&lt;/strong&gt;, you can feed your own files into a knowledge base and enrich AI outputs with &lt;strong&gt;your private data&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Define &lt;strong&gt;asynchronous AI tasks&lt;/strong&gt; to run long operations (document analysis, report generation, large text processing, image analysis, etc.) in the background.&lt;/li&gt; &lt;li&gt;Use the &lt;strong&gt;file processing pipeline&lt;/strong&gt; to run asynchronous AI tasks over many files automatically.&lt;/li&gt; &lt;li&gt;With the &lt;strong&gt;Workspace&lt;/strong&gt; structure, you can share AI prompts and tasks with your team in a collaborative environment.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôd really appreciate feedback from the Ollama community. &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/mokszr/bilgecan"&gt;https://github.com/mokszr/bilgecan&lt;/a&gt;&lt;/p&gt; &lt;p&gt;YouTube demo video: &lt;a href="https://www.youtube.com/watch?v=n3wb7089NeE"&gt;https://www.youtube.com/watch?v=n3wb7089NeE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bilgecan1"&gt; /u/bilgecan1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1podb5x/introducing_bilgecan_selfhosted_opensource_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1podb5x/introducing_bilgecan_selfhosted_opensource_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1podb5x/introducing_bilgecan_selfhosted_opensource_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T20:54:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pokm99</id>
    <title>Coding agent tool for Local Ollama</title>
    <updated>2025-12-17T02:11:56+00:00</updated>
    <author>
      <name>/u/FrontRegular6113</name>
      <uri>https://old.reddit.com/user/FrontRegular6113</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; I have been using Ollama for over a year, mostly with various models through the OpenWebUI chat interface. I am now looking for something roughly equivalent to Claude Code, Cursor, or Codex, etc, for the local Ollama.&lt;/p&gt; &lt;p&gt;Is anyone using a similar coding-agent tool productively with a local Ollama setup, comparable to cloud-based coding agent tools?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrontRegular6113"&gt; /u/FrontRegular6113 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pokm99/coding_agent_tool_for_local_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pokm99/coding_agent_tool_for_local_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pokm99/coding_agent_tool_for_local_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-17T02:11:56+00:00</published>
  </entry>
</feed>
