<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-02T17:28:12+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qs4fsv</id>
    <title>Llm for personal health</title>
    <updated>2026-01-31T14:56:14+00:00</updated>
    <author>
      <name>/u/pyare-p13</name>
      <uri>https://old.reddit.com/user/pyare-p13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pyare-p13"&gt; /u/pyare-p13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ArtificialInteligence/comments/1qs4ev5/llm_for_personal_health/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qs4fsv/llm_for_personal_health/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qs4fsv/llm_for_personal_health/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-31T14:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrkbsr</id>
    <title>Run Ollama on your Android!</title>
    <updated>2026-01-30T22:32:38+00:00</updated>
    <author>
      <name>/u/DutchOfBurdock</name>
      <uri>https://old.reddit.com/user/DutchOfBurdock</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Want to put this out here. I have a Samsung S20 and a Pixel 8 Pro. Both of these devices pack 12GB of RAM, one an octacore arrangement and the other a nonacore. Now, this is pure CPU and even Vulkan (despite hardware support), doesn't work.&lt;/p&gt; &lt;p&gt;First, get yourself Termux from F-Droid or GitHub. &lt;strong&gt;Don't use the Play Store version.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Upon launching Termux, update the package manager and install some things needed..&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pkg up pkg i build-essential git cmake golang git clone https://github.com/ollama/ollama.git cd ollama go generate ./... go build . &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If all went well, you'll end up with an &lt;code&gt;ollama&lt;/code&gt; executable in the folder.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./ollama serve &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Open a new terminal in the gitted ollama folder &lt;/p&gt; &lt;pre&gt;&lt;code&gt;./ollama pull smollm2 ./ollama run smollm2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This model should be small enough for even 4GB devices and is pretty fast.&lt;/p&gt; &lt;p&gt;Enjoy and start exploring!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DutchOfBurdock"&gt; /u/DutchOfBurdock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrkbsr/run_ollama_on_your_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrkbsr/run_ollama_on_your_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qrkbsr/run_ollama_on_your_android/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T22:32:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsjp2s</id>
    <title>Free AI Tool Training - 100 Licenses (Claude Code, Claude Desktop, OpenClaw)</title>
    <updated>2026-02-01T00:47:54+00:00</updated>
    <author>
      <name>/u/SeriousDocument7905</name>
      <uri>https://old.reddit.com/user/SeriousDocument7905</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qsjp2s/free_ai_tool_training_100_licenses_claude_code/"&gt; &lt;img alt="Free AI Tool Training - 100 Licenses (Claude Code, Claude Desktop, OpenClaw)" src="https://external-preview.redd.it/BfjyCn6CzN8PQQImI976uTwVuuLCP3_lcIwslOoXhTg.png?width=140&amp;amp;height=73&amp;amp;auto=webp&amp;amp;s=d9576a1d44e11039aeaeaee9e2c35a43613ec8bb" title="Free AI Tool Training - 100 Licenses (Claude Code, Claude Desktop, OpenClaw)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SeriousDocument7905"&gt; /u/SeriousDocument7905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/vibecoding/comments/1qsj42l/free_ai_tool_training_100_licenses_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qsjp2s/free_ai_tool_training_100_licenses_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qsjp2s/free_ai_tool_training_100_licenses_claude_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T00:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qry7r9</id>
    <title>[Ollama Cloud] 29.7% failure rate, 3,500+ errors in one session, support ignoring tickets for 2 weeks - Is this normal?</title>
    <updated>2026-01-31T09:46:10+00:00</updated>
    <author>
      <name>/u/Few-Point-3626</name>
      <uri>https://old.reddit.com/user/Few-Point-3626</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;'ve been using Ollama Cloud API for my production workflow (content moderation)&lt;/p&gt; &lt;p&gt;and I'm experiencing catastrophic reliability issues that are making the service&lt;/p&gt; &lt;p&gt;unusable.&lt;/p&gt; &lt;p&gt;## The Numbers (documented with full logs)&lt;/p&gt; &lt;p&gt;| Metric | Value |&lt;/p&gt; &lt;p&gt;|--------|-------|&lt;/p&gt; &lt;p&gt;| Total requests sent | 4,079 |&lt;/p&gt; &lt;p&gt;| Successful responses | 2,868 |&lt;/p&gt; &lt;p&gt;| **Failed requests** | **1,211** |&lt;/p&gt; &lt;p&gt;| **Failure rate** | **29.7%** |&lt;/p&gt; &lt;p&gt;## Incident Timeline&lt;/p&gt; &lt;p&gt;| Date | Error 429 | Error 500 | Success Rate |&lt;/p&gt; &lt;p&gt;|------|-----------|-----------|--------------|&lt;/p&gt; &lt;p&gt;| Dec 10, 2025 | 235 | 0 | 0% |&lt;/p&gt; &lt;p&gt;| Dec 20, 2025 | 0 | 30 | 0% |&lt;/p&gt; &lt;p&gt;| **Jan 4, 2026** | **3,508** | 0 | **0%** |&lt;/p&gt; &lt;p&gt;| Jan 29, 2026 | 0 | 0 | 86.8% |&lt;/p&gt; &lt;p&gt;| Jan 30, 2026 | 0 | 0 | 74.3% |&lt;/p&gt; &lt;p&gt;| **Jan 31, 2026** | 0 | **194** | **28.8%** |&lt;/p&gt; &lt;p&gt;Yes, you read that right: **3,508 consecutive 429 errors in 40 minutes** on&lt;/p&gt; &lt;p&gt;January 4th.&lt;/p&gt; &lt;p&gt;## The Pattern&lt;/p&gt; &lt;p&gt;Every session follows the same pattern:&lt;/p&gt; &lt;p&gt;- ~30 requests succeed normally&lt;/p&gt; &lt;p&gt;- Then the server crashes with 500 errors&lt;/p&gt; &lt;p&gt;- All subsequent requests fail&lt;/p&gt; &lt;p&gt;- I have to restart and hope for the best&lt;/p&gt; &lt;p&gt;## My Configuration&lt;/p&gt; &lt;p&gt;- Model: deepseek-v3.1:671b&lt;/p&gt; &lt;p&gt;- Concurrent requests: 3 (using 3 separate API keys)&lt;/p&gt; &lt;p&gt;- Workers per key: 1 (minimal load)&lt;/p&gt; &lt;p&gt;- Timeout: 25 seconds&lt;/p&gt; &lt;p&gt;I'm not hammering the API. 3 concurrent requests with 3 different API keys is&lt;/p&gt; &lt;p&gt;extremely conservative.&lt;/p&gt; &lt;p&gt;## Support Response&lt;/p&gt; &lt;p&gt;I opened a support ticket on **January 18th, 2026**.&lt;/p&gt; &lt;p&gt;**Response received: NONE.**&lt;/p&gt; &lt;p&gt;It's been 2 weeks. Radio silence. No acknowledgment, no &amp;quot;we're looking into it&amp;quot;,&lt;/p&gt; &lt;p&gt;nothing.&lt;/p&gt; &lt;p&gt;## Questions for the Community&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Is anyone else experiencing similar issues with deepseek models on Ollama Cloud?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Is this level of unreliability normal?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Has anyone actually gotten a response from Ollama support (&lt;a href="mailto:hello@ollama.com"&gt;hello@ollama.com&lt;/a&gt;)?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Are there alternative providers for deepseek-v3 that are more reliable?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;## What I'm Asking Ollama&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Investigate why your servers are returning 3,500+ 429 errors in a single session&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Investigate the 500 errors that crash the service after ~30 requests&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Respond to support tickets&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Credit for the failed requests that were still billed&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I have complete logs documenting every single error with timestamps. Happy to&lt;/p&gt; &lt;p&gt;share with Ollama support if they ever decide to respond.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**Edit:** I'll update this post if/when I get a response.&lt;/p&gt; &lt;p&gt;**Edit 2:** For those asking, my use case is legitimate content moderation for a&lt;/p&gt; &lt;p&gt;French platform. ~200-300 requests per day, nothing excessive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Point-3626"&gt; /u/Few-Point-3626 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qry7r9/ollama_cloud_297_failure_rate_3500_errors_in_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qry7r9/ollama_cloud_297_failure_rate_3500_errors_in_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qry7r9/ollama_cloud_297_failure_rate_3500_errors_in_one/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-31T09:46:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qslux9</id>
    <title>`Request timed out` when running `ollama launch claude` with `glm-4.7-flash:latest`</title>
    <updated>2026-02-01T02:23:15+00:00</updated>
    <author>
      <name>/u/o-rka</name>
      <uri>https://old.reddit.com/user/o-rka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running &lt;code&gt;claude-code&lt;/code&gt; via &lt;code&gt;ollama&lt;/code&gt; using the &lt;code&gt;glm-4.7-flash:latest&lt;/code&gt; model on a M4 MacMini and I've made sure to adjust my context window to 64k. Here's the specs below: &lt;/p&gt; &lt;p&gt;``` Chip: Apple M4 Pro Total Number of Cores: 14 (10 performance and 4 efficiency) Memory: 64 GB&lt;/p&gt; &lt;pre&gt;&lt;code&gt; Type: GPU Bus: Built-In Total Number of Cores: 20 Vendor: Apple (0x106b) Metal Support: Metal 3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Is there any other settings I can adjust or is my machine not powerful enough to handle the task? &lt;/p&gt; &lt;p&gt;The task being to modify a Nextflow pipeline based on the specifications in my &lt;code&gt;CLAUDE.md&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/o-rka"&gt; /u/o-rka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qslux9/request_timed_out_when_running_ollama_launch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qslux9/request_timed_out_when_running_ollama_launch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qslux9/request_timed_out_when_running_ollama_launch/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T02:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt661t</id>
    <title>Does that even make sense?</title>
    <updated>2026-02-01T18:21:36+00:00</updated>
    <author>
      <name>/u/artwik22</name>
      <uri>https://old.reddit.com/user/artwik22</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a homelab running on Intel n97 and 16gb of ram. Is there any llm model I could run?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/artwik22"&gt; /u/artwik22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt661t/does_that_even_make_sense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt661t/does_that_even_make_sense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qt661t/does_that_even_make_sense/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T18:21:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs233a</id>
    <title>Best open weight llm model to run with 8gb of vram</title>
    <updated>2026-01-31T13:16:18+00:00</updated>
    <author>
      <name>/u/Sweazou</name>
      <uri>https://old.reddit.com/user/Sweazou</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd like to get your thought on the best model you can use with 8gb of vram in 2026, with the best performance possible for general purpose and coding, the least censorship possible, i know this won't be as good as state of the art llm but i'd like to try something good i can run locally&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sweazou"&gt; /u/Sweazou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qs233a/best_open_weight_llm_model_to_run_with_8gb_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qs233a/best_open_weight_llm_model_to_run_with_8gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qs233a/best_open_weight_llm_model_to_run_with_8gb_of_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-31T13:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsqs24</id>
    <title>The two agentic loops - the architectural insight in how we built and scaled agents</title>
    <updated>2026-02-01T06:23:19+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey peeps - been building agents for the Fortune500 and seeing some patterns emerge that cut the &lt;strong&gt;gargantuan gap&lt;/strong&gt; from prototype to production &lt;/p&gt; &lt;p&gt;The post below introduces the concept of &amp;quot;two agentic loops&amp;quot;: the inner loop that handles reasoning and tool use, while the outer loop handles everything that makes agents ready for production‚Äîorchestration, guardrails, observability, and bounded execution. The outer loop is real infrastructure that needs to be built and maintained independently in a framework-friendly and protocol-first way. Hope you enjoy the read&lt;/p&gt; &lt;p&gt;&lt;a href="https://planoai.dev/blog/the-two-agentic-loops-how-to-design-and-scale-agentic-apps"&gt;https://planoai.dev/blog/the-two-agentic-loops-how-to-design-and-scale-agentic-apps&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qsqs24/the_two_agentic_loops_the_architectural_insight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qsqs24/the_two_agentic_loops_the_architectural_insight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qsqs24/the_two_agentic_loops_the_architectural_insight/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T06:23:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt42lf</id>
    <title>I can not have a quick respond when using Ollama run with Claude on my local machine</title>
    <updated>2026-02-01T17:07:21+00:00</updated>
    <author>
      <name>/u/Cultural_Somewhere70</name>
      <uri>https://old.reddit.com/user/Cultural_Somewhere70</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qt42lf/i_can_not_have_a_quick_respond_when_using_ollama/"&gt; &lt;img alt="I can not have a quick respond when using Ollama run with Claude on my local machine" src="https://b.thumbs.redditmedia.com/MPmmxYIQweWKa7Qu1T02Wh1p477Klw0-CIHtjRDGEWQ.jpg" title="I can not have a quick respond when using Ollama run with Claude on my local machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I am student in back end developer. I just found that we can run Ollama by Claude on local machine.&lt;/p&gt; &lt;p&gt;I just made it by the blog guideline and it was installed. But i actually facing some issues:&lt;/p&gt; &lt;p&gt;- I really want to know why it reply so slow, is that because i don't have GPU cause now i run it on CPU.&lt;/p&gt; &lt;p&gt;- How many RAM gb should i upgrade to make it faster? Current 24gb Ram.&lt;/p&gt; &lt;p&gt;- How do you run ollama by claude on your laptop?&lt;/p&gt; &lt;p&gt;- what i actually need to add and upgrade to run a quick respond by using AI local?&lt;/p&gt; &lt;p&gt;I am really appreciate!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p5ze5zd0zwgg1.png?width=872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c412ac53782d5194cd8055afb582551ddab9d1db"&gt;https://preview.redd.it/p5ze5zd0zwgg1.png?width=872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c412ac53782d5194cd8055afb582551ddab9d1db&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cultural_Somewhere70"&gt; /u/Cultural_Somewhere70 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt42lf/i_can_not_have_a_quick_respond_when_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt42lf/i_can_not_have_a_quick_respond_when_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qt42lf/i_can_not_have_a_quick_respond_when_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T17:07:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt8fph</id>
    <title>Sentinel: Monitoring logs with local AI (Ollama) &amp; .NET 8</title>
    <updated>2026-02-01T19:40:45+00:00</updated>
    <author>
      <name>/u/Itsaliensbro453</name>
      <uri>https://old.reddit.com/user/Itsaliensbro453</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Itsaliensbro453"&gt; /u/Itsaliensbro453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/SideProject/comments/1qt8fc1/sentinel_monitoring_logs_with_local_ai_ollama_net/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt8fph/sentinel_monitoring_logs_with_local_ai_ollama_net/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qt8fph/sentinel_monitoring_logs_with_local_ai_ollama_net/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T19:40:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtjq5i</id>
    <title>I was tired of benchmarking models on my mac, so I made Anubis</title>
    <updated>2026-02-02T03:18:47+00:00</updated>
    <author>
      <name>/u/peppaz</name>
      <uri>https://old.reddit.com/user/peppaz</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peppaz"&gt; /u/peppaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://devpadapp.com/anubis/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtjq5i/i_was_tired_of_benchmarking_models_on_my_mac_so_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtjq5i/i_was_tired_of_benchmarking_models_on_my_mac_so_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T03:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt5x7i</id>
    <title>Ollama on R9700 AI Pro</title>
    <updated>2026-02-01T18:12:55+00:00</updated>
    <author>
      <name>/u/grimescene2</name>
      <uri>https://old.reddit.com/user/grimescene2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello fellow Radeonans (I just made that up)&lt;/p&gt; &lt;p&gt;I recently procured the Radeon R9700 AI pro GPU with 32gb VRAM. The experience has been solid so far with Comfyui / Flux generation on Windows 11.&lt;/p&gt; &lt;p&gt;But I have not been able to run Ollama properly on the machine. The installation doesn‚Äôt detect the card, and then even after doing some hacks in the Environment Variables (thanks for Gemini) only the smaller (3-4B) models work. Anything greater than 8B just crashes it.&lt;/p&gt; &lt;p&gt;Has anyone here had similar experiences? Any fixes?&lt;/p&gt; &lt;p&gt;Would appreciate guidance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grimescene2"&gt; /u/grimescene2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt5x7i/ollama_on_r9700_ai_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt5x7i/ollama_on_r9700_ai_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qt5x7i/ollama_on_r9700_ai_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T18:12:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtjsmr</id>
    <title>AITAH for being upset with my partner for saying, "I'm going to beat you"</title>
    <updated>2026-02-02T03:21:55+00:00</updated>
    <author>
      <name>/u/4evrloyal-cowgirlife</name>
      <uri>https://old.reddit.com/user/4evrloyal-cowgirlife</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/4evrloyal-cowgirlife"&gt; /u/4evrloyal-cowgirlife &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AITAH/comments/1qtjrxd/aitah_for_being_upset_with_my_partner_for_saying/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtjsmr/aitah_for_being_upset_with_my_partner_for_saying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtjsmr/aitah_for_being_upset_with_my_partner_for_saying/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T03:21:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsjn38</id>
    <title>Running Ollama fully air-gapped, anyone else?</title>
    <updated>2026-02-01T00:45:31+00:00</updated>
    <author>
      <name>/u/thefilthybeard</name>
      <uri>https://old.reddit.com/user/thefilthybeard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been building AI tools that run fully air-gapped for classified environments. No internet, no cloud, everything local.&lt;/p&gt; &lt;p&gt;Ollama has been solid for this. Running it on hardware that never touches a network. Biggest challenges were model selection (needed stuff that performs well without massive VRAM) and building workflows that don't assume any external API calls.&lt;/p&gt; &lt;p&gt;Curious what others are doing for fully offline deployments. Anyone else running Ollama in secure or disconnected environments? What models are you using and what are you running it on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thefilthybeard"&gt; /u/thefilthybeard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qsjn38/running_ollama_fully_airgapped_anyone_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qsjn38/running_ollama_fully_airgapped_anyone_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qsjn38/running_ollama_fully_airgapped_anyone_else/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T00:45:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt6kik</id>
    <title>Vlm models on cpu</title>
    <updated>2026-02-01T18:35:21+00:00</updated>
    <author>
      <name>/u/uqurluuqur</name>
      <uri>https://old.reddit.com/user/uqurluuqur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt; &lt;p&gt;I am tasked to convert handwritten notebook texts. I have tried several models including:&lt;/p&gt; &lt;p&gt;Qwen2.5vl- 7b&lt;/p&gt; &lt;p&gt;Qwen2.5vl- 32b&lt;/p&gt; &lt;p&gt;Qwen3vl-32b&lt;/p&gt; &lt;p&gt;Llama3.2-vision11b&lt;/p&gt; &lt;p&gt;However, i am struggling with hallucinations. Instead of writing unable to read (which i ask for it in the prompt), models often start to hallucinate or getting stuck in the header (repeat loop). Improving or trying other prompts did not helped. I have tried preprocessing, which improved the quality but did not prevent hallucinations. Do you have any suggestions?&lt;/p&gt; &lt;p&gt;I have amd threadripper cpu and 64 gb ram. Speed is not an issue since it is a one time thing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uqurluuqur"&gt; /u/uqurluuqur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt6kik/vlm_models_on_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt6kik/vlm_models_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qt6kik/vlm_models_on_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T18:35:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt04u5</id>
    <title>OpenClaw For data scientist that support Ollama</title>
    <updated>2026-02-01T14:39:27+00:00</updated>
    <author>
      <name>/u/jasonhon2013</name>
      <uri>https://old.reddit.com/user/jasonhon2013</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qt04u5/openclaw_for_data_scientist_that_support_ollama/"&gt; &lt;img alt="OpenClaw For data scientist that support Ollama" src="https://external-preview.redd.it/6n0lheaeZysEyY3dN5Kt5g7XRf3lYCD1kKO5LEgkWkA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f1f98cfb816d1f258b4f3b19195340579de5e4d" title="OpenClaw For data scientist that support Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an open-source tool that works like OpenClaw (i.e., web searches all the necessary content in the background and provides you with data). It supports Ollama. You can give it a try‚Äîhehe, and maybe give me a little star as well!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonhon2013"&gt; /u/jasonhon2013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/JasonHonKL/PardusClawer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt04u5/openclaw_for_data_scientist_that_support_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qt04u5/openclaw_for_data_scientist_that_support_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T14:39:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtk2kr</id>
    <title>ollama cloud always 503 overload error</title>
    <updated>2026-02-02T03:34:35+00:00</updated>
    <author>
      <name>/u/Plenty_Umpire585</name>
      <uri>https://old.reddit.com/user/Plenty_Umpire585</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;503 {&amp;quot;type&amp;quot;:&amp;quot;error&amp;quot;,&amp;quot;error&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;overloaded_error&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Service Temporarily Unavailable&amp;quot;}&lt;/p&gt; &lt;p&gt;It happens too often&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plenty_Umpire585"&gt; /u/Plenty_Umpire585 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtk2kr/ollama_cloud_always_503_overload_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtk2kr/ollama_cloud_always_503_overload_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtk2kr/ollama_cloud_always_503_overload_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T03:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt7wzm</id>
    <title>Reprompt - Simple desktop GUI application to avoid writing the same prompts repeatedly</title>
    <updated>2026-02-01T19:22:11+00:00</updated>
    <author>
      <name>/u/PuzzleheadedHeat9056</name>
      <uri>https://old.reddit.com/user/PuzzleheadedHeat9056</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qt7wzm/reprompt_simple_desktop_gui_application_to_avoid/"&gt; &lt;img alt="Reprompt - Simple desktop GUI application to avoid writing the same prompts repeatedly" src="https://preview.redd.it/u520ax05nxgg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=b849674b757f4c804d3b68e8106d96bd54802b4a" title="Reprompt - Simple desktop GUI application to avoid writing the same prompts repeatedly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'd like to share the app I created last summer, and have been using it since then.&lt;br /&gt; It is called Reprompt - &lt;a href="https://github.com/grouzen/reprompt"&gt;https://github.com/grouzen/reprompt&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;It is a simple desktop GUI app written in Rust and egui that allows users to ask models the same questions without having to type the prompts repeatedly.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;I personally found it useful for language-related tasks, such as translation, correcting typos, and improving grammar. Currently, it supports Ollama only, but other providers can be easily added if needed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PuzzleheadedHeat9056"&gt; /u/PuzzleheadedHeat9056 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u520ax05nxgg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt7wzm/reprompt_simple_desktop_gui_application_to_avoid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qt7wzm/reprompt_simple_desktop_gui_application_to_avoid/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T19:22:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtzq2x</id>
    <title>Environmental Impact</title>
    <updated>2026-02-02T16:27:29+00:00</updated>
    <author>
      <name>/u/King_Penguin0s</name>
      <uri>https://old.reddit.com/user/King_Penguin0s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I've been really trying to cut down on my use of AI lately due to the environmental impacts as that's something I'm very passionate about. However there are some things In my workflow that I just can't live without anymore.&lt;/p&gt; &lt;p&gt;From this, I came across Ollama and the idea of running models locally and I'm wondering if doing this has the same, a better or worse environmental impact?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/King_Penguin0s"&gt; /u/King_Penguin0s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtzq2x/environmental_impact/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtzq2x/environmental_impact/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtzq2x/environmental_impact/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T16:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtzi14</id>
    <title>why does ollama pull a pre pulled model ? and how to prevent it ?</title>
    <updated>2026-02-02T16:19:32+00:00</updated>
    <author>
      <name>/u/Hot_Arachnid3547</name>
      <uri>https://old.reddit.com/user/Hot_Arachnid3547</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ollama run qwen2.5-coder:14b&lt;br /&gt; pulling manifest &lt;br /&gt; pulling ac9bc7a69dab: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 9.0 GB &lt;br /&gt; pulling 66b9ea09bd5b: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 68 B &lt;br /&gt; pulling 1e65450c3067: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.6 KB &lt;br /&gt; pulling 832dd9e00a68: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 11 KB &lt;br /&gt; pulling 0578f229f23a: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 488 B &lt;br /&gt; verifying sha256 digest &lt;br /&gt; writing manifest &lt;br /&gt; success &lt;/p&gt; &lt;p&gt;ollama list&lt;br /&gt; NAME ID SIZE MODIFIED &lt;br /&gt; qwen2.5-coder:14b 9ec8897f747e 9.0 GB 25 minutes ago &lt;br /&gt; llama2:latest 78e26419b446 3.8 GB 7 months ago &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hot_Arachnid3547"&gt; /u/Hot_Arachnid3547 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtzi14/why_does_ollama_pull_a_pre_pulled_model_and_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtzi14/why_does_ollama_pull_a_pre_pulled_model_and_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtzi14/why_does_ollama_pull_a_pre_pulled_model_and_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T16:19:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtxk63</id>
    <title>See what your AI agents see while browsing the web</title>
    <updated>2026-02-02T15:08:58+00:00</updated>
    <author>
      <name>/u/BlitzBrowser_</name>
      <uri>https://old.reddit.com/user/BlitzBrowser_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qtxk63/see_what_your_ai_agents_see_while_browsing_the_web/"&gt; &lt;img alt="See what your AI agents see while browsing the web" src="https://external-preview.redd.it/irW4oDS7drKOXE6ZPTLy9XVVo2a8cDoITkvasjyhu4o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd5adcc1b821911133871098276d826a0fcea755" title="See what your AI agents see while browsing the web" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlitzBrowser_"&gt; /u/BlitzBrowser_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rf1c38pzj3hg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtxk63/see_what_your_ai_agents_see_while_browsing_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtxk63/see_what_your_ai_agents_see_while_browsing_the_web/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T15:08:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtxzjp</id>
    <title>Ram issue</title>
    <updated>2026-02-02T15:24:46+00:00</updated>
    <author>
      <name>/u/Clear_Move_7686</name>
      <uri>https://old.reddit.com/user/Clear_Move_7686</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qtxzjp/ram_issue/"&gt; &lt;img alt="Ram issue" src="https://preview.redd.it/0x3m4cb1n3hg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0d104c6c175616106098ac45b5d9db17dca8db6" title="Ram issue" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, i was wondering why did it suddenly say i don't have enough ram to use qwen3:4b when i do have enough ram, and i did literally use it multiple times in the past (i deleted the chats). So why is it suddenly telling me i don't have enough??? For reference i have 32gb of ddr4. Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Clear_Move_7686"&gt; /u/Clear_Move_7686 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0x3m4cb1n3hg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtxzjp/ram_issue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtxzjp/ram_issue/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T15:24:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtqp9p</id>
    <title>175k+ publicly exposed Ollama servers, so I built a tool</title>
    <updated>2026-02-02T09:41:37+00:00</updated>
    <author>
      <name>/u/truthfly</name>
      <uri>https://old.reddit.com/user/truthfly</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/truthfly"&gt; /u/truthfly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qtqobb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtqp9p/175k_publicly_exposed_ollama_servers_so_i_built_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtqp9p/175k_publicly_exposed_ollama_servers_so_i_built_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T09:41:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtine0</id>
    <title>Released: VOR ‚Äî a hallucination-free runtime that forces LLMs to prove answers or abstain</title>
    <updated>2026-02-02T02:31:03+00:00</updated>
    <author>
      <name>/u/CulpritChaos</name>
      <uri>https://old.reddit.com/user/CulpritChaos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just open-sourced a project that might interest people here who are tired of hallucinations being treated as ‚Äújust a prompt issue.‚Äù VOR (Verified Observation Runtime) is a runtime layer that sits around LLMs and retrieval systems and enforces one rule: If an answer cannot be proven from observed evidence, the system must abstain. Highlights: 0.00% hallucination across demo + adversarial packs Explicit CONFLICT detection (not majority voting) Deterministic audits (hash-locked, replayable) Works with local models ‚Äî the verifier doesn‚Äôt care which LLM you use Clean-room witness instructions included This is not another RAG framework. It‚Äôs a governor for reasoning: models can propose, but they don‚Äôt decide. Public demo includes: CLI (neuralogix qa, audit, pack validate) Two packs: a normal demo corpus + a hostile adversarial pack Full test suite (legacy tests quarantined) Repo: &lt;a href="https://github.com/CULPRITCHAOS/VOR"&gt;https://github.com/CULPRITCHAOS/VOR&lt;/a&gt; Tag: v0.7.3-public.1 Witness guide: docs/WITNESS_RUN_MESSAGE.txt I‚Äôm looking for: People to run it locally (Windows/Linux/macOS) Ideas for harder adversarial packs Discussion on where a runtime like this fits in local stacks (Ollama, LM Studio, etc.) Happy to answer questions or take hits. This was built to be challenged.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CulpritChaos"&gt; /u/CulpritChaos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtine0/released_vor_a_hallucinationfree_runtime_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtine0/released_vor_a_hallucinationfree_runtime_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtine0/released_vor_a_hallucinationfree_runtime_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T02:31:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtyp8e</id>
    <title>üî• New to DGX ‚Äî Looking for Advice on Best AI Models &amp; Deployments!</title>
    <updated>2026-02-02T15:51:19+00:00</updated>
    <author>
      <name>/u/Character-Town-8188</name>
      <uri>https://old.reddit.com/user/Character-Town-8188</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt; &lt;p&gt;I recently acquired a NVIDIA DGX (Spark DGX) system, and I‚Äôm super excited to start putting it to good use. However, I‚Äôd really appreciate some community insight on what real-world AI workloads/models I should run to make the most out of this beast.&lt;/p&gt; &lt;p&gt;üß† What I‚Äôm Looking For&lt;/p&gt; &lt;p&gt;I want to:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Deploy AI models that make sense for this hardware ‚Ä¢ Use cases that are practical, impactful, and leverage the GPU power ‚Ä¢ Learn from others who have experience optimizing &amp;amp; deploying large models &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;üìå Questions I Have&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. What are the best models to run on a DGX today? ‚Ä¢ LLMs (which sizes?) ‚Ä¢ Vision models? ‚Ä¢ Multimodal? ‚Ä¢ Reinforcement learning? 2. Are there open-source alternatives worth deploying? (e.g., LLaMA, Stable Diffusion, Falcon, etc.) 3. What deployment frameworks do folks recommend? ‚Ä¢ Triton? ‚Ä¢ Ray? ‚Ä¢ Kubernetes? ‚Ä¢ Hugging Face Accelerate? 4. Do you have recommendations for benchmarking, optimizing performance, and scaling? 5. What real-world use cases have you found valuable ‚Äî inference, fine-tuning, research workloads, generative AI, embeddings, etc.? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;üõ†Ô∏è Some Context (Optional Details about My Setup)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ NVIDIA Spark DGX ‚Ä¢ (Optional): RAM/Storage you have ‚Ä¢ (Optional): Intended use ‚Äî research, product development, experimentation &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;üôè Thank You!&lt;/p&gt; &lt;p&gt;I‚Äôm eager to hear what you think ‚Äî whether it‚Äôs cool model recommendations, deployment tips, or links to open-source projects that run well on DGX hardware.&lt;/p&gt; &lt;p&gt;Thanks so much in advance! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Character-Town-8188"&gt; /u/Character-Town-8188 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtyp8e/new_to_dgx_looking_for_advice_on_best_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtyp8e/new_to_dgx_looking_for_advice_on_best_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtyp8e/new_to_dgx_looking_for_advice_on_best_ai_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T15:51:19+00:00</published>
  </entry>
</feed>
