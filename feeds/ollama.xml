<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-18T21:48:48+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qbwnjf</id>
    <title>Seeking Advice: Deploying Local LLMs for a Large-Scale Food &amp; Goods Distributor</title>
    <updated>2026-01-13T17:04:14+00:00</updated>
    <author>
      <name>/u/JPedrroo</name>
      <uri>https://old.reddit.com/user/JPedrroo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I‚Äôm a Software Analyst and Developer for a major distribution company in the state of Bahia, Brazil. We handle a massive operation ranging from food and beverages to cosmetics and hygiene products, serving basically the entire state in terms of city coverage.&lt;/p&gt; &lt;p&gt;I am currently exploring the possibility of implementing a local AI infrastructure to enhance productivity while maintaining strict privacy over our data. I am not an expert in AI, so I am still figuring out the best way to start. I have tested some local LLMs on my laptop, but I am unfamiliar with the technical nuances involved in a large-scale corporate implementation.&lt;/p&gt; &lt;p&gt;Initially, I thought of developing a system that reads database entries regarding expiry dates and turnover rates in our warehouse. The goal would be to automatically recommend flash promotions or stock transfers to our retail branches before products expire.&lt;/p&gt; &lt;p&gt;I'm seeking any feedback on this‚Äîpast experiences, technical advice, additional use case ideas, or anything relevant. Thank you all for your time and for any insights you can share!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JPedrroo"&gt; /u/JPedrroo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbwnjf/seeking_advice_deploying_local_llms_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbwnjf/seeking_advice_deploying_local_llms_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qbwnjf/seeking_advice_deploying_local_llms_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-13T17:04:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbrx4b</id>
    <title>Open Source Enterprise Search Engine (Generative AI Powered)</title>
    <updated>2026-01-13T13:55:10+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I‚Äôm excited to share something we‚Äôve been building for the past 6 months, a &lt;strong&gt;fully open-source Enterprise Search Platform&lt;/strong&gt; designed to bring powerful Enterprise Search to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, Local file uploads and more. You can deploy it and run it with just one docker compose command.&lt;/p&gt; &lt;p&gt;You can run the full platform locally. Recently, one of our users tried &lt;strong&gt;qwen3-vl:8b (16 FP)&lt;/strong&gt; with &lt;strong&gt;Ollama&lt;/strong&gt; and got very good results.&lt;/p&gt; &lt;p&gt;The entire system is built on a &lt;strong&gt;fully event-streaming architecture powered by Kafka&lt;/strong&gt;, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data.&lt;/p&gt; &lt;p&gt;At the core, the system uses an &lt;strong&gt;Agentic Graph RAG approach&lt;/strong&gt;, where retrieval is guided by an enterprise knowledge graph and reasoning agents. Instead of treating documents as flat text, agents reason over relationships between users, teams, entities, documents, and permissions, allowing more accurate, explainable, and permission-aware answers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep understanding of documents, user, organization and teams with enterprise knowledge graph&lt;/li&gt; &lt;li&gt;Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama&lt;/li&gt; &lt;li&gt;Use any provider that supports OpenAI compatible endpoints&lt;/li&gt; &lt;li&gt;Choose from 1,000+ embedding models&lt;/li&gt; &lt;li&gt;Visual Citations for every answer&lt;/li&gt; &lt;li&gt;Vision-Language Models and OCR for visual or scanned docs&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, or SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs for developers&lt;/li&gt; &lt;li&gt;All major file types support including pdfs with images, diagrams and charts&lt;/li&gt; &lt;li&gt;Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more&lt;/li&gt; &lt;li&gt;Reasoning Agent that plans before executing tasks&lt;/li&gt; &lt;li&gt;40+ Connectors allowing you to connect to your entire business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo Video:&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=xA9m3pwOgz8"&gt;https://www.youtube.com/watch?v=xA9m3pwOgz8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbrx4b/open_source_enterprise_search_engine_generative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbrx4b/open_source_enterprise_search_engine_generative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qbrx4b/open_source_enterprise_search_engine_generative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-13T13:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcmwsr</id>
    <title>New Ollama Desktop Client</title>
    <updated>2026-01-14T13:12:40+00:00</updated>
    <author>
      <name>/u/Odd-Feature-645</name>
      <uri>https://old.reddit.com/user/Odd-Feature-645</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qcmwsr/new_ollama_desktop_client/"&gt; &lt;img alt="New Ollama Desktop Client" src="https://b.thumbs.redditmedia.com/OdW8FFrF846eY3ZikgZHdl99hund16GW1AOkssAFKNU.jpg" title="New Ollama Desktop Client" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/barni007-pro/ollama_desktop_client"&gt;GitHub Ollama Desktop&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d8ikcl97ebdg1.png?width=706&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a1ca574611ed0ad2925680166b0a51fe62e7401"&gt;https://preview.redd.it/d8ikcl97ebdg1.png?width=706&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a1ca574611ed0ad2925680166b0a51fe62e7401&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Hi everyone! I wanted to create a more powerful, native desktop experience for Ollama. Most clients are just simple chat wrappers, so I built &amp;quot;Ollama Desktop&amp;quot; in VB.NET 8 with a focus on advanced tools. GitHub: https://github.com/barni007-pro/ollamGitHubGitHuba_desktop_client üöÄ High-Impact Features: üß† Local RAG Tool: Chat with your large PDF and Text documents using local knowledge extraction. üëÅÔ∏è Vision Support: Upload images or take screenshots directly to analyze them with multimodal models like gemma3. üíª Code Interpreter: The model can execute Python, PowerShell, or Batch scripts locally. Great for task automation! üìÑ Document Context: Easily import .pdf, .txt, or .json files directly into the chat context. üß™ JSON Mode &amp;amp; Tools: Support for structured responses and function calling. üìê LaTeX Support: Beautiful rendering of mathematical formulas. üõ†Ô∏è Under the Hood: Built with .NET 8 and VB.NET. Fast, lightweight, and specifically designed for Windows. Model switching on-the-fly during conversations. I‚Äôm looking for feedback and would love to hear which features you‚Äôd like to see next! &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Feature-645"&gt; /u/Odd-Feature-645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qcmwsr/new_ollama_desktop_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qcmwsr/new_ollama_desktop_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qcmwsr/new_ollama_desktop_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-14T13:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcnpc8</id>
    <title>Created my own Agent interface for Nemotron-3</title>
    <updated>2026-01-14T13:47:42+00:00</updated>
    <author>
      <name>/u/jovn1234567890</name>
      <uri>https://old.reddit.com/user/jovn1234567890</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jovn1234567890"&gt; /u/jovn1234567890 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://github.com/jacobbw/Waterfall"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qcnpc8/created_my_own_agent_interface_for_nemotron3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qcnpc8/created_my_own_agent_interface_for_nemotron3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-14T13:47:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcu22b</id>
    <title>help please</title>
    <updated>2026-01-14T17:49:51+00:00</updated>
    <author>
      <name>/u/Real_Macaron_1880</name>
      <uri>https://old.reddit.com/user/Real_Macaron_1880</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm new to local AI. I want to set it up focused on analyzing PDF documents, legal documents, judgments... Could someone advise me? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Real_Macaron_1880"&gt; /u/Real_Macaron_1880 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qcu22b/help_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qcu22b/help_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qcu22b/help_please/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-14T17:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcsn8m</id>
    <title>We tried to automate product labeling in one prompt. It failed. 27 steps later, we've processed 10,000+ products.</title>
    <updated>2026-01-14T16:58:42+00:00</updated>
    <author>
      <name>/u/No-Reindeer-9968</name>
      <uri>https://old.reddit.com/user/No-Reindeer-9968</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Reindeer-9968"&gt; /u/No-Reindeer-9968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qcsn8m/we_tried_to_automate_product_labeling_in_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qcsn8m/we_tried_to_automate_product_labeling_in_one/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-14T16:58:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qd7dmx</id>
    <title>Hey all- I built a self-hosted MCP server to run AI semantic search over your own databases, files, and codebases. Supports Ollama and cloud providers if you want. Thought you all might find a good use for it.</title>
    <updated>2026-01-15T02:33:30+00:00</updated>
    <author>
      <name>/u/mattv8</name>
      <uri>https://old.reddit.com/user/mattv8</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mattv8"&gt; /u/mattv8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/selfhosted/comments/1qbv3fm/ragtime_a_selfhosted_mcp_server_to_run_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qd7dmx/hey_all_i_built_a_selfhosted_mcp_server_to_run_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qd7dmx/hey_all_i_built_a_selfhosted_mcp_server_to_run_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-15T02:33:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdjdqq</id>
    <title>Persistent "STATUS_ACCESS_VIOLATION" and Server Errors in Ollama UI ‚Äì Help needed!</title>
    <updated>2026-01-15T13:23:12+00:00</updated>
    <author>
      <name>/u/AlexHardy08</name>
      <uri>https://old.reddit.com/user/AlexHardy08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qdjdqq/persistent_status_access_violation_and_server/"&gt; &lt;img alt="Persistent &amp;quot;STATUS_ACCESS_VIOLATION&amp;quot; and Server Errors in Ollama UI ‚Äì Help needed!" src="https://a.thumbs.redditmedia.com/ZjHCNfIpvQ9jOw_q3GtOhl7JX96v9UqUGryE9LJsu98.jpg" title="Persistent &amp;quot;STATUS_ACCESS_VIOLATION&amp;quot; and Server Errors in Ollama UI ‚Äì Help needed!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gek41ylzjidg1.png?width=678&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=225df9e46ba2874be86e011c39b4881188ab02a4"&gt;https://preview.redd.it/gek41ylzjidg1.png?width=678&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=225df9e46ba2874be86e011c39b4881188ab02a4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been running into a frustrating issue with my Ollama UI setup for about two weeks now, and I‚Äôm wondering if anyone else is experiencing the same or if the devs are aware of it.&lt;/p&gt; &lt;p&gt;I keep getting the browser error &lt;strong&gt;&amp;quot;STATUS_ACCESS_VIOLATION&amp;quot;&lt;/strong&gt; (as seen in the attached screenshot). It happens quite frequently in some chat sessions, while others work fine for a while. Sometimes, it's accompanied by a generic &amp;quot;server error&amp;quot; message.&lt;/p&gt; &lt;p&gt;The biggest issue is that whenever this happens, the text generation stops immediately. If I‚Äôm working on something important or a long prompt, I have to refresh and start the generation all over again.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A few details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This started happening about 2 weeks ago.&lt;/li&gt; &lt;li&gt;It seems to happen randomly but frequently enough to disrupt the workflow.&lt;/li&gt; &lt;li&gt;I've tried refreshing, but the problem eventually comes back.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Does anyone know what exactly causes this? Is it a memory management issue, or something related to how the UI communicates with the Ollama backend?&lt;/p&gt; &lt;p&gt;If anyone has a fix or a workaround (browser settings, update versions, etc.), please let me know. Hopefully, the Ollama/UI team can look into this!&lt;/p&gt; &lt;p&gt;I use latest version of ollama&lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexHardy08"&gt; /u/AlexHardy08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qdjdqq/persistent_status_access_violation_and_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qdjdqq/persistent_status_access_violation_and_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qdjdqq/persistent_status_access_violation_and_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-15T13:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qd4snz</id>
    <title>Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )</title>
    <updated>2026-01-15T00:39:37+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qd4snz/building_opensource_client_sided_code/"&gt; &lt;img alt="Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )" src="https://external-preview.redd.it/dWh2ODN1ZnRzZWRnMege6VYazrCNvPvrU2GG8tcd-8T7OQo9iRCGUYxRaIOc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cf0b9c7665aefc3f8dce5125368451b88544d40" title="Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, guys, I m building GitNexus, an opensource Code Intelligence Engine which works fully client sided in-browser. Think of DeepWiki but with understanding of codebase relations like IMPORTS - CALLS -DEFINES -IMPLEMENTS- EXTENDS relations.&lt;/p&gt; &lt;p&gt;What all features would be useful, any integrations, cool ideas, etc?&lt;/p&gt; &lt;p&gt;site: &lt;a href="https://gitnexus.vercel.app/"&gt;https://gitnexus.vercel.app/&lt;/a&gt;&lt;br /&gt; repo: &lt;a href="https://github.com/abhigyanpatwari/GitNexus"&gt;https://github.com/abhigyanpatwari/GitNexus&lt;/a&gt; (A ‚≠ê might help me convince my CTO to allot little time for this :-) )&lt;/p&gt; &lt;p&gt;Everything including the DB engine, embeddings model etc works inside your browser.&lt;/p&gt; &lt;p&gt;It combines Graph query capabilities with standard code context tools like semantic search, BM 25 index, etc. Due to graph it should be able to perform Blast radius detection of code changes, codebase audit etc reliably.&lt;/p&gt; &lt;p&gt;Working on exposing the browser tab through MCP so claude code / cursor, etc can use it for codebase audits, deep context of code connections etc preventing it from making breaking changes due to missed upstream and downstream dependencies.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/clkvriftsedg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qd4snz/building_opensource_client_sided_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qd4snz/building_opensource_client_sided_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-15T00:39:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdvy1k</id>
    <title>Open Notebook 1.5 - Introducing i18n Support (we speak Chinese now) :)</title>
    <updated>2026-01-15T21:11:17+00:00</updated>
    <author>
      <name>/u/lfnovo</name>
      <uri>https://old.reddit.com/user/lfnovo</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lfnovo"&gt; /u/lfnovo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenNotebook/comments/1qdvxeg/open_notebook_15_introducing_i18n_support_we/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qdvy1k/open_notebook_15_introducing_i18n_support_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qdvy1k/open_notebook_15_introducing_i18n_support_we/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-15T21:11:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeixz8</id>
    <title>Preventing hallucinations - what's working for me</title>
    <updated>2026-01-16T15:27:04+00:00</updated>
    <author>
      <name>/u/Financial-Local-5543</name>
      <uri>https://old.reddit.com/user/Financial-Local-5543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run several Facebook groups in which we explore academic articles; I found Claude and perplexity helpful for summarizing them so readers can get a quick overview. Hallucinations, of course can be a problem. In the article I share what has been working for me too minimize and prevent this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial-Local-5543"&gt; /u/Financial-Local-5543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai-consciousness.org/reducing-hallucinations-in-ai-how-to-get-reliable-ai-summaries-of-complex-articles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qeixz8/preventing_hallucinations_whats_working_for_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qeixz8/preventing_hallucinations_whats_working_for_me/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T15:27:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qejeq8</id>
    <title>I built a Glass-Box AI workstation for Ollama that shows you the raw token stream</title>
    <updated>2026-01-16T15:44:29+00:00</updated>
    <author>
      <name>/u/Ollie_IDE</name>
      <uri>https://old.reddit.com/user/Ollie_IDE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I've been working on &lt;strong&gt;Ollie&lt;/strong&gt; ‚Äì a desktop app for local LLMs that gives you full visibility into what's happening during inference.&lt;/p&gt; &lt;p&gt;I wanted to see the raw token stream and context window in real-time, especially when debugging prompts or optimizing local model behavior.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes it different:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Glass-Box Interface:&lt;/strong&gt; Watch tokens generate live with granular character breakdowns. Audit tokens sent to your LLM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Programmable Agents:&lt;/strong&gt; Configure custom system prompts and tools directly in the UI. Build agents for your exact workflow.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hybrid Support:&lt;/strong&gt; Connect to local Ollama, more local and remote APIs also available.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi-Modal Workspace:&lt;/strong&gt; Built-in editors for code, rich text, 3D objects, images, and video. One workspace for different types of work.&lt;/p&gt; &lt;p&gt;&lt;a href="https://costa-and-associates.com/ollie"&gt;Download at Ollie IDE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ollie_IDE"&gt; /u/Ollie_IDE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qejeq8/i_built_a_glassbox_ai_workstation_for_ollama_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qejeq8/i_built_a_glassbox_ai_workstation_for_ollama_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qejeq8/i_built_a_glassbox_ai_workstation_for_ollama_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T15:44:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeatiw</id>
    <title>New version of Raspberry Pie Generative AI card (HAT+ 2)</title>
    <updated>2026-01-16T08:43:42+00:00</updated>
    <author>
      <name>/u/Unique_Winner_5927</name>
      <uri>https://old.reddit.com/user/Unique_Winner_5927</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perfect for private assistants, industrial equipment, proof of concept, ...&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/"&gt;https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;#RaspberryPi #DataSovereignty #EmbeddedAI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unique_Winner_5927"&gt; /u/Unique_Winner_5927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qeatiw/new_version_of_raspberry_pie_generative_ai_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qeatiw/new_version_of_raspberry_pie_generative_ai_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qeatiw/new_version_of_raspberry_pie_generative_ai_card/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T08:43:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qef38r</id>
    <title>Prompt tool I built/use with Ollama daily - render prompt variations without worrying about text files</title>
    <updated>2026-01-16T12:49:45+00:00</updated>
    <author>
      <name>/u/springwasser</name>
      <uri>https://old.reddit.com/user/springwasser</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted this to another subreddit, but should have posted it here.. sorry if you've seen it.&lt;/p&gt; &lt;p&gt;This is a tool I built because I use it in local development. I know there are solutions for these things mixed into other software, but this is standalone and does just one thing really well for me.&lt;/p&gt; &lt;p&gt;- create/version/store prompts.. don't have to worry about text files unless I want to&lt;br /&gt; - runs from command line, can pipe stdout into anything.. eg Ollama, ci, git hooks&lt;br /&gt; - easily render variations of prompts on the fly, inject {{variables}} or inject files.. e.g. git diffs or documents&lt;br /&gt; - can store prompts globally or in projects, run anywhere&lt;/p&gt; &lt;p&gt;Basic usage:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Create a prompt.. paste in text $ promptg prompt new my-prompt # -or- $ echo &amp;quot;Create a prompt with pipe&amp;quot; | promptg prompt save hello # Then.. $ promptg get my-prompt | ollama run deepseek-r1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or more advanced, render with dynamic variables and insert files..&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# before.. cat prompt.txt | sed &amp;quot;s/{{lang}}/Python/g; s/{{code}}/$(cat myfile.py)/g&amp;quot; | ollama run mistral # now, replace dynamic {{templateValue}} and insert code/file. promptg get code-review --var lang=Python --var code@myfile.py | ollama run mistral &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Install:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;npm install -g @promptg/cli &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/springwasser"&gt; /u/springwasser &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qef38r/prompt_tool_i_builtuse_with_ollama_daily_render/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qef38r/prompt_tool_i_builtuse_with_ollama_daily_render/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qef38r/prompt_tool_i_builtuse_with_ollama_daily_render/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T12:49:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qersd1</id>
    <title>Suggestion on Renting an AI server for a month</title>
    <updated>2026-01-16T20:50:07+00:00</updated>
    <author>
      <name>/u/100yearsofhappiness</name>
      <uri>https://old.reddit.com/user/100yearsofhappiness</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/100yearsofhappiness"&gt; /u/100yearsofhappiness &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalAIServers/comments/1qerrtn/suggestion_on_renting_an_ai_server_for_a_month/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qersd1/suggestion_on_renting_an_ai_server_for_a_month/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qersd1/suggestion_on_renting_an_ai_server_for_a_month/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T20:50:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qemwau</id>
    <title>Best Compute Per Dollar for AI?</title>
    <updated>2026-01-16T17:49:11+00:00</updated>
    <author>
      <name>/u/NetTechMan</name>
      <uri>https://old.reddit.com/user/NetTechMan</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NetTechMan"&gt; /u/NetTechMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/homelab/comments/1qemvsi/best_compute_per_dollar_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qemwau/best_compute_per_dollar_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qemwau/best_compute_per_dollar_for_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T17:49:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qen6w1</id>
    <title>The Preprocessing Gap Between RAG and Agentic</title>
    <updated>2026-01-16T17:59:44+00:00</updated>
    <author>
      <name>/u/OnyxProyectoUno</name>
      <uri>https://old.reddit.com/user/OnyxProyectoUno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RAG is the standard way to connect documents to LLMs. Most people building RAGs know the steps by now: parse documents, chunk them, embed, store vectors, retrieve at query time. But something different happens when you're building systems that act rather than answer.&lt;/p&gt; &lt;h3&gt;The RAG mental model&lt;/h3&gt; &lt;p&gt;RAG preprocessing optimizes for retrieval. Someone asks a question, you find relevant chunks, you synthesize an answer. The whole pipeline is designed around that interaction pattern.&lt;/p&gt; &lt;p&gt;The work happens before anyone asks anything. Documents get parsed into text, extracting content from PDFs, Word docs, HTML, whatever format you're working with. Then chunking splits that text into pieces sized for context windows. You choose a strategy based on your content: split on paragraphs, headings, or fixed token counts. Overlap between chunks preserves context across boundaries. Finally, embedding converts each chunk into a vector where similar meanings cluster together. &amp;quot;The contract expires in December&amp;quot; ends up near &amp;quot;Agreement termination date: 12/31/2024&amp;quot; even though they share few words. That's what makes semantic search work.&lt;/p&gt; &lt;p&gt;Retrieval is similarity search over those vectors. Query comes in, gets embedded, you find the nearest chunks in vector space. For Q&amp;amp;A, this works well. You ask a question, the system finds relevant passages, an LLM synthesizes an answer. The whole architecture assumes a query-response pattern.&lt;/p&gt; &lt;p&gt;The requirements shift when you're building systems that act instead of answer.&lt;/p&gt; &lt;h3&gt;What agentic actually needs&lt;/h3&gt; &lt;p&gt;Consider a contract monitoring system. It tracks obligations across hundreds of agreements: Example Bank owes a quarterly audit report by the 15th, so the system sends a reminder on the 10th, flags it as overdue on the 16th, and escalates to legal on the 20th. The system doesn't just find text about deadlines. It acts on them.&lt;/p&gt; &lt;p&gt;That requires something different at the data layer. The system needs to understand that Party A owes Party B deliverable X by date Y under condition Z. And it needs to connect those facts across documents. Not just find text about obligations, but actually know what's owed to whom and when.&lt;/p&gt; &lt;p&gt;The preprocessing has to pull out that structure, not just preserve text for later search. You're not chunking paragraphs. You're turning &amp;quot;Example Bank shall submit quarterly compliance reports within 15 days of quarter end&amp;quot; into data you can query: party, obligation type, deadline, conditions. Think rows in a database, not passages in a search index.&lt;/p&gt; &lt;h3&gt;Two parallel paths&lt;/h3&gt; &lt;p&gt;The architecture ends up looking completely different.&lt;/p&gt; &lt;p&gt;RAG has a linear pipeline. Documents go in, chunking happens, embeddings get created, vectors get stored. At query time, search, retrieve, generate.&lt;/p&gt; &lt;p&gt;Agentic systems need two tracks running in parallel. The main one pulls structured data out of documents. An LLM reads each contract, extracts the obligations, parties, dates, and conditions, and writes them to a graph database. Why a graph? Because you're not just storing isolated facts, you're storing how they connect. Example Bank owes a report. That report is due quarterly. The obligation comes from Section 4.2 of Contract #1847. Those connections between entities are what graph databases are built for. This is what powers the actual monitoring.&lt;/p&gt; &lt;p&gt;But you still need embeddings. Just for different reasons.&lt;/p&gt; &lt;p&gt;The second track catches what extraction misses. Sometimes &amp;quot;the Lender&amp;quot; in paragraph 12 needs to connect to &amp;quot;Example Bank&amp;quot; from paragraph 3. Sometimes you don't know what patterns matter until you see them repeated across documents. The vector search helps you find connections that weren't obvious enough to extract upfront.&lt;/p&gt; &lt;p&gt;So you end up with two databases working together. The graph database stores entities and their relationships: who owes what to whom by when. The vector database helps you find things you didn't know to look for.&lt;/p&gt; &lt;p&gt;I wrote the rest on my &lt;a href="https://nickrichu.me/posts/the-preprocessing-gap-between-rag-and-agentic"&gt;blog&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OnyxProyectoUno"&gt; /u/OnyxProyectoUno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qen6w1/the_preprocessing_gap_between_rag_and_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qen6w1/the_preprocessing_gap_between_rag_and_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qen6w1/the_preprocessing_gap_between_rag_and_agentic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T17:59:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qepzot</id>
    <title>Polymcp Integrates Ollama ‚Äì Local and Cloud Execution Made Simple</title>
    <updated>2026-01-16T19:41:16+00:00</updated>
    <author>
      <name>/u/Just_Vugg_PolyMCP</name>
      <uri>https://old.reddit.com/user/Just_Vugg_PolyMCP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qepzot/polymcp_integrates_ollama_local_and_cloud/"&gt; &lt;img alt="Polymcp Integrates Ollama ‚Äì Local and Cloud Execution Made Simple" src="https://external-preview.redd.it/uRBPZkkzgOSFpr8EhLBqbooPoKdELZzAq8ZwsFfCuh8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbb437599f34354b12d63c620969d4b8f9783757" title="Polymcp Integrates Ollama ‚Äì Local and Cloud Execution Made Simple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Polymcp integrates with Ollama for local and cloud execution!&lt;/p&gt; &lt;p&gt;You can seamlessly run models like gpt-oss:120b, Kimi K2, Nemotron, and others with just a few lines of code. Here‚Äôs a simple example of how to use gpt-oss:120b via Ollama:&lt;/p&gt; &lt;p&gt;from polymcp.polyagent import PolyAgent, OllamaProvider, OpenAIProvider&lt;/p&gt; &lt;p&gt;def create_llm_provider():&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;Use Ollama with gpt-oss:120b.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;return OllamaProvider(model=&amp;quot;gpt-oss:120b&amp;quot;)&lt;/p&gt; &lt;p&gt;def main():&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;Execute a task using PolyAgent.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;llm_provider = create_llm_provider()&lt;/p&gt; &lt;p&gt;agent = PolyAgent(llm_provider=llm_provider, mcp_servers=[&amp;quot;http://localhost:8000/mcp&amp;quot;])&lt;/p&gt; &lt;p&gt;query = &amp;quot;What is the capital of France?&amp;quot;&lt;/p&gt; &lt;p&gt;print(f&amp;quot;Query: {query}&amp;quot;)&lt;/p&gt; &lt;p&gt;response = agent.run(query)&lt;/p&gt; &lt;p&gt;print(f&amp;quot;Response: {response}\n&amp;quot;)&lt;/p&gt; &lt;p&gt;if __name__ == &amp;quot;__main__&amp;quot;:&lt;/p&gt; &lt;p&gt;main()&lt;/p&gt; &lt;p&gt;This integration makes it easy to run your models locally or in the cloud. No extra setup required‚Äîjust integrate, run, and go.&lt;/p&gt; &lt;p&gt;Let me know how you‚Äôre using it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Vugg_PolyMCP"&gt; /u/Just_Vugg_PolyMCP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/poly-mcp/Polymcp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qepzot/polymcp_integrates_ollama_local_and_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qepzot/polymcp_integrates_ollama_local_and_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T19:41:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qepvki</id>
    <title>Do you actually need prompt engineering to get value from AI?</title>
    <updated>2026-01-16T19:36:53+00:00</updated>
    <author>
      <name>/u/Xthebuilder</name>
      <uri>https://old.reddit.com/user/Xthebuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been using AI daily for about 6 months while building a local AI inferencing app, and one thing that surprised me is how little prompt engineering mattered compared to other factors.&lt;/p&gt; &lt;p&gt;What ended up making the biggest difference for me was:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;giving the model enough &lt;strong&gt;context&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;iterating on ideas &lt;em&gt;with&lt;/em&gt; the model before writing real code&lt;/li&gt; &lt;li&gt;choosing models that are actually good at the specific task&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Because LLMs have some randomness, I found they‚Äôre most useful early on, when you‚Äôre still figuring things out. Iterating with the model helped surface bad assumptions before I committed to an approach. They‚Äôre especially good at starting broad and narrowing down if you keep the conversation going so context builds up.&lt;/p&gt; &lt;p&gt;When I add new features now, I don‚Äôt explain my app‚Äôs architecture anymore. I just link the relevant GitHub repos so the model can see how things are structured. That alone cut feature dev time from weeks to about a day in one case.&lt;/p&gt; &lt;p&gt;I‚Äôm not saying prompt engineering is useless, just that for most practical work, context, iteration, and model choice mattered more for me.&lt;/p&gt; &lt;p&gt;Curious how others here approach this. Has prompt engineering been critical for you, or have you seen similar results?&lt;/p&gt; &lt;p&gt;(I wrote up the full experience here if anyone wants more detail: &lt;a href="https://xthebuilder.github.io"&gt;https://xthebuilder.github.io&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xthebuilder"&gt; /u/Xthebuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T19:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfauq9</id>
    <title>Help a noob figure out how to achieve something in a game engine with Ollama</title>
    <updated>2026-01-17T11:26:10+00:00</updated>
    <author>
      <name>/u/MountainPlantation</name>
      <uri>https://old.reddit.com/user/MountainPlantation</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I want to use Ollama to integrate it with a game engine. It's already in the engine and working, but I have some questions on what model I should use, and any tips in general for the experiments I want to do. I understand most LLMs running locally will take a while to think and generate a response, but for now let's ignore that.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;NPC Chat with commands: I know most people have tried doing NPC chatbots in engines, but I was thinking I could try to spice that up by integrating commands on it. Like the LLM would have a list of commands, given by me, that it could use contextually, like /laugh /cry /givePlayer(item), things like that. And I can make a system that parses the string and extracts/executes the commands. I attempted this one time, not in engine, just by using regular chat GPT and it would eventually come up with its own commands that were not stipulated by me. How to avoid that? Is there a model I should use for that?&lt;/li&gt; &lt;li&gt;NPC consistency in character. I also tried one time to keep chat GPT in character, a peasant from the medieval ages, but I would ask about modern events like COVID and it would eventually break and talk about it as if he knew what it was.&lt;/li&gt; &lt;li&gt;NPC Memory. What if I wanted to have NPCs remember things they have witnessed? I imagine I should make a log system that keeps every action done to that npc (NPC was hit by Player. NPC killed bandit. NPC found 1 gold etc) and then adding it to the beggining of the prompt as a little memory. Is that enough?&lt;/li&gt; &lt;li&gt;Can I reliably limit the response length or is it finicky? Like, setting a limit of how many words per response &lt;/li&gt; &lt;li&gt;Is there a way to guarantee responses are always in character? Because sometimes some of the LLMs will say &amp;quot;I cannot answer to things related to that&amp;quot; and that would be a big immersion breaker &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And another general question, is there a way to train certain models to get them used to a certani context? like i said, using commands I create in my game, or training them to act like a specific type of character etc.&lt;/p&gt; &lt;p&gt;Again, other than my experiments with just the chat GPT window, I am pretty new to this. If you have advice on what models to use or best practices, I'm listening.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MountainPlantation"&gt; /u/MountainPlantation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-17T11:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfzbd4</id>
    <title>Ollama not detecting intel arc graphics</title>
    <updated>2026-01-18T04:52:04+00:00</updated>
    <author>
      <name>/u/Titanlucifer18</name>
      <uri>https://old.reddit.com/user/Titanlucifer18</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Thinkpad E14 G7, with intel core ultra 5 225H processor, running Fedora 43. I tried to install Ollama, but it did not detect any GPU. I tried to search docs but couldn‚Äôt find anything, or maybe I weren‚Äôt looking at the right place.&lt;/p&gt; &lt;p&gt;If anyone can guide me would be helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Titanlucifer18"&gt; /u/Titanlucifer18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfzbd4/ollama_not_detecting_intel_arc_graphics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfzbd4/ollama_not_detecting_intel_arc_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qfzbd4/ollama_not_detecting_intel_arc_graphics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T04:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfzb7g</id>
    <title>Ollama not detecting intel arc graphics</title>
    <updated>2026-01-18T04:51:50+00:00</updated>
    <author>
      <name>/u/Titanlucifer18</name>
      <uri>https://old.reddit.com/user/Titanlucifer18</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Thinkpad E14 G7, with intel core ultra 5 225H processor, running Fedora 43. I tried to install Ollama, but it did not detect any GPU. I tried to search docs but couldn‚Äôt find anything, or maybe I weren‚Äôt looking at the right place.&lt;/p&gt; &lt;p&gt;If anyone can guide me would be helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Titanlucifer18"&gt; /u/Titanlucifer18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfzb7g/ollama_not_detecting_intel_arc_graphics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfzb7g/ollama_not_detecting_intel_arc_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qfzb7g/ollama_not_detecting_intel_arc_graphics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T04:51:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg9pek</id>
    <title>[D] Validate Production GenAI Challenges - Seeking Feedback</title>
    <updated>2026-01-18T14:17:25+00:00</updated>
    <author>
      <name>/u/No_Barracuda_415</name>
      <uri>https://old.reddit.com/user/No_Barracuda_415</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Guys,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Quick Backstory:&lt;/strong&gt; While working on LLMOps in past 2 years, I felt chaos with massive LLM workflows where costs exploded without clear attribution(which agent/prompt/retries?), silent sensitive data leakage and compliance had no replayable audit trails. Peers in other teams and externally felt the same: fragmented tools (metrics but not LLM aware), no real-time controls and growing risks with scaling. We felt the major need was &lt;strong&gt;control over costs, security and auditability without overhauling with multiple stacks/tools or adding latency&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problems we're seeing:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Unexplained LLM Spend:&lt;/strong&gt; Total bill known, but no breakdown by model/agent/workflow/team/tenant. Inefficient prompts/retries hide waste.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Silent Security Risks:&lt;/strong&gt; PII/PHI/PCI, API keys, prompt injections/jailbreaks slip through without real-time detection/enforcement.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No Audit Trail:&lt;/strong&gt; Hard to explain AI decisions (prompts, tools, responses, routing, policies) to Security/Finance/Compliance.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Does this resonate with anyone running GenAI workflows/multi-agents?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Few open questions I am having:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is this problem space worth pursuing in production GenAI?&lt;/li&gt; &lt;li&gt;Biggest challenges in cost/security observability to prioritize?&lt;/li&gt; &lt;li&gt;Are there other big pains in observability/governance I'm missing?&lt;/li&gt; &lt;li&gt;How do you currently hack around these (custom scripts, LangSmith, manual reviews)?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Barracuda_415"&gt; /u/No_Barracuda_415 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qg9pek/d_validate_production_genai_challenges_seeking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qg9pek/d_validate_production_genai_challenges_seeking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qg9pek/d_validate_production_genai_challenges_seeking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T14:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfufg3</id>
    <title>Claude Code with Anthropic API compatibility</title>
    <updated>2026-01-18T01:01:12+00:00</updated>
    <author>
      <name>/u/GhettoFob</name>
      <uri>https://old.reddit.com/user/GhettoFob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qfufg3/claude_code_with_anthropic_api_compatibility/"&gt; &lt;img alt="Claude Code with Anthropic API compatibility" src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="Claude Code with Anthropic API compatibility" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhettoFob"&gt; /u/GhettoFob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/blog/claude"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfufg3/claude_code_with_anthropic_api_compatibility/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qfufg3/claude_code_with_anthropic_api_compatibility/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T01:01:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgbl6g</id>
    <title>(linux) i'm interested in historical roleplay (1600s)/early modern period), what would be your setup ?</title>
    <updated>2026-01-18T15:33:57+00:00</updated>
    <author>
      <name>/u/Mid-Pri6170</name>
      <uri>https://old.reddit.com/user/Mid-Pri6170</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;my longer term goal is to use gemini or other ai to make a little isometric world in Godot i can explore.&lt;/p&gt; &lt;p&gt;yesterday gemini had me instal olama and lama3 on my pc. &lt;/p&gt; &lt;p&gt;i only ran it in the terminal, but i am interested in what other things to consider to make it emersive.... considering cgpt etc are nerf'd&lt;/p&gt; &lt;p&gt;Gemini suggest Dolphin, Qwen and Nemo models too. however i was wondering if these models have a lot obscure trivia, knowledge of the period, language etc in them like the big llms do, otherwise they will quickly sound stale.&lt;/p&gt; &lt;p&gt;i was thinking there might be a specially trained model on period language/literature?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mid-Pri6170"&gt; /u/Mid-Pri6170 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgbl6g/linux_im_interested_in_historical_roleplay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgbl6g/linux_im_interested_in_historical_roleplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qgbl6g/linux_im_interested_in_historical_roleplay/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T15:33:57+00:00</published>
  </entry>
</feed>
