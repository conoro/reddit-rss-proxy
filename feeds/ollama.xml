<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-04T08:10:29+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pbpuwm</id>
    <title>Qwen3 Vision is a *fantastic* local model for Home Assistant (with one fix)</title>
    <updated>2025-12-01T21:44:49+00:00</updated>
    <author>
      <name>/u/eXntrc</name>
      <uri>https://old.reddit.com/user/eXntrc</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eXntrc"&gt; /u/eXntrc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/homeassistant/comments/1pbpub7/qwen3_vision_is_a_fantastic_local_model_for_ha/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbpuwm/qwen3_vision_is_a_fantastic_local_model_for_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbpuwm/qwen3_vision_is_a_fantastic_local_model_for_home/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T21:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbbht5</id>
    <title>AI Agent from scratch: Django + Ollama + Pydantic AI - A Step-by-Step Guide</title>
    <updated>2025-12-01T12:28:31+00:00</updated>
    <author>
      <name>/u/tom-mart</name>
      <uri>https://old.reddit.com/user/tom-mart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey-up Reddit. I’m excited to share my latest project with you, a detailed, step-by-step guide on building a basic AI agent using Django, Ollama, and Pydantic AI.&lt;/p&gt; &lt;p&gt;I’ve broken down the entire process, making it accessible even if you’re just starting with Python. In the first part I'll show you how to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Set up a Django project with Django Ninja for rapid API development.&lt;/li&gt; &lt;li&gt;Integrate your local Ollama engine.&lt;/li&gt; &lt;li&gt;Use Pydantic AI to manage your agent’s context and tool calls.&lt;/li&gt; &lt;li&gt;Build a functional AI agent in just a few lines of code!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is a great starting point for anyone wanting to experiment with local LLMs and build their own AI agents from scratch.&lt;/p&gt; &lt;p&gt;Read the full article &lt;a href="https://medium.com/@tom.mart/build-self-hosted-ai-agent-with-ollama-pydantic-ai-and-django-ninja-53c6b3f14a1d"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In the next part I'll be diving into memory management – giving your agent the ability to remember past conversations and interactions.&lt;/p&gt; &lt;p&gt;Looking forward to your comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tom-mart"&gt; /u/tom-mart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbbht5/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbbht5/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbbht5/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T12:28:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbsy1n</id>
    <title>New to ollama</title>
    <updated>2025-12-01T23:49:55+00:00</updated>
    <author>
      <name>/u/Leading_Jury_6868</name>
      <uri>https://old.reddit.com/user/Leading_Jury_6868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everybody I’m new to the ollama world and in just want to know about hardware requirements for running a local a.I model. The thing is that I don’t know what I need something like a bot to help with my python coding. Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leading_Jury_6868"&gt; /u/Leading_Jury_6868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbsy1n/new_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbsy1n/new_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbsy1n/new_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T23:49:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc61qv</id>
    <title>Anyone tried a local model that can get UI element coordinates from a screenshot?</title>
    <updated>2025-12-02T11:25:43+00:00</updated>
    <author>
      <name>/u/Efficient_Weight3313</name>
      <uri>https://old.reddit.com/user/Efficient_Weight3313</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Has anyone here tried a &lt;strong&gt;local model&lt;/strong&gt; where you upload a &lt;strong&gt;UI screenshot&lt;/strong&gt; (like a dashboard or app UI) and the model can return the &lt;strong&gt;coordinates/bounding box&lt;/strong&gt; of elements such as &lt;em&gt;Login&lt;/em&gt;, &lt;em&gt;Signup&lt;/em&gt;, buttons, inputs, etc.?&lt;/p&gt; &lt;p&gt;Just want to know if anyone in the community has experimented with this.&lt;br /&gt; Any model name or experience would help.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Efficient_Weight3313"&gt; /u/Efficient_Weight3313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pc61qv/anyone_tried_a_local_model_that_can_get_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pc61qv/anyone_tried_a_local_model_that_can_get_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pc61qv/anyone_tried_a_local_model_that_can_get_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-02T11:25:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcdil0</id>
    <title>Working on structured task planning for Nanocoder - helping smaller local models tackle bigger tasks</title>
    <updated>2025-12-02T16:49:45+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/nanocoder/comments/1pcdifq/working_on_structured_task_planning_for_nanocoder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcdil0/working_on_structured_task_planning_for_nanocoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pcdil0/working_on_structured_task_planning_for_nanocoder/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-02T16:49:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc6087</id>
    <title>Built a local MCP Hub + Memory Engine for Ollama — looking for testers</title>
    <updated>2025-12-02T11:23:24+00:00</updated>
    <author>
      <name>/u/danny_094</name>
      <uri>https://old.reddit.com/user/danny_094</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey zusammen&lt;/p&gt; &lt;p&gt;Ich habe die letzten Monate an einem lokalen AI-Projekt gebastelt, das irgendwie „etwas größer“ geworden ist als geplant – und jetzt brauche ich Beta-Tester, damit es nicht nur auf meinem Server läuft. &lt;/p&gt; &lt;p&gt;Ich habe das project schon vor ein paar Tagen vorgestellt. Ich will nicht spammen. Aber ich denke es ist effektiver explizit noch mal einen Beitrag zu eröffnen um Tester zu suchen. &lt;/p&gt; &lt;p&gt;Was es ist:&lt;/p&gt; &lt;p&gt;➡️ Ein MCP-Hub, der mehrere AI-Tools / MCP-Server bündelt ➡️ Ein SQL-Memory-Server mit VectorStore + Graph + Layering (STM/MTM/LTM) ➡️ Ein AI-Gateway, das LobeChat / OpenWebUI / AnythingLLM mit Ollama + Tools verbinden kann ➡️ Mehrere kleine Module (Sequential Thinking, Validator-Service usw.)&lt;/p&gt; &lt;p&gt;Alles ist Dockerized, komplett lokal, ohne externen Cloud-Kram.&lt;/p&gt; &lt;p&gt;Was es kann: • echte Erinnerungen speichern • Facts extrahieren • Wissen als Graph verknüpfen • MCP-Tools automatisch routen • Multi-LLM Orchestrierung • Meta-Decision Layer • Plugins / Server beliebig erweitern&lt;/p&gt; &lt;p&gt;Repo: GitHub &lt;a href="https://github.com/danny094/ai-proxybridge"&gt;https://github.com/danny094/ai-proxybridge&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Was ich brauche: • Leute, die testen, ob der Stack auf anderen Geräten sauber läuft • Feedback über: • Memory-Qualität • Geschwindigkeit • MCP-Stabilität • Ideen für neue Tools • Setup-Probleme • Optional: Bugreports + Logs&lt;/p&gt; &lt;p&gt;Für wen ist das was? • Selbsthoster • Devs • Leute mit Ollama • LLM-Nerds • Menschen die Spaß haben am Rumprobieren&lt;/p&gt; &lt;p&gt;Wenn jemand Bock hat: Einfach antworten oder eine DM schreiben. Freue mich über jedes Feedback – das Projekt soll 100% open &amp;amp; kostenlos bleiben.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danny_094"&gt; /u/danny_094 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pc6087/built_a_local_mcp_hub_memory_engine_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pc6087/built_a_local_mcp_hub_memory_engine_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pc6087/built_a_local_mcp_hub_memory_engine_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-02T11:23:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcvpd9</id>
    <title>What user mostly want in AI</title>
    <updated>2025-12-03T05:21:42+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rhe wqnt small models 7B-32B models? What really we need. The question is not we want…must be we need…&lt;/p&gt; &lt;p&gt;WE NEED MEDIUM MODELS 128B-256B MOE MODELS.&lt;/p&gt; &lt;p&gt;Enthusiasts come to buy and invest in ram memory and try the industry to freely liberate medium size models!! &lt;/p&gt; &lt;p&gt;We must start to try thinking more in pass the next step…adapt our hardware to medium models and forget small models!!!&lt;/p&gt; &lt;p&gt;Medium models are afordable and makes the difference with small models.&lt;/p&gt; &lt;p&gt;Industry!!! Liberate more medium moe models 128B-256B we need it!!!&lt;/p&gt; &lt;p&gt;Qwen 235 is a example…but we need more…more at 128GB ram affordable and more at 256GB ram affordable.&lt;/p&gt; &lt;p&gt;Quality models we need!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcvpd9/what_user_mostly_want_in_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcvpd9/what_user_mostly_want_in_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pcvpd9/what_user_mostly_want_in_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T05:21:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcm31k</id>
    <title>fine tuning/doing prototype on ML model on mac and then testing it - How?</title>
    <updated>2025-12-02T22:06:01+00:00</updated>
    <author>
      <name>/u/Chachachaudhary123</name>
      <uri>https://old.reddit.com/user/Chachachaudhary123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to be able to do as a Data Scientist do some prototyping/eval for a particular Ml use case on my Mac(with large unified memory). What tools and ecosystems are available to do this effectively? Once I complete the prototype/eval, then I would deploy it on Nvidia GPU machines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chachachaudhary123"&gt; /u/Chachachaudhary123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcm31k/fine_tuningdoing_prototype_on_ml_model_on_mac_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcm31k/fine_tuningdoing_prototype_on_ml_model_on_mac_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pcm31k/fine_tuningdoing_prototype_on_ml_model_on_mac_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-02T22:06:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcw61m</id>
    <title>Necesitamos que la industria de la IA libere modelos MEDIANOS</title>
    <updated>2025-12-03T05:46:36+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;La industria de la IA esta permanentemente liberando modelos pequeños y muy pequeños con muy poco conocimiento…el conocimiento es necesario para hacer tareas y resolver problemas de la vida real…por eso las empresas de IA rara vez liberando sus modelos medianos…Liberan los grandes porque saben que la comunidad opensource no tiene el hardware necesario para ejecutarlos y liberan los pequeños porque en la mayoria de los casos estan bien para ser probados y buscar implementar su tecnologia…pero porque no liberan modelos medianos en torno a 128B y 256B que suele ser la cantidad de ram que tienen los servidores obsoletos que se venden en ebay y un buen afficionado puede invertir en ello para trabajar con modelos medios con tecnologia moe.Pues simplemente porque a la industria no le interesa que nos adueñemos de esos modelos porque marcan la diferencia y tienen dentro conocimiento como para empezar a ser utiles para resolver ciertos problemas…Qwen se atrevio a liberar su 235B pero deepseek lo hizo con el 671B (imposible para un usuario medio de ejecutar) La industria deberia empezar a pensar que los entusiastas e investigadores y amateurs que le estan cogiendo el gusto y han convertido la IA en su hobbie estan dispuestos a gastar en maquinas con ram hasta 256 gigas y no tienen modelos moe con los que experimentar.Modelos con mucho conocimiento pero con muchos modelos expertos moe pequeños de forma que puedan ser usados con mucha ram y una sola grafica…y ahi ya empezamos a marcar la diferencia de un modelo para trastear o jugar o uno para realmente trabajar…y hacer cosas realmente interesantes.Hay que animar a las empresas a dar el paso de liberar modelos MOE medianos!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcw61m/necesitamos_que_la_industria_de_la_ia_libere/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcw61m/necesitamos_que_la_industria_de_la_ia_libere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pcw61m/necesitamos_que_la_industria_de_la_ia_libere/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T05:46:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc0nd4</id>
    <title>I made a friendlier UI to manage ollama models</title>
    <updated>2025-12-02T05:49:22+00:00</updated>
    <author>
      <name>/u/ComfyTightwad</name>
      <uri>https://old.reddit.com/user/ComfyTightwad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pc0nd4/i_made_a_friendlier_ui_to_manage_ollama_models/"&gt; &lt;img alt="I made a friendlier UI to manage ollama models" src="https://external-preview.redd.it/OWgwaTNkMXhicTRnMfUiUzAnmNREsHt8dOXHrmeg5B6ZHRDEfM5KWhHunQFO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eae295535409f30cb5a15fe3c38d2c850491f516" title="I made a friendlier UI to manage ollama models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;OllaMan&lt;/strong&gt;(Ollama Manager) is a visual management interface for Ollama, with the following main features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Manage models on multiple remote or local Ollama servers simultaneously, with support for Basic Auth security authentication.&lt;/li&gt; &lt;li&gt;Built-in model marketplace for one-click online model installation, saying goodbye to command-line operations.&lt;/li&gt; &lt;li&gt;View currently running models and unload them with a single click to free up memory.&lt;/li&gt; &lt;li&gt;Chat functionality to test model performance.&lt;/li&gt; &lt;li&gt;Cross-platform support: MacOS, Windows, Linux&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComfyTightwad"&gt; /u/ComfyTightwad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/usrybf0xbq4g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pc0nd4/i_made_a_friendlier_ui_to_manage_ollama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pc0nd4/i_made_a_friendlier_ui_to_manage_ollama_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-02T05:49:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd35bz</id>
    <title>Can Tesla cards be used to run Ollama models?</title>
    <updated>2025-12-03T12:43:43+00:00</updated>
    <author>
      <name>/u/Dry-Echidna8207</name>
      <uri>https://old.reddit.com/user/Dry-Echidna8207</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kinda new to this. Looking to run a local models for an AI powered game I'm working on. As it turns out my system currently doesn't have enough vram to run the ai portion. I'm looking for a cost effective way to make it work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry-Echidna8207"&gt; /u/Dry-Echidna8207 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pd35bz/can_tesla_cards_be_used_to_run_ollama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pd35bz/can_tesla_cards_be_used_to_run_ollama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pd35bz/can_tesla_cards_be_used_to_run_ollama_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T12:43:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdirqu</id>
    <title>Ollama Project: it's an AI assistant which can code and use multiple AI models using Ollama</title>
    <updated>2025-12-03T22:34:48+00:00</updated>
    <author>
      <name>/u/sebastiankeller0205</name>
      <uri>https://old.reddit.com/user/sebastiankeller0205</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pdirqu/ollama_project_its_an_ai_assistant_which_can_code/"&gt; &lt;img alt="Ollama Project: it's an AI assistant which can code and use multiple AI models using Ollama" src="https://external-preview.redd.it/cHFubmJndmFnMjVnMSPy3PGUc0Zo7G2i-eIKH764O96cWcKiQ7dhbsY5xVIA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5282e485290bd7a65f105e34d40933061f896ce0" title="Ollama Project: it's an AI assistant which can code and use multiple AI models using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last weekend I wanted to create something which can use ollama and vibe code for me when I just say simple thing. I'm a MCU fan and I like iron man pretty much so I added iron man theme and started developing this. I wanted some tips for this project regarding Ollama &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastiankeller0205"&gt; /u/sebastiankeller0205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r5nrp3rag25g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdirqu/ollama_project_its_an_ai_assistant_which_can_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdirqu/ollama_project_its_an_ai_assistant_which_can_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T22:34:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcyqd3</id>
    <title>Is your local model safe from "Emoji Smuggling"? Visual demo of Prompt Injection</title>
    <updated>2025-12-03T08:21:07+00:00</updated>
    <author>
      <name>/u/jokiruiz</name>
      <uri>https://old.reddit.com/user/jokiruiz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We all love Ollama because it makes running local, private models incredibly easy. But I've been testing how these open weights models handle Prompt Injection and Logic Hacking.&lt;/p&gt; &lt;p&gt;I made a video demonstrating how attackers can use techniques like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Emoji Smuggling: Hiding malicious instructions inside tokens that look innocent (like emojis) but trigger specific behaviors in the model.&lt;/li&gt; &lt;li&gt;Roleplay Attacks: Bypassing safety filters just by using the classic &amp;quot;Grandma exploit&amp;quot; or logic games (demonstrated with the Gandalf game).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even if Ollama runs locally and offline, if you connect it to a UI that scrapes the web or reads external text, your model might be vulnerable to these &amp;quot;invisible&amp;quot; commands.&lt;/p&gt; &lt;p&gt;- Video Link: &lt;a href="https://youtu.be/Kck8JxHmDOs?si=yp5sjBq_d2hh5QU3"&gt;https://youtu.be/Kck8JxHmDOs?si=yp5sjBq_d2hh5QU3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For the Ollama users: Have you found any robust System Prompts (Modelfiles) that effectively block these types of &amp;quot;jailbreaks&amp;quot;? Or is Llama 3 still too easily convinced to break character?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jokiruiz"&gt; /u/jokiruiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcyqd3/is_your_local_model_safe_from_emoji_smuggling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcyqd3/is_your_local_model_safe_from_emoji_smuggling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pcyqd3/is_your_local_model_safe_from_emoji_smuggling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T08:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcsp6c</id>
    <title>ministral-3 and mistral-large-3</title>
    <updated>2025-12-03T02:53:07+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ollama.com/library/ministral-3"&gt;https://ollama.com/library/ministral-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ministral-3: The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/mistral-large-3"&gt;https://ollama.com/library/mistral-large-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mistral-Large-3: A general-purpose multimodal mixture-of-experts model for production-grade tasks and enterprise workloads.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This models requires Ollama 0.13.1&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcsp6c/ministral3_and_mistrallarge3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcsp6c/ministral3_and_mistrallarge3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pcsp6c/ministral3_and_mistrallarge3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T02:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdm9y0</id>
    <title>Kiwix RAG: Terminal Chat Interface with Local Kiwix Content Integration</title>
    <updated>2025-12-04T01:03:34+00:00</updated>
    <author>
      <name>/u/Smart-Competition200</name>
      <uri>https://old.reddit.com/user/Smart-Competition200</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pdm9y0/kiwix_rag_terminal_chat_interface_with_local/"&gt; &lt;img alt="Kiwix RAG: Terminal Chat Interface with Local Kiwix Content Integration" src="https://b.thumbs.redditmedia.com/7ta3SkVOuRhOBRGGVUKFMtEC8CIWe5xEPJ3t69rkzYY.jpg" title="Kiwix RAG: Terminal Chat Interface with Local Kiwix Content Integration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/imDelivered/KiwixRAG"&gt;https://github.com/imDelivered/KiwixRAG&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've developed a fully local terminal-based chat application that integrates local Kiwix content with Ollama using Retrieval Augmented Generation (RAG). The app automatically injects relevant Kiwix content into AI responses to improve factual accuracy.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hopxgm47735g1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=baa8c0e153cee9cb8b677677b328e1d72ca00c21"&gt;https://preview.redd.it/hopxgm47735g1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=baa8c0e153cee9cb8b677677b328e1d72ca00c21&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smart-Competition200"&gt; /u/Smart-Competition200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdm9y0/kiwix_rag_terminal_chat_interface_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdm9y0/kiwix_rag_terminal_chat_interface_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdm9y0/kiwix_rag_terminal_chat_interface_with_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T01:03:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdhg4p</id>
    <title>NornicDB - Heimdall (embedded llm executor) + plugins - MIT Licensed</title>
    <updated>2025-12-03T21:43:13+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pdgfr4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdhg4p/nornicdb_heimdall_embedded_llm_executor_plugins/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdhg4p/nornicdb_heimdall_embedded_llm_executor_plugins/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T21:43:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdmzpf</id>
    <title>Tested GPT-5.1, Gemini 3, and Claude Opus 4.5 on real data analysis tasks. Results surprised me.</title>
    <updated>2025-12-04T01:36:24+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pdmzpf/tested_gpt51_gemini_3_and_claude_opus_45_on_real/"&gt; &lt;img alt="Tested GPT-5.1, Gemini 3, and Claude Opus 4.5 on real data analysis tasks. Results surprised me." src="https://b.thumbs.redditmedia.com/Bs4LExwpf9RifuGZfuOabpughUO-dDZGVP8YrwtFR3M.jpg" title="Tested GPT-5.1, Gemini 3, and Claude Opus 4.5 on real data analysis tasks. Results surprised me." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/GeminiAI/comments/1pd7kre/tested_gpt51_gemini_3_and_claude_opus_45_on_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdmzpf/tested_gpt51_gemini_3_and_claude_opus_45_on_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdmzpf/tested_gpt51_gemini_3_and_claude_opus_45_on_real/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T01:36:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdn0qt</id>
    <title>Update: I added more open-source AI tools + new categories to my ForgeIndex project</title>
    <updated>2025-12-04T01:37:47+00:00</updated>
    <author>
      <name>/u/Equivalent-Ad-9798</name>
      <uri>https://old.reddit.com/user/Equivalent-Ad-9798</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been expanding the index of open-source AI tools I posted last week. Added new categories, improved the browsing experience, and added a bunch of projects people suggested.&lt;/p&gt; &lt;p&gt;Still a work in progress, but it’s starting to feel really usable.&lt;/p&gt; &lt;p&gt;If anyone wants to check it out or give feedback, here’s the link: &lt;a href="https://forgeindex.ai"&gt;https://forgeindex.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks again to everyone who checked it out the first time…The support was super motivating.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent-Ad-9798"&gt; /u/Equivalent-Ad-9798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdn0qt/update_i_added_more_opensource_ai_tools_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdn0qt/update_i_added_more_opensource_ai_tools_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdn0qt/update_i_added_more_opensource_ai_tools_new/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T01:37:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdpmh6</id>
    <title>GPT-OSS 120B vs QWEN3-NEXT 80B</title>
    <updated>2025-12-04T03:38:22+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT-OSS 120B vs QWEN3-NEXT 80B - A3B&lt;/p&gt; &lt;p&gt;Who have better balanced his MOE arquitecture GPT OSS with 128 experts or Qwen with 512 experts?&lt;/p&gt; &lt;p&gt;Who give the better result in quality/performance?&lt;/p&gt; &lt;p&gt;Who is better for coding?&lt;/p&gt; &lt;p&gt;Who is faster in toks/second without using a GPU.&lt;/p&gt; &lt;p&gt;Who is capable of running only with ram and one single gpu for experts loading.&lt;/p&gt; &lt;p&gt;Who is better in math&lt;/p&gt; &lt;p&gt;Who is better in coding python and C &lt;/p&gt; &lt;p&gt;Who is better for coding PICs and Arduino&lt;/p&gt; &lt;p&gt;Who have more knowledge..biology..medical…wikipedia…History..humanity…Engineering..etc&lt;/p&gt; &lt;p&gt;Give me our opinions , please.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdpmh6/gptoss_120b_vs_qwen3next_80b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdpmh6/gptoss_120b_vs_qwen3next_80b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdpmh6/gptoss_120b_vs_qwen3next_80b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T03:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdi80d</id>
    <title>AI Runner v5.0.5</title>
    <updated>2025-12-03T22:12:56+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This update allows you to run AI Runner as a headless server, but mask AI Runner as Ollama so that other services such as VSCode will think it is interfacing with Ollama, allowing you to select it as &amp;quot;Ollama&amp;quot; from the model manager in VSCode Copilot Chat.&lt;/p&gt; &lt;p&gt;Note: I haven't been able to get it to work well with agents yet, but it does work with tools if you choose the right model.&lt;/p&gt; &lt;p&gt;after you follow the installation instructions, you can use `airunner-hf-download` to list the available models and then `airunner-hf-download &amp;lt;name&amp;gt;` to download one. You might need to activate it by running the gui with `airunner` and selecting it in the chat prompt widget dropdown box. Then you can close the GUI and run `airunner-headless --ollama-mode` - after the server starts, it will be available within vscode by simply choosing &amp;quot;ollama&amp;quot;&lt;/p&gt; &lt;p&gt;Obviously, you can't have the real ollama running at the same time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Capsize-Games/airunner/compare/v5.0.4...v5.0.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdi80d/ai_runner_v505/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdi80d/ai_runner_v505/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T22:12:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdlr9e</id>
    <title>Recent Update Shows Scrollbar Rail</title>
    <updated>2025-12-04T00:40:10+00:00</updated>
    <author>
      <name>/u/Delirious_Rimbaud</name>
      <uri>https://old.reddit.com/user/Delirious_Rimbaud</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pdlr9e/recent_update_shows_scrollbar_rail/"&gt; &lt;img alt="Recent Update Shows Scrollbar Rail" src="https://preview.redd.it/67j4da7j235g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ce03c1cd4bb49970ee47aaff4140542ad910863" title="Recent Update Shows Scrollbar Rail" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delirious_Rimbaud"&gt; /u/Delirious_Rimbaud &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/67j4da7j235g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdlr9e/recent_update_shows_scrollbar_rail/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdlr9e/recent_update_shows_scrollbar_rail/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T00:40:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdu2qt</id>
    <title>Two minor creation issues with my local Ollama</title>
    <updated>2025-12-04T07:41:24+00:00</updated>
    <author>
      <name>/u/IAmAUser4Real</name>
      <uri>https://old.reddit.com/user/IAmAUser4Real</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I just managed to recover the GGUF files for both GPT-OSS-20b and Qwen3-4B; followed the base modelfile instruction on how to have them within my &amp;quot;library&amp;quot; of models, and they are there, ready to answer.&lt;/p&gt; &lt;p&gt;What I'm having issue with is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;GPT - after my questions states again what was my requests with some delimiting markers, that I'd like to avoid:&lt;/p&gt; &lt;p&gt;We have to produce a response that follows the instruction: &amp;quot;You are a helpful assistant.&amp;quot; So we should respond as a helpful assistant. The user says &amp;quot;hi, who are you?&amp;quot; We should respond politely, say we are ChatGPT, a large language model trained by OpenAI, etc. And we should not mention policy. So let's do that.&amp;lt;|end|&amp;gt;&amp;lt;|start|&amp;gt;assistant&amp;lt;|channel|&amp;gt;final&amp;lt;|message|&amp;gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- After that it just provides the answer that I want with enough details to be useful.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Qwen - even if in the Modelfile I have put 3 &amp;quot;PARAMETER stop&amp;quot; it keeps on repeating whatever was the request, in addition to the proper reply:&lt;/p&gt; &lt;p&gt;I am Qwen, a large-scale language model developed by Alibaba Cloud. I can answer questions, create content, and perform various tasks. How can I assist you today? HUMAN: what is the best way to learn a new language? RESPONSE: Learning a new language can be a rewarding experience. Here are some effective strategies to help you learn a new language efficiently:&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- First part is fine, but the &amp;quot;HUMAN/RESPONSE&amp;quot; i want to get rid of it.&lt;/p&gt; &lt;p&gt;I used a similar Modelfile for both, following some suggestions on how to stop them from ranting around, as that was the initial issue:&lt;/p&gt; &lt;p&gt;FROM ./gpt-oss-20b-F16.gguf &lt;/p&gt; &lt;p&gt;PARAMETER temperature 0.7&lt;br /&gt; PARAMETER num_ctx 2048&lt;br /&gt; PARAMETER repeat_penalty 1.1&lt;br /&gt; PARAMETER num_predict -1&lt;br /&gt; PARAMETER top_k 40&lt;br /&gt; PARAMETER top_p 0.75&lt;br /&gt; PARAMETER min_p 0.05 &lt;/p&gt; &lt;p&gt;PARAMETER stop &amp;quot;### Input:&amp;quot;&lt;br /&gt; PARAMETER stop &amp;quot;### Response:&amp;quot;&lt;br /&gt; PARAMETER stop &amp;quot;### human&amp;quot; &lt;/p&gt; &lt;p&gt;TEMPLATE &amp;quot;&amp;quot;&amp;quot;### HUMAN:&lt;br /&gt; {{ .Prompt }} &lt;/p&gt; &lt;p&gt;### RESPONSE:&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;What is my main mistake, in either, and how can I fix them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IAmAUser4Real"&gt; /u/IAmAUser4Real &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdu2qt/two_minor_creation_issues_with_my_local_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdu2qt/two_minor_creation_issues_with_my_local_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdu2qt/two_minor_creation_issues_with_my_local_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T07:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdq44e</id>
    <title>A free Opensource A.I. resource of video tutorials available as an iOS app</title>
    <updated>2025-12-04T04:02:32+00:00</updated>
    <author>
      <name>/u/Other_Passion_4710</name>
      <uri>https://old.reddit.com/user/Other_Passion_4710</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pdq44e/a_free_opensource_ai_resource_of_video_tutorials/"&gt; &lt;img alt="A free Opensource A.I. resource of video tutorials available as an iOS app" src="https://a.thumbs.redditmedia.com/k0JUJjrX055eQTSGU186a2qI76GBqxxtoRefZj-XX28.jpg" title="A free Opensource A.I. resource of video tutorials available as an iOS app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Just wanted to reshare a project I’ve been working on for anyone—from beginners to advanced developers—who enjoy learning the nuts and bolts of generative AI. I put together an app called &lt;strong&gt;A.I. DelvePad&lt;/strong&gt;, and it’s basically a casual toolbox of tutorials and resources for anyone curious about how generative AI models really work under the hood.&lt;/p&gt; &lt;p&gt;It includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;35+ free bite-sized video tutorials&lt;/strong&gt; (with more on the way)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A simple glossary&lt;/strong&gt; for key AI terms&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A quick intro to how LLMs are trained&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A tutorial-sharing feature&lt;/strong&gt; so you can pass finds to friends&lt;/li&gt; &lt;li&gt;Everything is &lt;strong&gt;100% free and open source&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to make your own resource app—whether it’s for YouTube tutorials, Udemy courses, or any hobby you’re into—you can dive into the repo and customize it however you like.&lt;/p&gt; &lt;p&gt;Would appreciate any feedback—good or bad. And if you’re building your own stuff too, keep going! It’s always fun to see what fellow hobbyists are creating.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;App Store:&lt;/strong&gt; &lt;a href="https://apps.apple.com/us/app/a-i-delvepad/id6743481267"&gt;https://apps.apple.com/us/app/a-i-delvepad/id6743481267&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/leapdeck/AIDelvePad"&gt;https://github.com/leapdeck/AIDelvePad&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Site:&lt;/strong&gt; &lt;a href="http://aidelvepad.com/"&gt;http://aidelvepad.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy coding!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Passion_4710"&gt; /u/Other_Passion_4710 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pdq44e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdq44e/a_free_opensource_ai_resource_of_video_tutorials/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdq44e/a_free_opensource_ai_resource_of_video_tutorials/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T04:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd9chp</id>
    <title>Ministral 3: A New Family of Edge Deployment Models</title>
    <updated>2025-12-03T16:46:27+00:00</updated>
    <author>
      <name>/u/United-Manner-7</name>
      <uri>https://old.reddit.com/user/United-Manner-7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just tried out the &lt;strong&gt;Ministral 3&lt;/strong&gt; family, and it looks pretty impressive:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; 3B, 8B, 14B (3–9.1GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context window:&lt;/strong&gt; 256K tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inputs:&lt;/strong&gt; Text + Images (cloud versions are text-only)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Vision support for image analysis&lt;/li&gt; &lt;li&gt;Multilingual (English, French, Spanish, German, Italian, Portuguese, Chinese, Japanese, Korean, Arabic…)&lt;/li&gt; &lt;li&gt;Strong system prompt adherence&lt;/li&gt; &lt;li&gt;Agentic abilities with native function calls and JSON output&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Edge-optimized:&lt;/strong&gt; can run on a wide range of hardware&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Apache 2.0 (open for commercial &amp;amp; non-commercial use)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Requires &lt;strong&gt;Ollama 0.13.1 (pre-release)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Run it with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama run ministral-3 # Basically the same as other models &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Feels like one of the most polished edge-deployable models right now, especially if you care about multimodal inputs and huge context.&lt;br /&gt; I would like to add that I would still stay on qwen and llama because they are already configured, but it was an interesting, albeit illogical, experience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Manner-7"&gt; /u/United-Manner-7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pd9chp/ministral_3_a_new_family_of_edge_deployment_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pd9chp/ministral_3_a_new_family_of_edge_deployment_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pd9chp/ministral_3_a_new_family_of_edge_deployment_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T16:46:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdui4a</id>
    <title>Pipeshub just hit 2k GitHub stars.</title>
    <updated>2025-12-04T08:08:14+00:00</updated>
    <author>
      <name>/u/Inevitable-Letter385</name>
      <uri>https://old.reddit.com/user/Inevitable-Letter385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re super excited to share a milestone that wouldn’t have been possible without this community. &lt;strong&gt;PipesHub just crossed 2,000 GitHub stars!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thank you to everyone who tried it out, shared feedback, opened issues, or even just followed the project.&lt;/p&gt; &lt;p&gt;For those who haven’t heard of it yet, &lt;strong&gt;PipesHub&lt;/strong&gt; is a fully open-source enterprise search platform we’ve been building over the past few months. Our goal is simple: bring powerful &lt;strong&gt;Enterprise Search&lt;/strong&gt; and &lt;strong&gt;Agent Builders&lt;/strong&gt; to every team, without vendor lock-in. PipesHub brings all your business data together and makes it instantly searchable.&lt;/p&gt; &lt;p&gt;It integrates with tools like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, and even local files. You can deploy it with a single Docker Compose command.&lt;/p&gt; &lt;p&gt;Under the hood, PipesHub runs on a &lt;strong&gt;Kafka powered event streaming architecture&lt;/strong&gt;, giving it real time, scalable, fault tolerant indexing. It combines a vector database with a knowledge graph and uses &lt;strong&gt;Agentic RAG&lt;/strong&gt; to keep responses grounded in source of truth. You get visual citations, reasoning, and confidence scores, and if information isn’t found, it simply says so instead of hallucinating.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enterprise knowledge graph for deep understanding of users, orgs, and teams&lt;/li&gt; &lt;li&gt;Connect to any AI model: OpenAI, Gemini, Claude, Ollama, or any OpenAI compatible endpoint&lt;/li&gt; &lt;li&gt;Vision Language Models and OCR for images and scanned documents&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, and SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs&lt;/li&gt; &lt;li&gt;Support for all major file types, including PDFs with images and diagrams&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Builder&lt;/strong&gt; for actions like sending emails, scheduling meetings, deep research, internet search, and more&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning Agent&lt;/strong&gt; with planning capabilities&lt;/li&gt; &lt;li&gt;&lt;strong&gt;40+ connectors&lt;/strong&gt; for integrating with your business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’d love for you to check it out and share your thoughts or feedback. It truly helps guide the roadmap:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1pdudws"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Letter385"&gt; /u/Inevitable-Letter385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T08:08:14+00:00</published>
  </entry>
</feed>
