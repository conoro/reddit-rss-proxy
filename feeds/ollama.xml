<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-01T05:06:56+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ojce3c</id>
    <title>I'm making an AI similar to a vtuber using ollama, here's what I have so far! (looking for advice on anything, really)</title>
    <updated>2025-10-29T18:25:28+00:00</updated>
    <author>
      <name>/u/imfstr</name>
      <uri>https://old.reddit.com/user/imfstr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ojce3c/im_making_an_ai_similar_to_a_vtuber_using_ollama/"&gt; &lt;img alt="I'm making an AI similar to a vtuber using ollama, here's what I have so far! (looking for advice on anything, really)" src="https://external-preview.redd.it/4DvOzz1LiPBg9908ZIl3vFi3UeP1v96-JsAN51c-ckU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed40e91e0567c5535d6953a40bf4ac44f4ac964b" title="I'm making an AI similar to a vtuber using ollama, here's what I have so far! (looking for advice on anything, really)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Hey! I just wanted to start off by apologizing if I'm breaking any rules or anything. This is my first project I've wanted to showcase to the world so bare with me here. &lt;/li&gt; &lt;li&gt;A little about myself: I'm a compsci student, planning to have a career in programming, and to test myself, I've decided to start learning python and other parts of what was needed in this project from scratch. &lt;/li&gt; &lt;li&gt;In the shown video, you'll see a clip of me making my AI vtuber's dream setup. I really like the way everything has been going with her development, and I'm posting this not only to show other people, or because I'm also looking for advice with her, any mishaps you see or bad things I'd love to know!&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/imfstr"&gt; /u/imfstr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/mDZUoMdByZU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojce3c/im_making_an_ai_similar_to_a_vtuber_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojce3c/im_making_an_ai_similar_to_a_vtuber_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T18:25:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj82qv</id>
    <title>Ollama IPEX crashing with Intel B50 Pro (Ubuntu) and Llama diverse Llama3 models</title>
    <updated>2025-10-29T15:47:16+00:00</updated>
    <author>
      <name>/u/mffjs</name>
      <uri>https://old.reddit.com/user/mffjs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I wanted to start into my own local LLM for home assistant. So I bought a new Intel ARC B50 pro. I arrived yesterday. So I spent something like 6hrs on getting it to work in my Ubuntu server VM. &lt;/p&gt; &lt;p&gt;All drivers are present and working and I can use Mistral or Gemma with Ollama. (both local bare metal install and docker). Both recognize the GPU and use it. &lt;/p&gt; &lt;p&gt;But once I try to use any Llama3 model (8b), it crashes and does not answer. &lt;/p&gt; &lt;p&gt;So now I'm a bit frustrated, I tried quite a bit (also with some help from Gemini pro). But even after building a Intel specific docker container with some script, it is not working. I used the normal Ipex-Ollama and the docker built from the script under: &lt;a href="https://github.com/eleiton/ollama-intel-arc"&gt;https://github.com/eleiton/ollama-intel-arc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone a useful idea, how I can make use of my GPU with a LLM for now and use stuff like Llama3? Any software I did not consider? Would be great to use it with Home assistant and also with something like openwebui. &lt;/p&gt; &lt;p&gt;This is the text of the issue I opened in the IPEX Github: The IPEX-LLM packaged Ollama (v2.3.0-nightly build 20250725 for Ubuntu, from ollama-ipex-llm-2.3.0b20250725-ubuntu.tgz) crashes with SIGABRT due to an assertion failure in sdp_xmx_kernel.cpp when attempting to load or run Llama 3.1 models (e.g., llama3.1:8b, llama3.1:8b-instruct-q5_K_M). This occurs on an Intel Arc B50 Pro GPU with current drivers. Other models like gemma2:9b-instruct-q5_K_M work correctly with GPU acceleration on the same setup.&lt;/p&gt; &lt;p&gt;How to reproduce&lt;/p&gt; &lt;p&gt;Assuming a working Ubuntu system with appropriate Intel GPU drivers and the extracted ollama-ipex-llm-2.3.0b20250725-ubuntu package:&lt;/p&gt; &lt;p&gt;Set the required environment variables:&lt;/p&gt; &lt;p&gt;Bash export OLLAMA_LLM_LIBRARY=$(pwd)/llm_c_intel export LD_LIBRARY_PATH=$(pwd)/llm_c_intel/lib:${LD_LIBRARY_PATH} export ZES_ENABLE_SYSMAN=1 Start the Ollama server in the background: ./ollama serve &amp;amp; Attempt to run a Llama 3.1 model: ./ollama run llama3.1:8b &amp;quot;Test&amp;quot;&lt;/p&gt; &lt;p&gt;Observe the server process crashing with the SIGABRT signal and the assertion failure mentioned above in its logs.&lt;/p&gt; &lt;p&gt;Screenshots N/A - Relevant log output below.&lt;/p&gt; &lt;p&gt;Environment information&lt;/p&gt; &lt;p&gt;GPU: Intel Arc B50 Pro OS: Ubuntu 24.04.3 LTS (Noble Numbat) Kernel: 6.14.0-33-generic #33 24.04.1-Ubuntu GPU Drivers (from ppa:kobuk-team/intel-graphics): intel-opencl-icd: 25.35.35096.9-124.04ppa3 libze-intel-gpu1: 25.35.35096.9-124.04ppa3 libze1: 1.24.1-124.04ppa1&lt;/p&gt; &lt;p&gt;IPEX-LLM Ollama Version: v2.3.0-nightly (Build 20250725 from ollama-ipex-llm-2.3.0b20250725-ubuntu.tgz)&lt;/p&gt; &lt;p&gt;Additional context The model gemma2:9b-instruct-q5_K_M works correctly.&lt;/p&gt; &lt;p&gt;Key Log Output during Crash:&lt;/p&gt; &lt;p&gt;[...] ollama-bin: /home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_xmx_kernel.cpp:439: auto ggml_sycl_op_sdp_xmx_casual(...)::(anonymous class)::operator()() const: Assertion `false' failed. SIGABRT: abort PC=0x742c8f49eb2c m=3 sigcode=18446744073709551610 signal arrived during cgo execution [...] (Goroutine stack trace follows)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mffjs"&gt; /u/mffjs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj82qv/ollama_ipex_crashing_with_intel_b50_pro_ubuntu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj82qv/ollama_ipex_crashing_with_intel_b50_pro_ubuntu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oj82qv/ollama_ipex_crashing_with_intel_b50_pro_ubuntu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T15:47:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojfivb</id>
    <title>Ollama with ROCm 7.0.2 on Linux</title>
    <updated>2025-10-29T20:23:19+00:00</updated>
    <author>
      <name>/u/sky_100_coder</name>
      <uri>https://old.reddit.com/user/sky_100_coder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ojfivb/ollama_with_rocm_702_on_linux/"&gt; &lt;img alt="Ollama with ROCm 7.0.2 on Linux" src="https://b.thumbs.redditmedia.com/nrhKGpgmomO4_3c-PSmIkoVgLBYblE2l38vMfaAIrrY.jpg" title="Ollama with ROCm 7.0.2 on Linux" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good news: I just installed ROCm 7 on Kubuntu 24.0.4 and it works without any problems :-).&lt;/p&gt; &lt;p&gt;An inference with gps-oss:120b also runs excellently on 5x 7900 XTX, see screenshot. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6dffmphx04yf1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47d6560c7f71bb9a27ff866a80bebbe78aad6354"&gt;https://preview.redd.it/6dffmphx04yf1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47d6560c7f71bb9a27ff866a80bebbe78aad6354&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sky_100_coder"&gt; /u/sky_100_coder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojfivb/ollama_with_rocm_702_on_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojfivb/ollama_with_rocm_702_on_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojfivb/ollama_with_rocm_702_on_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T20:23:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojns5i</id>
    <title>Cloud models cannot find my tools within OpenWebUI</title>
    <updated>2025-10-30T02:13:13+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok, so like the title said. The ollama cloud models are all claiming they cannot see the tools I have served in my openwebui. But every local model tells me that they can. Can someone please help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojns5i/cloud_models_cannot_find_my_tools_within_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojns5i/cloud_models_cannot_find_my_tools_within_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojns5i/cloud_models_cannot_find_my_tools_within_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T02:13:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojaxtd</id>
    <title>Small OCR/Vision models on Ollama?</title>
    <updated>2025-10-29T17:32:24+00:00</updated>
    <author>
      <name>/u/10vatharam</name>
      <uri>https://old.reddit.com/user/10vatharam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the text says, am looking for small SOTA models that are under 8GB to run on non GPU Intel laptops. Speed is not an issue as much as accuracy.&lt;/p&gt; &lt;p&gt;what do people use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/10vatharam"&gt; /u/10vatharam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojaxtd/small_ocrvision_models_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojaxtd/small_ocrvision_models_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojaxtd/small_ocrvision_models_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T17:32:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1oixco7</id>
    <title>You can now run Ollama models in Jan</title>
    <updated>2025-10-29T06:36:13+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oixco7/you_can_now_run_ollama_models_in_jan/"&gt; &lt;img alt="You can now run Ollama models in Jan" src="https://external-preview.redd.it/ejRzdG9sOTR3enhmMcCDBcXLM7FuGRx_tU7lWtYju0rdZ_z9Tigs5M1JjYPZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=286dfff3e3fd21f860e12e3ddb26742d8696c447" title="You can now run Ollama models in Jan" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;, Emre from the Jan team here.&lt;/p&gt; &lt;p&gt;One of the most requested features for Jan was being able to use Ollama models without changing model folders. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan -&amp;gt; Settings -&amp;gt; Model Providers &lt;/li&gt; &lt;li&gt;Add Ollama as a Model Provider and set the base URL to &lt;a href="http://localhost:11434/v1"&gt;http://localhost:11434/v1&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Open a new chat &amp;amp; select your Ollama model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you haven't heard of Jan before: Jan is an open-source ChatGPT replacement, running AI models locally. Simpler than LM Studio, more flexible than ChatGPT. It's completely free, and analytics are opt-out.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Website: &lt;a href="https://www.jan.ai/"&gt;https://www.jan.ai/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/janhq/jan"&gt;https://github.com/janhq/jan&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm with the Jan team, happy to answer any questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8xhfay64wzxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oixco7/you_can_now_run_ollama_models_in_jan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oixco7/you_can_now_run_ollama_models_in_jan/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T06:36:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojvwqf</id>
    <title>Using OpenWebUI without SSL for local network stuff.</title>
    <updated>2025-10-30T10:21:11+00:00</updated>
    <author>
      <name>/u/crhylove3</name>
      <uri>https://old.reddit.com/user/crhylove3</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crhylove3"&gt; /u/crhylove3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenWebUI/comments/1ojvwjb/using_openwebui_without_ssl_for_local_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojvwqf/using_openwebui_without_ssl_for_local_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojvwqf/using_openwebui_without_ssl_for_local_network/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T10:21:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj5uf1</id>
    <title>I fine-tuned Llama 3.1 to speak a rare Spanish dialect (Aragonese) using Unsloth. It's now ridiculously fast &amp; easy (Full 5-min tutorial)</title>
    <updated>2025-10-29T14:21:07+00:00</updated>
    <author>
      <name>/u/jokiruiz</name>
      <uri>https://old.reddit.com/user/jokiruiz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been blown away by how easy the fine-tuning stack has become, especially with &lt;strong&gt;Unsloth&lt;/strong&gt; (2x faster, 50% less memory) and &lt;strong&gt;Ollama&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;As a fun personal project, I decided to &amp;quot;teach&amp;quot; AI my local dialect. I created the &amp;quot;Aragonese AI&amp;quot; (&amp;quot;Maño-IA&amp;quot;), an IA fine-tuned on Llama 3.1 that speaks with the slang and personality of my region in Spain.&lt;/p&gt; &lt;p&gt;The best part? The whole process is now absurdly fast. I recorded the full, no-BS tutorial showing how to go from a base model to your own custom AI running locally with Ollama in &lt;strong&gt;just 5 minutes.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you've been waiting to try fine-tuning, now is the time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;You can watch the 5-minute tutorial here:&lt;/strong&gt; &lt;a href="https://youtu.be/Cqpcvc9P-lQ"&gt;&lt;code&gt;https://youtu.be/Cqpcvc9P-lQ&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer any questions about the process. What personality would you tune?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jokiruiz"&gt; /u/jokiruiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj5uf1/i_finetuned_llama_31_to_speak_a_rare_spanish/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj5uf1/i_finetuned_llama_31_to_speak_a_rare_spanish/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oj5uf1/i_finetuned_llama_31_to_speak_a_rare_spanish/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T14:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj6q9w</id>
    <title>Ollama models, why only cloud??</title>
    <updated>2025-10-29T14:55:42+00:00</updated>
    <author>
      <name>/u/stiflers-m0m</name>
      <uri>https://old.reddit.com/user/stiflers-m0m</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im increasingly getting frustrated and looking at alternatives to Ollama. Their cloud only releases are frustrating. Yes i can learn how to go on hugging face and figure out which gguffs are available (if there even is one for that particular model) but at that point i might as well transition off to something else.&lt;/p&gt; &lt;p&gt;If there are any ollama devs, know that you are pushing folks away. In its current state, you are lagging behind and offering cloud only models also goes against why I selected ollama to begin with. Local AI.&lt;/p&gt; &lt;p&gt;Please turn this around, if this was the direction you are going i would have never selected ollama when i first started.&lt;/p&gt; &lt;p&gt;EDIT: THere is a lot of misunderstanding on what this is about. The shift to releaseing cloud only models is what im annoyed with, where is qwen3-vl for example. I enjoyned ollama due to its ease of use, and the provided library. its less helpful if the new models are cloud only. Lots of hate if peopledont drink the ollama koolaid and have frustrations. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stiflers-m0m"&gt; /u/stiflers-m0m &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj6q9w/ollama_models_why_only_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj6q9w/ollama_models_why_only_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oj6q9w/ollama_models_why_only_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T14:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojndmq</id>
    <title>Connect your Google Drive, Gmail, and local files — while keeping everything private</title>
    <updated>2025-10-30T01:54:33+00:00</updated>
    <author>
      <name>/u/Inevitable-Letter385</name>
      <uri>https://old.reddit.com/user/Inevitable-Letter385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I’m excited to share something we’ve been building for the past few months - &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;&lt;strong&gt;PipesHub&lt;/strong&gt;&lt;/a&gt;, a &lt;strong&gt;fully open-source Enterprise Search Platform&lt;/strong&gt; designed to bring powerful Enterprise Search to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, and even local file uploads. You can deploy it and run it with just one docker compose command.&lt;/p&gt; &lt;p&gt;You can run the full platform locally. Recently, one of our users tried &lt;strong&gt;qwen3-vl:8b&lt;/strong&gt; with &lt;strong&gt;Ollama&lt;/strong&gt; and got very good results.&lt;/p&gt; &lt;p&gt;The entire system is built on a &lt;strong&gt;fully event-streaming architecture powered by Kafka&lt;/strong&gt;, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep understanding of user, organization and teams with enterprise knowledge graph&lt;/li&gt; &lt;li&gt;Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama&lt;/li&gt; &lt;li&gt;Use any provider that supports OpenAI compatible endpoints&lt;/li&gt; &lt;li&gt;Choose from 1,000+ embedding models&lt;/li&gt; &lt;li&gt;Vision-Language Models and OCR for visual or scanned docs&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, or SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs for developers&lt;/li&gt; &lt;li&gt;All major file types support including pdfs with images, diagrams and charts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Features releasing early next month&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more&lt;/li&gt; &lt;li&gt;Reasoning Agent that plans before executing tasks&lt;/li&gt; &lt;li&gt;40+ Connectors allowing you to connect to your entire business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Note: PipesHub doesn’t upload any of your data to Google Drive or Gmail. You can simply query and search within your existing data stored in Google Drive or Gmail. You can stay 100% private if you use files from your local filesystem.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Check it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Letter385"&gt; /u/Inevitable-Letter385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojndmq/connect_your_google_drive_gmail_and_local_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojndmq/connect_your_google_drive_gmail_and_local_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojndmq/connect_your_google_drive_gmail_and_local_files/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T01:54:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojn7ye</id>
    <title>qwen3-vl:32b appears not to fit into a 24 GB GPU</title>
    <updated>2025-10-30T01:47:27+00:00</updated>
    <author>
      <name>/u/florinandrei</name>
      <uri>https://old.reddit.com/user/florinandrei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All previous models from the Ollama collection that had a size below 24 GB used to fit into a 24 GB GPU like an RTX 3090. E.g. qwen3:32b has a size of 20 GB and runs entirely on the GPU. 20.5 GB of VRAM are used out of the total of 24.&lt;/p&gt; &lt;p&gt;qwen3-vl:32b surprisingly breaks the pattern. It has a size of 21 GB. But 23.55 GB of VRAM are used, it spills into system RAM, and it runs slowly, distributed between GPU and CPU.&lt;/p&gt; &lt;p&gt;I use Open WebUI with default settings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/florinandrei"&gt; /u/florinandrei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojn7ye/qwen3vl32b_appears_not_to_fit_into_a_24_gb_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojn7ye/qwen3vl32b_appears_not_to_fit_into_a_24_gb_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojn7ye/qwen3vl32b_appears_not_to_fit_into_a_24_gb_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T01:47:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok3ien</id>
    <title>npcsh--the AI command line toolkit from Indiana-based research startup NPC Worldwide--featured on star-history</title>
    <updated>2025-10-30T15:56:10+00:00</updated>
    <author>
      <name>/u/BidWestern1056</name>
      <uri>https://old.reddit.com/user/BidWestern1056</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidWestern1056"&gt; /u/BidWestern1056 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.star-history.com/blog/npcsh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ok3ien/npcshthe_ai_command_line_toolkit_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ok3ien/npcshthe_ai_command_line_toolkit_from/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T15:56:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok9n15</id>
    <title>ollama not working with my amdgpu. is there a previous version curl command i can use?</title>
    <updated>2025-10-30T19:45:08+00:00</updated>
    <author>
      <name>/u/one_moar_time</name>
      <uri>https://old.reddit.com/user/one_moar_time</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ee, maybe the issue is with devstal because tinydolphin works as if its using rocm. here is that llm's ollama log output. Ill try a different version of devstral:&lt;br /&gt; (HERE IS WHAT I TRIED)&lt;br /&gt; ❯ ollama run devstral:24b-small-2505-q4_K_M&lt;/p&gt; &lt;p&gt;pulling manifest &lt;/p&gt; &lt;p&gt;pulling b3a2c9a8fef9: 100% ▕██████████████████▏ 14 GB &lt;/p&gt; &lt;p&gt;pulling ea9ec42474e0: 100% ▕██████████████████▏ 823 B &lt;/p&gt; &lt;p&gt;pulling 43070e2d4e53: 100% ▕██████████████████▏ 11 KB &lt;/p&gt; &lt;p&gt;pulling 5725afc40acd: 100% ▕██████████████████▏ 5.7 KB &lt;/p&gt; &lt;p&gt;pulling 3dc762df9951: 100% ▕██████████████████▏ 488 B &lt;/p&gt; &lt;p&gt;verifying sha256 digest &lt;/p&gt; &lt;p&gt;writing manifest &lt;/p&gt; &lt;p&gt;success &lt;/p&gt; &lt;p&gt;Error: 500 Internal Server Error: llama runner process has terminated: error:Heuristic Fetch Failed!&lt;/p&gt; &lt;p&gt;This message will be only be displayed once, unless the ROCBLAS_VERBOSE_HIPBLASLT_ERROR environment variable is set.&lt;/p&gt; &lt;p&gt;rocBLAS warning: hipBlasLT failed, falling back to tensile. &lt;/p&gt; &lt;p&gt;This message will be only be displayed once, unless the ROCBLAS_VERBOSE_TENSILE_ERROR environment variable is set.&lt;/p&gt; &lt;p&gt;Oct 30 15:50:30 tower ollama[908]: This message will be only be displayed once, unless the ROCBLAS_VERBOSE_TENSILE_ERROR environment variable is set.&lt;/p&gt; &lt;p&gt;Oct 30 15:50:30 tower ollama[908]: llama_context: ROCm0 compute buffer size = 281.01 MiB&lt;/p&gt; &lt;p&gt;Oct 30 15:50:30 tower ollama[908]: llama_context: ROCm_Host compute buffer size = 13.01 MiB&lt;/p&gt; &lt;p&gt;Oct 30 15:50:30 tower ollama[908]: llama_context: graph nodes = 798&lt;/p&gt; &lt;p&gt;Oct 30 15:50:30 tower ollama[908]: llama_context: graph splits = 2&lt;/p&gt; &lt;p&gt;Oct 30 15:50:30 tower ollama[908]: time=2025-10-30T15:50:30.408-04:00 level=INFO source=server.go:1274 msg=&amp;quot;llama runner started in 1.06 seconds&amp;quot;&lt;/p&gt; &lt;p&gt;Oct 30 15:50:30 tower ollama[908]: time=2025-10-30T15:50:30.408-04:00 level=INFO source=sched.go:493 msg=&amp;quot;loaded runners&amp;quot; count=1&lt;/p&gt; &lt;p&gt;Oct 30 15:50:30 tower ollama[908]: time=2025-10-30T15:50:30.408-04:00 level=INFO source=server.go:1236 msg=&amp;quot;waiting for llama runner to start responding&amp;quot;&lt;/p&gt; &lt;p&gt;Oct 30 15:50:30 tower ollama[908]: time=2025-10-30T15:50:30.409-04:00 level=INFO source=server.go:1274 msg=&amp;quot;llama runner started in 1.06 seconds&amp;quot;&lt;/p&gt; &lt;p&gt;Oct 30 15:50:30 tower ollama[908]: [GIN] 2025/10/30 - 15:50:30 | 200 | 1.690967859s | &lt;a href="http://127.0.0.1"&gt;127.0.0.1&lt;/a&gt; | POST &amp;quot;/api/generate&amp;quot;&lt;/p&gt; &lt;p&gt;Oct 30 15:50:32 tower ollama[908]: [GIN] 2025/10/30 - 15:50:32 | 200 | 287.358624ms | &lt;a href="http://127.0.0.1"&gt;127.0.0.1&lt;/a&gt; | POST &amp;quot;/api/chat&amp;quot;&lt;/p&gt; &lt;p&gt;e, i got rocm and its dependencies installed. its cachyos btw. tinydolphin works.. probably because its not asking for gpu help.&lt;/p&gt; &lt;p&gt;ORIGINAL POST: If i recall correctly, the current version isnt working right or somethign with amdgpu like some quirk?? here is the error i get:&lt;/p&gt; &lt;p&gt;❯ ollama run devstral&lt;/p&gt; &lt;p&gt;Error: 500 Internal Server Error: llama runner process has terminated: error:Heuristic Fetch Failed!&lt;/p&gt; &lt;p&gt;This message will be only be displayed once, unless the ROCBLAS_VERBOSE_HIPBLASLT_ERROR environment variable is set.&lt;/p&gt; &lt;p&gt;~ 9s&lt;/p&gt; &lt;p&gt;Oct 30 15:37:46 tower ollama[908]: r14 0x0&lt;/p&gt; &lt;p&gt;Oct 30 15:37:46 tower ollama[908]: r15 0x7f5908000e50&lt;/p&gt; &lt;p&gt;Oct 30 15:37:46 tower ollama[908]: rip 0x7f58e7988f9a&lt;/p&gt; &lt;p&gt;Oct 30 15:37:46 tower ollama[908]: rflags 0x10206&lt;/p&gt; &lt;p&gt;Oct 30 15:37:46 tower ollama[908]: cs 0x33&lt;/p&gt; &lt;p&gt;Oct 30 15:37:46 tower ollama[908]: fs 0x0&lt;/p&gt; &lt;p&gt;Oct 30 15:37:46 tower ollama[908]: gs 0x0&lt;/p&gt; &lt;p&gt;Oct 30 15:37:46 tower ollama[908]: time=2025-10-30T15:37:46.106-04:00 level=ERROR source=server.go:273 msg=&amp;quot;llama runner terminated&amp;quot; error=&amp;quot;exit status 2&amp;quot;&lt;/p&gt; &lt;p&gt;Oct 30 15:37:46 tower ollama[908]: time=2025-10-30T15:37:46.298-04:00 level=INFO source=sched.go:446 msg=&amp;quot;Load failed&amp;quot; model=/var/lib/ollama/.ollama/models/blobs/sha256-b3a2c9a8fef9be8d2ef951aecca36a36b9ea0b70abe9359eab4315bf4cd9be01 error=&amp;quot;llama runner process has terminated: error:Heuristic Fetch Failed!\nThis message will be only be displayed once, unless the ROCBLAS_VERBOSE_HIPBLASLT_ERROR environment variable is set.&amp;quot;&lt;/p&gt; &lt;p&gt;Oct 30 15:37:46 tower ollama[908]: [GIN] 2025/10/30 - 15:37:46 | 500 | 9.677721961s | &lt;a href="http://127.0.0.1"&gt;127.0.0.1&lt;/a&gt; | POST &amp;quot;/api/generate&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/one_moar_time"&gt; /u/one_moar_time &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ok9n15/ollama_not_working_with_my_amdgpu_is_there_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ok9n15/ollama_not_working_with_my_amdgpu_is_there_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ok9n15/ollama_not_working_with_my_amdgpu_is_there_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T19:45:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojvbxs</id>
    <title>New Feature: Note Context. How would you use something like this?</title>
    <updated>2025-10-30T09:46:50+00:00</updated>
    <author>
      <name>/u/Financial_Click9119</name>
      <uri>https://old.reddit.com/user/Financial_Click9119</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ojvbxs/new_feature_note_context_how_would_you_use/"&gt; &lt;img alt="New Feature: Note Context. How would you use something like this?" src="https://external-preview.redd.it/MDVobWo4dnR4N3lmMXYvmWrVLIVEfLx-q3_6dsuZtEaR_DjzHzPO-YE125uf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=36dcfbad2519d1489ce118ddd24a739fa4ef0d19" title="New Feature: Note Context. How would you use something like this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR: Made it possible for Mistral to read the context of connected notes.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;For note-taking, writing and research, I use the KettleKasten method to build large networks of notes. The process involves creating atomic notes and then expanding on them to create a network.&lt;br /&gt; You can learn more about this system &lt;a href="https://zettelkasten.de/overview/"&gt;here&lt;/a&gt;, and if you prefer a &lt;a href="https://www.youtube.com/watch?v=lo4v4t06uD0"&gt;video&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;The game plan with this is to create a note-taking app that uses an LLM to build insights on my notes while all running on localhost. &lt;/p&gt; &lt;p&gt;I do have a version on the web so that I can sync notes and work on the go.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial_Click9119"&gt; /u/Financial_Click9119 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kj2dq2vtx7yf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojvbxs/new_feature_note_context_how_would_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojvbxs/new_feature_note_context_how_would_you_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T09:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1okdsva</id>
    <title>Help with my chatbot</title>
    <updated>2025-10-30T22:32:49+00:00</updated>
    <author>
      <name>/u/federicookie</name>
      <uri>https://old.reddit.com/user/federicookie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm creating a chatbot as a small project for a Python workshop I'm taking. My idea is to make a chatbot based on a character I drew, and have it display different expressions depending on its emotions. I wanted to create a variable called &amp;quot;emotions&amp;quot; with different states so I could then assign animations to them. What I don't know is if there's an additional API that can help with this, recognizing emotions in sentences. If anyone has a recommendation or an idea of how I can do this, I would greatly appreciate it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/federicookie"&gt; /u/federicookie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okdsva/help_with_my_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okdsva/help_with_my_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okdsva/help_with_my_chatbot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T22:32:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1okdsx3</id>
    <title>Help with my chatbot</title>
    <updated>2025-10-30T22:32:52+00:00</updated>
    <author>
      <name>/u/federicookie</name>
      <uri>https://old.reddit.com/user/federicookie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm creating a chatbot as a small project for a Python workshop I'm taking. My idea is to make a chatbot based on a character I drew, and have it display different expressions depending on its emotions. I wanted to create a variable called &amp;quot;emotions&amp;quot; with different states so I could then assign animations to them. What I don't know is if there's an additional API that can help with this, recognizing emotions in sentences. If anyone has a recommendation or an idea of how I can do this, I would greatly appreciate it! &lt;/p&gt; &lt;p&gt;Oh, and I am using Pygame with PyCharm &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/federicookie"&gt; /u/federicookie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okdsx3/help_with_my_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okdsx3/help_with_my_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okdsx3/help_with_my_chatbot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T22:32:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1okc34e</id>
    <title>Fun Little Choose Your Own Adventure App</title>
    <updated>2025-10-30T21:21:09+00:00</updated>
    <author>
      <name>/u/thedelusionist</name>
      <uri>https://old.reddit.com/user/thedelusionist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1okc34e/fun_little_choose_your_own_adventure_app/"&gt; &lt;img alt="Fun Little Choose Your Own Adventure App" src="https://preview.redd.it/4c60r11zfbyf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=c39fa6e48131f9a305082d2d7483ff817373ef5f" title="Fun Little Choose Your Own Adventure App" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used to play DnD and love the choose you own adventure genre, so I made a mac app that lets you do it with custom local models through Ollama and if you don't have the compute, you can use a Groq API key.&lt;/p&gt; &lt;p&gt;Everything is local (except for Groq API calls), and free. Just fun little app I made for myself that I figured I would share. Enjoy!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/gmfennema/CYOLLMA"&gt;Github Repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thedelusionist"&gt; /u/thedelusionist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4c60r11zfbyf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okc34e/fun_little_choose_your_own_adventure_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okc34e/fun_little_choose_your_own_adventure_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T21:21:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1okhoit</id>
    <title>Since 12.3 Ollama doesn’t work on CPU only, how has this not been fixed yet?</title>
    <updated>2025-10-31T01:24:56+00:00</updated>
    <author>
      <name>/u/RegularPerson2020</name>
      <uri>https://old.reddit.com/user/RegularPerson2020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s amazing to me that forever we have been able to run smollm2, granite3.1-3b, the micro qwens on modest cpu mini pcs, then bam they won’t load. It’s been on Reddit, discord, yet nothing. I’m not asking for a fix just put out an old version of Ollama and call it “cpu shmucks” or something. No one with a 5090 is running Smollm2.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RegularPerson2020"&gt; /u/RegularPerson2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okhoit/since_123_ollama_doesnt_work_on_cpu_only_how_has/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okhoit/since_123_ollama_doesnt_work_on_cpu_only_how_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okhoit/since_123_ollama_doesnt_work_on_cpu_only_how_has/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-31T01:24:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1okw7yq</id>
    <title>Which open-source LLMs support schema?</title>
    <updated>2025-10-31T14:33:20+00:00</updated>
    <author>
      <name>/u/ThingRexCom</name>
      <uri>https://old.reddit.com/user/ThingRexCom</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThingRexCom"&gt; /u/ThingRexCom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1okw7jh/which_opensource_llms_support_schema/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okw7yq/which_opensource_llms_support_schema/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okw7yq/which_opensource_llms_support_schema/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-31T14:33:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol1u0r</id>
    <title>I'm currently solving a problem I have with ollama and lmstudio.</title>
    <updated>2025-10-31T18:07:33+00:00</updated>
    <author>
      <name>/u/Sileniced</name>
      <uri>https://old.reddit.com/user/Sileniced</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sileniced"&gt; /u/Sileniced &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ol1px9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ol1u0r/im_currently_solving_a_problem_i_have_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ol1u0r/im_currently_solving_a_problem_i_have_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-31T18:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1okw3i5</id>
    <title>How to Create a Personalized AI (Free &amp; Easy Guide). I made this English blog post after you told me my Spanish video wasn't accessible. Hope this helps!</title>
    <updated>2025-10-31T14:28:27+00:00</updated>
    <author>
      <name>/u/jokiruiz</name>
      <uri>https://old.reddit.com/user/jokiruiz</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jokiruiz"&gt; /u/jokiruiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1okw32m/how_to_create_a_personalized_ai_free_easy_guide_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okw3i5/how_to_create_a_personalized_ai_free_easy_guide_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okw3i5/how_to_create_a_personalized_ai_free_easy_guide_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-31T14:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1okw9ts</id>
    <title>TreeThinkerAgent, an open-source reasoning agent using LLMs + tools</title>
    <updated>2025-10-31T14:35:26+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1okw9ts/treethinkeragent_an_opensource_reasoning_agent/"&gt; &lt;img alt="TreeThinkerAgent, an open-source reasoning agent using LLMs + tools" src="https://preview.redd.it/fa8u7btlkgyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3283c979b595bb7dbdc1c65c867dd3721993b884" title="TreeThinkerAgent, an open-source reasoning agent using LLMs + tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone 👋&lt;/p&gt; &lt;p&gt;I’ve just released &lt;a href="https://github.com/Bessouat40/TreeThinkerAgent"&gt;&lt;strong&gt;TreeThinkerAgent&lt;/strong&gt;&lt;/a&gt;, a minimalist app built from scratch without any framework to explore &lt;strong&gt;multi-step reasoning&lt;/strong&gt; with LLMs using different providers including Ollama.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does it do?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This LLM application :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Plans a list of reasoning&lt;/li&gt; &lt;li&gt;Executes any needed tools per step&lt;/li&gt; &lt;li&gt;Builds a full &lt;strong&gt;reasoning tree&lt;/strong&gt; to make each decision traceable&lt;/li&gt; &lt;li&gt;Produces a final, professional summary as output&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I wanted something clean and understandable to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Play with &lt;strong&gt;autonomous agent planning&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Prototype research assistants that don’t rely on heavy infra&lt;/li&gt; &lt;li&gt;Focus on &lt;strong&gt;agentic logic&lt;/strong&gt;, not on tool integration complexity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;→ &lt;a href="https://github.com/Bessouat40/TreeThinkerAgent"&gt;https://github.com/Bessouat40/TreeThinkerAgent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think : feedback, ideas, improvements all welcome!TreeThinkerAgent, an open-source reasoning agent using LLMs + tools&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fa8u7btlkgyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okw9ts/treethinkeragent_an_opensource_reasoning_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okw9ts/treethinkeragent_an_opensource_reasoning_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-31T14:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1okbuub</id>
    <title>Your Ollama models just got a data analysis superpower - query 10GB files locally with your models</title>
    <updated>2025-10-30T21:11:47+00:00</updated>
    <author>
      <name>/u/Sea-Assignment6371</name>
      <uri>https://old.reddit.com/user/Sea-Assignment6371</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1okbuub/your_ollama_models_just_got_a_data_analysis/"&gt; &lt;img alt="Your Ollama models just got a data analysis superpower - query 10GB files locally with your models" src="https://external-preview.redd.it/dTdtMzhlYzZlYnlmMWlS5XgjX4Ps-WSRdX4z1sYfrnkFwn3pUrSEA6TnxsyB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66f98e16de7e1767f766dd2bf427ddbf17c3e3eb" title="Your Ollama models just got a data analysis superpower - query 10GB files locally with your models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;Built something for the local AI community - DataKit Assistant with native Ollama integration.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The combo:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Your local Ollama models + massive dataset analysis&lt;/p&gt; &lt;p&gt;- Query 10GB+ CSV/Parquet files entirely offline&lt;/p&gt; &lt;p&gt;- SQL + Python notebooks + AI assistance&lt;/p&gt; &lt;p&gt;- Zero cloud dependencies, zero uploads&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Perfect for:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Analyzing sensitive data with your own models&lt;/p&gt; &lt;p&gt;- Learning data analysis with AI guidance (completely private)&lt;/p&gt; &lt;p&gt;- Prototyping without API costs&lt;/p&gt; &lt;p&gt;Works with any Ollama model that handles structured data well.&lt;/p&gt; &lt;p&gt;Try it: &lt;a href="https://datakit.page"&gt;https://datakit.page&lt;/a&gt; and let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Assignment6371"&gt; /u/Sea-Assignment6371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6va4lec6ebyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okbuub/your_ollama_models_just_got_a_data_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okbuub/your_ollama_models_just_got_a_data_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T21:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1okxpa3</id>
    <title>10x slower Qwen3 and 2.5 VL</title>
    <updated>2025-10-31T15:30:39+00:00</updated>
    <author>
      <name>/u/RIP26770</name>
      <uri>https://old.reddit.com/user/RIP26770</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen 2.5 VL and Qwen 3 VL have become so slow since the last update that they are barely usable!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIP26770"&gt; /u/RIP26770 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okxpa3/10x_slower_qwen3_and_25_vl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okxpa3/10x_slower_qwen3_and_25_vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okxpa3/10x_slower_qwen3_and_25_vl/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-31T15:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1okw14t</id>
    <title>A quick update on Nanocoder and the Nano Collective 😄</title>
    <updated>2025-10-31T14:25:46+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1okw14t/a_quick_update_on_nanocoder_and_the_nano/"&gt; &lt;img alt="A quick update on Nanocoder and the Nano Collective 😄" src="https://preview.redd.it/mxzpa3kk0gyf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=d91fd17cb5daeee2776b5ac5f3057b653d8a07ba" title="A quick update on Nanocoder and the Nano Collective 😄" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;As is becoming a thing, I just wanted to share an update post on Nanocoder, the open-source, open-community coding CLI as well as the Nano Collective, the community behind building it!&lt;/p&gt; &lt;p&gt;Over the last few weeks we've been steadily growing, continuing to build out our vision for community-led, privacy-first and open source AI. &lt;/p&gt; &lt;p&gt;&lt;em&gt;Here are a couple of highlights:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Nanocoder&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We've just surpassed 750 stars on the GitHub repo with the number growing every day.&lt;/li&gt; &lt;li&gt;We're continuing to refine the software and make it better with several big updates to configuration. One of the common complaints was that configuring Nanocoder was pretty hard so now there's a configuration wizard built right into the CLI to help you set them up easily!&lt;/li&gt; &lt;li&gt;We released a new package called &lt;a href="https://github.com/Nano-Collective/get-md"&gt;get-md&lt;/a&gt; - this takes any website URL or HTML content and processes it into LLM optimized markdown. This is a great package which we'll continue to expand as another step towards privacy-focused AI.&lt;/li&gt; &lt;li&gt;We're about to begin training our own tiny models to offset some of the work within Nanocoder. For example, we're experimenting with a tiny language model that converts questions to bash commands. Hopefully an update soon on this and we'll fully open source it as well. The aim here to keep as much processing on device without having to rely on large models in the cloud.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Nano Collective&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This is all setup now and we have a basic website here: &lt;a href="https://nanocollective.org"&gt;https://nanocollective.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;We want to welcome everyone here to drive discussions and ideas.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you to everyone that is getting involved and supporting the project. As I've said previously, it's early days but direction, improvements and growth is happening every day. The vision has always been to build private, local-first AI for the community and it's amazing to be building one where so many people are getting involved 😊&lt;/p&gt; &lt;p&gt;That being said, any help within any domain is appreciated and welcomed.&lt;/p&gt; &lt;p&gt;If you want to get involved the links are below.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Link&lt;/strong&gt;: &lt;a href="https://github.com/Nano-Collective/nanocoder"&gt;https://github.com/Nano-Collective/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord Link&lt;/strong&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mxzpa3kk0gyf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okw14t/a_quick_update_on_nanocoder_and_the_nano/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okw14t/a_quick_update_on_nanocoder_and_the_nano/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-31T14:25:46+00:00</published>
  </entry>
</feed>
