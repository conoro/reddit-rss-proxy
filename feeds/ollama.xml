<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-27T21:06:28+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qmmp22</id>
    <title>Claude Code stuck on &lt;function=TaskList&gt; when using Ollama + Qwen3-Coder</title>
    <updated>2026-01-25T15:52:04+00:00</updated>
    <author>
      <name>/u/Healthy-Laugh-6745</name>
      <uri>https://old.reddit.com/user/Healthy-Laugh-6745</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm struggling to get Claude Code working with Ollama on my Mac M4 Max (48GB RAM). I strictly followed the official Ollama integration guide (&lt;a href="https://docs.ollama.com/integrations/claude-code"&gt;https://docs.ollama.com/integrations/claude-code&lt;/a&gt;), but I'm stuck in a loop.&lt;/p&gt; &lt;p&gt;Every time I ask the model to perform a file-based task (e.g., &amp;quot;create a txt file&amp;quot;), the process hangs indefinitely.&lt;/p&gt; &lt;p&gt;The model acknowledges the request.&lt;/p&gt; &lt;p&gt;It outputs: ‚ùØ &amp;lt;function=TaskList&amp;gt; ‚è∫&lt;/p&gt; &lt;p&gt;Nothing happens after that. No file is created, and the terminal just sits there with the &amp;quot;active&amp;quot; dot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Laugh-6745"&gt; /u/Healthy-Laugh-6745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T15:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qna4xh</id>
    <title>connecting dokuwiki zu ollama?</title>
    <updated>2026-01-26T08:06:47+00:00</updated>
    <author>
      <name>/u/Tennis0711</name>
      <uri>https://old.reddit.com/user/Tennis0711</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;br /&gt; at the moment wie download the txt files from dokuwiki and upload it to ollama. Can i connect dokuwiki direktly as knowledgebase to the ollama model?&lt;/p&gt; &lt;p&gt;greetings, lars&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tennis0711"&gt; /u/Tennis0711 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qna4xh/connecting_dokuwiki_zu_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qna4xh/connecting_dokuwiki_zu_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qna4xh/connecting_dokuwiki_zu_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T08:06:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmteov</id>
    <title>Claraverse is not dead, now you can use AI with more fun, more productivity, and more PRIVACY.</title>
    <updated>2026-01-25T19:52:48+00:00</updated>
    <author>
      <name>/u/aruntemme</name>
      <uri>https://old.reddit.com/user/aruntemme</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qmteov/claraverse_is_not_dead_now_you_can_use_ai_with/"&gt; &lt;img alt="Claraverse is not dead, now you can use AI with more fun, more productivity, and more PRIVACY." src="https://b.thumbs.redditmedia.com/3hQSxjjHHYxVm66vsPAtk8Nw52svybso3qjhCjI2TpM.jpg" title="Claraverse is not dead, now you can use AI with more fun, more productivity, and more PRIVACY." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;#x200b;&lt;/p&gt; &lt;p&gt;We updated our AI workspace to actually gets things done locally (not just another chatbot or AI slope)&lt;/p&gt; &lt;p&gt;we've been grinding on ClaraVerse for the past few months, and we just dropped a major update. If you're tired of AI tools that just... talk at you, just check this out...&lt;/p&gt; &lt;p&gt;best part is you can even connect ollama ü¶ô as a provider &lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Run it anywhere:&lt;/strong&gt; CLI tool that works on your laptop, VPS, cloud, whatever. No platform lock-in BS.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;50+ integrations:&lt;/strong&gt; Gmail, Sheets, Discord, Slack, you name it. we'll be adding more soon&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Actual automation:&lt;/strong&gt; Build agents that DO things, not just answer questions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chat-first workflow builder:&lt;/strong&gt; Like n8n/Zapier but for AI. Chat your way through creating workflows ask, create, iterate.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Everything becomes an API:&lt;/strong&gt; Seriously, every workflow you build = instant API endpoint or schedule it daily, hourly your choice.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One-liner&lt;/strong&gt;: It's an all-in-one platform (chat, image gen, agents, docs, search). Every tool is part of the package.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's actually new (beyond UI polish)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Built-in tools that agents and chats need:&lt;/p&gt; &lt;p&gt;- PPT, PDF, XLSX readers and creators&lt;/p&gt; &lt;p&gt;- Isolated code execution with dependency management&lt;/p&gt; &lt;p&gt;- Interactive chat so local LLMs can ask clarifying questions mid-prompt&lt;/p&gt; &lt;p&gt;- Search, scrape, image search, API tools, and memory all default&lt;/p&gt; &lt;p&gt;- Tool router if you have too many tools&lt;/p&gt; &lt;p&gt;- Memories that can remember and forget based on your usage&lt;/p&gt; &lt;p&gt;- Connect your Gmail, Sheets, Discord, Slack, and more&lt;/p&gt; &lt;p&gt;Simple UI works on phone responsive as hell&lt;/p&gt; &lt;p&gt;Try it and let us know&lt;/p&gt; &lt;p&gt;GitHub: github.com/claraverse-space/ClaraVerse&lt;/p&gt; &lt;p&gt;We're open source and privacy-first (chat and data stored in browser or DB, even when self-hosted - user's choice).&lt;/p&gt; &lt;p&gt;I use this myself every day. Honestly, I've seen worse tools raise fund and then lock everything behind subscriptions. This community helped build this with feedback, so it's staying free and open-source.&lt;/p&gt; &lt;p&gt;Happy to answer questions, take feature requests, or hear about how it crashes on your machine so we can fix and improve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aruntemme"&gt; /u/aruntemme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qmteov"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmteov/claraverse_is_not_dead_now_you_can_use_ai_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qmteov/claraverse_is_not_dead_now_you_can_use_ai_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T19:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm9cgp</id>
    <title>Ollama Models Ranked by VRAM Requirements</title>
    <updated>2026-01-25T04:36:31+00:00</updated>
    <author>
      <name>/u/AdventurousLion9548</name>
      <uri>https://old.reddit.com/user/AdventurousLion9548</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1250.08 GB | cogito-2.1:latest&lt;/p&gt; &lt;p&gt;1250.08 GB | cogito-2.1:671b&lt;/p&gt; &lt;p&gt;376.71 GB | deepseek-v3.1:latest&lt;/p&gt; &lt;p&gt;376.71 GB | deepseek-v3.1:671b&lt;/p&gt; &lt;p&gt;376.65 GB | deepseek-r1:671b&lt;/p&gt; &lt;p&gt;376.65 GB | deepseek-v3:latest&lt;/p&gt; &lt;p&gt;376.65 GB | deepseek-v3:671b&lt;/p&gt; &lt;p&gt;376.65 GB | r1-1776:671b&lt;/p&gt; &lt;p&gt;270.14 GB | qwen3-coder:480b&lt;/p&gt; &lt;p&gt;226.38 GB | llama3.1:405b&lt;/p&gt; &lt;p&gt;213.14 GB | hermes3:405b&lt;/p&gt; &lt;p&gt;133.43 GB | qwen3-vl:235b&lt;/p&gt; &lt;p&gt;132.39 GB | qwen3:235b&lt;/p&gt; &lt;p&gt;123.78 GB | deepseek-coder-v2:236b&lt;/p&gt; &lt;p&gt;123.78 GB | deepseek-v2:236b&lt;/p&gt; &lt;p&gt;123.78 GB | deepseek-v2.5:latest&lt;/p&gt; &lt;p&gt;123.78 GB | deepseek-v2.5:236b&lt;/p&gt; &lt;p&gt;94.51 GB | falcon:180b&lt;/p&gt; &lt;p&gt;74.05 GB | zephyr:141b&lt;/p&gt; &lt;p&gt;69.75 GB | devstral-2:latest&lt;/p&gt; &lt;p&gt;69.75 GB | devstral-2:123b&lt;/p&gt; &lt;p&gt;69.1 GB | dbrx:latest&lt;/p&gt; &lt;p&gt;69.1 GB | dbrx:132b&lt;/p&gt; &lt;p&gt;68.19 GB | mistral-large:latest&lt;/p&gt; &lt;p&gt;68.19 GB | mistral-large:123b&lt;/p&gt; &lt;p&gt;63.1 GB | megadolphin:latest&lt;/p&gt; &lt;p&gt;63.1 GB | megadolphin:120b&lt;/p&gt; &lt;p&gt;62.81 GB | llama4:latest&lt;/p&gt; &lt;p&gt;62.52 GB | command-a:latest&lt;/p&gt; &lt;p&gt;62.52 GB | command-a:111b&lt;/p&gt; &lt;p&gt;60.88 GB | gpt-oss:120b&lt;/p&gt; &lt;p&gt;60.88 GB | gpt-oss-safeguard:120b&lt;/p&gt; &lt;p&gt;58.57 GB | qwen:110b&lt;/p&gt; &lt;p&gt;55.15 GB | command-r-plus:latest&lt;/p&gt; &lt;p&gt;55.15 GB | command-r-plus:104b&lt;/p&gt; &lt;p&gt;50.87 GB | llama3.2-vision:90b&lt;/p&gt; &lt;p&gt;46.89 GB | qwen3-next:latest&lt;/p&gt; &lt;p&gt;46.89 GB | qwen3-next:80b&lt;/p&gt; &lt;p&gt;45.36 GB | qwen2.5vl:72b&lt;/p&gt; &lt;p&gt;44.16 GB | athene-v2:latest&lt;/p&gt; &lt;p&gt;44.16 GB | athene-v2:72b&lt;/p&gt; &lt;p&gt;44.16 GB | qwen2.5:72b&lt;/p&gt; &lt;p&gt;39.6 GB | cogito:70b&lt;/p&gt; &lt;p&gt;39.6 GB | deepseek-r1:70b&lt;/p&gt; &lt;p&gt;39.6 GB | llama3.1:70b&lt;/p&gt; &lt;p&gt;39.6 GB | llama3.3:latest&lt;/p&gt; &lt;p&gt;39.6 GB | llama3.3:70b&lt;/p&gt; &lt;p&gt;39.6 GB | nemotron:latest&lt;/p&gt; &lt;p&gt;39.6 GB | nemotron:70b&lt;/p&gt; &lt;p&gt;39.6 GB | r1-1776:latest&lt;/p&gt; &lt;p&gt;39.6 GB | r1-1776:70b&lt;/p&gt; &lt;p&gt;39.6 GB | tulu3:70b&lt;/p&gt; &lt;p&gt;38.4 GB | qwen2:72b&lt;/p&gt; &lt;p&gt;38.4 GB | qwen2-math:72b&lt;/p&gt; &lt;p&gt;38.18 GB | qwen:72b&lt;/p&gt; &lt;p&gt;37.22 GB | dolphin-llama3:70b&lt;/p&gt; &lt;p&gt;37.22 GB | firefunction-v2:latest&lt;/p&gt; &lt;p&gt;37.22 GB | firefunction-v2:70b&lt;/p&gt; &lt;p&gt;37.22 GB | hermes3:70b&lt;/p&gt; &lt;p&gt;37.22 GB | llama3:70b&lt;/p&gt; &lt;p&gt;37.22 GB | llama3-chatqa:70b&lt;/p&gt; &lt;p&gt;37.22 GB | llama3-gradient:70b&lt;/p&gt; &lt;p&gt;37.22 GB | llama3-groq-tool-use:70b&lt;/p&gt; &lt;p&gt;37.22 GB | reflection:latest&lt;/p&gt; &lt;p&gt;37.22 GB | reflection:70b&lt;/p&gt; &lt;p&gt;36.2 GB | codellama:70b&lt;/p&gt; &lt;p&gt;36.2 GB | llama2:70b&lt;/p&gt; &lt;p&gt;36.2 GB | llama2-uncensored:70b&lt;/p&gt; &lt;p&gt;36.2 GB | meditron:70b&lt;/p&gt; &lt;p&gt;36.2 GB | orca-mini:70b&lt;/p&gt; &lt;p&gt;36.2 GB | stable-beluga:70b&lt;/p&gt; &lt;p&gt;36.2 GB | wizard-math:70b&lt;/p&gt; &lt;p&gt;35.53 GB | deepseek-llm:67b&lt;/p&gt; &lt;p&gt;24.63 GB | dolphin-mixtral:latest&lt;/p&gt; &lt;p&gt;24.63 GB | mixtral:latest&lt;/p&gt; &lt;p&gt;24.63 GB | notux:latest&lt;/p&gt; &lt;p&gt;24.63 GB | nous-hermes2-mixtral:latest&lt;/p&gt; &lt;p&gt;22.6 GB | nemotron-3-nano:latest&lt;/p&gt; &lt;p&gt;22.6 GB | nemotron-3-nano:30b&lt;/p&gt; &lt;p&gt;22.17 GB | alfred:latest&lt;/p&gt; &lt;p&gt;22.17 GB | alfred:40b&lt;/p&gt; &lt;p&gt;22.17 GB | falcon:40b&lt;/p&gt; &lt;p&gt;19.71 GB | qwen2.5vl:32b&lt;/p&gt; &lt;p&gt;19.47 GB | qwen3-vl:32b&lt;/p&gt; &lt;p&gt;18.84 GB | aya:35b&lt;/p&gt; &lt;p&gt;18.81 GB | qwen3:32b&lt;/p&gt; &lt;p&gt;18.78 GB | llava:34b&lt;/p&gt; &lt;p&gt;18.49 GB | cogito:32b&lt;/p&gt; &lt;p&gt;18.49 GB | deepseek-r1:32b&lt;/p&gt; &lt;p&gt;18.49 GB | openthinker:32b&lt;/p&gt; &lt;p&gt;18.49 GB | qwen2.5:32b&lt;/p&gt; &lt;p&gt;18.49 GB | qwen2.5-coder:32b&lt;/p&gt; &lt;p&gt;18.49 GB | qwq:latest&lt;/p&gt; &lt;p&gt;18.49 GB | qwq:32b&lt;/p&gt; &lt;p&gt;18.44 GB | aya-expanse:32b&lt;/p&gt; &lt;p&gt;18.25 GB | qwen3-vl:30b&lt;/p&gt; &lt;p&gt;18.14 GB | olmo-3:32b&lt;/p&gt; &lt;p&gt;18.14 GB | olmo-3.1:latest&lt;/p&gt; &lt;p&gt;18.14 GB | olmo-3.1:32b&lt;/p&gt; &lt;p&gt;18.13 GB | nous-hermes2:34b&lt;/p&gt; &lt;p&gt;18.13 GB | yi:34b&lt;/p&gt; &lt;p&gt;18.02 GB | exaone-deep:32b&lt;/p&gt; &lt;p&gt;18.02 GB | exaone3.5:32b&lt;/p&gt; &lt;p&gt;17.92 GB | granite-code:34b&lt;/p&gt; &lt;p&gt;17.74 GB | codebooga:latest&lt;/p&gt; &lt;p&gt;17.74 GB | codebooga:34b&lt;/p&gt; &lt;p&gt;17.74 GB | codellama:34b&lt;/p&gt; &lt;p&gt;17.74 GB | phind-codellama:latest&lt;/p&gt; &lt;p&gt;17.74 GB | phind-codellama:34b&lt;/p&gt; &lt;p&gt;17.53 GB | deepseek-coder:33b&lt;/p&gt; &lt;p&gt;17.53 GB | wizardcoder:33b&lt;/p&gt; &lt;p&gt;17.43 GB | command-r:latest&lt;/p&gt; &lt;p&gt;17.43 GB | command-r:35b&lt;/p&gt; &lt;p&gt;17.28 GB | qwen3:30b&lt;/p&gt; &lt;p&gt;17.28 GB | qwen3-coder:latest&lt;/p&gt; &lt;p&gt;17.28 GB | qwen3-coder:30b&lt;/p&gt; &lt;p&gt;17.23 GB | qwen:32b&lt;/p&gt; &lt;p&gt;17.1 GB | vicuna:33b&lt;/p&gt; &lt;p&gt;17.1 GB | wizard-vicuna-uncensored:30b&lt;/p&gt; &lt;p&gt;16.2 GB | gemma3:27b&lt;/p&gt; &lt;p&gt;16.17 GB | translategemma:27b&lt;/p&gt; &lt;p&gt;15.5 GB | shieldgemma:27b&lt;/p&gt; &lt;p&gt;14.56 GB | gemma2:27b&lt;/p&gt; &lt;p&gt;14.42 GB | mistral-small3.1:latest&lt;/p&gt; &lt;p&gt;14.42 GB | mistral-small3.1:24b&lt;/p&gt; &lt;p&gt;14.14 GB | devstral-small-2:latest&lt;/p&gt; &lt;p&gt;14.14 GB | devstral-small-2:24b&lt;/p&gt; &lt;p&gt;14.14 GB | mistral-small3.2:latest&lt;/p&gt; &lt;p&gt;14.14 GB | mistral-small3.2:24b&lt;/p&gt; &lt;p&gt;13.35 GB | devstral:latest&lt;/p&gt; &lt;p&gt;13.35 GB | devstral:24b&lt;/p&gt; &lt;p&gt;13.35 GB | magistral:latest&lt;/p&gt; &lt;p&gt;13.35 GB | magistral:24b&lt;/p&gt; &lt;p&gt;13.35 GB | mistral-small:latest&lt;/p&gt; &lt;p&gt;13.35 GB | mistral-small:24b&lt;/p&gt; &lt;p&gt;12.85 GB | gpt-oss:latest&lt;/p&gt; &lt;p&gt;12.85 GB | gpt-oss:20b&lt;/p&gt; &lt;p&gt;12.85 GB | gpt-oss-safeguard:latest&lt;/p&gt; &lt;p&gt;12.85 GB | gpt-oss-safeguard:20b&lt;/p&gt; &lt;p&gt;12.4 GB | solar-pro:latest&lt;/p&gt; &lt;p&gt;12.4 GB | solar-pro:22b&lt;/p&gt; &lt;p&gt;11.71 GB | codestral:latest&lt;/p&gt; &lt;p&gt;11.71 GB | codestral:22b&lt;/p&gt; &lt;p&gt;11.71 GB | mistral-small:22b&lt;/p&gt; &lt;p&gt;10.82 GB | sailor2:20b&lt;/p&gt; &lt;p&gt;10.76 GB | granite-code:20b&lt;/p&gt; &lt;p&gt;10.55 GB | internlm2:20b&lt;/p&gt; &lt;p&gt;10.35 GB | phi4-reasoning:latest&lt;/p&gt; &lt;p&gt;10.35 GB | phi4-reasoning:14b&lt;/p&gt; &lt;p&gt;8.64 GB | qwen3:14b&lt;/p&gt; &lt;p&gt;8.46 GB | ministral-3:14b&lt;/p&gt; &lt;p&gt;8.44 GB | dolphincoder:15b&lt;/p&gt; &lt;p&gt;8.44 GB | starcoder2:15b&lt;/p&gt; &lt;p&gt;8.43 GB | phi4:latest&lt;/p&gt; &lt;p&gt;8.43 GB | phi4:14b&lt;/p&gt; &lt;p&gt;8.37 GB | cogito:14b&lt;/p&gt; &lt;p&gt;8.37 GB | deepcoder:latest&lt;/p&gt; &lt;p&gt;8.37 GB | deepcoder:14b&lt;/p&gt; &lt;p&gt;8.37 GB | deepseek-r1:14b&lt;/p&gt; &lt;p&gt;8.37 GB | qwen2.5:14b&lt;/p&gt; &lt;p&gt;8.37 GB | qwen2.5-coder:14b&lt;/p&gt; &lt;p&gt;8.37 GB | sqlcoder:15b&lt;/p&gt; &lt;p&gt;8.37 GB | starcoder:15b&lt;/p&gt; &lt;p&gt;8.29 GB | deepseek-coder-v2:latest&lt;/p&gt; &lt;p&gt;8.29 GB | deepseek-coder-v2:16b&lt;/p&gt; &lt;p&gt;8.29 GB | deepseek-v2:latest&lt;/p&gt; &lt;p&gt;8.29 GB | deepseek-v2:16b&lt;/p&gt; &lt;p&gt;7.78 GB | olmo2:13b&lt;/p&gt; &lt;p&gt;7.62 GB | qwen:14b&lt;/p&gt; &lt;p&gt;7.59 GB | gemma3:12b&lt;/p&gt; &lt;p&gt;7.55 GB | translategemma:12b&lt;/p&gt; &lt;p&gt;7.46 GB | llava:13b&lt;/p&gt; &lt;p&gt;7.35 GB | phi3:14b&lt;/p&gt; &lt;p&gt;7.28 GB | llama3.2-vision:latest&lt;/p&gt; &lt;p&gt;7.28 GB | llama3.2-vision:11b&lt;/p&gt; &lt;p&gt;7.03 GB | gemma3n:latest&lt;/p&gt; &lt;p&gt;6.86 GB | codellama:13b&lt;/p&gt; &lt;p&gt;6.86 GB | codeup:latest&lt;/p&gt; &lt;p&gt;6.86 GB | codeup:13b&lt;/p&gt; &lt;p&gt;6.86 GB | everythinglm:latest&lt;/p&gt; &lt;p&gt;6.86 GB | everythinglm:13b&lt;/p&gt; &lt;p&gt;6.86 GB | llama2:13b&lt;/p&gt; &lt;p&gt;6.86 GB | llama2-chinese:13b&lt;/p&gt; &lt;p&gt;6.86 GB | nexusraven:latest&lt;/p&gt; &lt;p&gt;6.86 GB | nexusraven:13b&lt;/p&gt; &lt;p&gt;6.86 GB | nous-hermes:13b&lt;/p&gt; &lt;p&gt;6.86 GB | open-orca-platypus2:latest&lt;/p&gt; &lt;p&gt;6.86 GB | open-orca-platypus2:13b&lt;/p&gt; &lt;p&gt;6.86 GB | orca-mini:13b&lt;/p&gt; &lt;p&gt;6.86 GB | orca2:13b&lt;/p&gt; &lt;p&gt;6.86 GB | stable-beluga:13b&lt;/p&gt; &lt;p&gt;6.86 GB | vicuna:13b&lt;/p&gt; &lt;p&gt;6.86 GB | wizard-math:13b&lt;/p&gt; &lt;p&gt;6.86 GB | wizard-vicuna:latest&lt;/p&gt; &lt;p&gt;6.86 GB | wizard-vicuna:13b&lt;/p&gt; &lt;p&gt;6.86 GB | wizard-vicuna-uncensored:13b&lt;/p&gt; &lt;p&gt;6.86 GB | wizardlm-uncensored:latest&lt;/p&gt; &lt;p&gt;6.86 GB | wizardlm-uncensored:13b&lt;/p&gt; &lt;p&gt;6.86 GB | xwinlm:13b&lt;/p&gt; &lt;p&gt;6.86 GB | yarn-llama2:13b&lt;/p&gt; &lt;p&gt;6.59 GB | mistral-nemo:latest&lt;/p&gt; &lt;p&gt;6.59 GB | mistral-nemo:12b&lt;/p&gt; &lt;p&gt;6.49 GB | stablelm2:12b&lt;/p&gt; &lt;p&gt;6.23 GB | deepseek-ocr:latest&lt;/p&gt; &lt;p&gt;6.23 GB | deepseek-ocr:3b&lt;/p&gt; &lt;p&gt;5.94 GB | falcon2:latest&lt;/p&gt; &lt;p&gt;5.94 GB | falcon2:11b&lt;/p&gt; &lt;p&gt;5.86 GB | falcon3:10b&lt;/p&gt; &lt;p&gt;5.72 GB | qwen3-vl:latest&lt;/p&gt; &lt;p&gt;5.72 GB | qwen3-vl:8b&lt;/p&gt; &lt;p&gt;5.66 GB | nous-hermes2:latest&lt;/p&gt; &lt;p&gt;5.66 GB | nous-hermes2:10.7b&lt;/p&gt; &lt;p&gt;5.66 GB | solar:latest&lt;/p&gt; &lt;p&gt;5.66 GB | solar:10.7b&lt;/p&gt; &lt;p&gt;5.61 GB | ministral-3:latest&lt;/p&gt; &lt;p&gt;5.61 GB | ministral-3:8b&lt;/p&gt; &lt;p&gt;5.56 GB | qwen2.5vl:latest&lt;/p&gt; &lt;p&gt;5.56 GB | qwen2.5vl:7b&lt;/p&gt; &lt;p&gt;5.4 GB | granite3-guardian:8b&lt;/p&gt; &lt;p&gt;5.37 GB | shieldgemma:latest&lt;/p&gt; &lt;p&gt;5.37 GB | shieldgemma:9b&lt;/p&gt; &lt;p&gt;5.16 GB | llava-llama3:latest&lt;/p&gt; &lt;p&gt;5.16 GB | llava-llama3:8b&lt;/p&gt; &lt;p&gt;5.1 GB | minicpm-v:latest&lt;/p&gt; &lt;p&gt;5.1 GB | minicpm-v:8b&lt;/p&gt; &lt;p&gt;5.08 GB | codegeex4:latest&lt;/p&gt; &lt;p&gt;5.08 GB | codegeex4:9b&lt;/p&gt; &lt;p&gt;5.08 GB | glm4:latest&lt;/p&gt; &lt;p&gt;5.08 GB | glm4:9b&lt;/p&gt; &lt;p&gt;5.07 GB | gemma2:latest&lt;/p&gt; &lt;p&gt;5.07 GB | gemma2:9b&lt;/p&gt; &lt;p&gt;4.88 GB | sailor2:latest&lt;/p&gt; &lt;p&gt;4.88 GB | sailor2:8b&lt;/p&gt; &lt;p&gt;4.87 GB | deepseek-r1:latest&lt;/p&gt; &lt;p&gt;4.87 GB | deepseek-r1:8b&lt;/p&gt; &lt;p&gt;4.87 GB | qwen3:latest&lt;/p&gt; &lt;p&gt;4.87 GB | qwen3:8b&lt;/p&gt; &lt;p&gt;4.76 GB | rnj-1:latest&lt;/p&gt; &lt;p&gt;4.76 GB | rnj-1:8b&lt;/p&gt; &lt;p&gt;4.71 GB | aya-expanse:latest&lt;/p&gt; &lt;p&gt;4.71 GB | aya-expanse:8b&lt;/p&gt; &lt;p&gt;4.71 GB | command-r7b:latest&lt;/p&gt; &lt;p&gt;4.71 GB | command-r7b:7b&lt;/p&gt; &lt;p&gt;4.71 GB | command-r7b-arabic:latest&lt;/p&gt; &lt;p&gt;4.71 GB | command-r7b-arabic:7b&lt;/p&gt; &lt;p&gt;4.69 GB | yi:9b&lt;/p&gt; &lt;p&gt;4.69 GB | yi-coder:latest&lt;/p&gt; &lt;p&gt;4.69 GB | yi-coder:9b&lt;/p&gt; &lt;p&gt;4.67 GB | codegemma:latest&lt;/p&gt; &lt;p&gt;4.67 GB | codegemma:7b&lt;/p&gt; &lt;p&gt;4.67 GB | gemma:latest&lt;/p&gt; &lt;p&gt;4.67 GB | gemma:7b&lt;/p&gt; &lt;p&gt;4.65 GB | granite3.1-dense:latest&lt;/p&gt; &lt;p&gt;4.65 GB | granite3.1-dense:8b&lt;/p&gt; &lt;p&gt;4.6 GB | granite3-dense:8b&lt;/p&gt; &lt;p&gt;4.6 GB | granite3.2:latest&lt;/p&gt; &lt;p&gt;4.6 GB | granite3.2:8b&lt;/p&gt; &lt;p&gt;4.6 GB | granite3.3:latest&lt;/p&gt; &lt;p&gt;4.6 GB | granite3.3:8b&lt;/p&gt; &lt;p&gt;4.58 GB | cogito:latest&lt;/p&gt; &lt;p&gt;4.58 GB | cogito:8b&lt;/p&gt; &lt;p&gt;4.58 GB | dolphin3:latest&lt;/p&gt; &lt;p&gt;4.58 GB | dolphin3:8b&lt;/p&gt; &lt;p&gt;4.58 GB | llama-guard3:latest&lt;/p&gt; &lt;p&gt;4.58 GB | llama-guard3:8b&lt;/p&gt; &lt;p&gt;4.58 GB | llama3.1:latest&lt;/p&gt; &lt;p&gt;4.58 GB | llama3.1:8b&lt;/p&gt; &lt;p&gt;4.58 GB | tulu3:latest&lt;/p&gt; &lt;p&gt;4.58 GB | tulu3:8b&lt;/p&gt; &lt;p&gt;4.47 GB | aya:latest&lt;/p&gt; &lt;p&gt;4.47 GB | aya:8b&lt;/p&gt; &lt;p&gt;4.44 GB | exaone-deep:latest&lt;/p&gt; &lt;p&gt;4.44 GB | exaone-deep:7.8b&lt;/p&gt; &lt;p&gt;4.44 GB | exaone3.5:latest&lt;/p&gt; &lt;p&gt;4.44 GB | exaone3.5:7.8b&lt;/p&gt; &lt;p&gt;4.41 GB | bakllava:latest&lt;/p&gt; &lt;p&gt;4.41 GB | bakllava:7b&lt;/p&gt; &lt;p&gt;4.41 GB | llama-pro:latest&lt;/p&gt; &lt;p&gt;4.41 GB | llava:latest&lt;/p&gt; &lt;p&gt;4.41 GB | llava:7b&lt;/p&gt; &lt;p&gt;4.41 GB | opencoder:latest&lt;/p&gt; &lt;p&gt;4.41 GB | opencoder:8b&lt;/p&gt; &lt;p&gt;4.39 GB | bespoke-minicheck:latest&lt;/p&gt; &lt;p&gt;4.39 GB | bespoke-minicheck:7b&lt;/p&gt; &lt;p&gt;4.36 GB | deepseek-r1:7b&lt;/p&gt; &lt;p&gt;4.36 GB | marco-o1:latest&lt;/p&gt; &lt;p&gt;4.36 GB | marco-o1:7b&lt;/p&gt; &lt;p&gt;4.36 GB | openthinker:latest&lt;/p&gt; &lt;p&gt;4.36 GB | openthinker:7b&lt;/p&gt; &lt;p&gt;4.36 GB | qwen2.5:latest&lt;/p&gt; &lt;p&gt;4.36 GB | qwen2.5:7b&lt;/p&gt; &lt;p&gt;4.36 GB | qwen2.5-coder:latest&lt;/p&gt; &lt;p&gt;4.36 GB | qwen2.5-coder:7b&lt;/p&gt; &lt;p&gt;4.36 GB | qwen3-embedding:latest&lt;/p&gt; &lt;p&gt;4.36 GB | qwen3-embedding:8b&lt;/p&gt; &lt;p&gt;4.34 GB | dolphin-llama3:latest&lt;/p&gt; &lt;p&gt;4.34 GB | dolphin-llama3:8b&lt;/p&gt; &lt;p&gt;4.34 GB | hermes3:latest&lt;/p&gt; &lt;p&gt;4.34 GB | hermes3:8b&lt;/p&gt; &lt;p&gt;4.34 GB | llama3:latest&lt;/p&gt; &lt;p&gt;4.34 GB | llama3:8b&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-chatqa:latest&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-chatqa:8b&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-gradient:latest&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-gradient:8b&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-groq-tool-use:latest&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-groq-tool-use:8b&lt;/p&gt; &lt;p&gt;4.28 GB | granite-code:8b&lt;/p&gt; &lt;p&gt;4.26 GB | falcon3:latest&lt;/p&gt; &lt;p&gt;4.26 GB | falcon3:7b&lt;/p&gt; &lt;p&gt;4.2 GB | qwen:7b&lt;/p&gt; &lt;p&gt;4.16 GB | olmo-3:latest&lt;/p&gt; &lt;p&gt;4.16 GB | olmo-3:7b&lt;/p&gt; &lt;p&gt;4.16 GB | olmo2:latest&lt;/p&gt; &lt;p&gt;4.16 GB | olmo2:7b&lt;/p&gt; &lt;p&gt;4.15 GB | internlm2:latest&lt;/p&gt; &lt;p&gt;4.15 GB | internlm2:7b&lt;/p&gt; &lt;p&gt;4.13 GB | qwen2:latest&lt;/p&gt; &lt;p&gt;4.13 GB | qwen2:7b&lt;/p&gt; &lt;p&gt;4.13 GB | qwen2-math:latest&lt;/p&gt; &lt;p&gt;4.13 GB | qwen2-math:7b&lt;/p&gt; &lt;p&gt;4.07 GB | mistral:latest&lt;/p&gt; &lt;p&gt;4.07 GB | mistral:7b&lt;/p&gt; &lt;p&gt;4.0 GB | starcoder:7b&lt;/p&gt; &lt;p&gt;3.94 GB | dolphincoder:latest&lt;/p&gt; &lt;p&gt;3.94 GB | dolphincoder:7b&lt;/p&gt; &lt;p&gt;3.92 GB | falcon:latest&lt;/p&gt; &lt;p&gt;3.92 GB | falcon:7b&lt;/p&gt; &lt;p&gt;3.89 GB | codeqwen:latest&lt;/p&gt; &lt;p&gt;3.89 GB | codeqwen:7b&lt;/p&gt; &lt;p&gt;3.83 GB | dolphin-mistral:latest&lt;/p&gt; &lt;p&gt;3.83 GB | dolphin-mistral:7b&lt;/p&gt; &lt;p&gt;3.83 GB | mathstral:latest&lt;/p&gt; &lt;p&gt;3.83 GB | mathstral:7b&lt;/p&gt; &lt;p&gt;3.83 GB | mistral-openorca:latest&lt;/p&gt; &lt;p&gt;3.83 GB | mistral-openorca:7b&lt;/p&gt; &lt;p&gt;3.83 GB | mistrallite:latest&lt;/p&gt; &lt;p&gt;3.83 GB | mistrallite:7b&lt;/p&gt; &lt;p&gt;3.83 GB | neural-chat:latest&lt;/p&gt; &lt;p&gt;3.83 GB | neural-chat:7b&lt;/p&gt; &lt;p&gt;3.83 GB | notus:latest&lt;/p&gt; &lt;p&gt;3.83 GB | notus:7b&lt;/p&gt; &lt;p&gt;3.83 GB | openchat:latest&lt;/p&gt; &lt;p&gt;3.83 GB | openchat:7b&lt;/p&gt; &lt;p&gt;3.83 GB | openhermes:latest&lt;/p&gt; &lt;p&gt;3.83 GB | samantha-mistral:latest&lt;/p&gt; &lt;p&gt;3.83 GB | samantha-mistral:7b&lt;/p&gt; &lt;p&gt;3.83 GB | sqlcoder:latest&lt;/p&gt; &lt;p&gt;3.83 GB | sqlcoder:7b&lt;/p&gt; &lt;p&gt;3.83 GB | starling-lm:latest&lt;/p&gt; &lt;p&gt;3.83 GB | starling-lm:7b&lt;/p&gt; &lt;p&gt;3.83 GB | wizard-math:latest&lt;/p&gt; &lt;p&gt;3.83 GB | wizard-math:7b&lt;/p&gt; &lt;p&gt;3.83 GB | wizardlm2:latest&lt;/p&gt; &lt;p&gt;3.83 GB | wizardlm2:7b&lt;/p&gt; &lt;p&gt;3.83 GB | yarn-mistral:latest&lt;/p&gt; &lt;p&gt;3.83 GB | yarn-mistral:7b&lt;/p&gt; &lt;p&gt;3.83 GB | zephyr:latest&lt;/p&gt; &lt;p&gt;3.83 GB | zephyr:7b&lt;/p&gt; &lt;p&gt;3.77 GB | starcoder2:7b&lt;/p&gt; &lt;p&gt;3.73 GB | deepseek-llm:latest&lt;/p&gt; &lt;p&gt;3.73 GB | deepseek-llm:7b&lt;/p&gt; &lt;p&gt;3.56 GB | codellama:latest&lt;/p&gt; &lt;p&gt;3.56 GB | codellama:7b&lt;/p&gt; &lt;p&gt;3.56 GB | deepseek-coder:6.7b&lt;/p&gt; &lt;p&gt;3.56 GB | duckdb-nsql:latest&lt;/p&gt; &lt;p&gt;3.56 GB | duckdb-nsql:7b&lt;/p&gt; &lt;p&gt;3.56 GB | llama2:latest&lt;/p&gt; &lt;p&gt;3.56 GB | llama2:7b&lt;/p&gt; &lt;p&gt;3.56 GB | llama2-chinese:latest&lt;/p&gt; &lt;p&gt;3.56 GB | llama2-chinese:7b&lt;/p&gt; &lt;p&gt;3.56 GB | llama2-uncensored:latest&lt;/p&gt; &lt;p&gt;3.56 GB | llama2-uncensored:7b&lt;/p&gt; &lt;p&gt;3.56 GB | magicoder:latest&lt;/p&gt; &lt;p&gt;3.56 GB | magicoder:7b&lt;/p&gt; &lt;p&gt;3.56 GB | meditron:latest&lt;/p&gt; &lt;p&gt;3.56 GB | meditron:7b&lt;/p&gt; &lt;p&gt;3.56 GB | medllama2:latest&lt;/p&gt; &lt;p&gt;3.56 GB | medllama2:7b&lt;/p&gt; &lt;p&gt;3.56 GB | nous-hermes:latest&lt;/p&gt; &lt;p&gt;3.56 GB | nous-hermes:7b&lt;/p&gt; &lt;p&gt;3.56 GB | orca-mini:7b&lt;/p&gt; &lt;p&gt;3.56 GB | orca2:latest&lt;/p&gt; &lt;p&gt;3.56 GB | orca2:7b&lt;/p&gt; &lt;p&gt;3.56 GB | stable-beluga:latest&lt;/p&gt; &lt;p&gt;3.56 GB | stable-beluga:7b&lt;/p&gt; &lt;p&gt;3.56 GB | vicuna:latest&lt;/p&gt; &lt;p&gt;3.56 GB | vicuna:7b&lt;/p&gt; &lt;p&gt;3.56 GB | wizard-vicuna-uncensored:latest&lt;/p&gt; &lt;p&gt;3.56 GB | wizard-vicuna-uncensored:7b&lt;/p&gt; &lt;p&gt;3.56 GB | xwinlm:latest&lt;/p&gt; &lt;p&gt;3.56 GB | xwinlm:7b&lt;/p&gt; &lt;p&gt;3.56 GB | yarn-llama2:latest&lt;/p&gt; &lt;p&gt;3.56 GB | yarn-llama2:7b&lt;/p&gt; &lt;p&gt;3.37 GB | smallthinker:latest&lt;/p&gt; &lt;p&gt;3.37 GB | smallthinker:3b&lt;/p&gt; &lt;p&gt;3.32 GB | deepscaler:latest&lt;/p&gt; &lt;p&gt;3.32 GB | deepscaler:1.5b&lt;/p&gt; &lt;p&gt;3.24 GB | yi:latest&lt;/p&gt; &lt;p&gt;3.24 GB | yi:6b&lt;/p&gt; &lt;p&gt;3.11 GB | gemma3:latest&lt;/p&gt; &lt;p&gt;3.11 GB | gemma3:4b&lt;/p&gt; &lt;p&gt;3.07 GB | qwen3-vl:4b&lt;/p&gt; &lt;p&gt;3.07 GB | translategemma:latest&lt;/p&gt; &lt;p&gt;3.07 GB | translategemma:4b&lt;/p&gt; &lt;p&gt;3.04 GB | granite4:1b&lt;/p&gt; &lt;p&gt;2.98 GB | qwen2.5vl:3b&lt;/p&gt; &lt;p&gt;2.94 GB | phi4-mini-reasoning:latest&lt;/p&gt; &lt;p&gt;2.94 GB | phi4-mini-reasoning:3.8b&lt;/p&gt; &lt;p&gt;2.75 GB | ministral-3:3b&lt;/p&gt; &lt;p&gt;2.73 GB | llava-phi3:latest&lt;/p&gt; &lt;p&gt;2.73 GB | llava-phi3:3.8b&lt;/p&gt; &lt;p&gt;2.51 GB | granite3-guardian:latest&lt;/p&gt; &lt;p&gt;2.51 GB | granite3-guardian:2b&lt;/p&gt; &lt;p&gt;2.51 GB | nemotron-mini:latest&lt;/p&gt; &lt;p&gt;2.51 GB | nemotron-mini:4b&lt;/p&gt; &lt;p&gt;2.33 GB | qwen3:4b&lt;/p&gt; &lt;p&gt;2.33 GB | qwen3-embedding:4b&lt;/p&gt; &lt;p&gt;2.32 GB | phi4-mini:latest&lt;/p&gt; &lt;p&gt;2.32 GB | phi4-mini:3.8b&lt;/p&gt; &lt;p&gt;2.27 GB | granite3.2-vision:latest&lt;/p&gt; &lt;p&gt;2.27 GB | granite3.2-vision:2b&lt;/p&gt; &lt;p&gt;2.17 GB | qwen:latest&lt;/p&gt; &lt;p&gt;2.17 GB | qwen:4b&lt;/p&gt; &lt;p&gt;2.09 GB | cogito:3b&lt;/p&gt; &lt;p&gt;2.03 GB | nuextract:latest&lt;/p&gt; &lt;p&gt;2.03 GB | nuextract:3.8b&lt;/p&gt; &lt;p&gt;2.03 GB | phi3:latest&lt;/p&gt; &lt;p&gt;2.03 GB | phi3:3.8b&lt;/p&gt; &lt;p&gt;2.03 GB | phi3.5:latest&lt;/p&gt; &lt;p&gt;2.03 GB | phi3.5:3.8b&lt;/p&gt; &lt;p&gt;1.96 GB | granite4:3b&lt;/p&gt; &lt;p&gt;1.92 GB | granite3-moe:3b&lt;/p&gt; &lt;p&gt;1.9 GB | granite3.1-moe:latest&lt;/p&gt; &lt;p&gt;1.9 GB | granite3.1-moe:3b&lt;/p&gt; &lt;p&gt;1.88 GB | hermes3:3b&lt;/p&gt; &lt;p&gt;1.88 GB | llama3.2:latest&lt;/p&gt; &lt;p&gt;1.88 GB | llama3.2:3b&lt;/p&gt; &lt;p&gt;1.87 GB | falcon3:3b&lt;/p&gt; &lt;p&gt;1.86 GB | granite-code:latest&lt;/p&gt; &lt;p&gt;1.86 GB | granite-code:3b&lt;/p&gt; &lt;p&gt;1.84 GB | orca-mini:latest&lt;/p&gt; &lt;p&gt;1.84 GB | orca-mini:3b&lt;/p&gt; &lt;p&gt;1.8 GB | qwen2.5:3b&lt;/p&gt; &lt;p&gt;1.8 GB | qwen2.5-coder:3b&lt;/p&gt; &lt;p&gt;1.76 GB | qwen3-vl:2b&lt;/p&gt; &lt;p&gt;1.71 GB | starcoder:latest&lt;/p&gt; &lt;p&gt;1.71 GB | starcoder:3b&lt;/p&gt; &lt;p&gt;1.7 GB | smollm2:latest&lt;/p&gt; &lt;p&gt;1.7 GB | smollm2:1.7b&lt;/p&gt; &lt;p&gt;1.66 GB | falcon3:1b&lt;/p&gt; &lt;p&gt;1.62 GB | moondream:latest&lt;/p&gt; &lt;p&gt;1.62 GB | moondream:1.8b&lt;/p&gt; &lt;p&gt;1.59 GB | shieldgemma:2b&lt;/p&gt; &lt;p&gt;1.59 GB | starcoder2:latest&lt;/p&gt; &lt;p&gt;1.59 GB | starcoder2:3b&lt;/p&gt; &lt;p&gt;1.56 GB | gemma:2b&lt;/p&gt; &lt;p&gt;1.53 GB | exaone-deep:2.4b&lt;/p&gt; &lt;p&gt;1.53 GB | exaone3.5:2.4b&lt;/p&gt; &lt;p&gt;1.52 GB | gemma2:2b&lt;/p&gt; &lt;p&gt;1.5 GB | stable-code:latest&lt;/p&gt; &lt;p&gt;1.5 GB | stable-code:3b&lt;/p&gt; &lt;p&gt;1.5 GB | stablelm-zephyr:latest&lt;/p&gt; &lt;p&gt;1.5 GB | stablelm-zephyr:3b&lt;/p&gt; &lt;p&gt;1.49 GB | dolphin-phi:latest&lt;/p&gt; &lt;p&gt;1.49 GB | dolphin-phi:2.7b&lt;/p&gt; &lt;p&gt;1.49 GB | granite3-dense:latest&lt;/p&gt; &lt;p&gt;1.49 GB | granite3-dense:2b&lt;/p&gt; &lt;p&gt;1.49 GB | llama-guard3:1b&lt;/p&gt; &lt;p&gt;1.49 GB | phi:latest&lt;/p&gt; &lt;p&gt;1.49 GB | phi:2.7b&lt;/p&gt; &lt;p&gt;1.46 GB | granite3.1-dense:2b&lt;/p&gt; &lt;p&gt;1.44 GB | codegemma:2b&lt;/p&gt; &lt;p&gt;1.44 GB | granite3.2:2b&lt;/p&gt; &lt;p&gt;1.44 GB | granite3.3:2b&lt;/p&gt; &lt;p&gt;1.32 GB | granite3.1-moe:1b&lt;/p&gt; &lt;p&gt;1.32 GB | opencoder:1.5b&lt;/p&gt; &lt;p&gt;1.27 GB | qwen3:1.7b&lt;/p&gt; &lt;p&gt;1.23 GB | llama3.2:1b&lt;/p&gt; &lt;p&gt;1.08 GB | bge-m3:latest&lt;/p&gt; &lt;p&gt;1.08 GB | snowflake-arctic-embed2:latest&lt;/p&gt; &lt;p&gt;1.04 GB | deepcoder:1.5b&lt;/p&gt; &lt;p&gt;1.04 GB | deepseek-r1:1.5b&lt;/p&gt; &lt;p&gt;1.04 GB | internlm2:1.8b&lt;/p&gt; &lt;p&gt;1.04 GB | qwen:1.8b&lt;/p&gt; &lt;p&gt;0.98 GB | sailor2:1b&lt;/p&gt; &lt;p&gt;0.92 GB | qwen2.5:1.5b&lt;/p&gt; &lt;p&gt;0.92 GB | qwen2.5-coder:1.5b&lt;/p&gt; &lt;p&gt;0.92 GB | smollm:latest&lt;/p&gt; &lt;p&gt;0.92 GB | smollm:1.7b&lt;/p&gt; &lt;p&gt;0.92 GB | stablelm2:latest&lt;/p&gt; &lt;p&gt;0.92 GB | stablelm2:1.6b&lt;/p&gt; &lt;p&gt;0.87 GB | qwen2:1.5b&lt;/p&gt; &lt;p&gt;0.87 GB | qwen2-math:1.5b&lt;/p&gt; &lt;p&gt;0.87 GB | reader-lm:latest&lt;/p&gt; &lt;p&gt;0.87 GB | reader-lm:1.5b&lt;/p&gt; &lt;p&gt;0.81 GB | yi-coder:1.5b&lt;/p&gt; &lt;p&gt;0.77 GB | granite3-moe:latest&lt;/p&gt; &lt;p&gt;0.77 GB | granite3-moe:1b&lt;/p&gt; &lt;p&gt;0.76 GB | gemma3:1b&lt;/p&gt; &lt;p&gt;0.72 GB | deepseek-coder:latest&lt;/p&gt; &lt;p&gt;0.72 GB | deepseek-coder:1.3b&lt;/p&gt; &lt;p&gt;0.68 GB | lfm2.5-thinking:latest&lt;/p&gt; &lt;p&gt;0.68 GB | lfm2.5-thinking:1.2b&lt;/p&gt; &lt;p&gt;0.68 GB | starcoder:1b&lt;/p&gt; &lt;p&gt;0.62 GB | bge-large:latest&lt;/p&gt; &lt;p&gt;0.62 GB | mxbai-embed-large:latest&lt;/p&gt; &lt;p&gt;0.62 GB | snowflake-arctic-embed:latest&lt;/p&gt; &lt;p&gt;0.6 GB | qwen3-embedding:0.6b&lt;/p&gt; &lt;p&gt;0.59 GB | tinydolphin:latest&lt;/p&gt; &lt;p&gt;0.59 GB | tinydolphin:1.1b&lt;/p&gt; &lt;p&gt;0.59 GB | tinyllama:latest&lt;/p&gt; &lt;p&gt;0.59 GB | tinyllama:1.1b&lt;/p&gt; &lt;p&gt;0.58 GB | embeddinggemma:latest&lt;/p&gt; &lt;p&gt;0.52 GB | paraphrase-multilingual:latest&lt;/p&gt; &lt;p&gt;0.49 GB | qwen3:0.6b&lt;/p&gt; &lt;p&gt;0.37 GB | qwen:0.5b&lt;/p&gt; &lt;p&gt;0.37 GB | qwen2.5:0.5b&lt;/p&gt; &lt;p&gt;0.37 GB | qwen2.5-coder:0.5b&lt;/p&gt; &lt;p&gt;0.33 GB | qwen2:0.5b&lt;/p&gt; &lt;p&gt;0.33 GB | reader-lm:0.5b&lt;/p&gt; &lt;p&gt;0.28 GB | functiongemma:latest&lt;/p&gt; &lt;p&gt;0.26 GB | nomic-embed-text:latest&lt;/p&gt; &lt;p&gt;0.06 GB | granite-embedding:latest&lt;/p&gt; &lt;p&gt;0.04 GB | all-minilm:latest&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventurousLion9548"&gt; /u/AdventurousLion9548 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T04:36:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnmj5x</id>
    <title>xsukax Ollama AI Prompt Generator - A Privacy-First Tool for Enhancing AI Prompts Locally</title>
    <updated>2026-01-26T17:27:07+00:00</updated>
    <author>
      <name>/u/apt-xsukax</name>
      <uri>https://old.reddit.com/user/apt-xsukax</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apt-xsukax"&gt; /u/apt-xsukax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/PromptEngineering/comments/1qnm14s/xsukax_ollama_ai_prompt_generator_a_privacyfirst/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnmj5x/xsukax_ollama_ai_prompt_generator_a_privacyfirst/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnmj5x/xsukax_ollama_ai_prompt_generator_a_privacyfirst/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T17:27:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnjla4</id>
    <title>Local image generation with Ollama + FLUX + Celeste AI</title>
    <updated>2026-01-26T15:45:48+00:00</updated>
    <author>
      <name>/u/Familiar_Print_4882</name>
      <uri>https://old.reddit.com/user/Familiar_Print_4882</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qnjla4/local_image_generation_with_ollama_flux_celeste_ai/"&gt; &lt;img alt="Local image generation with Ollama + FLUX + Celeste AI" src="https://preview.redd.it/lkdh65o9rpfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4d9e789cf33f12ea6fff2c60b35758b760a5385" title="Local image generation with Ollama + FLUX + Celeste AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Familiar_Print_4882"&gt; /u/Familiar_Print_4882 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lkdh65o9rpfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnjla4/local_image_generation_with_ollama_flux_celeste_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnjla4/local_image_generation_with_ollama_flux_celeste_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T15:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnd67t</id>
    <title>Any free ollama models that works well with Cline tool calling?</title>
    <updated>2026-01-26T11:08:04+00:00</updated>
    <author>
      <name>/u/mixoadrian</name>
      <uri>https://old.reddit.com/user/mixoadrian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, i use free local models with ollama on Cline.&lt;/p&gt; &lt;p&gt;I have been using quite a few, deepseek-coder:33b, Qwen3-cider:30b, llama3.1, gemma3:12b.&lt;/p&gt; &lt;p&gt;none work, nearly all tool calling would fail, and often not even able to read file.&lt;/p&gt; &lt;p&gt;it happens only recently, perhaps after an update i didnt realise.&lt;/p&gt; &lt;p&gt;IS this normal or is it just me?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mixoadrian"&gt; /u/mixoadrian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnd67t/any_free_ollama_models_that_works_well_with_cline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnd67t/any_free_ollama_models_that_works_well_with_cline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnd67t/any_free_ollama_models_that_works_well_with_cline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T11:08:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnp78i</id>
    <title>ollama-term: A sleek, terminal-based UI for Ollama ‚Äî because sometimes you just want to run LLMs without leaving the CLI</title>
    <updated>2026-01-26T18:56:33+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qnp78i/ollamaterm_a_sleek_terminalbased_ui_for_ollama/"&gt; &lt;img alt="ollama-term: A sleek, terminal-based UI for Ollama ‚Äî because sometimes you just want to run LLMs without leaving the CLI" src="https://b.thumbs.redditmedia.com/ydEn5j3DSjsy6n2Mbq0CtCmVEOhk93HNTZYu9u0TPio.jpg" title="ollama-term: A sleek, terminal-based UI for Ollama ‚Äî because sometimes you just want to run LLMs without leaving the CLI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/31ju1q1vpqfg1.gif"&gt;https://i.redd.it/31ju1q1vpqfg1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been using Ollama a ton to run local LLMs. I use linux and like the terminal so i create a ui in the terminal that connect to ollama&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/Laszlobeer/ollama-term"&gt;https://github.com/Laszlobeer/ollama-term&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some of the main features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Browse &amp;amp; Manage Models:&lt;/strong&gt; See all your pulled models, pull new ones, delete them, and view details, all in a navigable interface.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat Interface:&lt;/strong&gt; Have multi-turn conversations with any model. The chat pane is clean and focuses on readability.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context &amp;amp; System Prompt:&lt;/strong&gt; Easily set a system prompt and see how many tokens are in your current context window&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lightweight:&lt;/strong&gt; It's a single binary. Just download and run.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uw87e7g5qqfg1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bf0e282b9569d637a97cbd3e5ddf01fb50d98752"&gt;https://preview.redd.it/uw87e7g5qqfg1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bf0e282b9569d637a97cbd3e5ddf01fb50d98752&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnp78i/ollamaterm_a_sleek_terminalbased_ui_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnp78i/ollamaterm_a_sleek_terminalbased_ui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnp78i/ollamaterm_a_sleek_terminalbased_ui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T18:56:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnyhr5</id>
    <title>Ollama 0.15 can't run in Win svr 2022?</title>
    <updated>2026-01-27T00:36:17+00:00</updated>
    <author>
      <name>/u/IsaacWang</name>
      <uri>https://old.reddit.com/user/IsaacWang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The earlier version is ok. Click and dead after installing the 0.15 version.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IsaacWang"&gt; /u/IsaacWang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnyhr5/ollama_015_cant_run_in_win_svr_2022/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnyhr5/ollama_015_cant_run_in_win_svr_2022/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnyhr5/ollama_015_cant_run_in_win_svr_2022/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T00:36:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnsuou</id>
    <title>Advice needed!</title>
    <updated>2026-01-26T21:02:43+00:00</updated>
    <author>
      <name>/u/TheBlueFlashh</name>
      <uri>https://old.reddit.com/user/TheBlueFlashh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello I'm pretty new to LLM but id like to give it a go. I've got a 5070ti 16gb and a good cpu (although I think it doesnt matter?) and Id like to do some social work with it. That means reasoning from local info only and sumarizing for the most part. If I coud create audios from texts would be awesome as well.&lt;br /&gt; Magistral is my best bet right? how to go with audios?&lt;br /&gt; Thank you everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheBlueFlashh"&gt; /u/TheBlueFlashh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnsuou/advice_needed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnsuou/advice_needed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnsuou/advice_needed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T21:02:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo33hq</id>
    <title>What kind of models can I realistically run on my M2 MacBook Air with 8GB RAM?</title>
    <updated>2026-01-27T03:55:47+00:00</updated>
    <author>
      <name>/u/saintforlife1</name>
      <uri>https://old.reddit.com/user/saintforlife1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are there particular models optimized to run on everyday hardware like this fairly reasonable? TIA!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/saintforlife1"&gt; /u/saintforlife1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qo33hq/what_kind_of_models_can_i_realistically_run_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qo33hq/what_kind_of_models_can_i_realistically_run_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qo33hq/what_kind_of_models_can_i_realistically_run_on_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T03:55:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoe6ny</id>
    <title>Renting out the cheapest GPUs ! (CPU options available too)</title>
    <updated>2026-01-27T13:43:14+00:00</updated>
    <author>
      <name>/u/Comfortable-Wall-465</name>
      <uri>https://old.reddit.com/user/Comfortable-Wall-465</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there, I will keep it short, I am renting out GPUs at the &lt;strong&gt;cheapest price you can find out there&lt;/strong&gt;. The pricing are as follows:&lt;/p&gt; &lt;p&gt;RTX-4090: $0.15&lt;br /&gt; RTX-A6000: $0.3&lt;br /&gt; L40S: $0.40&lt;br /&gt; A100 SXM: $0.6&lt;br /&gt; H100: $1.2&lt;/p&gt; &lt;p&gt;(per hour)&lt;/p&gt; &lt;p&gt;To know more, feel free to DM or comment below!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Wall-465"&gt; /u/Comfortable-Wall-465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qoe6ny/renting_out_the_cheapest_gpus_cpu_options/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qoe6ny/renting_out_the_cheapest_gpus_cpu_options/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qoe6ny/renting_out_the_cheapest_gpus_cpu_options/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T13:43:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnpy7y</id>
    <title>Fine tuning open models and prep for robust deployment</title>
    <updated>2026-01-26T19:21:32+00:00</updated>
    <author>
      <name>/u/codes_astro</name>
      <uri>https://old.reddit.com/user/codes_astro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was at a tech event recently and lots of devs mentioned about problem with ML projects, and most common was deployments and production issues.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;note:&lt;/strong&gt; I'm part of the KitOps community&lt;/p&gt; &lt;p&gt;Training a model is crucial but usually the easy part due to tools like Unsloth and lots of other options. You fine-tune it, it works, results look good. But when you start building a product, everything gets messy:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;model files in notebooks&lt;/li&gt; &lt;li&gt;configs and prompts not tracked properly&lt;/li&gt; &lt;li&gt;deployment steps that only work on one machine&lt;/li&gt; &lt;li&gt;datasets or other assets are lying somewhere else&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even when training is clean, moving the model forward feels challenging with real products.&lt;/p&gt; &lt;p&gt;So I tried a full train ‚Üí push ‚Üí pull ‚Üí run flow to see if it could actually be simple.&lt;/p&gt; &lt;p&gt;I fine-tuned a model using Unsloth.&lt;/p&gt; &lt;p&gt;It was fast, becasue I kept it simple for testing purpose, and ran fine using official cookbook. Nothing fancy, just a real dataset and a IBM-Granite-4.0 model.&lt;/p&gt; &lt;p&gt;Training wasn‚Äôt the issue though. What mattered was what came next.&lt;/p&gt; &lt;p&gt;Instead of manually moving files around, I pushed the fine-tuned model to Hugging Face, then imported it into Jozu ML. Jozu treats models like proper versioned artifacts, not random folders.&lt;/p&gt; &lt;p&gt;From there, I used KitOps to pull the model locally. One command and I had everything - weights, configs, metadata in the right place.&lt;/p&gt; &lt;p&gt;After that, running inference or deploying was straightforward.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Now, let me give context on why Jozu or KitOps?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Kitops is only open-source AIML tool for packaging and versioning for ML and it follows best practices for Devops while taking care of AI usecases.&lt;/p&gt; &lt;p&gt;- Jozu is enterprise platform which can be run on-prem on any existing infra and when it comes to problems like hot reload and cold start or pods going offline when making changes in large scale application, it's 7x faster then other in terms of GPU optimization.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The main takeaway for me:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most ML pain isn‚Äôt about training better models.&lt;br /&gt; It‚Äôs about keeping things clean at scale.&lt;/p&gt; &lt;p&gt;Unsloth made training easy.&lt;br /&gt; KitOps kept things organized with versioning and packaging.&lt;br /&gt; Jozu handled production side things like tracking, security and deployment.&lt;/p&gt; &lt;p&gt;I wrote a detailed article &lt;a href="https://mranand.substack.com/p/from-training-to-deployment-push"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codes_astro"&gt; /u/codes_astro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnpy7y/fine_tuning_open_models_and_prep_for_robust/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnpy7y/fine_tuning_open_models_and_prep_for_robust/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnpy7y/fine_tuning_open_models_and_prep_for_robust/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T19:21:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo69l7</id>
    <title>When trying to run latest ollama using zip ollama-windows-arm64.zip from GitHub facing issue. Can Someone please tell me what the issue is.</title>
    <updated>2026-01-27T06:33:01+00:00</updated>
    <author>
      <name>/u/FINALISHERE</name>
      <uri>https://old.reddit.com/user/FINALISHERE</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qo69l7/when_trying_to_run_latest_ollama_using_zip/"&gt; &lt;img alt="When trying to run latest ollama using zip ollama-windows-arm64.zip from GitHub facing issue. Can Someone please tell me what the issue is." src="https://preview.redd.it/ry4lgcgu6ufg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfc0b1d6f647b9c2cd1cc7c784682e2e3c7770c5" title="When trying to run latest ollama using zip ollama-windows-arm64.zip from GitHub facing issue. Can Someone please tell me what the issue is." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can someone please tell me what the issue is.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FINALISHERE"&gt; /u/FINALISHERE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ry4lgcgu6ufg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qo69l7/when_trying_to_run_latest_ollama_using_zip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qo69l7/when_trying_to_run_latest_ollama_using_zip/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T06:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo7hhm</id>
    <title>Are my Pc requirements enough to run Ollama?</title>
    <updated>2026-01-27T07:43:37+00:00</updated>
    <author>
      <name>/u/SupermarketLost7854</name>
      <uri>https://old.reddit.com/user/SupermarketLost7854</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I been hearing that I need a top end pc to run ai models. Ollama peek my interest and I want to ask if my specs are enough. &lt;/p&gt; &lt;p&gt;AMD Ryzen 7 5700x 8 core processor, Memory DDR4 32Gbytes, NVDIA GeForce TRX 3070 8GBytes. &lt;/p&gt; &lt;p&gt;Thank you all and have a good day. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SupermarketLost7854"&gt; /u/SupermarketLost7854 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qo7hhm/are_my_pc_requirements_enough_to_run_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qo7hhm/are_my_pc_requirements_enough_to_run_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qo7hhm/are_my_pc_requirements_enough_to_run_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T07:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnpc7a</id>
    <title>Vulkan vs ROCm on RX 9070XT (RDNA4): 9% faster, 50% less power</title>
    <updated>2026-01-26T19:01:07+00:00</updated>
    <author>
      <name>/u/Due_Pea_372</name>
      <uri>https://old.reddit.com/user/Due_Pea_372</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Benchmarked Ollama 0.15.1 with qwen3-coder:30b on my RX 9070 XT (gfx1201, 16GB VRAM).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Vulkan&lt;/th&gt; &lt;th align="left"&gt;ROCm&lt;/th&gt; &lt;th align="left"&gt;Difference&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Tokens/s&lt;/td&gt; &lt;td align="left"&gt;52.5&lt;/td&gt; &lt;td align="left"&gt;48.2&lt;/td&gt; &lt;td align="left"&gt;+8.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Power&lt;/td&gt; &lt;td align="left"&gt;68 W&lt;/td&gt; &lt;td align="left"&gt;149 W&lt;/td&gt; &lt;td align="left"&gt;-54%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;VRAM&lt;/td&gt; &lt;td align="left"&gt;16.1 GB&lt;/td&gt; &lt;td align="left"&gt;15.8 GB&lt;/td&gt; &lt;td align="left"&gt;+2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TTFT&lt;/td&gt; &lt;td align="left"&gt;27.8 ms&lt;/td&gt; &lt;td align="left"&gt;22.9 ms&lt;/td&gt; &lt;td align="left"&gt;+21%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Temp&lt;/td&gt; &lt;td align="left"&gt;51¬∞C&lt;/td&gt; &lt;td align="left"&gt;47¬∞C&lt;/td&gt; &lt;td align="left"&gt;+8%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Key takeaway:&lt;/strong&gt; Vulkan is not only faster but dramatically more power efficient on RDNA4. ROCm draws 2x the power for worse performance.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt; Cachyos, ollama-vulkan / ollama-rocm from AUR, ~35 runs each.&lt;/p&gt; &lt;p&gt;Script used for benchmarking: &lt;a href="https://github.com/maeddesg/spielwiese"&gt;https://github.com/maeddesg/spielwiese&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone else seeing similar results on RDNA4? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Pea_372"&gt; /u/Due_Pea_372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnpc7a/vulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnpc7a/vulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnpc7a/vulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T19:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnjk4p</id>
    <title>SHELLper üêö: Qwen3 0.6B for More Reliable Multi-Turn Function Calling</title>
    <updated>2026-01-26T15:44:43+00:00</updated>
    <author>
      <name>/u/gabucz</name>
      <uri>https://old.reddit.com/user/gabucz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We fine-tuned a 0.6B model for converting English to executable bash commands. It's small enough to run locally on your laptop, giving you full data privacy.&lt;/p&gt; &lt;p&gt;Multi-turn tool calling is incredibly challenging for small models - before fine-tuning, Qwen3-0.6B had 84% single-call accuracy, which collapses to &lt;strong&gt;only 42% across 5 turns&lt;/strong&gt;! After our tuning, it reaches 100% on our test set, providing dependable multi-turn capabilities.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Parameters&lt;/th&gt; &lt;th align="left"&gt;Tool call accuracy (test set)&lt;/th&gt; &lt;th align="left"&gt;=&amp;gt; 5-turn tool call accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B Instruct (teacher)&lt;/td&gt; &lt;td align="left"&gt;235B&lt;/td&gt; &lt;td align="left"&gt;99%&lt;/td&gt; &lt;td align="left"&gt;95%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 0.6B (base)&lt;/td&gt; &lt;td align="left"&gt;0.6B&lt;/td&gt; &lt;td align="left"&gt;84%&lt;/td&gt; &lt;td align="left"&gt;42%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3 0.6B (tuned)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.6B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;100%&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;100%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/distil-labs/distil-SHELLper"&gt;https://github.com/distil-labs/distil-SHELLper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface model: &lt;a href="https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper"&gt;https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Quick Start&lt;/h1&gt; &lt;p&gt;Set up the environment:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Set up environment python -m venv .venv . .venv/bin/activate pip install openai huggingface_hub &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Dowload the model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;hf download distil-labs/distil-qwen3-0.6b-SHELLper --local-dir distil_model cd distil_model ollama create distil_model -f Modelfile cd .. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the assistant:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python filesystem_demo.py &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The demo asks for permission before executing commands (for safety) and restricts dangerous operations (like &lt;code&gt;rm -r /&lt;/code&gt;), so don't hesitate to try it!&lt;/p&gt; &lt;h1&gt;How We Trained SHELLper&lt;/h1&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;Multi-turn tool calling is exceptionally hard for small models - performance breaks down as tool calls chain, degrading with each turn. If prediction errors are independent (e.g. due to bad parameter values), an 80% accurate model has just a 33% chance of succeeding across 5 turns.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Single tool call accuracy&lt;/th&gt; &lt;th align="left"&gt;5-turn tool call accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;80%&lt;/td&gt; &lt;td align="left"&gt;33%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;90%&lt;/td&gt; &lt;td align="left"&gt;59%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;95%&lt;/td&gt; &lt;td align="left"&gt;77%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;99%&lt;/td&gt; &lt;td align="left"&gt;95%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;In this demo, we explored whether we could substantially boost a small model's multi-turn performance. We picked a task from the &lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html"&gt;Berkeley function calling leaderboard&lt;/a&gt; - the &lt;a href="https://github.com/ShishirPatil/gorilla/blob/main/berkeley-function-call-leaderboard/bfcl_eval/data/BFCL_v4_multi_turn_base.json"&gt;gorilla file system tool calling task&lt;/a&gt;. Our modifications were:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The original allows multiple tool calls per assistant turn ‚Üí we permit only one&lt;/li&gt; &lt;li&gt;Maximum 5 turns&lt;/li&gt; &lt;li&gt;Commands map to real bash (not gorilla filesystem functions)&lt;/li&gt; &lt;li&gt;Tool call outputs aren't added to the conversation history&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In short, an identical tool set, but simpler &lt;a href="https://github.com/distil-labs/distil-SHELLper/tree/main/data"&gt;train/test data.&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Training Pipeline&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Seed Data:&lt;/strong&gt; We wrote 20 simplified training conversations that span the available tools while remaining reasonably realistic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Synthetic Expansion:&lt;/strong&gt; Through our &lt;a href="https://www.distillabs.ai/blog/small-expert-agents-from-10-examples/?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=shellper"&gt;data synthesis pipeline&lt;/a&gt;, we scaled to thousands of examples.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For handling variable conversation lengths, we split each conversation into subsets ending with a tool call. For instance:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Input] User: List all files =&amp;gt; Model: ls -al =&amp;gt; User: go to directory models [Output] Model: cd models &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;... becomes 2 training points:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Input] User: List all files [Output] Model: ls -al [Input] User: List all files =&amp;gt; Model: ls -al =&amp;gt; User: go to directory models [Output] Model: cd models` &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Fine-tuning:&lt;/strong&gt; We went with &lt;strong&gt;Qwen3-0.6B&lt;/strong&gt; as the &lt;a href="https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning"&gt;best fine-tunable sub-1B&lt;/a&gt; model on our platform that handles tool calling.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Usage Examples&lt;/h1&gt; &lt;p&gt;The assistant takes natural language input, generates bash commands, and optionally runs them (after Y/N confirmation).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Basic filesystem operations&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; python filesystem_demo.py USER: List all files in the current directory COMMAND: ls USER: Create a new directory called test_folder COMMAND: mkdir test_folder USER: Navigate to test_folder COMMAND: cd test_folder &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Limitations and Next Steps&lt;/h1&gt; &lt;p&gt;Right now, we're limited to a basic bash tool set:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;no pipes, compound commands, or multiple tool calls per turn&lt;/li&gt; &lt;li&gt;no validation of invalid commands/parameters&lt;/li&gt; &lt;li&gt;5-turn conversation maximum&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We prioritized getting the simple case right before expanding to more complex scenarios. Up next: support for multiple tool calls (enabling more sophisticated agent workflows) and benchmarking on &lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html"&gt;BFCL&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you're using this in your bash workflows, track failing commands, append them to &lt;code&gt;data/train.jsonl&lt;/code&gt;, and retrain with the updated dataset (or experiment with a larger student model!).&lt;/p&gt; &lt;h1&gt;Discussion&lt;/h1&gt; &lt;p&gt;Interested to hear from the community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is anyone else working on fine-tuning small models for multi-turn tool calling?&lt;/li&gt; &lt;li&gt;What other &amp;quot;narrow but valuable&amp;quot; tasks could benefit from local, privacy-preserving models?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let us know your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gabucz"&gt; /u/gabucz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnjk4p/shellper_qwen3_06b_for_more_reliable_multiturn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnjk4p/shellper_qwen3_06b_for_more_reliable_multiturn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnjk4p/shellper_qwen3_06b_for_more_reliable_multiturn/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T15:44:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnersl</id>
    <title>I built a "Spatial" website for Ollama because I hate linear chats. (Local-first, no DB)</title>
    <updated>2026-01-26T12:31:18+00:00</updated>
    <author>
      <name>/u/yibie</name>
      <uri>https://old.reddit.com/user/yibie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qnersl/i_built_a_spatial_website_for_ollama_because_i/"&gt; &lt;img alt="I built a &amp;quot;Spatial&amp;quot; website for Ollama because I hate linear chats. (Local-first, no DB)" src="https://external-preview.redd.it/YjUzczlrM29ub2ZnMc1v4660SmLmGdaCwdxTOqPLhJLKWmMLYGebzhszHCXS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c84014d04ec9dff478c02a4e070e3416870f2d7" title="I built a &amp;quot;Spatial&amp;quot; website for Ollama because I hate linear chats. (Local-first, no DB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running Llama 3 locally via Ollama for a while, but I kept getting frustrated with the standard &amp;quot;ChatGPT-style&amp;quot; linear interface. My brain doesn't work in a straight line. I'm usually debugging code in one thread, writing docs in another, and brainstorming ideas in a third. In a linear chat, context gets polluted constantly.&lt;/p&gt; &lt;p&gt;So I built a tool called Project Nodal. It's a &amp;quot;Spatial Thinking OS&amp;quot; for your local LLMs.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Infinite Canvas: Drag and drop chat windows (Sticky Notes) anywhere.&lt;/li&gt; &lt;li&gt;Context Isolation: Group backend notes separate from frontend notes.&lt;/li&gt; &lt;li&gt;Forking: This is the big one. Click a message to &amp;quot;fork&amp;quot; it into a new branch/note. Great for &amp;quot;what if&amp;quot; scenarios without ruining the main thread.&lt;/li&gt; &lt;li&gt;100% Local: It uses IndexedDB. No backend database. Connects directly to your Ollama endpoint (or OpenAI if you want).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's open source and I just deployed a demo. &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/yibie/project-nodal"&gt;https://github.com/yibie/project-nodal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://project-nodal-ai.vercel.app/"&gt;https://project-nodal-ai.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;‚ö†Ô∏è A Note on Web Deployment (Vercel/Netlify)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/yibie/project-nodal#%EF%B8%8F-a-note-on-web-deployment-vercelnetlify"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are viewing this demo online (HTTPS), you &lt;strong&gt;cannot&lt;/strong&gt; connect to a local Ollama instance (HTTP) due to browser security policies (Mixed Content Blocking).&lt;/p&gt; &lt;p&gt;To use Local Ollama: Please &lt;a href="https://github.com/yibie/project-nodal"&gt;clone this repo&lt;/a&gt; and run it locally:&lt;/p&gt; &lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/yibie/project-nodal.git"&gt;&lt;code&gt;https://github.com/yibie/project-nodal.git&lt;/code&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;cd project-nodal&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;npm install&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;npm run dev&lt;/code&gt;&lt;/p&gt; &lt;p&gt;To use the Online Demo: Please use an OpenAI or DeepSeek API Key in the settings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yibie"&gt; /u/yibie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/utanig3onofg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnersl/i_built_a_spatial_website_for_ollama_because_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnersl/i_built_a_spatial_website_for_ollama_because_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T12:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoeksi</id>
    <title>Use Ollama to Test Multiple Code Generation Models With Koyeb Sandboxes</title>
    <updated>2026-01-27T13:59:07+00:00</updated>
    <author>
      <name>/u/Plus_Ad7909</name>
      <uri>https://old.reddit.com/user/Plus_Ad7909</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qoeksi/use_ollama_to_test_multiple_code_generation/"&gt; &lt;img alt="Use Ollama to Test Multiple Code Generation Models With Koyeb Sandboxes" src="https://external-preview.redd.it/Xb9rtzGFTk3Zt-MFQ-VopbgY1_WeYce_sBo8AKo3HWs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d78ec5326e405991cefc1a2e47385c4d8e7518bd" title="Use Ollama to Test Multiple Code Generation Models With Koyeb Sandboxes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plus_Ad7909"&gt; /u/Plus_Ad7909 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.koyeb.com/tutorials/use-ollama-to-test-multiple-code-generation-models-with-koyeb-sandboxes"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qoeksi/use_ollama_to_test_multiple_code_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qoeksi/use_ollama_to_test_multiple_code_generation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T13:59:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qodhdc</id>
    <title>Ollama and Kokoro to test TTS on n8n</title>
    <updated>2026-01-27T13:13:20+00:00</updated>
    <author>
      <name>/u/Unique_Winner_5927</name>
      <uri>https://old.reddit.com/user/Unique_Winner_5927</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to hear Qwen3 answer using Kokoro TTS locally.&lt;/p&gt; &lt;p&gt;Everything run locally:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ThomasPlantain/n8n/blob/main/assets/tts/readme.md"&gt;https://github.com/ThomasPlantain/n8n/blob/main/assets/tts/readme.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unique_Winner_5927"&gt; /u/Unique_Winner_5927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qodhdc/ollama_and_kokoro_to_test_tts_on_n8n/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qodhdc/ollama_and_kokoro_to_test_tts_on_n8n/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qodhdc/ollama_and_kokoro_to_test_tts_on_n8n/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T13:13:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qokj2n</id>
    <title>Streaming with Ollama on serverless GPUs seems fundamentally broken . curious how folks think about this</title>
    <updated>2026-01-27T17:34:40+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks , I‚Äôve been seeing a lot of Ollama users trying to run it on ‚Äúserverless‚Äù GPU platforms and hitting the same issue: streaming works great locally, but once Ollama is wrapped behind serverless APIs, streaming disappears.&lt;/p&gt; &lt;p&gt;From what I can tell, this isn‚Äôt really an Ollama limitation. It seems more like an execution-model mismatch. Most serverless GPU platforms treat inference as a batch job (run ‚Üí compute ‚Üí return), which kills long-lived processes and connections ‚Äî exactly what streaming depends on.&lt;/p&gt; &lt;p&gt;Curious how you guys are thinking about this tradeoff. Has anyone found a clean way to preserve token streaming and scale-to-zero semantics, or does this require runtime-level support rather than API tweaks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qokj2n/streaming_with_ollama_on_serverless_gpus_seems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qokj2n/streaming_with_ollama_on_serverless_gpus_seems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qokj2n/streaming_with_ollama_on_serverless_gpus_seems/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T17:34:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoosmm</id>
    <title>Prompt -&gt; Offline Voice AI App in ~11 mins. We forked Expo to bundle native on-device AI runtimes ‚Äî Replit agent builds a fully offline voice assistant</title>
    <updated>2026-01-27T20:01:36+00:00</updated>
    <author>
      <name>/u/thecoder12322</name>
      <uri>https://old.reddit.com/user/thecoder12322</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qoosmm/prompt_offline_voice_ai_app_in_11_mins_we_forked/"&gt; &lt;img alt="Prompt -&amp;gt; Offline Voice AI App in ~11 mins. We forked Expo to bundle native on-device AI runtimes ‚Äî Replit agent builds a fully offline voice assistant" src="https://external-preview.redd.it/I4kx3t6tsAxUZU8Mfm9wDRDqbpUQ0BpsglmKIWPB04Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93eabd046e94e6a2e3e11cdff1a27329133969e5" title="Prompt -&amp;gt; Offline Voice AI App in ~11 mins. We forked Expo to bundle native on-device AI runtimes ‚Äî Replit agent builds a fully offline voice assistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecoder12322"&gt; /u/thecoder12322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h4frf0uk3yfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qoosmm/prompt_offline_voice_ai_app_in_11_mins_we_forked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qoosmm/prompt_offline_voice_ai_app_in_11_mins_we_forked/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T20:01:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qochnc</id>
    <title>Do ollama models access the internet?</title>
    <updated>2026-01-27T12:28:07+00:00</updated>
    <author>
      <name>/u/Fancy_Purchase_9400</name>
      <uri>https://old.reddit.com/user/Fancy_Purchase_9400</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am new to ollama and I just wanted to know if the models downloaded locally (like mistral:7b) access the internet at all while using them (even if it is for maintaing any kind of logs). I have noticed a small spike in network usage (both for upload and download) while using the model, but I'm not sure if it is due to the usage of local ollama model, so I'm curious to know if it actually accesses the internet quietly. If so, how do I restrict it completely from accessing the internet? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fancy_Purchase_9400"&gt; /u/Fancy_Purchase_9400 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qochnc/do_ollama_models_access_the_internet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qochnc/do_ollama_models_access_the_internet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qochnc/do_ollama_models_access_the_internet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T12:28:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoi1op</id>
    <title>Tiny Ollama-powered CLI: Natural Language to shell commands (qwen2.5-coder default, easy model switch)</title>
    <updated>2026-01-27T16:08:37+00:00</updated>
    <author>
      <name>/u/ykushch</name>
      <uri>https://old.reddit.com/user/ykushch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/"&gt; &lt;img alt="Tiny Ollama-powered CLI: Natural Language to shell commands (qwen2.5-coder default, easy model switch)" src="https://external-preview.redd.it/JnNJEcF3jbp7PevNxANU0riqBFifG0zNxzy_XGtEtCw.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=3d94c3882da0b59d8f2197ce3c151ee720e44138" title="Tiny Ollama-powered CLI: Natural Language to shell commands (qwen2.5-coder default, easy model switch)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CLI that uses Ollama locally to translate natural language into shell commands. Supports &lt;code&gt;--model&lt;/code&gt; / &lt;code&gt;ASK_MODEL&lt;/code&gt; and &lt;code&gt;OLLAMA_HOST&lt;/code&gt;.&lt;br /&gt; Repo: &lt;a href="https://github.com/ykushch/ask"&gt;https://github.com/ykushch/ask&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/k5ldp45d1xfg1.gif"&gt;ask - natural language to shell commands&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ykushch"&gt; /u/ykushch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T16:08:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo9tsi</id>
    <title>NotebookLM For Teams</title>
    <updated>2026-01-27T10:04:14+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qo9tsi/notebooklm_for_teams/"&gt; &lt;img alt="NotebookLM For Teams" src="https://external-preview.redd.it/dmgyMjVieGg4dmZnMSLy8o5ur8LGz7971UKmCZkldIebkAvR30ypzPlMaeND.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0496edb208cda2f95e1b3739aef3d3d21e3a42e3" title="NotebookLM For Teams" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.&lt;/p&gt; &lt;p&gt;In short, it is NotebookLM for teams, as it connects any LLM to your internal knowledge sources (search engines, Drive, Calendar, Notion, Obsidian, and 15+ other connectors) and lets you chat with it in real time alongside your team.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here's a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Self-Hostable (with docker support)&lt;/li&gt; &lt;li&gt;Real Time Collaborative Chats&lt;/li&gt; &lt;li&gt;Real Time Commenting&lt;/li&gt; &lt;li&gt;Deep Agentic Agent&lt;/li&gt; &lt;li&gt;RBAC (Role Based Access for Teams Members)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs (OpenAI spec with LiteLLM)&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Local TTS/STT support.&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Slide Creation Support&lt;/li&gt; &lt;li&gt;Multilingual Podcast Support&lt;/li&gt; &lt;li&gt;Video Creation Agent&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zxqevbwh8vfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qo9tsi/notebooklm_for_teams/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qo9tsi/notebooklm_for_teams/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T10:04:14+00:00</published>
  </entry>
</feed>
