<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-16T08:10:00+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1q9bdj0</id>
    <title>STT and TTS compatible with ROCm</title>
    <updated>2026-01-10T18:22:15+00:00</updated>
    <author>
      <name>/u/EnvironmentalToe3130</name>
      <uri>https://old.reddit.com/user/EnvironmentalToe3130</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnvironmentalToe3130"&gt; /u/EnvironmentalToe3130 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1q9bd1v/stt_and_tts_compatible_with_rocm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9bdj0/stt_and_tts_compatible_with_rocm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q9bdj0/stt_and_tts_compatible_with_rocm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T18:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q99hsz</id>
    <title>Nvidia Quadro P400 2GB GDDR5 card good enough?</title>
    <updated>2026-01-10T17:09:37+00:00</updated>
    <author>
      <name>/u/Curious_Party_4683</name>
      <uri>https://old.reddit.com/user/Curious_Party_4683</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q99hsz/nvidia_quadro_p400_2gb_gddr5_card_good_enough/"&gt; &lt;img alt="Nvidia Quadro P400 2GB GDDR5 card good enough?" src="https://b.thumbs.redditmedia.com/Ir0lGRSqX5OLg3aYUy2LQyL0XiK7SGfDKqBhqb0hngM.jpg" title="Nvidia Quadro P400 2GB GDDR5 card good enough?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;qwen3-vl:8b refuses to work on my i7, 7th gen, windows machine. &lt;/p&gt; &lt;p&gt;will this cheap nvidia work? or what's the bare minimum card?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lj263g5g0kcg1.png?width=1639&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ba459647259dba77ccefd933da88fdfc646e3fa"&gt;https://preview.redd.it/lj263g5g0kcg1.png?width=1639&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ba459647259dba77ccefd933da88fdfc646e3fa&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Curious_Party_4683"&gt; /u/Curious_Party_4683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q99hsz/nvidia_quadro_p400_2gb_gddr5_card_good_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q99hsz/nvidia_quadro_p400_2gb_gddr5_card_good_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q99hsz/nvidia_quadro_p400_2gb_gddr5_card_good_enough/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T17:09:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9jdo8</id>
    <title>Is it possible to see AI Request and Response in Realtime on Llama</title>
    <updated>2026-01-10T23:36:56+00:00</updated>
    <author>
      <name>/u/Professional-Fun7765</name>
      <uri>https://old.reddit.com/user/Professional-Fun7765</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone&lt;/p&gt; &lt;p&gt;I am new in the world of Ollama so pardon me if this may sound like a stupid question. I am transitioning from GPT4All where when I make a request via API I can see in real time On the desktop app (on the server chat tab) The incoming request, the model thinking and the model generating a response. This was so great in debugging but GPT4All was slow for me so a colleague suggested Ollama and I can see much improvements in speed. I am currently integrating a Laravel App with Ollama and sending various request to the model and I wish I can be able to see the request and response in real time in Ollama just like I did in GPT4All Desktop App, so my question is whether or not this is possible? If it is then how can I go about configuring it?&lt;/p&gt; &lt;p&gt;if it helps I am on Windows and this is for my local development.&lt;/p&gt; &lt;p&gt;Thank you in advance, your input will be highly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Fun7765"&gt; /u/Professional-Fun7765 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9jdo8/is_it_possible_to_see_ai_request_and_response_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9jdo8/is_it_possible_to_see_ai_request_and_response_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q9jdo8/is_it_possible_to_see_ai_request_and_response_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T23:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9ff6k</id>
    <title>what AI models can I run locally on my PC with Ollama?</title>
    <updated>2026-01-10T20:57:13+00:00</updated>
    <author>
      <name>/u/Kitchen-Patience8176</name>
      <uri>https://old.reddit.com/user/Kitchen-Patience8176</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I‚Äôm pretty new to local AI and still learning, so sorry if this is a basic question.&lt;/p&gt; &lt;p&gt;I can‚Äôt afford a ChatGPT subscription anymore due to financial reasons, so I‚Äôm trying to use &lt;strong&gt;local models&lt;/strong&gt; instead. I‚Äôve installed &lt;strong&gt;Ollama&lt;/strong&gt;, and it works, but I don‚Äôt really know which models I should be using or what my PC can realistically handle.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ryzen 9 5900X&lt;/li&gt; &lt;li&gt;RTX 3080 (10GB VRAM)&lt;/li&gt; &lt;li&gt;32GB RAM&lt;/li&gt; &lt;li&gt;2TB NVMe SSD&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm mainly curious about:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Which models run well on this setup&lt;/li&gt; &lt;li&gt;What I &lt;em&gt;can‚Äôt&lt;/em&gt; run&lt;/li&gt; &lt;li&gt;How close local models can get to ChatGPT&lt;/li&gt; &lt;li&gt;If things like web search, fact-checking, or up-to-date info are possible locally (or any workarounds)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any beginner advice or model recommendations would really help.&lt;/p&gt; &lt;p&gt;Thanks üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kitchen-Patience8176"&gt; /u/Kitchen-Patience8176 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9ff6k/what_ai_models_can_i_run_locally_on_my_pc_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9ff6k/what_ai_models_can_i_run_locally_on_my_pc_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q9ff6k/what_ai_models_can_i_run_locally_on_my_pc_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T20:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q95z91</id>
    <title>Built a Local Research Agent with Ollama - No API Keys, Just Citations</title>
    <updated>2026-01-10T14:51:06+00:00</updated>
    <author>
      <name>/u/Xthebuilder</name>
      <uri>https://old.reddit.com/user/Xthebuilder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q95z91/built_a_local_research_agent_with_ollama_no_api/"&gt; &lt;img alt="Built a Local Research Agent with Ollama - No API Keys, Just Citations" src="https://b.thumbs.redditmedia.com/f-UABymhEJedVxkN3owagmqQGsW0QyXoIv6GlbP6OBc.jpg" title="Built a Local Research Agent with Ollama - No API Keys, Just Citations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a research agent that runs entirely locally using Ollama. Give it a topic, get back a markdown report with proper citations. Simple as that.&lt;/p&gt; &lt;p&gt;What It Does&lt;/p&gt; &lt;p&gt;The agent handles the full research workflow:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô Gathers sources asynchronously ‚àô Uses semantic embeddings to filter for relevance ‚àô Generates structured reports with citations ‚àô Everything stays on your machine &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Why I Built This&lt;/p&gt; &lt;p&gt;I wanted deep research capabilities without depending on cloud services or burning through API credits. With Ollama making local LLMs practical, it seemed like the obvious foundation.&lt;/p&gt; &lt;p&gt;How It Works&lt;/p&gt; &lt;p&gt;python research_agent.py &amp;quot;quantum computing applications&amp;quot;&lt;/p&gt; &lt;p&gt;The agent:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. Pulls sources from DuckDuckGo 2. Extracts and evaluates content using sentence-transformers 3. Runs quality checks on similarity scores 4. Generates a markdown report with references &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;All processing happens locally. No external APIs.&lt;/p&gt; &lt;p&gt;Design Choices (Explicit By Design)&lt;/p&gt; &lt;p&gt;Local-first: Works with any Ollama model - llama2, mistral, whatever you have running&lt;/p&gt; &lt;p&gt;Quality thresholds: Configurable similarity scores ensure sources are actually relevant&lt;/p&gt; &lt;p&gt;Async operations: Fast source gathering without blocking&lt;/p&gt; &lt;p&gt;Structured output: Clean markdown reports you can actually use&lt;/p&gt; &lt;p&gt;Tradeoffs&lt;/p&gt; &lt;p&gt;I optimized for:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô Privacy and offline workflows ‚àô Explicit configuration over automation ‚àô Simple setup (just Python + Ollama) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This means it‚Äôs not:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô A cloud-scale solution ‚àô Zero-configuration ‚àô Designed for multi-source integrations (yet) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What‚Äôs Next&lt;/p&gt; &lt;p&gt;Considering:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô PDF source support improvements ‚àô Local caching to avoid re-fetching ‚àô Better semantic chunking for long sources &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Code‚Äôs on GitHub: &lt;a href="https://github.com/Xthebuilder/Research_Agent"&gt;https://github.com/Xthebuilder/Research_Agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xthebuilder"&gt; /u/Xthebuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q95z91"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q95z91/built_a_local_research_agent_with_ollama_no_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q95z91/built_a_local_research_agent_with_ollama_no_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T14:51:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qansvc</id>
    <title>9 requests per hour - Seriously?</title>
    <updated>2026-01-12T06:40:12+00:00</updated>
    <author>
      <name>/u/MadeUpName94</name>
      <uri>https://old.reddit.com/user/MadeUpName94</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying ollama on the free plan with a cloud LLM and the more i use it the less i can.&lt;/p&gt; &lt;p&gt;9 requests and I got the &amp;quot;too many request per hour, give us money* - fucking 9?&lt;/p&gt; &lt;p&gt;I hadn't used it for several hours.&lt;/p&gt; &lt;p&gt;My &amp;quot;requests per hour&amp;quot; gets smaller and smaller every time use it :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadeUpName94"&gt; /u/MadeUpName94 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qansvc/9_requests_per_hour_seriously/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qansvc/9_requests_per_hour_seriously/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qansvc/9_requests_per_hour_seriously/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-12T06:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9svp5</id>
    <title>Looking for open source contributers | LocalAgent</title>
    <updated>2026-01-11T07:11:25+00:00</updated>
    <author>
      <name>/u/FriendshipCreepy8045</name>
      <uri>https://old.reddit.com/user/FriendshipCreepy8045</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;br /&gt; Hope you're all doing well.&lt;/p&gt; &lt;p&gt;So little background: I'm a frontend/performance engineer working as an IT consultant for the past year or so.&lt;br /&gt; Recently made a goal to learn and code more in python and basically entering the field of AI Applied engineering.&lt;br /&gt; I'm still learning concepts but with a little knowledge and claude, I made a researcher assistent that runs entirly on laptop(if you have a descent one using Ollama) or just use the default cloud.&lt;/p&gt; &lt;p&gt;I understand langchain quite a bit and might be worth checking out langraph to somehow migrate it into more controlled research assistent(controlling tools,tokens used etc.).&lt;br /&gt; So I need your help, I would really appretiate if you guys go ahead and check &amp;quot;&lt;a href="https://github.com/vedas-dixit/LocalAgent"&gt;https://github.com/vedas-dixit/LocalAgent&lt;/a&gt;&amp;quot; and let me know:&lt;/p&gt; &lt;p&gt;Your thoughts | Potential Improvements | Guidance *what i did right/wrong&lt;/p&gt; &lt;p&gt;or if i may ask, just some meaningful contribution to the project if you have time ;).&lt;/p&gt; &lt;p&gt;I posted about this like idk a month ago and got 100+ stars in a week so might have some potential but idk.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FriendshipCreepy8045"&gt; /u/FriendshipCreepy8045 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9svp5/looking_for_open_source_contributers_localagent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9svp5/looking_for_open_source_contributers_localagent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q9svp5/looking_for_open_source_contributers_localagent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-11T07:11:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qabwuw</id>
    <title>Docker ollama running on windows using system RAM, despite using VRAM and having plenty more available.</title>
    <updated>2026-01-11T21:39:05+00:00</updated>
    <author>
      <name>/u/Fit_Code_2107</name>
      <uri>https://old.reddit.com/user/Fit_Code_2107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Edit: Found the solution to this problem (tldr; WSL2 sucks), updated the post with the solution at the bottom of the post&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm trying to run ollama on docker (windows), and it looks like there's some memory double dipping going on and I'm not sure why. I'm trying to run a 20GB model on a 5090, I'm seeing BOTH my system and VRAM memory go up as much when I load the model.&lt;/p&gt; &lt;p&gt;System settings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;64 GB of RAM&lt;/li&gt; &lt;li&gt;RTX 5090 (32 GB of VRAM)&lt;/li&gt; &lt;li&gt;Model: olmo-3.1:32b-think (takes ~20Gb of RAM to load)&lt;/li&gt; &lt;li&gt;Docker version 29.1.3, build f52814d (running on WSL2)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;fwiw, &lt;code&gt;ollama ps&lt;/code&gt; does show the model loaded 100% on my GPU. Ran &lt;code&gt;nvidia-smi&lt;/code&gt; in the ollama container, and it looks &lt;em&gt;fine&lt;/em&gt; (I can see the ollama process running). While Windows task manager isn't able to pin down what process is responsible for the high gpu util, it does reflect memory utilization accurately. So I am using my GPU, I have plenty more VRAM to work with, so I'm not at all sure why system memory util spikes up 20GB during use.&lt;/p&gt; &lt;p&gt;I installed the windows native version of ollama to see if I could replicate, and I do not see my system memory spike using that approach. So it seems like the involvement of docker here is introducing some funk.&lt;/p&gt; &lt;p&gt;I've read through some similar posts here and saw there were issues a few years ago with docker on WSL2 and utilizing VRAM, but those issues seem to have since been resolved so hitting a dead end here. Wondering if anyone has had the same issue and has any tips?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;h1&gt;Solution/workaround&lt;/h1&gt; &lt;p&gt;Found the cause of the issue, it's a known &lt;a href="https://github.com/microsoft/WSL/issues/4166"&gt;WSL issue&lt;/a&gt; (that's been open for 7 years...). Apparently WSL doesn't do a great job with memory management and sometimes never releases the memory used back to the system (I say sometimes but for me it's ALL the time). In this case with ollama, never releases the RAM it uses to load the model to VRAM.&lt;/p&gt; &lt;p&gt;You can manually confirm this, and release the memory with the following commands (s/o to Kirk, a random commenter on this &lt;a href="https://medium.com/@Tanzim/how-to-run-ollama-in-windows-via-wsl-8ace765cee12"&gt;medium article I found&lt;/a&gt;)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Display memory usage free -h sudo su &amp;lt;&amp;lt;EOF # force memory write back to disk sync # clear cache from memory echo 3 &amp;gt; /proc/sys/vm/drop_caches EOF # display memory again (should be cleared) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also, looks like someone has a more automated solution out here too (haven't tried this so can't vouch but looks promising) : &lt;a href="https://github.com/arkane-systems/wsl-drop-cache?tab=readme-ov-file"&gt;https://github.com/arkane-systems/wsl-drop-cache?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: I am only sharing what I found.&lt;/p&gt; &lt;p&gt;While these are solutions, they may not be the best or most appropriate solution if you're doing anything &lt;em&gt;real&lt;/em&gt;. atm I'm just experimenting with this setup for personal reasons so it works for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit_Code_2107"&gt; /u/Fit_Code_2107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qabwuw/docker_ollama_running_on_windows_using_system_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qabwuw/docker_ollama_running_on_windows_using_system_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qabwuw/docker_ollama_running_on_windows_using_system_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-11T21:39:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qar8dn</id>
    <title>Which model to translate my blog posts, without sounding AI?</title>
    <updated>2026-01-12T10:15:47+00:00</updated>
    <author>
      <name>/u/Signal_Pin_3277</name>
      <uri>https://old.reddit.com/user/Signal_Pin_3277</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been writing blog posts in english, but I would like to translate them to french. I know I can just throw them at GPT, but it just ruins the tone, the sentences are very weird, I don't want a literal translation of the words, rather a natural translation with maybe french expressions.&lt;/p&gt; &lt;p&gt;I wonder, is there any model I could use? I tried Deepseek, GPT.&lt;/p&gt; &lt;p&gt;I don't mind a local model too, I have a 16 GB rtx 5060. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Signal_Pin_3277"&gt; /u/Signal_Pin_3277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qar8dn/which_model_to_translate_my_blog_posts_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qar8dn/which_model_to_translate_my_blog_posts_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qar8dn/which_model_to_translate_my_blog_posts_without/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-12T10:15:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb46gv</id>
    <title>How to Evaluate AI Agents? (Part 2)</title>
    <updated>2026-01-12T19:14:13+00:00</updated>
    <author>
      <name>/u/Ok_Constant_9886</name>
      <uri>https://old.reddit.com/user/Ok_Constant_9886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qb46gv/how_to_evaluate_ai_agents_part_2/"&gt; &lt;img alt="How to Evaluate AI Agents? (Part 2)" src="https://b.thumbs.redditmedia.com/vqEYTMRHAAFPd_bbFtc_RJ6KQ8SiOUccMhIa1YRo8Kg.jpg" title="How to Evaluate AI Agents? (Part 2)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Constant_9886"&gt; /u/Ok_Constant_9886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AIEval/comments/1qb43wg/how_to_evaluate_ai_agents_part_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qb46gv/how_to_evaluate_ai_agents_part_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qb46gv/how_to_evaluate_ai_agents_part_2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-12T19:14:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb06tm</id>
    <title>Docker on Linux or Nah?</title>
    <updated>2026-01-12T16:53:05+00:00</updated>
    <author>
      <name>/u/Honest-Cheesecake275</name>
      <uri>https://old.reddit.com/user/Honest-Cheesecake275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My ADHD impulses got the better of me and I jumped the gun and installed Ollama locally. Then installed the Docker container then saw that there is a Docker container that streamlines setup of WebUI. &lt;/p&gt; &lt;p&gt;What‚Äôs the most idiot proof way to set this up?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Cheesecake275"&gt; /u/Honest-Cheesecake275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qb06tm/docker_on_linux_or_nah/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qb06tm/docker_on_linux_or_nah/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qb06tm/docker_on_linux_or_nah/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-12T16:53:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbm9f8</id>
    <title>Introducing T.H.U.V.U, an open source coding agent for local and cloud LLMs</title>
    <updated>2026-01-13T08:43:15+00:00</updated>
    <author>
      <name>/u/Strange-Flamingo-248</name>
      <uri>https://old.reddit.com/user/Strange-Flamingo-248</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Strange-Flamingo-248"&gt; /u/Strange-Flamingo-248 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/CodingAgents/comments/1qbllft/introducing_thuvu_an_open_source_coding_agent_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbm9f8/introducing_thuvu_an_open_source_coding_agent_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qbm9f8/introducing_thuvu_an_open_source_coding_agent_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-13T08:43:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb4jaq</id>
    <title>Chat With Your Favorite GitHub Repositories via CLI with the new RAGLight Feature</title>
    <updated>2026-01-12T19:27:02+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qb4jaq/chat_with_your_favorite_github_repositories_via/"&gt; &lt;img alt="Chat With Your Favorite GitHub Repositories via CLI with the new RAGLight Feature" src="https://external-preview.redd.it/emxuM2h3d3R5eWNnMVATT2HSzAMyFYCPqt4__cDa5qVULAiZlHb92ktmmK-c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7893a18ffbbe36345ddb3b5ea5e3be2d2ad9c1a7" title="Chat With Your Favorite GitHub Repositories via CLI with the new RAGLight Feature" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve just pushed a new feature to &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;RAGLight&lt;/a&gt;: you can now &lt;strong&gt;chat directly with your favorite GitHub repositories from the CLI&lt;/strong&gt; using your favorite models.&lt;/p&gt; &lt;p&gt;No setup nightmare, no complex infra, just point to one or several GitHub repos, let RAGLight ingest them, and start asking questions !&lt;/p&gt; &lt;p&gt;In the demo I used an &lt;strong&gt;Ollama&lt;/strong&gt; embedding model and an &lt;strong&gt;OpenAI&lt;/strong&gt; LLM, let's try it with your favorite model provider üöÄ&lt;/p&gt; &lt;p&gt;You can also use &lt;strong&gt;RAGLight&lt;/strong&gt; in your codebase if you want to setup easily a RAG.&lt;/p&gt; &lt;p&gt;Github repository : &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;https://github.com/Bessouat40/RAGLight&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2lu95uwtyycg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qb4jaq/chat_with_your_favorite_github_repositories_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qb4jaq/chat_with_your_favorite_github_repositories_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-12T19:27:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbk8l4</id>
    <title>Privacy-First Voice Assistant with AI web-enabled search</title>
    <updated>2026-01-13T06:37:21+00:00</updated>
    <author>
      <name>/u/dsept</name>
      <uri>https://old.reddit.com/user/dsept</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qbk8l4/privacyfirst_voice_assistant_with_ai_webenabled/"&gt; &lt;img alt="Privacy-First Voice Assistant with AI web-enabled search" src="https://preview.redd.it/gt9geaosa2dg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3decc717b0d67d9619de634dc5e356db36c40897" title="Privacy-First Voice Assistant with AI web-enabled search" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dsept"&gt; /u/dsept &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gt9geaosa2dg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbk8l4/privacyfirst_voice_assistant_with_ai_webenabled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qbk8l4/privacyfirst_voice_assistant_with_ai_webenabled/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-13T06:37:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc42lj</id>
    <title>M.2 to 4x Pcie for extra GPU Power Question</title>
    <updated>2026-01-13T21:33:08+00:00</updated>
    <author>
      <name>/u/Far_Gur_3974</name>
      <uri>https://old.reddit.com/user/Far_Gur_3974</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Gur_3974"&gt; /u/Far_Gur_3974 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1qc3r7x/m2_to_4x_pcie_for_extra_gpu_power_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qc42lj/m2_to_4x_pcie_for_extra_gpu_power_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qc42lj/m2_to_4x_pcie_for_extra_gpu_power_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-13T21:33:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbwnjf</id>
    <title>Seeking Advice: Deploying Local LLMs for a Large-Scale Food &amp; Goods Distributor</title>
    <updated>2026-01-13T17:04:14+00:00</updated>
    <author>
      <name>/u/JPedrroo</name>
      <uri>https://old.reddit.com/user/JPedrroo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I‚Äôm a Software Analyst and Developer for a major distribution company in the state of Bahia, Brazil. We handle a massive operation ranging from food and beverages to cosmetics and hygiene products, serving basically the entire state in terms of city coverage.&lt;/p&gt; &lt;p&gt;I am currently exploring the possibility of implementing a local AI infrastructure to enhance productivity while maintaining strict privacy over our data. I am not an expert in AI, so I am still figuring out the best way to start. I have tested some local LLMs on my laptop, but I am unfamiliar with the technical nuances involved in a large-scale corporate implementation.&lt;/p&gt; &lt;p&gt;Initially, I thought of developing a system that reads database entries regarding expiry dates and turnover rates in our warehouse. The goal would be to automatically recommend flash promotions or stock transfers to our retail branches before products expire.&lt;/p&gt; &lt;p&gt;I'm seeking any feedback on this‚Äîpast experiences, technical advice, additional use case ideas, or anything relevant. Thank you all for your time and for any insights you can share!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JPedrroo"&gt; /u/JPedrroo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbwnjf/seeking_advice_deploying_local_llms_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbwnjf/seeking_advice_deploying_local_llms_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qbwnjf/seeking_advice_deploying_local_llms_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-13T17:04:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcmwsr</id>
    <title>New Ollama Desktop Client</title>
    <updated>2026-01-14T13:12:40+00:00</updated>
    <author>
      <name>/u/Odd-Feature-645</name>
      <uri>https://old.reddit.com/user/Odd-Feature-645</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qcmwsr/new_ollama_desktop_client/"&gt; &lt;img alt="New Ollama Desktop Client" src="https://b.thumbs.redditmedia.com/OdW8FFrF846eY3ZikgZHdl99hund16GW1AOkssAFKNU.jpg" title="New Ollama Desktop Client" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/barni007-pro/ollama_desktop_client"&gt;GitHub Ollama Desktop&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d8ikcl97ebdg1.png?width=706&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a1ca574611ed0ad2925680166b0a51fe62e7401"&gt;https://preview.redd.it/d8ikcl97ebdg1.png?width=706&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a1ca574611ed0ad2925680166b0a51fe62e7401&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Hi everyone! I wanted to create a more powerful, native desktop experience for Ollama. Most clients are just simple chat wrappers, so I built &amp;quot;Ollama Desktop&amp;quot; in VB.NET 8 with a focus on advanced tools. GitHub: https://github.com/barni007-pro/ollamGitHubGitHuba_desktop_client üöÄ High-Impact Features: üß† Local RAG Tool: Chat with your large PDF and Text documents using local knowledge extraction. üëÅÔ∏è Vision Support: Upload images or take screenshots directly to analyze them with multimodal models like gemma3. üíª Code Interpreter: The model can execute Python, PowerShell, or Batch scripts locally. Great for task automation! üìÑ Document Context: Easily import .pdf, .txt, or .json files directly into the chat context. üß™ JSON Mode &amp;amp; Tools: Support for structured responses and function calling. üìê LaTeX Support: Beautiful rendering of mathematical formulas. üõ†Ô∏è Under the Hood: Built with .NET 8 and VB.NET. Fast, lightweight, and specifically designed for Windows. Model switching on-the-fly during conversations. I‚Äôm looking for feedback and would love to hear which features you‚Äôd like to see next! &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Feature-645"&gt; /u/Odd-Feature-645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qcmwsr/new_ollama_desktop_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qcmwsr/new_ollama_desktop_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qcmwsr/new_ollama_desktop_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-14T13:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbrx4b</id>
    <title>Open Source Enterprise Search Engine (Generative AI Powered)</title>
    <updated>2026-01-13T13:55:10+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I‚Äôm excited to share something we‚Äôve been building for the past 6 months, a &lt;strong&gt;fully open-source Enterprise Search Platform&lt;/strong&gt; designed to bring powerful Enterprise Search to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, Local file uploads and more. You can deploy it and run it with just one docker compose command.&lt;/p&gt; &lt;p&gt;You can run the full platform locally. Recently, one of our users tried &lt;strong&gt;qwen3-vl:8b (16 FP)&lt;/strong&gt; with &lt;strong&gt;Ollama&lt;/strong&gt; and got very good results.&lt;/p&gt; &lt;p&gt;The entire system is built on a &lt;strong&gt;fully event-streaming architecture powered by Kafka&lt;/strong&gt;, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data.&lt;/p&gt; &lt;p&gt;At the core, the system uses an &lt;strong&gt;Agentic Graph RAG approach&lt;/strong&gt;, where retrieval is guided by an enterprise knowledge graph and reasoning agents. Instead of treating documents as flat text, agents reason over relationships between users, teams, entities, documents, and permissions, allowing more accurate, explainable, and permission-aware answers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep understanding of documents, user, organization and teams with enterprise knowledge graph&lt;/li&gt; &lt;li&gt;Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama&lt;/li&gt; &lt;li&gt;Use any provider that supports OpenAI compatible endpoints&lt;/li&gt; &lt;li&gt;Choose from 1,000+ embedding models&lt;/li&gt; &lt;li&gt;Visual Citations for every answer&lt;/li&gt; &lt;li&gt;Vision-Language Models and OCR for visual or scanned docs&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, or SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs for developers&lt;/li&gt; &lt;li&gt;All major file types support including pdfs with images, diagrams and charts&lt;/li&gt; &lt;li&gt;Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more&lt;/li&gt; &lt;li&gt;Reasoning Agent that plans before executing tasks&lt;/li&gt; &lt;li&gt;40+ Connectors allowing you to connect to your entire business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo Video:&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=xA9m3pwOgz8"&gt;https://www.youtube.com/watch?v=xA9m3pwOgz8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbrx4b/open_source_enterprise_search_engine_generative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbrx4b/open_source_enterprise_search_engine_generative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qbrx4b/open_source_enterprise_search_engine_generative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-13T13:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcnpc8</id>
    <title>Created my own Agent interface for Nemotron-3</title>
    <updated>2026-01-14T13:47:42+00:00</updated>
    <author>
      <name>/u/jovn1234567890</name>
      <uri>https://old.reddit.com/user/jovn1234567890</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jovn1234567890"&gt; /u/jovn1234567890 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://github.com/jacobbw/Waterfall"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qcnpc8/created_my_own_agent_interface_for_nemotron3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qcnpc8/created_my_own_agent_interface_for_nemotron3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-14T13:47:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcu22b</id>
    <title>help please</title>
    <updated>2026-01-14T17:49:51+00:00</updated>
    <author>
      <name>/u/Real_Macaron_1880</name>
      <uri>https://old.reddit.com/user/Real_Macaron_1880</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm new to local AI. I want to set it up focused on analyzing PDF documents, legal documents, judgments... Could someone advise me? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Real_Macaron_1880"&gt; /u/Real_Macaron_1880 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qcu22b/help_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qcu22b/help_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qcu22b/help_please/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-14T17:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcsn8m</id>
    <title>We tried to automate product labeling in one prompt. It failed. 27 steps later, we've processed 10,000+ products.</title>
    <updated>2026-01-14T16:58:42+00:00</updated>
    <author>
      <name>/u/No-Reindeer-9968</name>
      <uri>https://old.reddit.com/user/No-Reindeer-9968</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Reindeer-9968"&gt; /u/No-Reindeer-9968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qcsn8m/we_tried_to_automate_product_labeling_in_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qcsn8m/we_tried_to_automate_product_labeling_in_one/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-14T16:58:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qd7dmx</id>
    <title>Hey all- I built a self-hosted MCP server to run AI semantic search over your own databases, files, and codebases. Supports Ollama and cloud providers if you want. Thought you all might find a good use for it.</title>
    <updated>2026-01-15T02:33:30+00:00</updated>
    <author>
      <name>/u/mattv8</name>
      <uri>https://old.reddit.com/user/mattv8</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mattv8"&gt; /u/mattv8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/selfhosted/comments/1qbv3fm/ragtime_a_selfhosted_mcp_server_to_run_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qd7dmx/hey_all_i_built_a_selfhosted_mcp_server_to_run_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qd7dmx/hey_all_i_built_a_selfhosted_mcp_server_to_run_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-15T02:33:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdjdqq</id>
    <title>Persistent "STATUS_ACCESS_VIOLATION" and Server Errors in Ollama UI ‚Äì Help needed!</title>
    <updated>2026-01-15T13:23:12+00:00</updated>
    <author>
      <name>/u/AlexHardy08</name>
      <uri>https://old.reddit.com/user/AlexHardy08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qdjdqq/persistent_status_access_violation_and_server/"&gt; &lt;img alt="Persistent &amp;quot;STATUS_ACCESS_VIOLATION&amp;quot; and Server Errors in Ollama UI ‚Äì Help needed!" src="https://a.thumbs.redditmedia.com/ZjHCNfIpvQ9jOw_q3GtOhl7JX96v9UqUGryE9LJsu98.jpg" title="Persistent &amp;quot;STATUS_ACCESS_VIOLATION&amp;quot; and Server Errors in Ollama UI ‚Äì Help needed!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gek41ylzjidg1.png?width=678&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=225df9e46ba2874be86e011c39b4881188ab02a4"&gt;https://preview.redd.it/gek41ylzjidg1.png?width=678&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=225df9e46ba2874be86e011c39b4881188ab02a4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been running into a frustrating issue with my Ollama UI setup for about two weeks now, and I‚Äôm wondering if anyone else is experiencing the same or if the devs are aware of it.&lt;/p&gt; &lt;p&gt;I keep getting the browser error &lt;strong&gt;&amp;quot;STATUS_ACCESS_VIOLATION&amp;quot;&lt;/strong&gt; (as seen in the attached screenshot). It happens quite frequently in some chat sessions, while others work fine for a while. Sometimes, it's accompanied by a generic &amp;quot;server error&amp;quot; message.&lt;/p&gt; &lt;p&gt;The biggest issue is that whenever this happens, the text generation stops immediately. If I‚Äôm working on something important or a long prompt, I have to refresh and start the generation all over again.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A few details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This started happening about 2 weeks ago.&lt;/li&gt; &lt;li&gt;It seems to happen randomly but frequently enough to disrupt the workflow.&lt;/li&gt; &lt;li&gt;I've tried refreshing, but the problem eventually comes back.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Does anyone know what exactly causes this? Is it a memory management issue, or something related to how the UI communicates with the Ollama backend?&lt;/p&gt; &lt;p&gt;If anyone has a fix or a workaround (browser settings, update versions, etc.), please let me know. Hopefully, the Ollama/UI team can look into this!&lt;/p&gt; &lt;p&gt;I use latest version of ollama&lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexHardy08"&gt; /u/AlexHardy08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qdjdqq/persistent_status_access_violation_and_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qdjdqq/persistent_status_access_violation_and_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qdjdqq/persistent_status_access_violation_and_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-15T13:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qd4snz</id>
    <title>Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )</title>
    <updated>2026-01-15T00:39:37+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qd4snz/building_opensource_client_sided_code/"&gt; &lt;img alt="Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )" src="https://external-preview.redd.it/dWh2ODN1ZnRzZWRnMege6VYazrCNvPvrU2GG8tcd-8T7OQo9iRCGUYxRaIOc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cf0b9c7665aefc3f8dce5125368451b88544d40" title="Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, guys, I m building GitNexus, an opensource Code Intelligence Engine which works fully client sided in-browser. Think of DeepWiki but with understanding of codebase relations like IMPORTS - CALLS -DEFINES -IMPLEMENTS- EXTENDS relations.&lt;/p&gt; &lt;p&gt;What all features would be useful, any integrations, cool ideas, etc?&lt;/p&gt; &lt;p&gt;site: &lt;a href="https://gitnexus.vercel.app/"&gt;https://gitnexus.vercel.app/&lt;/a&gt;&lt;br /&gt; repo: &lt;a href="https://github.com/abhigyanpatwari/GitNexus"&gt;https://github.com/abhigyanpatwari/GitNexus&lt;/a&gt; (A ‚≠ê might help me convince my CTO to allot little time for this :-) )&lt;/p&gt; &lt;p&gt;Everything including the DB engine, embeddings model etc works inside your browser.&lt;/p&gt; &lt;p&gt;It combines Graph query capabilities with standard code context tools like semantic search, BM 25 index, etc. Due to graph it should be able to perform Blast radius detection of code changes, codebase audit etc reliably.&lt;/p&gt; &lt;p&gt;Working on exposing the browser tab through MCP so claude code / cursor, etc can use it for codebase audits, deep context of code connections etc preventing it from making breaking changes due to missed upstream and downstream dependencies.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/clkvriftsedg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qd4snz/building_opensource_client_sided_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qd4snz/building_opensource_client_sided_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-15T00:39:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdvy1k</id>
    <title>Open Notebook 1.5 - Introducing i18n Support (we speak Chinese now) :)</title>
    <updated>2026-01-15T21:11:17+00:00</updated>
    <author>
      <name>/u/lfnovo</name>
      <uri>https://old.reddit.com/user/lfnovo</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lfnovo"&gt; /u/lfnovo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenNotebook/comments/1qdvxeg/open_notebook_15_introducing_i18n_support_we/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qdvy1k/open_notebook_15_introducing_i18n_support_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qdvy1k/open_notebook_15_introducing_i18n_support_we/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-15T21:11:17+00:00</published>
  </entry>
</feed>
