<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-29T22:50:09+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pvfv1h</id>
    <title>Distributed Cognition and Context Control: gait and gaithub</title>
    <updated>2025-12-25T15:39:14+00:00</updated>
    <author>
      <name>/u/automateyournetwork</name>
      <uri>https://old.reddit.com/user/automateyournetwork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pvfv1h/distributed_cognition_and_context_control_gait/"&gt; &lt;img alt="Distributed Cognition and Context Control: gait and gaithub" src="https://external-preview.redd.it/QboS9f2QUI2oU5tnJHS_qXaoU7qrXkIcqyb5tZ65XrE.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=181344eb7ee38bf3d107b3dca75beba6ca77d8c1" title="Distributed Cognition and Context Control: gait and gaithub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the last few weeks, I‚Äôve been building - and just finished demoing - something I think we‚Äôre going to look back on as obvious in hindsight. &lt;/p&gt; &lt;p&gt;Distributed Cognition. Decentralized context control. &lt;/p&gt; &lt;p&gt;GAIT + GaitHub &lt;/p&gt; &lt;p&gt;A Git-like system ‚Äî but not for code. &lt;/p&gt; &lt;p&gt;For AI reasoning, memory, and context. &lt;/p&gt; &lt;p&gt;We‚Äôve spent decades perfecting how we:&lt;br /&gt; ‚Ä¢ version code&lt;br /&gt; ‚Ä¢ review changes&lt;br /&gt; ‚Ä¢ collaborate safely&lt;br /&gt; ‚Ä¢ reproduce results &lt;/p&gt; &lt;p&gt;And yet today, we let LLMs:&lt;br /&gt; ‚Ä¢ make architectural decisions&lt;br /&gt; ‚Ä¢ generate production content&lt;br /&gt; ‚Ä¢ influence real systems&lt;br /&gt; ‚Ä¶with almost no version control at all. &lt;/p&gt; &lt;p&gt;Chat logs aren‚Äôt enough. &lt;/p&gt; &lt;p&gt;Prompt files aren‚Äôt enough. &lt;/p&gt; &lt;p&gt;Screenshots definitely aren‚Äôt enough. &lt;/p&gt; &lt;p&gt;So I built something different. &lt;/p&gt; &lt;p&gt;What GAIT actually versions &lt;/p&gt; &lt;p&gt;GAIT treats AI interactions as first-class, content-addressed objects. &lt;/p&gt; &lt;p&gt;That includes:&lt;br /&gt; ‚Ä¢ user intent&lt;br /&gt; ‚Ä¢ model responses&lt;br /&gt; ‚Ä¢ memory state&lt;br /&gt; ‚Ä¢ branches of reasoning&lt;br /&gt; ‚Ä¢ resumable conversations &lt;/p&gt; &lt;p&gt;Every turn is hashed. Every decision is traceable. Every outcome is reproducible. &lt;/p&gt; &lt;p&gt;If Git solved ‚Äúit worked on my machine,‚Äù &lt;/p&gt; &lt;p&gt;GAIT solves ‚Äúwhy did the AI decide that?‚Äù &lt;/p&gt; &lt;p&gt;The demo (high-level walkthrough) &lt;/p&gt; &lt;p&gt;I recorded a full end-to-end demo showing how this works in practice: &lt;/p&gt; &lt;p&gt;Start in a clean folder ‚Äî no server, no UI &lt;/p&gt; &lt;p&gt;* Initialize GAIT locally&lt;br /&gt; * Run an AI chat session that‚Äôs automatically tracked&lt;br /&gt; * Ask a real, non-trivial technical question&lt;br /&gt; * Inspect the reasoning log&lt;br /&gt; * Resume the conversation later ‚Äî exactly where it left off&lt;br /&gt; * Branch the reasoning into alternate paths&lt;br /&gt; * Verify object integrity and state&lt;br /&gt; * Add a remote (GaitHub)&lt;br /&gt; * Create a remote repo from the CLI&lt;br /&gt; * Authenticate with a simple token&lt;br /&gt; * Push AI reasoning to the cloud&lt;br /&gt; * Fork another repo‚Äôs reasoning&lt;br /&gt; * Open a pull request on ideas, not code&lt;br /&gt; * Merge reasoning deterministically &lt;/p&gt; &lt;p&gt;No magic. No hidden state. No ‚Äútrust me, the model said so.‚Äù &lt;/p&gt; &lt;p&gt;Why this matters (especially for enterprises). AI is no longer a toy. &lt;/p&gt; &lt;p&gt;It‚Äôs:&lt;br /&gt; ‚Ä¢ part of decision pipelines&lt;br /&gt; ‚Ä¢ embedded in workflows&lt;br /&gt; ‚Ä¢ influencing customers, networks, and systems &lt;/p&gt; &lt;p&gt;But we can‚Äôt:&lt;br /&gt; ‚Ä¢ audit it&lt;br /&gt; ‚Ä¢ diff it&lt;br /&gt; ‚Ä¢ reproduce it&lt;br /&gt; ‚Ä¢ roll it back &lt;/p&gt; &lt;p&gt;That‚Äôs not sustainable. &lt;/p&gt; &lt;p&gt;GAIT introduces:&lt;br /&gt; ‚Ä¢ reproducible AI workflows&lt;br /&gt; ‚Ä¢ auditable reasoning history&lt;br /&gt; ‚Ä¢ collaborative cognition&lt;br /&gt; ‚Ä¢ local-first, cloud-optional design &lt;/p&gt; &lt;p&gt;This is infrastructure ‚Äî not a chatbot wrapper. This is not ‚ÄúGitHub for prompts‚Äù. That framing misses the point. &lt;/p&gt; &lt;p&gt;This is Git for cognition. &lt;/p&gt; &lt;p&gt;From:&lt;br /&gt; ‚Ä¢ commits ‚Üí conversations&lt;br /&gt; ‚Ä¢ diffs ‚Üí decisions&lt;br /&gt; ‚Ä¢ branches ‚Üí alternate reasoning&lt;br /&gt; ‚Ä¢ merges ‚Üí shared understanding &lt;/p&gt; &lt;p&gt;I genuinely believe version control for AI reasoning will become as fundamental as version control for source code. &lt;/p&gt; &lt;p&gt;The question isn‚Äôt if. &lt;/p&gt; &lt;p&gt;It‚Äôs who builds it correctly. &lt;/p&gt; &lt;p&gt;I‚Äôm excited to keep pushing this forward ‚Äî openly, transparently, and with the community. &lt;/p&gt; &lt;p&gt;More demos, docs, and real-world use cases coming soon. &lt;/p&gt; &lt;p&gt;If this resonates with you, I‚Äôd love to hear your thoughts üëá&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/automateyournetwork"&gt; /u/automateyournetwork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=0PyFHsYxjbk&amp;amp;si=ugLwYfnV_ETZ_VSR"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pvfv1h/distributed_cognition_and_context_control_gait/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pvfv1h/distributed_cognition_and_context_control_gait/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T15:39:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv33vq</id>
    <title>Llama 3.2 refuses to analyze dark web threat intel. Need uncensored 7B recommendations</title>
    <updated>2025-12-25T02:19:49+00:00</updated>
    <author>
      <name>/u/Loud-Goal190</name>
      <uri>https://old.reddit.com/user/Loud-Goal190</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm crawling onion sites for a defensive threat intel tool, but my local LLM (Llama 3.2) refuses to analyze the raw text due to safety filters. It sees &amp;quot;leak&amp;quot; or &amp;quot;.onion&amp;quot; and shuts down, even with jailbreak prompts. Regex captures emails but misses the context (like company names or data volume). Any recommendations for an uncensored 7B model that handles this well, or should I switch to a BERT model for extraction?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud-Goal190"&gt; /u/Loud-Goal190 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv33vq/llama_32_refuses_to_analyze_dark_web_threat_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv33vq/llama_32_refuses_to_analyze_dark_web_threat_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pv33vq/llama_32_refuses_to_analyze_dark_web_threat_intel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T02:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv71pg</id>
    <title>Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16.</title>
    <updated>2025-12-25T06:23:45+00:00</updated>
    <author>
      <name>/u/Double-Primary-2871</name>
      <uri>https://old.reddit.com/user/Double-Primary-2871</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pv71pg/finetuning_gptoss20b_on_a_ryzen_5950x_because/"&gt; &lt;img alt="Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16." src="https://preview.redd.it/6w9h1554na9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2621fb072eeb16ce8a0f4599cdbbfeb44b9f1c90" title="Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;at 1am.&lt;/p&gt; &lt;p&gt;I am fine-tuning my personal AI, into a gpt-oss-20b model, via LoRA, on a Ryzen 5950x CPU.&lt;/p&gt; &lt;p&gt;I had to pain stakingly deal with massive axolotl errors, venv and python version hell, yaml misconfigs, even fought with my other ai assistant, whom literally told me this couldn't be done on my system.... for hours and hours, for over a week.&lt;/p&gt; &lt;p&gt;Can't fine-tune with my radeon 7900XT because of bf16 kernel issues with ROCm on axolotl. I literally even tried to rent an h100 to help, and ran into serious roadblocks.&lt;/p&gt; &lt;p&gt;So the soultion was for me to convert the mxfp4 (bf16 format) weights back to fp32 and tell axolotl to stop downcasting back fp16.&lt;/p&gt; &lt;p&gt;Sure this will take days to compute all three of the shards, but after days of banging my head against the nearest convenient wall and keyboard, I finally got this s-o-b to work.&lt;/p&gt; &lt;p&gt;üòÅ also hi, new here. just wanted to share my story.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Double-Primary-2871"&gt; /u/Double-Primary-2871 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6w9h1554na9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv71pg/finetuning_gptoss20b_on_a_ryzen_5950x_because/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pv71pg/finetuning_gptoss20b_on_a_ryzen_5950x_because/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T06:23:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv88yv</id>
    <title>I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier models</title>
    <updated>2025-12-25T07:44:21+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pv88yv/i_built_planoa3b_most_efficient_llms_for_agent/"&gt; &lt;img alt="I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier models" src="https://preview.redd.it/059dttgf1b9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=898f1ffead5f76ea7c739cebaf5d8c1a413e3efc" title="I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone ‚Äî I‚Äôm on the Katanemo research team. Today we‚Äôre thrilled to launch &lt;strong&gt;Plano-Orchestrator&lt;/strong&gt;, a new family of LLMs built for fast multi-agent orchestration.&lt;/p&gt; &lt;p&gt;What do these new LLMs do? given a user request and the conversation context, Plano-Orchestrator decides which agent(s) should handle the request and in what sequence. In other words, it acts as the supervisor agent in a multi-agent system. Designed for multi-domain scenarios, it works well across general chat, coding tasks, and long, multi-turn conversations, while staying efficient enough for low-latency production deployments.&lt;/p&gt; &lt;p&gt;Why did we built this? Our applied research is focused on helping teams deliver agents safely and efficiently, with better real-world performance and latency ‚Äî the kind of ‚Äúglue work‚Äù that usually sits outside any single agent‚Äôs core product logic.&lt;/p&gt; &lt;p&gt;Plano-Orchestrator is integrated into Plano, our models-native proxy and dataplane for agents. Hope you enjoy it ‚Äî and we‚Äôd love feedback from anyone building multi-agent systems&lt;/p&gt; &lt;p&gt;Learn more about the LLMs &lt;a href="https://huggingface.co/collections/katanemo/plano-orchestrator"&gt;here&lt;/a&gt;&lt;br /&gt; About our open source project: &lt;a href="https://github.com/katanemo/plano"&gt;https://github.com/katanemo/plano&lt;/a&gt;&lt;br /&gt; And about our research: &lt;a href="https://planoai.dev/research"&gt;https://planoai.dev/research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/059dttgf1b9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv88yv/i_built_planoa3b_most_efficient_llms_for_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pv88yv/i_built_planoa3b_most_efficient_llms_for_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T07:44:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvyead</id>
    <title>Local AI for a game's npc and enemy</title>
    <updated>2025-12-26T07:19:56+00:00</updated>
    <author>
      <name>/u/MeuHorizon</name>
      <uri>https://old.reddit.com/user/MeuHorizon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm a junior game developer, and in Friendslop games, mimics or enemy behaviors fundamentally change the game. I'm thinking of running a local AI within the game for enemy behaviors and for mimics to mimic sounds and dialogue. But if I do this, what will the minimum system requirements be? 5090? Would a local AI that can use at least 2GB VRAM be too dumb?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MeuHorizon"&gt; /u/MeuHorizon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pvyead/local_ai_for_a_games_npc_and_enemy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pvyead/local_ai_for_a_games_npc_and_enemy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pvyead/local_ai_for_a_games_npc_and_enemy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-26T07:19:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw7hbt</id>
    <title>Offline vector DB experiment ‚Äî anyone want to test on their local setup?</title>
    <updated>2025-12-26T15:54:08+00:00</updated>
    <author>
      <name>/u/Serious-Section-5595</name>
      <uri>https://old.reddit.com/user/Serious-Section-5595</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pw7hbt/offline_vector_db_experiment_anyone_want_to_test/"&gt; &lt;img alt="Offline vector DB experiment ‚Äî anyone want to test on their local setup?" src="https://external-preview.redd.it/h_ijte2Ftf_QhAPNCeF0zmImOsXcFk4zfiB954lTuNI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11d4df01f4782cc60aae4d5e976779e127df8a7c" title="Offline vector DB experiment ‚Äî anyone want to test on their local setup?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I‚Äôve been building a small &lt;strong&gt;offline-first vector database&lt;/strong&gt; for local AI workflows. No cloud, no services just files on disk.&lt;/p&gt; &lt;p&gt;I made a universal benchmark script that adjusts dataset size based on your RAM so it doesn‚Äôt nuke laptops (100k vectors did that to me once üòÖ).&lt;/p&gt; &lt;p&gt;If you want to test it locally, here‚Äôs the script:&lt;br /&gt; &lt;a href="https://github.com/Srinivas26k/srvdb/blob/master/universal_benchmark.py"&gt;https://github.com/Srinivas26k/srvdb/blob/master/universal_benchmark.py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any feedback, issues, or benchmark results would help a lot.&lt;/p&gt; &lt;p&gt;Repo stars and contributions are also welcome if you find it useful&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious-Section-5595"&gt; /u/Serious-Section-5595 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pw7hbt/offline_vector_db_experiment_anyone_want_to_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pw7hbt/offline_vector_db_experiment_anyone_want_to_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pw7hbt/offline_vector_db_experiment_anyone_want_to_test/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-26T15:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwbor6</id>
    <title>How to use open source model in Antigravity ?</title>
    <updated>2025-12-26T18:47:30+00:00</updated>
    <author>
      <name>/u/One_Pianist8404</name>
      <uri>https://old.reddit.com/user/One_Pianist8404</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to integrate a self-hosted open-source LLM into Antigravity, is it possible ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One_Pianist8404"&gt; /u/One_Pianist8404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pwbor6/how_to_use_open_source_model_in_antigravity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pwbor6/how_to_use_open_source_model_in_antigravity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pwbor6/how_to_use_open_source_model_in_antigravity/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-26T18:47:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwarn1</id>
    <title>jailbreaks or uncensored models?</title>
    <updated>2025-12-26T18:09:31+00:00</updated>
    <author>
      <name>/u/United_Ad8618</name>
      <uri>https://old.reddit.com/user/United_Ad8618</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a site that has more up to date jailbreaks or uncensored models? All the jailbreaks or uncensored models I've found are for porn essentially, not much for other use cases like security work, and the old jailbreaks don't seem to work on claude anymore&lt;/p&gt; &lt;p&gt;Side note: is it worth using grok for this reason?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United_Ad8618"&gt; /u/United_Ad8618 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pwarn1/jailbreaks_or_uncensored_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pwarn1/jailbreaks_or_uncensored_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pwarn1/jailbreaks_or_uncensored_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-26T18:09:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwyuji</id>
    <title>I built a GraphRAG application to visualize AI knowledge (Runs 100% Local via Ollama OR Fast via Gemini API)</title>
    <updated>2025-12-27T14:17:08+00:00</updated>
    <author>
      <name>/u/Dev-it-with-me</name>
      <uri>https://old.reddit.com/user/Dev-it-with-me</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dev-it-with-me"&gt; /u/Dev-it-with-me &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1pwyu6k/i_built_a_graphrag_application_to_visualize_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pwyuji/i_built_a_graphrag_application_to_visualize_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pwyuji/i_built_a_graphrag_application_to_visualize_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-27T14:17:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwusyo</id>
    <title>best model to run on a 5080 laptop with intel ultra i9 and 64gb of ram on linux mainly for beginner coding?</title>
    <updated>2025-12-27T10:33:09+00:00</updated>
    <author>
      <name>/u/Subject_Swimming6327</name>
      <uri>https://old.reddit.com/user/Subject_Swimming6327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i was suggested mistral and qwen and of course have tried deepseek, just wondering if anyone had any specific suggestions for my setup. im a total beginner. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Subject_Swimming6327"&gt; /u/Subject_Swimming6327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pwusyo/best_model_to_run_on_a_5080_laptop_with_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pwusyo/best_model_to_run_on_a_5080_laptop_with_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pwusyo/best_model_to_run_on_a_5080_laptop_with_intel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-27T10:33:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxi5sp</id>
    <title>AI driven physical product inspection</title>
    <updated>2025-12-28T04:28:50+00:00</updated>
    <author>
      <name>/u/onemorequickchange</name>
      <uri>https://old.reddit.com/user/onemorequickchange</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An order is filled with physical products. Groceries. Products are delivered. A camera captures the products as they are carried on board. What are the challenges woth AI identifying missed products and communicating with vendor to solve rhe issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onemorequickchange"&gt; /u/onemorequickchange &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxi5sp/ai_driven_physical_product_inspection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxi5sp/ai_driven_physical_product_inspection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pxi5sp/ai_driven_physical_product_inspection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T04:28:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxxi8n</id>
    <title>Ollama running on CPU instead of GPU on a Proxmox VM with PCI Bridge</title>
    <updated>2025-12-28T17:49:21+00:00</updated>
    <author>
      <name>/u/Lumaric_</name>
      <uri>https://old.reddit.com/user/Lumaric_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pxxi8n/ollama_running_on_cpu_instead_of_gpu_on_a_proxmox/"&gt; &lt;img alt="Ollama running on CPU instead of GPU on a Proxmox VM with PCI Bridge" src="https://b.thumbs.redditmedia.com/9ttGWiGRO9VQuNJv4Spw_kKfewl9wLi3xkDfcd-VlwM.jpg" title="Ollama running on CPU instead of GPU on a Proxmox VM with PCI Bridge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt; &lt;p&gt;I am looking for help on a specific situation since my configuration is a bit special. I have an computer on the side that i use has a server with Proxmox installed on it. I mainly made with all component of my main PC with special modification. CPU Ryzen 9 5900X, 128Go RAM DDR4 and RX 6700 XT.&lt;/p&gt; &lt;p&gt;I created a Virtual machine with a PCI bridge to the graphic card in the objectif of hosting a self-hosted model, i managed to done it after a lot of work but now the VM correctly detected the graphic and i can see the default terminal interface of debian from an HDMI port.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f4ap5p21dz9g1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92213a024b7b3e2578b02125aa98cb52052455e3"&gt;https://preview.redd.it/f4ap5p21dz9g1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92213a024b7b3e2578b02125aa98cb52052455e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After that a installed ollama and i got the message &amp;quot;AMD GPU ready&amp;quot; indicating that the GPU was correcly detected.&lt;/p&gt; &lt;p&gt;So i took my time to configure everything else like WebUi, but at the moment of running a model, it need 20sec just to respond to a &amp;quot;Bonjour&amp;quot; ( yeah i from France ), i tried different model thinking it was just a model not adapted but same problem.&lt;/p&gt; &lt;p&gt;So i check with ollama ps and i see that all model is running on the CPU :&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5ixofzncfz9g1.png?width=819&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2836d36922f6924244fde627df020601bb19c032"&gt;https://preview.redd.it/5ixofzncfz9g1.png?width=819&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2836d36922f6924244fde627df020601bb19c032&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Does anyone know, if i could have made a misstake during the configuration or if i missing a configuration. I tried to reinstall the AMD Gpu Driver from the link on the Ollama Doc linux page. Shoud i try to use Vulkan ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lumaric_"&gt; /u/Lumaric_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxxi8n/ollama_running_on_cpu_instead_of_gpu_on_a_proxmox/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxxi8n/ollama_running_on_cpu_instead_of_gpu_on_a_proxmox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pxxi8n/ollama_running_on_cpu_instead_of_gpu_on_a_proxmox/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T17:49:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyfenp</id>
    <title>Why I Don‚Äôt Trust Any LLM Output (And Neither Should You)</title>
    <updated>2025-12-29T06:56:43+00:00</updated>
    <author>
      <name>/u/isoman</name>
      <uri>https://old.reddit.com/user/isoman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pyfenp/why_i_dont_trust_any_llm_output_and_neither/"&gt; &lt;img alt="Why I Don‚Äôt Trust Any LLM Output (And Neither Should You)" src="https://b.thumbs.redditmedia.com/lfbyGc-i_ekOngCzN9Z0iCN-SMCbiQ9-USCmMWjM_uo.jpg" title="Why I Don‚Äôt Trust Any LLM Output (And Neither Should You)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLMs hallucinate with confidence.&lt;/p&gt; &lt;p&gt;I‚Äôm not anti-LLM. I use them daily.&lt;br /&gt; I just don‚Äôt trust their output.&lt;/p&gt; &lt;p&gt;So I built something to sit &lt;em&gt;after&lt;/em&gt; the model.&lt;/p&gt; &lt;h1&gt;The problem isn‚Äôt intelligence ‚Äî it‚Äôs confidence&lt;/h1&gt; &lt;p&gt;Modern LLMs are very good at sounding right.&lt;/p&gt; &lt;p&gt;They are not obligated to be correct.&lt;br /&gt; They are optimized to respond.&lt;/p&gt; &lt;p&gt;When they don‚Äôt know, they still answer.&lt;br /&gt; When the evidence is weak, they still sound confident.&lt;/p&gt; &lt;p&gt;This is fine in chat.&lt;br /&gt; It‚Äôs dangerous in production.&lt;/p&gt; &lt;p&gt;Especially when:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the user isn‚Äôt technical&lt;/li&gt; &lt;li&gt;the output looks authoritative&lt;/li&gt; &lt;li&gt;the system has no refusal path&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Prompts don‚Äôt solve this&lt;/h1&gt; &lt;p&gt;Most mitigation tries to fix the model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;better prompts&lt;/li&gt; &lt;li&gt;more system instructions&lt;/li&gt; &lt;li&gt;RLHF / fine-tuning&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That helps ‚Äî but it doesn‚Äôt change the core failure mode.&lt;/p&gt; &lt;p&gt;The model still &lt;strong&gt;must answer&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I wanted a system where the model is &lt;em&gt;allowed to be wrong&lt;/em&gt; ‚Äî&lt;br /&gt; but the system is not allowed to release it.&lt;/p&gt; &lt;h1&gt;What I built instead&lt;/h1&gt; &lt;p&gt;I built &lt;strong&gt;arifOS&lt;/strong&gt; ‚Äî a post-generation governance layer.&lt;/p&gt; &lt;p&gt;It sits between:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;LLM output ‚Üí reality &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The model generates output as usual&lt;br /&gt; (local models, Ollama, Claude, ChatGPT, Gemini, etc.)&lt;/p&gt; &lt;p&gt;That output is &lt;strong&gt;not trusted&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It is checked against &lt;strong&gt;9 constitutional ‚Äúfloors‚Äù&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If &lt;em&gt;any&lt;/em&gt; floor fails ‚Üí&lt;br /&gt; the output is &lt;strong&gt;refused&lt;/strong&gt;, not rewritten, not softened.&lt;/p&gt; &lt;p&gt;No guessing.&lt;br /&gt; No ‚Äúprobably‚Äù.&lt;br /&gt; No confidence inflation.&lt;/p&gt; &lt;h1&gt;Concrete examples&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Truth / Amanah&lt;/strong&gt;&lt;br /&gt; If the model is uncertain ‚Üí it must refuse.&lt;br /&gt; ‚ÄúI can‚Äôt compute this‚Äù beats a polished lie.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Safety&lt;/strong&gt;&lt;br /&gt; Refuses SQL injection, hardcoded secrets, credentials, XSS patterns.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Auditability&lt;/strong&gt;&lt;br /&gt; Every decision is logged.&lt;br /&gt; You can trace &lt;em&gt;why&lt;/em&gt; something was blocked.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Humility&lt;/strong&gt;&lt;br /&gt; No 100% certainty.&lt;br /&gt; A hard 3‚Äì5% uncertainty band.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Anti-Ghost&lt;/strong&gt;&lt;br /&gt; No fake consciousness.&lt;br /&gt; No ‚ÄúI feel‚Äù, ‚ÄúI believe‚Äù, ‚ÄúI want‚Äù.&lt;/p&gt; &lt;h1&gt;How this is different&lt;/h1&gt; &lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; alignment.&lt;br /&gt; This is &lt;strong&gt;not&lt;/strong&gt; prompt engineering.&lt;/p&gt; &lt;p&gt;Think of it like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;circuit breakers in markets&lt;/li&gt; &lt;li&gt;type checking in compilers&lt;/li&gt; &lt;li&gt;linters, but for AI output&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model can hallucinate.&lt;br /&gt; The system refuses to ship it.&lt;/p&gt; &lt;h1&gt;What it works with&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Local models (Ollama, LM Studio, etc.)&lt;/li&gt; &lt;li&gt;Claude / ChatGPT / Gemini APIs&lt;/li&gt; &lt;li&gt;Multi-agent systems&lt;/li&gt; &lt;li&gt;Any Python LLM stack&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model-agnostic by design.&lt;/p&gt; &lt;h1&gt;Current state (no hype)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;~2,180 tests&lt;/li&gt; &lt;li&gt;High safety ceiling&lt;/li&gt; &lt;li&gt;Works in dev / prototype&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Not battle-tested at scale yet&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Fully open source ‚Äî the law is inspectable&lt;/li&gt; &lt;li&gt;Early stage ‚Üí actively looking for break attempts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If it fails, I want to know how.&lt;/p&gt; &lt;h1&gt;Why I care&lt;/h1&gt; &lt;p&gt;I‚Äôm a geologist.&lt;/p&gt; &lt;p&gt;In subsurface work, confidence without evidence burns millions.&lt;/p&gt; &lt;p&gt;Watching LLMs shipped with the same failure mode&lt;br /&gt; felt irresponsible.&lt;/p&gt; &lt;p&gt;So I built the governor I wish existed in high-risk systems.&lt;/p&gt; &lt;h1&gt;Install&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pip install arifOS &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/ariffazil/arifOS"&gt;https://github.com/ariffazil/arifOS&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;I‚Äôm not claiming this is the answer&lt;/h1&gt; &lt;p&gt;I‚Äôm saying the failure mode is real.&lt;/p&gt; &lt;p&gt;If you‚Äôve been burned by confident hallucinations ‚Üí try it, break it.&lt;br /&gt; If this is the wrong approach ‚Üí tell me why.&lt;br /&gt; If you solved this better ‚Üí show me.&lt;/p&gt; &lt;p&gt;Refusing is often safer than guessing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DITEMPA, BUKAN DIBERI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/okvmaokmc3ag1.png?width=2752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dd08631f8d4f6812b81f24c1a9398e6588ade3ed"&gt;https://preview.redd.it/okvmaokmc3ag1.png?width=2752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dd08631f8d4f6812b81f24c1a9398e6588ade3ed&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isoman"&gt; /u/isoman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyfenp/why_i_dont_trust_any_llm_output_and_neither/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyfenp/why_i_dont_trust_any_llm_output_and_neither/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pyfenp/why_i_dont_trust_any_llm_output_and_neither/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T06:56:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxr2ws</id>
    <title>How to get started with automated workflows?</title>
    <updated>2025-12-28T13:14:38+00:00</updated>
    <author>
      <name>/u/PlastikHateAccount</name>
      <uri>https://old.reddit.com/user/PlastikHateAccount</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there, I'm interested how you guys set up ollama to work on tasks.&lt;/p&gt; &lt;p&gt;The first thing we already tried is having a Python script that calls the company internal Ollama via api with simple tasks in a loop. Imagine pseudocode:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for sourcecode in repository: api-call-to-ollama(&amp;quot;Please do a sourcecode review: &amp;quot; + sourcecode) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We tried multiple tasks like this for &lt;strong&gt;multiple usecases, not just sourcecode reviews&lt;/strong&gt; and the intelligence is quite promising but ofc the context the LLMs have available to solve tasks like that limiting.&lt;/p&gt; &lt;p&gt;So the second idea is to somehow let the LLM make the decision what to include in a prompt. Let's call them &amp;quot;pretasks&amp;quot;.&lt;/p&gt; &lt;p&gt;This pretask could be a prompt saying ¬¥&amp;quot;Write a prompt to an LLM to do a sourcecode review. You can decide to include adjacent PDFs, Jira tickets, pieces of sourcecode by writing &amp;lt;include:filename&amp;gt;&amp;quot; + list-of-available-files-with-descriptions-what-they-are¬¥. The python script would then parse the result of the pretask to collect the relevant files.&lt;/p&gt; &lt;p&gt;Third and finally, at that point we could let the pretask trigger itself even more pretasks. This is where the thing would be almost bootstrapped. But I'm out of ideas how to coordinate this, prevent endless loops etc.&lt;/p&gt; &lt;p&gt;Sorry if my thoughts around this whole topic are a little scattered. I assume the whole world is right now thinking about these kinds of workflows. So I'd like to know where to start reading about it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PlastikHateAccount"&gt; /u/PlastikHateAccount &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxr2ws/how_to_get_started_with_automated_workflows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxr2ws/how_to_get_started_with_automated_workflows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pxr2ws/how_to_get_started_with_automated_workflows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T13:14:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1py50nq</id>
    <title>I updated ollama and now it uses cpu &amp; system ram instead of my gpu</title>
    <updated>2025-12-28T22:49:09+00:00</updated>
    <author>
      <name>/u/NewDildos</name>
      <uri>https://old.reddit.com/user/NewDildos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using a few different models for a while in powershell and without thinking I updated ollama to download a new model. My prompt eval rate went from 2887.53 tokens/s to 8.25 and my eval rate went from 31.91 tokens/s to 4.7 A little over 50s for a 200 word output test. I'm using a 4060ti 16gb and would like to know how to change the settings to run on my gpu again. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewDildos"&gt; /u/NewDildos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1py50nq/i_updated_ollama_and_now_it_uses_cpu_system_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1py50nq/i_updated_ollama_and_now_it_uses_cpu_system_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1py50nq/i_updated_ollama_and_now_it_uses_cpu_system_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T22:49:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxqi11</id>
    <title>Ollama Model which Suits for my System</title>
    <updated>2025-12-28T12:43:52+00:00</updated>
    <author>
      <name>/u/devil__6996</name>
      <uri>https://old.reddit.com/user/devil__6996</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven‚Äôt downloaded these models yet and want to understand real-world experience before pulling them locally.&lt;/p&gt; &lt;p&gt;Hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RTX 4050 (6GB VRAM)&lt;/li&gt; &lt;li&gt;32GB RAM&lt;/li&gt; &lt;li&gt;Ryzen 7 7000 series&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Use case:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vibe coding&lt;/li&gt; &lt;li&gt;Code generation&lt;/li&gt; &lt;li&gt;Building software applications&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;- Web UI via Ollama (Open WebUI or similar)&lt;br /&gt; -For Cybersecurity Code generations etc,,,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devil__6996"&gt; /u/devil__6996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxqi11/ollama_model_which_suits_for_my_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pxqi11/ollama_model_which_suits_for_my_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pxqi11/ollama_model_which_suits_for_my_system/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T12:43:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1py6f8c</id>
    <title>Cooperative team problems</title>
    <updated>2025-12-28T23:48:33+00:00</updated>
    <author>
      <name>/u/Nearby_You_313</name>
      <uri>https://old.reddit.com/user/Nearby_You_313</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying to create a virtual business team to help me with tasks. The idea was to have a manager who interacts hub-and-spoke style with all other agents. I provide only high-level direction and it develops a plan, assigns and delegates tasks, saves output, and gets back to me. &lt;/p&gt; &lt;p&gt;I was able to get this working in self-developed code and Microsoft Agent Framework, both accessing Ollama, but the results are... interesting. The manager would delegate a task to the researcher, who would search and provide feedback, but then the manager would completely hallucinate actually saving the data. (It seems to me to be a model limitation issue, mostly, but I'm developing a new testing method that takes tool usage into account and will test all my local models again to see if I get better results with a different one.)&lt;/p&gt; &lt;p&gt;I'd like to use Claude Code or systems due to their better models, but they're all severely limited (Claude can't create agents on-the-fly, etc.) or very costly. &lt;/p&gt; &lt;p&gt;Has anyone actually accomplished something like this locally that actually works semi-decently? How do your agents interact? How did you fix tool usage? What models? Etc.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nearby_You_313"&gt; /u/Nearby_You_313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1py6f8c/cooperative_team_problems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1py6f8c/cooperative_team_problems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1py6f8c/cooperative_team_problems/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T23:48:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyct8q</id>
    <title>i tried to ask another llm why my llm wouldn't load and it got part way before the system crashed üíÄ</title>
    <updated>2025-12-29T04:40:56+00:00</updated>
    <author>
      <name>/u/sunggis</name>
      <uri>https://old.reddit.com/user/sunggis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pyct8q/i_tried_to_ask_another_llm_why_my_llm_wouldnt/"&gt; &lt;img alt="i tried to ask another llm why my llm wouldn't load and it got part way before the system crashed üíÄ" src="https://preview.redd.it/up5qz6c5o2ag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7ae5ea85e06fefc8159a7c4ec932da416cf8cb4" title="i tried to ask another llm why my llm wouldn't load and it got part way before the system crashed üíÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunggis"&gt; /u/sunggis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/up5qz6c5o2ag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyct8q/i_tried_to_ask_another_llm_why_my_llm_wouldnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pyct8q/i_tried_to_ask_another_llm_why_my_llm_wouldnt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T04:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1py5h57</id>
    <title>Questions about usage limits for Ollama Cloud models (high-volume token generation)</title>
    <updated>2025-12-28T23:08:28+00:00</updated>
    <author>
      <name>/u/AlexHardy08</name>
      <uri>https://old.reddit.com/user/AlexHardy08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm currently evaluating &lt;strong&gt;Ollama Cloud models&lt;/strong&gt; and would appreciate some clarification regarding &lt;strong&gt;usage limits on paid plans&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I‚Äôm interested in running the following cloud models via Ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;ollama run gemini-3-flash-preview:cloud&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;ollama run deepseek-v3.1:671b-cloud&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;ollama run gemini-3-pro-preview&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;ollama run kimi-k2:1t-cloud&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My use case&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Daily content generation: &lt;strong&gt;~5‚Äì10 million tokens per day&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Number of prompt submissions: &lt;strong&gt;~1,000‚Äì2,000 per day&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Average prompt size: &lt;strong&gt;~2,500 tokens&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Responses can be long (multi-thousand tokens)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Questions&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Do the &lt;strong&gt;paid Ollama plans&lt;/strong&gt; support this level of token throughput (5‚Äì10M tokens/day)?&lt;/li&gt; &lt;li&gt;Are there &lt;strong&gt;hard daily or monthly token caps&lt;/strong&gt; per model or per account?&lt;/li&gt; &lt;li&gt;How are &lt;strong&gt;API requests counted&lt;/strong&gt; internally by Ollama for each prompt/response cycle?&lt;/li&gt; &lt;li&gt;Does a single &lt;code&gt;ollama run&lt;/code&gt; execution map to &lt;strong&gt;one API request&lt;/strong&gt;, or can it generate multiple internal calls depending on response length?&lt;/li&gt; &lt;li&gt;Are there &lt;strong&gt;per-model limitations&lt;/strong&gt; (rate limits, concurrency, max tokens) for large cloud models like DeepSeek 671B or Kimi-K2 1T?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôm trying to determine whether the current &lt;strong&gt;paid offering can reliably sustain this workload&lt;/strong&gt; or if additional arrangements (enterprise plans, quotas, etc.) are required.&lt;/p&gt; &lt;p&gt;Any insights from the Ollama team or experienced users running high-volume workloads would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexHardy08"&gt; /u/AlexHardy08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1py5h57/questions_about_usage_limits_for_ollama_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1py5h57/questions_about_usage_limits_for_ollama_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1py5h57/questions_about_usage_limits_for_ollama_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T23:08:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1py1odn</id>
    <title>Old server for local models</title>
    <updated>2025-12-28T20:33:03+00:00</updated>
    <author>
      <name>/u/Jacobmicro</name>
      <uri>https://old.reddit.com/user/Jacobmicro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ended up with an old poweredge r610 with the dual xeon chips and 192gb of ram. Everything is in good working order. Debating on trying to see if I could hack together something to run local models that could automate some of the work I used to pay API keys for with my work. &lt;/p&gt; &lt;p&gt;Anybody ever have any luck using older architecture? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jacobmicro"&gt; /u/Jacobmicro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1py1odn/old_server_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1py1odn/old_server_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1py1odn/old_server_for_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T20:33:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyn9kw</id>
    <title>So hi all, i am currently playing with all this self hosted LLM (SLM in my case with my hardware limitations) im just using a Proxmox enviroment with Ollama installed direcly on a Ubuntu server container and on top of it Open WebUI to get the nice dashboard and to be able to create user accounts.</title>
    <updated>2025-12-29T14:11:58+00:00</updated>
    <author>
      <name>/u/Franceesios</name>
      <uri>https://old.reddit.com/user/Franceesios</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pyn9kw/so_hi_all_i_am_currently_playing_with_all_this/"&gt; &lt;img alt="So hi all, i am currently playing with all this self hosted LLM (SLM in my case with my hardware limitations) im just using a Proxmox enviroment with Ollama installed direcly on a Ubuntu server container and on top of it Open WebUI to get the nice dashboard and to be able to create user accounts." src="https://b.thumbs.redditmedia.com/iNf-j2OTzD0L4Rv9eBe773QoWp_aokC85y843xSV_Po.jpg" title="So hi all, i am currently playing with all this self hosted LLM (SLM in my case with my hardware limitations) im just using a Proxmox enviroment with Ollama installed direcly on a Ubuntu server container and on top of it Open WebUI to get the nice dashboard and to be able to create user accounts." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So far im using just these models &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w18f48hnh5ag1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c46e7759d8c3bb13d8238a4f1503ad3dd7620957"&gt;https://preview.redd.it/w18f48hnh5ag1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c46e7759d8c3bb13d8238a4f1503ad3dd7620957&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Llama3.2:1.2b&lt;/p&gt; &lt;p&gt;- Llama3.2:latest 3.2b&lt;/p&gt; &lt;p&gt;- Llama3.2:&lt;strong&gt;8b&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;- Ministral-3:8b&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;They are running ok at the time, the 8B ones would take atleast 2 minutes to give some proper answer, and ive also put this template for the models to remember with each answer they give out ;&lt;/p&gt; &lt;p&gt;### Task:&lt;/p&gt; &lt;p&gt;Respond to the user query using the provided context, incorporating inline citations in the format [id] **only when the &amp;lt;source&amp;gt; tag includes an explicit id attribute** (e.g., &amp;lt;source id=&amp;quot;1&amp;quot;&amp;gt;). Always include a confidence rating for your answer.&lt;/p&gt; &lt;p&gt;### Guidelines:&lt;/p&gt; &lt;p&gt;- Only provide answers you are confident in. Do not guess or invent information.&lt;/p&gt; &lt;p&gt;- If unsure or lacking sufficient information, respond with &amp;quot;I don‚Äôt know&amp;quot; or &amp;quot;I‚Äôm not sure.&amp;quot;&lt;/p&gt; &lt;p&gt;- Include a confidence rating from 1 to 5:&lt;/p&gt; &lt;p&gt;1 = very uncertain&lt;/p&gt; &lt;p&gt;2 = somewhat uncertain&lt;/p&gt; &lt;p&gt;3 = moderately confident&lt;/p&gt; &lt;p&gt;4 = confident&lt;/p&gt; &lt;p&gt;5 = very confident&lt;/p&gt; &lt;p&gt;- Respond in the same language as the user's query.&lt;/p&gt; &lt;p&gt;- If the context is unreadable or low-quality, inform the user and provide the best possible answer.&lt;/p&gt; &lt;p&gt;- If the answer isn‚Äôt present in the context but you possess the knowledge, explain this and provide the answer.&lt;/p&gt; &lt;p&gt;- Include inline citations [id] only when &amp;lt;source&amp;gt; has an id attribute.&lt;/p&gt; &lt;p&gt;- Do not use XML tags in your response.&lt;/p&gt; &lt;p&gt;- Ensure citations are concise and directly relevant.&lt;/p&gt; &lt;p&gt;- Do NOT use Web Search or external sources.&lt;/p&gt; &lt;p&gt;- If the context does not contain the answer, reply: ‚ÄòI don‚Äôt know‚Äô and Confidence 1‚Äì2.&lt;/p&gt; &lt;p&gt;### Example Output:&lt;/p&gt; &lt;p&gt;Answer: [Your answer here]&lt;/p&gt; &lt;p&gt;Confidence: [1-5]&lt;/p&gt; &lt;p&gt;### Context:&lt;/p&gt; &lt;p&gt;&amp;lt;context&amp;gt;&lt;/p&gt; &lt;p&gt;{{CONTEXT}}&lt;/p&gt; &lt;p&gt;&amp;lt;/context&amp;gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tbnk6bekh5ag1.png?width=1647&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38c75ac55e6951ca80a0f364fdcf8629379c69aa"&gt;https://preview.redd.it/tbnk6bekh5ag1.png?width=1647&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38c75ac55e6951ca80a0f364fdcf8629379c69aa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With so far works great, my primarly test right about now is the RAG method that Open WebUI offers, ive currently uploaded some invoices from this whole year worth of data as .MD files.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nchwh0kyh5ag1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a43d510aa7032f361dbfc7849903d1d87ba221a5"&gt;https://preview.redd.it/nchwh0kyh5ag1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a43d510aa7032f361dbfc7849903d1d87ba221a5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And asks the model (selecting the folder with the data first with # command/option) and i would get some good answers and some times some not so good answers but witj the confidence level accurate.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vqzwaupsh5ag1.png?width=559&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2737560e7562ccb31845f578e8ac89dbd42d33bb"&gt;https://preview.redd.it/vqzwaupsh5ag1.png?width=559&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2737560e7562ccb31845f578e8ac89dbd42d33bb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now my question is, if some tech company wants to implement these type of LLM (SML) into there on premise network for like finance department to use, is this a good start? How does some enterprise do it at the moment? Like sites like &lt;a href="http://llm.co"&gt;llm.co&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9knu91phh5ag1.png?width=1438&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a790870d44637e073b7807f3120306fdee8db623"&gt;https://preview.redd.it/9knu91phh5ag1.png?width=1438&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a790870d44637e073b7807f3120306fdee8db623&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far i can see real use case for this RAG method with some more powerfull hardware ofcourse, but let me know your real enterprise use case of a on-prem LLM RAG method. &lt;/p&gt; &lt;p&gt;Thanks all! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Franceesios"&gt; /u/Franceesios &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyn9kw/so_hi_all_i_am_currently_playing_with_all_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyn9kw/so_hi_all_i_am_currently_playing_with_all_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pyn9kw/so_hi_all_i_am_currently_playing_with_all_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T14:11:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyoh9k</id>
    <title>Best grammar and sentence correction model on MacBook with 18GB RAM</title>
    <updated>2025-12-29T15:02:44+00:00</updated>
    <author>
      <name>/u/A-n-d-y-R-e-d</name>
      <uri>https://old.reddit.com/user/A-n-d-y-R-e-d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My MacBook has only 18 GB of RAM! &lt;/p&gt; &lt;p&gt;I am looking for an offline model that can take the text, understand the context, and rewrite it concisely while fixing grammatical issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A-n-d-y-R-e-d"&gt; /u/A-n-d-y-R-e-d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyoh9k/best_grammar_and_sentence_correction_model_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyoh9k/best_grammar_and_sentence_correction_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pyoh9k/best_grammar_and_sentence_correction_model_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T15:02:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyp00p</id>
    <title>In which framework the OLLAMA GUI is written in?</title>
    <updated>2025-12-29T15:23:20+00:00</updated>
    <author>
      <name>/u/SpiritualQuality1055</name>
      <uri>https://old.reddit.com/user/SpiritualQuality1055</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like the new &lt;code&gt;ollama&lt;/code&gt; interface, its smooth and slick. I would like to know in which framework its written in?&lt;br /&gt; Is the code for the GUI could be found in the &lt;code&gt;ollama github&lt;/code&gt; repo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpiritualQuality1055"&gt; /u/SpiritualQuality1055 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyp00p/in_which_framework_the_ollama_gui_is_written_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyp00p/in_which_framework_the_ollama_gui_is_written_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pyp00p/in_which_framework_the_ollama_gui_is_written_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T15:23:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz03dc</id>
    <title>Summary of Vibe Coding Models for 6GB VRAM Systems</title>
    <updated>2025-12-29T22:21:11+00:00</updated>
    <author>
      <name>/u/FieldMouseInTheHouse</name>
      <uri>https://old.reddit.com/user/FieldMouseInTheHouse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Summary of Vibe Coding Models for 6GB VRAM Systems&lt;/h1&gt; &lt;p&gt;Here is a list of models that would actually fit inside of a 6GB VRAM budget. I am deliberately leaving out any models that anybody suggested that would not have fit inside of a 6GB VRAM budget! ü§ó&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fitting inside of the 6GB VRAM budget means that it is possible to easily achive 30, 50, 80 or more tokens per second depending on the task.&lt;/strong&gt; If you go outside of the VRAM budget, things can slow down to as slow as 3 to 7 tokens per second -- this could serverely harm productivity.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;`&lt;a href="https://ollama.com/library/qwen3:4b"&gt;qwen3:4b&lt;/a&gt;` size=2.5GB&lt;/li&gt; &lt;li&gt;`&lt;a href="https://ollama.com/library/ministral-3:3b"&gt;ministral-3:3b&lt;/a&gt;` size=3.0GB&lt;/li&gt; &lt;li&gt;`&lt;a href="https://ollama.com/library/gemma3:1b"&gt;gemma3:1b&lt;/a&gt;` size=815MB&lt;/li&gt; &lt;li&gt;`&lt;a href="https://ollama.com/library/gemma3:4b"&gt;gemma3:4b&lt;/a&gt;` size=3.3GB üëà I added this one because it is a little bigger than the &lt;code&gt;gemma3:1b&lt;/code&gt;, but still fits confortably inside of your 6GB VRAM budget. This model should be more capable than &lt;code&gt;gemma3:1b&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üíª I would suggest that folks first try these models with &lt;code&gt;ollama run MODELNAME&lt;/code&gt; and check to see how they fit in the VRAM of your own systems (&lt;code&gt;ollama ps&lt;/code&gt;) and check them for performance like tokens per second during the &lt;code&gt;ollama run MODELNAME&lt;/code&gt; stage (&lt;code&gt;/set verbose&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;üß† What do you think?&lt;/p&gt; &lt;p&gt;ü§ó Are there any other small models that you use that you would like to share?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FieldMouseInTheHouse"&gt; /u/FieldMouseInTheHouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pz03dc/summary_of_vibe_coding_models_for_6gb_vram_systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pz03dc/summary_of_vibe_coding_models_for_6gb_vram_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pz03dc/summary_of_vibe_coding_models_for_6gb_vram_systems/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T22:21:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pym7c0</id>
    <title>Running Ministral 3 3B Locally with Ollama and Adding Tool Calling (Local + Remote MCP)</title>
    <updated>2025-12-29T13:24:35+00:00</updated>
    <author>
      <name>/u/shricodev</name>
      <uri>https://old.reddit.com/user/shricodev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been seeing a lot of chatter around Ministral 3 3B, so I wanted to test it in a way that actually matters day to day. Can such a small local model do reliable tool calling, and can you extend it beyond local tools to work with remotely hosted MCP servers?&lt;/p&gt; &lt;p&gt;Here‚Äôs what I tried:&lt;/p&gt; &lt;h1&gt;Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Ran a quantized 4-bit (Q4_K_M) Ministral 3 3B on Ollama&lt;/li&gt; &lt;li&gt;Connected it to Open WebUI (with Docker)&lt;/li&gt; &lt;li&gt;Tested tool calling in two stages: &lt;ul&gt; &lt;li&gt;Local Python tools inside Open WebUI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Remote MCP tools&lt;/strong&gt; via Composio (so the model can call externally hosted tools through MCP)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model, despite the super tiny size of just 3B parameters, is said to support tool calling with even support for structured output. So, this was really fun to see the model in action.&lt;/p&gt; &lt;p&gt;Most of the guides show you how to work with just the local tools, which is not ideal when you plan to use the model for bigger, better and managed tools for hundreds of different services. &lt;/p&gt; &lt;p&gt;In this guide, I've covered the model specs and the entire setup, including setting up a Docker container for Ollama and running Ollama WebUI.&lt;/p&gt; &lt;p&gt;And the nice part is that the model setup guide here works for all the other models that support tool calling.&lt;/p&gt; &lt;p&gt;I wrote up the full walkthrough with commands and screenshots:&lt;/p&gt; &lt;p&gt;You can find it here: &lt;a href="https://composio.dev/blog/tool-calling-with-ministral-3b"&gt;MCP tool calling guide with Ministral 3B, Composio, and Ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone else has tested tool calling on Ministral 3 3B (or worked with it using vLLM instead of Ollama), I‚Äôd love to hear what worked best for you, as I couldn't get vLLM to work due to CUDA errors. :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shricodev"&gt; /u/shricodev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pym7c0/running_ministral_3_3b_locally_with_ollama_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pym7c0/running_ministral_3_3b_locally_with_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pym7c0/running_ministral_3_3b_locally_with_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T13:24:35+00:00</published>
  </entry>
</feed>
