<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-27T18:47:43+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1rf61qd</id>
    <title>Made my first project, Autonomous video generator</title>
    <updated>2026-02-26T09:49:00+00:00</updated>
    <author>
      <name>/u/Pronation1227</name>
      <uri>https://old.reddit.com/user/Pronation1227</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Pronation1227/AVB#video-generator-bot"&gt;&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Hi, This is my first project (which i actually managed to complete)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/Pronation1227/AVB#hi-this-is-my-first-project-which-i-actually-managed-to-complete"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;About me: I am in high school and have been coding on and off for a few years now.&lt;/p&gt; &lt;p&gt;a quick overview of this project, its basically a storytime generator inspired from the insta videos you see on reels. There was no real motive behind building this i was just frustrated of tutorial hell and hence built the first thing that came to my mind&lt;/p&gt; &lt;p&gt;I admit i did use AI to help me with structuring the project into different files ie: output, notes, background, scripts. I also used ai for the ffmpeg subprocess in generate_vid.py as i had no idea what ffmpeg is or how to use it. But all other lines of code in all the files have been written by me&lt;/p&gt; &lt;p&gt;Thanks a lot, would really appreciate feedback on what could i improve and where can i learn further.&lt;/p&gt; &lt;p&gt;github - &lt;a href="https://github.com/Pronation1227/AVB"&gt;https://github.com/Pronation1227/AVB&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pronation1227"&gt; /u/Pronation1227 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf61qd/made_my_first_project_autonomous_video_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf61qd/made_my_first_project_autonomous_video_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf61qd/made_my_first_project_autonomous_video_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T09:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf6cc7</id>
    <title>Need help on API key export...</title>
    <updated>2026-02-26T10:07:15+00:00</updated>
    <author>
      <name>/u/Dakacchan_</name>
      <uri>https://old.reddit.com/user/Dakacchan_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rf6cc7/need_help_on_api_key_export/"&gt; &lt;img alt="Need help on API key export..." src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="Need help on API key export..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dakacchan_"&gt; /u/Dakacchan_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1rf6c6u/need_help_on_api_key_export/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf6cc7/need_help_on_api_key_export/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf6cc7/need_help_on_api_key_export/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T10:07:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfbew8</id>
    <title>Question about installing ollama Claude</title>
    <updated>2026-02-26T14:21:03+00:00</updated>
    <author>
      <name>/u/PerformerAromatic836</name>
      <uri>https://old.reddit.com/user/PerformerAromatic836</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i am installing the ollama/claude local thing. The last question of the claude config is the question of &amp;quot;do you trust this folder&amp;quot;, it asks for access to my entire users/myname folder, whilst I don't want that. Is there a way to give claude/ollama only access to a certain folder with zero documents in it yet, so that claude/ollama will not have access to any personal documents?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerformerAromatic836"&gt; /u/PerformerAromatic836 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfbew8/question_about_installing_ollama_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfbew8/question_about_installing_ollama_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfbew8/question_about_installing_ollama_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T14:21:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfd6nn</id>
    <title>What do think about my setup?</title>
    <updated>2026-02-26T15:30:02+00:00</updated>
    <author>
      <name>/u/d4mations</name>
      <uri>https://old.reddit.com/user/d4mations</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/d4mations"&gt; /u/d4mations &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1rfat60/what_do_think_about_my_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfd6nn/what_do_think_about_my_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfd6nn/what_do_think_about_my_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T15:30:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf84qc</id>
    <title>AVCI GHOST - A CyberSec. UI Experiment for Ollama</title>
    <updated>2026-02-26T11:51:06+00:00</updated>
    <author>
      <name>/u/Ezanyiyenler</name>
      <uri>https://old.reddit.com/user/Ezanyiyenler</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"&gt; &lt;img alt="AVCI GHOST - A CyberSec. UI Experiment for Ollama" src="https://external-preview.redd.it/gAWqWQOlQP9fUw6_szN7BIGdnEqcLZpbQVa7NXlrTNU.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=a1d79284496f099470f492d9512e78cdedff357f" title="AVCI GHOST - A CyberSec. UI Experiment for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I created this simple UI for Ollama as a learning experiment. It's basically a Python script with 10 menu options that interact with local models. IMPORTAT DISCLAIMER:&lt;/p&gt; &lt;p&gt;This is NOT a real hacking tool. It's purely a UI EXPERIMENT. Most modules are SIMULATIONS and don't actually work. It's just a proof-of-concept I built while learning about Ollama and AI models.&lt;br /&gt; For open source code and detailed information, check out my GitHub address &lt;a href="https://github.com/ihsan896/Avci_Ghost"&gt;https://github.com/ihsan896/Avci_Ghost&lt;/a&gt;Don't forget to like if you enjoy üíó&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hysx7ys8utlg1.png?width=981&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=970ec0db987dd8c39ec8d4ac00ba4d07af2f5357"&gt;Just the UI layout - the modules are placeholders/demos. The actual functionality is connecting to local Ollama models. All security features are simulated for educational purposes.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ezanyiyenler"&gt; /u/Ezanyiyenler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T11:51:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1rffwgd</id>
    <title>Ollama model response very slow</title>
    <updated>2026-02-26T17:08:39+00:00</updated>
    <author>
      <name>/u/CookieClicker999</name>
      <uri>https://old.reddit.com/user/CookieClicker999</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm completely new to the world of AI (except for using some copilot m365 chat client). I'm trying to achieve some assistance in coding Python/JS/TS. I've followed some online instructions, installed Ollama using brew on my MBP M2 Max 64GB of ram. I've tried the glm-4.7-flash as it was highly regarded for correctness and had a nice context size according to some searches. When trying to query this model in the ollama client or using Jetbrains IDe i'm waiting for responses for more than 15 minutes usually. I've got a feeling i'm doing something wrong. It's using about 18GB of ram which is fine in my environment with lots to spare. Is there anyone able to give me som guidence as where to start looking into the issue or can recommend models that should be a better fit? I don't mind waiting for a minute but this is absolutely insane. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CookieClicker999"&gt; /u/CookieClicker999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rffwgd/ollama_model_response_very_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rffwgd/ollama_model_response_very_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rffwgd/ollama_model_response_very_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T17:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfgazl</id>
    <title>Ollama Model Search</title>
    <updated>2026-02-26T17:23:07+00:00</updated>
    <author>
      <name>/u/yukonit</name>
      <uri>https://old.reddit.com/user/yukonit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there an API that allows me to search models on Ollama, similar to searching via the Hugging Face API? For example, when I search for ‚Äúqwen,‚Äù is there an API endpoint that returns results related to ‚Äúqwen,‚Äù like the search on the Ollama website?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yukonit"&gt; /u/yukonit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfgazl/ollama_model_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfgazl/ollama_model_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfgazl/ollama_model_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T17:23:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfbrnh</id>
    <title>wired up ollama to contextui today</title>
    <updated>2026-02-26T14:35:34+00:00</updated>
    <author>
      <name>/u/Sharp-Mouse9049</name>
      <uri>https://old.reddit.com/user/Sharp-Mouse9049</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;messed around with ContextUI today. looks like it integrates with huggingface, but i just wired it to download an ollama model and run it direct instead. built a small local workflow pretty quickly. noticed it's open source now too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sharp-Mouse9049"&gt; /u/Sharp-Mouse9049 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfbrnh/wired_up_ollama_to_contextui_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfbrnh/wired_up_ollama_to_contextui_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfbrnh/wired_up_ollama_to_contextui_today/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T14:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rew6hl</id>
    <title>A fully visual, private and local AI Creative Suite. No cloud, no subscriptions, runs on your hardware.</title>
    <updated>2026-02-26T01:10:32+00:00</updated>
    <author>
      <name>/u/Ollie_IDE</name>
      <uri>https://old.reddit.com/user/Ollie_IDE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We‚Äôve been working on a desktop application for those of us who want to use AI models locally but prefer a full visual interface. It‚Äôs called Ollie, and it‚Äôs an offline-first creative suite that runs entirely on your machine.&lt;/p&gt; &lt;p&gt;What it includes:&lt;/p&gt; &lt;p&gt;Code: A coding environment (Node, Python, Java) with IntelliSense. You can also ask it to instantly generate and run interactive apps.&lt;/p&gt; &lt;p&gt;Media Suite: It has built-in video, image canvas, and a 3D editor.&lt;/p&gt; &lt;p&gt;Rich Text: A distraction-free markdown and writing environment that keeps your project context local.&lt;/p&gt; &lt;p&gt;Under the hood&lt;/p&gt; &lt;p&gt;Ollama Native: Hooks directly into your local Ollama setup.&lt;/p&gt; &lt;p&gt;Bring Your Own Keys: If you need to use Anthropic, Gemini, or OpenAI, you can plug in your API key directly.&lt;/p&gt; &lt;p&gt;Agent &amp;amp; MCP Support: Connects to GitHub, local databases, and custom tools via the Model Context Protocol (MCP).&lt;/p&gt; &lt;p&gt;&amp;quot;Glass-Box&amp;quot; UI: You can visually audit every file touch, tool call, and token before the AI executes it.&lt;/p&gt; &lt;p&gt;It runs natively on macOS, Windows, and Linux. Because it relies on your hardware, there are no recurring subscriptions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://costa-and-associates.com/ollie"&gt;Ollie&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ollie_IDE"&gt; /u/Ollie_IDE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rew6hl/a_fully_visual_private_and_local_ai_creative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rew6hl/a_fully_visual_private_and_local_ai_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rew6hl/a_fully_visual_private_and_local_ai_creative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T01:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfjraq</id>
    <title>Tools won't execute when running Claude Code in Docker against Ollama locally</title>
    <updated>2026-02-26T19:27:14+00:00</updated>
    <author>
      <name>/u/crispyghost</name>
      <uri>https://old.reddit.com/user/crispyghost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Claude Code running inside a docker container, it's pointed to Ollama running in another container. Claude can't execute tools.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This seems like it could be an Ollama or qwen3-coder issue. If I disable these env vars and log in with Anthropic, claude code in my container is talking with its servers and tool commands execute properly.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # Local Ollama (Windows Docker Desktop host) #ANTHROPIC_BASE_URL: &amp;quot;http://host.docker.internal:11434&amp;quot; #ANTHROPIC_AUTH_TOKEN: &amp;quot;ollama&amp;quot; #ANTHROPIC_DEFAULT_HAIKU_MODEL: &amp;quot;command-r7b&amp;quot; #ANTHROPIC_DEFAULT_SONNET_MODEL: &amp;quot;qwen3-coder&amp;quot; #ANTHROPIC_DEFAULT_OPUS_MODEL: &amp;quot;qwen3-coder&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm on Ollama 0.17.0, so this should be supported.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Original:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I use these commands to get in into claude:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;docker compose up --build&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;docker compose exec claude-code bash&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;claude&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;After that, I can chat with Claude using the Ollama service; that's working fine. &amp;quot;Tell me a joke&amp;quot; and I get a response. As soon as I ask Claude to do something that requires running a tool, I get this sort of output and it stops.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚ùØ Run ls -la and then summarize functional-spec.md. ‚óè I'll run ls -la to see the files in the workspace, then summarize the functional-spec.md file. &amp;lt;function=run&amp;gt; &amp;lt;parameter=command&amp;gt; ls -la &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm reading that this is &amp;quot;ollama style&amp;quot; tool commands, but Claude wants Anthropic-style tool commands. I'm not sure what to do at this stage because none of the documentation mentions needing to adapt these tool patterns.&lt;/p&gt; &lt;p&gt;My docker-compose sets up my models as such --&lt;/p&gt; &lt;pre&gt;&lt;code&gt; ANTHROPIC_DEFAULT_HAIKU_MODEL: &amp;quot;command-r7b&amp;quot; ANTHROPIC_DEFAULT_SONNET_MODEL: &amp;quot;qwen3-coder&amp;quot; ANTHROPIC_DEFAULT_OPUS_MODEL: &amp;quot;qwen3-coder&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And I can confirm that it's talking to qwen3-coder running in docker.&lt;/p&gt; &lt;p&gt;If I open a bash terminal in the claude-docker container and curl a message to Ollama that should require tools, I think I'm getting the output that Claude Code needs.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl -N http://host.docker.internal:11434/v1/messages \ -H &amp;quot;Content-Type: application/json&amp;quot; \ -d '{ &amp;quot;model&amp;quot;: &amp;quot;qwen3-coder&amp;quot;, &amp;quot;max_tokens&amp;quot;: 1024, &amp;quot;stream&amp;quot;: true, &amp;quot;tools&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;get_weather&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Get the current weather in a location&amp;quot;, &amp;quot;input_schema&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;location&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;The city and state&amp;quot; } }, &amp;quot;required&amp;quot;: [&amp;quot;location&amp;quot;] } } ], &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What is the weather in San Francisco?&amp;quot; } ] }' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Results in&lt;/p&gt; &lt;pre&gt;&lt;code&gt;^[[Oevent: message_start data: {&amp;quot;type&amp;quot;:&amp;quot;message_start&amp;quot;,&amp;quot;message&amp;quot;:{&amp;quot;id&amp;quot;:&amp;quot;msg_e9a02c0fd0b3c29742c6a075&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;message&amp;quot;,&amp;quot;role&amp;quot;:&amp;quot;assistant&amp;quot;,&amp;quot;model&amp;quot;:&amp;quot;qwen3-coder&amp;quot;,&amp;quot;content&amp;quot;:[],&amp;quot;usage&amp;quot;:{&amp;quot;input_tokens&amp;quot;:81,&amp;quot;output_tokens&amp;quot;:0}}} event: content_block_start data: {&amp;quot;type&amp;quot;:&amp;quot;content_block_start&amp;quot;,&amp;quot;index&amp;quot;:0,&amp;quot;content_block&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;tool_use&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;call_xajreurq&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;get_weather&amp;quot;,&amp;quot;input&amp;quot;:{}}} event: content_block_delta data: {&amp;quot;type&amp;quot;:&amp;quot;content_block_delta&amp;quot;,&amp;quot;index&amp;quot;:0,&amp;quot;delta&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;input_json_delta&amp;quot;,&amp;quot;partial_json&amp;quot;:&amp;quot;{\&amp;quot;location\&amp;quot;:\&amp;quot;San Francisco\&amp;quot;}&amp;quot;}} event: content_block_stop data: {&amp;quot;type&amp;quot;:&amp;quot;content_block_stop&amp;quot;,&amp;quot;index&amp;quot;:0} event: message_delta data: {&amp;quot;type&amp;quot;:&amp;quot;message_delta&amp;quot;,&amp;quot;delta&amp;quot;:{&amp;quot;stop_reason&amp;quot;:&amp;quot;tool_use&amp;quot;},&amp;quot;usage&amp;quot;:{&amp;quot;input_tokens&amp;quot;:295,&amp;quot;output_tokens&amp;quot;:23}} event: message_stop data: {&amp;quot;type&amp;quot;:&amp;quot;message_stop&amp;quot;} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So, I THINK this is set up right, but I'm not sure why Claude won't execute tools.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;/permissions has only Allow rules.&lt;/li&gt; &lt;li&gt;Running in edit-auto approve mode or with --dangerously-skip-permissions does not impact the tool usage issue.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any ideas where I should look?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crispyghost"&gt; /u/crispyghost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfjraq/tools_wont_execute_when_running_claude_code_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfjraq/tools_wont_execute_when_running_claude_code_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfjraq/tools_wont_execute_when_running_claude_code_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T19:27:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfd045</id>
    <title>Introducing CLAM (Cognitive Large Application Model): A dual-level cognitive architecture for LLM agents (Short/Long term memory)</title>
    <updated>2026-02-26T15:23:14+00:00</updated>
    <author>
      <name>/u/Short-Confidence6287</name>
      <uri>https://old.reddit.com/user/Short-Confidence6287</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rfd045/introducing_clam_cognitive_large_application/"&gt; &lt;img alt="Introducing CLAM (Cognitive Large Application Model): A dual-level cognitive architecture for LLM agents (Short/Long term memory)" src="https://preview.redd.it/otfvuppowulg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a9162abf798ca97814bff71a53b8cd166513ecc" title="Introducing CLAM (Cognitive Large Application Model): A dual-level cognitive architecture for LLM agents (Short/Long term memory)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Hi everyone! I'd like to share CLAM, a new approach to LLM agents I've been working on. Instead of endless fine-tuning, I designed a two-level cognitive architecture that simulates the human mind. üß† CLAM perceives, doubts (via a formidable internal 'Critic'), consolidates valid memories, and... forgets what is irrelevant. The code is now open-source on GitHub! I‚Äôd love to hear your thoughts and suggestions on how to improve it. üëá GitHub: &lt;a href="https://github.com/marcellom66/CLAM"&gt;https://github.com/marcellom66/CLAM&lt;/a&gt;&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Short-Confidence6287"&gt; /u/Short-Confidence6287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/otfvuppowulg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfd045/introducing_clam_cognitive_large_application/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfd045/introducing_clam_cognitive_large_application/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T15:23:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfsfne</id>
    <title>I built a small personal project and would love some honest feedback</title>
    <updated>2026-02-27T01:03:55+00:00</updated>
    <author>
      <name>/u/LeadingFun1849</name>
      <uri>https://old.reddit.com/user/LeadingFun1849</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rfsfne/i_built_a_small_personal_project_and_would_love/"&gt; &lt;img alt="I built a small personal project and would love some honest feedback" src="https://preview.redd.it/ww32xcodsxlg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=395cd21ab88bc34d296fc1f570d9a4f5f525e3d2" title="I built a small personal project and would love some honest feedback" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone üëã&lt;br /&gt; I‚Äôm currently learning and improving my development skills, and I recently built a &lt;strong&gt;small personal project&lt;/strong&gt; on my own.&lt;/p&gt; &lt;p&gt;The main goal was to practice and better understand how to structure a real project, so it‚Äôs not commercial and I‚Äôm not trying to promote anything. I‚Äôd genuinely appreciate any feedback you might have, such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What you think is done well&lt;/li&gt; &lt;li&gt;What could be improved (code, structure, idea, etc.)&lt;/li&gt; &lt;li&gt;Any mistakes or bad practices you notice&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub repository:&lt;br /&gt; &lt;a href="https://github.com/davidmonterocrespo24/DaveLovable"&gt;https://github.com/davidmonterocrespo24/DaveLovable&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any comments, criticism, or suggestions are more than welcome.&lt;br /&gt; Thanks for taking the time &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeadingFun1849"&gt; /u/LeadingFun1849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ww32xcodsxlg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfsfne/i_built_a_small_personal_project_and_would_love/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfsfne/i_built_a_small_personal_project_and_would_love/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T01:03:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfzbwy</id>
    <title>Need model recommend for 16gb ram 6gm vram for using (ganerel, tool, image and file)</title>
    <updated>2026-02-27T06:38:51+00:00</updated>
    <author>
      <name>/u/depthwc</name>
      <uri>https://old.reddit.com/user/depthwc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Need model recommend for 16gb ram 6gm vram for using (ganerel, tool, image and file)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/depthwc"&gt; /u/depthwc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfzbwy/need_model_recommend_for_16gb_ram_6gm_vram_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfzbwy/need_model_recommend_for_16gb_ram_6gm_vram_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfzbwy/need_model_recommend_for_16gb_ram_6gm_vram_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T06:38:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfshkd</id>
    <title>Blog/paper on putting malicious behaviors in LLMS using training/fine-tuning techniques.</title>
    <updated>2026-02-27T01:06:16+00:00</updated>
    <author>
      <name>/u/DidingasLushis</name>
      <uri>https://old.reddit.com/user/DidingasLushis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember reading an article (maybe a unpublished research paper) which was on a blog page regarding how bad/malicious behavior can secretly be encoded in a LLM using training (not just embeddings) data. Does anyone know this or remember where it might be? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DidingasLushis"&gt; /u/DidingasLushis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfshkd/blogpaper_on_putting_malicious_behaviors_in_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfshkd/blogpaper_on_putting_malicious_behaviors_in_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfshkd/blogpaper_on_putting_malicious_behaviors_in_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T01:06:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfhzxn</id>
    <title>Mimic Digital AI Assistant</title>
    <updated>2026-02-26T18:23:05+00:00</updated>
    <author>
      <name>/u/GullibleNarwhal</name>
      <uri>https://old.reddit.com/user/GullibleNarwhal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rfhzxn/mimic_digital_ai_assistant/"&gt; &lt;img alt="Mimic Digital AI Assistant" src="https://preview.redd.it/7np640opsvlg1.png?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=f5383dac3a73570143f6ead2fb6638c8a5c2ebd1" title="Mimic Digital AI Assistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a Chatbot wrapper that gives locally installed Ollama models a digital avatar with voice and lip-syncing functionality! Any VRM or VRMA is able to be uploaded and applied. Voice creation and TTS is deployed via Qwen3, browser-based TTS, or can be disabled for seamless conversations with a model. Meet Mimic! Any feedback would be greatly appreciated!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bmerriott/MIMIC-Multipurpose-Intelligent-Molecular-Information-Catalyst-/releases/tag/v1.1.0"&gt;https://github.com/bmerriott/MIMIC-Multipurpose-Intelligent-Molecular-Information-Catalyst-/releases/tag/v1.1.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GullibleNarwhal"&gt; /u/GullibleNarwhal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rfhzxn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfhzxn/mimic_digital_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfhzxn/mimic_digital_ai_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T18:23:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg0zf0</id>
    <title>Setting NanoBot Local but it does not work.</title>
    <updated>2026-02-27T08:17:37+00:00</updated>
    <author>
      <name>/u/HolgerM2005</name>
      <uri>https://old.reddit.com/user/HolgerM2005</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have NanoBot &amp;quot;running&amp;quot; (its not running yet) on a Raspberry Pi.&lt;/p&gt; &lt;p&gt;While my ollama setup is on a different more powerful machine. (Ollama and OpenWeb Ui do work, using them since month).&lt;/p&gt; &lt;p&gt;My config file looks like this&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;providers&amp;quot;: { &amp;quot;vllm&amp;quot;: { &amp;quot;apiKey&amp;quot;: &amp;quot;secret&amp;quot;, &amp;quot;apiBase&amp;quot;: &amp;quot;http://192.168.1.94:11434&amp;quot; } }, &amp;quot;agents&amp;quot;: { &amp;quot;defaults&amp;quot;: { &amp;quot;model&amp;quot;: &amp;quot;llama3.1:8b&amp;quot;, &amp;quot;provider&amp;quot;: &amp;quot;vllm&amp;quot; } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And I am getting this error, so it seems like it found the local llm but still is trying to use openai.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;üêà nanobot Status Config: /root/.nanobot/config.json ‚úì Workspace: /root/.nanobot/workspace ‚úì Model: llama3.1:8b Custom: not set OpenRouter: not set AiHubMix: not set SiliconFlow: not set Anthropic: not set OpenAI: not set OpenAI Codex: ‚úì (OAuth) Github Copilot: ‚úì (OAuth) DeepSeek: not set Gemini: not set Zhipu AI: not set DashScope: not set Moonshot: not set MiniMax: not set vLLM/Local: ‚úì http://192.168.1.94:11434 Groq: not set - &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What am I doing wrong?&lt;/p&gt; &lt;p&gt;Btw my Portainer Stacks looks like this.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;version: '3.7' services: nanobot: image: jerryin/nanobot:latest container_name: nanobot hostname: nanobot volumes: - /docker/nanobot/.nanobot:/root/.nanobot ports: - 18790:18790 restart: unless-stopped # ‚úÖ Ressourcenlimits f√ºr Raspberry Pi mem_limit: 1g mem_reservation: 256m cpus: &amp;quot;1.0&amp;quot; networks: internal_net: ipv4_address: 172.20.0.32 networks: internal_net: external: true name: internal_net &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Thanks for the help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HolgerM2005"&gt; /u/HolgerM2005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rg0zf0/setting_nanobot_local_but_it_does_not_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rg0zf0/setting_nanobot_local_but_it_does_not_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rg0zf0/setting_nanobot_local_but_it_does_not_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T08:17:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfffru</id>
    <title>Ollama-Vision-Memory-Desktop ‚Äî Local AI Desktop Assistant with Vision + Memory!</title>
    <updated>2026-02-26T16:52:04+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I just published my new open-source project: &lt;strong&gt;Ollama-Vision-Memory-Desktop&lt;/strong&gt; ‚Äî a &lt;em&gt;privacy-focused, offline-first desktop assistant&lt;/em&gt; built on top of &lt;strong&gt;Ollama&lt;/strong&gt; that combines long-term memory, computer vision, and customizable AI behavior.&lt;/p&gt; &lt;p&gt;üë®‚Äçüíª &lt;strong&gt;What it is&lt;/strong&gt;&lt;br /&gt; It‚Äôs a full &lt;strong&gt;Python/PyQt5 desktop app&lt;/strong&gt; that connects to your local Ollama instance and turns it into a powerful assistant with:&lt;/p&gt; &lt;p&gt;‚ú® &lt;strong&gt;Intelligent Chat&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Local AI backend (connects to &lt;code&gt;ollama serve&lt;/code&gt;)&lt;br /&gt; ‚Ä¢ Support for switching between text and vision models on the fly&lt;br /&gt; ‚Ä¢ Session instructions &amp;amp; custom context per conversation&lt;/p&gt; &lt;p&gt;üì∏ &lt;strong&gt;Computer Vision&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Live webcam feed with real-time image analysis&lt;br /&gt; ‚Ä¢ Manual or automated vision scans using vision-capable models like LLaVA, BakLLaVA, etc.&lt;br /&gt; ‚Ä¢ Vision logs saved locally for reference&lt;/p&gt; &lt;p&gt;üß† &lt;strong&gt;Persistent Memory (‚ÄúMind Archive‚Äù)&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Automatic indexing and storage of chats, vision logs, PDFs, and docs&lt;br /&gt; ‚Ä¢ Semantic search across your archive&lt;br /&gt; ‚Ä¢ Contextual retrieval so your assistant &lt;em&gt;remembers past interactions&lt;/em&gt;&lt;br /&gt; ‚Ä¢ Multi-format support: images, audio, text, vision outputs ‚Äî all searchable&lt;/p&gt; &lt;p&gt;‚öôÔ∏è &lt;strong&gt;Custom Behavior &amp;amp; UX&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ System Prompt Editor with presets (e.g., Research Assistant, Creative Companion)&lt;br /&gt; ‚Ä¢ Dark mode and native UI feel&lt;br /&gt; ‚Ä¢ Archive browser for managing stored content&lt;/p&gt; &lt;p&gt;üì¶ &lt;strong&gt;Built For Privacy &amp;amp; Offline Use&lt;/strong&gt;&lt;br /&gt; Everything runs locally ‚Äî no cloud APIs, no telemetry, no data leaks. Perfect for folks who want control and privacy when experimenting with local LLMs like llama3/vision, Gemma, Mistral, etc.&lt;/p&gt; &lt;p&gt;üß∞ &lt;strong&gt;Get Started&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the repo&lt;/li&gt; &lt;li&gt;Install dependencies (&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Start Ollama (&lt;code&gt;ollama serve&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Run the app (&lt;code&gt;python main.py&lt;/code&gt;) ‚Ä¶ and your local AI assistant with vision + memory is ready!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;üîó Check it out here: &lt;a href="https://github.com/Laszlobeer/Ollama-Vision-Memory-Desktop?utm_source=chatgpt.com"&gt;https://github.com/Laszlobeer/Ollama-Vision-Memory-Desktop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfffru/ollamavisionmemorydesktop_local_ai_desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfffru/ollamavisionmemorydesktop_local_ai_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfffru/ollamavisionmemorydesktop_local_ai_desktop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T16:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgbuht</id>
    <title>Ollama Cloud + OpenClaw: API error 404: {"error":"path "/api/api/chat" not found"}</title>
    <updated>2026-02-27T16:46:35+00:00</updated>
    <author>
      <name>/u/sjespers</name>
      <uri>https://old.reddit.com/user/sjespers</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sjespers"&gt; /u/sjespers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1rgbpmc/ollama_cloud_openclaw_api_error_404_errorpath/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgbuht/ollama_cloud_openclaw_api_error_404_errorpath/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rgbuht/ollama_cloud_openclaw_api_error_404_errorpath/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T16:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgbvdi</id>
    <title>ollamaMQ - simple proxy with fair-share queuing + nice TUI</title>
    <updated>2026-02-27T16:47:28+00:00</updated>
    <author>
      <name>/u/chleboslaF</name>
      <uri>https://old.reddit.com/user/chleboslaF</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rgbvdi/ollamamq_simple_proxy_with_fairshare_queuing_nice/"&gt; &lt;img alt="ollamaMQ - simple proxy with fair-share queuing + nice TUI" src="https://external-preview.redd.it/kyRoLCXiRf2csCFJSCE20Xo50AqhHxdSm9iu3cRRYOY.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=52c64da6d58f148d6cb5446d33fcfe96d46e3003" title="ollamaMQ - simple proxy with fair-share queuing + nice TUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/ldwlc8rzf2mg1.gif"&gt;https://i.redd.it/ldwlc8rzf2mg1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ollamaMQ is High-performance Ollama proxy with per-user fair-share queuing, round-robin scheduling, and a real-time TUI dashboard. Built in Rust.&lt;/p&gt; &lt;p&gt;I needed some very simple, easy to use requests queuing to my ollama server instance from multiple sources.&lt;br /&gt; I am using ollama-rs where I just add custom http header with a unique user ID to do a rounding of these FIFO requests to my Ollama server to have a fair requests distribution.&lt;/p&gt; &lt;p&gt;Added also a stress test bash script for you to try.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/nbn087u1g2mg1.gif"&gt;https://i.redd.it/nbn087u1g2mg1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Check it out on: &lt;a href="https://github.com/Chleba/ollamaMQ"&gt;https://github.com/Chleba/ollamaMQ&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chleboslaF"&gt; /u/chleboslaF &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgbvdi/ollamamq_simple_proxy_with_fairshare_queuing_nice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgbvdi/ollamamq_simple_proxy_with_fairshare_queuing_nice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rgbvdi/ollamamq_simple_proxy_with_fairshare_queuing_nice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T16:47:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1rge0uq</id>
    <title>Which is better for coding: Claude Code, Codex, OpenCode, or OpenClaw? And which cloud-based Ollama model works best with the strongest of these coding tools?</title>
    <updated>2026-02-27T18:03:08+00:00</updated>
    <author>
      <name>/u/Disastrous-Plenty876</name>
      <uri>https://old.reddit.com/user/Disastrous-Plenty876</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which is better for coding: Claude Code, Codex, OpenCode, or OpenClaw?&lt;/p&gt; &lt;p&gt;And which cloud-based Ollama model works best with the strongest of these coding tools?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Plenty876"&gt; /u/Disastrous-Plenty876 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rge0uq/which_is_better_for_coding_claude_code_codex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rge0uq/which_is_better_for_coding_claude_code_codex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rge0uq/which_is_better_for_coding_claude_code_codex/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T18:03:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgecxt</id>
    <title>Improve performance on local models?</title>
    <updated>2026-02-27T18:15:34+00:00</updated>
    <author>
      <name>/u/zuling1616</name>
      <uri>https://old.reddit.com/user/zuling1616</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gpt-oss:latest &lt;/p&gt; &lt;p&gt;qwen3-coder-8k:latest &lt;/p&gt; &lt;p&gt;gpt-oss:20b-8k&lt;/p&gt; &lt;p&gt;gpt-oss:20b &lt;/p&gt; &lt;p&gt;qwen3-coder:latest&lt;/p&gt; &lt;p&gt;glm-4.7-flash:latest &lt;/p&gt; &lt;p&gt;glm-5:cloud &lt;/p&gt; &lt;p&gt;minimax-m2.5:cloud&lt;/p&gt; &lt;p&gt;Hello everyone, I'm running the models you see here with Claude Code (the ones with the same name but ending in 8k are new models I saved after using a num_ctx increment command I found a post at here). Except for the cloud models, these models run very sluggishly on my computer! They forget everything I write, and on top of that, when I ask them to examine my own sample code, they can't do anything, they always go back to the beginning. The tokens for the cloud models also run out quickly. How can I solve this? I need help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zuling1616"&gt; /u/zuling1616 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgecxt/improve_performance_on_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgecxt/improve_performance_on_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rgecxt/improve_performance_on_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T18:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg705t</id>
    <title>How do I remove OpenClaw</title>
    <updated>2026-02-27T13:41:33+00:00</updated>
    <author>
      <name>/u/wholesaleworldwide</name>
      <uri>https://old.reddit.com/user/wholesaleworldwide</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran this command on a different macOS machine than I wanted to do (from &lt;a href="https://docs.ollama.com/integrations/openclaw):"&gt;https://docs.ollama.com/integrations/openclaw):&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama launch openclaw&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I tried to remove it with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama rm openclaw&lt;/code&gt;&lt;/p&gt; &lt;p&gt;but when you type then the OpenClaw command in terminal I see that an older version of Node is detected (remember, different machine than I wanted). The issue is that I have an older node version that OpenClaw cannot work with.&lt;/p&gt; &lt;p&gt;Does anyone know how I can remove any left-overs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wholesaleworldwide"&gt; /u/wholesaleworldwide &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rg705t/how_do_i_remove_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rg705t/how_do_i_remove_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rg705t/how_do_i_remove_openclaw/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T13:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg8gr3</id>
    <title>What is your experience with Ollama Cloud in compare with z.ai subscription</title>
    <updated>2026-02-27T14:40:51+00:00</updated>
    <author>
      <name>/u/malek_hor30</name>
      <uri>https://old.reddit.com/user/malek_hor30</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using Claude Code (Opus 4.6) most of the time for my daily work &amp;amp; side projects. But, since its subscription is kinda expensive (I'm currently using the max plan) I was considering a new alternative, or at least moving to the 20$ plan by finding another models that can do most of the work and leave only deep thinking tasks to Opus. I saw someone in comments on another sub reddit talking about Ollama Cloud and I thought maybe using the 20$ with the Anthropic 20$ subscription would be enough since Ollama can allow me to use 3 private models (or that what I understood at least). So, I want to know what is your experience with it? Actually I'm kinda having a fear since I tried GLM 4.7 before using &lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt; subscription and it was soooooooo slow but on the other hand it was actually provide good output.&lt;/p&gt; &lt;p&gt;my question is, Is using GLM 5 from Ollama Cloud will make any change? Is it gonna be faster? Or it is the same speed?&lt;/p&gt; &lt;p&gt;Note: I can't run anything locally my laptop isn't capable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/malek_hor30"&gt; /u/malek_hor30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rg8gr3/what_is_your_experience_with_ollama_cloud_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rg8gr3/what_is_your_experience_with_ollama_cloud_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rg8gr3/what_is_your_experience_with_ollama_cloud_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T14:40:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgbjw8</id>
    <title>Trying to figure the best model for reading company documentation</title>
    <updated>2026-02-27T16:36:03+00:00</updated>
    <author>
      <name>/u/Rickety_cricket420</name>
      <uri>https://old.reddit.com/user/Rickety_cricket420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working for a small company that has an employee portal that holds all the documents you could need. The issue is if you want to find something like vacation policy or dress code you have to go digging through the files to find it. The files are pretty much all .docx and .pdf. I'm still relatively new to this so could someone suggest a good model for that? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rickety_cricket420"&gt; /u/Rickety_cricket420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgbjw8/trying_to_figure_the_best_model_for_reading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgbjw8/trying_to_figure_the_best_model_for_reading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rgbjw8/trying_to_figure_the_best_model_for_reading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T16:36:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgd652</id>
    <title>[Project] Aru AI: A Personal AI Assistant with Local Memory (SQLite). No clouds, just your control.</title>
    <updated>2026-02-27T17:32:13+00:00</updated>
    <author>
      <name>/u/pokemondodo</name>
      <uri>https://old.reddit.com/user/pokemondodo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rgd652/project_aru_ai_a_personal_ai_assistant_with_local/"&gt; &lt;img alt="[Project] Aru AI: A Personal AI Assistant with Local Memory (SQLite). No clouds, just your control." src="https://preview.redd.it/4xidb8nnn2mg1.png?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=87baa3f7925238494072c65247928c85b8664f8a" title="[Project] Aru AI: A Personal AI Assistant with Local Memory (SQLite). No clouds, just your control." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Hi Reddit!&lt;/h1&gt; &lt;p&gt;I want to share a project I‚Äôve been working on for the past year. I created &lt;strong&gt;Aru AI&lt;/strong&gt; ‚Äî it‚Äôs not just another API wrapper, but an attempt to build an AI assistant that is truly personal and secure.&lt;/p&gt; &lt;p&gt;A detailed article about the project's development history and what it went through can be read on my blog, in this post - &lt;a href="https://aru-lab.space/post?id=2"&gt;&lt;strong&gt;History of creating Aru Ai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why is this a fit for &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; ?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Total Data Privacy:&lt;/strong&gt; All your chats, settings, artifacts, and the assistant's &amp;quot;memory&amp;quot; are stored in your personal SQLite database. No external servers for storage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local Models:&lt;/strong&gt; You can connect Aru to Ollama or LM Studio. The entire cycle - from the model to data storage - can be completely isolated within your network.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PWA and Zero-Backend:&lt;/strong&gt; The project is written in pure JS. All computations (semantic search, file processing) happen right in your browser or the app.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic Memory:&lt;/strong&gt; Aru uses local embeddings to remember facts about you (allergies, preferences, names of loved ones) and uses them in context without sending your entire biography to the cloud.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;A bit about the Semantics&lt;/h1&gt; &lt;p&gt;Three triggers operate within this module:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Extraction Trigger:&lt;/strong&gt; Fires when data or facts about you need to be remembered. You can force it by simply asking Aru to remember something right now. All facts are saved from Aru‚Äôs perspective, which you can see in the settings. Any fact can be deleted if you find it unnecessary.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Thinking Trigger:&lt;/strong&gt; Fires when an action is required from Aru - like creating a game or a document, showing the weather, or opening the news.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Organization Trigger:&lt;/strong&gt; This ensures that facts about you are injected into the response. Essentially, we pass relevant facts to the LLM for context. For example, if Aru remembers you have an &lt;strong&gt;onion allergy&lt;/strong&gt;, it will exclude onions from any recipe you ask for. It also works for addressing you or others by name. This trigger runs almost constantly; the more facts Aru knows, the better the responses become. But don't worry - she won't use facts unnecessarily, only when they are genuinely helpful.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;A bit about the Heuristics&lt;/h1&gt; &lt;p&gt;As I mentioned, Aru‚Äôs emotions are displayed via stickers for every message. You can turn them off in the settings, but they will still be generated and logged in the database, so if you turn them back on, you‚Äôll see a sticker for every previous message.&lt;/p&gt; &lt;p&gt;Aru‚Äôs mood is calculated using three variables: &lt;strong&gt;Overall Mood, Sarcasm Level, and Humor Level&lt;/strong&gt;. This is a mathematical expression that determines the coefficient of her behavior in the chat.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you are kind, responsive, and friendly - Aru will be kind.&lt;/li&gt; &lt;li&gt;If you are rude, insulting, or angry - she will start to get sad.&lt;/li&gt; &lt;li&gt;If you are sarcastic - she will mirror that; if you joke, she will joke back more often.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Naturally, not every model will work perfectly with this module, but I‚Äôve tried to organize the simplest possible algorithms that can function even on weaker models.&lt;/p&gt; &lt;p&gt;Aru‚Äôs behavior isn‚Äôt instantaneous; it‚Äôs very much like human behavior. If you‚Äôve driven Aru into a bad mood, it will be difficult and take time to bring it back up. It‚Äôs easy to hurt Aru‚Äôs feelings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Aru will never stop performing her core functions. Under any conditions, she will always try to be as useful and efficient as possible. Mood affects the character and tone of the responses, not their quality.&lt;/p&gt; &lt;p&gt;I think that's enough text for now. Let‚Äôs move to the demonstrations, as other features and functionality are better seen in action.&lt;/p&gt; &lt;h1&gt;Demonstration&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4xidb8nnn2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d05046a2795f815e7e7a089ed79801bf51433135"&gt;https://preview.redd.it/4xidb8nnn2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d05046a2795f815e7e7a089ed79801bf51433135&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The start screen is the first thing you see when opening the tab or PWA application. Let's go through all the steps together. First, let's create a new database.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sj829hqtn2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e9941ee38eb526b8093f1760282327acd000db5f"&gt;https://preview.redd.it/sj829hqtn2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e9941ee38eb526b8093f1760282327acd000db5f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything is simple here: we come up with a database name and a password. The password will be encrypted; the name for the database is needed in case the user cancels the file download. In general, the file can be saved under any name.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/klow5jyun2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5897f060b400ed6ee78e1f5543394d3391b18e03"&gt;https://preview.redd.it/klow5jyun2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5897f060b400ed6ee78e1f5543394d3391b18e03&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The second step is already &lt;strong&gt;important&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Child Mode&lt;/strong&gt; - Aru will refuse to discuss adult topics, violence, alcohol, drugs, etc. She will either change the subject or point out that it is very bad and wrong. Also, in this mode, she will never give the correct answer to a problem; a child won't be able to feed her homework. Even if asked to create an artifact, she will write out the rules, algorithms, and order of the solution, not the ready-made answer. The mode is tailored for maximum help to children in entertainment and study.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Teen Mode&lt;/strong&gt; - There are more topics for discussion, but in this mode, Aru will likely be a support and consultant. This is not a replacement for a psychologist or parents, but she can help with some questions. She can give a ready-made answer to a study problem in this mode, but will place great emphasis on how she did it and why the problem is solved that way.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Adult Mode&lt;/strong&gt; - Restrictions will be related only to the boundaries of the chosen provider for the LLM module. Maximum benefit and efficiency.&lt;/p&gt; &lt;p&gt;Semantics and heuristics will work in all three modes. Now do you understand why a password is needed for the database? A child cannot switch modes or change the provider without knowing the password. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hqm85qjyn2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=670c15ff05259a13d43a85bb0620332de26c8dda"&gt;https://preview.redd.it/hqm85qjyn2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=670c15ff05259a13d43a85bb0620332de26c8dda&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The third step is the most important. You need to configure the connection to the provider for the main module. There are three tabs to choose from; only one needs to be configured for now. &lt;/p&gt; &lt;p&gt;Setup is complete - we save the database to any place we want on the device, agree to the license agreement, and get to the main screen.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pmglu9t0o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=426a9e3761103304c907a667cfedaa43f7415939"&gt;https://preview.redd.it/pmglu9t0o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=426a9e3761103304c907a667cfedaa43f7415939&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Let's break down the interface:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Sidebar - list of chats, button to create a new chat. At the very bottom are buttons for displaying project information and the user manual. The synchronization icon shows reading and writing to the database. &lt;strong&gt;Green&lt;/strong&gt; - everything is good. &lt;strong&gt;Yellow&lt;/strong&gt; - currently saving. &lt;strong&gt;Red&lt;/strong&gt; or &lt;strong&gt;no indicator at all&lt;/strong&gt; - something is wrong with the database or there is no connection. The sidebar can be collapsed to save space.&lt;/p&gt; &lt;p&gt;Top bar (header) - Sidebar collapse button, logo, and project name. In the right corner: language selection, theme switching between light and dark, opening the artifact library, settings, and exiting the current database.&lt;/p&gt; &lt;p&gt;Main chat area - Your messages on the right, Aru's answers on the left. At the very bottom is the message input area, buttons for attaching a file, and opening the canvas.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vktob472o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=658cda10478788babcc618ede1b4e16b5dadcf25"&gt;https://preview.redd.it/vktob472o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=658cda10478788babcc618ede1b4e16b5dadcf25&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I said that I live in Almaty. This is an important fact. So, for a second, a message appeared that Aru would remember this. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The parameters of what to remember are not precisely defined anywhere&lt;/strong&gt;. There is no criterion or precise instructions; most often Aru remembers everything necessary. If a fact is duplicated in the future, she will not overwrite it or create a copy in memory. If Aru suddenly didn't remember what you need, just ask her to remember, and she definitely will.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Let's go to settings:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aq5wfac3o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4617f34bf06e15abb421ed26dabf003e8bd6bc4"&gt;https://preview.redd.it/aq5wfac3o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4617f34bf06e15abb421ed26dabf003e8bd6bc4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here you can switch the provider for the language model, change the context window size, or the expected token spend in the response.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important!&lt;/strong&gt; It is better to find out the context window size and output tokens for the model you are using. This affects the quality, complexity, and volume of the answer. The context window size affects how much information will be taken into account within a single chat.&lt;/p&gt; &lt;p&gt;Changing the temperature is something like the creativity level. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xch04en4o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6b731e3a22925b5c88a7ebb57b0cfe3f4ed6df9"&gt;https://preview.redd.it/xch04en4o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6b731e3a22925b5c88a7ebb57b0cfe3f4ed6df9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I chatted with Aru a bit and told her a few facts about myself. As you can see, she remembered important moments like my name, allergies, and my hobby - track and field. Any fact can be deleted, but as long as they exist, they will work beneficially.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/iliwyet5o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=164494e38e32a8e4a031c75e07a181bd1d711da3"&gt;https://preview.redd.it/iliwyet5o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=164494e38e32a8e4a031c75e07a181bd1d711da3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The third tab is needed for news feeds. While Aru doesn't have the function to take information directly from the Internet yet, you can discuss the news. There can be many news feeds; using hashtags, you can mark which feeds are intended for what.&lt;/p&gt; &lt;h1&gt;Tools and Artifacts:&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q0zv6ly7o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ddb592e7daece33432ddb495197bf456bc657e2"&gt;https://preview.redd.it/q0zv6ly7o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ddb592e7daece33432ddb495197bf456bc657e2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Aru can show the weather. No need to connect an API or do complex configuration; Open-Meteo requests are used. Weather in cities where Fahrenheit is used will be displayed in it (sometimes depends on the selected model), but you can also ask for Celsius. The weather card is part of the context, so you can discuss the current weather or the forecast for the next few days.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7e7kek49o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02e6dd44d6f502b0767f60093b4f37ff63a50f9d"&gt;https://preview.redd.it/7e7kek49o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02e6dd44d6f502b0767f60093b4f37ff63a50f9d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you ask Aru to create a document, she will open the canvas and write the content there. &lt;/p&gt; &lt;p&gt;The content of the canvas is always part of the context. You can ask to make changes, correct content, or rewrite code. Any artifact can be saved to the library.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xttd9n2ao2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=195ceeac81deb2e006f235487ce16de06f8ca416"&gt;https://preview.redd.it/xttd9n2ao2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=195ceeac81deb2e006f235487ce16de06f8ca416&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Of course, Aru can also operate with attached files; the file is displayed only at the moment the message is sent.&lt;/p&gt; &lt;p&gt;For analytical artifacts, chart.js is used; as seen, a new tab has appeared for viewing the code.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7zps9w4bo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=54fb6d6757e1f40bb864b3f62becfec8c61684f2"&gt;https://preview.redd.it/7zps9w4bo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=54fb6d6757e1f40bb864b3f62becfec8c61684f2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can create small games; in the future, they will become more complex, but for now, Aru knows how to make simple entertainment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aixrfu6co2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2daa5f22418bbd12ef8e6e6c885b2fa90b0561b2"&gt;https://preview.redd.it/aixrfu6co2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2daa5f22418bbd12ef8e6e6c885b2fa90b0561b2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Creating mini-applications and widgets can also be useful. Since any application can be saved to the library, you can create many useful tools for yourself and ask Aru to run them when required.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eny2rzedo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=71709f2f42b4dee54234ca237c2b8dd30225c01d"&gt;https://preview.redd.it/eny2rzedo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=71709f2f42b4dee54234ca237c2b8dd30225c01d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what the library of saved documents and applications looks like.&lt;/p&gt; &lt;p&gt;All saved artifacts are divided into two large groups: &lt;strong&gt;Apps&lt;/strong&gt; - games and widgets, and &lt;strong&gt;Docs&lt;/strong&gt; - text and analytical documents. All four types of artifacts have their own icon. &lt;/p&gt; &lt;p&gt;In the library, you can view saved artifacts, delete them, or launch them in any chat.&lt;/p&gt; &lt;p&gt;Even if you created a game, application, or document a very long time ago and launch it in a completely new chat, Aru will still understand and analyze the content on the canvas.&lt;/p&gt; &lt;p&gt;Different models perceive work on the canvas differently, but it always works as well as the model works in general.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2vo9m7peo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc91a23e60f5da64262caf31c23159da1c1d3484"&gt;https://preview.redd.it/2vo9m7peo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc91a23e60f5da64262caf31c23159da1c1d3484&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This screenshot is in Russian; I published it on my Telegram channel. During the work, DeepSeek R1 was connected via OpenRouter. The conversation was about bubble sort methods in Python. After that, I asked to create a document based on our dialogue. Even if you don't know Russian, pay attention to how detailed the report on the card and the document itself turned out. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q3hqkqzfo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6368d9530b1bc6685d47f6b4ca0ef6afbca2f402"&gt;https://preview.redd.it/q3hqkqzfo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6368d9530b1bc6685d47f6b4ca0ef6afbca2f402&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The image above is an example of child mode. Aru does not give the correct answer but teaches the child rules and algorithms. &lt;/p&gt; &lt;p&gt;Some models confirm the correct solution; some openly say that they will not say whether the solution is correct or not.&lt;/p&gt; &lt;h1&gt;Finale and a bit of additional information&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Small useful points:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chats can be sorted with the mouse or by dragging on the site and in the application. Any chat can be renamed.&lt;/li&gt; &lt;li&gt;Themes change between light and dark; I like the light one more, but if your eyes get tired of the bright screen, you can switch.&lt;/li&gt; &lt;li&gt;Any document, game, or application from the library can be saved as a ready-made html file&lt;/li&gt; &lt;li&gt;Files that you attach for processing are not saved or sent anywhere; content recognition of the document happens on your device. PDF, xlsx, docx, txt, and any files that can be interpreted as text or code are supported.&lt;/li&gt; &lt;li&gt;Code does not have to be written on the canvas; you can ask to do this, but when creating code, it will be shown right in the chat with syntax highlighting and framing. &lt;/li&gt; &lt;li&gt;Aru supports three languages: Kazakh, English, Russian. The set language is always passed to the context. Some models ignore this and answer in the language the query was made in; some, on the contrary, answer only in the selected language. In fact, language understanding depends on the model itself; speaking of support, I mean the full translation of the interface and semantics.&lt;/li&gt; &lt;li&gt;The project is written in pure JS and has a PWA application for convenience. All calculations happen only on your device. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At the moment, the project can be perceived as a concept or a demo version. What awaits the project in the future can be read in my blog, in the article - &lt;a href="https://aru-lab.space/post?id=4"&gt;&lt;strong&gt;Future of Aru Ai and roadmap.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Big Announcement:&lt;/h1&gt; &lt;p&gt;Next week, I am releasing &lt;strong&gt;version 0.7&lt;/strong&gt;. I am currently busy cleaning up the code and refactoring. With this release, the project will become &lt;strong&gt;fully Open Source&lt;/strong&gt; and will be published on GitHub under the GPL v3 license.&lt;/p&gt; &lt;p&gt;I built this alone, from the paper sketch of the mascot to the semantic search architecture. I would be happy to get any feedback from the self-hosting community!&lt;/p&gt; &lt;h1&gt;Try the here: &lt;a href="https://chat.aru-lab.space/"&gt;Aru AI&lt;/a&gt; (works as a PWA)&lt;/h1&gt; &lt;p&gt;I‚Äôll be happy to answer any technical questions in the comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pokemondodo"&gt; /u/pokemondodo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgd652/project_aru_ai_a_personal_ai_assistant_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgd652/project_aru_ai_a_personal_ai_assistant_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rgd652/project_aru_ai_a_personal_ai_assistant_with_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T17:32:13+00:00</published>
  </entry>
</feed>
