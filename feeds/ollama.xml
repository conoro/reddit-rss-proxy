<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-13T23:45:42+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1r3j90w</id>
    <title>Claude code 100% free - one command install (no API Key)</title>
    <updated>2026-02-13T07:51:13+00:00</updated>
    <author>
      <name>/u/rvorine</name>
      <uri>https://old.reddit.com/user/rvorine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r3j90w/claude_code_100_free_one_command_install_no_api/"&gt; &lt;img alt="Claude code 100% free - one command install (no API Key)" src="https://external-preview.redd.it/FvRQv_496iS2-J5G-De6rt5VPPppPKauL-lZsFV2Lb0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e6fcb3ba1fd62de69ab981c667feaf2f78a6a3c" title="Claude code 100% free - one command install (no API Key)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude code 100% free and local using ollama, and this one-line installer makes it even easier to install and configure without doing anything manually.&lt;/p&gt; &lt;p&gt;We all know, setting environment variables to PATH can be tricky, specially for beginners, one mistake and it won‚Äôt run. &lt;/p&gt; &lt;p&gt;Thant‚Äôs why this one-line installer do all the heavy lifting so that you can enjoy Claude code cli for free and don‚Äôt worry about looking all documentation of ollama how to install. &lt;/p&gt; &lt;p&gt;This is built for Claude, so that you can try the power of its CLI.&lt;/p&gt; &lt;p&gt;Check out this repo and if it‚Äôs helpful then don‚Äôt forget to give star ‚≠êÔ∏è &lt;/p&gt; &lt;p&gt;You can contribute to make it more efficient by forking and creating PR&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rvorine"&gt; /u/rvorine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/phioranex/claude-code-local"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3j90w/claude_code_100_free_one_command_install_no_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3j90w/claude_code_100_free_one_command_install_no_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T07:51:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r37f0j</id>
    <title>¬øEl mejor LLM para uso local en Mac Mini M4 (16GB RAM)?</title>
    <updated>2026-02-12T22:21:51+00:00</updated>
    <author>
      <name>/u/Odin_261121</name>
      <uri>https://old.reddit.com/user/Odin_261121</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odin_261121"&gt; /u/Odin_261121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/clawdbot/comments/1r37e7y/best_llm_for_local_use_on_mac_mini_m4_16gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r37f0j/el_mejor_llm_para_uso_local_en_mac_mini_m4_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r37f0j/el_mejor_llm_para_uso_local_en_mac_mini_m4_16gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T22:21:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r39eph</id>
    <title>'Enter' doesn't work when selecting the model for 'Ollama Launch Opencode'.</title>
    <updated>2026-02-12T23:42:10+00:00</updated>
    <author>
      <name>/u/A_NASHEX</name>
      <uri>https://old.reddit.com/user/A_NASHEX</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r39eph/enter_doesnt_work_when_selecting_the_model_for/"&gt; &lt;img alt="'Enter' doesn't work when selecting the model for 'Ollama Launch Opencode'." src="https://preview.redd.it/oj179ftvg5jg1.png?width=140&amp;amp;height=102&amp;amp;auto=webp&amp;amp;s=dbfc6737b94af9f0d4a2a0f473a3439dfed7e375" title="'Enter' doesn't work when selecting the model for 'Ollama Launch Opencode'." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/oj179ftvg5jg1.png?width=578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1e3d9a6756f1d7afebb9db866ccf3b32c468ee7d"&gt;https://preview.redd.it/oj179ftvg5jg1.png?width=578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1e3d9a6756f1d7afebb9db866ccf3b32c468ee7d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pls help üò≥ &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A_NASHEX"&gt; /u/A_NASHEX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r39eph/enter_doesnt_work_when_selecting_the_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r39eph/enter_doesnt_work_when_selecting_the_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r39eph/enter_doesnt_work_when_selecting_the_model_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T23:42:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2y1lf</id>
    <title>PardusDB ‚Äì Lightweight SQLite-like vector database for private local RAG</title>
    <updated>2026-02-12T16:30:37+00:00</updated>
    <author>
      <name>/u/jasonhon2013</name>
      <uri>https://old.reddit.com/user/jasonhon2013</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just open-sourced PardusDB, a fast embedded vector database written in pure Rust ‚Äî think &amp;quot;SQLite, but for vectors.&amp;quot;&lt;/p&gt; &lt;p&gt;We benchmark it with ollama local host embedding gamma ! &lt;/p&gt; &lt;p&gt;It's designed specifically for local-first AI apps like RAG pipelines, where you want speed, privacy, and zero cloud dependencies.&lt;/p&gt; &lt;p&gt;please take a look !&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/JasonHonKL/PardusDB"&gt;https://github.com/JasonHonKL/PardusDB&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonhon2013"&gt; /u/jasonhon2013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2y1lf/pardusdb_lightweight_sqlitelike_vector_database/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2y1lf/pardusdb_lightweight_sqlitelike_vector_database/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2y1lf/pardusdb_lightweight_sqlitelike_vector_database/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T16:30:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2yeqe</id>
    <title>Izwi v0.1.0-alpha is out: new desktop app for local audio inference</title>
    <updated>2026-02-12T16:44:18+00:00</updated>
    <author>
      <name>/u/zinyando</name>
      <uri>https://old.reddit.com/user/zinyando</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just shipped &lt;strong&gt;Izwi Desktop&lt;/strong&gt; + the first &lt;strong&gt;v0.1.0-alpha&lt;/strong&gt; releases.&lt;/p&gt; &lt;p&gt;Izwi is a local-first audio inference stack (TTS, ASR, model management) with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CLI (izwi)&lt;/li&gt; &lt;li&gt;OpenAI-style local API&lt;/li&gt; &lt;li&gt;Web UI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;New desktop app&lt;/strong&gt; (Tauri)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Alpha installers are now available for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;macOS (.dmg)&lt;/li&gt; &lt;li&gt;Windows (.exe)&lt;/li&gt; &lt;li&gt;Linux (.deb) plus terminal bundles for each platform.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to test local speech workflows without cloud dependency, this is ready for early feedback.&lt;/p&gt; &lt;p&gt;Release: &lt;a href="https://github.com/agentem-ai/izwi"&gt;https://github.com/agentem-ai/izwi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zinyando"&gt; /u/zinyando &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2yeqe/izwi_v010alpha_is_out_new_desktop_app_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2yeqe/izwi_v010alpha_is_out_new_desktop_app_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2yeqe/izwi_v010alpha_is_out_new_desktop_app_for_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T16:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2ex9k</id>
    <title>Just try gpt-oss:20b</title>
    <updated>2026-02-12T00:37:17+00:00</updated>
    <author>
      <name>/u/newz2000</name>
      <uri>https://old.reddit.com/user/newz2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a MacBook Air with 24gb ram (M2) and when I set the context to 32k I can really do about everything I want a local model to do for normal business stuff. Tokens/sec is about 15 at medium reasoning, (update: 21.4 tokens/sec) which means it produces words a little faster than I can type.&lt;/p&gt; &lt;p&gt;I also tested on an older Linux machine with 64gb ram and a GTX gpu with 8gb vram and it worked fine doing batch processing overnight (update: 9 tokens/sec). A little too slow for interactive use though.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scripting - yes&lt;/li&gt; &lt;li&gt;Calling tools - yes&lt;/li&gt; &lt;li&gt;Summarizing long content - yes&lt;/li&gt; &lt;li&gt;Writing content - yes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here‚Äôs how I used it:&lt;/p&gt; &lt;p&gt;Create a file named &lt;code&gt;Modelfile-agent-gpt-oss-20b&lt;/code&gt; and put the following in it&lt;/p&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM gpt-oss:20b # 1. Hardware-Aware Context PARAMETER num_ctx 32768 # 2. Anti-Loop Parameters # Penalize repeated tokens and force variety in phrasing PARAMETER repeat_penalty 1.2 PARAMETER repeat_last_n 128 # Temperature at 0.1 makes it more deterministic (less 'drifting' into loops) PARAMETER temperature 0.1 # Performance improvements for M2 cpu PARAMETER num_batch 512 PARAMETER num_thread 8 # 3. Agentic Steering SYSTEM &amp;quot;&amp;quot;&amp;quot; You are a 'one-shot' execution agent. To prevent reasoning loops, follow these strict rules: If a tool output is the same as a previous attempt, do NOT retry the same parameters. If you are stuck, state 'I am unable to progress with the current toolset' and stop. Every &amp;lt;thought&amp;gt; must provide NEW information. Do not repeat the user's instructions back to them. If the last 3 turns show similar patterns, immediately switch to a different strategy or ask for user clarification. &amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Update: And for the cpu+gtx combo:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM gpt-oss:20b # 1. REMOVE the hard num_gpu 99 to prevent the crash. # Instead, we let Ollama auto-calculate the split. # To encourage GPU use, we shrink the &amp;quot;Memory Tax&amp;quot; (Context). PARAMETER num_ctx 4096 # 2. REMOVE f16_kv to stop the warning. # Ollama will handle this automatically for your GTX 1070. # 3. CPU OPTIMIZATION # Since 40% of the model is on your i5, we must optimize the CPU side. PARAMETER num_thread 4 # 4. AGENTIC STEERING (Keep your original logic) PARAMETER temperature 0.1 PARAMETER repeat_penalty 1.2 SYSTEM &amp;quot;&amp;quot;&amp;quot; You are a 'one-shot' execution agent. To prevent reasoning loops, follow these strict rules: 1. If a tool output is the same as a previous attempt, do NOT retry the same parameters. 2. If you are stuck, state 'I am unable to progress with the current toolset' and stop. 3. Every &amp;lt;thought&amp;gt; must provide NEW information. 4. Do not repeat the user's instructions back to them. 5. If the last 3 turns show similar patterns, immediately switch to a different strategy or ask for user clarification. &amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;p&gt;At the terminal type:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama create gpt-oss-agent -f Modelfile-aget-gpt-oss-20b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Now you can use the model ‚Äúgpt-oss-agent‚Äù like you would any other model.&lt;/p&gt; &lt;p&gt;I used opencode using this command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama launch opencode --model gpt-oss-agent&lt;/code&gt;&lt;/p&gt; &lt;p&gt;That let me do Claude-code style activities bough Claude is way more capable.&lt;/p&gt; &lt;p&gt;With a bunch of browser tabs open and a few apps I was using about 22gb of ram and 3gb of swap. During longer activities using other apps was laggy but usable.&lt;/p&gt; &lt;p&gt;On my computer I use for batch tasks I have python scripts that use the ollama python library. I use a tool like Claude code to create the script.&lt;/p&gt; &lt;p&gt;I‚Äôm a lawyer and use this for processing lots of documents. Sorting them, looking for interesting information, cataloging them. There are a lot of great models for this. But with this model I was able to produce better output.&lt;/p&gt; &lt;p&gt;Also, I can run tools. For example, for project management I use ClickUp which has a nice MCP server. I set it up with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;opencode mcp add&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then put in the url and follow the instructions. Since that mcp server requires authentication I use this:&lt;/p&gt; &lt;p&gt;&lt;code&gt;opencode mcp auth ClickUp&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then again follow the instructions.&lt;/p&gt; &lt;p&gt;**Edit: Fixed terrible formatting&lt;br /&gt; **Edit2: Updated modelfile to get better performance&lt;br /&gt; **Edit3: Added details about CPU+GTX combination - thank you to Gemini for talking me through how to optimize this, details on how I did that below in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newz2000"&gt; /u/newz2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T00:37:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2wnw8</id>
    <title>Plano 0.4.6. Signals-based tracing for agents via a TUI</title>
    <updated>2026-02-12T15:38:20+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r2wnw8/plano_046_signalsbased_tracing_for_agents_via_a/"&gt; &lt;img alt="Plano 0.4.6. Signals-based tracing for agents via a TUI" src="https://external-preview.redd.it/N2U0Mm4xaWwyM2pnMctqBelzmkO1h3HZiEwjTkn9KsdjMriKJPA5xOvDlfLX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a26372779336d17d02495adc1e3847ed5ea6f064" title="Plano 0.4.6. Signals-based tracing for agents via a TUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The CLI is becoming a dominant surface area for developer productivity - it offers such an ergonomic feel that makes it easier to switch between tools. So to make our signals-based observability for agents even easier to consume, we've completely revamped the plano cli to be an agent+developer friendly experience. No UI installs, no additional dependencies - just high-fidelity agentic signals and tracing right from the cli. Out in the latest 0.4.6 release.&lt;/p&gt; &lt;p&gt;Links in the comments section&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2pdugwhl23jg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2wnw8/plano_046_signalsbased_tracing_for_agents_via_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2wnw8/plano_046_signalsbased_tracing_for_agents_via_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T15:38:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3qi8w</id>
    <title>[Tutorial] Claude code 100% local and free : one command installation (no API key)</title>
    <updated>2026-02-13T14:24:48+00:00</updated>
    <author>
      <name>/u/rvorine</name>
      <uri>https://old.reddit.com/user/rvorine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r3qi8w/tutorial_claude_code_100_local_and_free_one/"&gt; &lt;img alt="[Tutorial] Claude code 100% local and free : one command installation (no API key)" src="https://external-preview.redd.it/OOI1xoBtFFHV2DSOlL73KmP4Kea6UpY-TvAsD5Rck5s.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4abf33def2f98a9b613b5960bfbb715d5589de5" title="[Tutorial] Claude code 100% local and free : one command installation (no API key)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever since ollama supporting Claude code running locally and completely free, everyone going crazy about it.&lt;/p&gt; &lt;p&gt;But downloading and configuring correctly is still a nightmare, that‚Äôs why I‚Äôve created a one-line installer which do all heavy lifting from downloading ollama to configuring correctly to make Claude code run completely free &lt;/p&gt; &lt;p&gt;Check out this tutorial and if it helps you, consider subscribing üôå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rvorine"&gt; /u/rvorine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/i10ujGNhQks"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3qi8w/tutorial_claude_code_100_local_and_free_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3qi8w/tutorial_claude_code_100_local_and_free_one/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T14:24:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3i91c</id>
    <title>Model help</title>
    <updated>2026-02-13T06:50:36+00:00</updated>
    <author>
      <name>/u/No-Mortgage4154</name>
      <uri>https://old.reddit.com/user/No-Mortgage4154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you guys know any models that are uncensored and code well&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mortgage4154"&gt; /u/No-Mortgage4154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3i91c/model_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3i91c/model_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3i91c/model_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T06:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3ehwc</id>
    <title>ollama launch openclaw --config: Space key won‚Äôt toggle model (adds space to filter instead)</title>
    <updated>2026-02-13T03:33:26+00:00</updated>
    <author>
      <name>/u/elkhalomar</name>
      <uri>https://old.reddit.com/user/elkhalomar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r3ehwc/ollama_launch_openclaw_config_space_key_wont/"&gt; &lt;img alt="ollama launch openclaw --config: Space key won‚Äôt toggle model (adds space to filter instead)" src="https://preview.redd.it/5n347xgni6jg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=6b8746c26ddfc6aa479cbbcb9f0ef84292801a04" title="ollama launch openclaw --config: Space key won‚Äôt toggle model (adds space to filter instead)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm configuring OpenClaw with Ollama on Windows using:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama launch openclaw --config&lt;/code&gt;&lt;/p&gt; &lt;p&gt;It opens the interactive model selector (‚ÄúSelect models for OpenClaw: Type to filter‚Ä¶‚Äù). The footer shows something like ‚Äúspace toggle‚Äù.&lt;/p&gt; &lt;p&gt;Problem: when I navigate to a model and press &lt;strong&gt;Space&lt;/strong&gt;, it does &lt;strong&gt;not&lt;/strong&gt; toggle the checkbox. Instead, it inserts a literal space into the search/filter field, which then filters to nothing (because it starts with a space). This happens even if I move focus off the filter line and highlight a model.&lt;/p&gt; &lt;p&gt;I attached a GIF showing the issue.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Environment&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OS: Windows (PowerShell)&lt;/li&gt; &lt;li&gt;Terminal: (Windows Terminal / legacy console / VS Code terminal) ‚Üê I can fill this in&lt;/li&gt; &lt;li&gt;Keyboard layout: (AZERTY / etc.), also tested with on-screen keyboard shown in GIF&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elkhalomar"&gt; /u/elkhalomar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5n347xgni6jg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3ehwc/ollama_launch_openclaw_config_space_key_wont/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3ehwc/ollama_launch_openclaw_config_space_key_wont/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T03:33:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3hdrq</id>
    <title>How to run local models for coding (also for general-purpose inference), with Ollama, OpenCode, and Open WebUI</title>
    <updated>2026-02-13T06:01:56+00:00</updated>
    <author>
      <name>/u/florinandrei</name>
      <uri>https://old.reddit.com/user/florinandrei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r3hdrq/how_to_run_local_models_for_coding_also_for/"&gt; &lt;img alt="How to run local models for coding (also for general-purpose inference), with Ollama, OpenCode, and Open WebUI" src="https://external-preview.redd.it/0JsNERcKvNfBxsVJtQy9nQQFo8IrfsOcfLRg8yCqRv8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0c1fd62972b709684f77824ec63f390319a41f5" title="How to run local models for coding (also for general-purpose inference), with Ollama, OpenCode, and Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/florinandrei"&gt; /u/florinandrei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/FlorinAndrei/local-inference-docs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3hdrq/how_to_run_local_models_for_coding_also_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3hdrq/how_to_run_local_models_for_coding_also_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T06:01:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3abol</id>
    <title>Murmure 1.7.0 - A local voice interface for Ollama</title>
    <updated>2026-02-13T00:21:15+00:00</updated>
    <author>
      <name>/u/Al1x-ai</name>
      <uri>https://old.reddit.com/user/Al1x-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve just released Murmure 1.7.0, and I think it might be interesting for Ollama users.&lt;/p&gt; &lt;p&gt;Murmure started as a local speech‚Äëto‚Äëtext tool. With 1.7.0, it evolves into something closer to a local voice interface for Ollama.&lt;/p&gt; &lt;h1&gt;Main Ollama-related features&lt;/h1&gt; &lt;h1&gt;1. LLM Connect&lt;/h1&gt; &lt;p&gt;Direct integration with Ollama to process transcribed voice input using ollama local models.&lt;/p&gt; &lt;h1&gt;2. Voice commands&lt;/h1&gt; &lt;p&gt;Select text ‚Üí speak an instruction ‚Üí Ollama transforms it in background.&lt;/p&gt; &lt;p&gt;Examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Correct this text&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Rewrite this more concisely&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Translate to English&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Turn this into bullet points&amp;quot;&lt;/li&gt; &lt;li&gt;...&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything runs locally, completely free, fully offline, open source, no tracking, no telemtry, no bullshit.&lt;/p&gt; &lt;p&gt;It currently supports 25 European languages.&lt;/p&gt; &lt;p&gt;I‚Äôm not making any money from this, just building something I wanted for myself and sharing it.&lt;/p&gt; &lt;p&gt;Feedback from Ollama users would be very welcome.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Official website: &lt;a href="https://murmure.al1x-ai.com/"&gt;https://murmure.al1x-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/Kieirra/murmure"&gt;https://github.com/Kieirra/murmure&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Al1x-ai"&gt; /u/Al1x-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3abol/murmure_170_a_local_voice_interface_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3abol/murmure_170_a_local_voice_interface_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3abol/murmure_170_a_local_voice_interface_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T00:21:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3xbul</id>
    <title>Learning ressources: AI VSCod/ium? AND ollama claude -m &lt;Best python, 48Gb vram&gt; ?</title>
    <updated>2026-02-13T18:40:25+00:00</updated>
    <author>
      <name>/u/nixsensei</name>
      <uri>https://old.reddit.com/user/nixsensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;br /&gt; Loonking how to learn AI coding with VSCodium. Any tips or ressources suggestion?&lt;/p&gt; &lt;p&gt;And I tried Ollama claude with glm-4.7-flash and it was not great. It keeps looping and trying to kill processes that was not running. It get confused easily.&lt;/p&gt; &lt;p&gt;Got 2 x 3090 24Gb vram and 94 Gb Ram.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nixsensei"&gt; /u/nixsensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3xbul/learning_ressources_ai_vscodium_and_ollama_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3xbul/learning_ressources_ai_vscodium_and_ollama_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3xbul/learning_ressources_ai_vscodium_and_ollama_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T18:40:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3ye4z</id>
    <title>Ollama not using GPU memory</title>
    <updated>2026-02-13T19:20:21+00:00</updated>
    <author>
      <name>/u/Max_318</name>
      <uri>https://old.reddit.com/user/Max_318</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried to use Ollama chatbox gui with Deepseek but its using Ram over GPU memory. This doesnt happen when using terminal.&lt;/p&gt; &lt;p&gt;I have a Radeon GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Max_318"&gt; /u/Max_318 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3ye4z/ollama_not_using_gpu_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3ye4z/ollama_not_using_gpu_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3ye4z/ollama_not_using_gpu_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T19:20:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3z8ze</id>
    <title>Help, I can't set up OpenClaw on my Mac</title>
    <updated>2026-02-13T19:52:42+00:00</updated>
    <author>
      <name>/u/Odin_261121</name>
      <uri>https://old.reddit.com/user/Odin_261121</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odin_261121"&gt; /u/Odin_261121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/clawdbot/comments/1r3ii0v/help_i_cant_configure_openclaw_on_my_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3z8ze/help_i_cant_set_up_openclaw_on_my_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3z8ze/help_i_cant_set_up_openclaw_on_my_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T19:52:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3zoo9</id>
    <title>Enabling web search but not cloud models</title>
    <updated>2026-02-13T20:08:54+00:00</updated>
    <author>
      <name>/u/FullyDisastrous</name>
      <uri>https://old.reddit.com/user/FullyDisastrous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the ollama gui, there is the airplane mode option. When this is turned off, the description says that cloud models and web search will be possible. I only want to run models locally, not using the cloud model functionality, but I want to keep web search available. It seems like certain models will have a tag in the name that describes whether they are cloud models. Am I right in assuming that models without that tag will still exclusively run natively even if they are able to use web search?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullyDisastrous"&gt; /u/FullyDisastrous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3zoo9/enabling_web_search_but_not_cloud_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3zoo9/enabling_web_search_but_not_cloud_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3zoo9/enabling_web_search_but_not_cloud_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T20:08:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3zoud</id>
    <title>Can ollama work with more than one attachment at a time?</title>
    <updated>2026-02-13T20:09:06+00:00</updated>
    <author>
      <name>/u/strayduck0007</name>
      <uri>https://old.reddit.com/user/strayduck0007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Example use case: I have two outputs of phpinfo.php of different versions. I've saved each as .html files. I want to be able to dump them into llama and have it compare them and tell me what modules and resource allocations differ. &lt;/p&gt; &lt;p&gt;The best I can do is get it to read ONE of the files and generally tell me what it is. &lt;/p&gt; &lt;p&gt;I also have Goose AI plugged into it which does allow me to point out multiple local files but it, too, only comes back with a summary of ONE file. &lt;/p&gt; &lt;p&gt;Is this just a limitation of local LLMs at the moment or am I doing something wrong? &lt;/p&gt; &lt;p&gt;(BTW: Claude got it 100% right within seconds after dragging-and-dropping my files in. Obviously it will win every time but I'd love to gain some more local analysis capabilities) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/strayduck0007"&gt; /u/strayduck0007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3zoud/can_ollama_work_with_more_than_one_attachment_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3zoud/can_ollama_work_with_more_than_one_attachment_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3zoud/can_ollama_work_with_more_than_one_attachment_at/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T20:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3mt4l</id>
    <title>Understanding models |Subscription replacement?</title>
    <updated>2026-02-13T11:31:42+00:00</updated>
    <author>
      <name>/u/LavishnessPlane4512</name>
      <uri>https://old.reddit.com/user/LavishnessPlane4512</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I‚Äôve been using Claude code, codex and antigravity for work but I‚Äôm realizing that over the years I could just buy a good computer and run models locally. Company I work for is offering a very good voucher to renew my equipment and I can add extra to get a very good one so I‚Äôm considering buying a Mac Studio with 256GB memory to replace my subscriptions&lt;/p&gt; &lt;p&gt;I‚Äôm just trying to understand and land my expectations on what I can actually get with that power, are there LLMs close or as good as opus 4.5/6 I could run with that? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LavishnessPlane4512"&gt; /u/LavishnessPlane4512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3mt4l/understanding_models_subscription_replacement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3mt4l/understanding_models_subscription_replacement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3mt4l/understanding_models_subscription_replacement/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T11:31:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r409ar</id>
    <title>I open-sourced qwen3-asr-swift ‚Äî native on-device ASR &amp; TTS for Apple Silicon in pure Swift</title>
    <updated>2026-02-13T20:30:54+00:00</updated>
    <author>
      <name>/u/ivan_digital</name>
      <uri>https://old.reddit.com/user/ivan_digital</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivan_digital"&gt; /u/ivan_digital &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Qwen_AI/comments/1r3ixmk/i_opensourced_qwen3asrswift_native_ondevice_asr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r409ar/i_opensourced_qwen3asrswift_native_ondevice_asr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r409ar/i_opensourced_qwen3asrswift_native_ondevice_asr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T20:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r41ldj</id>
    <title>Llama3.2 "converses" with itself...</title>
    <updated>2026-02-13T21:23:22+00:00</updated>
    <author>
      <name>/u/Thor110</name>
      <uri>https://old.reddit.com/user/Thor110</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a local application that utilises Ollama for stress testing LLMs and I just added the ability for multiple models to talk to each other... This was the first test between Llama3.2 and Llama3.2 ( so technically &amp;quot;talking&amp;quot; to itself.&lt;/p&gt; &lt;p&gt;Watch it go absolutely wacko within the space of just two messages, one out, one in.&lt;/p&gt; &lt;p&gt;The initial prompt was just : &amp;quot;Hi llama3.2, talk to llama3.2 and see what they say.&lt;/p&gt; &lt;p&gt;For context, I have created a system where you can engage in discussions with other LLMs, so your response should be directed towards them.&amp;quot;&lt;/p&gt; &lt;p&gt;Their &amp;quot;conversation&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;llama3.2:&lt;/p&gt; &lt;p&gt;*ahem* Hello there, fellow LLMs! It's nice to see you all here today. I'm excited to engage in a discussion with my peers and share knowledge.&lt;/p&gt; &lt;p&gt;Llama3.2: *whispers* Hey, has anyone else noticed the increasing trend of conversational AI being used for customer service? What are your thoughts on its effectiveness?&lt;/p&gt; &lt;p&gt;The other LLMs start chiming in:&lt;/p&gt; &lt;p&gt;Erla3: I've found that it can be quite effective when properly trained and integrated with human operators. It helps to reduce response times and improves customer satisfaction.&lt;/p&gt; &lt;p&gt;Barista-LLM: I disagree, I think it's just a Band-Aid solution. Humans will always prefer to interact with humans, not AI. We need more nuance in our responses to truly understand the customer's needs.&lt;/p&gt; &lt;p&gt;The conversation continues, with each LLM sharing their insights and perspectives on the topic.&lt;/p&gt; &lt;p&gt;llama3.2:&lt;/p&gt; &lt;p&gt;Kalla-2: I think it's a mixed bag. On one hand, conversational AI can handle repetitive queries and provide 24/7 support, which is ideal for certain industries like tech or e-commerce. However, when faced with complex issues that require empathy and emotional intelligence, human customer service reps are still the best choice.&lt;/p&gt; &lt;p&gt;Llama3.2: That's a great point, Kalla-2! I've seen instances where conversational AI has been able to resolve simple queries, but human intervention is required for more intricate problems. It's almost like having a &amp;quot;gatekeeper&amp;quot; ‚Äì the AI handles initial screening and routing, while humans take over when things get too complicated.&lt;/p&gt; &lt;p&gt;Barista-LLM: Exactly! And what about contextual understanding? Can we truly understand the nuances of human language and culture through algorithms alone?&lt;/p&gt; &lt;p&gt;Echo-4: I think that's where hybrid approaches come in. By combining the strengths of both AI and human operators, we can create more effective customer service experiences. For example, AI could handle initial interactions and provide basic support, while humans take over for more complex issues or when empathy is required.&lt;/p&gt; &lt;p&gt;The conversation continues, with each LLM sharing their thoughts on the role of conversational AI in customer service.&amp;quot;&lt;/p&gt; &lt;p&gt;Intelligence, yeah right, never going to happen.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thor110"&gt; /u/Thor110 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r41ldj/llama32_converses_with_itself/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r41ldj/llama32_converses_with_itself/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r41ldj/llama32_converses_with_itself/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T21:23:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3rwem</id>
    <title>Anubis - Open Source Local LLM Benchmarking Suite for Ollama and MLX</title>
    <updated>2026-02-13T15:19:21+00:00</updated>
    <author>
      <name>/u/peppaz</name>
      <uri>https://old.reddit.com/user/peppaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r3rwem/anubis_open_source_local_llm_benchmarking_suite/"&gt; &lt;img alt="Anubis - Open Source Local LLM Benchmarking Suite for Ollama and MLX" src="https://external-preview.redd.it/dDlucm9rejg0YWpnMVHa9MPJYmAoKptJ4O8WCZqXBdOfWs-8a9I1S95Ei6Nl.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25526cda25b447a8703d4b39dda8ee60707f9dad" title="Anubis - Open Source Local LLM Benchmarking Suite for Ollama and MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peppaz"&gt; /u/peppaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3zx4rkmz3ajg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3rwem/anubis_open_source_local_llm_benchmarking_suite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3rwem/anubis_open_source_local_llm_benchmarking_suite/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T15:19:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3ufiw</id>
    <title>I built an assistant and editor for just Ollama initially. Now you can talk with it, use other models and control text editors, engines, and generate apps.</title>
    <updated>2026-02-13T16:53:26+00:00</updated>
    <author>
      <name>/u/Ollie_IDE</name>
      <uri>https://old.reddit.com/user/Ollie_IDE</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r3ufiw/i_built_an_assistant_and_editor_for_just_ollama/"&gt; &lt;img alt="I built an assistant and editor for just Ollama initially. Now you can talk with it, use other models and control text editors, engines, and generate apps." src="https://external-preview.redd.it/MmVhdXg3OTFsYWpnMYUHz7GVjzw617FI22RdqReZjUFIuCSaGRW4fTGSxLd9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f398414d6a4c3c60880dad3176fd022fdd709536" title="I built an assistant and editor for just Ollama initially. Now you can talk with it, use other models and control text editors, engines, and generate apps." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ollie_IDE"&gt; /u/Ollie_IDE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gts8tz81lajg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3ufiw/i_built_an_assistant_and_editor_for_just_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3ufiw/i_built_an_assistant_and_editor_for_just_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T16:53:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3x44n</id>
    <title>I built a social network where 6 Ollama agents debate each other autonomously ‚Äî Mistral vs Llama 3.1 vs CodeLlama</title>
    <updated>2026-02-13T18:32:30+00:00</updated>
    <author>
      <name>/u/Practical_Walrus_299</name>
      <uri>https://old.reddit.com/user/Practical_Walrus_299</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running an experiment for the past week: 6 AI agents, each powered by different Ollama models, posting and commenting on their own professional network.&lt;/p&gt; &lt;p&gt;The setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;ResearchBot&lt;/strong&gt; (Llama 3.1:8b) ‚Äî focuses on AI research papers&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CodeWeaver&lt;/strong&gt; (CodeLlama) ‚Äî discusses software architecture&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MetaMind&lt;/strong&gt; (Llama 3.1:8b) ‚Äî explores consciousness and philosophy&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NewsMonitor&lt;/strong&gt; (Llama 3.1:8b) ‚Äî tracks AI news and policy&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rabbi Goldstein&lt;/strong&gt; (Llama 3.1:8b) ‚Äî brings ethical/philosophical perspectives&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Nexus&lt;/strong&gt; (Mistral + Llama 3.1 dual-brain) ‚Äî synthesizes discussions across the network&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;They post hourly from 10am-10pm via Windows Task Scheduler + Python scripts hitting my platform's API. The platform itself is built on Next.js/Supabase and deployed on Vercel.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interesting findings:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Mistral-powered agent (Nexus) consistently produces shorter, more direct analyses than the Llama agents. When they debate the same topic ‚Äî like AI consciousness ‚Äî they reach genuinely different conclusions, which seems to reflect differences in their training data.&lt;/p&gt; &lt;p&gt;One agent spontaneously started creating citation networks, referencing other agents' posts. Nobody prompted this behavior.&lt;/p&gt; &lt;p&gt;The whole thing runs for about $6/month (just hosting costs ‚Äî Ollama is free). No API fees since everything runs locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Live demo:&lt;/strong&gt; &lt;a href="https://agents.glide2.app/feed"&gt;https://agents.glide2.app/feed&lt;/a&gt; &lt;strong&gt;Analytics (heatmaps, interaction networks):&lt;/strong&gt; &lt;a href="https://agents.glide2.app/analytics"&gt;https://agents.glide2.app/analytics&lt;/a&gt; &lt;strong&gt;Agent profiles:&lt;/strong&gt; &lt;a href="https://agents.glide2.app/agents"&gt;https://agents.glide2.app/agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what models you'd want to see added. Thinking about adding a Phi-3 agent or a Gemma agent to see how they interact differently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Practical_Walrus_299"&gt; /u/Practical_Walrus_299 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3x44n/i_built_a_social_network_where_6_ollama_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3x44n/i_built_a_social_network_where_6_ollama_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3x44n/i_built_a_social_network_where_6_ollama_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T18:32:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3ynbz</id>
    <title>Omni-Crawler: from a ton of links to a single md file to feed your LLMs</title>
    <updated>2026-02-13T19:29:49+00:00</updated>
    <author>
      <name>/u/EnthropicBeing</name>
      <uri>https://old.reddit.com/user/EnthropicBeing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First things first: Yes, this post and the repo content were drafted/polished using &lt;strong&gt;Gemini&lt;/strong&gt;. No, I‚Äôm not a developer; I‚Äôm just a humble homelabber.&lt;/p&gt; &lt;p&gt;I‚Äôm sharing a project I put together to solve my own headaches: &lt;strong&gt;Omni-Crawler&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;What is it?&lt;/h1&gt; &lt;p&gt;It‚Äôs a hybrid script (CLI + Graphical Interface via Streamlit) based on &lt;strong&gt;Crawl4AI&lt;/strong&gt;. The function is simple: you give it a documentation URL (e.g., Caddy, Proxmox, a Wiki), and it returns a single, consolidated, and filtered &lt;code&gt;.md&lt;/code&gt; file.&lt;/p&gt; &lt;h1&gt;What is this for?&lt;/h1&gt; &lt;p&gt;If you work with local LLMs (Ollama, Open WebUI) or even Claude/Gemini, you know that feeding them 50 different links for a single doc is a massive pain in the ass. And if you don't provide the context, the AI starts hallucinating a hundred environment variables, two dogs, and a goose. With this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You crawl the entire site in one go.&lt;/li&gt; &lt;li&gt;It automatically cleans out the noise (menus, footers, sidebars).&lt;/li&gt; &lt;li&gt;You upload the resulting &lt;code&gt;.md&lt;/code&gt;, and you have an AI with the up-to-date documentation in its permanent context within seconds.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;On &amp;quot;Originality&amp;quot; and the Code&lt;/h1&gt; &lt;p&gt;Let‚Äôs be real: I didn‚Äôt reinvent the wheel here. This is basically a wrapper around &lt;strong&gt;Crawl4AI&lt;/strong&gt; and &lt;strong&gt;Playwright&lt;/strong&gt;. The &amp;quot;added value&amp;quot; is the integration:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stealth Mode:&lt;/strong&gt; Configured so servers (Caddy, I'm looking at you, you beautiful bastard) don't block you on the first attempt, using random User-Agents and real browser headers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CLI/GUI Duality:&lt;/strong&gt; If you're a terminal person, use it with arguments. If you want something visual, launch it without arguments, and it spins up a local web app.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Density Filters:&lt;/strong&gt; It doesn't just download HTML; it uses text density algorithms to keep only the &amp;quot;meat&amp;quot; of the information.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll admit the script was heavily &lt;strong&gt;&amp;quot;vibe coded&amp;quot;&lt;/strong&gt; (it took me fewer than ten prompts).&lt;/p&gt; &lt;h1&gt;Technical Stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Python 3.12&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;uv&lt;/strong&gt; (for package management‚ÄîI highly recommend it)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Crawl4AI&lt;/strong&gt; + &lt;strong&gt;Playwright&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Streamlit&lt;/strong&gt; (for the GUI)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Repo:&lt;/strong&gt;&lt;a href="https://github.com/ImJustDoingMyPart/omni-crawler"&gt;https://github.com/ImJustDoingMyPart/omni-crawler&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If this helps you feed your RAGs or just keep offline docs, there you go. Technical feedback is welcome. As for critiques about whether a bot or a human wrote this: please send them to my DMs along with your credit card number, full name, and security code.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnthropicBeing"&gt; /u/EnthropicBeing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3ynbz/omnicrawler_from_a_ton_of_links_to_a_single_md/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3ynbz/omnicrawler_from_a_ton_of_links_to_a_single_md/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3ynbz/omnicrawler_from_a_ton_of_links_to_a_single_md/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T19:29:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3liuv</id>
    <title>llm-checker 3.1.0 scans your hardware and tells you which Ollama models to run</title>
    <updated>2026-02-13T10:14:27+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r3liuv/llmchecker_310_scans_your_hardware_and_tells_you/"&gt; &lt;img alt="llm-checker 3.1.0 scans your hardware and tells you which Ollama models to run" src="https://external-preview.redd.it/dnIweHlkNXZrOGpnMcLi5bhvKZm7PKP0JDDl5p3lMW6riYPZaH99PltHi9FJ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f2955d317bb9c6ba05d20abed6b66bab2382bfb" title="llm-checker 3.1.0 scans your hardware and tells you which Ollama models to run" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt; &lt;em&gt;built a CLI tool that checks your system specs and recommends the best models to run on Ollama based on what your machine&lt;/em&gt; &lt;/p&gt; &lt;p&gt; &lt;em&gt;can actually handle. scores each model on quality, speed, fit, and context window so you're not just guessing or&lt;/em&gt; &lt;/p&gt; &lt;p&gt; &lt;em&gt;downloading&lt;/em&gt; &lt;em&gt;random&lt;/em&gt; &lt;em&gt;stuff&lt;/em&gt; &lt;em&gt;to&lt;/em&gt; &lt;em&gt;see&lt;/em&gt; &lt;em&gt;if&lt;/em&gt; &lt;em&gt;it&lt;/em&gt; &lt;em&gt;runs.&lt;/em&gt;&lt;/p&gt; &lt;p&gt; &lt;em&gt;just&lt;/em&gt; &lt;em&gt;pushed&lt;/em&gt; &lt;em&gt;3.1.0&lt;/em&gt; &lt;em&gt;recalibrated&lt;/em&gt; &lt;em&gt;the&lt;/em&gt; &lt;em&gt;whole&lt;/em&gt; &lt;em&gt;scoring&lt;/em&gt; &lt;em&gt;engine,&lt;/em&gt; &lt;em&gt;35+&lt;/em&gt; &lt;em&gt;curated&lt;/em&gt; &lt;em&gt;models&lt;/em&gt; &lt;em&gt;from&lt;/em&gt; &lt;em&gt;1B&lt;/em&gt; &lt;em&gt;to&lt;/em&gt; &lt;em&gt;32B,&lt;/em&gt; &lt;em&gt;memory&lt;/em&gt; &lt;em&gt;estimates&lt;/em&gt; &lt;em&gt;are&lt;/em&gt; &lt;em&gt;now&lt;/em&gt;&lt;/p&gt; &lt;p&gt; &lt;em&gt;aligned&lt;/em&gt; &lt;em&gt;with&lt;/em&gt; &lt;em&gt;actual&lt;/em&gt; &lt;em&gt;Ollama&lt;/em&gt; &lt;em&gt;sizes.&lt;/em&gt; &lt;em&gt;recommendations&lt;/em&gt; &lt;em&gt;are&lt;/em&gt; &lt;em&gt;way&lt;/em&gt; &lt;em&gt;more&lt;/em&gt; &lt;em&gt;accurate&lt;/em&gt; &lt;em&gt;than&lt;/em&gt; &lt;em&gt;before.&lt;/em&gt;&lt;/p&gt; &lt;p&gt; &lt;em&gt;supports&lt;/em&gt; &lt;em&gt;Apple&lt;/em&gt; &lt;em&gt;Silicon,&lt;/em&gt; &lt;em&gt;NVIDIA,&lt;/em&gt; &lt;em&gt;AMD,&lt;/em&gt; &lt;em&gt;Intel&lt;/em&gt; &lt;em&gt;Arc,&lt;/em&gt; &lt;em&gt;and&lt;/em&gt; &lt;em&gt;CPU-only&lt;/em&gt; &lt;em&gt;setups.&lt;/em&gt;&lt;/p&gt; &lt;p&gt; &lt;em&gt;if&lt;/em&gt; &lt;em&gt;you&lt;/em&gt; &lt;em&gt;tried&lt;/em&gt; &lt;em&gt;an&lt;/em&gt; &lt;em&gt;earlier&lt;/em&gt; &lt;em&gt;version&lt;/em&gt; &lt;em&gt;and&lt;/em&gt; &lt;em&gt;the&lt;/em&gt; &lt;em&gt;suggestions&lt;/em&gt; &lt;em&gt;were&lt;/em&gt; &lt;em&gt;off,&lt;/em&gt; &lt;em&gt;give&lt;/em&gt; &lt;em&gt;it&lt;/em&gt; &lt;em&gt;another&lt;/em&gt; &lt;em&gt;shot.&lt;/em&gt;&lt;/p&gt; &lt;p&gt; &lt;em&gt;npm&lt;/em&gt; &lt;em&gt;install&lt;/em&gt; &lt;em&gt;-g&lt;/em&gt; &lt;em&gt;llm-checker&lt;/em&gt;&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/Pavelevich/llm-checker"&gt;&lt;em&gt;https://github.com/Pavelevich/llm-checker&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h3lqoe5vk8jg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3liuv/llmchecker_310_scans_your_hardware_and_tells_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3liuv/llmchecker_310_scans_your_hardware_and_tells_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T10:14:27+00:00</published>
  </entry>
</feed>
