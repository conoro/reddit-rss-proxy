<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-24T05:12:20+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pqvnsf</id>
    <title>It's just a basic script." Okay, watch my $40 Agent build a full Cyberpunk Landing Page (HTML+CSS) from scratch. No edits.</title>
    <updated>2025-12-19T20:36:30+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pqvnsf/its_just_a_basic_script_okay_watch_my_40_agent/"&gt; &lt;img alt="It's just a basic script.&amp;quot; Okay, watch my $40 Agent build a full Cyberpunk Landing Page (HTML+CSS) from scratch. No edits." src="https://external-preview.redd.it/bW1ta2Q1YWIxODhnMR5tpIwuFOO1PPtkDnxq5jShnTHZF3q4px8E_0nOycIe.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de23241b6f421b0c7de4a3b598f7bc33570f1978" title="It's just a basic script.&amp;quot; Okay, watch my $40 Agent build a full Cyberpunk Landing Page (HTML+CSS) from scratch. No edits." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Some people said a local agent can't do complex tasks. So I asked it to build a responsive landing page for a fictional AI startup.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Result:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single file HTML + Embedded CSS.&lt;/li&gt; &lt;li&gt;Dark Mode &amp;amp; Neon aesthetics perfectly matched prompt instructions.&lt;/li&gt; &lt;li&gt;Working Hover states &amp;amp; Flexbox layout.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero human coding involved.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Model: Qwen 2.5 Coder / Llama 3 running locally via Ollama.&lt;/em&gt; &lt;em&gt;This is why I raised the price. It actually works.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x06dnw8b188g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pqvnsf/its_just_a_basic_script_okay_watch_my_40_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pqvnsf/its_just_a_basic_script_okay_watch_my_40_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-19T20:36:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq69o5</id>
    <title>New functiongemma model: not worth downloading</title>
    <updated>2025-12-18T23:55:37+00:00</updated>
    <author>
      <name>/u/New_Cranberry_6451</name>
      <uri>https://old.reddit.com/user/New_Cranberry_6451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! Just wanted to share with you my awful experience with the new &lt;strong&gt;functiongemma&lt;/strong&gt; model at &lt;a href="https://ollama.com/library/functiongemma"&gt;https://ollama.com/library/functiongemma&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have a valid MCP toolset that works great with other very small models such as qwen3:1.7b. I obtain quite reliable function calls. So, an even smaller model that could do this with the same quality sounds great. I downloaded the &lt;strong&gt;functiongemma:270m-it-fp16&lt;/strong&gt; version of 552MB and deleted after the second test. My prompt:&lt;/p&gt; &lt;p&gt;&lt;em&gt;&amp;quot;List files in /&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;and the response:&lt;/p&gt; &lt;p&gt;&lt;em&gt;&amp;quot;Calling FSUtils operation folder in path /&amp;quot;&lt;/em&gt; &lt;/p&gt; &lt;p&gt;(in my toolset the folder operation is to create a folder)&lt;/p&gt; &lt;p&gt;The fact that it understands it must CREATE something when in a 4 word sentence the only verb is LIST, tells me I must delete it and forget it even exists. Zero reliability, don't waste your time even trying, &lt;strong&gt;qwen3:1.7b&lt;/strong&gt; is the smallest model I rely on for function calling and haven't found any other smaller model that does this job better. &lt;/p&gt; &lt;p&gt;¬øWhich small model do you use for MCP function calling?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Cranberry_6451"&gt; /u/New_Cranberry_6451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pq69o5/new_functiongemma_model_not_worth_downloading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pq69o5/new_functiongemma_model_not_worth_downloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pq69o5/new_functiongemma_model_not_worth_downloading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T23:55:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppxsi6</id>
    <title>Ollama supports Google's new open source model, FunctionGemma</title>
    <updated>2025-12-18T18:12:03+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ppxsi6/ollama_supports_googles_new_open_source_model/"&gt; &lt;img alt="Ollama supports Google's new open source model, FunctionGemma" src="https://external-preview.redd.it/bTdlcHQ1MWw2MDhnMe6-hUKa0vwIzCycmlVIgMQ01unutf-BuqAEwIyDruBY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=926c0547b1b83b18ea8c3460c0bbd9c99947dcd8" title="Ollama supports Google's new open source model, FunctionGemma" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FunctionGemma is a specialized version of Google's Gemma 3 270M model fine-tuned explicitly for function calling.&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run functiongemma&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Note: This model requires Ollama v0.13.5 or later&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lqhwf01l608g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppxsi6/ollama_supports_googles_new_open_source_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ppxsi6/ollama_supports_googles_new_open_source_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T18:12:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqbrtw</id>
    <title>Two years ago, I was just a math major. Now I've built the 1.5B router model used by HuggingFace. Can I bring it to Cursor?</title>
    <updated>2025-12-19T04:20:35+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pqbrtw/two_years_ago_i_was_just_a_math_major_now_ive/"&gt; &lt;img alt="Two years ago, I was just a math major. Now I've built the 1.5B router model used by HuggingFace. Can I bring it to Cursor?" src="https://preview.redd.it/qyzyzwin738g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0d7884eabf83b94aeb03f74c4f1cdacf9196716" title="Two years ago, I was just a math major. Now I've built the 1.5B router model used by HuggingFace. Can I bring it to Cursor?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm part of a small models-research and infrastructure startup tackling problems in the application delivery space for AI projects -- basically, working to close the gap between an AI prototype and production. As part of our research efforts, one big focus area for us is model routing: helping developers deploy and utilize different models for different use cases and scenarios.&lt;/p&gt; &lt;p&gt;Over the past year, I built Arch-Router 1.5B, a small and efficient LLM trained via Rust-based stack, and &lt;em&gt;also&lt;/em&gt; delivered through a Rust data plane. The core insight behind Arch-Router is simple: policy-based routing gives developers the right constructs to automate behavior, grounded in their &lt;em&gt;own evals&lt;/em&gt; of which LLMs are best for specific coding and agentic tasks.&lt;/p&gt; &lt;p&gt;In contrast, existing routing approaches have limitations in real-world use. They typically optimize for benchmark performance while neglecting human preferences driven by subjective evaluation criteria. For instance, some routers are trained to achieve optimal performance on benchmarks like MMLU or GPQA, which don‚Äôt reflect the subjective and task-specific judgments that users often make in practice. These approaches are also less flexible because they are typically trained on a limited pool of models, and usually require retraining and architectural modifications to support new models or use cases.&lt;/p&gt; &lt;p&gt;Our approach is already proving out at scale. Hugging Face went live with our dataplane two weeks ago, and our Rust router/egress layer now handles 1M+ user interactions, including coding use cases in HuggingChat. Hope the community finds it helpful. More details on the project are on GitHub: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And if you‚Äôre a Claude Code user, you can instantly use the router for code routing scenarios via our example guide there under demos/use_cases/claude_code_router. Still looking at ways to bring this natively into Cursor. If there are ways I can push this upstream it would be great. Tips?&lt;/p&gt; &lt;p&gt;In any event, hope you you all find this useful üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qyzyzwin738g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pqbrtw/two_years_ago_i_was_just_a_math_major_now_ive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pqbrtw/two_years_ago_i_was_just_a_math_major_now_ive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-19T04:20:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1prhxhi</id>
    <title>Ubuntu Server Solution that will allow me to locally chat with about 100 PDFs</title>
    <updated>2025-12-20T16:11:26+00:00</updated>
    <author>
      <name>/u/chribonn</name>
      <uri>https://old.reddit.com/user/chribonn</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chribonn"&gt; /u/chribonn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1prhx39/ubuntu_server_solution_that_will_allow_me_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1prhxhi/ubuntu_server_solution_that_will_allow_me_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1prhxhi/ubuntu_server_solution_that_will_allow_me_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-20T16:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1prsa4i</id>
    <title>Meine Bridge Pipeline Isolierte Code-sandbox + Graph Ordnung</title>
    <updated>2025-12-20T23:50:48+00:00</updated>
    <author>
      <name>/u/danny_094</name>
      <uri>https://old.reddit.com/user/danny_094</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey Leute! Update zu meiner lokalen AI-Pipeline&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Seit meinem letzten Post habe ich weiter an meiner selbst-gehosteten AI-Pipeline gearbeitet. Das hier ist noch nicht im GitHub Repo ‚Äì kommt aber heute oder morgen!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code-Sandbox&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Die KI kann jetzt Code in einer isolierten Docker-Sandbox ausf√ºhren ‚Äì und du hast &lt;strong&gt;live Zugriff auf dasselbe Terminal&lt;/strong&gt;!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Du startest die Sandbox √ºber die WebUI (erster Start dauert etwas, danach instant)&lt;/li&gt; &lt;li&gt;Terminal l√§uft direkt im Browser (ttyd)&lt;/li&gt; &lt;li&gt;Die KI nutzt &lt;strong&gt;dieselbe&lt;/strong&gt; Sandbox ‚Äì installierte Packages bleiben erhalten&lt;/li&gt; &lt;li&gt;Du kannst jedes Docker-Image nutzen und eigene Regeln definieren&lt;/li&gt; &lt;li&gt;Alles √ºber eine Registry konfigurierbar&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hei√üt: Du installierst &lt;code&gt;pandas&lt;/code&gt;, die KI kann es sofort nutzen. Keine getrennten Umgebungen mehr!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Memory Maintenance&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Wenn du gerade Pause machst, lass die KI doch den Memory-Graph aufr√§umen:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Duplikate finden und mergen&lt;/li&gt; &lt;li&gt;Wichtige Fakten ins Langzeitged√§chtnis verschieben&lt;/li&gt; &lt;li&gt;Zusammenfassungen erstellen&lt;/li&gt; &lt;li&gt;Kontext-Beziehungen optimieren&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Alles l√§uft im Hintergrund ‚Äì du siehst den Fortschritt live in der UI.&lt;/p&gt; &lt;p&gt;Wiki mit Anleitung folgt!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1prsa4i/video/1tnhpas46g8g1/player"&gt;https://reddit.com/link/1prsa4i/video/1tnhpas46g8g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's a highly professional video. :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danny_094"&gt; /u/danny_094 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1prsa4i/meine_bridge_pipeline_isolierte_codesandbox_graph/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1prsa4i/meine_bridge_pipeline_isolierte_codesandbox_graph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1prsa4i/meine_bridge_pipeline_isolierte_codesandbox_graph/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-20T23:50:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1preq59</id>
    <title>Project using GPT-OSS:20b, Llama3.2:3b and two old NVIDIA Teslas. Future is odd.</title>
    <updated>2025-12-20T13:46:42+00:00</updated>
    <author>
      <name>/u/Nerdaxic</name>
      <uri>https://old.reddit.com/user/Nerdaxic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1preq59/project_using_gptoss20b_llama323b_and_two_old/"&gt; &lt;img alt="Project using GPT-OSS:20b, Llama3.2:3b and two old NVIDIA Teslas. Future is odd." src="https://external-preview.redd.it/wN8TpTSD0ngh2cSr2uiHrDoHkT0WM7PVSurfhkG0VMM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74e15dd66ced860e2e295326a61553b6a565184d" title="Project using GPT-OSS:20b, Llama3.2:3b and two old NVIDIA Teslas. Future is odd." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nerdaxic"&gt; /u/Nerdaxic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jgb6giq42d8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1preq59/project_using_gptoss20b_llama323b_and_two_old/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1preq59/project_using_gptoss20b_llama323b_and_two_old/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-20T13:46:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1prhf2o</id>
    <title>How to Fine-Tune and Deploy an Open-Source Model</title>
    <updated>2025-12-20T15:49:47+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open-source language models are powerful, but they are trained to be general. They don‚Äôt know your data, your workflows, or how your system actually works.&lt;/p&gt; &lt;p&gt;Fine-tuning is how you adapt a pre-trained model to your use case.&lt;br /&gt; You train it on your own examples so it learns the patterns, tone, and behavior that matter for your application, while keeping its general language skills.&lt;/p&gt; &lt;p&gt;Once the model is fine-tuned, deployment becomes the next step.&lt;br /&gt; A fine-tuned model is only useful if it can be accessed reliably, with low latency, and in a way that fits into existing applications.&lt;/p&gt; &lt;p&gt;The workflow I followed is straightforward:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;prepare a task-specific dataset&lt;/li&gt; &lt;li&gt;fine-tune the model using an efficient method like LoRA&lt;/li&gt; &lt;li&gt;deploy the result as a stable API endpoint&lt;/li&gt; &lt;li&gt;test and iterate based on real usage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I documented the full process and recorded a &lt;a href="https://www.youtube.com/watch?v=gqCZ_sFha7E"&gt;walkthrough&lt;/a&gt; showing how this works end to end.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1prhf2o/how_to_finetune_and_deploy_an_opensource_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1prhf2o/how_to_finetune_and_deploy_an_opensource_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1prhf2o/how_to_finetune_and_deploy_an_opensource_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-20T15:49:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps1w2h</id>
    <title>MiniMax 2.1???</title>
    <updated>2025-12-21T08:40:36+00:00</updated>
    <author>
      <name>/u/Carinaaaatian</name>
      <uri>https://old.reddit.com/user/Carinaaaatian</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Carinaaaatian"&gt; /u/Carinaaaatian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pr5llx/minimax_21/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ps1w2h/minimax_21/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ps1w2h/minimax_21/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T08:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1psgww9</id>
    <title>My Local Agent built this Stealth Game in one go. I‚Äôm tired of choosing projects. YOU tell me what to build next.</title>
    <updated>2025-12-21T20:58:55+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1psgww9/my_local_agent_built_this_stealth_game_in_one_go/"&gt; &lt;img alt="My Local Agent built this Stealth Game in one go. I‚Äôm tired of choosing projects. YOU tell me what to build next." src="https://external-preview.redd.it/ZjZ1YzFhMTlmbThnMbAx8nw-haBMqpHU8uMF0BTU1OL2PHX3BGtYaHk_p9hR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57906f7658d10fe43c7d9e33fffb77a8ce9526bc" title="My Local Agent built this Stealth Game in one go. I‚Äôm tired of choosing projects. YOU tell me what to build next." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Qwen3-30B locally on RTX 4070. People think these videos are cherry-picked. Fine.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Watch the video (It handled raycasting, AI patrol paths, and collision logic autonomously).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comment a game idea/mechanic below.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;I will feed the &lt;strong&gt;top upvoted comment&lt;/strong&gt; directly into the agent as a prompt ‚Äì UNEDITED.&lt;/li&gt; &lt;li&gt;I will post the result tomorrow.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Let's see if it breaks or shines. Do your worst (but keep it Python/2D).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hytjwvx8fm8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1psgww9/my_local_agent_built_this_stealth_game_in_one_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1psgww9/my_local_agent_built_this_stealth_game_in_one_go/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T20:58:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps4aic</id>
    <title>Is There a good AI video generator like veo 3 on ollama?</title>
    <updated>2025-12-21T11:18:18+00:00</updated>
    <author>
      <name>/u/Remote-Solid-8360</name>
      <uri>https://old.reddit.com/user/Remote-Solid-8360</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remote-Solid-8360"&gt; /u/Remote-Solid-8360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ps4aic/is_there_a_good_ai_video_generator_like_veo_3_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ps4aic/is_there_a_good_ai_video_generator_like_veo_3_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ps4aic/is_there_a_good_ai_video_generator_like_veo_3_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T11:18:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1przacs</id>
    <title>RTX 4070 in Action: What Your New System Could Look Like</title>
    <updated>2025-12-21T05:59:15+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1przacs/rtx_4070_in_action_what_your_new_system_could/"&gt; &lt;img alt="RTX 4070 in Action: What Your New System Could Look Like" src="https://external-preview.redd.it/Z3I3a3U5Y3F4aDhnMWOTU7UqQZJD7PNBptsO8kG3EZV_3sbv7lEF5DFvfaH3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=923667882654b33cb2fa4de2d82239cd2a8c6e6e" title="RTX 4070 in Action: What Your New System Could Look Like" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Super-Bot: The Ultimate Autonomous AI Agent for Windows&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; Meet &lt;strong&gt;Super-Bot&lt;/strong&gt;, your self-learning development companion. This isn't just a chatbot‚Äîit's an autonomous agent that acts. It writes code, executes commands, fixes its own errors, and even &amp;quot;sees&amp;quot; your screen to validate applications.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-Provider Support:&lt;/strong&gt; Seamlessly integrates with local LLMs (Ollama, LM Studio) and top cloud APIs (GPT-4, Claude 3.5, Gemini, xAI).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-Healing Engine:&lt;/strong&gt; Automatically detects bugs, learns from them, and fixes code without your intervention.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vision Capabilities:&lt;/strong&gt; Uses AI vision to look at your screen and verify if GUI apps or websites look correct.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Memory:&lt;/strong&gt; Remembers successful coding patterns to solve future tasks faster.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware-Locked Security:&lt;/strong&gt; Includes a robust licensing system locked to your specific machine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy to Use:&lt;/strong&gt; Delivered as a standalone Windows EXE‚Äîno complex Python environment setup needed.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/krzfnb2qxh8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1przacs/rtx_4070_in_action_what_your_new_system_could/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1przacs/rtx_4070_in_action_what_your_new_system_could/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T05:59:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1psfyme</id>
    <title>AI REAL USEFUL WORKING IN REAL LIFE , LLAMA.CPP</title>
    <updated>2025-12-21T20:17:00+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1psfydn/ai_real_useful_working_in_real_life_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1psfyme/ai_real_useful_working_in_real_life_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1psfyme/ai_real_useful_working_in_real_life_llamacpp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T20:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1psa03p</id>
    <title>Low-code AI tools, live MCP servers, inspection, and agentic chat in one Spring AI playground.</title>
    <updated>2025-12-21T16:08:25+00:00</updated>
    <author>
      <name>/u/kr-jmlab</name>
      <uri>https://old.reddit.com/user/kr-jmlab</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kr-jmlab"&gt; /u/kr-jmlab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ps9yej"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1psa03p/lowcode_ai_tools_live_mcp_servers_inspection_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1psa03p/lowcode_ai_tools_live_mcp_servers_inspection_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T16:08:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1psihel</id>
    <title>Ultima 2 Challenge: COMPLETED. ‚úÖ You asked for a tile-based RPG engine with state management. The Agent delivered.</title>
    <updated>2025-12-21T22:07:11+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1psihel/ultima_2_challenge_completed_you_asked_for_a/"&gt; &lt;img alt="Ultima 2 Challenge: COMPLETED. ‚úÖ You asked for a tile-based RPG engine with state management. The Agent delivered." src="https://external-preview.redd.it/dG1xZW56djhybThnMf3CgSBkjdJpI-IDVjUSNCAXZBQqMepkGuNeLK6QbT2A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6958813c6087f43027e16fe1218311a585ae8a49" title="Ultima 2 Challenge: COMPLETED. ‚úÖ You asked for a tile-based RPG engine with state management. The Agent delivered." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Under the hood (as seen in the video):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;State Machine:&lt;/strong&gt; Fully implemented. Seamless switching between &lt;code&gt;OVERWORLD&lt;/code&gt; and &lt;code&gt;TOWN&lt;/code&gt; states based on tile triggers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Persistence:&lt;/strong&gt; The agent handles coordinate resets when entering/exiting zones.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tile Engine:&lt;/strong&gt; Dynamic rendering of 4 different terrain types + walls.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logic:&lt;/strong&gt; Turn-based movement, collision detection (water/walls), and NPC interaction logic.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Verdict:&lt;/strong&gt; This required maintaining context across multiple class structures and game loops. A massive win for local 30B models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9xbp2an8rm8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1psihel/ultima_2_challenge_completed_you_asked_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1psihel/ultima_2_challenge_completed_you_asked_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T22:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pskkhp</id>
    <title>Any hope for my Linux laptop?</title>
    <updated>2025-12-21T23:40:39+00:00</updated>
    <author>
      <name>/u/AccordionPianist</name>
      <uri>https://old.reddit.com/user/AccordionPianist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 13 year old laptop (build date 2012-10) with 12 GB RAM running Ubuntu. Integrated graphics, ASUS machine K56CA. Do I have a snowballs chance in hell of running a local AI and what model should I strive for even?&lt;/p&gt; &lt;p&gt;By the way I‚Äôve used Upscayl but it only works on the lowest possible setting. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AccordionPianist"&gt; /u/AccordionPianist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pskkhp/any_hope_for_my_linux_laptop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pskkhp/any_hope_for_my_linux_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pskkhp/any_hope_for_my_linux_laptop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T23:40:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps7bzx</id>
    <title>Opensource models less than 30b with highest edit-diff success rate</title>
    <updated>2025-12-21T14:08:11+00:00</updated>
    <author>
      <name>/u/Express_Quail_1493</name>
      <uri>https://old.reddit.com/user/Express_Quail_1493</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;currently I'm struggling to find one that has solid successful edit-diff consistency. devstral-small-2 is the only one that stays consistent for me but its not super smart as top contender. its a good enough model. qwen3-coder-30b keeps getting failing in their edit-diff attempts&lt;/p&gt; &lt;p&gt;what is your experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Quail_1493"&gt; /u/Express_Quail_1493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ps7bzx/opensource_models_less_than_30b_with_highest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ps7bzx/opensource_models_less_than_30b_with_highest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ps7bzx/opensource_models_less_than_30b_with_highest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T14:08:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt6u00</id>
    <title>Exclusive Holiday Offer! Perplexity AI PRO 1-Year Subscription ‚Äì Save 90%!</title>
    <updated>2025-12-22T18:22:15+00:00</updated>
    <author>
      <name>/u/A2uniquenickname</name>
      <uri>https://old.reddit.com/user/A2uniquenickname</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pt6u00/exclusive_holiday_offer_perplexity_ai_pro_1year/"&gt; &lt;img alt="Exclusive Holiday Offer! Perplexity AI PRO 1-Year Subscription ‚Äì Save 90%!" src="https://preview.redd.it/l7i791akss8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5880d65caad3ce59e1f5508d0383eef7130d534f" title="Exclusive Holiday Offer! Perplexity AI PRO 1-Year Subscription ‚Äì Save 90%!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get Perplexity AI PRO (1-Year) ‚Äì at 90% OFF!&lt;/p&gt; &lt;p&gt;Order here: &lt;a href="https://cheapgpts.store/Perplexity"&gt;CHEAPGPT.STORE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Plan: 12 Months&lt;/p&gt; &lt;p&gt;üí≥ Pay with: PayPal or Revolut or your favorite payment method&lt;/p&gt; &lt;p&gt;Reddit reviews: &lt;a href="https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu"&gt;FEEDBACK POST&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TrustPilot: &lt;a href="https://www.trustpilot.com/review/cheapgpt.store"&gt;TrustPilot FEEDBACK&lt;/a&gt; &lt;/p&gt; &lt;p&gt;NEW YEAR BONUS: Apply code PROMO5 for extra discount OFF your order! &lt;/p&gt; &lt;p&gt;BONUS!: Enjoy the AI Powered automated web browser. (Presented by Perplexity) included WITH YOUR PURCHASE!&lt;/p&gt; &lt;p&gt;Trusted and the cheapest! Check all feedbacks before you purchase&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A2uniquenickname"&gt; /u/A2uniquenickname &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l7i791akss8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pt6u00/exclusive_holiday_offer_perplexity_ai_pro_1year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pt6u00/exclusive_holiday_offer_perplexity_ai_pro_1year/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-22T18:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pske2v</id>
    <title>virtual pet / life simulation using Ollama and Unity 6</title>
    <updated>2025-12-21T23:32:22+00:00</updated>
    <author>
      <name>/u/rzarekta</name>
      <uri>https://old.reddit.com/user/rzarekta</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pske2v/virtual_pet_life_simulation_using_ollama_and/"&gt; &lt;img alt="virtual pet / life simulation using Ollama and Unity 6" src="https://external-preview.redd.it/eTdyYWZodTc2bjhnMXUjJLkZMiuDknoqj2U9nKzIooYTPd9cVtalvt_A-w2b.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc908aaaf303ce0483f79c3f1fb4417f43feaa87" title="virtual pet / life simulation using Ollama and Unity 6" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a virtual pet / life simulation in Unity 6, and it‚Äôs slowly turning into a living little ecosystem. This is a prototype, no fancy graphics or eye candy has been added. &lt;/p&gt; &lt;p&gt;Each creature is fully AI-driven, the AI controls all movement and decisions. They choose where to go, when to wander, when to eat, when to sleep, and when to interact. The green squares are food, and the purple rectangles are beds, which they seek out naturally based on their needs.&lt;/p&gt; &lt;p&gt;You can talk to the creatures individually, and they also talk amongst themselves. What you say to one creature can influence how it behaves and how it talks to others. Conversations aren‚Äôt isolated, they actually affect memory, mood, and social relationships.&lt;/p&gt; &lt;p&gt;You can also give direct commands like &lt;em&gt;stop&lt;/em&gt;, &lt;em&gt;go left&lt;/em&gt;, &lt;em&gt;go right&lt;/em&gt;, &lt;em&gt;follow&lt;/em&gt;, or &lt;em&gt;find another creature&lt;/em&gt;. The creatures don‚Äôt blindly obey, they evaluate each command based on personality, trust, current needs, and survival priorities, then respond honestly.&lt;/p&gt; &lt;p&gt;All AI logic and dialogue run fully locally using Ollama, on an RTX 2070 (8GB) AI server.&lt;/p&gt; &lt;p&gt;Watching emergent behavior form instead of scripting it has been wild.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzarekta"&gt; /u/rzarekta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9519oat76n8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pske2v/virtual_pet_life_simulation_using_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pske2v/virtual_pet_life_simulation_using_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T23:32:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt771o</id>
    <title>Prompt Injection demo in Ollama - help, please?</title>
    <updated>2025-12-22T18:36:00+00:00</updated>
    <author>
      <name>/u/West-Candy-5732</name>
      <uri>https://old.reddit.com/user/West-Candy-5732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, everyone. &lt;/p&gt; &lt;p&gt;I am working on my project for a Cybersecurity class and I would like to showcase the risks of Prompt Injection. I had this idea in my mind with many different things, but I wanted to actually start with something simple. However, even using small models like Phi3 or GPT2, I fail to actually override the system prompt (classic example of a translator agent, in my case English -&amp;gt; German), and get it to say &amp;quot;Haha, I got hacked!&amp;quot;. &lt;/p&gt; &lt;p&gt;Is there some prompt injection security in Ollama that I am not aware of? Can it be turned off?&lt;/p&gt; &lt;p&gt;Alternatively: do you guys have any better ideas how to demonstrate this? I tried using an API (Claude), but the results I got were not what I expected, quite quirky.&lt;/p&gt; &lt;p&gt;Thanks in advance for the help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/West-Candy-5732"&gt; /u/West-Candy-5732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pt771o/prompt_injection_demo_in_ollama_help_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pt771o/prompt_injection_demo_in_ollama_help_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pt771o/prompt_injection_demo_in_ollama_help_please/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-22T18:36:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt9tv0</id>
    <title>Title: Update: Yesterday it was 2D. Today, my Local Agent (Qwen 30B) figured out 3D Raycasting. Built from scratch in Python with no 3D engines.</title>
    <updated>2025-12-22T20:19:31+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pt9tv0/title_update_yesterday_it_was_2d_today_my_local/"&gt; &lt;img alt="Title: Update: Yesterday it was 2D. Today, my Local Agent (Qwen 30B) figured out 3D Raycasting. Built from scratch in Python with no 3D engines." src="https://external-preview.redd.it/ZDJjcXY5czlkdDhnMVpKx18zu2Al60haJHCIipoecy1_uH38KnrawZp01IuI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bbeb7e4bb3c56f42b0266cde29ca822530ff055" title="Title: Update: Yesterday it was 2D. Today, my Local Agent (Qwen 30B) figured out 3D Raycasting. Built from scratch in Python with no 3D engines." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following my previous post where the agent built a 2D tile engine, I pushed it to the next level: &lt;strong&gt;3D Raycasting.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Challenge:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create a Wolfenstein 3D style engine in pure Python (&lt;code&gt;pygame&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;No 3D libraries allowed, just raw math (Trigonometry).&lt;/li&gt; &lt;li&gt;Must handle wall collisions and perspective correction.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Result:&lt;/strong&gt; The agent (running on Qwen 30B via Ollama/LM Studio) successfully implemented the &lt;strong&gt;DDA Algorithm&lt;/strong&gt;. It initially struggled with a &amp;quot;barcode effect&amp;quot; and low FPS, but after a few autonomous feedback loops, it optimized the rendering to draw 4-pixel strips instead of single lines.&lt;/p&gt; &lt;p&gt;It also autonomously implemented &lt;strong&gt;Directional Shading&lt;/strong&gt; (lighter color for X-walls, darker for Y-walls) to give it that &amp;quot;Cyberpunk/Tron&amp;quot; depth.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/th2iyeo9dt8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pt9tv0/title_update_yesterday_it_was_2d_today_my_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pt9tv0/title_update_yesterday_it_was_2d_today_my_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-22T20:19:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptqyzh</id>
    <title>Local vs VPS...</title>
    <updated>2025-12-23T10:47:53+00:00</updated>
    <author>
      <name>/u/pagurix</name>
      <uri>https://old.reddit.com/user/pagurix</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pagurix"&gt; /u/pagurix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1ptp9dq/local_vs_vps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ptqyzh/local_vs_vps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ptqyzh/local_vs_vps/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-23T10:47:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptqvvh</id>
    <title>Ollama for 3D models</title>
    <updated>2025-12-23T10:42:12+00:00</updated>
    <author>
      <name>/u/Digital_Calendar_695</name>
      <uri>https://old.reddit.com/user/Digital_Calendar_695</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ptqvvh/ollama_for_3d_models/"&gt; &lt;img alt="Ollama for 3D models" src="https://external-preview.redd.it/zj7DSc3w-zwxzS2_x9K_PvO2eD7C1IjPQMGbnhyQXVU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e0fb3cbb460579fb92b2abb3e828cb0973f7f48" title="Ollama for 3D models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have check this video using local LLMs to create 3D models in Blender?&lt;/p&gt; &lt;p&gt;It seems small models cannot handle many tasks Has anyone tried bigger local models with MCP like this one?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Digital_Calendar_695"&gt; /u/Digital_Calendar_695 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/0PSOCFHBAfw?si=eDYokRcNPD5iYDBL"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ptqvvh/ollama_for_3d_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ptqvvh/ollama_for_3d_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-23T10:42:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu409q</id>
    <title>Ollama not outputing for Qwen3 80B Next Instruct, but works for Thinking model. Nothing in log.</title>
    <updated>2025-12-23T20:19:39+00:00</updated>
    <author>
      <name>/u/vulcan4d</name>
      <uri>https://old.reddit.com/user/vulcan4d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a weird issue where Ollama does not give me any output for Gwen3 Next 80B Instruct though it gives me token results. I see the same thing running in terminal. When I pull up the log I don't see anything useful. Anyone come accross something like this? Everything is on the latest version. I tried Q4 down to Q2 Quants, but the thinking version of this model works without any issues.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/27ooi0og209g1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55579ada7461fa7258cc1c6a908111b1fb957005"&gt;https://preview.redd.it/27ooi0og209g1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55579ada7461fa7258cc1c6a908111b1fb957005&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The log shows absolutely nothing useful&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ts6lb8t7309g1.png?width=1341&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84785ddb224466e38803a10a37f8d05bab3c08d7"&gt;Running from Open WebUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j9ujcugk309g1.png?width=1351&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b31d610451aa2550cba448960ec82e2c6b09c22"&gt;Running locally via terminal&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vulcan4d"&gt; /u/vulcan4d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pu409q/ollama_not_outputing_for_qwen3_80b_next_instruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pu409q/ollama_not_outputing_for_qwen3_80b_next_instruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pu409q/ollama_not_outputing_for_qwen3_80b_next_instruct/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-23T20:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu6sgl</id>
    <title>I built a native Go runtime to give local Llama 3 "Real Hands" (File System + Browser)</title>
    <updated>2025-12-23T22:17:40+00:00</updated>
    <author>
      <name>/u/AgencySpecific</name>
      <uri>https://old.reddit.com/user/AgencySpecific</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Frustration: Running DeepSeek V3 or Llama 3 locally via Ollama is amazing, but let's be honest: they are &amp;quot;Brains in Jars.&amp;quot;&lt;/p&gt; &lt;p&gt;They can write incredible code, but they can't save it. They can plan research, but they can't browse the docs. I got sick of the &amp;quot;Chat -&amp;gt; Copy Code -&amp;gt; Alt-Tab -&amp;gt; Paste -&amp;gt; Error&amp;quot; loop.&lt;/p&gt; &lt;p&gt;The Project (Runiq): I didn't want another fragile Python wrapper that breaks my venv every week. So I built a standalone MCP Server in Go.&lt;/p&gt; &lt;p&gt;What it actually does:&lt;/p&gt; &lt;p&gt;File System Access: You prompt: &amp;quot;Refactor the ./src folder.&amp;quot; Runiq actually reads the files, sends the context to Ollama, and applies the edits locally.&lt;/p&gt; &lt;p&gt;Stealth Browser: You prompt: &amp;quot;Check the docs at stripe.com.&amp;quot; It spins up a headless browser (bypassing Cloudflare) to give the model real-time context.&lt;/p&gt; &lt;p&gt;The &amp;quot;Air Gap&amp;quot; Firewall: Giving a local model root is scary. Runiq intercepts every write or delete syscall. You get a native OS popup to approve the action. It can't wipe your drive unless you say yes.&lt;/p&gt; &lt;p&gt;Why Go?&lt;/p&gt; &lt;p&gt;Speed: It's instant.&lt;/p&gt; &lt;p&gt;Portability: Single 12MB binary. No pip install, no Docker.&lt;/p&gt; &lt;p&gt;Safety: Memory safe and strictly typed.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/qaysSE/runiq"&gt;https://github.com/qaysSE/runiq&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I built this to turn my local Ollama setup into a fully autonomous agent. Let me know what you think of the architecture.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AgencySpecific"&gt; /u/AgencySpecific &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pu6sgl/i_built_a_native_go_runtime_to_give_local_llama_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pu6sgl/i_built_a_native_go_runtime_to_give_local_llama_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pu6sgl/i_built_a_native_go_runtime_to_give_local_llama_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-23T22:17:40+00:00</published>
  </entry>
</feed>
