<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-19T22:23:49+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1nizasb</id>
    <title>computron_9000</title>
    <updated>2025-09-17T00:59:45+00:00</updated>
    <author>
      <name>/u/larz01larz</name>
      <uri>https://old.reddit.com/user/larz01larz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nizasb/computron_9000/"&gt; &lt;img alt="computron_9000" src="https://external-preview.redd.it/rx0LgLIbfwDLaMi2xA7oRUXabQDybdqzNQj1X9cd914.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b126cb1c8b3bcdc563ad80a3688099487c5fad9f" title="computron_9000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Still working on &lt;a href="https://github.com/lefoulkrod/computron_9000"&gt;computron&lt;/a&gt;. It's not really just a chat UI on top of ollama, althought it does do that. It is more like my own personal AI assistant. I've been adding a bunch of tools and agents to it so it can do web research, write and run code, execute shell commands. It's kind of big heap of agents and tools but I'm slowly stitching it together into something useful. Take a look and if interested in contributing feel free to submit a PR.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mih4jhyzimpf1.png?width=3744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc36c55b82bae69f66cad127da1441c7bcb8bbd1"&gt;https://preview.redd.it/mih4jhyzimpf1.png?width=3744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc36c55b82bae69f66cad127da1441c7bcb8bbd1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/larz01larz"&gt; /u/larz01larz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nizasb/computron_9000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nizasb/computron_9000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nizasb/computron_9000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T00:59:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj75jk</id>
    <title>Can I use Cursor Agent (or similar) with a local LLM setup (8B / 13B)?</title>
    <updated>2025-09-17T08:05:00+00:00</updated>
    <author>
      <name>/u/BudgetPurple3002</name>
      <uri>https://old.reddit.com/user/BudgetPurple3002</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BudgetPurple3002"&gt; /u/BudgetPurple3002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nj758c/can_i_use_cursor_agent_or_similar_with_a_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nj75jk/can_i_use_cursor_agent_or_similar_with_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nj75jk/can_i_use_cursor_agent_or_similar_with_a_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T08:05:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj9o91</id>
    <title>Autonomous Pen testing AI.</title>
    <updated>2025-09-17T10:43:38+00:00</updated>
    <author>
      <name>/u/SkillPatient6465</name>
      <uri>https://old.reddit.com/user/SkillPatient6465</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkillPatient6465"&gt; /u/SkillPatient6465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AI_Agents/comments/1nj98dl/autonomous_pen_testing_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nj9o91/autonomous_pen_testing_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nj9o91/autonomous_pen_testing_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T10:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj3s85</id>
    <title>ArchGW 0.3.12 üöÄ Model aliases: allow clients to use friendly, semantic names and swap out underlying models without changing application code.</title>
    <updated>2025-09-17T04:39:53+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nj3s85/archgw_0312_model_aliases_allow_clients_to_use/"&gt; &lt;img alt="ArchGW 0.3.12 üöÄ Model aliases: allow clients to use friendly, semantic names and swap out underlying models without changing application code." src="https://preview.redd.it/igehvjyamnpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db6eae6aeac91ce9c0e5bfab3913e58f5380f1d9" title="ArchGW 0.3.12 üöÄ Model aliases: allow clients to use friendly, semantic names and swap out underlying models without changing application code." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I added this lightweight abstraction to &lt;a href="https://github.com/katanemo/archgw"&gt;archgw&lt;/a&gt; to decouple app code from specific model names. Instead of sprinkling hardcoded model names like&lt;code&gt;gpt-4o-mini&lt;/code&gt; or &lt;code&gt;llama3.2&lt;/code&gt; everywhere, you point to an &lt;em&gt;alias&lt;/em&gt; that encodes intent, and allows you to test new models, swap out the config safely without having to do codewide search/replace every time you want to experiment with a new model or version.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;arch.summarize.v1 ‚Üí cheap/fast summarization arch.v1 ‚Üí default ‚Äúlatest‚Äù general-purpose model arch.reasoning.v1 ‚Üí heavier reasoning &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The app calls the alias, not the vendor. Swap the model in config, and the entire system updates without touching code. Of course, you would want to use models compatible. Meaning if you map an embedding model to an alias, when the application expects a chat model, it won't be a good day.&lt;/p&gt; &lt;p&gt;Where are we headed with this...&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Guardrails -&amp;gt; Apply safety, cost, or latency rules at the alias level: arch.reasoning.v1:&lt;/p&gt; &lt;p&gt;arch.reasoning.v1: target: gpt-oss-120b guardrails: max_latency: 5s block_categories: [‚Äújailbreak‚Äù, ‚ÄúPII‚Äù]&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Fallbacks -&amp;gt; Provide a chain if a model fails or hits quota:&lt;/p&gt; &lt;p&gt;arch.summarize.v1: target: gpt-4o-mini fallback: llama3.2&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Traffic splitting &amp;amp; canaries -&amp;gt; Let an alias fan out traffic across multiple targets:&lt;/p&gt; &lt;p&gt;arch.v1: targets: - model: llama3.2 weight: 80 - model: gpt-4o-mini weight: 20&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/igehvjyamnpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nj3s85/archgw_0312_model_aliases_allow_clients_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nj3s85/archgw_0312_model_aliases_allow_clients_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T04:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjuoo</id>
    <title>Made a tutorial app for LLM basics: A.I. DelvePad - iOS Opensource</title>
    <updated>2025-09-17T17:39:56+00:00</updated>
    <author>
      <name>/u/Other_Passion_4710</name>
      <uri>https://old.reddit.com/user/Other_Passion_4710</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1njjuoo/made_a_tutorial_app_for_llm_basics_ai_delvepad/"&gt; &lt;img alt="Made a tutorial app for LLM basics: A.I. DelvePad - iOS Opensource" src="https://b.thumbs.redditmedia.com/HIVCeWKeDd_gE60MPzg0Fp_7OEUdurpTcssJl-2hNaw.jpg" title="Made a tutorial app for LLM basics: A.I. DelvePad - iOS Opensource" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I saw there are lots of AI wrapper apps made, but few having tutorials about LLM training and specs.&lt;/p&gt; &lt;p&gt;I built one called A.I. DelvePad ‚Äî a free Opensource iOS app designed for anyone who wants to get a basic foundation in generative AI.&lt;/p&gt; &lt;p&gt;It has :&lt;/p&gt; &lt;p&gt;‚Ä¢Bite-sized video tutorials you can watch on the go&lt;/p&gt; &lt;p&gt;‚Ä¢A glossary of key AI terms&lt;/p&gt; &lt;p&gt;‚Ä¢A quick overview of how LLMs are trained&lt;/p&gt; &lt;p&gt;‚Ä¢A tutorial sharing function so you can pass what you learn to friends&lt;/p&gt; &lt;p&gt;‚Ä¢All tutorials are all free.&lt;/p&gt; &lt;p&gt;Looking to get more feedback, would love to hear yours. Some LLM development is done in Go and Rust. If you‚Äôve been curious about AI models but didn‚Äôt know where to start, this might be a good starter pack for you.&lt;/p&gt; &lt;p&gt;App Store link : &lt;a href="https://apps.apple.com/us/app/a-i-delvepad/id6743481267"&gt;https://apps.apple.com/us/app/a-i-delvepad/id6743481267&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/leapdeck/AIDelvePad"&gt;https://github.com/leapdeck/AIDelvePad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Site: &lt;a href="http://aidelvepad.com"&gt;http://aidelvepad.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love any input you‚Äôve got, please share. And if you‚Äôre building too ‚Äî keep going! Enjoy making mobile projects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Passion_4710"&gt; /u/Other_Passion_4710 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1njjuoo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1njjuoo/made_a_tutorial_app_for_llm_basics_ai_delvepad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1njjuoo/made_a_tutorial_app_for_llm_basics_ai_delvepad/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T17:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1njxih7</id>
    <title>how to hide thoughts</title>
    <updated>2025-09-18T03:17:03+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What command to add at prompt to hide thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1njxih7/how_to_hide_thoughts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1njxih7/how_to_hide_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1njxih7/how_to_hide_thoughts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-18T03:17:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1njkndb</id>
    <title>How do I get ollama to show only the installed models in the app?</title>
    <updated>2025-09-17T18:08:12+00:00</updated>
    <author>
      <name>/u/temploupegarou</name>
      <uri>https://old.reddit.com/user/temploupegarou</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1njkndb/how_do_i_get_ollama_to_show_only_the_installed/"&gt; &lt;img alt="How do I get ollama to show only the installed models in the app?" src="https://a.thumbs.redditmedia.com/xVSTNenrT_UGgiT2x27GmNMcQiKZF4bkuIncOlj3iW0.jpg" title="How do I get ollama to show only the installed models in the app?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently built a new pc and sold my old laptop that had ollama on it and had been away from the scene for a bit. Next thing I know there's a whole app and no need to install openWebUI - win! but this app shows me ALL the available models and the setting screen doesn't have anything to make this happen. &lt;/p&gt; &lt;p&gt;The app:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9xhxv2bclrpf1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12e49860f4475d746fec1ee75a279419337d6d8d"&gt;https://preview.redd.it/9xhxv2bclrpf1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12e49860f4475d746fec1ee75a279419337d6d8d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Installed models: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zzudxwgcmrpf1.png?width=634&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b13e00b9a1156bfea4c6491eb3e03d57fefadd0d"&gt;https://preview.redd.it/zzudxwgcmrpf1.png?width=634&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b13e00b9a1156bfea4c6491eb3e03d57fefadd0d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want only these to be shown in the app. A few times now I've clicked on a model that didn't exist and it starts downloading it which is annoying. I can install models manually. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/temploupegarou"&gt; /u/temploupegarou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1njkndb/how_do_i_get_ollama_to_show_only_the_installed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1njkndb/how_do_i_get_ollama_to_show_only_the_installed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1njkndb/how_do_i_get_ollama_to_show_only_the_installed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T18:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nje7r4</id>
    <title>Coding on CLI</title>
    <updated>2025-09-17T14:10:30+00:00</updated>
    <author>
      <name>/u/booknerdcarp</name>
      <uri>https://old.reddit.com/user/booknerdcarp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a particular model that will function like Claude Code (especially writing to files) that can be used with Ollama? The costs and limits are a pain!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/booknerdcarp"&gt; /u/booknerdcarp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nje7r4/coding_on_cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nje7r4/coding_on_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nje7r4/coding_on_cli/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T14:10:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkm9ea</id>
    <title>Uncensored LLM Site</title>
    <updated>2025-09-18T22:21:35+00:00</updated>
    <author>
      <name>/u/StevenMango</name>
      <uri>https://old.reddit.com/user/StevenMango</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi ! Looking for some advice on where I can find out more about Uncensored or Abliterated LLM. Have just joined the scene and am a complete novice on these matters..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StevenMango"&gt; /u/StevenMango &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkm9ea/uncensored_llm_site/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkm9ea/uncensored_llm_site/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nkm9ea/uncensored_llm_site/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-18T22:21:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk4cbi</id>
    <title>Flashy sentient agi</title>
    <updated>2025-09-18T10:12:41+00:00</updated>
    <author>
      <name>/u/Adventurous-Lunch332</name>
      <uri>https://old.reddit.com/user/Adventurous-Lunch332</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sentient GRID hype: flashy multi-agent orchestration, passing summaries, marketing spectacle. Reality: it is not AGI. Multi-step reasoning fades quickly, context fragments, and infrastructure costs rise sharply. GRID focuses on complexity and modularity rather than practical performance or deep understanding.&lt;/p&gt; &lt;p&gt;A better approach is to fine-tune specific parameters in a single model, activating only the most relevant ones for each task. Combine this with detailed Chain-of-Thought reasoning, integrate relevant tools dynamically for fact-checking and information retrieval, and feed in high-quality, curated data. Flexible tool budgets allow the model to explore deeply without wasting compute or losing efficiency, preserving reasoning, coherence, and output quality across complex tasks.&lt;/p&gt; &lt;p&gt;Benefits of this approach include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full context reasoning preserved, avoiding the degradation seen in multi-agent GRID setups&lt;/li&gt; &lt;li&gt;Efficient compute usage while maintaining high performance&lt;/li&gt; &lt;li&gt;Anti-fragile design that adapts locally and handles dynamic or unexpected data&lt;/li&gt; &lt;li&gt;Flexible, dynamic tool calls triggered by uncertainty, ensuring depth where needed&lt;/li&gt; &lt;li&gt;Transparent, traceable reasoning steps that make debugging and validation easier&lt;/li&gt; &lt;li&gt;Multi-step reasoning maintained across tasks and domains&lt;/li&gt; &lt;li&gt;Dynamic integration of external knowledge without breaking context or flow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Tradeoff: GRID is flashy and modular, but reasoning is shallow, brittle, and costly. This fine-tuned single-model system is practical, efficient, deeply reasoning, anti-fragile, and optimized for real-world AI applications.&lt;/p&gt; &lt;p&gt;Full in-depth discussion covers edge-level AI workflow, CoT reasoning, tool orchestration strategies, and task-specific parameter activation for maximum performance and efficiency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Lunch332"&gt; /u/Adventurous-Lunch332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk4cbi/flashy_sentient_agi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk4cbi/flashy_sentient_agi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nk4cbi/flashy_sentient_agi/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-18T10:12:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk5oba</id>
    <title>Uncensored AI model for from 4b Max 8b</title>
    <updated>2025-09-18T11:27:54+00:00</updated>
    <author>
      <name>/u/Francetor</name>
      <uri>https://old.reddit.com/user/Francetor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I want to host an AI on a mini PC with Linux/Ubuntu operating system (Beelink MINI-S13 Pro Mini PC, Intel Twin Alder Lake-N150 Processor (up to 3.60 GHz), Mini Computer, 16 GB RAM, 500 GB SSD, Office Desktop, Dual HDMI/WiFi 6/BT 5.2/RJ45/WOL). &lt;/p&gt; &lt;p&gt;I have an existential problem and I don't know which model to use, I tried one from 1.5b and one from 3.8b (I don't remember the names) but unfortunately they suffer from various hallucinations (the moon is full of lava wtf). Could you recommend me a preferably uncensored model that goes in a range of 4b maximum 8b (I would like to have a bit of speed). Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Francetor"&gt; /u/Francetor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk5oba/uncensored_ai_model_for_from_4b_max_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk5oba/uncensored_ai_model_for_from_4b_max_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nk5oba/uncensored_ai_model_for_from_4b_max_8b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-18T11:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk9a6z</id>
    <title>How to calculate and estimate GPU usage of Foundation Model</title>
    <updated>2025-09-18T14:07:42+00:00</updated>
    <author>
      <name>/u/Tough_Wrangler_6075</name>
      <uri>https://old.reddit.com/user/Tough_Wrangler_6075</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nk9a6z/how_to_calculate_and_estimate_gpu_usage_of/"&gt; &lt;img alt="How to calculate and estimate GPU usage of Foundation Model" src="https://external-preview.redd.it/l8zbwfivRPZFibDrfAVyiwk3xzzVRMZEG-8qAUEaMrk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=398873bb225b3ffa88d2fe1166113ba87f9957a0" title="How to calculate and estimate GPU usage of Foundation Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I wrote an article about how to actually calculate the cost of gpu in term's you used open model and using your own setup. I used reference from AI Engineering book and actually compare by my own. I found that, open model with greater parameter of course better at reasoning but very consume more computation. Hope it will help you to understanding the the calculation. Happy reading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tough_Wrangler_6075"&gt; /u/Tough_Wrangler_6075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@zackydzacky/how-to-calculate-and-estimate-gpu-usage-of-foundation-model-f2e493af339b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk9a6z/how_to_calculate_and_estimate_gpu_usage_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nk9a6z/how_to_calculate_and_estimate_gpu_usage_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-18T14:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkk0s5</id>
    <title>Offline Ollama GUI Help</title>
    <updated>2025-09-18T20:52:30+00:00</updated>
    <author>
      <name>/u/ExplorerOk996</name>
      <uri>https://old.reddit.com/user/ExplorerOk996</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying to get the Ollama GUI working on an offline windows 10 pc with no luck. It works fine with the command prompt as far as I know. If I try to use ollama app.exe, it just &amp;quot;hangs&amp;quot;. &lt;/p&gt; &lt;p&gt;I downloaded the ollama windows installer from the ollama website on my laptop. I then copied that installer onto the pc and ran it. After that, I copied models from my laptop over to the pc. I feel like I might be missing some additional required files. Downloading files on my laptop and copying them over is the only method I currently have to update the pc (the pc is more powerful than the laptop). I'm not too worried about it working, but it would be nice to have.&lt;/p&gt; &lt;p&gt;Any help would be appreciated. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExplorerOk996"&gt; /u/ExplorerOk996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkk0s5/offline_ollama_gui_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkk0s5/offline_ollama_gui_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nkk0s5/offline_ollama_gui_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-18T20:52:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk1qkf</id>
    <title>LLM VRAM/RAM Calculator</title>
    <updated>2025-09-18T07:22:22+00:00</updated>
    <author>
      <name>/u/SmilingGen</name>
      <uri>https://old.reddit.com/user/SmilingGen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a simple tool to estimate how much memory is needed to run GGUF models locally, based on your desired maximum context size.&lt;/p&gt; &lt;p&gt;You just paste the direct download URL of a GGUF model (for example, from Hugging Face), enter the context length you plan to use, and it will give you an approximate memory requirement.&lt;/p&gt; &lt;p&gt;It‚Äôs especially useful if you're trying to figure out whether a model will fit in your available VRAM or RAM, or when comparing different quantization levels like Q4_K_M vs Q8_0.&lt;/p&gt; &lt;p&gt;The tool is completely free and open-source. You can try it here: &lt;a href="https://www.kolosal.ai/memory-calculator"&gt;https://www.kolosal.ai/memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And check out the code on GitHub: &lt;a href="https://github.com/KolosalAI/model-memory-calculator"&gt;https://github.com/KolosalAI/model-memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd really appreciate any feedback, suggestions, or bug reports if you decide to give it a try.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SmilingGen"&gt; /u/SmilingGen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk1qkf/llm_vramram_calculator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk1qkf/llm_vramram_calculator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nk1qkf/llm_vramram_calculator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-18T07:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkqvqx</id>
    <title>Is 1070ti no longer supported?</title>
    <updated>2025-09-19T01:53:04+00:00</updated>
    <author>
      <name>/u/allknowing2012</name>
      <uri>https://old.reddit.com/user/allknowing2012</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Went to use ollama today and it was no longer working. (did a quick search after updating).&lt;br /&gt; From my googling - it appears 1070ti is not CUDA arch 6.1 or better?&lt;br /&gt; From log...&lt;br /&gt; C:\a\ollama\ollama\ml\backend\ggml\ggml\src\ggml-cuda\common.cuh:106: ggml was not compiled with any CUDA arch &amp;lt;= 610&lt;/p&gt; &lt;p&gt;Am I pooched for even doing the simplest queries?&lt;/p&gt; &lt;p&gt;Update: Thanks for the comments - installing the 0.12.0 and it is working again!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/allknowing2012"&gt; /u/allknowing2012 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkqvqx/is_1070ti_no_longer_supported/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkqvqx/is_1070ti_no_longer_supported/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nkqvqx/is_1070ti_no_longer_supported/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T01:53:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlbqmj</id>
    <title>An Ollama user seeking uncensored models that can generate images</title>
    <updated>2025-09-19T18:39:45+00:00</updated>
    <author>
      <name>/u/Ghostone89</name>
      <uri>https://old.reddit.com/user/Ghostone89</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been liking the privacy and freedom of running models locally. I've primarily been doing it for roleplay and creative writing, but I'm looking to take things further&lt;/p&gt; &lt;p&gt;My goal is to find a model that is:&lt;/p&gt; &lt;p&gt;Uncensored: I need something with minimal to no filters for creative, long-form roleplay.&lt;/p&gt; &lt;p&gt;Image-capable: The key is a model that can actually generate and send images within the chat, not just analyze them.&lt;/p&gt; &lt;p&gt;I know that multimodal models like LLaVA exist, but I'm looking for specific recommendations from people who have used these models for this particular purpose. Which model do you recommend for combining uncensored roleplay with in-chat image generation? Are there any specific workflows or UIs that make this seamless?&lt;/p&gt; &lt;p&gt;Currently I know some sites are able to do this but I want to know if there are open-source ones too&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ghostone89"&gt; /u/Ghostone89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlbqmj/an_ollama_user_seeking_uncensored_models_that_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlbqmj/an_ollama_user_seeking_uncensored_models_that_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlbqmj/an_ollama_user_seeking_uncensored_models_that_can/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T18:39:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkpg8z</id>
    <title>Integrating LLM with enterprise DB</title>
    <updated>2025-09-19T00:45:22+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw lots of ads about how you could integrate LLM to pull data from your own database. For example, an AI can get data from your CRM db etc. I want to do similar but not sure where to start. &lt;/p&gt; &lt;p&gt;any suggestion or sample project as reference are most welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkpg8z/integrating_llm_with_enterprise_db/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkpg8z/integrating_llm_with_enterprise_db/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nkpg8z/integrating_llm_with_enterprise_db/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T00:45:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nl3wnx</id>
    <title>ADAM - First Agile Digital Assistant for Managers</title>
    <updated>2025-09-19T13:41:21+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ADAM is a personal project based on ollama LLM that really tackles Agile Project management issues. Ask ADAM about Agile and Traditional project management practices. &lt;/p&gt; &lt;p&gt;For sneak peak, visit the site. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://adam-showcase.vercel.app/#videos"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl3wnx/adam_first_agile_digital_assistant_for_managers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nl3wnx/adam_first_agile_digital_assistant_for_managers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T13:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nl7ro3</id>
    <title>Arch Dolphin 3 stuck</title>
    <updated>2025-09-19T16:09:44+00:00</updated>
    <author>
      <name>/u/Representative-Gur71</name>
      <uri>https://old.reddit.com/user/Representative-Gur71</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nl7ro3/arch_dolphin_3_stuck/"&gt; &lt;img alt="Arch Dolphin 3 stuck" src="https://preview.redd.it/vtfexfo3b5qf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18109aa7946325075ac9bb7ee61371c6ec6e55e5" title="Arch Dolphin 3 stuck" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to install ollama and dolphin but my console gets stuck here, doesnt move.&lt;/p&gt; &lt;p&gt;Any solutions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Representative-Gur71"&gt; /u/Representative-Gur71 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vtfexfo3b5qf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl7ro3/arch_dolphin_3_stuck/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nl7ro3/arch_dolphin_3_stuck/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T16:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkswz1</id>
    <title>Using ollama on home network, multiple devices</title>
    <updated>2025-09-19T03:30:54+00:00</updated>
    <author>
      <name>/u/Smellslikebuttsinher</name>
      <uri>https://old.reddit.com/user/Smellslikebuttsinher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nkswz1/using_ollama_on_home_network_multiple_devices/"&gt; &lt;img alt="Using ollama on home network, multiple devices" src="https://b.thumbs.redditmedia.com/El1yh9cvTpnlEy1D2jXDfkHNNWxuywBAg4V4LSFVhhk.jpg" title="Using ollama on home network, multiple devices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone just after some help/advice..&lt;/p&gt; &lt;p&gt;Ive been mucking round with setting up jellyfin at home and had fun with that so thought I'd try out setting up ollama as the next wee project.&lt;/p&gt; &lt;p&gt;So I followed a network chuck tutorial and that had me setup Ubuntu using wsl, then got ollama working fine in the terminal window, next installed docker and got open webui going but couldn't get it to talk to ollama.&lt;/p&gt; &lt;p&gt;There's plenty of stuff out there saying to change firewall settings/modify ollama to see everything etc but nothing works and I've been trying for a couple days on and off now.. &lt;/p&gt; &lt;p&gt;I got frustrated and decided to just download the wi dows app version which works fine and noticed there was a setting to see it across the home network. Can anyone explain how this works? I have 3 models installed on the windows app now so can i just open and leave this running and access it from another computer on the same home network or does it have to be installed on each individual machine if we go this route rather then the linux/docker way..&lt;/p&gt; &lt;p&gt;The pc its hosted on has far more power to run an ai which is why I'd like to have that do all the work and access it from another laptop at home.&lt;/p&gt; &lt;p&gt;Any help or advice is appreciated and thanks in advance :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smellslikebuttsinher"&gt; /u/Smellslikebuttsinher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nkswz1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkswz1/using_ollama_on_home_network_multiple_devices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nkswz1/using_ollama_on_home_network_multiple_devices/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T03:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlccd8</id>
    <title>Which Linux for my app</title>
    <updated>2025-09-19T19:02:37+00:00</updated>
    <author>
      <name>/u/trucmuch83</name>
      <uri>https://old.reddit.com/user/trucmuch83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I‚Äôve been experimenting with app development on a Raspberry Pi 5 üòÖ, but now I‚Äôm looking to upgrade to a new computer so I can run larger models. I‚Äôm planning to get a decent GPU and set up my LLM on Linux ‚Äî any recommendations for which distro works best? Thanks a lot for the help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trucmuch83"&gt; /u/trucmuch83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlccd8/which_linux_for_my_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlccd8/which_linux_for_my_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlccd8/which_linux_for_my_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T19:02:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlemes</id>
    <title>Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp; Streamlit</title>
    <updated>2025-09-19T20:31:33+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nlemes/build_a_local_ai_agent_with_mcp_tools_using/"&gt; &lt;img alt="Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp;amp; Streamlit" src="https://external-preview.redd.it/rq8k6bkBVDqS3EaB-6PmZwrrp9mjAeoX2Tt37ubIdpg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76f91a095d5d6782424d54183f94d9fb060dd411" title="Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp;amp; Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/Baa-z7cum1g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlemes/build_a_local_ai_agent_with_mcp_tools_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlemes/build_a_local_ai_agent_with_mcp_tools_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T20:31:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nl8a7w</id>
    <title>[Project] I created an AI photo organizer that uses Ollama to sort photos, filter duplicates, and write Instagram captions.</title>
    <updated>2025-09-19T16:29:01+00:00</updated>
    <author>
      <name>/u/summitsc</name>
      <uri>https://old.reddit.com/user/summitsc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone at &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I wanted to share a Python project I've been working on called the &lt;strong&gt;AI Instagram Organizer&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; I had thousands of photos from a recent trip, and the thought of manually sorting them, finding the best ones, and thinking of captions was overwhelming. I wanted a way to automate this using local LLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution:&lt;/strong&gt; I built a script that uses a multimodal model via Ollama (like LLaVA, Gemma, or Llama 3.2 Vision) to do all the heavy lifting.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Chronological Sorting:&lt;/strong&gt; It reads EXIF data to organize posts by the date they were taken.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Duplicate Filtering:&lt;/strong&gt; It uses multiple perceptual hashes and a dynamic threshold to remove repetitive shots.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Caption &amp;amp; Hashtag Generation:&lt;/strong&gt; For each post folder it creates, it writes several descriptive caption options and a list of hashtags.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Handles HEIC Files:&lt;/strong&gt; It automatically converts Apple's HEIC format to JPG.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs been a really fun project and a great way to explore what's possible with local vision models. I'd love to get your feedback and see if it's useful to anyone else!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/summitsingh/ai-instagram-organizer"&gt;https://github.com/summitsingh/ai-instagram-organizer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since this is my first time building an open-source AI project, any feedback is welcome. And if you like it, a star on GitHub would really make my day! ‚≠ê&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/summitsc"&gt; /u/summitsc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl8a7w/project_i_created_an_ai_photo_organizer_that_uses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl8a7w/project_i_created_an_ai_photo_organizer_that_uses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nl8a7w/project_i_created_an_ai_photo_organizer_that_uses/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T16:29:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlbxhi</id>
    <title>Simple RAG design architecture</title>
    <updated>2025-09-19T18:47:08+00:00</updated>
    <author>
      <name>/u/Tough_Wrangler_6075</name>
      <uri>https://old.reddit.com/user/Tough_Wrangler_6075</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nlbxhi/simple_rag_design_architecture/"&gt; &lt;img alt="Simple RAG design architecture" src="https://preview.redd.it/80ry05gb36qf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fdf0e5eedcef7768325005fe953976cc41d1977" title="Simple RAG design architecture" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am trying to make a design architecture for my RAG system. If you guys have any suggestions or feedback. Please, I would be happy to hear that &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tough_Wrangler_6075"&gt; /u/Tough_Wrangler_6075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/80ry05gb36qf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlbxhi/simple_rag_design_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlbxhi/simple_rag_design_architecture/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T18:47:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nl0d45</id>
    <title>Ollama or LM Studio?</title>
    <updated>2025-09-19T10:55:22+00:00</updated>
    <author>
      <name>/u/Artaherzadeh</name>
      <uri>https://old.reddit.com/user/Artaherzadeh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to install and run it on my PC, which has a 12600k CPU, 6700XT AMD GPU 12G, and 32GB RAM. Which one is better in terms of features, UI, performance and etc?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Artaherzadeh"&gt; /u/Artaherzadeh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl0d45/ollama_or_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl0d45/ollama_or_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nl0d45/ollama_or_lm_studio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T10:55:22+00:00</published>
  </entry>
</feed>
