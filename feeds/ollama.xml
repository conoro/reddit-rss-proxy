<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-06T14:48:41+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1n87ngf</id>
    <title>Can I use Ollama + OpenWebUI through Docker Engine (In Terminal) or only through Desktop version?</title>
    <updated>2025-09-04T11:33:50+00:00</updated>
    <author>
      <name>/u/PracticalAd6966</name>
      <uri>https://old.reddit.com/user/PracticalAd6966</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently on Linux PC and I really need to use Docker Engine and as I understand they have conflicting files so I can use only one of them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PracticalAd6966"&gt; /u/PracticalAd6966 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n87ngf/can_i_use_ollama_openwebui_through_docker_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n87ngf/can_i_use_ollama_openwebui_through_docker_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n87ngf/can_i_use_ollama_openwebui_through_docker_engine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T11:33:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8c48y</id>
    <title>Clustering apple silicon and nvidia gpu based server.</title>
    <updated>2025-09-04T14:43:45+00:00</updated>
    <author>
      <name>/u/Pedroxns</name>
      <uri>https://old.reddit.com/user/Pedroxns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, &lt;/p&gt; &lt;p&gt;I was just scrolling through reddit and saw a post about the Apple silicon performance.&lt;/p&gt; &lt;p&gt;I already have 2 mac minis here, one M2 + 8gb and one M4+16gb, nothing too fancy, and i'm already running ollama on a PVE VM with a ryzen 5600g + 3060 12gb + 10gb ram( server has 32gb total), nothing fancy either but runs 4b and 7b models for my frigate and my home assistant instances.&lt;/p&gt; &lt;p&gt;My question is; whould I see any high gain by running ollama on the M4 rather than on the 3060? Could I/should I try clustering the 3 machines to run faster/bigger models?&lt;/p&gt; &lt;p&gt;Thanks in advance for the advices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pedroxns"&gt; /u/Pedroxns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8c48y/clustering_apple_silicon_and_nvidia_gpu_based/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8c48y/clustering_apple_silicon_and_nvidia_gpu_based/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8c48y/clustering_apple_silicon_and_nvidia_gpu_based/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T14:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7uhkv</id>
    <title>Hows your experience running Ollama on Apple Sillicon M1, M2, M3 or M4</title>
    <updated>2025-09-03T23:29:58+00:00</updated>
    <author>
      <name>/u/Cultural-You-7096</name>
      <uri>https://old.reddit.com/user/Cultural-You-7096</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How's the experience, Does it run welll like web versions or is it slow. I'm concerned becuase I want to get a Macbook Pro just to run models .&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cultural-You-7096"&gt; /u/Cultural-You-7096 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T23:29:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8irsb</id>
    <title>Posible Ollama usage on this MCP server</title>
    <updated>2025-09-04T18:53:36+00:00</updated>
    <author>
      <name>/u/FreddyDEE90</name>
      <uri>https://old.reddit.com/user/FreddyDEE90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does any of you know if it's possible to use this MCP server &lt;a href="https://github.com/rinadelph/Agent-MCP"&gt;https://github.com/rinadelph/Agent-MCP&lt;/a&gt; with Ollama instead of the openai API key ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FreddyDEE90"&gt; /u/FreddyDEE90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8irsb/posible_ollama_usage_on_this_mcp_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8irsb/posible_ollama_usage_on_this_mcp_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8irsb/posible_ollama_usage_on_this_mcp_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T18:53:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n89sa4</id>
    <title>Most affordable AI computer with GPU (“GPUter”) you can build in 2025?</title>
    <updated>2025-09-04T13:13:36+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n89sa4/most_affordable_ai_computer_with_gpu_gputer_you/"&gt; &lt;img alt="Most affordable AI computer with GPU (“GPUter”) you can build in 2025?" src="https://preview.redd.it/bk6tf5l2e5nf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8da7afc16f4d8ff260c98ad24de5cc8adc50a222" title="Most affordable AI computer with GPU (“GPUter”) you can build in 2025?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bk6tf5l2e5nf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n89sa4/most_affordable_ai_computer_with_gpu_gputer_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n89sa4/most_affordable_ai_computer_with_gpu_gputer_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T13:13:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n91fzn</id>
    <title>Enhanched Chat Interface (You can locally use Google AI mode or Perplexity without setting up anything, just install claraverse and download any small model and good to go)</title>
    <updated>2025-09-05T10:21:02+00:00</updated>
    <author>
      <name>/u/aruntemme</name>
      <uri>https://old.reddit.com/user/aruntemme</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n91fzn/enhanched_chat_interface_you_can_locally_use/"&gt; &lt;img alt="Enhanched Chat Interface (You can locally use Google AI mode or Perplexity without setting up anything, just install claraverse and download any small model and good to go)" src="https://external-preview.redd.it/3ht2kM5gZ_4ghWmHZyOjxU4vR3qpoqFiFfmo_5HzM4U.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9bf626a6e6e572ad9a4677a3e8da97a67fe21f51" title="Enhanched Chat Interface (You can locally use Google AI mode or Perplexity without setting up anything, just install claraverse and download any small model and good to go)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aruntemme"&gt; /u/aruntemme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=bGW0-554RZU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n91fzn/enhanched_chat_interface_you_can_locally_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n91fzn/enhanched_chat_interface_you_can_locally_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T10:21:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8l2hu</id>
    <title>Built an offline AI CLI that generates apps and runs code safely</title>
    <updated>2025-09-04T20:21:45+00:00</updated>
    <author>
      <name>/u/Sea-Reception-2697</name>
      <uri>https://old.reddit.com/user/Sea-Reception-2697</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Reception-2697"&gt; /u/Sea-Reception-2697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/selfhosted/comments/1n8l20c/built_an_offline_ai_cli_that_generates_apps_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8l2hu/built_an_offline_ai_cli_that_generates_apps_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8l2hu/built_an_offline_ai_cli_that_generates_apps_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T20:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8cmcq</id>
    <title>Faster Ollama</title>
    <updated>2025-09-04T15:02:27+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One of my favorite Linux benchmark sites. &lt;/p&gt; &lt;p&gt;ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization - Phoronix &lt;a href="https://share.google/2lCqH4Imkt2dmeS2G"&gt;https://share.google/2lCqH4Imkt2dmeS2G&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;On metal, I see a 2-3% speedup in token rate. On a single RTX 4090 I see a ~7% speedup.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8cmcq/faster_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8cmcq/faster_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8cmcq/faster_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T15:02:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n952sx</id>
    <title>Récupérer les messages erreurs ollama api nodeJs</title>
    <updated>2025-09-05T13:19:24+00:00</updated>
    <author>
      <name>/u/Weekly_Method5407</name>
      <uri>https://old.reddit.com/user/Weekly_Method5407</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;J’utilise actuellement pour mon projet l’api Ollama nodeJs. Ceci dit, j’effectue des route api vers ceci et j’aimerais pouvoir récupérer les erreurs qui parfois s’affiche comme « Fetch failed » j’aimerais pouvoir l’afficher côté frontend et pouvoir ensuite effectuer des actions comme réessayer ect… merci d’avance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weekly_Method5407"&gt; /u/Weekly_Method5407 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n952sx/récupérer_les_messages_erreurs_ollama_api_nodejs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n952sx/récupérer_les_messages_erreurs_ollama_api_nodejs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n952sx/récupérer_les_messages_erreurs_ollama_api_nodejs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T13:19:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8hfm7</id>
    <title>I made a simple C# agent that uses local Ollama models to manage my file system</title>
    <updated>2025-09-04T18:02:32+00:00</updated>
    <author>
      <name>/u/cride20</name>
      <uri>https://old.reddit.com/user/cride20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm a huge fan of running models locally and wanted to build something practical with them. So, I created &lt;strong&gt;AI Slop&lt;/strong&gt;: a C# console agent that lets you use natural language to create folders, write files, and manage a workspace.&lt;/p&gt; &lt;p&gt;It's all powered by Ollama and a model capable of tool use (I've had great success with qwen3-coder:30b-a3b-q4_K_M). The agent follows a strict &amp;quot;think-act-observe&amp;quot; loop.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Prompting Strategies:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Strict JSON Output:&lt;/strong&gt; The prompt demands that the only output is a single raw JSON object with two keys: &amp;quot;thought&amp;quot; and &amp;quot;tool_call&amp;quot;. No markdown, no preamble. This makes parsing super reliable.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;One Tool at a Time:&lt;/strong&gt; This is the most critical rule in the prompt. I explicitly forbid the model from trying to chain commands in one response. This forces it to wait for feedback from the environment after every single action, which prevents it from getting lost or making assumptions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Situational Awareness:&lt;/strong&gt; I encourage it to constantly use the GetWorkspaceEntries tool to check the contents of its current directory before acting, which dramatically reduces errors.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Defined Toolset:&lt;/strong&gt; The prompt includes a &amp;quot;manual&amp;quot; for all the available C# functions, including the tool name, description, and argument format (e.g., CreateFile, OpenFolder, TaskDone).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It's been fascinating to see how a well-structured prompt can turn a general-purpose LLM into a reliable tool-using agent.&lt;/p&gt; &lt;p&gt;The project is open source if you want to check out the full system prompt or run it yourself!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/cride9/AISlop"&gt;cride9/AISlop&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What other tools do you think would be useful for an agent like this?&lt;br /&gt; Inspired by the Manus project&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example output &amp;amp; workflow is located here:&lt;/strong&gt; &lt;a href="https://github.com/cride9/AISlop/blob/master/example/EXAMPLE_WORKFLOW.md"&gt;EXAMPLE_WORKFLOW.md&lt;/a&gt; &lt;a href="https://github.com/cride9/AISlop/blob/master/example/EXAMPLE_OUTPUT.md"&gt;EXAMPLE_OUTPUT.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example video about the Agent: &lt;a href="https://www.youtube.com/watch?v=rZmKbu9Q9w4"&gt;AISlop: A General AI Agent | OpenSource&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cride20"&gt; /u/cride20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8hfm7/i_made_a_simple_c_agent_that_uses_local_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8hfm7/i_made_a_simple_c_agent_that_uses_local_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8hfm7/i_made_a_simple_c_agent_that_uses_local_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T18:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8td0n</id>
    <title>Can you offload the entire LLM functionality to Ollama Turbo which means local hardware does not require GPU?</title>
    <updated>2025-09-05T02:23:53+00:00</updated>
    <author>
      <name>/u/ComedianObjective572</name>
      <uri>https://old.reddit.com/user/ComedianObjective572</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! Idk if this is a stupid idea for this Ollama Turbo. I want to run GPT-OSS but I don’t have the necessary hardware for it. Is it possible to offload the entire Ollama functionality to Ollama Turbo. &lt;/p&gt; &lt;p&gt;For example, local server with Ubuntu Server, Intel Core I5, 8GB RAM, NO GPU which servers Front End and Back End functionality to 3 computers. If I need to run an GPT-OSS, can I offload the entire thing to Ollama Turbo?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComedianObjective572"&gt; /u/ComedianObjective572 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8td0n/can_you_offload_the_entire_llm_functionality_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8td0n/can_you_offload_the_entire_llm_functionality_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8td0n/can_you_offload_the_entire_llm_functionality_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T02:23:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8vkv7</id>
    <title>Spam ending up being published?</title>
    <updated>2025-09-05T04:16:32+00:00</updated>
    <author>
      <name>/u/gnu-trix</name>
      <uri>https://old.reddit.com/user/gnu-trix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So... has anyone seen this?&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/puffymattresscode2025/puffy-mattress-coupon-code-2025-verified"&gt;https://ollama.com/puffymattresscode2025/puffy-mattress-coupon-code-2025-verified&lt;/a&gt;&lt;/p&gt; &lt;p&gt;^^ DISCLAIMER: you almost certainly should not pull this. I'm just pointing it out.&lt;/p&gt; &lt;p&gt;It says it was published 5 days ago. I'm REALLY super curious as to what it is, so if anyone has a VLAN'd Qubes with a VPN'd remote desktop running Ollama, could you pull it and try it out and report back here what it does? (Only suggesting this as a testing ground because there's AI malware now. I have no idea what makes models &amp;quot;run&amp;quot; - maybe it's not executable and wouldn't matter?)&lt;/p&gt; &lt;p&gt;But yeah anyway, do spam AI models often end up being published on Ollama, or is this a rare occurrence?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gnu-trix"&gt; /u/gnu-trix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8vkv7/spam_ending_up_being_published/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8vkv7/spam_ending_up_being_published/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8vkv7/spam_ending_up_being_published/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T04:16:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9beia</id>
    <title>What hardware would you get for $1500 or less...Mac M4 pro 24GB or AMD Ryzen AI Max+ 395 64gb?</title>
    <updated>2025-09-05T17:25:26+00:00</updated>
    <author>
      <name>/u/abrandis</name>
      <uri>https://old.reddit.com/user/abrandis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm in the need of more powerful LLM hardware, something in the affordable price range of $1500ish... It seems to me the two best options are the Ryzen AI Max+ 395 64gb or a Mac mini 4 pro with about 24gb , what is this subreddit feeling? &lt;/p&gt; &lt;p&gt;The Amd seems like it has better value in terms of specs and memory' but I hear a lot of issues with GPU driver support/performance...but I have neither so I'm curious ...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abrandis"&gt; /u/abrandis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9beia/what_hardware_would_you_get_for_1500_or_lessmac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9beia/what_hardware_would_you_get_for_1500_or_lessmac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9beia/what_hardware_would_you_get_for_1500_or_lessmac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T17:25:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8kigw</id>
    <title>Got Gemma running locally on a Raspberry Pi 5 with Ollama</title>
    <updated>2025-09-04T20:00:23+00:00</updated>
    <author>
      <name>/u/Ricardo_Sappia</name>
      <uri>https://old.reddit.com/user/Ricardo_Sappia</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n8kigw/got_gemma_running_locally_on_a_raspberry_pi_5/"&gt; &lt;img alt="Got Gemma running locally on a Raspberry Pi 5 with Ollama" src="https://b.thumbs.redditmedia.com/ncJBXF0mM3RxpxHtBdAzWA1EyKrG2x3mjY__CLeR-qM.jpg" title="Got Gemma running locally on a Raspberry Pi 5 with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share a quick win: I got Gemma 2B (GGUF) running fully offline on a Raspberry Pi 5 (4GB) using Ollama.&lt;/p&gt; &lt;p&gt;It’s part of a side project called Dashi — a modular e-paper dashboard that displays Strava, Garmin, and weather data… plus motivational messages generated by a local AI fox .&lt;/p&gt; &lt;p&gt;No cloud, no API keys — just local inference and surprisingly smooth performance for short outputs.&lt;/p&gt; &lt;p&gt;You can see more details here if curious: 👉 &lt;a href="https://www.hackster.io/rsappia/e-paper-dashboard-where-sport-ai-and-paper-meet-10c0f0"&gt;https://www.hackster.io/rsappia/e-paper-dashboard-where-sport-ai-and-paper-meet-10c0f0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer any questions about the setup or the integration!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ricardo_Sappia"&gt; /u/Ricardo_Sappia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n8kigw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8kigw/got_gemma_running_locally_on_a_raspberry_pi_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8kigw/got_gemma_running_locally_on_a_raspberry_pi_5/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T20:00:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8k3j3</id>
    <title>Power Up your Ollama Models! Thanks to you guys, I made this framework that lets your models watch the screen and help you out! (Open Source and Local)</title>
    <updated>2025-09-04T19:44:13+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n8k3j3/power_up_your_ollama_models_thanks_to_you_guys_i/"&gt; &lt;img alt="Power Up your Ollama Models! Thanks to you guys, I made this framework that lets your models watch the screen and help you out! (Open Source and Local)" src="https://external-preview.redd.it/dW9pNmwzeGxiN25mMc1Nh3OUDLTuFDtnMrFXEDpwYIUIEihHJF3jJPncl3qU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf64148d6af2db710bc91a6b36681250b2ab77fc" title="Power Up your Ollama Models! Thanks to you guys, I made this framework that lets your models watch the screen and help you out! (Open Source and Local)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; Observer now has an Overlay and Shortcut features! Now you can run agents that help you out at any time while watching your screen.&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;I'm back with another Observer update c:&lt;/p&gt; &lt;p&gt;Thank you so much for your support and feedback! I'm still working hard to make Observer useful in a variety of ways. And i'm trying to make Local models accessible to everyone!&lt;/p&gt; &lt;p&gt;So this update is an Overlay that lets your agents give you information on top of whatever you're doing. The obvious use case is helping out in coding problems, but there are other really cool things you can do with it! (specially adding the overlay to other already working agents). These are some cases where the Overlay can be useful:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Coding Assistant:&lt;/strong&gt; Use a shortcut and send whatever problem you're seeing to an LLM for it to solve it.&lt;br /&gt; &lt;strong&gt;Writing Assistant:&lt;/strong&gt; Send the text you're looking at to an LLM to get suggestions on what to write better or how to construct a better story.&lt;br /&gt; &lt;strong&gt;Activity Tracker:&lt;/strong&gt; Have an agent log on the overlay the last time you were doing something specific, then just by glancing at it you can get an idea of how much time you've spent doing something.&lt;br /&gt; &lt;strong&gt;Distraction Logger:&lt;/strong&gt; Same as the activity tracker, you just get messages passively when it thinks you're distracted.&lt;br /&gt; &lt;strong&gt;Video Watching Companion:&lt;/strong&gt; Watch a video and have a model label every new topic discussed and see it in the overlay!&lt;/p&gt; &lt;p&gt;Or any other agent you already had working, just &lt;strong&gt;power it up&lt;/strong&gt; by seeing what it's doing with the Overlay!&lt;/p&gt; &lt;p&gt;This is the projects &lt;a href="https://github.com/Roy3838/Observer"&gt;Github&lt;/a&gt; (completely open source)&lt;br /&gt; And the discord: &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you have any questions or ideas i'll be hanging out here for a while!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3h656ewlb7nf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8k3j3/power_up_your_ollama_models_thanks_to_you_guys_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8k3j3/power_up_your_ollama_models_thanks_to_you_guys_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T19:44:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9hv8a</id>
    <title>Why are CPU peaks every 5 min</title>
    <updated>2025-09-05T21:39:17+00:00</updated>
    <author>
      <name>/u/jordi_at</name>
      <uri>https://old.reddit.com/user/jordi_at</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I have a virtual machine running Debian, Ollama and OpenUi on Proxmox and I noticed that every five minutes there is a CPU peak of almost 20% of CPU usage. This is the only VM with this behaviour, that's why I'm asking this sub as I guess that is Ollama related. Any idea of the possible reason? Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jordi_at"&gt; /u/jordi_at &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9hv8a/why_are_cpu_peaks_every_5_min/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9hv8a/why_are_cpu_peaks_every_5_min/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9hv8a/why_are_cpu_peaks_every_5_min/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T21:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9siw6</id>
    <title>Ollama lagging too much..</title>
    <updated>2025-09-06T06:33:50+00:00</updated>
    <author>
      <name>/u/Current-Passion-9783</name>
      <uri>https://old.reddit.com/user/Current-Passion-9783</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys, I have downloaded ollama and in cmd I downloaded ollama mistrial, but it's lagging my laptop too much, I am making a ai assistant but it feels like it's freezing, can someone tell what can I do so it doesn't lag?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Current-Passion-9783"&gt; /u/Current-Passion-9783 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9siw6/ollama_lagging_too_much/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9siw6/ollama_lagging_too_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9siw6/ollama_lagging_too_much/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T06:33:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n95hr0</id>
    <title>Unsloth just released their GGUF of Kimi-K2-Instruct-0905!</title>
    <updated>2025-09-05T13:36:53+00:00</updated>
    <author>
      <name>/u/TheAndyGeorge</name>
      <uri>https://old.reddit.com/user/TheAndyGeorge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n95hr0/unsloth_just_released_their_gguf_of/"&gt; &lt;img alt="Unsloth just released their GGUF of Kimi-K2-Instruct-0905!" src="https://external-preview.redd.it/u42y4pGiiWpLArGTxtLnpU7XIOrkkmzZ5xAid1ozch8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7d4e7a1b9c3b96563747fc8517c620156b1d622" title="Unsloth just released their GGUF of Kimi-K2-Instruct-0905!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheAndyGeorge"&gt; /u/TheAndyGeorge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Kimi-K2-Instruct-0905-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n95hr0/unsloth_just_released_their_gguf_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n95hr0/unsloth_just_released_their_gguf_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T13:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9b4ak</id>
    <title>Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler</title>
    <updated>2025-09-05T17:14:37+00:00</updated>
    <author>
      <name>/u/Fluid-Engineering769</name>
      <uri>https://old.reddit.com/user/Fluid-Engineering769</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n9b4ak/websitecrawler_extract_data_from_websites_in_llm/"&gt; &lt;img alt="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" src="https://external-preview.redd.it/CWPKsqhHvC8Ru3Bore9-VfKiik9UmiU4LtZd8GtJiug.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d523c17e313fe34a441cd56f43ec9625de5e76a" title="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluid-Engineering769"&gt; /u/Fluid-Engineering769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pc8544/Website-Crawler"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9b4ak/websitecrawler_extract_data_from_websites_in_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9b4ak/websitecrawler_extract_data_from_websites_in_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T17:14:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9o314</id>
    <title>smaller model ever (quantized or not) that supports input images</title>
    <updated>2025-09-06T02:27:23+00:00</updated>
    <author>
      <name>/u/gabrielevinci</name>
      <uri>https://old.reddit.com/user/gabrielevinci</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I have installed Ollama recently because I need it for my project. &lt;/p&gt; &lt;p&gt;My aim is to &amp;quot;describe images&amp;quot; as quickly as possible. &lt;/p&gt; &lt;p&gt;I have a 3060OC with 12 vram, so little for this world (I bought it when the AI ​​was not known and it never served me to play)&lt;/p&gt; &lt;p&gt;What is the best model for my purpose?&lt;/p&gt; &lt;p&gt;I tried Gemma3: 4b and it seems to work very well, but I want to use the most efficient model for my purpose. &lt;/p&gt; &lt;p&gt;Do you know someone?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gabrielevinci"&gt; /u/gabrielevinci &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9o314/smaller_model_ever_quantized_or_not_that_supports/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9o314/smaller_model_ever_quantized_or_not_that_supports/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9o314/smaller_model_ever_quantized_or_not_that_supports/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T02:27:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9vr24</id>
    <title>gpt-oss:20b and structured outputs and ollama</title>
    <updated>2025-09-06T10:01:14+00:00</updated>
    <author>
      <name>/u/DifficultTomatillo29</name>
      <uri>https://old.reddit.com/user/DifficultTomatillo29</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just doesn't work for me - am I doing anything wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DifficultTomatillo29"&gt; /u/DifficultTomatillo29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9vr24/gptoss20b_and_structured_outputs_and_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9vr24/gptoss20b_and_structured_outputs_and_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9vr24/gptoss20b_and_structured_outputs_and_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T10:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9i1f1</id>
    <title>First time user confused by models.</title>
    <updated>2025-09-05T21:46:32+00:00</updated>
    <author>
      <name>/u/Xanaxaria</name>
      <uri>https://old.reddit.com/user/Xanaxaria</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm completely new to this. I'm still figuring out how to use everything but I was wondering what models were best for NSFW language editing. I'm a porn translator and have been using AI to help edit but all ais (chatgpt, deepseek, etc) generally block the stuff I translate. So I'm looking for the best language handling model that can process NSFW language. I don't need it to translate so in find with it only being good at English but I need it to grammatically edit larger amounts of text.&lt;/p&gt; &lt;p&gt;Is there a guide or a website that has a master list of models for what the model is good at? Or would anyone know a good model that fits this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xanaxaria"&gt; /u/Xanaxaria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9i1f1/first_time_user_confused_by_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9i1f1/first_time_user_confused_by_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9i1f1/first_time_user_confused_by_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T21:46:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9vruw</id>
    <title>ollama and audio</title>
    <updated>2025-09-06T10:02:29+00:00</updated>
    <author>
      <name>/u/DifficultTomatillo29</name>
      <uri>https://old.reddit.com/user/DifficultTomatillo29</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;are there any models under ollama that support receiving audio, and is there any way to send it? note - I DO NOT want a transcription - I want to ask things about the audio - .... age, gender, quality - that sort of thing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DifficultTomatillo29"&gt; /u/DifficultTomatillo29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9vruw/ollama_and_audio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9vruw/ollama_and_audio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9vruw/ollama_and_audio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T10:02:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9tbry</id>
    <title>Any small good gui for ollama?</title>
    <updated>2025-09-06T07:23:15+00:00</updated>
    <author>
      <name>/u/MountainGolf2679</name>
      <uri>https://old.reddit.com/user/MountainGolf2679</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not looking for a huge gui, something small and safe to use that support&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;sending images.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;editing messages both of user and model.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MountainGolf2679"&gt; /u/MountainGolf2679 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9tbry/any_small_good_gui_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9tbry/any_small_good_gui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9tbry/any_small_good_gui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T07:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9zd9l</id>
    <title>ollama pipelines keep failing in repeatable ways. global fix map just shipped, dedicated ollama page plus dr. wfgy</title>
    <updated>2025-09-06T13:19:29+00:00</updated>
    <author>
      <name>/u/onestardao</name>
      <uri>https://old.reddit.com/user/onestardao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n9zd9l/ollama_pipelines_keep_failing_in_repeatable_ways/"&gt; &lt;img alt="ollama pipelines keep failing in repeatable ways. global fix map just shipped, dedicated ollama page plus dr. wfgy" src="https://preview.redd.it/pts15p50pjnf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31dfff2be1d431de4cea404db4af1fd2f7c09d79" title="ollama pipelines keep failing in repeatable ways. global fix map just shipped, dedicated ollama page plus dr. wfgy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;last week i posted the 16 problem map. today is the upgrade. we now have the global fix map with a dedicated ollama page, and a live dr. wfgy （ChatGPT shared page with pre-trained data, just paste your bug screenshot to it you will get the answer) &lt;/p&gt; &lt;p&gt;on the map home who triages bugs in plain chat. we keep the same idea, a semantic firewall before generation. you drop it in front of output, it checks ΔS and λ, loops or resets if unstable, then lets the model speak only when the state is clean. no infra change, no sdk.&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;why this matters for ollama&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;your embeddings look fine, answers drift. the firewall treats semantic ≠ embedding as No 5 and clamps it.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;doc exists, retrieval never lands on it. traceability is No 8, we add ids and contracts so citations stop lying.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;first call after a model switch crashes or returns garbage. pre deploy collapse is No 16, the page shows the warmup and version pins that avoid it.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;background jobs run before the store is ready. bootstrap ordering is No 14, you get a minimal start order and swap recipe.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;long context entropy and routing noise. No 2 and No 9 have quick checks you can run in text.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;before vs after (what changed from last post)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;before: patch after the fact, add rerankers and regex, fight the same bug next week.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;after: accept only stable semantic states before output. measure ΔS ≤ 0.45, coverage ≥ 0.70, λ convergent. once the path holds, that class stays fixed.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;new for this release: a one page ollama guide with store agnostic knobs, and a chat based dr. wfgy who maps your symptom to the right No and gives a minimal prescription.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;how to self test in one minute&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;open a fresh chat with your model.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;paste TXT OS or WFGY core (plain text files).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;ask: “use wfgy to analyze my ollama pipeline and show which No i’m hitting.” the file is written for models to read. no plugins, no tool setup.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;one link only, bookmark this&lt;/p&gt; &lt;p&gt;Ollama Global Fix Map page&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/onestardao/WFGY/blob/main/ProblemMap/GlobalFixMap/LocalDeploy_Inference/ollama.md"&gt;https://github.com/onestardao/WFGY/blob/main/ProblemMap/GlobalFixMap/LocalDeploy_Inference/ollama.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;last note for ollama folks: &lt;/p&gt; &lt;p&gt;the global fix map is already 300+ pages. coverage buckets include &lt;/p&gt; &lt;p&gt;LocalDeploy_Inference, Vector DBs and Stores, RAG plus VectorDB, Retrieval, Embeddings, Chunking, Language and Locale, DocumentAI_OCR, Agents and Orchestration, Safety PromptIntegrity, PromptAssembly, OpsDeploy, Automation, Eval and Observability, Governance, Memory Long Context, Multimodal Long Context, DevTools CodeAI. the high-impact ones for ollama are LocalDeploy_Inference, Vector DBs and Stores, RAG plus VectorDB, Retrieval, Embeddings, OpsDeploy, and Safety PromptIntegrity. &lt;/p&gt; &lt;p&gt;every page gives a symptom checklist, acceptance targets, and a minimal repair plan you can run in text.&lt;/p&gt; &lt;p&gt;Thank for reading my work , if anything ollama community want to add , please let me know. &lt;/p&gt; &lt;p&gt;&lt;sup&gt;__________^&lt;/sup&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onestardao"&gt; /u/onestardao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pts15p50pjnf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9zd9l/ollama_pipelines_keep_failing_in_repeatable_ways/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9zd9l/ollama_pipelines_keep_failing_in_repeatable_ways/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T13:19:29+00:00</published>
  </entry>
</feed>
