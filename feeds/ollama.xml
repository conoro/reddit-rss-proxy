<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-20T17:37:28+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1m2tiy1</id>
    <title>Gaming Desktop is Overkill?</title>
    <updated>2025-07-18T05:01:59+00:00</updated>
    <author>
      <name>/u/MrJaver</name>
      <uri>https://old.reddit.com/user/MrJaver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanna have an AI for coding (java backend, react frontend) inside Jetbrains IDE. I pay for a license but the cloud AI quota is very small but don't feel like paying as AI doesn't do all that much, just convenience for debugging, plus it's kinda slow going to/from the network. Jetbrains recently added local ollama support, so I wanna give it a try but I don't know what I'm doing. I got:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2019 16&amp;quot; macbook pro 2.4 GHz 8-Core Intel Core i9/AMD Radeon Pro 5500M 4 GB/32 GB 2667 MHz DDR4&lt;/li&gt; &lt;li&gt;A gaming desktop with 32gb ram ddr4, i7 12 gen, RTX 3060ti, about 100gb m.2 pcie3 and 600gb HDD&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I tried running deepseek-r1:8b on my MacBook and it was unacceptably slow, printing &amp;quot;thinking&amp;quot; steps and then replying. Guess I don't care that it's thinking out loud but it took like a whole minute to reply to &amp;quot;hello&amp;quot;. I didn't see much GPU processing usage, just GPU memory, maybe I need to configure something?&lt;/p&gt; &lt;p&gt;I could try to use some lightweight model but then I don't want the model to give me wrong answers, does that matter at all for coding? I read there are models curated for coding, I'll try some...&lt;/p&gt; &lt;p&gt;Another idea is that I have this gaming desktop standing around, I could start it up and run a model on there, is that overkill for what I need? Also, not much high-speed storage there, although I can buy another ssd if it's worth the trouble. Not sure how I can connect my MacBook to PC, they are both connected to wifi, I can also try ethernet/usb cord - does that matter?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrJaver"&gt; /u/MrJaver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2tiy1/gaming_desktop_is_overkill/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2tiy1/gaming_desktop_is_overkill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m2tiy1/gaming_desktop_is_overkill/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-18T05:01:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2uj1f</id>
    <title>Spy Search CLI supports Ollama</title>
    <updated>2025-07-18T06:00:59+00:00</updated>
    <author>
      <name>/u/jasonhon2013</name>
      <uri>https://old.reddit.com/user/jasonhon2013</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really want to say thank you to the Ollama community! I just released my second open-source project, which is native (and originally designed for Ollama). The idea is to replace the Gemini CLI with lightning speed. Similar to the previous spy search, this open-source project will be really quick if you are using Mistral models! I hope you enjoy it. Once again, thank you so much for your support. I just can't reach this level without Ollama's support! (Yeah, give me an upvote or stars if you love this idea!)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/JasonHonKL/spy-search-cli"&gt;https://github.com/JasonHonKL/spy-search-cli&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonhon2013"&gt; /u/jasonhon2013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2uj1f/spy_search_cli_supports_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2uj1f/spy_search_cli_supports_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m2uj1f/spy_search_cli_supports_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-18T06:00:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2qnfk</id>
    <title>RouteGPT - the chrome extension for chatgpt that means no more pedaling to the model selector (powered by Ollama and Arch-Router LLM)</title>
    <updated>2025-07-18T02:31:30+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m2qnfk/routegpt_the_chrome_extension_for_chatgpt_that/"&gt; &lt;img alt="RouteGPT - the chrome extension for chatgpt that means no more pedaling to the model selector (powered by Ollama and Arch-Router LLM)" src="https://external-preview.redd.it/ZmNqcDI1MjN4aWRmMXNi_tiJEhKI4dMkHPB85mAr78mWhl9BNIxK-HEB1dv0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2a13dec5570f74069b49dd90d50a8dc0e6318b7" title="RouteGPT - the chrome extension for chatgpt that means no more pedaling to the model selector (powered by Ollama and Arch-Router LLM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;f you are a ChatGPT pro user like me, you are probably frustrated and tired of pedaling to the model selector drop down to pick a model, prompt that model and then repeat that cycle all over again. Well that pedaling goes away with RouteGPT.&lt;/p&gt; &lt;p&gt;RouteGPT is a Chrome extension for &lt;a href="http://chatgpt.com"&gt;chatgpt.com&lt;/a&gt; that automatically selects the right OpenAI model for your prompt based on preferences you define. For example: ‚Äúcreative novel writing, story ideas, imaginative prose‚Äù ‚Üí GPT-4o, or ‚Äúcritical analysis, deep insights, and market research ‚Äù ‚Üí o3 &lt;/p&gt; &lt;p&gt;Instead of switching models manually, RouteGPT handles it for you ‚Äî like automatic transmission for your ChatGPT experience.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Extension link&lt;/strong&gt; : &lt;a href="https://chromewebstore.google.com/search/RouteGPT"&gt;https://chromewebstore.google.com/search/RouteGPT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S:&lt;/strong&gt; The extension is an experiment - I &lt;em&gt;vibe coded&lt;/em&gt; it in 7 days - and a means to demonstrate some of our technology. My hope is to be helpful to those who might benefit from this, and drive a discussion about the science and infrastructure work underneath that could enable the most ambitious teams to move faster in building great agents&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;: &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2506.16655"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h8o8m223xidf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2qnfk/routegpt_the_chrome_extension_for_chatgpt_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m2qnfk/routegpt_the_chrome_extension_for_chatgpt_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-18T02:31:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3htty</id>
    <title>Nvidia GTX-1080Ti 11GB Vram</title>
    <updated>2025-07-18T23:52:35+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran into problems when I replace the &lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840"&gt;GTX-1070&lt;/a&gt; with&lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877"&gt; GTX 1080Ti&lt;/a&gt;. NVTOP would show about 7GB of VRAM usage. So I had to adjust the num_gpu value to 63. Nice improvement.&lt;/p&gt; &lt;p&gt;These my steps:&lt;/p&gt; &lt;p&gt;&lt;code&gt;time ollama run --verbose gemma3:12b-it-qat&lt;/code&gt;&lt;br /&gt; &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;/set parameter num_gpu 63&lt;/code&gt;&lt;br /&gt; &lt;code&gt;Set parameter 'num_gpu' to '63'&lt;/code&gt;&lt;br /&gt; &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;/save mygemma3&lt;/code&gt;&lt;br /&gt; Created new model 'mygemma3'&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;NAME&lt;/th&gt; &lt;th align="left"&gt;eval rate&lt;/th&gt; &lt;th align="left"&gt;prompt eval rate&lt;/th&gt; &lt;th align="left"&gt;total duration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:12b-it-qat&lt;/td&gt; &lt;td align="left"&gt;6.69&lt;/td&gt; &lt;td align="left"&gt;118.6&lt;/td&gt; &lt;td align="left"&gt;3m2.831s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mygemma3:latest&lt;/td&gt; &lt;td align="left"&gt;24.74&lt;/td&gt; &lt;td align="left"&gt;349.2&lt;/td&gt; &lt;td align="left"&gt;0m38.677s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Here are a few other models:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;NAME&lt;/th&gt; &lt;th align="left"&gt;eval rate&lt;/th&gt; &lt;th align="left"&gt;prompt eval rate&lt;/th&gt; &lt;th align="left"&gt;total duration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek-r1:14b&lt;/td&gt; &lt;td align="left"&gt;22.72&lt;/td&gt; &lt;td align="left"&gt;51.83&lt;/td&gt; &lt;td align="left"&gt;34.07208103&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mygemma3:latest&lt;/td&gt; &lt;td align="left"&gt;23.97&lt;/td&gt; &lt;td align="left"&gt;321.68&lt;/td&gt; &lt;td align="left"&gt;47.22412009&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:12b&lt;/td&gt; &lt;td align="left"&gt;16.84&lt;/td&gt; &lt;td align="left"&gt;96.54&lt;/td&gt; &lt;td align="left"&gt;1m20.845913225&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:12b-it-qat&lt;/td&gt; &lt;td align="left"&gt;13.33&lt;/td&gt; &lt;td align="left"&gt;159.54&lt;/td&gt; &lt;td align="left"&gt;1m36.518625216&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:27b&lt;/td&gt; &lt;td align="left"&gt;3.65&lt;/td&gt; &lt;td align="left"&gt;9.49&lt;/td&gt; &lt;td align="left"&gt;7m30.344502487&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3n:e2b-it-q8_0&lt;/td&gt; &lt;td align="left"&gt;45.95&lt;/td&gt; &lt;td align="left"&gt;183.27&lt;/td&gt; &lt;td align="left"&gt;30.09576316&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite3.1-moe:3b-instruct-q8_0&lt;/td&gt; &lt;td align="left"&gt;88.46&lt;/td&gt; &lt;td align="left"&gt;546.45&lt;/td&gt; &lt;td align="left"&gt;8.24215104&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama3.1:8b&lt;/td&gt; &lt;td align="left"&gt;38.29&lt;/td&gt; &lt;td align="left"&gt;174.13&lt;/td&gt; &lt;td align="left"&gt;16.73243012&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minicpm-v:8b&lt;/td&gt; &lt;td align="left"&gt;37.67&lt;/td&gt; &lt;td align="left"&gt;188.41&lt;/td&gt; &lt;td align="left"&gt;4.663153513&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mistral:7b-instruct-v0.2-q5_K_M&lt;/td&gt; &lt;td align="left"&gt;40.33&lt;/td&gt; &lt;td align="left"&gt;176.14&lt;/td&gt; &lt;td align="left"&gt;5.90872581&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmo2:13b&lt;/td&gt; &lt;td align="left"&gt;12.18&lt;/td&gt; &lt;td align="left"&gt;107.56&lt;/td&gt; &lt;td align="left"&gt;26.67653928&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;phi4:14b&lt;/td&gt; &lt;td align="left"&gt;23.56&lt;/td&gt; &lt;td align="left"&gt;116.84&lt;/td&gt; &lt;td align="left"&gt;16.40753603&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3:14b&lt;/td&gt; &lt;td align="left"&gt;22.66&lt;/td&gt; &lt;td align="left"&gt;156.32&lt;/td&gt; &lt;td align="left"&gt;36.78135622&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I had each model create a CSV format from the ollama --verbose output and the following models failed.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;FAILED:&lt;/p&gt; &lt;p&gt;minicpm-v:8b &lt;/p&gt; &lt;p&gt;olmo2:13b &lt;/p&gt; &lt;p&gt;granite3.1-moe:3b-instruct-q8_0 &lt;/p&gt; &lt;p&gt;mistral:7b-instruct-v0.2-q5_K_M &lt;/p&gt; &lt;p&gt;gemma3n:e2b-it-q8_0&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I cut GPU total power from 250 to 188 using: &lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo nvidia-smi -i 0 -pl 188&lt;/code&gt; &lt;/p&gt; &lt;p&gt;Resulted in 'eval rate' &lt;/p&gt; &lt;p&gt;250 watts=24.7 &lt;/p&gt; &lt;p&gt;188 watts=23.6&lt;/p&gt; &lt;p&gt;Not much of a hit to drop 25% power usage. I also tested the bare minimum of 125 watts but that resulted in a 25% reduction in eval rate. Still that makes running several cards viable. &lt;/p&gt; &lt;p&gt;I have a more in depth review on my &lt;a href="https://tabletuser.blogspot.com/2025/07/gtx-1080ti-optimized-for-ollama.html"&gt;blog&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3htty/nvidia_gtx1080ti_11gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3htty/nvidia_gtx1080ti_11gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3htty/nvidia_gtx1080ti_11gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-18T23:52:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1m346zm</id>
    <title>Built Ollamaton - Universal MCP Client for Ollama (CLI/API/GUI)</title>
    <updated>2025-07-18T14:43:28+00:00</updated>
    <author>
      <name>/u/inventorado</name>
      <uri>https://old.reddit.com/user/inventorado</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m346zm/built_ollamaton_universal_mcp_client_for_ollama/"&gt; &lt;img alt="Built Ollamaton - Universal MCP Client for Ollama (CLI/API/GUI)" src="https://external-preview.redd.it/SuDxJUkGbblEMhWNwAdUMq9vE644u0P-tro1ImMeakI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c757484d6433d24eb9d362ceb1f0d3237f882db4" title="Built Ollamaton - Universal MCP Client for Ollama (CLI/API/GUI)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inventorado"&gt; /u/inventorado &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/mcp/comments/1m332dj/built_ollamaton_universal_mcp_client_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m346zm/built_ollamaton_universal_mcp_client_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m346zm/built_ollamaton_universal_mcp_client_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-18T14:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3pz4z</id>
    <title>LANGCHAIN + DEEPSEEK OLLAMA = LONG WAIT AND RANDOM BLOB</title>
    <updated>2025-07-19T07:12:38+00:00</updated>
    <author>
      <name>/u/ComedianObjective572</name>
      <uri>https://old.reddit.com/user/ComedianObjective572</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m3pz4z/langchain_deepseek_ollama_long_wait_and_random/"&gt; &lt;img alt="LANGCHAIN + DEEPSEEK OLLAMA = LONG WAIT AND RANDOM BLOB" src="https://preview.redd.it/xcvoam7w6sdf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dda6a9c735044f73516c2ec142e84a3bae64fd7" title="LANGCHAIN + DEEPSEEK OLLAMA = LONG WAIT AND RANDOM BLOB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there! I currently built an AI Agent for Business needs. However, I tried DeepSeek for LLM and it was a long wait and a random Blob. Is it just me or does this happen to you?&lt;/p&gt; &lt;p&gt;P.S. Prefered Model is Qwen3 and Code Qwen 2.5. I just want to explore if there are better models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComedianObjective572"&gt; /u/ComedianObjective572 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xcvoam7w6sdf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3pz4z/langchain_deepseek_ollama_long_wait_and_random/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3pz4z/langchain_deepseek_ollama_long_wait_and_random/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T07:12:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3qa4o</id>
    <title>Simple way to run ollama on an air gapped Server?</title>
    <updated>2025-07-19T07:32:32+00:00</updated>
    <author>
      <name>/u/Toeeni</name>
      <uri>https://old.reddit.com/user/Toeeni</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Guys,&lt;/p&gt; &lt;p&gt;what is the simplest way to run ollama on an air gapped Server? I don't find any solutions yet to just download ollama and a llm and transfer it to the server to run it there.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Toeeni"&gt; /u/Toeeni &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3qa4o/simple_way_to_run_ollama_on_an_air_gapped_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3qa4o/simple_way_to_run_ollama_on_an_air_gapped_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3qa4o/simple_way_to_run_ollama_on_an_air_gapped_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T07:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3um14</id>
    <title>Getting ollama to work with a GTX 1660 on nixos</title>
    <updated>2025-07-19T12:06:09+00:00</updated>
    <author>
      <name>/u/H-L_echelle</name>
      <uri>https://old.reddit.com/user/H-L_echelle</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/H-L_echelle"&gt; /u/H-L_echelle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/NixOS/comments/1m3rba1/getting_ollama_to_work_with_a_gtx_1660/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3um14/getting_ollama_to_work_with_a_gtx_1660_on_nixos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3um14/getting_ollama_to_work_with_a_gtx_1660_on_nixos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T12:06:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3jon2</id>
    <title>Meet "Z840 Pascal" | My ugly old z840 stuffed with cheap Pascal cards from Ebay, running llama4:scout @ 5 tokens/second</title>
    <updated>2025-07-19T01:22:39+00:00</updated>
    <author>
      <name>/u/Wooden_Push_4137</name>
      <uri>https://old.reddit.com/user/Wooden_Push_4137</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m3jon2/meet_z840_pascal_my_ugly_old_z840_stuffed_with/"&gt; &lt;img alt="Meet &amp;quot;Z840 Pascal&amp;quot; | My ugly old z840 stuffed with cheap Pascal cards from Ebay, running llama4:scout @ 5 tokens/second" src="https://b.thumbs.redditmedia.com/1ygDUJCHEercPbRDEvI4Bko-akX2AP05oi_4mTcTMPg.jpg" title="Meet &amp;quot;Z840 Pascal&amp;quot; | My ugly old z840 stuffed with cheap Pascal cards from Ebay, running llama4:scout @ 5 tokens/second" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do I know how to have a Friday night, or what?!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fadmmyilgqdf1.png?width=1244&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23a6fb59a1ea448ad2e44beb0def5c7127921b28"&gt;https://preview.redd.it/fadmmyilgqdf1.png?width=1244&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23a6fb59a1ea448ad2e44beb0def5c7127921b28&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8hedwqaveqdf1.png?width=1320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa3ef4d2d7467bcdee27ce3e6ea6da803b00f723"&gt;https://preview.redd.it/8hedwqaveqdf1.png?width=1320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa3ef4d2d7467bcdee27ce3e6ea6da803b00f723&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ovvb5tytfqdf1.png?width=1660&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3198b36dc403a9f3eec6af5682ba854d5e056e29"&gt;https://preview.redd.it/ovvb5tytfqdf1.png?width=1660&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3198b36dc403a9f3eec6af5682ba854d5e056e29&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d9hzgu7yfqdf1.png?width=1662&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=195b3b0613bfb987aaaabe7e81c2cf3995091715"&gt;https://preview.redd.it/d9hzgu7yfqdf1.png?width=1662&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=195b3b0613bfb987aaaabe7e81c2cf3995091715&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden_Push_4137"&gt; /u/Wooden_Push_4137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3jon2/meet_z840_pascal_my_ugly_old_z840_stuffed_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3jon2/meet_z840_pascal_my_ugly_old_z840_stuffed_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3jon2/meet_z840_pascal_my_ugly_old_z840_stuffed_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T01:22:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3g64z</id>
    <title>Hate my PM Job so I Tried to Automate it with a Custom CUA Agent</title>
    <updated>2025-07-18T22:38:12+00:00</updated>
    <author>
      <name>/u/Defiant-Plan-1393</name>
      <uri>https://old.reddit.com/user/Defiant-Plan-1393</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rather than using one of the traceable, available tools, I decided to make my own computer use and MCP agent, SOFIA (Sort of Functional Interactive Agent), for ollama and openai to try and automate my job by hosting it on my VPN. The tech probably just isn't there yet, but I came up with an agent that can successfully navigate apps on my desktop.&lt;/p&gt; &lt;p&gt;You can see the github: &lt;a href="https://github.com/akim42003/SOFIA"&gt;https://github.com/akim42003/SOFIA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The CUA architecture uses a custom omniparser layer and filter to get positional information about the desktop, which ensures almost perfect accuracy for mouse manipulation without damaging the context. It is reasonable effective using mistral-small3.1:24b, but is obviously much slower and less accurate than using GPT. I did notice that embedding the thought process into the modelfile made a big difference in the agents ability to breakdown tasks and execute tools sequentially.&lt;/p&gt; &lt;p&gt;I do genuinely use this tool as an email and calendar assistant.&lt;/p&gt; &lt;p&gt;It also contains a desktop, hastily put together version of cluely I made for fun. I would love to discuss this project and any similar experiences other people have had.&lt;/p&gt; &lt;p&gt;As a side note if anyone wants to get me out of PM hell by hiring me as a SWE that would be great!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Defiant-Plan-1393"&gt; /u/Defiant-Plan-1393 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3g64z/hate_my_pm_job_so_i_tried_to_automate_it_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3g64z/hate_my_pm_job_so_i_tried_to_automate_it_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3g64z/hate_my_pm_job_so_i_tried_to_automate_it_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-18T22:38:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3xvg8</id>
    <title>Finetuning a model</title>
    <updated>2025-07-19T14:41:07+00:00</updated>
    <author>
      <name>/u/Private_Tank</name>
      <uri>https://old.reddit.com/user/Private_Tank</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;br /&gt; im kinda new to ollama and have a big project. I have a private cookbook which I populated with a lot of recipies. I mean there are over 1000 recipes in it, including personal ratings. Now I want to finetune the ai so I can talk to my cookbook if that makes sense.&lt;/p&gt; &lt;p&gt;&amp;quot;What is the best soup&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;I have ingedients x,y,z what can you recommend&amp;quot;&lt;/p&gt; &lt;p&gt;How would you tackle this task?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private_Tank"&gt; /u/Private_Tank &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3xvg8/finetuning_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3xvg8/finetuning_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3xvg8/finetuning_a_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T14:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4d1a1</id>
    <title>Ollama + ollama-mcp-bridge problem by Open Web UI</title>
    <updated>2025-07-20T01:49:38+00:00</updated>
    <author>
      <name>/u/carlosetabosa</name>
      <uri>https://old.reddit.com/user/carlosetabosa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m4d1a1/ollama_ollamamcpbridge_problem_by_open_web_ui/"&gt; &lt;img alt="Ollama + ollama-mcp-bridge problem by Open Web UI" src="https://a.thumbs.redditmedia.com/TCakJ4ogaz5-kdouKm5nMHoc_aMmjax2TaZEZ6AWhG8.jpg" title="Ollama + ollama-mcp-bridge problem by Open Web UI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ERROR | ollama_mcp_bridge.proxy_service:proxy_chat_with_tools:52 - Chat proxy failed: {&amp;quot;error&amp;quot;:&amp;quot;model is required&amp;quot;}&lt;br /&gt; ERROR | ollama_mcp_bridge.api:chat:49 - /api/chat failed: {&amp;quot;error&amp;quot;:&amp;quot;model is required&amp;quot;}&amp;quot;POST /api/chat HTTP/1.1&amp;quot; 400 Bad Request&lt;/p&gt; &lt;p&gt;I'm trying llama3.2 by Ollama with my Open WebUI.&lt;br /&gt; I have configured the tool in Manage Tool Servers:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a4w9jo2npxdf1.png?width=696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2ef1a61bc52382235f946819f1d1b025b870e67"&gt;https://preview.redd.it/a4w9jo2npxdf1.png?width=696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2ef1a61bc52382235f946819f1d1b025b870e67&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This phase is OK, because I can see my MCP in the chat screen, just like that:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cxshijivpxdf1.png?width=683&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9892901bb03f6e07aa880a092216ef358d1a3f4d"&gt;https://preview.redd.it/cxshijivpxdf1.png?width=683&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9892901bb03f6e07aa880a092216ef358d1a3f4d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However I'm asking somenthing that calls a MCP and the LLM calls the correct MCP but it does not put the model argument:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zuz24m05qxdf1.png?width=2056&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3afafca61e4301c0c86ac84a770ea6d5573449c7"&gt;https://preview.redd.it/zuz24m05qxdf1.png?width=2056&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3afafca61e4301c0c86ac84a770ea6d5573449c7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Someone?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carlosetabosa"&gt; /u/carlosetabosa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4d1a1/ollama_ollamamcpbridge_problem_by_open_web_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4d1a1/ollama_ollamamcpbridge_problem_by_open_web_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4d1a1/ollama_ollamamcpbridge_problem_by_open_web_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T01:49:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3zbas</id>
    <title>introducing computron_9000</title>
    <updated>2025-07-19T15:41:59+00:00</updated>
    <author>
      <name>/u/larz01larz</name>
      <uri>https://old.reddit.com/user/larz01larz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on an AI personal assistant that runs on local hardware and currently uses Ollama as its inference backend. I've got plans to add a lot more capabilities beyond what it can do right now which is; search the web, search reddit, work on the filesystem, write and execute code (in containers), and do deep research on a topic. &lt;/p&gt; &lt;p&gt;It's still a WIP and the setup instructions aren't great. You'll have the best luck if you are running it on linux, at least for the code execution. Everything else should be OS agnostic.&lt;/p&gt; &lt;p&gt;Give it a try and let me know what features you'd like me to add. If you get stuck, let me know and I'll help you get setup.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lefoulkrod/computron_9000/"&gt;https://github.com/lefoulkrod/computron_9000/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/larz01larz"&gt; /u/larz01larz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3zbas/introducing_computron_9000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3zbas/introducing_computron_9000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3zbas/introducing_computron_9000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T15:41:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3zfae</id>
    <title>RTX (RTX 3090/4090/5090) GPU vs Apple M4 Max/M3 Ultra. Is RTX worth it over when over MSRP?</title>
    <updated>2025-07-19T15:46:40+00:00</updated>
    <author>
      <name>/u/bikers301</name>
      <uri>https://old.reddit.com/user/bikers301</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I need a computer to run LLM jobs (likely qwen 2.5 32B Q4)&lt;/p&gt; &lt;p&gt;What I'm Doing:&lt;/p&gt; &lt;p&gt;I'm using a LLM hosted on a computer to run Celery Redis jobs. It pulls one report of ~20,000 characters to answer about 15 qualitative questions per job. I'd like to run minimum 6 of these jobs per hour. Preferably more. Plan is to run this 24/7 for months on end.&lt;/p&gt; &lt;p&gt;Question: Hardware - RTX 3090 vs 4090 vs 5090 vs M4 Max vs M3 Ultra&lt;/p&gt; &lt;p&gt;I know the GPUS will heavily out perform the M4 Max and M3 Ultra, but what makes more sense from a bang for your buck performance? I'm looking at grabbing a Mac Studio (M4 Max) with 48GB memory for ~$2,500. But would the performance be that terrible compared to a RTX 5090?&lt;/p&gt; &lt;p&gt;If I could find a RTX 5090 at MSRP that would be a different story, but I haven't see any drops since May for a FE.&lt;/p&gt; &lt;p&gt;Open to thoughts or suggestions? I'd like to make a system for sub $3k preferably.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bikers301"&gt; /u/bikers301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3zfae/rtx_rtx_309040905090_gpu_vs_apple_m4_maxm3_ultra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3zfae/rtx_rtx_309040905090_gpu_vs_apple_m4_maxm3_ultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3zfae/rtx_rtx_309040905090_gpu_vs_apple_m4_maxm3_ultra/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T15:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4izdn</id>
    <title>My Fine-Tuned Model Keeps Echoing Prompts or Giving Blank/Generic Responses</title>
    <updated>2025-07-20T07:32:43+00:00</updated>
    <author>
      <name>/u/Srmxz</name>
      <uri>https://old.reddit.com/user/Srmxz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I‚Äôve been working on fine-tuning open-source LLMs like Phi-3 and LLaMA 3 using Unsloth in Google Colab, targeting a chatbot for customer support (around 500 prompt-response examples).&lt;/p&gt; &lt;p&gt;I‚Äôm facing the same recurring issues no matter what I do:&lt;/p&gt; &lt;p&gt;‚∏ª&lt;/p&gt; &lt;p&gt;‚ùó The problems: 1. The model often responds with the exact same prompt I gave it, instead of the intended response. 2. Sometimes it returns blank output. 3. When it does respond, it gives very generic or off-topic answers, not the specific ones from my training data.&lt;/p&gt; &lt;p&gt;‚∏ª&lt;/p&gt; &lt;p&gt;üõ†Ô∏è My Setup: ‚Ä¢ Using Unsloth + FastLanguageModel ‚Ä¢ Trained on a .json or .jsonl dataset with format:&lt;/p&gt; &lt;p&gt;{ &amp;quot;prompt&amp;quot;: &amp;quot;How long does it take to get a refund?&amp;quot;, &amp;quot;response&amp;quot;: &amp;quot;Refunds typically take 5‚Äì7 business days.&amp;quot; }&lt;/p&gt; &lt;p&gt;Wrapped in training with:&lt;/p&gt; &lt;p&gt;f&amp;quot;### Input: {prompt}\n### Output: {response}&amp;lt;|endoftext|&amp;gt;&amp;quot;&lt;/p&gt; &lt;p&gt;Inference via:&lt;/p&gt; &lt;p&gt;messages = [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;How long does it take to get a refund?&amp;quot;}] tokenizer.apply_chat_template(...)&lt;/p&gt; &lt;p&gt;What I‚Äôve tried: ‚Ä¢ Training with both 3 and 10 epochs ‚Ä¢ Training both Phi-3-mini and LLaMA 3 8B with LoRA (4-bit) ‚Ä¢ Testing with correct Modelfile templates in Ollama like:&lt;/p&gt; &lt;p&gt;TEMPLATE &amp;quot;&amp;quot;&amp;quot;### Input: {{ .Prompt }}\n### Output:&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;Why is the model not learning my input-output structure properly? ‚Ä¢ Is there a better way to format the prompts or structure the dataset? ‚Ä¢ Could the model size (like Phi-3) be a bottleneck? ‚Ä¢ Should I be adding system prompts or few-shot examples at inference?&lt;/p&gt; &lt;p&gt;Any advice, shared experiences, or working examples would help a lot. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Srmxz"&gt; /u/Srmxz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4izdn/my_finetuned_model_keeps_echoing_prompts_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4izdn/my_finetuned_model_keeps_echoing_prompts_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4izdn/my_finetuned_model_keeps_echoing_prompts_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T07:32:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m41wh3</id>
    <title>Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler</title>
    <updated>2025-07-19T17:29:07+00:00</updated>
    <author>
      <name>/u/PsychologicalTap1541</name>
      <uri>https://old.reddit.com/user/PsychologicalTap1541</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m41wh3/websitecrawler_extract_data_from_websites_in_llm/"&gt; &lt;img alt="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" src="https://external-preview.redd.it/nrxKAdkehTr03Y2fP2Hf-vrA-7QLtEcGdfnlKNEGSps.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1aec063dacb6b8a3700910c3ce49f2b57a32057" title="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PsychologicalTap1541"&gt; /u/PsychologicalTap1541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pc8544/Website-Crawler"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m41wh3/websitecrawler_extract_data_from_websites_in_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m41wh3/websitecrawler_extract_data_from_websites_in_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T17:29:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4leia</id>
    <title>Re-ranking support using SQLite RAG with haiku.rag</title>
    <updated>2025-07-20T10:12:02+00:00</updated>
    <author>
      <name>/u/gogozad</name>
      <uri>https://old.reddit.com/user/gogozad</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gogozad"&gt; /u/gogozad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Rag/comments/1m4kfzh/reranking_support_using_sqlite_rag_with_haikurag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4leia/reranking_support_using_sqlite_rag_with_haikurag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4leia/reranking_support_using_sqlite_rag_with_haikurag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T10:12:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4igz7</id>
    <title>Gpu support</title>
    <updated>2025-07-20T07:00:21+00:00</updated>
    <author>
      <name>/u/Ok-Band6009</name>
      <uri>https://old.reddit.com/user/Ok-Band6009</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys how long do you think its gonna take for ollama to add support for the new AMD cards, my 10th gen i5 is kinda struggling, my 9060xt 16gb would perform a lot better&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Band6009"&gt; /u/Ok-Band6009 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4igz7/gpu_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4igz7/gpu_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4igz7/gpu_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T07:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4k2t1</id>
    <title>ollama models and Hugging Face models use case</title>
    <updated>2025-07-20T08:45:14+00:00</updated>
    <author>
      <name>/u/cipherninjabyte</name>
      <uri>https://old.reddit.com/user/cipherninjabyte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just curious what would you use ollama models and hugging face models for ? writing articles locally or fine tuning or what else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cipherninjabyte"&gt; /u/cipherninjabyte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4k2t1/ollama_models_and_hugging_face_models_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4k2t1/ollama_models_and_hugging_face_models_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4k2t1/ollama_models_and_hugging_face_models_use_case/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T08:45:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4tg9s</id>
    <title>Is there a way to use Ollama with vscode copilot in agent mode?</title>
    <updated>2025-07-20T16:38:18+00:00</updated>
    <author>
      <name>/u/richsonreddit</name>
      <uri>https://old.reddit.com/user/richsonreddit</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richsonreddit"&gt; /u/richsonreddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1m4t85h/is_there_a_way_to_use_ollama_with_vscode_copilot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4tg9s/is_there_a_way_to_use_ollama_with_vscode_copilot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4tg9s/is_there_a_way_to_use_ollama_with_vscode_copilot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T16:38:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4j5qp</id>
    <title>When is SmolLM3 coming on Ollama?</title>
    <updated>2025-07-20T07:44:40+00:00</updated>
    <author>
      <name>/u/falconHigh13</name>
      <uri>https://old.reddit.com/user/falconHigh13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have tried the new Huggingface Model on different platforms and even hosting locally but its very slow and take a lot of compute. I even tried huggingface Inference API and its not working. So when is this model coming on Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/falconHigh13"&gt; /u/falconHigh13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4j5qp/when_is_smollm3_coming_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4j5qp/when_is_smollm3_coming_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4j5qp/when_is_smollm3_coming_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T07:44:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4ploe</id>
    <title>mistral-small3.2:latest 15B takes 28GB VRAM?</title>
    <updated>2025-07-20T13:58:54+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;NAME ID SIZE PROCESSOR UNTIL mistral-small3.2:latest 5a408ab55df5 28 GB 38%/62% CPU/GPU 36 minutes from now 7900 XTX 24gb vram ryzen 7900 64GB RAM Question: Mistral size on disk is 15GB. Why it needs 28GB of VRAM and does not fit into 24GB GPU? ollama version is 0.9.6 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4ploe/mistralsmall32latest_15b_takes_28gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4ploe/mistralsmall32latest_15b_takes_28gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4ploe/mistralsmall32latest_15b_takes_28gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T13:58:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4624q</id>
    <title>i just managed to run tinyllama1.1b and n8n in a low-end android phone</title>
    <updated>2025-07-19T20:22:28+00:00</updated>
    <author>
      <name>/u/actuallytech</name>
      <uri>https://old.reddit.com/user/actuallytech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m4624q/i_just_managed_to_run_tinyllama11b_and_n8n_in_a/"&gt; &lt;img alt="i just managed to run tinyllama1.1b and n8n in a low-end android phone" src="https://b.thumbs.redditmedia.com/6dh4kc8nJaE5b1uWFo_bUTN5U0tOMrcyUHvZJ6zjJ-U.jpg" title="i just managed to run tinyllama1.1b and n8n in a low-end android phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the phone i used is an samsung m32 6gb ram with a mediatek G80 &lt;/p&gt; &lt;p&gt;i runned in a Debian via proot-distro in Termux (no root) and i can access both locally, It‚Äôs working better than I expected &lt;/p&gt; &lt;p&gt;i dont know is there any way to use its gpu &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/actuallytech"&gt; /u/actuallytech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m4624q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4624q/i_just_managed_to_run_tinyllama11b_and_n8n_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4624q/i_just_managed_to_run_tinyllama11b_and_n8n_in_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T20:22:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4rl2z</id>
    <title>Anyone else tracking their local LLMs‚Äô performance? I built a tool to make it easier</title>
    <updated>2025-07-20T15:23:18+00:00</updated>
    <author>
      <name>/u/Hades_7658</name>
      <uri>https://old.reddit.com/user/Hades_7658</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I've been running some LLMs locally and was curious how others are keeping tabs on model performance, latency, and token usage. I didn‚Äôt find a lightweight tool that fit my needs, so I started working on one myself.&lt;/p&gt; &lt;p&gt;It‚Äôs a simple dashboard + API setup that helps me monitor and analyze what's going on under the hood mainly for performance tuning and observability. Still early days, but it‚Äôs been surprisingly useful for understanding how my models are behaving over time.&lt;/p&gt; &lt;p&gt;Curious how the rest of you handle observability. Do you use logs, custom scripts, or something else? I‚Äôll drop a link in the comments in case anyone wants to check it out or build on top of it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hades_7658"&gt; /u/Hades_7658 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4rl2z/anyone_else_tracking_their_local_llms_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4rl2z/anyone_else_tracking_their_local_llms_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4rl2z/anyone_else_tracking_their_local_llms_performance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T15:23:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4nclb</id>
    <title>ChatGPT-like Voice LLM</title>
    <updated>2025-07-20T12:08:58+00:00</updated>
    <author>
      <name>/u/embracing_athena</name>
      <uri>https://old.reddit.com/user/embracing_athena</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like the ChaGPT voice mode where I was able to converse with the AI with voice but that is limited to 15 minutes or so daily.&lt;/p&gt; &lt;p&gt;My question is, is there an LLM that I can run with Ollama to achieve the same but with no limits? I feel like any LLM can be used but at the same time seems like I'm feeling I'm missing something. Any extra software must be used along with Ollama for this work?&lt;/p&gt; &lt;p&gt;Please excuse me for my bad English.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/embracing_athena"&gt; /u/embracing_athena &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4nclb/chatgptlike_voice_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4nclb/chatgptlike_voice_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4nclb/chatgptlike_voice_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T12:08:58+00:00</published>
  </entry>
</feed>
