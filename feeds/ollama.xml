<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-22T15:34:37+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1p1cyqc</id>
    <title>How to disable thinking in Qwen3 VL models?</title>
    <updated>2025-11-19T16:53:48+00:00</updated>
    <author>
      <name>/u/eyueldk</name>
      <uri>https://old.reddit.com/user/eyueldk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to run qwen3-vl:2b on ollama and I want to disable thinking. I've tried all the suggested techniques such as: adding /no_think in prompt and system prompt; running /set nothink in interactive mode; and setting --think=false. Yet the Model ALWAYS thinks long and hard even for a simple &amp;quot;hello.&amp;quot;&lt;/p&gt; &lt;p&gt;Has anyone encountered this and found a solution? I'm stuck at the moment. Thanks.&lt;/p&gt; &lt;p&gt;Note: I looked into a previous &lt;a href="https://www.reddit.com/r/ollama/comments/1kcybew/qwen3_disable_thinking_in_ollama/https://www.reddit.com/r/ollama/comments/1kcybew/qwen3_disable_thinking_in_ollama/"&gt;post&lt;/a&gt; but none of the solutions worked.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eyueldk"&gt; /u/eyueldk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1cyqc/how_to_disable_thinking_in_qwen3_vl_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1cyqc/how_to_disable_thinking_in_qwen3_vl_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1cyqc/how_to_disable_thinking_in_qwen3_vl_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T16:53:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1157m</id>
    <title>Modest but reliably accurate LLM</title>
    <updated>2025-11-19T07:26:02+00:00</updated>
    <author>
      <name>/u/SignificanceFlat1460</name>
      <uri>https://old.reddit.com/user/SignificanceFlat1460</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I want to run LLM on my hardware which is a &lt;em&gt;bit&lt;/em&gt; old. My Laptop is a FX505DU&lt;/p&gt; &lt;p&gt;GTX 1660 Ti 6GB Ryzen 7 3750H 16 GB RAM&lt;/p&gt; &lt;p&gt;OK it's a bit more than just a bit old haha. But I wanted to run an LLM that can accurately answer questions related to my CV when applying for jobs. I know some will recommend readily available solutions like gpt-4/5 or Gemini but I want to do this for my own project to see if I can actually do it. Any help would be great. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SignificanceFlat1460"&gt; /u/SignificanceFlat1460 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1157m/modest_but_reliably_accurate_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1157m/modest_but_reliably_accurate_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1157m/modest_but_reliably_accurate_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T07:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1lngs</id>
    <title>Anyone noticed "Premium requests" within their usage tab? What is this for?</title>
    <updated>2025-11-19T22:14:42+00:00</updated>
    <author>
      <name>/u/Active-Shock-7739</name>
      <uri>https://old.reddit.com/user/Active-Shock-7739</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p1lngs/anyone_noticed_premium_requests_within_their/"&gt; &lt;img alt="Anyone noticed &amp;quot;Premium requests&amp;quot; within their usage tab? What is this for?" src="https://b.thumbs.redditmedia.com/aN3C9QQnWaslpPlrBlAPlTC5LHjVGecdhUCQT6ynxLg.jpg" title="Anyone noticed &amp;quot;Premium requests&amp;quot; within their usage tab? What is this for?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am subscribed to the pro plan and used to see just Hourly and Weekly usage, now i see Premium requests as well, but not sure what it is for.&lt;/p&gt; &lt;p&gt;I tried googling and looking up info in their docs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/is6o4ko7ga2g1.png?width=901&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f8bc61f37e553daf7f6f04042bc1a9974b7d427f"&gt;https://preview.redd.it/is6o4ko7ga2g1.png?width=901&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f8bc61f37e553daf7f6f04042bc1a9974b7d427f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Active-Shock-7739"&gt; /u/Active-Shock-7739 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1lngs/anyone_noticed_premium_requests_within_their/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1lngs/anyone_noticed_premium_requests_within_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1lngs/anyone_noticed_premium_requests_within_their/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T22:14:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1vauu</id>
    <title>Mimir - VSCode plugin - Multi-agent parallel studio, code intelligence, vector db search, chat participant - MIT licensed - can use ollama completely</title>
    <updated>2025-11-20T05:44:02+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p1v9fk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1vauu/mimir_vscode_plugin_multiagent_parallel_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1vauu/mimir_vscode_plugin_multiagent_parallel_studio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T05:44:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1izx2</id>
    <title>Ollama Not Using GPU on RTX 5070 Ti (Blackwell)</title>
    <updated>2025-11-19T20:33:42+00:00</updated>
    <author>
      <name>/u/deparko</name>
      <uri>https://old.reddit.com/user/deparko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt; community,&lt;/p&gt; &lt;p&gt;I'm experiencing an issue where Ollama 0.12.11 fails to use the GPU for local models on my RTX 5070 Ti. The GPU is functional and accessible (nvidia-smi works, other services use GPU successfully), but Ollama immediately falls back to CPU-only mode.&lt;/p&gt; &lt;h1&gt;System Details&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: NVIDIA GeForce RTX 5070 Ti (16GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU Compute Capability&lt;/strong&gt;: 12.0 (Blackwell architecture - very new)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU Driver&lt;/strong&gt;: 580.95.05&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CUDA Runtime&lt;/strong&gt;: 12.2.140&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: Ubuntu 25.04 (Linux 6.14.0-35-generic)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Version&lt;/strong&gt;: 0.12.11 (latest, clean install)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Installation&lt;/strong&gt;: Standalone binary via systemd service&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Symptoms&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;All local models show &lt;code&gt;size_vram: 0 MB&lt;/code&gt; in &lt;code&gt;ollama ps&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Logs show: &lt;code&gt;&amp;quot;discovering available GPUs...&amp;quot;&lt;/code&gt; ‚Üí &lt;code&gt;&amp;quot;inference compute&amp;quot; id=cpu library=cpu&lt;/code&gt; ‚Üí &lt;code&gt;&amp;quot;total vram&amp;quot;=&amp;quot;0 B&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Models run on CPU (slow - ~60+ seconds for simple queries)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No error messages&lt;/strong&gt; - Ollama silently falls back to CPU&lt;/li&gt; &lt;li&gt;GPU is functional: &lt;code&gt;nvidia-smi&lt;/code&gt; works, RAG service uses GPU for embeddings/reranking successfully&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What Worked Before&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;This worked before November 17, 2025.&lt;/strong&gt; Logs from Nov 17 show:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;ggml_cuda_init: found 1 CUDA devices&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Models successfully offloaded to GPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;After a system reboot on Nov 18, GPU detection stopped working.&lt;/p&gt; &lt;h1&gt;What I've Tried&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;‚úÖ Environment variables (&lt;code&gt;OLLAMA_NUM_GPU=1&lt;/code&gt;, &lt;code&gt;CUDA_VISIBLE_DEVICES=0&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;‚úÖ Reinstalled Ollama binary (v0.12.11 from GitHub releases)&lt;/li&gt; &lt;li&gt;‚úÖ Manual CUDA library path configuration (&lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;‚úÖ Symlinks for CUDA libraries&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Clean install&lt;/strong&gt; - complete removal of all Ollama files/configs + fresh install&lt;/li&gt; &lt;li&gt;‚úÖ Minimal configuration (removed all manual overrides, let Ollama auto-discover)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;: All attempts show the same behavior - GPU discovery runs but immediately falls back to CPU within ~13ms.&lt;/p&gt; &lt;h1&gt;Current Configuration&lt;/h1&gt; &lt;p&gt;Minimal systemd override (no manual library paths):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Service] Environment=OLLAMA_MODELS=/mnt/shared/ollama-models/models Environment=CUDA_VISIBLE_DEVICES=0 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Hypothesis&lt;/h1&gt; &lt;p&gt;I suspect &lt;strong&gt;Ollama 0.12.11 doesn't support Compute Capability 12.0 (Blackwell architecture)&lt;/strong&gt; yet. The RTX 5070 Ti is very new hardware, and Ollama's bundled CUDA runners may not include kernels compiled for CC 12.0. When initialization fails, Ollama gracefully falls back to CPU without error messages.&lt;/p&gt; &lt;h1&gt;Questions&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Has anyone else with RTX 50-series GPUs (Blackwell) experienced this?&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Is there a known issue or workaround for CC 12.0 support?&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Are there any debug flags or logs that would show why CUDA initialization fails?&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Should I try rolling back to an older Ollama version that worked before Nov 17?&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Additional Info&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Cloud models work fine (authenticated with Ollama Cloud)&lt;/li&gt; &lt;li&gt;RAG service successfully uses GPU for embeddings/reranking (confirms GPU is functional)&lt;/li&gt; &lt;li&gt;Models tested: &lt;code&gt;qwen3:14b&lt;/code&gt;, &lt;code&gt;llama3.1:8b&lt;/code&gt;, &lt;code&gt;qwen:14b&lt;/code&gt; - all show same behavior&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks in advance for any insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deparko"&gt; /u/deparko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1izx2/ollama_not_using_gpu_on_rtx_5070_ti_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1izx2/ollama_not_using_gpu_on_rtx_5070_ti_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1izx2/ollama_not_using_gpu_on_rtx_5070_ti_blackwell/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T20:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2akip</id>
    <title>Perplexity AI PRO - 1 YEAR at 90% Discount ‚Äì Don‚Äôt Miss Out!</title>
    <updated>2025-11-20T18:18:44+00:00</updated>
    <author>
      <name>/u/Verza-</name>
      <uri>https://old.reddit.com/user/Verza-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p2akip/perplexity_ai_pro_1_year_at_90_discount_dont_miss/"&gt; &lt;img alt="Perplexity AI PRO - 1 YEAR at 90% Discount ‚Äì Don‚Äôt Miss Out!" src="https://preview.redd.it/n3tnktqreg2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64e35a540fb60bb61d4a150741bf99c0cb7d9deb" title="Perplexity AI PRO - 1 YEAR at 90% Discount ‚Äì Don‚Äôt Miss Out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get Perplexity AI PRO (1-Year) ‚Äì at 90% OFF!&lt;/p&gt; &lt;p&gt;Order here: &lt;a href="https://cheapgpts.store/Perplexity"&gt;CHEAPGPT.STORE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Plan: 12 Months&lt;/p&gt; &lt;p&gt;üí≥ Pay with: PayPal or Revolut&lt;/p&gt; &lt;p&gt;Reddit reviews: &lt;a href="https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu"&gt;FEEDBACK POST&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TrustPilot: &lt;a href="https://www.trustpilot.com/review/cheapgpt.store"&gt;TrustPilot FEEDBACK&lt;/a&gt;&lt;br /&gt; Bonus: Apply code PROMO5 for $5 OFF your order! &lt;/p&gt; &lt;p&gt;BONUS!: Enjoy the AI Powered automated web browser. (Presented by Perplexity) included!&lt;/p&gt; &lt;p&gt;Trusted and the cheapest!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Verza-"&gt; /u/Verza- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n3tnktqreg2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2akip/perplexity_ai_pro_1_year_at_90_discount_dont_miss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2akip/perplexity_ai_pro_1_year_at_90_discount_dont_miss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T18:18:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1rv91</id>
    <title>Browser extension Powered by Ollama for Code Reviews on Gitlab and Azure DO</title>
    <updated>2025-11-20T02:47:17+00:00</updated>
    <author>
      <name>/u/Brilliant-Vehicle994</name>
      <uri>https://old.reddit.com/user/Brilliant-Vehicle994</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p1rv91/browser_extension_powered_by_ollama_for_code/"&gt; &lt;img alt="Browser extension Powered by Ollama for Code Reviews on Gitlab and Azure DO" src="https://preview.redd.it/eki2xx0bsb2g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=0c9381ba8c12943fb74f536960db2c403d6897c8" title="Browser extension Powered by Ollama for Code Reviews on Gitlab and Azure DO" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello friends,&lt;/p&gt; &lt;p&gt;I just want to let the community know about my open source project ThinkReview that is now powered by Ollama&lt;br /&gt; It does code reviews for Pull and merge requests on Gitlab and Azure DO , summarize the changes , find security issues , best practices and provide scoring , in addition conversations to chat with your OR and dive deeper.&lt;/p&gt; &lt;p&gt;The project is open source under AGPL 3.0 license : &lt;a href="https://github.com/Thinkode/thinkreview-browser-extension"&gt;https://github.com/Thinkode/thinkreview-browser-extension&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and is available on chrome store &lt;a href="https://chromewebstore.google.com/detail/thinkreview-ai-code-revie/bpgkhgbchmlmpjjpmlaiejhnnbkdjdjn"&gt;https://chromewebstore.google.com/detail/thinkreview-ai-code-revie/bpgkhgbchmlmpjjpmlaiejhnnbkdjdjn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love for some of you to try and give me some feedback&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant-Vehicle994"&gt; /u/Brilliant-Vehicle994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eki2xx0bsb2g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1rv91/browser_extension_powered_by_ollama_for_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1rv91/browser_extension_powered_by_ollama_for_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T02:47:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1836v</id>
    <title>An Open-Source Agent Foundation Model with Interactive ScalingÔºÅMiroThinker V1.0 just launched!</title>
    <updated>2025-11-19T13:45:54+00:00</updated>
    <author>
      <name>/u/wuqiao</name>
      <uri>https://old.reddit.com/user/wuqiao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p1836v/an_opensource_agent_foundation_model_with/"&gt; &lt;img alt="An Open-Source Agent Foundation Model with Interactive ScalingÔºÅMiroThinker V1.0 just launched!" src="https://external-preview.redd.it/Jeli8vyNHpi1OW6VpCLC7sqFTicW7HMwR1zgB4aSLV4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c40f788dfb4b80413245504088417de6a745393" title="An Open-Source Agent Foundation Model with Interactive ScalingÔºÅMiroThinker V1.0 just launched!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiroThinker v1.0 just launched recently! We're back with a MASSIVE update that's gonna blow your mind!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Code&lt;/strong&gt;Ôºö&lt;a href="https://github.com/MiroMindAI/MiroThinker"&gt;https://github.com/MiroMindAI/MiroThinker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Paper&lt;/strong&gt;Ôºö&lt;a href="https://huggingface.co/papers/2511.11793"&gt;https://huggingface.co/papers/2511.11793&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're introducing the &amp;quot;Interactive Scaling&amp;quot; - a completely new dimension for AI scaling! Instead of just throwing more data/params at models, we let agents learn through deep environmental interaction. The more they practice &amp;amp; reflect, the smarter they get! &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;256K Context + 600-Turn Tool Interaction&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance That Slaps:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;BrowseComp: 47.1% accuracy (nearly matches OpenAI DeepResearch at 51.5%)&lt;/li&gt; &lt;li&gt;Chinese tasks (BrowseComp-ZH): 7.7pp better than DeepSeek-v3.2&lt;/li&gt; &lt;li&gt;First-tier performance across HLE, GAIA, xBench-DeepSearch, SEAL-0&lt;/li&gt; &lt;li&gt;Competing head-to-head with GPT, Grok, Claude&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% Open Source&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Full model weights ‚úÖ &lt;/li&gt; &lt;li&gt;Complete toolchains ‚úÖ &lt;/li&gt; &lt;li&gt;Interaction frameworks ‚úÖ&lt;/li&gt; &lt;li&gt;Because transparency &amp;gt; black boxes&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about the Interactive Scaling approach or benchmarks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wuqiao"&gt; /u/wuqiao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-72B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1836v/an_opensource_agent_foundation_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1836v/an_opensource_agent_foundation_model_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T13:45:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1aurp</id>
    <title>An update to Nanocoder üî•</title>
    <updated>2025-11-19T15:35:49+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p1aurp/an_update_to_nanocoder/"&gt; &lt;img alt="An update to Nanocoder üî•" src="https://b.thumbs.redditmedia.com/o-J_tGC-BVWwO-0BEfEwLQLLmDRKaHlx0Lt83jP6NzI.jpg" title="An update to Nanocoder üî•" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/r4j2v8emc82g1.png?width=1734&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e09117983a322fd00410d747da2c1cff7cdda800"&gt;https://preview.redd.it/r4j2v8emc82g1.png?width=1734&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e09117983a322fd00410d747da2c1cff7cdda800&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;Just a quick update on Nanocoder - the open-source, open-community coding CLI that's built with privacy + local-first in mind. You may have seen posts on here before with updates!&lt;/p&gt; &lt;p&gt;One of the first comments on the last post was about starting a dedicated sub-reddit for those interested enough. We've now created this and will slowly phase to use it as an additional channel to provide updates and interact with the AI community over other sub-reddits.&lt;/p&gt; &lt;p&gt;We can't thank everyone enough though that has engaged so positively with the project on sub-reddits like &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;. It means a lot and the community we're building as grown hugely since we started in August.&lt;/p&gt; &lt;p&gt;If you want to join our sub-reddit, you can find it here: &lt;a href="/r/nanocoder"&gt;r/nanocoder&lt;/a&gt; - again, we'll breathe more life into this page as time goes along!&lt;/p&gt; &lt;p&gt;As for what's happening in the world of Nanocoder:&lt;/p&gt; &lt;p&gt;- We're almost at 1K stars!!!&lt;/p&gt; &lt;p&gt;- We've fully switched to use AI SDK now over LangGraph. This has been a fantastic change and one that allows us to expand capabilities of the agent.&lt;/p&gt; &lt;p&gt;- You can now tag files into context with `@`.&lt;/p&gt; &lt;p&gt;- You can no track context usage with the `/usage` command.&lt;/p&gt; &lt;p&gt;- One of our main goals is to make Nanocoder work well and reliably with smaller and smaller models. To do this, we've continued to work on everything from fine-tuned models to better tool orchestration and context management. &lt;/p&gt; &lt;p&gt;We're now at a point where models like `gpt-oss:20b` are reliably working well within the CLI for smaller coding tasks. This is ongoing but we're improving every week. The end vision is to be able to code using Nanocoder totally locally with no need for APIs if you don't want them!&lt;/p&gt; &lt;p&gt;- Continued work to build a small language model into &lt;a href="https://github.com/Nano-Collective/get-md"&gt;get-md&lt;/a&gt; for more accurate and context aware markdown generation for LLMs.&lt;/p&gt; &lt;p&gt;If you're interested in the project, we're a completely open collective building privacy-focused AI. We actively invite all contributions to help build a tool for the community by the community! I'd love for you to get involved :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;em&gt;GitHub Repo&lt;/em&gt;: &lt;a href="https://github.com/Nano-Collective/nanocoder"&gt;https://github.com/Nano-Collective/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Discord&lt;/em&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1aurp/an_update_to_nanocoder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1aurp/an_update_to_nanocoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1aurp/an_update_to_nanocoder/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T15:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p219ok</id>
    <title>Webui agent model in vscode</title>
    <updated>2025-11-20T11:56:47+00:00</updated>
    <author>
      <name>/u/TheRealFAG69</name>
      <uri>https://old.reddit.com/user/TheRealFAG69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it possible to use a custom webui model with a knowledge base in vscode? It would be very handy for VHDL coding &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealFAG69"&gt; /u/TheRealFAG69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p219ok/webui_agent_model_in_vscode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p219ok/webui_agent_model_in_vscode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p219ok/webui_agent_model_in_vscode/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T11:56:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p27i64</id>
    <title>Ollama signin docker compose</title>
    <updated>2025-11-20T16:23:36+00:00</updated>
    <author>
      <name>/u/Brilliant_Anxiety_36</name>
      <uri>https://old.reddit.com/user/Brilliant_Anxiety_36</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I have a question im trying to build a stack on docker compose with openwebui and ollama but when i access the container of ollama and i run &lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I get this: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use &amp;quot;ollama [command] --help&amp;quot; for more information about a command. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wich is normal but when i run&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama singin&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Nothing happens. When i do it on ubuntu i get the link to access to ollama and give access to the machine. &lt;/p&gt; &lt;p&gt;I'm ok using ollama directly on the machine but i will like to use it on the stack. Im guessing ollama signin is not availeable yet for containers? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant_Anxiety_36"&gt; /u/Brilliant_Anxiety_36 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p27i64/ollama_signin_docker_compose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p27i64/ollama_signin_docker_compose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p27i64/ollama_signin_docker_compose/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T16:23:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1mnra</id>
    <title>DeepSeek-OCR</title>
    <updated>2025-11-19T22:55:37+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek-OCR is a vision-language model that can perform token-efficient optical character recognition (OCR).&lt;/p&gt; &lt;p&gt;DeepSeek-OCR requires Ollama v0.13.0 or later.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/deepseek-ocr"&gt;https://ollama.com/library/deepseek-ocr&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1mnra/deepseekocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1mnra/deepseekocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1mnra/deepseekocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T22:55:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1p25jl7</id>
    <title>We trained an SLM assistants for assistance with commit messages on TypeScript codebases - Qwen 3 model (0.6B parameters) that you can run locally!</title>
    <updated>2025-11-20T15:08:26+00:00</updated>
    <author>
      <name>/u/kruszczynski</name>
      <uri>https://old.reddit.com/user/kruszczynski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p25jl7/we_trained_an_slm_assistants_for_assistance_with/"&gt; &lt;img alt="We trained an SLM assistants for assistance with commit messages on TypeScript codebases - Qwen 3 model (0.6B parameters) that you can run locally!" src="https://preview.redd.it/etw8u82jgf2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=404ecde2c4e912c91cbf66ac37ebd425fa6bbb15" title="We trained an SLM assistants for assistance with commit messages on TypeScript codebases - Qwen 3 model (0.6B parameters) that you can run locally!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kruszczynski"&gt; /u/kruszczynski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/etw8u82jgf2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p25jl7/we_trained_an_slm_assistants_for_assistance_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p25jl7/we_trained_an_slm_assistants_for_assistance_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T15:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1p264l9</id>
    <title>Host open-source LLM on a local server and access it Publicly</title>
    <updated>2025-11-20T15:31:30+00:00</updated>
    <author>
      <name>/u/ibjects</name>
      <uri>https://old.reddit.com/user/ibjects</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p264l9/host_opensource_llm_on_a_local_server_and_access/"&gt; &lt;img alt="Host open-source LLM on a local server and access it Publicly" src="https://external-preview.redd.it/aKJxI02wF_Cz77qgjK9fKjpcdmyKIzBmMYNw1dRgElo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3de5bdd2442b26d9f880ac7a730349c5ee8e0369" title="Host open-source LLM on a local server and access it Publicly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ibjects"&gt; /u/ibjects &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ibjects.medium.com/950f48c6858e?source=friends_link&amp;amp;sk=9caf3c3738c9538cd7b0a54def6c6b18"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p264l9/host_opensource_llm_on_a_local_server_and_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p264l9/host_opensource_llm_on_a_local_server_and_access/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T15:31:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2evio</id>
    <title>Delta Dialogue for local model conversations with report drafting</title>
    <updated>2025-11-20T20:57:28+00:00</updated>
    <author>
      <name>/u/spreader123</name>
      <uri>https://old.reddit.com/user/spreader123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I rediscovered a project I built a few months ago to help me flesh out ideas, draft designs, and rapidly explore concepts. It‚Äôs called Delta Dialogue, and it‚Äôs a local conversation framework for Ollama models that lets them collaborate on any topic you throw at them.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;What it does&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Core idea: Treat each exchange as a ‚Äúdelta‚Äù (a change-state) so conversations build coherently over rounds.&lt;/li&gt; &lt;li&gt;Local-first: Runs with Ollama models only; no web search. Results depend on model training, but are generally solid.&lt;/li&gt; &lt;li&gt;Flexible use: Works for brainstorming, instructions, design ideas, foreign concepts, you name it, they will give it the old college try. &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;How it works&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model setup: You can run multiple distinct models (one of each), or a single model that chains off its own messages.&lt;/li&gt; &lt;li&gt;Rounds: Set 1‚Äì10 rounds. Example: 2 models √ó 10 rounds = 20 replies in a single chain.&lt;/li&gt; &lt;li&gt;Parallel reporting: Each model updates an executive report before finishing its main turn. It starts as a copy of the live feed, then gets refined as the discussion deepens. The report evolves in parallel to the raw dialogue.&lt;/li&gt; &lt;li&gt;Open-ended: There‚Äôs no automatic conclusion step; you can stop any time.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Setup and usage instructions&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Install requirements: &lt;ul&gt; &lt;li&gt;Ollama, Python, and project dependencies.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Launch: &lt;ul&gt; &lt;li&gt;Run Launchfluidoracle.bat.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Select models: &lt;ul&gt; &lt;li&gt;Click the first model to populate the ‚Äúmodel theater‚Äù staging area.&lt;/li&gt; &lt;li&gt;Add more models: Ctrl+click additional models to include them in the discourse.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Prime the session: &lt;ul&gt; &lt;li&gt;Click Think Mode, then click Activate All beneath the staging area.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Set rounds: &lt;ul&gt; &lt;li&gt;Choose a count from 1‚Äì10 (recommend &amp;gt;1).&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Example: 5 models √ó 10 rounds = 50 messages total.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Enter your prompt: &lt;ul&gt; &lt;li&gt;Type into the Fluid Prompt input.&lt;/li&gt; &lt;li&gt;Important: Do not highlight text at this stage; highlighting clears the staging area.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Start the report: &lt;ul&gt; &lt;li&gt;Click Start Report to activate the executive report drafting process.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Initiate dialogue: &lt;ul&gt; &lt;li&gt;Click Initiate Fluid Dialogue.&lt;/li&gt; &lt;li&gt;You can resize the layout; after initiating, highlighting won‚Äôt affect the queued run (models may visually disappear from staging, but the queue is already loaded).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Wait for responses: &lt;ul&gt; &lt;li&gt;Depending on your machine, expect the first reply after a short delay.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Known limits and tips&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt length: There‚Äôs a character limit. I typically keep inputs under ~20 paragraphs. &lt;ul&gt; &lt;li&gt;Symptom: Empty, instant responses mean the prompt was too long‚Äîshorten it.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;UI quirk: Highlighting text before initiating can clear selected models from staging.&lt;/li&gt; &lt;li&gt;Performance: On lower-spec machines, choose lighter Ollama models to avoid slowdowns.&lt;/li&gt; &lt;li&gt;Portability: Built pre‚Äìturbo/cloud; should be straightforward to port to a cloud setup if you want bigger conversations.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Looking for feedback!!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spreader123"&gt; /u/spreader123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Yufok1/Delta_Dialogue"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2evio/delta_dialogue_for_local_model_conversations_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2evio/delta_dialogue_for_local_model_conversations_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T20:57:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2954y</id>
    <title>Evaluating 5090 Desktops for running LLMs locally/ollama</title>
    <updated>2025-11-20T17:24:29+00:00</updated>
    <author>
      <name>/u/Excellent_Composer42</name>
      <uri>https://old.reddit.com/user/Excellent_Composer42</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Composer42"&gt; /u/Excellent_Composer42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1p294oo/evaluating_5090_desktops_for_running_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2954y/evaluating_5090_desktops_for_running_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2954y/evaluating_5090_desktops_for_running_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T17:24:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1p22trj</id>
    <title>Computer Use with Gemini 3 pro</title>
    <updated>2025-11-20T13:13:17+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p22trj/computer_use_with_gemini_3_pro/"&gt; &lt;img alt="Computer Use with Gemini 3 pro" src="https://external-preview.redd.it/Ynh4bWs0aTl3ZTJnMUjWqlAeWHcDEffTfb8EVeAbSnSLMt2a5iQQQ8LfY2lH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b2a472e07bbd2da794d06bc33de06e735e9298d" title="Computer Use with Gemini 3 pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemini 3 pro for Computer Use.&lt;/p&gt; &lt;p&gt;Built with the new windows sandboxes.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs : &lt;a href="https://cua.ai/docs/example-usecases/gemini-complex-ui-navigation"&gt;https://cua.ai/docs/example-usecases/gemini-complex-ui-navigation&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4g3t80r9we2g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p22trj/computer_use_with_gemini_3_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p22trj/computer_use_with_gemini_3_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T13:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2gzxk</id>
    <title>Cortex got a massive update! (ollama UI desktop ap)</title>
    <updated>2025-11-20T22:18:14+00:00</updated>
    <author>
      <name>/u/Ok-Function-7101</name>
      <uri>https://old.reddit.com/user/Ok-Function-7101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p2gzxk/cortex_got_a_massive_update_ollama_ui_desktop_ap/"&gt; &lt;img alt="Cortex got a massive update! (ollama UI desktop ap)" src="https://a.thumbs.redditmedia.com/twAZvPomgU64duwvRIJ4q3p_WaI91rdw3r3eSX7-LB0.jpg" title="Cortex got a massive update! (ollama UI desktop ap)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its entirely open-source and you're invited to come try it out! &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://dovvnloading.github.io/Cortex/"&gt;https://dovvnloading.github.io/Cortex/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8fkh4lhclh2g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9dd2214c5ff9b533cdc596a7217359ed0d07689"&gt;https://preview.redd.it/8fkh4lhclh2g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9dd2214c5ff9b533cdc596a7217359ed0d07689&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ht1e8mhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b86ab8106804a525f3690e85f42c8c2b8612d0cf"&gt;https://preview.redd.it/ht1e8mhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b86ab8106804a525f3690e85f42c8c2b8612d0cf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6jvmrmhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eee7558190f4d4564fdc4284b7e207fb08d4ae7c"&gt;https://preview.redd.it/6jvmrmhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eee7558190f4d4564fdc4284b7e207fb08d4ae7c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j3kzilhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a374639836dc6abdf736458797d3d328c32df70"&gt;https://preview.redd.it/j3kzilhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a374639836dc6abdf736458797d3d328c32df70&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7mpmalhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb9ac99722711d9d4a0981247c3bf883fc88bd0"&gt;https://preview.redd.it/7mpmalhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb9ac99722711d9d4a0981247c3bf883fc88bd0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xm3kklhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b72d44f1aa1495b585e2cc5a08148bf91f53e19"&gt;https://preview.redd.it/xm3kklhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b72d44f1aa1495b585e2cc5a08148bf91f53e19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hza6cnhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85ab8a70697092d5fb0c13e1446f2033fefb707e"&gt;https://preview.redd.it/hza6cnhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85ab8a70697092d5fb0c13e1446f2033fefb707e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kvbxrlhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e538e902ef4db617145140d9ffa90f6df440302"&gt;https://preview.redd.it/kvbxrlhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e538e902ef4db617145140d9ffa90f6df440302&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Function-7101"&gt; /u/Ok-Function-7101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2gzxk/cortex_got_a_massive_update_ollama_ui_desktop_ap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2gzxk/cortex_got_a_massive_update_ollama_ui_desktop_ap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2gzxk/cortex_got_a_massive_update_ollama_ui_desktop_ap/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T22:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2mro7</id>
    <title>Best &lt; $20k Configuration</title>
    <updated>2025-11-21T02:30:03+00:00</updated>
    <author>
      <name>/u/JMWTech</name>
      <uri>https://old.reddit.com/user/JMWTech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What would you build with $20k to train a model(s) and operate a on-prem chat bot for document and policy retrieval?&lt;/p&gt; &lt;p&gt;I've received quotes from &amp;quot;workstations&amp;quot; with 5090s to rack mounted servers running either four L4s (ewww) to a dual proc single RTX Pro 6000. Just want to make sure we're not wasting money and getting the most bang for the buck.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JMWTech"&gt; /u/JMWTech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2mro7/best_20k_configuration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2mro7/best_20k_configuration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2mro7/best_20k_configuration/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T02:30:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3lfua</id>
    <title>üöÄ Just Finished an INSANE MCP + LangChain + Claude Course ‚Äî Mind = Blown ü§Ø</title>
    <updated>2025-11-22T05:40:59+00:00</updated>
    <author>
      <name>/u/Distinct-Truth7165</name>
      <uri>https://old.reddit.com/user/Distinct-Truth7165</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Distinct-Truth7165"&gt; /u/Distinct-Truth7165 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/learnmachinelearning/comments/1p3lfgj/just_finished_an_insane_mcp_langchain_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3lfua/just_finished_an_insane_mcp_langchain_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p3lfua/just_finished_an_insane_mcp_langchain_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-22T05:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p32ism</id>
    <title>4096 token limit</title>
    <updated>2025-11-21T16:04:50+00:00</updated>
    <author>
      <name>/u/aleglr20</name>
      <uri>https://old.reddit.com/user/aleglr20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I‚Äôm not sure if this is the right subreddit for this question ‚Äî if not, please let me know where I should post it.&lt;br /&gt; Anyway, I‚Äôm working on a Java project using the &lt;code&gt;spring-ai-starter-model-openai&lt;/code&gt; dependency, and I‚Äôm currently using &lt;code&gt;gemma3:4b&lt;/code&gt; through Ollama, which exposes OpenAI-compatible endpoints.&lt;/p&gt; &lt;p&gt;I have a chat method where I pass a text as context and then ask a question about it. The text and the question are combined into a single prompt that I send to the model.&lt;br /&gt; From the JSON response, I noticed the token usage data, and I discovered that if I go above roughly 4,070 tokens, the model gives a wrong or incoherent answer ‚Äî it no longer follows the question or the provided context.&lt;/p&gt; &lt;p&gt;Can someone explain to me how the 4,096-token limit works? Even if the model has a 128k context window?&lt;br /&gt; Is the 4,096-token limit related to the output, the prompt, or both? Because I‚Äôm experiencing issues specifically when the &lt;em&gt;prompt&lt;/em&gt; gets too large, even before the output is generated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aleglr20"&gt; /u/aleglr20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p32ism/4096_token_limit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p32ism/4096_token_limit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p32ism/4096_token_limit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T16:04:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3czc0</id>
    <title>What am I missing?</title>
    <updated>2025-11-21T22:51:53+00:00</updated>
    <author>
      <name>/u/SaltbushBillJP</name>
      <uri>https://old.reddit.com/user/SaltbushBillJP</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Page Assist, testing several LLMs and trying to have OLLAMA extract specific values from multiple files (each being an exported email - I've tested PDF and txt formats).&lt;/p&gt; &lt;p&gt;The emails are responses from local government acknowledging permit applications and I want to extract registration number and submission date from each email (file). &lt;/p&gt; &lt;p&gt;This works for one or two, then the response is completed with blank values or N/A or some other rubbish.&lt;/p&gt; &lt;p&gt;I've loaded about 6 of these files into a Knowledge Base and selected it for the evaluation&lt;/p&gt; &lt;p&gt;Should I edit RAG settings?&lt;/p&gt; &lt;p&gt;What else do I need to do so the query correctly evaluates more than 2 documents?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SaltbushBillJP"&gt; /u/SaltbushBillJP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3czc0/what_am_i_missing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3czc0/what_am_i_missing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p3czc0/what_am_i_missing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T22:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p33czu</id>
    <title>Ollama Grid Search v0.9.2: Enhanced LLM Evaluation and Comparison</title>
    <updated>2025-11-21T16:37:01+00:00</updated>
    <author>
      <name>/u/grudev</name>
      <uri>https://old.reddit.com/user/grudev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Happy to announce the release of &lt;a href="https://github.com/dezoito/ollama-grid-search/"&gt;&lt;strong&gt;Ollama Grid Search v0.9.2&lt;/strong&gt;&lt;/a&gt;, a tool created to improve the experience of those of use evaluating and experimenting with multiple LLMs&lt;/p&gt; &lt;p&gt;This addresses issues with damaged &lt;code&gt;.dmg&lt;/code&gt; files that some users experienced during installation (a result of GitHub actions script + Apple's signing requirements). The build process has been updated to improve the setup for all macOS users, particularly those on Apple Silicon (M1/M2/M3/M4) devices.&lt;/p&gt; &lt;h1&gt;About Ollama Grid Search&lt;/h1&gt; &lt;p&gt;For those new to the project, Ollama Grid Search is a desktop application that automates the process of evaluating and comparing multiple Large Language Models (LLMs). Whether you're fine-tuning prompts, selecting the best model for your use case, or conducting A/B tests, this tool will make your life easier.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-Model Testing&lt;/strong&gt;: Automatically fetch and test multiple models from your Ollama servers&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grid Search&lt;/strong&gt;: Iterate over combinations of models, prompts, and parameters&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A/B Testing&lt;/strong&gt;: Compare responses from different prompts and models side-by-side&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt Management&lt;/strong&gt;: Built-in prompt database with autocomplete functionality&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Experiment Logs&lt;/strong&gt;: Track, review, and re-run past experiments&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Concurrent Inference&lt;/strong&gt;: Support for parallel inference calls to speed up evaluations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Results&lt;/strong&gt;: Easy-to-read interface for comparing model outputs&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Getting Started&lt;/h1&gt; &lt;p&gt;Download the latest release from our &lt;a href="https://github.com/dezoito/ollama-grid-search/releases"&gt;releases page&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/dezoito/ollama-grid-search"&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/dezoito/ollama-grid-search/blob/main/CHANGELOG.md"&gt;Full Changelog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://dezoito.github.io/2023/12/27/rust-ollama-grid-search.html"&gt;In-depth Grid Search Tutorial&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grudev"&gt; /u/grudev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p33czu/ollama_grid_search_v092_enhanced_llm_evaluation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p33czu/ollama_grid_search_v092_enhanced_llm_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p33czu/ollama_grid_search_v092_enhanced_llm_evaluation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T16:37:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p361yc</id>
    <title>MCP Script - an Agent Oriented Programming Language</title>
    <updated>2025-11-21T18:17:58+00:00</updated>
    <author>
      <name>/u/atinylittleshell</name>
      <uri>https://old.reddit.com/user/atinylittleshell</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building a scripting language for composing agentic workflows using MCP as the fundamental building block. It's in super early stage but I'm curious to see if you would find something like this useful. &lt;/p&gt; &lt;p&gt;Repo is here: &lt;a href="https://github.com/mcpscript/mcpscript"&gt;https://github.com/mcpscript/mcpscript&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this language, models, tools, agents, conversations are first-class native constructs. Every function is a tool, every tool is a function - they can be called deterministically or given to an agent. &lt;/p&gt; &lt;p&gt;``` // Configure a local model using Ollama model gpt { provider: &amp;quot;openai&amp;quot;, apiKey: &amp;quot;ollama&amp;quot;, baseURL: &amp;quot;http://localhost:11434/v1&amp;quot;, model: &amp;quot;gpt-oss:20b&amp;quot;, temperature: 0.1 }&lt;/p&gt; &lt;p&gt;// Set up the filesystem MCP server mcp filesystem { command: &amp;quot;npx&amp;quot;, args: [&amp;quot;-y&amp;quot;, &amp;quot;@modelcontextprotocol/server-filesystem@latest&amp;quot;], stderr: &amp;quot;ignore&amp;quot; }&lt;/p&gt; &lt;p&gt;// Read the memory file (AGENTS.md) from the current directory memoryContent = filesystem.read_file({ path: &amp;quot;AGENTS.md&amp;quot; }) print(&amp;quot;Memory file loaded successfully (&amp;quot; + memoryContent.length + &amp;quot; characters)&amp;quot;)&lt;/p&gt; &lt;p&gt;// Define a coding agent with access to filesystem tools agent CodingAgent { model: gpt, systemPrompt: &amp;quot;You are an expert software developer assistant. You have access to filesystem tools and can help with code analysis, debugging, and development tasks. Be concise and helpful.&amp;quot;, tools: [filesystem] } ```&lt;/p&gt; &lt;p&gt;Messages can be piped to an agent or a conversation, so you can -&lt;/p&gt; &lt;p&gt;``` // send a message to an agent convo = &amp;quot;help me fix this bug&amp;quot; | CodingAgent&lt;/p&gt; &lt;p&gt;// append a message to a conversation convo = convo | &amp;quot;review the fix&amp;quot;&lt;/p&gt; &lt;p&gt;// pass that to another agent convo = convo | ReviewAgent&lt;/p&gt; &lt;p&gt;// or just chain them together &amp;quot;help me fix this bug&amp;quot; | CodingAgent | &amp;quot;review the fix&amp;quot; | ReviewAgent ```&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atinylittleshell"&gt; /u/atinylittleshell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p361yc/mcp_script_an_agent_oriented_programming_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p361yc/mcp_script_an_agent_oriented_programming_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p361yc/mcp_script_an_agent_oriented_programming_language/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T18:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3vpni</id>
    <title>Your local Ollama agents can be just as good as closed-source models - I open-sourced Stanford's ACE framework that makes agents learn from mistakes</title>
    <updated>2025-11-22T15:17:51+00:00</updated>
    <author>
      <name>/u/cheetguy</name>
      <uri>https://old.reddit.com/user/cheetguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I implemented Stanford's &lt;a href="https://arxiv.org/abs/2510.04618"&gt;Agentic Context Engineering paper&lt;/a&gt; for Ollama. The framework makes agents learn from their own execution feedback through in-context learning instead of fine-tuning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; Agent runs task ‚Üí reflects on what worked/failed ‚Üí curates strategies into playbook ‚Üí uses playbook on next run&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Improvement:&lt;/strong&gt; Paper shows +17.1pp accuracy improvement vs base LLM (‚âà+40% relative improvement) on agent benchmarks (DeepSeek-V3.1 non-thinking mode), helping close the gap with closed-source models. All through in-context learning (no fine-tuning needed).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Open-Source Implementation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop into existing agents in ~10 lines of code&lt;/li&gt; &lt;li&gt;Works with any Ollama model (Llama, Qwen, Mistral, DeepSeek, etc.)&lt;/li&gt; &lt;li&gt;Real-world test on browser automation agent: &lt;ul&gt; &lt;li&gt;30% ‚Üí 100% success rate&lt;/li&gt; &lt;li&gt;82% fewer steps&lt;/li&gt; &lt;li&gt;65% decrease in token cost&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;https://github.com/kayba-ai/agentic-context-engine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ollama Starter Template: &lt;a href="https://github.com/kayba-ai/agentic-context-engine/blob/main/examples/ollama/ollama_starter_template.py"&gt;https://github.com/kayba-ai/agentic-context-engine/blob/main/examples/ollama/ollama_starter_template.py&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear if anyone tries this with Ollama! Especially curious how it performs with different Ollama models.&lt;/p&gt; &lt;p&gt;I'm currently actively improving this based on feedback - &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;‚≠ê the repo&lt;/a&gt; so you can stay updated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cheetguy"&gt; /u/cheetguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-22T15:17:51+00:00</published>
  </entry>
</feed>
