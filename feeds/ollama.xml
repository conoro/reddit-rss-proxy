<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-13T18:47:29+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1q7xej4</id>
    <title>I built Plano - a framework-friendly data plane with orchestration for agents</title>
    <updated>2026-01-09T03:32:09+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q7xej4/i_built_plano_a_frameworkfriendly_data_plane_with/"&gt; &lt;img alt="I built Plano - a framework-friendly data plane with orchestration for agents" src="https://preview.redd.it/hn2gtphwt8cg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ecdde49cb7c4b04ae32b067c6ade544a6dd2179" title="I built Plano - a framework-friendly data plane with orchestration for agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thrilled to be launching &lt;a href="https://github.com/katanemo/plano"&gt;Plano&lt;/a&gt; today - delivery infrastructure for agentic apps: An edge and service proxy server with orchestration for AI agents. Plano's core purpose is to offload all the plumbing work required to deliver agents to production so that developers can stay focused on core product logic.&lt;/p&gt; &lt;p&gt;Plano runs alongside your app servers (cloud, on-prem, or local dev) deployed as a side-car, and leaves GPUs where your models are hosted.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On the ground AI practitioners will tell you that calling an LLM is not the hard part. The really hard part is delivering agentic applications to production quickly and reliably, then iterating without rewriting system code every time. In practice, teams keep rebuilding the same concerns that sit outside any single agent‚Äôs core logic:&lt;/p&gt; &lt;p&gt;This includes model agility - the ability to pull from a large set of LLMs and swap providers without refactoring prompts or streaming handlers. Developers need to learn from production by collecting signals and traces that tell them what to fix. They also need consistent policy enforcement for moderation and jailbreak protection, rather than sprinkling hooks across codebases. And they need multi-agent patterns to improve performance and latency without turning their app into orchestration glue.&lt;/p&gt; &lt;p&gt;These concerns get rebuilt and maintained inside fast-changing frameworks and application code, coupling product logic to infrastructure decisions. It‚Äôs brittle, and pulls teams away from core product work into plumbing they shouldn‚Äôt have to own.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Plano does&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Plano moves core delivery concerns out of process into a modular proxy and dataplane designed for agents. It supports inbound listeners (agent orchestration, safety and moderation hooks), outbound listeners (hosted or API-based LLM routing), or both together. Plano provides the following capabilities via a unified dataplane:&lt;/p&gt; &lt;p&gt;- Orchestration: Low-latency routing and handoff between agents. Add or change agents without modifying app code, and evolve strategies centrally instead of duplicating logic across services.&lt;/p&gt; &lt;p&gt;- Guardrails &amp;amp; Memory Hooks: Apply jailbreak protection, content policies, and context workflows (rewriting, retrieval, redaction) once via filter chains. This centralizes governance and ensures consistent behavior across your stack.&lt;/p&gt; &lt;p&gt;- Model Agility: Route by model name, semantic alias, or preference-based policies. Swap or add models without refactoring prompts, tool calls, or streaming handlers.&lt;/p&gt; &lt;p&gt;- Agentic Signals‚Ñ¢: Zero-code capture of behavior signals, traces, and metrics across every agent, surfacing traces, token usage, and learning signals in one place.&lt;/p&gt; &lt;p&gt;The goal is to keep application code focused on product logic while Plano owns delivery mechanics.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;More on Architecture&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Plano has two main parts:&lt;/p&gt; &lt;p&gt;Envoy-based data plane. Uses Envoy‚Äôs HTTP connection management to talk to model APIs, services, and tool backends. We didn‚Äôt build a separate model server‚ÄîEnvoy already handles streaming, retries, timeouts, and connection pooling. Some of us are core Envoy contributors at Katanemo.&lt;/p&gt; &lt;p&gt;Brightstaff, a lightweight controller and state machine written in Rust. It inspects prompts and conversation state, decides which agents to call and in what order, and coordinates routing and fallback. It uses small LLMs (1‚Äì4B parameters) trained for constrained routing and orchestration. These models do not generate responses and fall back to static policies on failure. The models are open sourced here: &lt;a href="https://huggingface.co/katanemo"&gt;https://huggingface.co/katanemo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hn2gtphwt8cg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q7xej4/i_built_plano_a_frameworkfriendly_data_plane_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q7xej4/i_built_plano_a_frameworkfriendly_data_plane_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T03:32:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1q81rvx</id>
    <title>Make an AI continue mid-sentence?</title>
    <updated>2026-01-09T07:18:41+00:00</updated>
    <author>
      <name>/u/poobumfartwee</name>
      <uri>https://old.reddit.com/user/poobumfartwee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know a little how AI works, it just predicts the next word in a sentence. However, when I ask ollama `1 + 1 = ` then it answers `Yes, 1 + 1 is 2`.&lt;/p&gt; &lt;p&gt;How do I make it simply continue a sentence of my choosing as if it was the one that said it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/poobumfartwee"&gt; /u/poobumfartwee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q81rvx/make_an_ai_continue_midsentence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q81rvx/make_an_ai_continue_midsentence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q81rvx/make_an_ai_continue_midsentence/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T07:18:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1q88mrg</id>
    <title>Trying to get mistral-small running on arch linux</title>
    <updated>2026-01-09T13:45:28+00:00</updated>
    <author>
      <name>/u/keldrin_</name>
      <uri>https://old.reddit.com/user/keldrin_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I am currently trying to get mistral-small running on my PC.&lt;/p&gt; &lt;p&gt;Hardware: CPU: AMD Ryzen 5 4600G, GPU: Nvidia GeForce RTX 4060&lt;/p&gt; &lt;p&gt;I have arch linux installed and the desktop running on the internal AMD Graphics card, the nvidia-dkms drivers are installed and ollama-cuda. The ollama server is running (via systemd) and as user i already downloaded the mistral-small llm.&lt;/p&gt; &lt;p&gt;Now, when I run &lt;code&gt;ollama run mistral-small&lt;/code&gt; i can see in nvtop that GPU memory jumps up to around 75% as expected and after a couple of seconds I get my ollama prompt &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;But then, things don't run like I think they should be. I enter my message (&amp;quot;Hello, who are you?&amp;quot;) and then I wait... quite some time.&lt;/p&gt; &lt;p&gt;In nvtop I see CPU usage going up to 80-120% (for the ollama process), GPU is stuck at 0%. Sometimes it also says N/A. Every 10-20 seconds it spits out 4-6 letters and I see a very little spike in GPU usage (maybe 5% for a split second)&lt;/p&gt; &lt;p&gt;Something is clearly going wrong but I don't even know where to start troubleshooting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keldrin_"&gt; /u/keldrin_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q88mrg/trying_to_get_mistralsmall_running_on_arch_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q88mrg/trying_to_get_mistralsmall_running_on_arch_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q88mrg/trying_to_get_mistralsmall_running_on_arch_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T13:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8bz5a</id>
    <title>I learnt about LLM Evals the hard way ‚Äì here's what actually matters</title>
    <updated>2026-01-09T15:56:19+00:00</updated>
    <author>
      <name>/u/sunglasses-guy</name>
      <uri>https://old.reddit.com/user/sunglasses-guy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunglasses-guy"&gt; /u/sunglasses-guy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AIEval/comments/1q8buxb/i_learnt_about_llm_evals_the_hard_way_heres_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q8bz5a/i_learnt_about_llm_evals_the_hard_way_heres_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q8bz5a/i_learnt_about_llm_evals_the_hard_way_heres_what/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T15:56:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q81go3</id>
    <title>Fine-tune SLMs 2x faster, with TuneKit!</title>
    <updated>2026-01-09T07:00:11+00:00</updated>
    <author>
      <name>/u/Consistent_One7493</name>
      <uri>https://old.reddit.com/user/Consistent_One7493</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q81go3/finetune_slms_2x_faster_with_tunekit/"&gt; &lt;img alt="Fine-tune SLMs 2x faster, with TuneKit!" src="https://external-preview.redd.it/am5iNndqbDZ2OWNnMbT87PqvDogJwy2Ycy4k1LQ7g2Ze7paTRMjPgpbX3pAb.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99bb2501df81b364dce9e80f8492ae8f18d19e39" title="Fine-tune SLMs 2x faster, with TuneKit!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Fine-tuning SLMs the way I wish it worked!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Same model. Same prompt. Completely different results. That's what fine-tuning does (when you can actually get it running).&lt;/p&gt; &lt;p&gt;I got tired of the setup nightmare. So I built:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TuneKit&lt;/strong&gt;: Upload your data. Get a notebook. Train free on Colab (2x faster with Unsloth AI). &lt;/p&gt; &lt;p&gt;&lt;strong&gt;No GPUs to rent. No scripts to write. No cost. Just results!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Üí GitHub: &lt;a href="https://github.com/riyanshibohra/TuneKit"&gt;&lt;strong&gt;https://github.com/riyanshibohra/TuneKit&lt;/strong&gt;&lt;/a&gt; (please star the repo if you find it interesting!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_One7493"&gt; /u/Consistent_One7493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9586ijk6v9cg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q81go3/finetune_slms_2x_faster_with_tunekit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q81go3/finetune_slms_2x_faster_with_tunekit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T07:00:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8a8uy</id>
    <title>RAGLight Framework Update : Reranking, Memory, VLM PDF Parser &amp; More!</title>
    <updated>2026-01-09T14:50:41+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Quick update on &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;RAGLight&lt;/a&gt;, my framework for building RAG pipelines in a few lines of code. Try it easily using your favorite Ollama model üéâ&lt;/p&gt; &lt;h1&gt;Better Reranking&lt;/h1&gt; &lt;p&gt;Classic RAG now retrieves more docs and reranks them for higher-quality answers.&lt;/p&gt; &lt;h1&gt;Memory Support&lt;/h1&gt; &lt;p&gt;RAG now includes memory for multi-turn conversations.&lt;/p&gt; &lt;h1&gt;New PDF Parser (with VLM)&lt;/h1&gt; &lt;p&gt;A new PDF parser based on a vision-language model can extract content from images, diagrams, and charts inside PDFs.&lt;/p&gt; &lt;h1&gt;Agentic RAG Refactor&lt;/h1&gt; &lt;p&gt;Agentic RAG has been rewritten using &lt;strong&gt;LangChain&lt;/strong&gt; for better tools, compatibility, and reliability.&lt;/p&gt; &lt;h1&gt;Dependency Updates&lt;/h1&gt; &lt;p&gt;All dependencies refreshed to fix vulnerabilities and improve stability.&lt;/p&gt; &lt;p&gt;üëâ Repo: &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;https://github.com/Bessouat40/RAGLight&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üëâ Documentation : &lt;a href="https://raglight.mintlify.app/"&gt;https://raglight.mintlify.app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to get feedback or questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q8a8uy/raglight_framework_update_reranking_memory_vlm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q8a8uy/raglight_framework_update_reranking_memory_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q8a8uy/raglight_framework_update_reranking_memory_vlm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T14:50:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1q84177</id>
    <title>Create specialized Ollama models in 30 seconds</title>
    <updated>2026-01-09T09:39:13+00:00</updated>
    <author>
      <name>/u/ComfyTightwad</name>
      <uri>https://old.reddit.com/user/ComfyTightwad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q84177/create_specialized_ollama_models_in_30_seconds/"&gt; &lt;img alt="Create specialized Ollama models in 30 seconds" src="https://external-preview.redd.it/ZTc4MXF5aGJuYWNnMS7UCVRbAZLkQIWiFFfUkEkFXIGpC2B-Hyedu35-gMRt.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8033c7563f557c495097d3cb70ed2e912bbf268d" title="Create specialized Ollama models in 30 seconds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just released a new update for OllaMan(Ollama Manager), and it includes a Model Factory to make local agent creation effortless.&lt;/p&gt; &lt;p&gt;Pick a base model (Llama 3, Mistral, etc.).&lt;/p&gt; &lt;p&gt;Set your System Prompt (or use one of the built-in presets).&lt;/p&gt; &lt;p&gt;Tweak Parameters visually (Temp, TopP, TopK).&lt;/p&gt; &lt;p&gt;Click Create.&lt;/p&gt; &lt;p&gt;Boom. You have a custom, specialized model ready to use throughout the app (and via the Ollama CLI).&lt;/p&gt; &lt;p&gt;It's Free and runs locally on your machine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComfyTightwad"&gt; /u/ComfyTightwad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rqpjr0gbnacg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q84177/create_specialized_ollama_models_in_30_seconds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q84177/create_specialized_ollama_models_in_30_seconds/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T09:39:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8zuli</id>
    <title>Method to run 30B Parameter Model</title>
    <updated>2026-01-10T09:29:21+00:00</updated>
    <author>
      <name>/u/Constant_Record_9691</name>
      <uri>https://old.reddit.com/user/Constant_Record_9691</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a decent laptop (3050ti) but nowhere near enough VRAM to runt the model I have in mind. Any free online options?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Constant_Record_9691"&gt; /u/Constant_Record_9691 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q8zuli/method_to_run_30b_parameter_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q8zuli/method_to_run_30b_parameter_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q8zuli/method_to_run_30b_parameter_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T09:29:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8li8b</id>
    <title>I built an Ollama LLM client for Mac OS9. Because why not.</title>
    <updated>2026-01-09T21:52:06+00:00</updated>
    <author>
      <name>/u/MishyJari</name>
      <uri>https://old.reddit.com/user/MishyJari</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q8li8b/i_built_an_ollama_llm_client_for_mac_os9_because/"&gt; &lt;img alt="I built an Ollama LLM client for Mac OS9. Because why not." src="https://external-preview.redd.it/9y2VEONxbCzpJuXVrahQ402wEWsxlCGaL_u9A1Jv6-0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea922d28cb55f27614a0b2f9d3c73af1344cefd9" title="I built an Ollama LLM client for Mac OS9. Because why not." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MishyJari"&gt; /u/MishyJari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bq29memb5ecg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q8li8b/i_built_an_ollama_llm_client_for_mac_os9_because/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q8li8b/i_built_an_ollama_llm_client_for_mac_os9_because/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T21:52:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9bdj0</id>
    <title>STT and TTS compatible with ROCm</title>
    <updated>2026-01-10T18:22:15+00:00</updated>
    <author>
      <name>/u/EnvironmentalToe3130</name>
      <uri>https://old.reddit.com/user/EnvironmentalToe3130</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnvironmentalToe3130"&gt; /u/EnvironmentalToe3130 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1q9bd1v/stt_and_tts_compatible_with_rocm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9bdj0/stt_and_tts_compatible_with_rocm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q9bdj0/stt_and_tts_compatible_with_rocm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T18:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q99hsz</id>
    <title>Nvidia Quadro P400 2GB GDDR5 card good enough?</title>
    <updated>2026-01-10T17:09:37+00:00</updated>
    <author>
      <name>/u/Curious_Party_4683</name>
      <uri>https://old.reddit.com/user/Curious_Party_4683</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q99hsz/nvidia_quadro_p400_2gb_gddr5_card_good_enough/"&gt; &lt;img alt="Nvidia Quadro P400 2GB GDDR5 card good enough?" src="https://b.thumbs.redditmedia.com/Ir0lGRSqX5OLg3aYUy2LQyL0XiK7SGfDKqBhqb0hngM.jpg" title="Nvidia Quadro P400 2GB GDDR5 card good enough?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;qwen3-vl:8b refuses to work on my i7, 7th gen, windows machine. &lt;/p&gt; &lt;p&gt;will this cheap nvidia work? or what's the bare minimum card?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lj263g5g0kcg1.png?width=1639&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ba459647259dba77ccefd933da88fdfc646e3fa"&gt;https://preview.redd.it/lj263g5g0kcg1.png?width=1639&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ba459647259dba77ccefd933da88fdfc646e3fa&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Curious_Party_4683"&gt; /u/Curious_Party_4683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q99hsz/nvidia_quadro_p400_2gb_gddr5_card_good_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q99hsz/nvidia_quadro_p400_2gb_gddr5_card_good_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q99hsz/nvidia_quadro_p400_2gb_gddr5_card_good_enough/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T17:09:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9ff6k</id>
    <title>what AI models can I run locally on my PC with Ollama?</title>
    <updated>2026-01-10T20:57:13+00:00</updated>
    <author>
      <name>/u/Kitchen-Patience8176</name>
      <uri>https://old.reddit.com/user/Kitchen-Patience8176</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I‚Äôm pretty new to local AI and still learning, so sorry if this is a basic question.&lt;/p&gt; &lt;p&gt;I can‚Äôt afford a ChatGPT subscription anymore due to financial reasons, so I‚Äôm trying to use &lt;strong&gt;local models&lt;/strong&gt; instead. I‚Äôve installed &lt;strong&gt;Ollama&lt;/strong&gt;, and it works, but I don‚Äôt really know which models I should be using or what my PC can realistically handle.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ryzen 9 5900X&lt;/li&gt; &lt;li&gt;RTX 3080 (10GB VRAM)&lt;/li&gt; &lt;li&gt;32GB RAM&lt;/li&gt; &lt;li&gt;2TB NVMe SSD&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm mainly curious about:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Which models run well on this setup&lt;/li&gt; &lt;li&gt;What I &lt;em&gt;can‚Äôt&lt;/em&gt; run&lt;/li&gt; &lt;li&gt;How close local models can get to ChatGPT&lt;/li&gt; &lt;li&gt;If things like web search, fact-checking, or up-to-date info are possible locally (or any workarounds)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any beginner advice or model recommendations would really help.&lt;/p&gt; &lt;p&gt;Thanks üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kitchen-Patience8176"&gt; /u/Kitchen-Patience8176 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9ff6k/what_ai_models_can_i_run_locally_on_my_pc_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9ff6k/what_ai_models_can_i_run_locally_on_my_pc_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q9ff6k/what_ai_models_can_i_run_locally_on_my_pc_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T20:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9jdo8</id>
    <title>Is it possible to see AI Request and Response in Realtime on Llama</title>
    <updated>2026-01-10T23:36:56+00:00</updated>
    <author>
      <name>/u/Professional-Fun7765</name>
      <uri>https://old.reddit.com/user/Professional-Fun7765</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone&lt;/p&gt; &lt;p&gt;I am new in the world of Ollama so pardon me if this may sound like a stupid question. I am transitioning from GPT4All where when I make a request via API I can see in real time On the desktop app (on the server chat tab) The incoming request, the model thinking and the model generating a response. This was so great in debugging but GPT4All was slow for me so a colleague suggested Ollama and I can see much improvements in speed. I am currently integrating a Laravel App with Ollama and sending various request to the model and I wish I can be able to see the request and response in real time in Ollama just like I did in GPT4All Desktop App, so my question is whether or not this is possible? If it is then how can I go about configuring it?&lt;/p&gt; &lt;p&gt;if it helps I am on Windows and this is for my local development.&lt;/p&gt; &lt;p&gt;Thank you in advance, your input will be highly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Fun7765"&gt; /u/Professional-Fun7765 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9jdo8/is_it_possible_to_see_ai_request_and_response_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9jdo8/is_it_possible_to_see_ai_request_and_response_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q9jdo8/is_it_possible_to_see_ai_request_and_response_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T23:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q95z91</id>
    <title>Built a Local Research Agent with Ollama - No API Keys, Just Citations</title>
    <updated>2026-01-10T14:51:06+00:00</updated>
    <author>
      <name>/u/Xthebuilder</name>
      <uri>https://old.reddit.com/user/Xthebuilder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q95z91/built_a_local_research_agent_with_ollama_no_api/"&gt; &lt;img alt="Built a Local Research Agent with Ollama - No API Keys, Just Citations" src="https://b.thumbs.redditmedia.com/f-UABymhEJedVxkN3owagmqQGsW0QyXoIv6GlbP6OBc.jpg" title="Built a Local Research Agent with Ollama - No API Keys, Just Citations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a research agent that runs entirely locally using Ollama. Give it a topic, get back a markdown report with proper citations. Simple as that.&lt;/p&gt; &lt;p&gt;What It Does&lt;/p&gt; &lt;p&gt;The agent handles the full research workflow:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô Gathers sources asynchronously ‚àô Uses semantic embeddings to filter for relevance ‚àô Generates structured reports with citations ‚àô Everything stays on your machine &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Why I Built This&lt;/p&gt; &lt;p&gt;I wanted deep research capabilities without depending on cloud services or burning through API credits. With Ollama making local LLMs practical, it seemed like the obvious foundation.&lt;/p&gt; &lt;p&gt;How It Works&lt;/p&gt; &lt;p&gt;python research_agent.py &amp;quot;quantum computing applications&amp;quot;&lt;/p&gt; &lt;p&gt;The agent:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. Pulls sources from DuckDuckGo 2. Extracts and evaluates content using sentence-transformers 3. Runs quality checks on similarity scores 4. Generates a markdown report with references &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;All processing happens locally. No external APIs.&lt;/p&gt; &lt;p&gt;Design Choices (Explicit By Design)&lt;/p&gt; &lt;p&gt;Local-first: Works with any Ollama model - llama2, mistral, whatever you have running&lt;/p&gt; &lt;p&gt;Quality thresholds: Configurable similarity scores ensure sources are actually relevant&lt;/p&gt; &lt;p&gt;Async operations: Fast source gathering without blocking&lt;/p&gt; &lt;p&gt;Structured output: Clean markdown reports you can actually use&lt;/p&gt; &lt;p&gt;Tradeoffs&lt;/p&gt; &lt;p&gt;I optimized for:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô Privacy and offline workflows ‚àô Explicit configuration over automation ‚àô Simple setup (just Python + Ollama) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This means it‚Äôs not:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô A cloud-scale solution ‚àô Zero-configuration ‚àô Designed for multi-source integrations (yet) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What‚Äôs Next&lt;/p&gt; &lt;p&gt;Considering:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô PDF source support improvements ‚àô Local caching to avoid re-fetching ‚àô Better semantic chunking for long sources &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Code‚Äôs on GitHub: &lt;a href="https://github.com/Xthebuilder/Research_Agent"&gt;https://github.com/Xthebuilder/Research_Agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xthebuilder"&gt; /u/Xthebuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q95z91"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q95z91/built_a_local_research_agent_with_ollama_no_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q95z91/built_a_local_research_agent_with_ollama_no_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T14:51:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9svp5</id>
    <title>Looking for open source contributers | LocalAgent</title>
    <updated>2026-01-11T07:11:25+00:00</updated>
    <author>
      <name>/u/FriendshipCreepy8045</name>
      <uri>https://old.reddit.com/user/FriendshipCreepy8045</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;br /&gt; Hope you're all doing well.&lt;/p&gt; &lt;p&gt;So little background: I'm a frontend/performance engineer working as an IT consultant for the past year or so.&lt;br /&gt; Recently made a goal to learn and code more in python and basically entering the field of AI Applied engineering.&lt;br /&gt; I'm still learning concepts but with a little knowledge and claude, I made a researcher assistent that runs entirly on laptop(if you have a descent one using Ollama) or just use the default cloud.&lt;/p&gt; &lt;p&gt;I understand langchain quite a bit and might be worth checking out langraph to somehow migrate it into more controlled research assistent(controlling tools,tokens used etc.).&lt;br /&gt; So I need your help, I would really appretiate if you guys go ahead and check &amp;quot;&lt;a href="https://github.com/vedas-dixit/LocalAgent"&gt;https://github.com/vedas-dixit/LocalAgent&lt;/a&gt;&amp;quot; and let me know:&lt;/p&gt; &lt;p&gt;Your thoughts | Potential Improvements | Guidance *what i did right/wrong&lt;/p&gt; &lt;p&gt;or if i may ask, just some meaningful contribution to the project if you have time ;).&lt;/p&gt; &lt;p&gt;I posted about this like idk a month ago and got 100+ stars in a week so might have some potential but idk.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FriendshipCreepy8045"&gt; /u/FriendshipCreepy8045 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9svp5/looking_for_open_source_contributers_localagent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9svp5/looking_for_open_source_contributers_localagent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q9svp5/looking_for_open_source_contributers_localagent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-11T07:11:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qansvc</id>
    <title>9 requests per hour - Seriously?</title>
    <updated>2026-01-12T06:40:12+00:00</updated>
    <author>
      <name>/u/MadeUpName94</name>
      <uri>https://old.reddit.com/user/MadeUpName94</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying ollama on the free plan with a cloud LLM and the more i use it the less i can.&lt;/p&gt; &lt;p&gt;9 requests and I got the &amp;quot;too many request per hour, give us money* - fucking 9?&lt;/p&gt; &lt;p&gt;I hadn't used it for several hours.&lt;/p&gt; &lt;p&gt;My &amp;quot;requests per hour&amp;quot; gets smaller and smaller every time use it :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadeUpName94"&gt; /u/MadeUpName94 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qansvc/9_requests_per_hour_seriously/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qansvc/9_requests_per_hour_seriously/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qansvc/9_requests_per_hour_seriously/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-12T06:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qabwuw</id>
    <title>Docker ollama running on windows using system RAM, despite using VRAM and having plenty more available.</title>
    <updated>2026-01-11T21:39:05+00:00</updated>
    <author>
      <name>/u/Fit_Code_2107</name>
      <uri>https://old.reddit.com/user/Fit_Code_2107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Edit: Found the solution to this problem (tldr; WSL2 sucks), updated the post with the solution at the bottom of the post&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm trying to run ollama on docker (windows), and it looks like there's some memory double dipping going on and I'm not sure why. I'm trying to run a 20GB model on a 5090, I'm seeing BOTH my system and VRAM memory go up as much when I load the model.&lt;/p&gt; &lt;p&gt;System settings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;64 GB of RAM&lt;/li&gt; &lt;li&gt;RTX 5090 (32 GB of VRAM)&lt;/li&gt; &lt;li&gt;Model: olmo-3.1:32b-think (takes ~20Gb of RAM to load)&lt;/li&gt; &lt;li&gt;Docker version 29.1.3, build f52814d (running on WSL2)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;fwiw, &lt;code&gt;ollama ps&lt;/code&gt; does show the model loaded 100% on my GPU. Ran &lt;code&gt;nvidia-smi&lt;/code&gt; in the ollama container, and it looks &lt;em&gt;fine&lt;/em&gt; (I can see the ollama process running). While Windows task manager isn't able to pin down what process is responsible for the high gpu util, it does reflect memory utilization accurately. So I am using my GPU, I have plenty more VRAM to work with, so I'm not at all sure why system memory util spikes up 20GB during use.&lt;/p&gt; &lt;p&gt;I installed the windows native version of ollama to see if I could replicate, and I do not see my system memory spike using that approach. So it seems like the involvement of docker here is introducing some funk.&lt;/p&gt; &lt;p&gt;I've read through some similar posts here and saw there were issues a few years ago with docker on WSL2 and utilizing VRAM, but those issues seem to have since been resolved so hitting a dead end here. Wondering if anyone has had the same issue and has any tips?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;h1&gt;Solution/workaround&lt;/h1&gt; &lt;p&gt;Found the cause of the issue, it's a known &lt;a href="https://github.com/microsoft/WSL/issues/4166"&gt;WSL issue&lt;/a&gt; (that's been open for 7 years...). Apparently WSL doesn't do a great job with memory management and sometimes never releases the memory used back to the system (I say sometimes but for me it's ALL the time). In this case with ollama, never releases the RAM it uses to load the model to VRAM.&lt;/p&gt; &lt;p&gt;You can manually confirm this, and release the memory with the following commands (s/o to Kirk, a random commenter on this &lt;a href="https://medium.com/@Tanzim/how-to-run-ollama-in-windows-via-wsl-8ace765cee12"&gt;medium article I found&lt;/a&gt;)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Display memory usage free -h sudo su &amp;lt;&amp;lt;EOF # force memory write back to disk sync # clear cache from memory echo 3 &amp;gt; /proc/sys/vm/drop_caches EOF # display memory again (should be cleared) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also, looks like someone has a more automated solution out here too (haven't tried this so can't vouch but looks promising) : &lt;a href="https://github.com/arkane-systems/wsl-drop-cache?tab=readme-ov-file"&gt;https://github.com/arkane-systems/wsl-drop-cache?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: I am only sharing what I found.&lt;/p&gt; &lt;p&gt;While these are solutions, they may not be the best or most appropriate solution if you're doing anything &lt;em&gt;real&lt;/em&gt;. atm I'm just experimenting with this setup for personal reasons so it works for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit_Code_2107"&gt; /u/Fit_Code_2107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qabwuw/docker_ollama_running_on_windows_using_system_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qabwuw/docker_ollama_running_on_windows_using_system_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qabwuw/docker_ollama_running_on_windows_using_system_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-11T21:39:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qar8dn</id>
    <title>Which model to translate my blog posts, without sounding AI?</title>
    <updated>2026-01-12T10:15:47+00:00</updated>
    <author>
      <name>/u/Signal_Pin_3277</name>
      <uri>https://old.reddit.com/user/Signal_Pin_3277</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been writing blog posts in english, but I would like to translate them to french. I know I can just throw them at GPT, but it just ruins the tone, the sentences are very weird, I don't want a literal translation of the words, rather a natural translation with maybe french expressions.&lt;/p&gt; &lt;p&gt;I wonder, is there any model I could use? I tried Deepseek, GPT.&lt;/p&gt; &lt;p&gt;I don't mind a local model too, I have a 16 GB rtx 5060. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Signal_Pin_3277"&gt; /u/Signal_Pin_3277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qar8dn/which_model_to_translate_my_blog_posts_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qar8dn/which_model_to_translate_my_blog_posts_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qar8dn/which_model_to_translate_my_blog_posts_without/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-12T10:15:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb46gv</id>
    <title>How to Evaluate AI Agents? (Part 2)</title>
    <updated>2026-01-12T19:14:13+00:00</updated>
    <author>
      <name>/u/Ok_Constant_9886</name>
      <uri>https://old.reddit.com/user/Ok_Constant_9886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qb46gv/how_to_evaluate_ai_agents_part_2/"&gt; &lt;img alt="How to Evaluate AI Agents? (Part 2)" src="https://b.thumbs.redditmedia.com/vqEYTMRHAAFPd_bbFtc_RJ6KQ8SiOUccMhIa1YRo8Kg.jpg" title="How to Evaluate AI Agents? (Part 2)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Constant_9886"&gt; /u/Ok_Constant_9886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AIEval/comments/1qb43wg/how_to_evaluate_ai_agents_part_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qb46gv/how_to_evaluate_ai_agents_part_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qb46gv/how_to_evaluate_ai_agents_part_2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-12T19:14:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb06tm</id>
    <title>Docker on Linux or Nah?</title>
    <updated>2026-01-12T16:53:05+00:00</updated>
    <author>
      <name>/u/Honest-Cheesecake275</name>
      <uri>https://old.reddit.com/user/Honest-Cheesecake275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My ADHD impulses got the better of me and I jumped the gun and installed Ollama locally. Then installed the Docker container then saw that there is a Docker container that streamlines setup of WebUI. &lt;/p&gt; &lt;p&gt;What‚Äôs the most idiot proof way to set this up?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Cheesecake275"&gt; /u/Honest-Cheesecake275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qb06tm/docker_on_linux_or_nah/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qb06tm/docker_on_linux_or_nah/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qb06tm/docker_on_linux_or_nah/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-12T16:53:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbm9f8</id>
    <title>Introducing T.H.U.V.U, an open source coding agent for local and cloud LLMs</title>
    <updated>2026-01-13T08:43:15+00:00</updated>
    <author>
      <name>/u/Strange-Flamingo-248</name>
      <uri>https://old.reddit.com/user/Strange-Flamingo-248</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Strange-Flamingo-248"&gt; /u/Strange-Flamingo-248 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/CodingAgents/comments/1qbllft/introducing_thuvu_an_open_source_coding_agent_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbm9f8/introducing_thuvu_an_open_source_coding_agent_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qbm9f8/introducing_thuvu_an_open_source_coding_agent_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-13T08:43:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb4jaq</id>
    <title>Chat With Your Favorite GitHub Repositories via CLI with the new RAGLight Feature</title>
    <updated>2026-01-12T19:27:02+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qb4jaq/chat_with_your_favorite_github_repositories_via/"&gt; &lt;img alt="Chat With Your Favorite GitHub Repositories via CLI with the new RAGLight Feature" src="https://external-preview.redd.it/emxuM2h3d3R5eWNnMVATT2HSzAMyFYCPqt4__cDa5qVULAiZlHb92ktmmK-c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7893a18ffbbe36345ddb3b5ea5e3be2d2ad9c1a7" title="Chat With Your Favorite GitHub Repositories via CLI with the new RAGLight Feature" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve just pushed a new feature to &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;RAGLight&lt;/a&gt;: you can now &lt;strong&gt;chat directly with your favorite GitHub repositories from the CLI&lt;/strong&gt; using your favorite models.&lt;/p&gt; &lt;p&gt;No setup nightmare, no complex infra, just point to one or several GitHub repos, let RAGLight ingest them, and start asking questions !&lt;/p&gt; &lt;p&gt;In the demo I used an &lt;strong&gt;Ollama&lt;/strong&gt; embedding model and an &lt;strong&gt;OpenAI&lt;/strong&gt; LLM, let's try it with your favorite model provider üöÄ&lt;/p&gt; &lt;p&gt;You can also use &lt;strong&gt;RAGLight&lt;/strong&gt; in your codebase if you want to setup easily a RAG.&lt;/p&gt; &lt;p&gt;Github repository : &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;https://github.com/Bessouat40/RAGLight&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2lu95uwtyycg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qb4jaq/chat_with_your_favorite_github_repositories_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qb4jaq/chat_with_your_favorite_github_repositories_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-12T19:27:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbk8l4</id>
    <title>Privacy-First Voice Assistant with AI web-enabled search</title>
    <updated>2026-01-13T06:37:21+00:00</updated>
    <author>
      <name>/u/dsept</name>
      <uri>https://old.reddit.com/user/dsept</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qbk8l4/privacyfirst_voice_assistant_with_ai_webenabled/"&gt; &lt;img alt="Privacy-First Voice Assistant with AI web-enabled search" src="https://preview.redd.it/gt9geaosa2dg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3decc717b0d67d9619de634dc5e356db36c40897" title="Privacy-First Voice Assistant with AI web-enabled search" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dsept"&gt; /u/dsept &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gt9geaosa2dg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbk8l4/privacyfirst_voice_assistant_with_ai_webenabled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qbk8l4/privacyfirst_voice_assistant_with_ai_webenabled/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-13T06:37:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbwnjf</id>
    <title>Seeking Advice: Deploying Local LLMs for a Large-Scale Food &amp; Goods Distributor</title>
    <updated>2026-01-13T17:04:14+00:00</updated>
    <author>
      <name>/u/JPedrroo</name>
      <uri>https://old.reddit.com/user/JPedrroo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I‚Äôm a Software Analyst and Developer for a major distribution company in the state of Bahia, Brazil. We handle a massive operation ranging from food and beverages to cosmetics and hygiene products, serving basically the entire state in terms of city coverage.&lt;/p&gt; &lt;p&gt;I am currently exploring the possibility of implementing a local AI infrastructure to enhance productivity while maintaining strict privacy over our data. I am not an expert in AI, so I am still figuring out the best way to start. I have tested some local LLMs on my laptop, but I am unfamiliar with the technical nuances involved in a large-scale corporate implementation.&lt;/p&gt; &lt;p&gt;Initially, I thought of developing a system that reads database entries regarding expiry dates and turnover rates in our warehouse. The goal would be to automatically recommend flash promotions or stock transfers to our retail branches before products expire.&lt;/p&gt; &lt;p&gt;I'm seeking any feedback on this‚Äîpast experiences, technical advice, additional use case ideas, or anything relevant. Thank you all for your time and for any insights you can share!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JPedrroo"&gt; /u/JPedrroo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbwnjf/seeking_advice_deploying_local_llms_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbwnjf/seeking_advice_deploying_local_llms_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qbwnjf/seeking_advice_deploying_local_llms_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-13T17:04:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbrx4b</id>
    <title>Open Source Enterprise Search Engine (Generative AI Powered)</title>
    <updated>2026-01-13T13:55:10+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I‚Äôm excited to share something we‚Äôve been building for the past 6 months, a &lt;strong&gt;fully open-source Enterprise Search Platform&lt;/strong&gt; designed to bring powerful Enterprise Search to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, Local file uploads and more. You can deploy it and run it with just one docker compose command.&lt;/p&gt; &lt;p&gt;You can run the full platform locally. Recently, one of our users tried &lt;strong&gt;qwen3-vl:8b (16 FP)&lt;/strong&gt; with &lt;strong&gt;Ollama&lt;/strong&gt; and got very good results.&lt;/p&gt; &lt;p&gt;The entire system is built on a &lt;strong&gt;fully event-streaming architecture powered by Kafka&lt;/strong&gt;, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data.&lt;/p&gt; &lt;p&gt;At the core, the system uses an &lt;strong&gt;Agentic Graph RAG approach&lt;/strong&gt;, where retrieval is guided by an enterprise knowledge graph and reasoning agents. Instead of treating documents as flat text, agents reason over relationships between users, teams, entities, documents, and permissions, allowing more accurate, explainable, and permission-aware answers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep understanding of user, organization and teams with enterprise knowledge graph&lt;/li&gt; &lt;li&gt;Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama&lt;/li&gt; &lt;li&gt;Use any provider that supports OpenAI compatible endpoints&lt;/li&gt; &lt;li&gt;Choose from 1,000+ embedding models&lt;/li&gt; &lt;li&gt;Visual Citations for every answer&lt;/li&gt; &lt;li&gt;Vision-Language Models and OCR for visual or scanned docs&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, or SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs for developers&lt;/li&gt; &lt;li&gt;All major file types support including pdfs with images, diagrams and charts&lt;/li&gt; &lt;li&gt;Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more&lt;/li&gt; &lt;li&gt;Reasoning Agent that plans before executing tasks&lt;/li&gt; &lt;li&gt;40+ Connectors allowing you to connect to your entire business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo Video:&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=xA9m3pwOgz8"&gt;https://www.youtube.com/watch?v=xA9m3pwOgz8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbrx4b/open_source_enterprise_search_engine_generative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qbrx4b/open_source_enterprise_search_engine_generative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qbrx4b/open_source_enterprise_search_engine_generative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-13T13:55:10+00:00</published>
  </entry>
</feed>
