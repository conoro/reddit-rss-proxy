<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-12T16:39:43+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1lvfej4</id>
    <title>ngrok for AI models - Serve Ollama models with a cloud API using Local Runners</title>
    <updated>2025-07-09T10:33:33+00:00</updated>
    <author>
      <name>/u/Sumanth_077</name>
      <uri>https://old.reddit.com/user/Sumanth_077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, we‚Äôve built ngrok for AI models ‚Äî and it works seamlessly with Ollama.&lt;/p&gt; &lt;p&gt;We built Local Runners to let you serve AI models, MCP servers, or agents directly from your own machine and expose them through a secure Clarifai endpoint. No need to spin up a web server, manage routing, or deploy to the cloud. Just run the model locally and get a working API endpoint instantly.&lt;/p&gt; &lt;p&gt;If you're running open-source models with Ollama, Local Runners let you keep compute and data local while still connecting to agent frameworks, APIs, or workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Run ‚Äì Start a local runner pointing to your model&lt;br /&gt; Tunnel ‚Äì It opens a secure connection to a hosted API endpoint&lt;br /&gt; Requests ‚Äì API calls are routed to your machine&lt;br /&gt; Response ‚Äì Your model processes them locally and returns the result&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this helps:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Skip building a server or deploying just to test a model&lt;/li&gt; &lt;li&gt;Wire local models into LangGraph, CrewAI, or custom agent loops&lt;/li&gt; &lt;li&gt;Access local files, private tools, or data sources from your model&lt;/li&gt; &lt;li&gt;Use your existing hardware for inference, especially for token hungry models and agents, reducing cloud costs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôve put together a short tutorial that shows how you can expose local models, MCP servers, tools, and agents securely using Local Runners, without deploying anything to the cloud.&lt;br /&gt; &lt;a href="https://youtu.be/JOdtZDmCFfk"&gt;https://youtu.be/JOdtZDmCFfk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear how you're running Ollama models or building agent workflows around them. Fire away in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sumanth_077"&gt; /u/Sumanth_077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvfej4/ngrok_for_ai_models_serve_ollama_models_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvfej4/ngrok_for_ai_models_serve_ollama_models_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lvfej4/ngrok_for_ai_models_serve_ollama_models_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T10:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lutct9</id>
    <title>My little tribute to Ollama</title>
    <updated>2025-07-08T16:37:18+00:00</updated>
    <author>
      <name>/u/valdecircarvalho</name>
      <uri>https://old.reddit.com/user/valdecircarvalho</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lutct9/my_little_tribute_to_ollama/"&gt; &lt;img alt="My little tribute to Ollama" src="https://preview.redd.it/5n0izf2mhobf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9e966c0d12b9995e3120a8ec5a0782e30b0f651" title="My little tribute to Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdecircarvalho"&gt; /u/valdecircarvalho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5n0izf2mhobf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lutct9/my_little_tribute_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lutct9/my_little_tribute_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T16:37:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv672x</id>
    <title>I used Ollama to build a Cursor for PDFs</title>
    <updated>2025-07-09T01:25:50+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lv672x/i_used_ollama_to_build_a_cursor_for_pdfs/"&gt; &lt;img alt="I used Ollama to build a Cursor for PDFs" src="https://external-preview.redd.it/eW1tYWxxOGYycmJmMXdyH2g3gh74ax-OLM0Bn_sh-0xXIf9gZjZs7gXuAEvK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e52e99d0c98b4bb58943f1138c44430623ddf356" title="I used Ollama to build a Cursor for PDFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like using Cursor while coding, but there are a lot of other tasks outside of code that would also benefit from having an agent on the side - things like reading through long documents and filling out forms. &lt;/p&gt; &lt;p&gt;So, as a fun experiment, I built an agent with search with a PDF viewer on the side. I've found it to be super helpful - and I'd love feedback on where you'd like to see this go!&lt;/p&gt; &lt;p&gt;If you'd like to try it out: &lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="http://github.com/morphik-org/morphik-core"&gt;github.com/morphik-org/morphik-core&lt;/a&gt;&lt;br /&gt; Website: &lt;a href="http://morphik.ai"&gt;morphik.ai&lt;/a&gt; (Look for the PDF Viewer section!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3jj1br8f2rbf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lv672x/i_used_ollama_to_build_a_cursor_for_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lv672x/i_used_ollama_to_build_a_cursor_for_pdfs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T01:25:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvqcfw</id>
    <title>Thoughts on grabbing a 5060 Ti 16G as a noob?</title>
    <updated>2025-07-09T18:28:21+00:00</updated>
    <author>
      <name>/u/SKX007J1</name>
      <uri>https://old.reddit.com/user/SKX007J1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For someone wanting to get started with ollama and experiment with self-hosting hosting how does the 5060 Ti 16G stack up for the price point of ¬£390/$500. &lt;/p&gt; &lt;p&gt;What would you get with that sort of budget if your goal was just learning rather than productivity? Any ways to mitigate that they nerfed the bandwidth of the memory? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SKX007J1"&gt; /u/SKX007J1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvqcfw/thoughts_on_grabbing_a_5060_ti_16g_as_a_noob/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvqcfw/thoughts_on_grabbing_a_5060_ti_16g_as_a_noob/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lvqcfw/thoughts_on_grabbing_a_5060_ti_16g_as_a_noob/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T18:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwi70x</id>
    <title>I'm cloud architect and I'm searching of there an LLM that can help me to create technical documentation and solution design for business need.</title>
    <updated>2025-07-10T17:09:12+00:00</updated>
    <author>
      <name>/u/KindheartednessHot90</name>
      <uri>https://old.reddit.com/user/KindheartednessHot90</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KindheartednessHot90"&gt; /u/KindheartednessHot90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwi70x/im_cloud_architect_and_im_searching_of_there_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwi70x/im_cloud_architect_and_im_searching_of_there_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lwi70x/im_cloud_architect_and_im_searching_of_there_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-10T17:09:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw1o6d</id>
    <title>Smollm ? Coding models?</title>
    <updated>2025-07-10T02:39:39+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's a good coding model? Is is there plans for the new smollm3? It would need prompting cues to be built in. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lw1o6d/smollm_coding_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lw1o6d/smollm_coding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lw1o6d/smollm_coding_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-10T02:39:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwg067</id>
    <title>Public and Private local setups: how I have a public facing OpenWebUI and private GPU</title>
    <updated>2025-07-10T15:42:56+00:00</updated>
    <author>
      <name>/u/brulak</name>
      <uri>https://old.reddit.com/user/brulak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Haven't seen too many talk about this, so I figure I'd throw my hat in on this. &lt;/p&gt; &lt;p&gt;I have 2x3090 at home. It runs ubuntu with ollama. I have devstral, llama3.2 etc. &lt;/p&gt; &lt;p&gt;I setup a Digital ocean droplet. &lt;/p&gt; &lt;p&gt;It sits behind a digital ocean firewall and it has the local firewall (ufw) set up as well. &lt;/p&gt; &lt;p&gt;I set up a VPN between the two boxes. OpenWebUi is configured to connect with ollama via the VPN. So, it connects with 10.0.0.1. &lt;/p&gt; &lt;p&gt;When you visit the OpenWebUI server, it shows the models from my GPU rig. &lt;/p&gt; &lt;p&gt;Performance wise: the round trip is a bit slower than you'd want. If i'm at home, I connect directly to the box without the Droplet to eliminate the round trip cost. Then performance is amazing. Espcially with &lt;a href="http://continue.dev"&gt;continue.dev&lt;/a&gt; and devstral or qwen. &lt;/p&gt; &lt;p&gt;If I'm out of the house, either on my laptop or my phone the performance is manageable. &lt;/p&gt; &lt;p&gt;Feel free to ask me anything else I might have missed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brulak"&gt; /u/brulak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwg067/public_and_private_local_setups_how_i_have_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwg067/public_and_private_local_setups_how_i_have_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lwg067/public_and_private_local_setups_how_i_have_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-10T15:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwhuvw</id>
    <title>Index academic papers and extract metadata with LLMs (Ollama Integrated)</title>
    <updated>2025-07-10T16:56:15+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Ollama community, want to share my latest project about academic papers PDF metadata extraction&lt;/p&gt; &lt;ul&gt; &lt;li&gt;extracting metadata (title, authors, abstract)&lt;/li&gt; &lt;li&gt;relationship (which author has which papers) and&lt;/li&gt; &lt;li&gt;embeddings for semantic search&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I don't see any similar comprehensive example published, so would like to share mine. The library has &lt;a href="https://cocoindex.io/docs/ai/llm#ollama"&gt;native Ollama Integration&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Python source code: &lt;a href="https://github.com/cocoindex-io/cocoindex/tree/main/examples/paper_metadata"&gt;https://github.com/cocoindex-io/cocoindex/tree/main/examples/paper_metadata&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full write up: &lt;a href="https://cocoindex.io/blogs/academic-papers-indexing/"&gt;https://cocoindex.io/blogs/academic-papers-indexing/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Appreciate a star on the repo if it is helpful, thanks! And would love to learn your suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwhuvw/index_academic_papers_and_extract_metadata_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwhuvw/index_academic_papers_and_extract_metadata_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lwhuvw/index_academic_papers_and_extract_metadata_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-10T16:56:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx49vx</id>
    <title>100k dollars budget only for equipment. for business for cloud renting.</title>
    <updated>2025-07-11T11:20:22+00:00</updated>
    <author>
      <name>/u/Free_Care_2006</name>
      <uri>https://old.reddit.com/user/Free_Care_2006</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;you have 100k. In what do you invest and why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Free_Care_2006"&gt; /u/Free_Care_2006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lx49vx/100k_dollars_budget_only_for_equipment_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lx49vx/100k_dollars_budget_only_for_equipment_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lx49vx/100k_dollars_budget_only_for_equipment_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T11:20:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwccef</id>
    <title>How do you reduce hallucinations on agents of small models?</title>
    <updated>2025-07-10T13:10:37+00:00</updated>
    <author>
      <name>/u/mynameismati</name>
      <uri>https://old.reddit.com/user/mynameismati</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been reading about different techniques like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAG &lt;/li&gt; &lt;li&gt;Context Engineering &lt;/li&gt; &lt;li&gt;Memory management&lt;/li&gt; &lt;li&gt;Prompt Engineering &lt;/li&gt; &lt;li&gt;Fine-tuning models for your specific case &lt;/li&gt; &lt;li&gt;Reducing context through re-adaptation and use of micro-agents while splitting tasks into smaller ones and having shorter pipelines.&lt;/li&gt; &lt;li&gt;...others&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And as of now what has been most useful for me is reducing context, and be in control of every token for the prompt as well as the token while trying to maintain the most direct way for the agent to go to the tool and do the desired task.&lt;/p&gt; &lt;p&gt;Agents that evaluate prompts, parse the input to a specific format trying to reduce tokens, call the agent that handles certain tasks and evaluate tool choosing by other agent has been also useful but I think I am over-complicating.&lt;/p&gt; &lt;p&gt;What has been your approach? All of these things I do have been with 7b-8b-14b models. I cant go larget as my GPU is 8gb of VRAM and low cost. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mynameismati"&gt; /u/mynameismati &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwccef/how_do_you_reduce_hallucinations_on_agents_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwccef/how_do_you_reduce_hallucinations_on_agents_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lwccef/how_do_you_reduce_hallucinations_on_agents_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-10T13:10:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lww3in</id>
    <title>What kind of performance boost will I see with a modern GPU</title>
    <updated>2025-07-11T03:03:04+00:00</updated>
    <author>
      <name>/u/leathermartini</name>
      <uri>https://old.reddit.com/user/leathermartini</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I set up an Ollama server to let my Home Assistant do some voice control features and possibly stand in for Alexa/Google. Using an old (5 year) gaming/streaming PC (GeForce GTX 1660 Super GPU) to serve it. I've managed to get it mostly functional BUT it is... Not fast. Simple tasks (turn on lights, query the current weather) are handled locally and work fine. Others (play a song, check the forecast, questions it has to parse with the LLM) take 60-240 seconds to process. Checking the logs it looks like each Ollama request takes 60ish seconds.&lt;/p&gt; &lt;p&gt;I'm trying to work out the cost of making this feasible. But I don't have a ton of gaming hardware just sitting around. The cheap options look to be getting a GTX 5060 or so and swapping video cards. Benchmarks say I should see a jump around 140-200% with that. (Next option would be a new machine with a bigger power supply and other options...)&lt;/p&gt; &lt;p&gt;Basically I want to know what benchmark to look at and how to see how it might impact ollama's performance. &lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leathermartini"&gt; /u/leathermartini &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lww3in/what_kind_of_performance_boost_will_i_see_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lww3in/what_kind_of_performance_boost_will_i_see_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lww3in/what_kind_of_performance_boost_will_i_see_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T03:03:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwxnqn</id>
    <title>üöÄ Built a transparent metrics proxy for Ollama - zero config changes needed!</title>
    <updated>2025-07-11T04:25:31+00:00</updated>
    <author>
      <name>/u/firedog7881</name>
      <uri>https://old.reddit.com/user/firedog7881</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just finished this little tool that adds Prometheus monitoring to Ollama without touching your existing client setup. Your apps still connect to localhost:11434 like normal, but now you get detailed metrics and analytics.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; - Intercepts Ollama API calls to collect metrics (latency, tokens/sec, error rates) - Stores detailed analytics (prompts, timings, token counts) - Exposes Prometheus metrics for dashboards - Works with any Ollama client - no code changes needed&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Installation is stupid simple:&lt;/strong&gt; &lt;code&gt;bash git clone https://github.com/bmeyer99/Ollama_Proxy_Wrapper cd Ollama_Proxy_Wrapper quick_install.bat &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Then just use Ollama commands normally:&lt;/strong&gt; &lt;code&gt;bash ollama_metrics.bat run phi4 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Boom - metrics at &lt;code&gt;http://localhost:11434/metrics&lt;/code&gt; and searchable analytics for debugging slow requests.&lt;/p&gt; &lt;p&gt;The proxy runs Ollama on a hidden port (11435) and sits transparently on the default port (11434). Everything just works‚Ñ¢Ô∏è&lt;/p&gt; &lt;p&gt;Perfect for anyone running Ollama in production or just wanting to understand their model performance better.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/bmeyer99/Ollama_Proxy_Wrapper"&gt;https://github.com/bmeyer99/Ollama_Proxy_Wrapper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/firedog7881"&gt; /u/firedog7881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwxnqn/built_a_transparent_metrics_proxy_for_ollama_zero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwxnqn/built_a_transparent_metrics_proxy_for_ollama_zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lwxnqn/built_a_transparent_metrics_proxy_for_ollama_zero/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T04:25:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx45fg</id>
    <title>Ollama Auto Start Despite removed from "Open at Login"</title>
    <updated>2025-07-11T11:13:34+00:00</updated>
    <author>
      <name>/u/Ok-Mix-646</name>
      <uri>https://old.reddit.com/user/Ok-Mix-646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lx45fg/ollama_auto_start_despite_removed_from_open_at/"&gt; &lt;img alt="Ollama Auto Start Despite removed from &amp;quot;Open at Login&amp;quot;" src="https://b.thumbs.redditmedia.com/H5gbzUNdSAKkhBj3cvclCDiU4XP-WvIW5b5y17kVmJs.jpg" title="Ollama Auto Start Despite removed from &amp;quot;Open at Login&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Mix-646"&gt; /u/Ok-Mix-646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ollama/comments/1lujfpb/ollama_auto_start_despite_removed_from_open_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lx45fg/ollama_auto_start_despite_removed_from_open_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lx45fg/ollama_auto_start_despite_removed_from_open_at/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T11:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxj3rq</id>
    <title>I have not used Ollama in a year. Has it gotten faster?</title>
    <updated>2025-07-11T21:35:08+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalAIServers/comments/1lxj3cc/i_have_not_used_ollama_in_a_year_has_it_gotten/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxj3rq/i_have_not_used_ollama_in_a_year_has_it_gotten/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxj3rq/i_have_not_used_ollama_in_a_year_has_it_gotten/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T21:35:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwbuvc</id>
    <title>Can I build a self hosted LLM server for 300 users?</title>
    <updated>2025-07-10T12:47:58+00:00</updated>
    <author>
      <name>/u/tornshorts</name>
      <uri>https://old.reddit.com/user/tornshorts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, trying to get a feel if I'm in over my head here.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt; I'm a sysadmin for a 300 person law firm. One of the owners here is really into AI and wants to give all of our users a ChatGPT-like experience.&lt;/p&gt; &lt;p&gt;The vision is to have a tool that everyone can use strictly for drafting legal documents based on their notes, grammar correction, formatting emails, and that sort of thing. We're not using it for legal research, just editorial purposes.&lt;/p&gt; &lt;p&gt;Since we often deal with documents that include PII, having a self-hosted, in-house solution is way more appealing than letting people throw client info into ChatGPT. So we're thinking of hosting our own LLM, putting it behind a username/password login, maybe adding 2FA, and only allowing access from inside the office or over VPN.&lt;/p&gt; &lt;p&gt;Now, all of this sounds... kind of simple to me. I've got experience setting up servers, and I have a general, theoretical idea of the hardware requirements to get this running. I even set up an Ollama/WebUI server at home for personal use, so I‚Äôve got at least a little hands-on experience with how this kind of build works.&lt;/p&gt; &lt;p&gt;What I‚Äôm not sure about is scalability. Can this actually support 300+ users? Am I underestimating what building a PC with a few GPUs can handle? Is user creation and management going to be a major headache? Am I missing something big here?&lt;/p&gt; &lt;p&gt;I might just be overthinking this, but I fully admit I‚Äôm not an expert on LLMs. I‚Äôm just a techy dude watching YouTube builds thinking, ‚ÄúYeah, I can do that too.‚Äù&lt;/p&gt; &lt;p&gt;Any advice or insight would be really appreciated. Thanks!&lt;/p&gt; &lt;p&gt;EDIT: I got a lot more feedback than I anticipated and I‚Äôm so thankful for everyone‚Äôs insight and suggestions. While this sounds like a fun challenge for me to tackle, I‚Äôm now understanding that doing this is going to be a full time job. I‚Äôm the only one on my team skilled enough to potentially pull this off but it‚Äôs going to take me away from my day to day responsibilities. Our IT dept is already a skeleton crew and I don‚Äôt feel comfortable adding this to our already full plate. We‚Äôre going to look into cloud solutions instead. Thanks everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tornshorts"&gt; /u/tornshorts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwbuvc/can_i_build_a_self_hosted_llm_server_for_300_users/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwbuvc/can_i_build_a_self_hosted_llm_server_for_300_users/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lwbuvc/can_i_build_a_self_hosted_llm_server_for_300_users/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-10T12:47:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxf2d6</id>
    <title>Advice Needed: Best way to replace Together API with self-hosted LLM for high-concurrency app</title>
    <updated>2025-07-11T18:51:57+00:00</updated>
    <author>
      <name>/u/Wild_King_1035</name>
      <uri>https://old.reddit.com/user/Wild_King_1035</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using the Together API to power LLM features in my app, but I've run out of credits and want to move to a self-hosted solution (like Ollama or similar open-source models). My main concern is handling high amounts of concurrent users‚Äîright now, my understanding is that a single model instance processes requests sequentially, which could lead to bottlenecks.&lt;/p&gt; &lt;p&gt;For those who have experience with self-hosted LLMs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What‚Äôs the best architecture for supporting many simultaneous users?&lt;/li&gt; &lt;li&gt;Is it better to run multiple model instances in containers and load balance between them, or should I look at cloud GPU servers?&lt;/li&gt; &lt;li&gt;Are there any best practices for scaling, queueing, or managing resource usage?&lt;/li&gt; &lt;li&gt;Any recommendations for open-source models or deployment strategies that work well for production?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear how others have handled this. I'm a novice at this kind of architecture, but my app is currently live on the App Store and so I definitely want to implement a scalable method of handling user calls to my LLaMA model. The app is not earning money right now, and it's costing me quite a bit with hosting and other services, so low-cost methods would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wild_King_1035"&gt; /u/Wild_King_1035 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxf2d6/advice_needed_best_way_to_replace_together_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxf2d6/advice_needed_best_way_to_replace_together_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxf2d6/advice_needed_best_way_to_replace_together_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T18:51:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx0o4p</id>
    <title>Ollama + OpenWebUI + documents</title>
    <updated>2025-07-11T07:27:49+00:00</updated>
    <author>
      <name>/u/ZimmerFrameThief</name>
      <uri>https://old.reddit.com/user/ZimmerFrameThief</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry if this is quite obvious or listed somewhere - I couldn't google it.&lt;/p&gt; &lt;p&gt;I run ollama with OpenWebUI in a docker environment (separate containers, same custom network) on Unraird.&lt;br /&gt; All works as it should - LLM Q&amp;amp;A is as expected - except that the LLMs say they can't interact with the documents.&lt;br /&gt; OpenWebUI has a document (and image) upload functionality - the documents appear to upload - and the LLMs can see the file names, but when I ask them to do anything with the document content, they say they don't have the functionality.&lt;br /&gt; I assumed this was an ollama thing.. but maybe it's an OpenWebUI thing? I'm pretty new to this, so don't know what I don't know.&lt;/p&gt; &lt;p&gt;Side note - don't know if it's possible to give any of the LLMs access to the net? but that would be cool too!&lt;/p&gt; &lt;p&gt;EDIT: I just use the mainstream LLMs like Deepseek, Gemma, Qewn, Minstrel, Llam etc. And I am only needing them to read/interpret the contents of document - not to edit or do anything else.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZimmerFrameThief"&gt; /u/ZimmerFrameThief &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lx0o4p/ollama_openwebui_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lx0o4p/ollama_openwebui_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lx0o4p/ollama_openwebui_documents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T07:27:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxtolv</id>
    <title>Requirements and architecture for a good enough model with scientific papers RAG</title>
    <updated>2025-07-12T06:33:50+00:00</updated>
    <author>
      <name>/u/RRUser</name>
      <uri>https://old.reddit.com/user/RRUser</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I have been tasked to build a POC for our lab of a &amp;quot;Research agent&amp;quot; that can go though our curated list of 200 scientific publications and patents, and use it as a base to brainstorm ideas.&lt;/p&gt; &lt;p&gt;My initial pitch was to setup the dabase with something like scibert embeddings, host the best local model our GPUs can run, and iterate with prompting and auxiliary agents in pydantic AI to improve performance.&lt;/p&gt; &lt;p&gt;Do you see this task and approach reasonable? The goal is to avoid services like notebookLM and specialize the outputs by customizing the prompt and workflow. &lt;/p&gt; &lt;p&gt;The recent post by the guy who wanted to implement something for 300 users got me worried that I may be a bit over my head. This would be for 2/5 users top, never concurrent, and we can queue the task and wait for it a few hours of needed. I am now wondering if models that could fit in a single GPU (llama 8B, since I need a large context window) are good enough to understand something as complex as a parent, as I am used to using API calls to the big models. &lt;/p&gt; &lt;p&gt;Sorry if this kind of post is not allowed, but the internet is kinda fuzzy about the true capabilities of these models, and I would like to set the right expectations with our team.&lt;/p&gt; &lt;p&gt;If you have any suggestions on how to improve performance on highly technical documents I appreciate them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RRUser"&gt; /u/RRUser &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxtolv/requirements_and_architecture_for_a_good_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxtolv/requirements_and_architecture_for_a_good_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxtolv/requirements_and_architecture_for_a_good_enough/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T06:33:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxj4ca</id>
    <title>What is your favorite Local LLM and why?</title>
    <updated>2025-07-11T21:35:43+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalAIServers/comments/1lxc8hb/what_is_your_favorite_local_llm_and_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxj4ca/what_is_your_favorite_local_llm_and_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxj4ca/what_is_your_favorite_local_llm_and_why/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T21:35:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyxe0</id>
    <title>Build an AI-Powered Image Search Engine Using Ollama and LangChain</title>
    <updated>2025-07-12T12:10:51+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lxyxe0/build_an_aipowered_image_search_engine_using/"&gt; &lt;img alt="Build an AI-Powered Image Search Engine Using Ollama and LangChain" src="https://external-preview.redd.it/1-TbC7xgICLdfvDtCoZXXwzT0BxWOljUGaLj15PAyT8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e429570269e4e3675c1cd2ce999a212302f0201" title="Build an AI-Powered Image Search Engine Using Ollama and LangChain" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/S9ugRzGjFtA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxyxe0/build_an_aipowered_image_search_engine_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxyxe0/build_an_aipowered_image_search_engine_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T12:10:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxvdhu</id>
    <title>Henceforth ‚Ä¶</title>
    <updated>2025-07-12T08:22:04+00:00</updated>
    <author>
      <name>/u/But-I-Am-a-Robot</name>
      <uri>https://old.reddit.com/user/But-I-Am-a-Robot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Overly joyous posters in this group shall be referred to as Ollama Lama Ding Dongs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/But-I-Am-a-Robot"&gt; /u/But-I-Am-a-Robot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxvdhu/henceforth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxvdhu/henceforth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxvdhu/henceforth/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T08:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxzg14</id>
    <title>Github copilot with Ollama - need to sign in?</title>
    <updated>2025-07-12T12:38:01+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, now that Github copilot for Visual Studio Code supports Ollama, i consider using it instead of Continue. However, it seems like you can only get to the model switcher dialogue when you are signed into github?&lt;/p&gt; &lt;p&gt;Of course, i don't want to sign in to anything, that's why i want to use my local ollama instance in the 1st place!&lt;/p&gt; &lt;p&gt;Has anyone found a workaround to use Ollama with copilot without having to sign in?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxzg14/github_copilot_with_ollama_need_to_sign_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxzg14/github_copilot_with_ollama_need_to_sign_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxzg14/github_copilot_with_ollama_need_to_sign_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T12:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxd4j0</id>
    <title>Two guys on a bus</title>
    <updated>2025-07-11T17:35:12+00:00</updated>
    <author>
      <name>/u/TodoLoQueCompartimos</name>
      <uri>https://old.reddit.com/user/TodoLoQueCompartimos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lxd4j0/two_guys_on_a_bus/"&gt; &lt;img alt="Two guys on a bus" src="https://preview.redd.it/kiuj2t0o6acf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258cace7d27d200a281a0ce5f08e1ed7c163739d" title="Two guys on a bus" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TodoLoQueCompartimos"&gt; /u/TodoLoQueCompartimos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kiuj2t0o6acf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxd4j0/two_guys_on_a_bus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxd4j0/two_guys_on_a_bus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T17:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxzo4w</id>
    <title>I built a little CLI tool to do Ollama powered "deep" research from your terminal</title>
    <updated>2025-07-12T12:49:15+00:00</updated>
    <author>
      <name>/u/LightIn_</name>
      <uri>https://old.reddit.com/user/LightIn_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lxzo4w/i_built_a_little_cli_tool_to_do_ollama_powered/"&gt; &lt;img alt="I built a little CLI tool to do Ollama powered &amp;quot;deep&amp;quot; research from your terminal" src="https://preview.redd.it/aryz1e0hwfcf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70c17e0dfdca28b999b4bd63dd9bf81a030be45d" title="I built a little CLI tool to do Ollama powered &amp;quot;deep&amp;quot; research from your terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;I‚Äôve been messing around with local LLMs lately (with Ollama) and‚Ä¶ well, I ended up making a tiny CLI tool that tries to do ‚Äúdeep‚Äù research from your terminal.&lt;/p&gt; &lt;p&gt;It‚Äôs called &lt;strong&gt;deepsearch&lt;/strong&gt;. Basically you give it a question, and it tries to break it down into smaller sub-questions, search stuff on Wikipedia and DuckDuckGo, filter what seems relevant, summarize it all, and give you a final answer. Like‚Ä¶ what a human would do, I guess.&lt;/p&gt; &lt;p&gt;Here‚Äôs the repo if you‚Äôre curious:&lt;br /&gt; &lt;a href="https://github.com/LightInn/deepsearch"&gt;https://github.com/LightInn/deepsearch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don‚Äôt really know if this is &lt;em&gt;good&lt;/em&gt; (and even less if it's somewhat usefull :c ), just trying to glue something like this together. Honestly, it‚Äôs probably pretty rough, and I‚Äôm sure there are better ways to do what it does. But I thought it was a fun experiment and figured someone else might find it interesting too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LightIn_"&gt; /u/LightIn_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aryz1e0hwfcf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxzo4w/i_built_a_little_cli_tool_to_do_ollama_powered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxzo4w/i_built_a_little_cli_tool_to_do_ollama_powered/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T12:49:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxmtc1</id>
    <title>Thank you Ollama team! Observer AI launches tonight! üöÄ I built the local open-source screen-watching tool you guys asked for.</title>
    <updated>2025-07-12T00:21:19+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lxmtc1/thank_you_ollama_team_observer_ai_launches/"&gt; &lt;img alt="Thank you Ollama team! Observer AI launches tonight! üöÄ I built the local open-source screen-watching tool you guys asked for." src="https://external-preview.redd.it/OGRoeWJxNG82Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76ef79c27e133311336216d3d1ce535a146b2fa5" title="Thank you Ollama team! Observer AI launches tonight! üöÄ I built the local open-source screen-watching tool you guys asked for." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; The open-source tool that lets local LLMs watch your screen launches tonight! Thanks to your feedback, it now has a &lt;strong&gt;1-command install (completely offline no certs to accept)&lt;/strong&gt;, supports &lt;strong&gt;any OpenAI-compatible API&lt;/strong&gt;, and has &lt;strong&gt;mobile support&lt;/strong&gt;. I'd love your feedback!&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;You guys are so amazing! After all the feedback from my last post, I'm very happy to announce that Observer AI is almost officially launched! I want to thank everyone for their encouragement and ideas.&lt;/p&gt; &lt;p&gt;For those who are new, Observer AI is a privacy-first, open-source tool to build your own micro-agents that watch your screen (or camera) and trigger simple actions, all running 100% locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New in the last few days(Directly from your feedback!):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;‚úÖ 1-Command 100% Local Install:&lt;/strong&gt; I made it super simple. Just run docker compose up --build and the entire stack runs locally. No certs to accept or &amp;quot;online activation&amp;quot; needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚úÖ Universal Model Support:&lt;/strong&gt; You're no longer limited to Ollama! You can now connect to &lt;strong&gt;any endpoint that uses the OpenAI v1/chat standard&lt;/strong&gt;. This includes local servers like LM Studio, Llama.cpp, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚úÖ Mobile Support:&lt;/strong&gt; You can now use the app on your phone, using its camera and microphone as sensors. (Note: Mobile browsers don't support screen sharing).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Roadmap:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I hope that I'm just getting started. Here's what I will focus on next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Standalone Desktop App:&lt;/strong&gt; A 1-click installer for a native app experience. (With inference and everything!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Telegram Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Slack Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Sharing:&lt;/strong&gt; Easily share your creations with others via a simple link.&lt;/li&gt; &lt;li&gt;And much more!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Let's Build Together:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is a tool built for tinkerers, builders, and privacy advocates like you. Your feedback is crucial.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub (Please Star if you find it cool!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;App Link (Try it in your browser no install!):&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord (Join the community):&lt;/strong&gt; &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out in the comments all day. Let me know what you think and what you'd like to see next. Thank you again!&lt;/p&gt; &lt;p&gt;PS. Sorry to everyone who &lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ksu3dt4o6ccf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxmtc1/thank_you_ollama_team_observer_ai_launches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxmtc1/thank_you_ollama_team_observer_ai_launches/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T00:21:19+00:00</published>
  </entry>
</feed>
