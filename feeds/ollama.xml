<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-24T16:51:03+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1p2mro7</id>
    <title>Best &lt; $20k Configuration</title>
    <updated>2025-11-21T02:30:03+00:00</updated>
    <author>
      <name>/u/JMWTech</name>
      <uri>https://old.reddit.com/user/JMWTech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What would you build with $20k to train a model(s) and operate a on-prem chat bot for document and policy retrieval?&lt;/p&gt; &lt;p&gt;I've received quotes from &amp;quot;workstations&amp;quot; with 5090s to rack mounted servers running either four L4s (ewww) to a dual proc single RTX Pro 6000. Just want to make sure we're not wasting money and getting the most bang for the buck.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JMWTech"&gt; /u/JMWTech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2mro7/best_20k_configuration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2mro7/best_20k_configuration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2mro7/best_20k_configuration/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T02:30:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3lfua</id>
    <title>üöÄ Just Finished an INSANE MCP + LangChain + Claude Course ‚Äî Mind = Blown ü§Ø</title>
    <updated>2025-11-22T05:40:59+00:00</updated>
    <author>
      <name>/u/Distinct-Truth7165</name>
      <uri>https://old.reddit.com/user/Distinct-Truth7165</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Distinct-Truth7165"&gt; /u/Distinct-Truth7165 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/learnmachinelearning/comments/1p3lfgj/just_finished_an_insane_mcp_langchain_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3lfua/just_finished_an_insane_mcp_langchain_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p3lfua/just_finished_an_insane_mcp_langchain_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-22T05:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p32ism</id>
    <title>4096 token limit</title>
    <updated>2025-11-21T16:04:50+00:00</updated>
    <author>
      <name>/u/aleglr20</name>
      <uri>https://old.reddit.com/user/aleglr20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I‚Äôm not sure if this is the right subreddit for this question ‚Äî if not, please let me know where I should post it.&lt;br /&gt; Anyway, I‚Äôm working on a Java project using the &lt;code&gt;spring-ai-starter-model-openai&lt;/code&gt; dependency, and I‚Äôm currently using &lt;code&gt;gemma3:4b&lt;/code&gt; through Ollama, which exposes OpenAI-compatible endpoints.&lt;/p&gt; &lt;p&gt;I have a chat method where I pass a text as context and then ask a question about it. The text and the question are combined into a single prompt that I send to the model.&lt;br /&gt; From the JSON response, I noticed the token usage data, and I discovered that if I go above roughly 4,070 tokens, the model gives a wrong or incoherent answer ‚Äî it no longer follows the question or the provided context.&lt;/p&gt; &lt;p&gt;Can someone explain to me how the 4,096-token limit works? Even if the model has a 128k context window?&lt;br /&gt; Is the 4,096-token limit related to the output, the prompt, or both? Because I‚Äôm experiencing issues specifically when the &lt;em&gt;prompt&lt;/em&gt; gets too large, even before the output is generated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aleglr20"&gt; /u/aleglr20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p32ism/4096_token_limit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p32ism/4096_token_limit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p32ism/4096_token_limit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T16:04:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3czc0</id>
    <title>What am I missing?</title>
    <updated>2025-11-21T22:51:53+00:00</updated>
    <author>
      <name>/u/SaltbushBillJP</name>
      <uri>https://old.reddit.com/user/SaltbushBillJP</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Page Assist, testing several LLMs and trying to have OLLAMA extract specific values from multiple files (each being an exported email - I've tested PDF and txt formats).&lt;/p&gt; &lt;p&gt;The emails are responses from local government acknowledging permit applications and I want to extract registration number and submission date from each email (file). &lt;/p&gt; &lt;p&gt;This works for one or two, then the response is completed with blank values or N/A or some other rubbish.&lt;/p&gt; &lt;p&gt;I've loaded about 6 of these files into a Knowledge Base and selected it for the evaluation&lt;/p&gt; &lt;p&gt;Should I edit RAG settings?&lt;/p&gt; &lt;p&gt;What else do I need to do so the query correctly evaluates more than 2 documents?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SaltbushBillJP"&gt; /u/SaltbushBillJP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3czc0/what_am_i_missing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3czc0/what_am_i_missing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p3czc0/what_am_i_missing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T22:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p33czu</id>
    <title>Ollama Grid Search v0.9.2: Enhanced LLM Evaluation and Comparison</title>
    <updated>2025-11-21T16:37:01+00:00</updated>
    <author>
      <name>/u/grudev</name>
      <uri>https://old.reddit.com/user/grudev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Happy to announce the release of &lt;a href="https://github.com/dezoito/ollama-grid-search/"&gt;&lt;strong&gt;Ollama Grid Search v0.9.2&lt;/strong&gt;&lt;/a&gt;, a tool created to improve the experience of those of use evaluating and experimenting with multiple LLMs&lt;/p&gt; &lt;p&gt;This addresses issues with damaged &lt;code&gt;.dmg&lt;/code&gt; files that some users experienced during installation (a result of GitHub actions script + Apple's signing requirements). The build process has been updated to improve the setup for all macOS users, particularly those on Apple Silicon (M1/M2/M3/M4) devices.&lt;/p&gt; &lt;h1&gt;About Ollama Grid Search&lt;/h1&gt; &lt;p&gt;For those new to the project, Ollama Grid Search is a desktop application that automates the process of evaluating and comparing multiple Large Language Models (LLMs). Whether you're fine-tuning prompts, selecting the best model for your use case, or conducting A/B tests, this tool will make your life easier.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-Model Testing&lt;/strong&gt;: Automatically fetch and test multiple models from your Ollama servers&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grid Search&lt;/strong&gt;: Iterate over combinations of models, prompts, and parameters&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A/B Testing&lt;/strong&gt;: Compare responses from different prompts and models side-by-side&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt Management&lt;/strong&gt;: Built-in prompt database with autocomplete functionality&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Experiment Logs&lt;/strong&gt;: Track, review, and re-run past experiments&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Concurrent Inference&lt;/strong&gt;: Support for parallel inference calls to speed up evaluations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Results&lt;/strong&gt;: Easy-to-read interface for comparing model outputs&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Getting Started&lt;/h1&gt; &lt;p&gt;Download the latest release from our &lt;a href="https://github.com/dezoito/ollama-grid-search/releases"&gt;releases page&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/dezoito/ollama-grid-search"&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/dezoito/ollama-grid-search/blob/main/CHANGELOG.md"&gt;Full Changelog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://dezoito.github.io/2023/12/27/rust-ollama-grid-search.html"&gt;In-depth Grid Search Tutorial&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grudev"&gt; /u/grudev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p33czu/ollama_grid_search_v092_enhanced_llm_evaluation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p33czu/ollama_grid_search_v092_enhanced_llm_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p33czu/ollama_grid_search_v092_enhanced_llm_evaluation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T16:37:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3zoo5</id>
    <title>Where to run OpenWeb UI?</title>
    <updated>2025-11-22T17:58:22+00:00</updated>
    <author>
      <name>/u/Mundane_Local_2992</name>
      <uri>https://old.reddit.com/user/Mundane_Local_2992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I have a very simple question. I am running a very elaborate agent on an ollama qwen3 installation on a remote linux server (with a rtx3090, 5950x and 64gb of ram). the setup runs great and fast when I directly query ollama on the server via terminal, but it seems that running OpenWebUI on the same server and then accessing it via &amp;lt;server ip&amp;gt;:5050 causes a lot of added latency (responses that takes 14 seconds via terminal takes over 25 seconds when asking OpenWebUI via web browser. so my question is, is the issue that I am running OpenWebUI on the same server as my ollama installation, and I should actually run OpenWebUI on my local machine, which point to the server? or is there a more performant ui that I can use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane_Local_2992"&gt; /u/Mundane_Local_2992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3zoo5/where_to_run_openweb_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3zoo5/where_to_run_openweb_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p3zoo5/where_to_run_openweb_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-22T17:58:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4eoje</id>
    <title>Will an RTX 6000 run the gpt-oss:120b model?</title>
    <updated>2025-11-23T05:31:08+00:00</updated>
    <author>
      <name>/u/pawnstew</name>
      <uri>https://old.reddit.com/user/pawnstew</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sizing a GPU for work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pawnstew"&gt; /u/pawnstew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4eoje/will_an_rtx_6000_run_the_gptoss120b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4eoje/will_an_rtx_6000_run_the_gptoss120b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4eoje/will_an_rtx_6000_run_the_gptoss120b_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T05:31:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4f8iv</id>
    <title>GPT-OSS 20B: Capacit√©s et Tests</title>
    <updated>2025-11-23T06:02:53+00:00</updated>
    <author>
      <name>/u/Desperate_Yellow_541</name>
      <uri>https://old.reddit.com/user/Desperate_Yellow_541</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Le mythe de la taille : comment un 20B bien r√©gl√© peut rivaliser avec les ‚Äúg√©ants‚Äù sur des t√¢ches d‚Äôextraction r√©elles&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Mod√®le : &lt;strong&gt;GPT-OSS 20B&lt;/strong&gt; (local)&lt;/li&gt; &lt;li&gt;Contexte : &lt;strong&gt;65k tokens&lt;/strong&gt; (menu PDF OCR bien sale)&lt;/li&gt; &lt;li&gt;T√¢che : extraire des &lt;strong&gt;prix corrects&lt;/strong&gt; d‚Äôun document bruit√©&lt;/li&gt; &lt;li&gt;Cl√© du succ√®s : &lt;strong&gt;Mirostat v2 + Repeat Penalty √† 1.2&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Id√©e centrale : le probl√®me n‚Äôest pas ‚Äúle mod√®le est trop petit‚Äù, mais &lt;strong&gt;comment on le contraint&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Contexte : ce qu‚Äôon essaie vraiment de faire&lt;/h1&gt; &lt;p&gt;L‚Äôindustrie de l‚ÄôIA est obs√©d√©e par la taille : 70B, 405B, 1T de param√®tres‚Ä¶&lt;br /&gt; Mais sur du &lt;strong&gt;local&lt;/strong&gt;, avec un 20B, je me suis rendu compte que :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Cas d‚Äôusage concret :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Un &lt;strong&gt;menu de restaurant&lt;/strong&gt; en PDF,&lt;/li&gt; &lt;li&gt;Pass√© par un &lt;strong&gt;OCR imparfait&lt;/strong&gt; (chiffres coll√©s, espaces bizarres, accents foutus),&lt;/li&gt; &lt;li&gt;Inject√© dans une fen√™tre de contexte de &lt;strong&gt;65 536 tokens&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Objectif : extraire un tableau propre &lt;em&gt;plat ‚Üí prix&lt;/em&gt;, &lt;strong&gt;sans halluciner&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Spoiler : au d√©part, le mod√®le a fait n‚Äôimporte quoi‚Ä¶ mais pas pour les raisons qu‚Äôon croit.&lt;/p&gt; &lt;h1&gt;I. L‚Äôillusion de la ‚Äúparesse cognitive‚Äù des LLM locaux&lt;/h1&gt; &lt;p&gt;Quand on joue avec des LLM locaux, on voit souvent :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄúD√©sol√©, ce texte est trop long‚Äù ‚Üí refus implicite&lt;/li&gt; &lt;li&gt;Boucles de texte ‚Üí le mod√®le r√©p√®te la m√™me phrase&lt;/li&gt; &lt;li&gt;Hallucinations de chiffres ou de lignes enti√®res&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;R√©flexe classique :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Sauf que dans mon cas, sur ce menu OCR :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Le mod√®le &lt;strong&gt;comprenait&lt;/strong&gt; globalement la structure,&lt;/li&gt; &lt;li&gt;Mais il avait un comportement ‚Äúparesseux‚Äù : &lt;ul&gt; &lt;li&gt;Il attribuait le &lt;strong&gt;m√™me prix&lt;/strong&gt; √† plusieurs plats,&lt;/li&gt; &lt;li&gt;Ou il ‚Äúinventait‚Äù des prix plausibles visuellement, mais &lt;strong&gt;faux&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ce n‚Äô√©tait pas un probl√®me de compr√©hension linguistique.&lt;br /&gt; C‚Äô√©tait une &lt;strong&gt;optimisation probabiliste&lt;/strong&gt; :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Le mod√®le se cale dans une orni√®re : ‚Äúj‚Äôai d√©j√† √©crit 150, √ßa marche bien, je vais remettre 150‚Äù.&lt;br /&gt; Notre job n‚Äôa pas √©t√© de ‚Äúlui expliquer mieux‚Äù le menu,&lt;br /&gt; mais de &lt;strong&gt;rendre cette orni√®re tr√®s co√ªteuse&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;II. Le ‚ÄúSettings Engineering‚Äù : trianguler les hyperparam√®tres&lt;/h1&gt; &lt;p&gt;Pour transformer ce g√©n√©rateur de texte probabiliste en quelque chose qui ressemble √† un &lt;strong&gt;agent de raisonnement fiable&lt;/strong&gt;, j‚Äôai fini par traiter les r√©glages comme un vrai espace de recherche.&lt;/p&gt; &lt;p&gt;J‚Äôai identifi√© trois leviers qui, ensemble, changent radicalement le comportement :&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Levier de stabilit√©&lt;/strong&gt; : Mirostat vs Top_K / Top_P&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Levier de v√©rit√©&lt;/strong&gt; : la p√©nalit√© de r√©p√©tition (Repeat Penalty)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Levier de charge cognitive&lt;/strong&gt; : ‚Äúforcer‚Äù le mod√®le √† r√©fl√©chir (System 2-like)&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;1. Mirostat v2 : stabiliser le chaos sur 65k tokens&lt;/h1&gt; &lt;p&gt;Les r√©glages classiques (Top_K, Top_P) sont des &lt;strong&gt;filtres statiques&lt;/strong&gt; :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ils coupent la queue de distribution,&lt;/li&gt; &lt;li&gt;Mais ils ne s‚Äôadaptent pas √† la &lt;strong&gt;difficult√© locale&lt;/strong&gt; du passage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sur un contexte de &lt;strong&gt;65k tokens&lt;/strong&gt;, tu as des zones :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Faciles&lt;/strong&gt; : traduction simple, texte tr√®s clair,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ambigu√´s&lt;/strong&gt; : chiffres flous, OCR parasit√©, colonnes bancales.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Avec Top_K / Top_P seul, le mod√®le n‚Äôa aucune conscience que ‚Äúl√†, c‚Äôest chaud‚Äù, ou ‚Äúl√†, c‚Äôest trivial‚Äù.&lt;/p&gt; &lt;p&gt;Avec &lt;strong&gt;Mirostat v2&lt;/strong&gt;, le comportement change :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;L‚Äôalgorithme surveille une forme de &lt;strong&gt;perplexit√©&lt;/strong&gt; (surprise du mod√®le) &lt;em&gt;en temps r√©el&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Intuition (simplifi√©e) : &lt;ul&gt; &lt;li&gt;Si le mod√®le &lt;strong&gt;boucle&lt;/strong&gt; ‚Üí la perplexit√© chute (trop pr√©visible) ‚Üí ‚Üí Mirostat &lt;strong&gt;injecte du chaos&lt;/strong&gt;, oblige le mod√®le √† sortir de la boucle.&lt;/li&gt; &lt;li&gt;Si le mod√®le &lt;strong&gt;part en d√©lire&lt;/strong&gt; ‚Üí perplexit√© explose ‚Üí ‚Üí Mirostat &lt;strong&gt;resserre&lt;/strong&gt; la g√©n√©ration vers quelque chose de plus coh√©rent.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;R√©sultat empirique :&lt;br /&gt; Sur ce menu OCR, Mirostat a rendu les r√©ponses :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Moins sujettes aux &lt;strong&gt;boucles&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Et plus robustes sur des zones ‚Äúcass√©es‚Äù du texte source.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. La p√©nalit√© de r√©p√©tition 1.2 : casser la voie facile&lt;/h1&gt; &lt;p&gt;La vraie bascule, pour l‚Äôextraction de &lt;strong&gt;prix&lt;/strong&gt;, est venue de la &lt;strong&gt;Repeat Penalty&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Ce qu‚Äôon lit partout :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄúMettez 1.05 ou 1.1, au-del√† vous tuez la qualit√© du texte.‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ce que j‚Äôai observ√© :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pour de la &lt;strong&gt;donn√©e chiffr√©e&lt;/strong&gt; dans un contexte bruit√©, le &lt;strong&gt;seuil critique&lt;/strong&gt; n‚Äôest pas 1.1, mais plut√¥t &lt;strong&gt;1.2&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Intuition :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;√Ä 1.1 : &lt;ul&gt; &lt;li&gt;Le ‚Äúco√ªt‚Äù de r√©√©crire &lt;code&gt;150&lt;/code&gt; est encore &lt;strong&gt;faible&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Le ‚Äúco√ªt‚Äù d‚Äôaller chercher &lt;code&gt;190&lt;/code&gt; dans le contexte OCR est &lt;strong&gt;plus √©lev√©&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Le mod√®le choisit : &lt;em&gt;‚ÄúJe recycle 150, c‚Äôest plus confortable.‚Äù&lt;/em&gt; ‚Üí &lt;strong&gt;Hallucination par r√©p√©tition.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;√Ä 1.2 : &lt;ul&gt; &lt;li&gt;La r√©p√©tition de &lt;code&gt;150&lt;/code&gt; devient &lt;strong&gt;prohibitive&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Le mod√®le se retrouve &lt;strong&gt;bloqu√©&lt;/strong&gt; :‚ÄúJe veux √©crire un prix, mais je ne peux pas r√©√©crire 150.‚Äù&lt;/li&gt; &lt;li&gt;Son seul chemin ‚Äúpeu co√ªteux‚Äù devient alors : ‚Üí &lt;strong&gt;replonger dans le contexte&lt;/strong&gt;, chercher un autre motif valide (&lt;code&gt;190&lt;/code&gt;, &lt;code&gt;135&lt;/code&gt;, etc.).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ce n‚Äôest √©videmment pas une ‚Äúv√©rification logique‚Äù au sens humain, mais :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;En pratique, la mont√©e √† &lt;strong&gt;1.2&lt;/strong&gt; a fait dispara√Ætre un grand nombre de &lt;strong&gt;lignes dupliqu√©es&lt;/strong&gt; ou ‚Äúprix coll√©s partout‚Äù.&lt;/p&gt; &lt;h1&gt;III. Quand le mod√®le commence √† ‚Äúcompter‚Äù : √©mergence d‚Äôun pseudo System 2&lt;/h1&gt; &lt;p&gt;Une fois Mirostat v2 activ√© + Repeat Penalty √† 1.2, un truc int√©ressant est arriv√© dans les logs de r√©flexion (Chain-of-Thought).&lt;/p&gt; &lt;p&gt;Le mod√®le a g√©n√©r√© un raisonnement du type :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;C‚Äôest exactement le genre de comportement que j‚Äôattends d‚Äôun mod√®le ‚Äúplus gros‚Äù sur ce type de t√¢che :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Aligner&lt;/strong&gt; une liste de plats avec une liste de prix,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compter&lt;/strong&gt; les √©l√©ments des deux c√¥t√©s,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Recomposer&lt;/strong&gt; un prix cass√© par l‚ÄôOCR (&lt;code&gt;1 9 0&lt;/code&gt; ‚Üí &lt;code&gt;190&lt;/code&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ce qui est int√©ressant ici :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ce n‚Äôest pas ‚Äúmagiquement‚Äù apparu gr√¢ce au nombre de param√®tres,&lt;/li&gt; &lt;li&gt;C‚Äôest apparu &lt;strong&gt;apr√®s&lt;/strong&gt; qu‚Äôon ait &lt;strong&gt;ferm√© les voies faciles&lt;/strong&gt; : &lt;ul&gt; &lt;li&gt;refus,&lt;/li&gt; &lt;li&gt;r√©p√©tition,&lt;/li&gt; &lt;li&gt;hallucination d√©corative.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;On a, en quelque sorte, forc√© le mod√®le √† adopter un comportement plus proche d‚Äôun &lt;strong&gt;System 2&lt;/strong&gt; (au sens Kahneman) :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Et √ßa, avec &lt;strong&gt;un 20B local&lt;/strong&gt;, sur une machine perso, pas un datacenter.&lt;/p&gt; &lt;h1&gt;Conclusion : on n‚Äôa pas (toujours) besoin de mod√®les plus gros, on a besoin de meilleures contraintes&lt;/h1&gt; &lt;p&gt;Ce que cette exp√©rience sugg√®re pour les &lt;strong&gt;LLM locaux&lt;/strong&gt; :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pour beaucoup de t√¢ches &lt;strong&gt;administratives / d‚Äôextraction documentaire&lt;/strong&gt; : &lt;ul&gt; &lt;li&gt;factures, menus, tableaux OCR, listes, etc.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Le vrai levier n‚Äôest &lt;strong&gt;pas forc√©ment&lt;/strong&gt; : &lt;ul&gt; &lt;li&gt;‚Äúmonter en taille‚Äù,&lt;/li&gt; &lt;li&gt;‚Äúpasser au closed-source cloud‚Äù.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Mais plut√¥t :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;En pratique, √ßa veut dire :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jouer s√©rieusement avec : &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Temp√©rature&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mirostat / Top_P&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Repeat Penalty&lt;/strong&gt;,&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Et accepter que pour des t√¢ches &lt;strong&gt;critiques sur les chiffres&lt;/strong&gt;, un mod√®le ‚Äúmoins fluide‚Äù mais &lt;strong&gt;plus contraint&lt;/strong&gt; est &lt;strong&gt;pr√©f√©rable&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Annexe : le ‚ÄúPreset‚Äù qui a d√©clench√© le changement&lt;/h1&gt; &lt;p&gt;Pour ceux qui veulent le preset brut, voici la config qui a march√© &lt;strong&gt;dans mon cas&lt;/strong&gt;&lt;br /&gt; (GPT-OSS 20B via Ollama / OpenWebUI, extraction de prix sur menu OCR, contexte 65k) :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Mod√®le : GPT-OSS 20B Contexte : 65 536 tokens T√¢che : extraction de prix depuis un PDF OCR bruit√© [Architecture cognitive] Mirostat : ON (Mode 2) Mirostat Tau : 5.0 Mirostat Eta : 0.1 Reasoning Effort : High [Param√®tres de contrainte] Temp√©rature : 0.60 Top_p : 0.90 Repeat Penalty : 1.20 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Je ne pr√©tends pas que ce preset est universel, mais :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sur ce cas d‚Äôusage, il a fait la diff√©rence entre : &lt;ul&gt; &lt;li&gt;un mod√®le qui ‚Äúfait joli mais faux‚Äù,&lt;/li&gt; &lt;li&gt;et un mod√®le qui &lt;strong&gt;compte&lt;/strong&gt; et &lt;strong&gt;recroise&lt;/strong&gt; les donn√©es du PDF.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Si √ßa int√©resse du monde, je peux partager dans un autre post :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Un exemple de prompt complet (avec consignes d‚Äôextraction),&lt;/li&gt; &lt;li&gt;La structure de validation que j‚Äôutilise derri√®re (checks simples type ‚Äúnombre de lignes / coh√©rence des prix‚Äù),&lt;/li&gt; &lt;li&gt;Et pourquoi &lt;strong&gt;je teste toujours d‚Äôabord des changements de settings&lt;/strong&gt; avant de conclure qu‚Äôun mod√®le est ‚Äútrop petit‚Äù.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Yellow_541"&gt; /u/Desperate_Yellow_541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4f8iv/gptoss_20b_capacit√©s_et_tests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4f8iv/gptoss_20b_capacit√©s_et_tests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4f8iv/gptoss_20b_capacit√©s_et_tests/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T06:02:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4f9j0</id>
    <title>GPT-OSS 20B: Capacit√©s et Tests</title>
    <updated>2025-11-23T06:04:36+00:00</updated>
    <author>
      <name>/u/Desperate_Yellow_541</name>
      <uri>https://old.reddit.com/user/Desperate_Yellow_541</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Yellow_541"&gt; /u/Desperate_Yellow_541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ollama/comments/1p4f8iv/gptoss_20b_capacit√©s_et_tests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4f9j0/gptoss_20b_capacit√©s_et_tests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4f9j0/gptoss_20b_capacit√©s_et_tests/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T06:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4qhpr</id>
    <title>Manus</title>
    <updated>2025-11-23T16:09:16+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://manus.im/invitation/BHKBHIT0WJFORVO"&gt;https://manus.im/invitation/BHKBHIT0WJFORVO&lt;/a&gt; login with gmail or apple, or microsoft&lt;/p&gt; &lt;p&gt;Find redeem section in invite friends and use this code to get 1000 credits: njexode&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4qhpr/manus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4qhpr/manus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4qhpr/manus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T16:09:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ie4a</id>
    <title>Help with a prompt?</title>
    <updated>2025-11-23T09:19:57+00:00</updated>
    <author>
      <name>/u/SystemAromatic</name>
      <uri>https://old.reddit.com/user/SystemAromatic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, i'm having a problem. There is this program that gives me a question and 4 Answers but i cant copypaste and i'm pretty lazy to rewrite it all. I've Tried a couple of prompt but i'm pretty new to this and i don't really know if there even is a proper way to do what i want to do, any suggestion? &lt;/p&gt; &lt;p&gt;Tl;dr: Prompt for ollama to read my screen (?) And answer without copypaste &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SystemAromatic"&gt; /u/SystemAromatic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4ie4a/help_with_a_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4ie4a/help_with_a_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4ie4a/help_with_a_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T09:19:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4jen4</id>
    <title>ToolNeuron ‚Äî Privacy-first AI hub on Android (offline + online models, plugin support)</title>
    <updated>2025-11-23T10:23:14+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p4jen4/toolneuron_privacyfirst_ai_hub_on_android_offline/"&gt; &lt;img alt="ToolNeuron ‚Äî Privacy-first AI hub on Android (offline + online models, plugin support)" src="https://external-preview.redd.it/AuuQ6Jw6VH9e6ZU098Q0G_x_fI41xouhvcXLl_FeCi8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a50a03c0422760aefd8861c44779d24f9c6b02ab" title="ToolNeuron ‚Äî Privacy-first AI hub on Android (offline + online models, plugin support)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://tool-neuron.vercel.app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4jen4/toolneuron_privacyfirst_ai_hub_on_android_offline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4jen4/toolneuron_privacyfirst_ai_hub_on_android_offline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T10:23:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4lxnr</id>
    <title>Best Local Coding Agent Model for 64GB RAM and 12GB VRAM?</title>
    <updated>2025-11-23T12:49:55+00:00</updated>
    <author>
      <name>/u/fallen0523</name>
      <uri>https://old.reddit.com/user/fallen0523</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallen0523"&gt; /u/fallen0523 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1p4lwyc/best_local_coding_agent_model_for_64gb_ram_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4lxnr/best_local_coding_agent_model_for_64gb_ram_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4lxnr/best_local_coding_agent_model_for_64gb_ram_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T12:49:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4od24</id>
    <title>Summarize and manage local files?</title>
    <updated>2025-11-23T14:42:32+00:00</updated>
    <author>
      <name>/u/cl326</name>
      <uri>https://old.reddit.com/user/cl326</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a huge number of files on my laptop. They are not well organized or named. I‚Äôve removed all duplicates that I can by comparing hashes and names. Now I‚Äôd like to use Ollama to summarize each file in a folder so ai can get an idea of what the file is about if I can‚Äôt tell by the name. The files are mostly MS Office documents, PDFs, and images. Is there a model that you‚Äôd suggest? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cl326"&gt; /u/cl326 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4od24/summarize_and_manage_local_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4od24/summarize_and_manage_local_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4od24/summarize_and_manage_local_files/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T14:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3vpni</id>
    <title>Your local Ollama agents can be just as good as closed-source models - I open-sourced Stanford's ACE framework that makes agents learn from mistakes</title>
    <updated>2025-11-22T15:17:51+00:00</updated>
    <author>
      <name>/u/cheetguy</name>
      <uri>https://old.reddit.com/user/cheetguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I implemented Stanford's &lt;a href="https://arxiv.org/abs/2510.04618"&gt;Agentic Context Engineering paper&lt;/a&gt; for Ollama. The framework makes agents learn from their own execution feedback through in-context learning instead of fine-tuning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; Agent runs task ‚Üí reflects on what worked/failed ‚Üí curates strategies into playbook ‚Üí uses playbook on next run&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Improvement:&lt;/strong&gt; Paper shows +17.1pp accuracy improvement vs base LLM (‚âà+40% relative improvement) on agent benchmarks (DeepSeek-V3.1 non-thinking mode), helping close the gap with closed-source models. All through in-context learning (no fine-tuning needed).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Open-Source Implementation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop into existing agents in ~10 lines of code&lt;/li&gt; &lt;li&gt;Works with any Ollama model (Llama, Qwen, Mistral, DeepSeek, etc.)&lt;/li&gt; &lt;li&gt;Real-world test on browser automation agent: &lt;ul&gt; &lt;li&gt;30% ‚Üí 100% success rate&lt;/li&gt; &lt;li&gt;82% fewer steps&lt;/li&gt; &lt;li&gt;65% decrease in token cost&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;https://github.com/kayba-ai/agentic-context-engine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ollama Starter Template: &lt;a href="https://github.com/kayba-ai/agentic-context-engine/blob/main/examples/ollama/ollama_starter_template.py"&gt;https://github.com/kayba-ai/agentic-context-engine/blob/main/examples/ollama/ollama_starter_template.py&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear if anyone tries this with Ollama! Especially curious how it performs with different Ollama models.&lt;/p&gt; &lt;p&gt;I'm currently actively improving this based on feedback - &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;‚≠ê the repo&lt;/a&gt; so you can stay updated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cheetguy"&gt; /u/cheetguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-22T15:17:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p52b8z</id>
    <title>RAG follow-ups not working ‚Äî Qwen2.5 ignores previous context and gives unrelated answers</title>
    <updated>2025-11-24T00:10:24+00:00</updated>
    <author>
      <name>/u/NoBlackberry3264</name>
      <uri>https://old.reddit.com/user/NoBlackberry3264</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm building a &lt;strong&gt;RAG-based chat system&lt;/strong&gt; using FastAPI + &lt;strong&gt;Qwen/Qwen2.5-7B-Instruct&lt;/strong&gt;, and I‚Äôm running into an issue with follow-up queries.&lt;/p&gt; &lt;p&gt;The first query works fine, retrieving relevant documents from my knowledge base. But when the user asks a follow-up question, the model completely ignores previous context and fetches unrelated information.&lt;/p&gt; &lt;h1&gt;Example Payload (Client Request)&lt;/h1&gt; &lt;p&gt;Here‚Äôs the structure of the payload my client sends:&lt;br /&gt; {&lt;/p&gt; &lt;p&gt;&amp;quot;system_persona&amp;quot;: &amp;quot;KB&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;system_prompt&amp;quot;: { ... }, &lt;/p&gt; &lt;p&gt;&amp;quot;context&amp;quot;: [&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;pageUrl&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;sourceUrl&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;pageUrl&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;sourceUrl&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;],&lt;/p&gt; &lt;p&gt;&amp;quot;chat_history&amp;quot;: [&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;query&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;response&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;query&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;response&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;],&lt;/p&gt; &lt;p&gt;&amp;quot;query&amp;quot;: &amp;quot;nabil bank ko baryama bhana?&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Issue:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Follow-ups are not linked to previous conversation.&lt;/li&gt; &lt;li&gt;Chat history is sent but not effectively used.&lt;/li&gt; &lt;li&gt;Retrieval is based only on the latest query.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoBlackberry3264"&gt; /u/NoBlackberry3264 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p52b8z/rag_followups_not_working_qwen25_ignores_previous/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p52b8z/rag_followups_not_working_qwen25_ignores_previous/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p52b8z/rag_followups_not_working_qwen25_ignores_previous/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T00:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4z140</id>
    <title>Need some honest opinions on GPU Ai in a box</title>
    <updated>2025-11-23T21:49:26+00:00</updated>
    <author>
      <name>/u/Whyme-__-</name>
      <uri>https://old.reddit.com/user/Whyme-__-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is related to Nvidia Spark and its competitors. I have a use case where I have to deliver my Ai software to deploy on customer infrastructure. I have 3 8b models fine tuned for each use case. I want to know if using a Nvidia spark or similar GPU in a box is worth the investment for privacy, speed and economics. &lt;/p&gt; &lt;p&gt;For my use case my models and software burn about $2000 per month if I rent a pod using runpod and I have to be extra careful due to rate limits. I want to consider running my models using llamacpp or ollama or offering direct inference for customer using their own on prem machine shipped by me. &lt;/p&gt; &lt;p&gt;Here are my 2 concern:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Are these machines stable enough to deploy in production environments? I know they run Linux and my software stack is dockerized so won‚Äôt affect much. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Cost: I know as we enter the new year GPU cost might go down but should that be something to wait it out or get 1 DGX spark box and test things out to see the functionality and ease of deployment? I can always repurpose the box for my startup instead of relying on Runpod GPU. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This community has helped me a lot in the past, I‚Äôm hoping to get some answers from the community regarding these issues. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whyme-__-"&gt; /u/Whyme-__- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4z140/need_some_honest_opinions_on_gpu_ai_in_a_box/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4z140/need_some_honest_opinions_on_gpu_ai_in_a_box/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4z140/need_some_honest_opinions_on_gpu_ai_in_a_box/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T21:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4wq3l</id>
    <title>ollama troubles</title>
    <updated>2025-11-23T20:15:31+00:00</updated>
    <author>
      <name>/u/Remote-Ad8602</name>
      <uri>https://old.reddit.com/user/Remote-Ad8602</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi people&lt;br /&gt; I recently heard about ollama and tought of giving it a try after installing the app i tried to download some existing models like gpt and qwen when try to give a prompt for the model the screen keeps loading for long time and so on, one time i had to wait 44 mins just to see an error message so i tried removing the app and reinstall again even after multiple tries I still couldn't figure out what's wrong is it my computer or some problems with the app, I use a Macbook Air M4 chip &lt;/p&gt; &lt;p&gt;has anyone faced the same issue, please let me know is there any remedy to get it working normally let me know guys....&lt;/p&gt; &lt;p&gt;cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remote-Ad8602"&gt; /u/Remote-Ad8602 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4wq3l/ollama_troubles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4wq3l/ollama_troubles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4wq3l/ollama_troubles/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T20:15:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4o31k</id>
    <title>Nanocoder VS Code Plugin is Coming Along!</title>
    <updated>2025-11-23T14:30:09+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p4o31k/nanocoder_vs_code_plugin_is_coming_along/"&gt; &lt;img alt="Nanocoder VS Code Plugin is Coming Along!" src="https://preview.redd.it/0bdpxyrvm03g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a452110a43bda75376e5ffac7e29e49df77ae63" title="Nanocoder VS Code Plugin is Coming Along!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0bdpxyrvm03g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4o31k/nanocoder_vs_code_plugin_is_coming_along/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4o31k/nanocoder_vs_code_plugin_is_coming_along/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T14:30:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5b6pd</id>
    <title>Win 7 days of unlimited API access on GLM-4.6! 7 winners</title>
    <updated>2025-11-24T07:53:03+00:00</updated>
    <author>
      <name>/u/cobra91310</name>
      <uri>https://old.reddit.com/user/cobra91310</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cobra91310"&gt; /u/cobra91310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ZaiGLM/comments/1p5b6cw/win_7_days_of_unlimited_api_access_on_glm46_7/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5b6pd/win_7_days_of_unlimited_api_access_on_glm46_7/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5b6pd/win_7_days_of_unlimited_api_access_on_glm46_7/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T07:53:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p56i8v</id>
    <title>M.I.M.I.R - Now with visual intelligence built in for embeddings - MIT licensed - use with local ollama or llama.cpp for full control over your data</title>
    <updated>2025-11-24T03:29:23+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p56i8v/mimir_now_with_visual_intelligence_built_in_for/"&gt; &lt;img alt="M.I.M.I.R - Now with visual intelligence built in for embeddings - MIT licensed - use with local ollama or llama.cpp for full control over your data" src="https://preview.redd.it/icw2e1nbj43g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6af6e5db3bde853b33115fc981027e4bea84b11" title="M.I.M.I.R - Now with visual intelligence built in for embeddings - MIT licensed - use with local ollama or llama.cpp for full control over your data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/icw2e1nbj43g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p56i8v/mimir_now_with_visual_intelligence_built_in_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p56i8v/mimir_now_with_visual_intelligence_built_in_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T03:29:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5hoqh</id>
    <title>Mistral-Small3.2:latest - Broken after a recent Ollama update?</title>
    <updated>2025-11-24T13:56:05+00:00</updated>
    <author>
      <name>/u/Comfortable_Ad_8117</name>
      <uri>https://old.reddit.com/user/Comfortable_Ad_8117</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p5hoqh/mistralsmall32latest_broken_after_a_recent_ollama/"&gt; &lt;img alt="Mistral-Small3.2:latest - Broken after a recent Ollama update?" src="https://b.thumbs.redditmedia.com/Rw0CGw1zj8CmJZE93FbqE6s_PBI7HNwH1lT2EDsSJss.jpg" title="Mistral-Small3.2:latest - Broken after a recent Ollama update?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone else seen this behavior? Repetitive characters when interacting with the 24b Mistral model? This was working fine until a recent update of Ollama. Any suggestions for an alternative model around the same size that has similar vision capability and instruction following skills. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dzdq4ybhn73g1.png?width=1030&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30993c9fa83fb47de7923593a007e0a3fc7677fa"&gt;https://preview.redd.it/dzdq4ybhn73g1.png?width=1030&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30993c9fa83fb47de7923593a007e0a3fc7677fa&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Ad_8117"&gt; /u/Comfortable_Ad_8117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5hoqh/mistralsmall32latest_broken_after_a_recent_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5hoqh/mistralsmall32latest_broken_after_a_recent_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5hoqh/mistralsmall32latest_broken_after_a_recent_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T13:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5lwse</id>
    <title>Neural Network?</title>
    <updated>2025-11-24T16:40:04+00:00</updated>
    <author>
      <name>/u/spreader123</name>
      <uri>https://old.reddit.com/user/spreader123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p5lwse/neural_network/"&gt; &lt;img alt="Neural Network?" src="https://b.thumbs.redditmedia.com/p3QvsuKFfOO8YLriXQxxpJVaY4BrKd-MO8_AO9SEQSg.jpg" title="Neural Network?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spreader123"&gt; /u/spreader123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/neuralnetworks/comments/1p5jjig/neural_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5lwse/neural_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5lwse/neural_network/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T16:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5jy43</id>
    <title>archgw (0.3.20) - removed all (500mb) python deps in the request path. Ollama and Rust-first now</title>
    <updated>2025-11-24T15:27:32+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/katanemo/archgw"&gt;archgw&lt;/a&gt; (a models-native sidecar proxy for AI agents) offered two capabilities that required loading small LLMs in memory: guardrails to prevent jailbreak attempts, and function-calling for routing requests to the right downstream tool or agent. These built-in features required the project running a thread-safe python process that used libs like transformers, torch, safetensors, etc. 500M in dependencies, not to mention all the security vulnerabilities in the dep tree. Not hating on python, but our GH project was flagged with all sorts of issues.&lt;/p&gt; &lt;p&gt;Those models are loaded as a separate out-of-process server via ollama/lama.cpp which you all know are built in C++/Go. Lighter, faster and safer. And ONLY if the developer uses these features of the product. This meant 9000 lines of less code, a total start time of &amp;lt;2 seconds (vs 30+ seconds), etc.&lt;/p&gt; &lt;p&gt;Why archgw? So that you can build AI agents in any language or framework and offload the plumbing work in AI (like agent routing/hand-off, guardrails, zero-code logs and traces, and a unified API for all LLMs) to a durable piece of infrastructure, deployed as a sidecar.&lt;/p&gt; &lt;p&gt;Proud of this release, so sharing üôè&lt;/p&gt; &lt;p&gt;P.S Sample demos, the CLI and some tests still use python. But we'll move those over to Rust in the coming months. We are punting convenience for robustness.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5jy43/archgw_0320_removed_all_500mb_python_deps_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5jy43/archgw_0320_removed_all_500mb_python_deps_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5jy43/archgw_0320_removed_all_500mb_python_deps_in_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T15:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5lmcw</id>
    <title>Need clarifications or advice with coding and ollama.</title>
    <updated>2025-11-24T16:29:37+00:00</updated>
    <author>
      <name>/u/Lotus-006</name>
      <uri>https://old.reddit.com/user/Lotus-006</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, i'm not sure if tbe models can works like in vscode for coding with ai like claude sonnet or gpt with agent mode.&lt;/p&gt; &lt;p&gt;i tried some days before but the speed to react was slow even with my hardware i9-13900k and 96gb ddr5 at 6800mhz and RTX Msi 5070ti oc 16gb gen 5&lt;/p&gt; &lt;p&gt;i tried with ollama and also on docker desktop but not very usefull as in vscode compared to claude or gpt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lotus-006"&gt; /u/Lotus-006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5lmcw/need_clarifications_or_advice_with_coding_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5lmcw/need_clarifications_or_advice_with_coding_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5lmcw/need_clarifications_or_advice_with_coding_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T16:29:37+00:00</published>
  </entry>
</feed>
