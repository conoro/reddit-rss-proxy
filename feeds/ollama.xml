<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-15T10:50:36+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mpt812</id>
    <title>Could you use RAG and Wikidumps to keep AI in the loop?</title>
    <updated>2025-08-14T06:53:15+00:00</updated>
    <author>
      <name>/u/C_S_Student45</name>
      <uri>https://old.reddit.com/user/C_S_Student45</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/C_S_Student45"&gt; /u/C_S_Student45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/selfhosted/comments/1mpt7tb/could_you_use_rag_and_wikidumps_to_keep_ai_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpt812/could_you_use_rag_and_wikidumps_to_keep_ai_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpt812/could_you_use_rag_and_wikidumps_to_keep_ai_in_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T06:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqa2ol</id>
    <title>Oss 20B is dumb</title>
    <updated>2025-08-14T19:10:58+00:00</updated>
    <author>
      <name>/u/Playful-Jeweler-1601</name>
      <uri>https://old.reddit.com/user/Playful-Jeweler-1601</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mqa2ol/oss_20b_is_dumb/"&gt; &lt;img alt="Oss 20B is dumb" src="https://a.thumbs.redditmedia.com/a14rRih2rGnby2KkZuBcmcAJln9mMcnpJMAjxt0DoA0.jpg" title="Oss 20B is dumb" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/u81m9u5ra1jf1.png?width=1425&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a36e6423961b45c9e15dd80151ce02bcc7d2d0d"&gt;https://preview.redd.it/u81m9u5ra1jf1.png?width=1425&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a36e6423961b45c9e15dd80151ce02bcc7d2d0d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Playful-Jeweler-1601"&gt; /u/Playful-Jeweler-1601 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqa2ol/oss_20b_is_dumb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqa2ol/oss_20b_is_dumb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqa2ol/oss_20b_is_dumb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T19:10:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpqf95</id>
    <title>Pruned GPT-OSS 6.0B kinda works</title>
    <updated>2025-08-14T04:17:01+00:00</updated>
    <author>
      <name>/u/Quiet-Engineer110</name>
      <uri>https://old.reddit.com/user/Quiet-Engineer110</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mpqf95/pruned_gptoss_60b_kinda_works/"&gt; &lt;img alt="Pruned GPT-OSS 6.0B kinda works" src="https://external-preview.redd.it/aaoKLInTgXWvAC3h_YKai0S41TEi4sEQ5dlZR6riJuY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a47d9c1b49d8d7aed9f6e4058ae49360afadc00f" title="Pruned GPT-OSS 6.0B kinda works" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Engineer110"&gt; /u/Quiet-Engineer110 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AmanPriyanshu/gpt-oss-6.0b-specialized-all-pruned-moe-only-7-experts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpqf95/pruned_gptoss_60b_kinda_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpqf95/pruned_gptoss_60b_kinda_works/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T04:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpwni3</id>
    <title>Is there a standard oci image format for models?</title>
    <updated>2025-08-14T10:19:35+00:00</updated>
    <author>
      <name>/u/Grouchy-Friend4235</name>
      <uri>https://old.reddit.com/user/Grouchy-Friend4235</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grouchy-Friend4235"&gt; /u/Grouchy-Friend4235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mpwn3f/is_there_a_standard_oci_image_format_for_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpwni3/is_there_a_standard_oci_image_format_for_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpwni3/is_there_a_standard_oci_image_format_for_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T10:19:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpg9tp</id>
    <title>What are your thoughts on GPT-OSS 120B for programming?</title>
    <updated>2025-08-13T20:51:20+00:00</updated>
    <author>
      <name>/u/ajmusic15</name>
      <uri>https://old.reddit.com/user/ajmusic15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are your thoughts on GPT-OSS 120B for programming? Specifically, how does it compare to a dense model such as Devstral or a MoE model such as Qwen-Coder 30B?&lt;/p&gt; &lt;p&gt;I am running GPT-OSS 120B on my 96 GB DDR5 + RTX 5080 with MoE weight offloading to the CPU (LM Studio does not allow me to specify how many MoE weights I will send to the CPU) and I am having mixed opinions on coding due to censorship (there are certain pentesting tools that I try to use, but I always run into ethical issues and I don't want to waste time on Advanced Prompting).&lt;/p&gt; &lt;p&gt;But anyway, I'm impressed that once the context is processed (which takes ages), the inference starts running at ~20 tk/s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajmusic15"&gt; /u/ajmusic15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpg9tp/what_are_your_thoughts_on_gptoss_120b_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpg9tp/what_are_your_thoughts_on_gptoss_120b_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpg9tp/what_are_your_thoughts_on_gptoss_120b_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T20:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpxpwd</id>
    <title>Looking for an ISP in India that allows server hosting (no static IP needed)</title>
    <updated>2025-08-14T11:16:44+00:00</updated>
    <author>
      <name>/u/Formal_Jeweler_488</name>
      <uri>https://old.reddit.com/user/Formal_Jeweler_488</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm currently exploring internet service providers in India that would let me host my own servers from home. I don‚Äôt need a static IP at the moment‚Äîjust a reliable connection that allows inbound traffic and won‚Äôt block me from serving content externally.&lt;/p&gt; &lt;p&gt;I‚Äôm not looking for anything enterprise-grade, just something solid enough to get my host online and accessible. Preferably something with decent upload speeds and minimal restrictions on port forwarding.&lt;/p&gt; &lt;p&gt;Would love to hear your recommendations on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ISPs that allow this kind of setup&lt;/li&gt; &lt;li&gt;Plans that offer good value for hosting&lt;/li&gt; &lt;li&gt;Any caveats or gotchas I should be aware of&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks in advance for any insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Formal_Jeweler_488"&gt; /u/Formal_Jeweler_488 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpxpwd/looking_for_an_isp_in_india_that_allows_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpxpwd/looking_for_an_isp_in_india_that_allows_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpxpwd/looking_for_an_isp_in_india_that_allows_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T11:16:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqcsbh</id>
    <title>Ollama AI Therapist</title>
    <updated>2025-08-14T20:48:58+00:00</updated>
    <author>
      <name>/u/IWriteTheBuggyCode</name>
      <uri>https://old.reddit.com/user/IWriteTheBuggyCode</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking to set up Ollama to run a local LLM to be a therapist. I have a couple questions.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;What model to use?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How do I make it remember our previous conversations?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Can it be set up to work on speech rather than text?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IWriteTheBuggyCode"&gt; /u/IWriteTheBuggyCode &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqcsbh/ollama_ai_therapist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqcsbh/ollama_ai_therapist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqcsbh/ollama_ai_therapist/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T20:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqb2xa</id>
    <title>Trying to buy a house</title>
    <updated>2025-08-14T19:47:17+00:00</updated>
    <author>
      <name>/u/GBT55</name>
      <uri>https://old.reddit.com/user/GBT55</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I‚Äôm looking for a house to buy (spanish market ü§Æ) with the help of chatGPT deep research.&lt;/p&gt; &lt;p&gt;The thing is I am giving very specific parameters to search only the type of houses i‚Äôm interested in&lt;/p&gt; &lt;p&gt;It is very good but it has a quota limit so I‚Äôm wondering if there‚Äôs any other type of model that can scrape a website with very specific parameters and get actual valid urls &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GBT55"&gt; /u/GBT55 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqb2xa/trying_to_buy_a_house/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqb2xa/trying_to_buy_a_house/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqb2xa/trying_to_buy_a_house/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T19:47:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpmcpe</id>
    <title>AMD Radeon RX 480 8GB benchmark finally working</title>
    <updated>2025-08-14T01:01:02+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mpm728/amd_radeon_rx_480_8gb_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpmcpe/amd_radeon_rx_480_8gb_benchmark_finally_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpmcpe/amd_radeon_rx_480_8gb_benchmark_finally_working/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T01:01:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp00lq</id>
    <title>DataKit + Ollama = Your Data, Your AI, Your Way!</title>
    <updated>2025-08-13T09:45:29+00:00</updated>
    <author>
      <name>/u/Sea-Assignment6371</name>
      <uri>https://old.reddit.com/user/Sea-Assignment6371</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mp00lq/datakit_ollama_your_data_your_ai_your_way/"&gt; &lt;img alt="DataKit + Ollama = Your Data, Your AI, Your Way!" src="https://external-preview.redd.it/MHVraTFpb2RjcmlmMau2qwmbGQWxbzW-5uoWUCNNcejArm0w5kuQ7jVz7rlm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bc53a8330392c0bda995715e81225dfbd789b81" title="DataKit + Ollama = Your Data, Your AI, Your Way!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt; community! Excited to share that DataKit now has native Ollama integration! Run your favorite local AI models directly in your data workflows. 100% Privacy - Your data NEVER leaves your machine. Zero API Costs - No subscriptions, no surprises. No Rate Limits - Process as much as you want. Full Control - Your infrastructure, your rules.&lt;/p&gt; &lt;p&gt;Install Ollama ‚Üí &lt;a href="https://ollama.com"&gt;https://ollama.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Run `OLLAMA_ORIGINS=&amp;quot;&lt;a href="https://datakit.page/"&gt;https://datakit.page&lt;/a&gt;&amp;quot; ollama serve`. Jump on Firefox.&lt;/p&gt; &lt;p&gt;Open DataKit ‚Üí &lt;a href="https://datakit.page"&gt;https://datakit.page&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Start building! - SQL queries + AI, all local&lt;/p&gt; &lt;p&gt;Try it out and let me know what you think! Would love to hear about the workflows you create.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Assignment6371"&gt; /u/Sea-Assignment6371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/whi92hodcrif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mp00lq/datakit_ollama_your_data_your_ai_your_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mp00lq/datakit_ollama_your_data_your_ai_your_way/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T09:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq5pag</id>
    <title>Ollama but for mobile, with a cloud fallback</title>
    <updated>2025-08-14T16:35:38+00:00</updated>
    <author>
      <name>/u/thecoder12322</name>
      <uri>https://old.reddit.com/user/thecoder12322</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;We‚Äôre building something like Ollama, but for mobile. It runs models fully on-device for speed and privacy, and can fall back to the cloud when needed.&lt;/p&gt; &lt;p&gt;I‚Äôd love your feedback ‚Äî especially around how you‚Äôre currently using local LLMs and what features you‚Äôd want on mobile.&lt;/p&gt; &lt;p&gt;üöÄ Check out our Product Hunt launch here: &lt;a href="https://www.producthunt.com/products/runanywhere"&gt;https://www.producthunt.com/products/runanywhere&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We‚Äôre also working on a complete AI voice flow that runs entirely locally (no internet needed) ‚Äî updates coming soon.&lt;/p&gt; &lt;p&gt;Cheers, RunAnywhere Team&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecoder12322"&gt; /u/thecoder12322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq5pag/ollama_but_for_mobile_with_a_cloud_fallback/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq5pag/ollama_but_for_mobile_with_a_cloud_fallback/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mq5pag/ollama_but_for_mobile_with_a_cloud_fallback/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T16:35:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq6itc</id>
    <title>Seeking Feedback on My AI Inference PC Build</title>
    <updated>2025-08-14T17:05:13+00:00</updated>
    <author>
      <name>/u/nightcrawler2164</name>
      <uri>https://old.reddit.com/user/nightcrawler2164</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nightcrawler2164"&gt; /u/nightcrawler2164 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/PcBuild/comments/1mq6gvc/seeking_feedback_on_my_ai_inference_pc_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq6itc/seeking_feedback_on_my_ai_inference_pc_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mq6itc/seeking_feedback_on_my_ai_inference_pc_build/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T17:05:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpqopt</id>
    <title>Making your prompts better with GEPA-Lite using Ollama!</title>
    <updated>2025-08-14T04:31:22+00:00</updated>
    <author>
      <name>/u/AnyIce3007</name>
      <uri>https://old.reddit.com/user/AnyIce3007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link: &lt;a href="https://github.com/egmaminta/GEPA-Lite"&gt;https://github.com/egmaminta/GEPA-Lite&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;ForTheLoveOfCode&lt;/h1&gt; &lt;p&gt;GEPA-Lite is a lightweight implementation based on the proposed GEPA prompt optimization method that is custom fit for single-task applications. It's built on the core principle of LLM self-reflection, self-improvement, streamlined.&lt;/p&gt; &lt;p&gt;Developed in the spirit of open-source initiatives like Google Summer of Code 2025 and For the Love of Code 2025, this project leverages Gemma (ollama::gemma3n:e4b) as its core model. The project also offers optional support for the Gemini API, allowing access to powerful models like gemini-2.5-flash-lite, gemini-2.5-flash, and gemini-2.5-pro.&lt;/p&gt; &lt;p&gt;Feel free to check it out. I'd also appreciate if you can give a Star ‚≠êÔ∏è!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnyIce3007"&gt; /u/AnyIce3007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpqopt/making_your_prompts_better_with_gepalite_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpqopt/making_your_prompts_better_with_gepalite_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpqopt/making_your_prompts_better_with_gepalite_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T04:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq2e83</id>
    <title>ollama local model slow</title>
    <updated>2025-08-14T14:34:07+00:00</updated>
    <author>
      <name>/u/Designer_Addendum69</name>
      <uri>https://old.reddit.com/user/Designer_Addendum69</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Designer_Addendum69"&gt; /u/Designer_Addendum69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/CLine/comments/1mq2dzw/ollama_local_model_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq2e83/ollama_local_model_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mq2e83/ollama_local_model_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T14:34:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq5ao9</id>
    <title>Run models on Android.</title>
    <updated>2025-08-14T16:20:50+00:00</updated>
    <author>
      <name>/u/Cobalt_Astronomer</name>
      <uri>https://old.reddit.com/user/Cobalt_Astronomer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any software like ollama or lm studio to run models on Android. I have a phone with decent specifications.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cobalt_Astronomer"&gt; /u/Cobalt_Astronomer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq5ao9/run_models_on_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq5ao9/run_models_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mq5ao9/run_models_on_android/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T16:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqeo2s</id>
    <title>Ollama AI Life Coach</title>
    <updated>2025-08-14T21:58:48+00:00</updated>
    <author>
      <name>/u/setentaydos</name>
      <uri>https://old.reddit.com/user/setentaydos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by another post in which OP asked to setup an AI Therapist (please don‚Äôt do this, go with a professional), I wondered about this use case of leveraging AI as a life coach in career, personal finance, and other topics.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; ‚Å†What model to use?&lt;/li&gt; &lt;li&gt; ‚Å†How do I make it remember our previous conversations?&lt;/li&gt; &lt;li&gt; ‚Å†Can it be set up to work on speech rather than text?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôm on a MacBook Pro, M4, 24Gb Ram, I can‚Äôt run beefy models, but the questions above can point me to ways of doing an efficient use of models in general. TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/setentaydos"&gt; /u/setentaydos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqeo2s/ollama_ai_life_coach/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqeo2s/ollama_ai_life_coach/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqeo2s/ollama_ai_life_coach/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T21:58:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqat3c</id>
    <title>M1 Pro MacBook with 16 GB of RAM</title>
    <updated>2025-08-14T19:37:11+00:00</updated>
    <author>
      <name>/u/CobusGreyling</name>
      <uri>https://old.reddit.com/user/CobusGreyling</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best model I can run with reasonable latency? I pulled and ran the GPT-OSS-30b model and inference is excruciating slow...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CobusGreyling"&gt; /u/CobusGreyling &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqat3c/m1_pro_macbook_with_16_gb_of_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqat3c/m1_pro_macbook_with_16_gb_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqat3c/m1_pro_macbook_with_16_gb_of_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T19:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpi9rq</id>
    <title>I just had my first contributor to my open source AI coding agent and it feels great!</title>
    <updated>2025-08-13T22:08:51+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mpi9rq/i_just_had_my_first_contributor_to_my_open_source/"&gt; &lt;img alt="I just had my first contributor to my open source AI coding agent and it feels great!" src="https://preview.redd.it/xat5ofgh1vif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=814ecd9f096ab0c04978681b31dddbd4374c1779" title="I just had my first contributor to my open source AI coding agent and it feels great!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week I released a rough-around-the-edges open source AI coding agent that runs in your terminal through Ollama and OpenRouter as well as any OpenAI compatible API. I published about wanting to grow it into a community and after a couple days I had my first contributor with a pull request adding some amazing features! &lt;/p&gt; &lt;p&gt;As my first proper open source project (normally I've built closed source as part of my day job), to get people taking an interest enough to star, fork and contribute is an incredible feeling, even if it is very early days!&lt;/p&gt; &lt;p&gt;This project is totally free and I want to build a community around it. I believe access to AI to help people create should be available to everyone for free and not necessarily controlled by big companies.&lt;/p&gt; &lt;p&gt;I would love your help! Whether you're interested in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding support for new AI providers&lt;/li&gt; &lt;li&gt;Improving tool functionality&lt;/li&gt; &lt;li&gt;Enhancing the user experience&lt;/li&gt; &lt;li&gt;Writing documentation&lt;/li&gt; &lt;li&gt;Reporting bugs or suggesting features&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All contributions are welcome! Here is the link if you're interested: &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But yes, this post is just me celebrating üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xat5ofgh1vif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpi9rq/i_just_had_my_first_contributor_to_my_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpi9rq/i_just_had_my_first_contributor_to_my_open_source/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T22:08:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqh7jt</id>
    <title>Dose feeding LLM the framework Documentation give better results?</title>
    <updated>2025-08-14T23:40:23+00:00</updated>
    <author>
      <name>/u/AbdullahZeine</name>
      <uri>https://old.reddit.com/user/AbdullahZeine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i am thinking if i can do RAG for my tech stack documentation and connected with Ollama response and see how will 8b model could go am curious if someone try what am thinking about and what results &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AbdullahZeine"&gt; /u/AbdullahZeine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqh7jt/dose_feeding_llm_the_framework_documentation_give/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqh7jt/dose_feeding_llm_the_framework_documentation_give/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqh7jt/dose_feeding_llm_the_framework_documentation_give/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T23:40:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq6pi7</id>
    <title>Ollama but for realtime Speech-to-Text</title>
    <updated>2025-08-14T17:11:42+00:00</updated>
    <author>
      <name>/u/AggressiveHunt2300</name>
      <uri>https://old.reddit.com/user/AggressiveHunt2300</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Docs: &lt;a href="https://docs.hyprnote.com/owhisper/what-is-this"&gt;https://docs.hyprnote.com/owhisper/what-is-this&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CLI Demo: &lt;a href="https://asciinema.org/a/733110"&gt;https://asciinema.org/a/733110&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quick Start:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;brew tap fastrepl/hyprnote &amp;amp;&amp;amp; brew install owhisper owhisper pull whisper-cpp-base-q8-en owhisper run whisper-cpp-base-q8-en &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Other model like moonshine is also supported)&lt;/p&gt; &lt;p&gt;Love to hear what you guys think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggressiveHunt2300"&gt; /u/AggressiveHunt2300 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq6pi7/ollama_but_for_realtime_speechtotext/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq6pi7/ollama_but_for_realtime_speechtotext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mq6pi7/ollama_but_for_realtime_speechtotext/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T17:11:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq1izk</id>
    <title>Easy RAG using Ollama</title>
    <updated>2025-08-14T14:01:12+00:00</updated>
    <author>
      <name>/u/gogozad</name>
      <uri>https://old.reddit.com/user/gogozad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama people,&lt;/p&gt; &lt;p&gt;I am the author of &lt;a href="https://github.com/ggozad/oterm"&gt;oterm&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/ggozad/haiku.rag"&gt;haiku.rag&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I created an &lt;a href="https://ggozad.github.io/oterm/rag_example"&gt;example&lt;/a&gt; on how to combine these two to get fully local RAG, running on Ollama and without the need of external vector databases or servers other than Ollama. &lt;/p&gt; &lt;p&gt;You can see a demo and detailed instructions at the &lt;code&gt;oterm&lt;/code&gt;s &lt;a href="https://ggozad.github.io/oterm/rag_example"&gt;docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking forward to your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gogozad"&gt; /u/gogozad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq1izk/easy_rag_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq1izk/easy_rag_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mq1izk/easy_rag_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T14:01:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqrnz6</id>
    <title>ollama in window11 with rx6600</title>
    <updated>2025-08-15T08:23:53+00:00</updated>
    <author>
      <name>/u/Anxious_Scarcity_250</name>
      <uri>https://old.reddit.com/user/Anxious_Scarcity_250</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;5600x / 32GB ram / rx6600 8GB &lt;/p&gt; &lt;p&gt;I couldn't use my rx6600 with ollama app version -the latest. It was CPU 100%.&lt;/p&gt; &lt;p&gt;Finallly It works with open-webui and little old version of ollama. Some file replacement for amd rocm needed. check below.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ByronLeeeee/Ollama-For-AMD-Installer/releases"&gt;https://github.com/ByronLeeeee/Ollama-For-AMD-Installer/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It works with gpt-oss 20b for maximum, but answers slow. And if just after using other models and RAM is not free enough, It cause ollama down. CPU/GPU 50/50.&lt;/p&gt; &lt;p&gt;-Good to use&lt;/p&gt; &lt;p&gt;Qwen3:8b-q4_K_M. 5.2GB GPU 100%. Qwen3:14b-q4_K_M. 9.3GB CPU/GPU 27%/73% Gemma3:12b-it-q4_K_M. 8.1GB CPU/GPU 32%/68%&lt;/p&gt; &lt;p&gt;Ratio changed as session get longer. Cpu works much.&lt;/p&gt; &lt;p&gt;-And smaller models &lt;/p&gt; &lt;p&gt;Fast, but ust available.&lt;/p&gt; &lt;p&gt;-Works, but Sucks&lt;/p&gt; &lt;p&gt;exaone-deep , clova-x-seed&lt;/p&gt; &lt;p&gt;ÏóòÏßÄ, ÎÑ§Ïù¥Î≤Ñ ÎÑàÎÑ® „ÖÖ„ÖÇ Í∞àÍ∏∏Ïù¥ Î©ÄÎã§. Î†àÎîßÏùÄ Ï§ÑÎ∞îÍæ∏Í∏∞Í∞Ä Ï†úÎ©ãÎåÄÎ°úÎÑ§.&lt;/p&gt; &lt;p&gt;Thank you for watching.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxious_Scarcity_250"&gt; /u/Anxious_Scarcity_250 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqrnz6/ollama_in_window11_with_rx6600/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqrnz6/ollama_in_window11_with_rx6600/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqrnz6/ollama_in_window11_with_rx6600/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T08:23:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqr4st</id>
    <title>Speculative decoding via Arch (candidate release 0.4.0) - requesting feedback.</title>
    <updated>2025-08-15T07:54:05+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mqr4st/speculative_decoding_via_arch_candidate_release/"&gt; &lt;img alt="Speculative decoding via Arch (candidate release 0.4.0) - requesting feedback." src="https://preview.redd.it/eh87ityv25jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5437d1a2a47a9727494605a6c6a4e3b9e7fdbccb" title="Speculative decoding via Arch (candidate release 0.4.0) - requesting feedback." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are gearing up for a pretty big release and looking for feedback. One of the advantages in being a &lt;a href="https://github.com/katanemo/archgw"&gt;universal access&lt;/a&gt; layer for LLMs is that you can do some smarts that can help all developers build faster and more responsive agentic UX. The feature we are building and exploring with design partner is first-class support for speculative decoding.&lt;/p&gt; &lt;p&gt;Speculative decoding is a technique whereby a draft model (usually smaller) is engaged to produce tokens and the candidate set is verified by a target model. The set of candidate tokens produced by a draft model can be verified via logits by the target model, and verification can happen in parallel (each token in the sequence produced can be verified concurrently) to speed response time.&lt;/p&gt; &lt;p&gt;This is what OpenAI uses to accelerate the speed of its responses especially in cases where outputs can be guaranteed to come from the same distribution. The user experience could be something along the following lines or it be configured once per model. Here the draft_window is the number of tokens to verify, the max_accept_run tells us after how many failed verifications should we give up and just send all the remaining traffic to the target model etc.&lt;/p&gt; &lt;p&gt;Of course this work assumes a low RTT between the target and draft model so that speculative decoding is faster without compromising quality.&lt;/p&gt; &lt;p&gt;Question: would you want to improve the latency of responses, lower your token cost, and how do you feel about this functionality. Or would you want something simpler?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;POST /v1/chat/completions { &amp;quot;model&amp;quot;: &amp;quot;target:gpt-large@2025-06&amp;quot;, &amp;quot;speculative&amp;quot;: { &amp;quot;draft_model&amp;quot;: &amp;quot;draft:small@v3&amp;quot;, &amp;quot;max_draft_window&amp;quot;: 8, &amp;quot;min_accept_run&amp;quot;: 2, &amp;quot;verify_logprobs&amp;quot;: false }, &amp;quot;messages&amp;quot;: [...], &amp;quot;stream&amp;quot;: true } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eh87ityv25jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqr4st/speculative_decoding_via_arch_candidate_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqr4st/speculative_decoding_via_arch_candidate_release/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T07:54:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqrv10</id>
    <title>Isn't Ollama Turbo exactly the one thing that one tried to avoid by chasing Ollama in the first place?</title>
    <updated>2025-08-15T08:35:31+00:00</updated>
    <author>
      <name>/u/Tasty-Base-9900</name>
      <uri>https://old.reddit.com/user/Tasty-Base-9900</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry typo in the title... should be choosing not chasing ;-)&lt;/p&gt; &lt;p&gt;Imho the biggest selling point for Ollama is that one can run one's models locally or within one's own infrastructure so one doesn't have to trust an external infrastructure provider with say one's data. Doesn't Ollama Turbo run exactly against this philosophy?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tasty-Base-9900"&gt; /u/Tasty-Base-9900 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqrv10/isnt_ollama_turbo_exactly_the_one_thing_that_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqrv10/isnt_ollama_turbo_exactly_the_one_thing_that_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqrv10/isnt_ollama_turbo_exactly_the_one_thing_that_one/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T08:35:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqk7wk</id>
    <title>Local Open Source Alternative to NotebookLM</title>
    <updated>2025-08-15T01:53:03+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a &lt;strong&gt;Highly Customizable AI Research Agent&lt;/strong&gt; that connects to your personal external sources and Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;üìä &lt;strong&gt;Featu&lt;/strong&gt;res&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üéôÔ∏è &lt;strong&gt;Podca&lt;/strong&gt;sts&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Support for local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt; &lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt; &lt;li&gt;Multiple TTS providers supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ÑπÔ∏è &lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search Engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Jira&lt;/li&gt; &lt;li&gt;ClickUp&lt;/li&gt; &lt;li&gt;Confluence&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;Youtube Videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;Discord&lt;/li&gt; &lt;li&gt;and more to come.....&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üîñ &lt;strong&gt;Cross-Browser Extens&lt;/strong&gt;ion&lt;/p&gt; &lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqk7wk/local_open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqk7wk/local_open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqk7wk/local_open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T01:53:03+00:00</published>
  </entry>
</feed>
