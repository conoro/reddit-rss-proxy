<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-09T23:36:17+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ltlgag</id>
    <title>OrangePi Zero 3 runs Ollama</title>
    <updated>2025-07-07T04:53:36+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those that are curious about running LLM on &lt;a href="https://en.wikipedia.org/wiki/Single-board_computer"&gt;SBC&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Here is &lt;a href="http://www.orangepi.org/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-Zero-3.html"&gt;Orange Pi Zero 3&lt;/a&gt; (aarch64) packed with 4gb DDR4 running Debian 12 'Bookworm'/ &lt;a href="https://dietpi.com/#home"&gt;DietPi&lt;/a&gt; using &lt;code&gt;ollama -v&lt;/code&gt; 0.9.5&lt;/p&gt; &lt;p&gt;I even used &lt;a href="https://ollama.com/library/llama3.2:1b"&gt;llama3.2:1b&lt;/a&gt; to create this markdown table:&lt;/p&gt; &lt;p&gt;*Eval Rate Tokens per Second is average of 3 runs.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MODEL&lt;/th&gt; &lt;th align="left"&gt;SIZE GB&lt;/th&gt; &lt;th align="left"&gt;EVAL RATE TS/S&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:1b&lt;/td&gt; &lt;td align="left"&gt;1.4&lt;/td&gt; &lt;td align="left"&gt;3.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama3.2:1b&lt;/td&gt; &lt;td align="left"&gt;2.2&lt;/td&gt; &lt;td align="left"&gt;3.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5:1.5b-instruct-q5_K_M&lt;/td&gt; &lt;td align="left"&gt;1.7&lt;/td&gt; &lt;td align="left"&gt;2.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tinydolphin:1.1b-v2.8-q6_K&lt;/td&gt; &lt;td align="left"&gt;1.6&lt;/td&gt; &lt;td align="left"&gt;2.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tinyllama:1.1b-chat-v1-q6_K&lt;/td&gt; &lt;td align="left"&gt;1.3&lt;/td&gt; &lt;td align="left"&gt;2.52&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Here is the &lt;code&gt;ollama run --verbose llama3.2:1b&lt;/code&gt; numbers from creating markdown table&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Duration&lt;/td&gt; &lt;td align="left"&gt;2m54.721763625s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Load Duration&lt;/td&gt; &lt;td align="left"&gt;41.594289562s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt Eval Count&lt;/td&gt; &lt;td align="left"&gt;389 token(s)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt Eval Duration&lt;/td&gt; &lt;td align="left"&gt;1m17.397468287s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt Eval Rate&lt;/td&gt; &lt;td align="left"&gt;5.03 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Eval Count&lt;/td&gt; &lt;td align="left"&gt;163 token(s)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Eval Duration&lt;/td&gt; &lt;td align="left"&gt;55.571782235s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Eval Rate&lt;/td&gt; &lt;td align="left"&gt;2.93 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I was able to run &lt;a href="https://ollama.com/library/llama3.2:3b-instruct-q5_K_M"&gt;llama3.2:3b-instruct-q5_K_M&lt;/a&gt; and &lt;code&gt;ollama ps&lt;/code&gt; reported 4.0GB usage. Eval Rate dropped to 1.21 Tokens/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltlgag/orangepi_zero_3_runs_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltlgag/orangepi_zero_3_runs_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltlgag/orangepi_zero_3_runs_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T04:53:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltqs3f</id>
    <title>should i replace gemma 3?</title>
    <updated>2025-07-07T10:34:52+00:00</updated>
    <author>
      <name>/u/Otherwise-Brick4923</name>
      <uri>https://old.reddit.com/user/Otherwise-Brick4923</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi everyone,&lt;/strong&gt;&lt;br /&gt; I'm trying to create a workflow that can check a client's order against the supplier's order confirmation for any discrepancies. Everything is working quite well so far, but when I started testing the system by intentionally introducing errors, Gemma simply ignored them.&lt;/p&gt; &lt;p&gt;For example:&lt;br /&gt; The client's name is &lt;strong&gt;Lius&lt;/strong&gt;, but I entered &lt;strong&gt;Dius&lt;/strong&gt;, and Gemma marked it as correct.&lt;/p&gt; &lt;p&gt;Now I'm considering switching to the new &lt;strong&gt;Gemma 3n&lt;/strong&gt;, hoping it might perform better.&lt;/p&gt; &lt;p&gt;Has anyone experienced something similar or have an idea why Gemma isn't recognizing these errors?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise-Brick4923"&gt; /u/Otherwise-Brick4923 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltqs3f/should_i_replace_gemma_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltqs3f/should_i_replace_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltqs3f/should_i_replace_gemma_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T10:34:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lud29c</id>
    <title>please critique my python ollama api that interfaces with a bash terminal</title>
    <updated>2025-07-08T02:20:15+00:00</updated>
    <author>
      <name>/u/printingbooks</name>
      <uri>https://old.reddit.com/user/printingbooks</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://pastebin.com/HnTg2M6X"&gt;https://pastebin.com/HnTg2M6X&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ask me questions if you want. it isnt totally complete. devstral outputs JSON coded stuff like indicating if somthing is a command to a chat message or even a keystroke(but this isnt fully implemented yet)&lt;/p&gt; &lt;p&gt;thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/printingbooks"&gt; /u/printingbooks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lud29c/please_critique_my_python_ollama_api_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lud29c/please_critique_my_python_ollama_api_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lud29c/please_critique_my_python_ollama_api_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T02:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1luhl9d</id>
    <title>Ollama still using cuda even after replacing gpu</title>
    <updated>2025-07-08T06:33:52+00:00</updated>
    <author>
      <name>/u/KoftaBozo2235</name>
      <uri>https://old.reddit.com/user/KoftaBozo2235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used to have llama3.1 running in Ubuntu WSL on an rtx 4070, but now ive replaced it with a 9070xt and it wont work on the gpu no matter what i do. I've installed rocm, set environment variables, tried uninstalling nvidia libraries, but it still shows supported_gpu=0 whenever i run serve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KoftaBozo2235"&gt; /u/KoftaBozo2235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luhl9d/ollama_still_using_cuda_even_after_replacing_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luhl9d/ollama_still_using_cuda_even_after_replacing_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1luhl9d/ollama_still_using_cuda_even_after_replacing_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T06:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1luhllb</id>
    <title>Ollama using GPU when run standalone but CPU when run through Llamaindex?</title>
    <updated>2025-07-08T06:34:28+00:00</updated>
    <author>
      <name>/u/Neogohan1</name>
      <uri>https://old.reddit.com/user/Neogohan1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I'm just trying to go through initial setup of llamaindex using ollama running the following code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from llama_index.llms.ollama import Ollama llm=Ollama(model=&amp;quot;deepseek-r1&amp;quot;,request_timeout=360.0) resp = llm.complete(&amp;quot;Who is Paul Graham?&amp;quot;) print(resp) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When I run this i can see my RAM and CPU going up but GPU stays 0%.&lt;/p&gt; &lt;p&gt;However if I open a cmd prompt and just use &amp;quot;ollama run deepseek-r1&amp;quot; and prompt the model there, i can see it runs on GPU at like 30%, and is much faster. Is there a way to ensure it runs on GPU when I use it as part of a python script/using llamaindex?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neogohan1"&gt; /u/Neogohan1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luhllb/ollama_using_gpu_when_run_standalone_but_cpu_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luhllb/ollama_using_gpu_when_run_standalone_but_cpu_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1luhllb/ollama_using_gpu_when_run_standalone_but_cpu_when/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T06:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lttm5g</id>
    <title>Want to create a private LLM for ingesting engineering handbooks &amp; IP.</title>
    <updated>2025-07-07T13:03:07+00:00</updated>
    <author>
      <name>/u/The_ZMD</name>
      <uri>https://old.reddit.com/user/The_ZMD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to create a ollama-private gpt on my pc. This will be primarily used to ingest couple of engineering handbook so that it can understand some technical stuff. Some of my research papers, subjects/books I read for education, so it knows what I know and what I don't know.&lt;/p&gt; &lt;p&gt;Additonally I need it to compare multiple vendor data, give me best option do some basic analysis, generate report, etc. Do I need to start from scratch or something similar exists? Like a pre trained neural netrowk (like Physics inspired neural network)?&lt;/p&gt; &lt;p&gt;PC specs: 10850k, 32 gb ram, 6900xt, multiple gen 4 ssd and hdd.&lt;/p&gt; &lt;p&gt;Any help is appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_ZMD"&gt; /u/The_ZMD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lttm5g/want_to_create_a_private_llm_for_ingesting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lttm5g/want_to_create_a_private_llm_for_ingesting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lttm5g/want_to_create_a_private_llm_for_ingesting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T13:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lujfpb</id>
    <title>Ollama Auto Start Despite removed from "Open at Login"</title>
    <updated>2025-07-08T08:39:22+00:00</updated>
    <author>
      <name>/u/frozencoconut03</name>
      <uri>https://old.reddit.com/user/frozencoconut03</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lujfpb/ollama_auto_start_despite_removed_from_open_at/"&gt; &lt;img alt="Ollama Auto Start Despite removed from &amp;quot;Open at Login&amp;quot;" src="https://b.thumbs.redditmedia.com/H5gbzUNdSAKkhBj3cvclCDiU4XP-WvIW5b5y17kVmJs.jpg" title="Ollama Auto Start Despite removed from &amp;quot;Open at Login&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am on a Mac, and for whatever reason, Ollama starts auto starting when I log in to my Mac, despite it not being in the &amp;quot;Open at Login&amp;quot; section. Anyway to fix it?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jv4v2cyb4mbf1.png?width=944&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9febfaccabf15413a49a4183951c308b9e6c9743"&gt;https://preview.redd.it/jv4v2cyb4mbf1.png?width=944&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9febfaccabf15413a49a4183951c308b9e6c9743&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frozencoconut03"&gt; /u/frozencoconut03 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lujfpb/ollama_auto_start_despite_removed_from_open_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lujfpb/ollama_auto_start_despite_removed_from_open_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lujfpb/ollama_auto_start_despite_removed_from_open_at/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T08:39:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lujrg3</id>
    <title>Tool calls issue since v0.8.0</title>
    <updated>2025-07-08T09:01:45+00:00</updated>
    <author>
      <name>/u/awolCZ</name>
      <uri>https://old.reddit.com/user/awolCZ</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, &lt;/p&gt; &lt;p&gt;we are having some issues with gemma3 tools model (PetrosStav) since Ollama v0.8.0. Any help would be appreciated because we are struggling with this for some time.&lt;/p&gt; &lt;p&gt;In v0.7.1, which is the last version which works as expected for us with PetrosStav/gemma3-tools model, tool calls are correctly returned in json parameter - tool_calls. But in 0.8.0, tools calls are returned in content of the message, like this:&lt;br /&gt; {&amp;quot;role&amp;quot;:&amp;quot;assistant&amp;quot;,&amp;quot;content&amp;quot;:&amp;quot;```tool_call\n{\&amp;quot;name\&amp;quot;: \&amp;quot;filterData\&amp;quot;, \&amp;quot;parameters\&amp;quot;: {\&amp;quot;start_datetime\&amp;quot;: \&amp;quot;2025-07-08T00:00:00+02:00\&amp;quot;, \&amp;quot;end_datetime\&amp;quot;: \&amp;quot;2025-07-08T23:59:59+02:00\&amp;quot;}}\n```&amp;quot;}&lt;/p&gt; &lt;p&gt;I'm not sure what exactly changed as changelog was mentioning tool calls streaming only, but it seems like Modelfile of gemma3-tools model somehow became incompatible with Ollama 0.8.0+&lt;/p&gt; &lt;p&gt;Any advice on how to fix this? &lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/awolCZ"&gt; /u/awolCZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lujrg3/tool_calls_issue_since_v080/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lujrg3/tool_calls_issue_since_v080/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lujrg3/tool_calls_issue_since_v080/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T09:01:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lujtdv</id>
    <title>A Good LLM for Python.</title>
    <updated>2025-07-08T09:05:31+00:00</updated>
    <author>
      <name>/u/SultanGreat</name>
      <uri>https://old.reddit.com/user/SultanGreat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a mac m1 mini 8gb and I want the best possible programming (python) llm. So far I tried gemma, llama, deepseek-coder, codellama-pyrhon and a lot more. Some didn't run smoothly others were worse&lt;/p&gt; &lt;p&gt;Currently am using qwen2.5-code 7b, which is good but I want a python focussed llm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SultanGreat"&gt; /u/SultanGreat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lujtdv/a_good_llm_for_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lujtdv/a_good_llm_for_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lujtdv/a_good_llm_for_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T09:05:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lujwy0</id>
    <title>Being a psychologist to your (over)thinking LLM</title>
    <updated>2025-07-08T09:12:20+00:00</updated>
    <author>
      <name>/u/specy_dev</name>
      <uri>https://old.reddit.com/user/specy_dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How reasoning models tend to overthink and why they are not always the best choice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/specy_dev"&gt; /u/specy_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://specy.app/blog/posts/being-a-psychologist-to-your-overthinking-llm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lujwy0/being_a_psychologist_to_your_overthinking_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lujwy0/being_a_psychologist_to_your_overthinking_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T09:12:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lut5wd</id>
    <title>Haunted by the llama</title>
    <updated>2025-07-08T16:30:03+00:00</updated>
    <author>
      <name>/u/frozencoconut03</name>
      <uri>https://old.reddit.com/user/frozencoconut03</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am on a Mac, and I have a problem with Ollama autostarting despite not being under the Open at Login tab. Tried a few fixes, but nothing works, so I figured I'd uninstall it completely since I have completed my project. Hence, I deleted it from the Application folder, deleted the ~/.ollama, and on restart.... THE OLLAMA IS BACK THERE STARING AT ME, ASKING ME TO ADD IT BACK TO APPLICATION AS IT RUNS BETTER THERE??? Bro idk, I have tried googling but found no solution. Please save me from this nightmare&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frozencoconut03"&gt; /u/frozencoconut03 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lut5wd/haunted_by_the_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lut5wd/haunted_by_the_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lut5wd/haunted_by_the_llama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T16:30:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1luwnt5</id>
    <title>Ollama models for debugging code</title>
    <updated>2025-07-08T18:42:44+00:00</updated>
    <author>
      <name>/u/uncager</name>
      <uri>https://old.reddit.com/user/uncager</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a fairly small TSQL stored procedure but I noticed I had a bug in it. Before I fixed it, I thought I'd run it by some local ollama models, asking them to find any bugs. I tried:&lt;br /&gt; qwen2.5-coder:14b&lt;br /&gt; deepseek-coder-v2:16b&lt;br /&gt; codellama:13b&lt;br /&gt; sqlcoder:15b&lt;br /&gt; NONE of them caught the bug, although they all babbled about better parameter value checking and error catching and logging and a lot more useless garbage that I didn't ask for. I asked Claude and it pointed out the bug right away. I was really hoping to be able to run AI locally for debugging source code I'd rather not upload to some service for some employee there to get to see. Too soon? Or is there some way now to get Claude-level smarts locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uncager"&gt; /u/uncager &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luwnt5/ollama_models_for_debugging_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luwnt5/ollama_models_for_debugging_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1luwnt5/ollama_models_for_debugging_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T18:42:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lucq0b</id>
    <title>codex-&gt;ollama (airgapped)</title>
    <updated>2025-07-08T02:03:39+00:00</updated>
    <author>
      <name>/u/neurostream</name>
      <uri>https://old.reddit.com/user/neurostream</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lucq0b/codexollama_airgapped/"&gt; &lt;img alt="codex-&amp;gt;ollama (airgapped)" src="https://external-preview.redd.it/R2JhsIcLOAV6dx1FDhG0En51rNtJ_9CXmjw-Xp6cleg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52a869c1d852237aad81466ecf37b39fd9c9cb4e" title="codex-&amp;gt;ollama (airgapped)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;it's been out there that openai's codex cli agent now has support for other providers, and it also works with local ollama.&lt;/p&gt; &lt;p&gt;trying it out was less involved than i thought. there's no OpenAI account settings, bindings, tokens, or registration cookie calls... it just works like any other shell command.&lt;/p&gt; &lt;p&gt;you set the model name (from your &amp;quot;ollama ls&amp;quot; output) and local ollama port with &amp;quot;codex --config&amp;quot; options (see example below).&lt;/p&gt; &lt;p&gt;&lt;em&gt;installing&lt;/em&gt; download the cli for your os/arch (you can brew install codex on macos). i extracted codex-exec-x86_64-unknown-linux-gnu.tar.gz for my ubuntu thinkpad and renamed it &amp;quot;codex&amp;quot;. &lt;/p&gt; &lt;p&gt;same with codex-exec and code-linux-sandbox (not sure if all 3 are required or just the main codex util, but i just put them all in the PATH.&lt;/p&gt; &lt;p&gt;&lt;em&gt;internet access/airgapping&lt;/em&gt;&lt;/p&gt; &lt;p&gt;internet route from the machine running it isn't required. but you might end up using it in an internet workflow where codex might, for example, use curl to trigger a remote webhook or git to push a branch to your remote repo.&lt;/p&gt; &lt;p&gt;&lt;em&gt;example&lt;/em&gt; shell&amp;gt; cd myrepo shell&amp;gt; codex exec --config model_provider=ollama --config model_providers.ollama.base_url=&lt;a href="http://127.0.01:11423/v1"&gt;http://127.0.01:11423/v1&lt;/a&gt; --config model=qwen3:235b-a22b-q8_0 &amp;quot;summarize what this whole code repo is about&amp;quot;&lt;/p&gt; &lt;p&gt;codex will run shell commands from the current folder to figure it out.. like ls, find , cat, and grep. it outputs the response (describing the repo, in this case) to stdout and returns to the shell prompt.&lt;/p&gt; &lt;p&gt;leave off the &amp;quot;exec&amp;quot; to start in terminal UI mode, which can you supervise tasks in continuous context and without scripting. but i think many will find the power for complex projects is in chaining codex runs together with scripts (like piping a codex exec output back into codex, etc).&lt;/p&gt; &lt;p&gt;you can create a -/.codex/config.toml file and move the --config switches there to keep your command line clean. There are more configuration options (like setting the context size) documented in the github repo for codex.&lt;/p&gt; &lt;p&gt;&lt;em&gt;read/write and allowed shell commands&lt;/em&gt; that example above is &amp;quot;read only&amp;quot;, but for read-write look at &amp;quot;codex help&amp;quot; to see the &amp;quot;--dangerously&amp;quot; switch, which overrides all the &lt;em&gt;sandboxing&lt;/em&gt; and &lt;em&gt;approval&lt;/em&gt; policies (the actual configuration topics that switch should bring your attention to for safe use). then, your prompts can make/update/delete files (code, scripts, documentation, etc) and folders and even run other commands.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Tool calling models and MCP&lt;/em&gt; the model you set has to support tool calling, and i also prefer reasoning models - which significantly narrows down the available options for tools+thinking models i'd &amp;quot;ollama pull&amp;quot; for this. but i've only been able to get qwen3 to be consistent. (anyone know how make other tool models get along with codex better? deepseek-r1 sometimes works) &lt;/p&gt; &lt;p&gt;the latest codex releases also supports using codex as an both an mcp server and mcp client - which i don't know how to do yet (help?); but that might stabilize the consistency across different tool-enabled models.&lt;/p&gt; &lt;p&gt;&lt;em&gt;one-off codex runs vs codexes of codexes of codexes&lt;/em&gt; I think working with smaller models locally will mean less &amp;quot;build huge app in one prompt while i sleep&amp;quot; -type of magical experiences rn. So I'm expecting to decompose my projects and workflows with a bunch of smaller codex script modules. i've also never used langchain or langraph, but maybe harnessing codex with those frameworks is where i should look next? &lt;/p&gt; &lt;p&gt;i'm a more of network cable infra monkey irl , so i hope this clicks with those who are coming from where i'm at.&lt;/p&gt; &lt;p&gt;&lt;em&gt;TL;DR&lt;/em&gt; you can run: &lt;/p&gt; &lt;p&gt;&lt;em&gt;codex &amp;quot;summarize the git history of this branch&amp;quot;&lt;/em&gt; &lt;/p&gt; &lt;p&gt;and it works with local ollama tool models without talking to openai by putting &lt;a href="http://127.0.01:11423/v1"&gt;http://127.0.01:11423/v1&lt;/a&gt; and the model name (like qwen3) in the config.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neurostream"&gt; /u/neurostream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/openai/codex/releases"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lucq0b/codexollama_airgapped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lucq0b/codexollama_airgapped/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T02:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv4o6y</id>
    <title>Why is this model from HF telling me it's a boy or girl or man or woman then goes on an endless rant?</title>
    <updated>2025-07-09T00:12:12+00:00</updated>
    <author>
      <name>/u/omni_shaNker</name>
      <uri>https://old.reddit.com/user/omni_shaNker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying different models from HF, for example:&lt;br /&gt; &lt;a href="https://huggingface.co/TheBloke/law-LLM-GGUF/tree/main"&gt;https://huggingface.co/TheBloke/law-LLM-GGUF/tree/main&lt;/a&gt;&lt;br /&gt; and I do&lt;br /&gt; ollama run &lt;a href="http://hf.co/TheBloke/law-LLM-GGUF"&gt;hf.co/TheBloke/law-LLM-GGUF&lt;/a&gt;&lt;br /&gt; and it downloads the model and runs it but when I ask it &amp;quot;what can you help me with&amp;quot; it totally goes off the rails. Am I doing something wrong or am I missing a step? I'm somewhat new to this and have been having great results with the models listed in the ollama repo/directory.&lt;/p&gt; &lt;p&gt;NOTE: This post has 2.7K views as of this note, and 0 upvotes. Why is it unpopular to ask this question? Do people on this sub not really know why something like this happens and what the solution is. I assumed I would find some Ollama experts on here. Doesn't look like it... &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omni_shaNker"&gt; /u/omni_shaNker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lv4o6y/why_is_this_model_from_hf_telling_me_its_a_boy_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lv4o6y/why_is_this_model_from_hf_telling_me_its_a_boy_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lv4o6y/why_is_this_model_from_hf_telling_me_its_a_boy_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T00:12:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1luwmuz</id>
    <title>Nvidia Game Ready &lt;or&gt; Studio Drivers - is one better for LLMs?</title>
    <updated>2025-07-08T18:41:44+00:00</updated>
    <author>
      <name>/u/beedunc</name>
      <uri>https://old.reddit.com/user/beedunc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does it matter which one I'm running regarding speed, etc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beedunc"&gt; /u/beedunc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luwmuz/nvidia_game_ready_or_studio_drivers_is_one_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luwmuz/nvidia_game_ready_or_studio_drivers_is_one_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1luwmuz/nvidia_game_ready_or_studio_drivers_is_one_better/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T18:41:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1luwp11</id>
    <title>What is the best LLM I can use? (I'm new in this sector)</title>
    <updated>2025-07-08T18:43:55+00:00</updated>
    <author>
      <name>/u/No-Studio9085</name>
      <uri>https://old.reddit.com/user/No-Studio9085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PC:&lt;/p&gt; &lt;p&gt;RTX 3060&lt;/p&gt; &lt;p&gt;12GB VRAM&lt;/p&gt; &lt;p&gt;16GB RAM&lt;/p&gt; &lt;p&gt;i5 12400F&lt;/p&gt; &lt;p&gt;I would actually like it for two situations:&lt;/p&gt; &lt;p&gt;- One that is for specific tasks or specifics situations&lt;/p&gt; &lt;p&gt;- And another that works well for roleplay&lt;/p&gt; &lt;p&gt;Thanks&amp;lt;3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Studio9085"&gt; /u/No-Studio9085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luwp11/what_is_the_best_llm_i_can_use_im_new_in_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luwp11/what_is_the_best_llm_i_can_use_im_new_in_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1luwp11/what_is_the_best_llm_i_can_use_im_new_in_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T18:43:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1luuyej</id>
    <title>Built an easy way to schedule prompts powered by MCP and Ollama using our open source LLM client</title>
    <updated>2025-07-08T17:38:10+00:00</updated>
    <author>
      <name>/u/WalrusVegetable4506</name>
      <uri>https://old.reddit.com/user/WalrusVegetable4506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1luuyej/built_an_easy_way_to_schedule_prompts_powered_by/"&gt; &lt;img alt="Built an easy way to schedule prompts powered by MCP and Ollama using our open source LLM client" src="https://preview.redd.it/2donn13krobf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fac95eb0aa08a701d4006cd7493461fd37d0427" title="Built an easy way to schedule prompts powered by MCP and Ollama using our open source LLM client" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Every time we've shared our project we've gotten awesome feedback from this community so I'm excited to share we added scheduled tasks to Tome.&lt;/p&gt; &lt;p&gt;If you haven't seen my past posts, the tl;dr is Tome is an &lt;a href="https://github.com/runebookai/tome"&gt;open source desktop app&lt;/a&gt; for Mac or Windows that lets you connect local or remote models to MCP servers and chat with them.&lt;/p&gt; &lt;p&gt;As of our latest releases you can now run hourly or daily scheduled tasks, here's some examples from my screenshot (though I'm sure y'all will come up with way better ones :)):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Summarizing top Steam games on sale once per day&lt;/li&gt; &lt;li&gt;Periodically parsing Tome‚Äôs own log files&lt;/li&gt; &lt;li&gt;Checking Best Buy for handheld gaming deals&lt;/li&gt; &lt;li&gt;Summarizing Slack messages and generating to-dos&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's free to use, you just hook up Ollama or an API key of your choice, install some MCP servers, and you can chat or schedule any prompts you want. The MCP servers I'm using in my examples are Playwright, Discord, Slack, and Brave Search - let me know if you're interested in a tutorial and I'm happy to throw one together.&lt;/p&gt; &lt;p&gt;Would love any feedback (good or bad!) here or &lt;a href="https://discord.gg/9CH6us29YA"&gt;on our Discord&lt;/a&gt;, you can download the latest release here: &lt;a href="https://github.com/runebookai/tome/releases/tag/0.9.2"&gt;https://github.com/runebookai/tome/releases/tag/0.9.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for checking us out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WalrusVegetable4506"&gt; /u/WalrusVegetable4506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2donn13krobf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luuyej/built_an_easy_way_to_schedule_prompts_powered_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1luuyej/built_an_easy_way_to_schedule_prompts_powered_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T17:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv7n2g</id>
    <title>Can I just download the files for a model?</title>
    <updated>2025-07-09T02:37:11+00:00</updated>
    <author>
      <name>/u/General174512</name>
      <uri>https://old.reddit.com/user/General174512</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to be able to put the Deepseek R1 on a USB for use on my other computers, is it possible to just download a model (like clicking a download button), and then being able to throw it onto the USB?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/General174512"&gt; /u/General174512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lv7n2g/can_i_just_download_the_files_for_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lv7n2g/can_i_just_download_the_files_for_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lv7n2g/can_i_just_download_the_files_for_a_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T02:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lug5su</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-07-08T05:04:15+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a &lt;strong&gt;Highly Customizable AI Research Agent&lt;/strong&gt; that connects to your personal external sources and search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, Discord, and more coming soon.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;üìä &lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt; &lt;li&gt;Offers a RAG-as-a-Service API Backend&lt;/li&gt; &lt;li&gt;50+ File extensions supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üéôÔ∏è &lt;strong&gt;Podcasts&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt; &lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt; &lt;li&gt;Multiple TTS providers supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ÑπÔ∏è &lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;YouTube videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;Discord&lt;/li&gt; &lt;li&gt;...and more on the way&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üîñ &lt;strong&gt;Cross-Browser Extension&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lug5su/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lug5su/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lug5su/open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T05:04:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvhdm4</id>
    <title>Starting model delay</title>
    <updated>2025-07-09T12:21:59+00:00</updated>
    <author>
      <name>/u/thexdroid</name>
      <uri>https://old.reddit.com/user/thexdroid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My program uses the API, if the server is still loading the model it will raise an error due timeout. Is there a way, using the API (I could not found, sorry) to know if the model is loaded? Using ollama ps show the model in memory but it won't say it is ready to use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thexdroid"&gt; /u/thexdroid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvhdm4/starting_model_delay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvhdm4/starting_model_delay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lvhdm4/starting_model_delay/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T12:21:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv672x</id>
    <title>I used Ollama to build a Cursor for PDFs</title>
    <updated>2025-07-09T01:25:50+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lv672x/i_used_ollama_to_build_a_cursor_for_pdfs/"&gt; &lt;img alt="I used Ollama to build a Cursor for PDFs" src="https://external-preview.redd.it/eW1tYWxxOGYycmJmMXdyH2g3gh74ax-OLM0Bn_sh-0xXIf9gZjZs7gXuAEvK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e52e99d0c98b4bb58943f1138c44430623ddf356" title="I used Ollama to build a Cursor for PDFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like using Cursor while coding, but there are a lot of other tasks outside of code that would also benefit from having an agent on the side - things like reading through long documents and filling out forms. &lt;/p&gt; &lt;p&gt;So, as a fun experiment, I built an agent with search with a PDF viewer on the side. I've found it to be super helpful - and I'd love feedback on where you'd like to see this go!&lt;/p&gt; &lt;p&gt;If you'd like to try it out: &lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="http://github.com/morphik-org/morphik-core"&gt;github.com/morphik-org/morphik-core&lt;/a&gt;&lt;br /&gt; Website: &lt;a href="http://morphik.ai"&gt;morphik.ai&lt;/a&gt; (Look for the PDF Viewer section!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3jj1br8f2rbf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lv672x/i_used_ollama_to_build_a_cursor_for_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lv672x/i_used_ollama_to_build_a_cursor_for_pdfs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T01:25:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvfej4</id>
    <title>ngrok for AI models - Serve Ollama models with a cloud API using Local Runners</title>
    <updated>2025-07-09T10:33:33+00:00</updated>
    <author>
      <name>/u/Sumanth_077</name>
      <uri>https://old.reddit.com/user/Sumanth_077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, we‚Äôve built ngrok for AI models ‚Äî and it works seamlessly with Ollama.&lt;/p&gt; &lt;p&gt;We built Local Runners to let you serve AI models, MCP servers, or agents directly from your own machine and expose them through a secure Clarifai endpoint. No need to spin up a web server, manage routing, or deploy to the cloud. Just run the model locally and get a working API endpoint instantly.&lt;/p&gt; &lt;p&gt;If you're running open-source models with Ollama, Local Runners let you keep compute and data local while still connecting to agent frameworks, APIs, or workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Run ‚Äì Start a local runner pointing to your model&lt;br /&gt; Tunnel ‚Äì It opens a secure connection to a hosted API endpoint&lt;br /&gt; Requests ‚Äì API calls are routed to your machine&lt;br /&gt; Response ‚Äì Your model processes them locally and returns the result&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this helps:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Skip building a server or deploying just to test a model&lt;/li&gt; &lt;li&gt;Wire local models into LangGraph, CrewAI, or custom agent loops&lt;/li&gt; &lt;li&gt;Access local files, private tools, or data sources from your model&lt;/li&gt; &lt;li&gt;Use your existing hardware for inference, especially for token hungry models and agents, reducing cloud costs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôve put together a short tutorial that shows how you can expose local models, MCP servers, tools, and agents securely using Local Runners, without deploying anything to the cloud.&lt;br /&gt; &lt;a href="https://youtu.be/JOdtZDmCFfk"&gt;https://youtu.be/JOdtZDmCFfk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear how you're running Ollama models or building agent workflows around them. Fire away in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sumanth_077"&gt; /u/Sumanth_077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvfej4/ngrok_for_ai_models_serve_ollama_models_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvfej4/ngrok_for_ai_models_serve_ollama_models_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lvfej4/ngrok_for_ai_models_serve_ollama_models_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T10:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvk3o1</id>
    <title>Best model for my coding the correct concepts for something complicated</title>
    <updated>2025-07-09T14:24:00+00:00</updated>
    <author>
      <name>/u/beginnerflipper</name>
      <uri>https://old.reddit.com/user/beginnerflipper</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 3080ti, 32gb of ram, and a 7800x3d. I can debug code, but I want to make sure it gets the concepts down from an academic paper and use it to write code and use packages already developed. Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beginnerflipper"&gt; /u/beginnerflipper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvk3o1/best_model_for_my_coding_the_correct_concepts_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvk3o1/best_model_for_my_coding_the_correct_concepts_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lvk3o1/best_model_for_my_coding_the_correct_concepts_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T14:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lutct9</id>
    <title>My little tribute to Ollama</title>
    <updated>2025-07-08T16:37:18+00:00</updated>
    <author>
      <name>/u/valdecircarvalho</name>
      <uri>https://old.reddit.com/user/valdecircarvalho</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lutct9/my_little_tribute_to_ollama/"&gt; &lt;img alt="My little tribute to Ollama" src="https://preview.redd.it/5n0izf2mhobf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9e966c0d12b9995e3120a8ec5a0782e30b0f651" title="My little tribute to Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdecircarvalho"&gt; /u/valdecircarvalho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5n0izf2mhobf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lutct9/my_little_tribute_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lutct9/my_little_tribute_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T16:37:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvqcfw</id>
    <title>Thoughts on grabbing a 5060 Ti 16G as a noob?</title>
    <updated>2025-07-09T18:28:21+00:00</updated>
    <author>
      <name>/u/SKX007J1</name>
      <uri>https://old.reddit.com/user/SKX007J1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For someone wanting to get started with ollama and experiment with self-hosting hosting how does the 5060 Ti 16G stack up for the price point of ¬£390/$500. &lt;/p&gt; &lt;p&gt;What would you get with that sort of budget if your goal was just learning rather than productivity? Any ways to mitigate that they nerfed the bandwidth of the memory? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SKX007J1"&gt; /u/SKX007J1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvqcfw/thoughts_on_grabbing_a_5060_ti_16g_as_a_noob/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvqcfw/thoughts_on_grabbing_a_5060_ti_16g_as_a_noob/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lvqcfw/thoughts_on_grabbing_a_5060_ti_16g_as_a_noob/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T18:28:21+00:00</published>
  </entry>
</feed>
