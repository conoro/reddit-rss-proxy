<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-07T05:53:31+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1q1ictw</id>
    <title>igpu + dgpu for reducing cpu load</title>
    <updated>2026-01-01T23:19:53+00:00</updated>
    <author>
      <name>/u/sultan_papagani</name>
      <uri>https://old.reddit.com/user/sultan_papagani</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i wanted to share my findings on using iGPU + dGPU to reduce cpu load during inference.&lt;/p&gt; &lt;p&gt;Prompt: write a booking website for hotels Model: gpt-oss:latest igpu: intel arrow lake integrated graphics dgpu: rtx5060 system ram: 32gb&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;CPU offloading + dGPU (cuda)&lt;/p&gt; &lt;p&gt;Size: 14GB&lt;br /&gt; Processor: 57% CPU / 43% GPU&lt;br /&gt; Context: 32K All 8 CPU cores fully utilized (100% per core) Total CPU load: ~33–47% Fans ramp up and system is loud&lt;/p&gt; &lt;p&gt;Total duration: 2m 42s Prompt eval: 73 tokens @ ~68 tok/s Generation: 3756 tokens @ ~25.7 tok/s&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;iGPU + dGPU only (vulkan)&lt;/p&gt; &lt;p&gt;Size: 14GB&lt;br /&gt; Processor: 100% GPU&lt;br /&gt; Context: 32K CPU usage drops to ~1–6% System stays quiet&lt;/p&gt; &lt;p&gt;Total duration: 10m 30s Prompt eval: 73 tokens @ ~46.8 tok/s Generation: 4213 tokens @ ~6.7 tok/s&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Running fully on iGPU + dGPU dramatically reduces CPU load and noise, but generation speed drops significantly. For long or non-interactive runs, this tradeoff can be worth it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sultan_papagani"&gt; /u/sultan_papagani &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q1ictw/igpu_dgpu_for_reducing_cpu_load/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q1ictw/igpu_dgpu_for_reducing_cpu_load/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q1ictw/igpu_dgpu_for_reducing_cpu_load/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-01T23:19:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1q179er</id>
    <title>Ollama models to specific GPU</title>
    <updated>2026-01-01T15:51:10+00:00</updated>
    <author>
      <name>/u/NormalSmoke1</name>
      <uri>https://old.reddit.com/user/NormalSmoke1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to hard force the OLLAMA model to specifically sit on a designated GPU. As I looked through the OLLAMA docs, it says to use the CUDA visible devices in the python script, but isn't there somewhere in the unix configuration I can set at startup? I have multiple 3090's and I would like to have the model on sit on one, so the other is free for other agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NormalSmoke1"&gt; /u/NormalSmoke1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q179er/ollama_models_to_specific_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q179er/ollama_models_to_specific_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q179er/ollama_models_to_specific_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-01T15:51:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1q23t1w</id>
    <title>Registry off or is my connection?</title>
    <updated>2026-01-02T16:54:44+00:00</updated>
    <author>
      <name>/u/OppenheimerDaSilva</name>
      <uri>https://old.reddit.com/user/OppenheimerDaSilva</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi fellas, since december of last year I cannot pull any image of ollama, I always receive timeout. It's something wth my connection?&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;ollama pull gpt-oss:20b ─╯&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;Error: pull model manifest: Get &amp;quot;&lt;a href="https://registry.ollama.ai/v2/library/gpt-oss/manifests/20b%22:"&gt;https://registry.ollama.ai/v2/library/gpt-oss/manifests/20b&amp;quot;:&lt;/a&gt; dial tcp 172.67.182.229:443: i/o timeout&lt;br /&gt; ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OppenheimerDaSilva"&gt; /u/OppenheimerDaSilva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q23t1w/registry_off_or_is_my_connection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q23t1w/registry_off_or_is_my_connection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q23t1w/registry_off_or_is_my_connection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-02T16:54:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2dgwf</id>
    <title>Anyway to make joycaption into a chatbot?</title>
    <updated>2026-01-02T22:58:30+00:00</updated>
    <author>
      <name>/u/Zantorn</name>
      <uri>https://old.reddit.com/user/Zantorn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Complete noob here&lt;/p&gt; &lt;p&gt;Anyway to make joycaption into a chatbot? &lt;/p&gt; &lt;p&gt;Want to have it look at images and react to the, give opinions, have conversation about them etc. Is this possible to do locally? If so what should i use to get started? I have Ollama and LMStudio but not sure if those are the best options for this as im pretty new to&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zantorn"&gt; /u/Zantorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2dgwf/anyway_to_make_joycaption_into_a_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2dgwf/anyway_to_make_joycaption_into_a_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q2dgwf/anyway_to_make_joycaption_into_a_chatbot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-02T22:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1ud1h</id>
    <title>Does Open WebUI actually crawl links with Ollama, or is it just hallucinating based on the URL?</title>
    <updated>2026-01-02T09:32:31+00:00</updated>
    <author>
      <name>/u/Whole-Competition223</name>
      <uri>https://old.reddit.com/user/Whole-Competition223</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I recently started using &lt;strong&gt;Open WebUI&lt;/strong&gt; integrated with &lt;strong&gt;Ollama&lt;/strong&gt;. Today, I tried giving a specific URL to an LLM using the &lt;code&gt;#&lt;/code&gt; prefix and asked it to summarize the content in Korean.&lt;/p&gt; &lt;p&gt;At first, I was quite impressed because the summary looked very plausible and well-structured. However, I later found out that Ollama models, by default, cannot access the internet or visit external links.&lt;/p&gt; &lt;p&gt;This leaves me with a few questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;How did it generate the summary?&lt;/strong&gt; Was the LLM just &amp;quot;guessing&amp;quot; the content based on the words in the URL and its pre-existing training data? Or does Open WebUI pass some scraped metadata to the model?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Is there a way to enable &amp;quot;real&amp;quot; web browsing?&lt;/strong&gt; I want the model to actually visit the link and analyze the current page content. Are there specific functions, tools, or configurations in Open WebUI (like RAG settings) that allow Ollama models to access external websites?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'd love to hear how you guys handle web-based tasks with local LLMs. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Competition223"&gt; /u/Whole-Competition223 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q1ud1h/does_open_webui_actually_crawl_links_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q1ud1h/does_open_webui_actually_crawl_links_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q1ud1h/does_open_webui_actually_crawl_links_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-02T09:32:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2hp7u</id>
    <title>Integrated Mistral Nemo (12B) into a custom Space Discovery Engine (Project ARIS) for local anomaly detection.</title>
    <updated>2026-01-03T01:58:21+00:00</updated>
    <author>
      <name>/u/Limp-Regular3741</name>
      <uri>https://old.reddit.com/user/Limp-Regular3741</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share a real-world use case for local LLMs. I’ve built a discovery engine called Project ARIS that uses Mistral Nemo as a reasoning layer for astronomical data.&lt;/p&gt; &lt;p&gt;The Stack:&lt;/p&gt; &lt;p&gt;Model: Mistral Nemo 12B (Q4_K_M) running via Ollama.&lt;/p&gt; &lt;p&gt;Hardware: Lenovo Yoga 7 (Ryzen AI 7, 24GB RAM) on Nobara Linux.&lt;/p&gt; &lt;p&gt;Integration: Tauri/Rust backend calling the Ollama API.&lt;/p&gt; &lt;p&gt;How I’m using the LLM:&lt;/p&gt; &lt;p&gt;Contextual Memory: It reads previous session reports from a local folder and greets me with a verbal recap on boot.&lt;/p&gt; &lt;p&gt;Intent Parsing: I built a custom terminal where Nemo translates &amp;quot;fuzzy&amp;quot; natural language into structured MAST API queries.&lt;/p&gt; &lt;p&gt;Anomaly Scoring: It parses spectral data to flag &amp;quot;out of the ordinary&amp;quot; signatures that don't fit standard star/planet profiles.&lt;/p&gt; &lt;p&gt;It’s amazing how much a 12B model can do when given a specific toolset and a sandboxed terminal. Happy to answer any questions about the Rust/Ollama bridge!&lt;/p&gt; &lt;p&gt;A preview of Project ARIS can be found here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/glowseedstudio/Project-ARIS"&gt;https://github.com/glowseedstudio/Project-ARIS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limp-Regular3741"&gt; /u/Limp-Regular3741 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2hp7u/integrated_mistral_nemo_12b_into_a_custom_space/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2hp7u/integrated_mistral_nemo_12b_into_a_custom_space/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q2hp7u/integrated_mistral_nemo_12b_into_a_custom_space/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T01:58:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q37qyq</id>
    <title>Any Vision model on pair with GPT-OSS 120B?</title>
    <updated>2026-01-03T22:10:21+00:00</updated>
    <author>
      <name>/u/Altair12311</name>
      <uri>https://old.reddit.com/user/Altair12311</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! new to local ai selfhosting!&lt;/p&gt; &lt;p&gt;I do enjoy a lot my experiences and now i was having a tiny doubt... I do like GPT-OSS but i do enjoy a lot share &amp;quot;Images&amp;quot; with the AI like GPT-5 so the AI can watch the image and help me with the problem... GPT-OSS 120B doesn't have that feature and cannot recognize images as far i know... &lt;/p&gt; &lt;p&gt;Which other option i do have?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altair12311"&gt; /u/Altair12311 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q37qyq/any_vision_model_on_pair_with_gptoss_120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q37qyq/any_vision_model_on_pair_with_gptoss_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q37qyq/any_vision_model_on_pair_with_gptoss_120b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T22:10:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3a4yl</id>
    <title>[Experimental] Gemma 3 4B - Dark CoT: Pushing 4B Reasoning to 33%+ on GPQA Diamond</title>
    <updated>2026-01-03T23:48:56+00:00</updated>
    <author>
      <name>/u/AlexHardy08</name>
      <uri>https://old.reddit.com/user/AlexHardy08</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexHardy08"&gt; /u/AlexHardy08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1q38og2/experimental_gemma_3_4b_dark_cot_pushing_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q3a4yl/experimental_gemma_3_4b_dark_cot_pushing_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q3a4yl/experimental_gemma_3_4b_dark_cot_pushing_4b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T23:48:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2wny9</id>
    <title>Offline agent testing chat mode using Ollama as the judge (EvalView)</title>
    <updated>2026-01-03T15:00:12+00:00</updated>
    <author>
      <name>/u/hidai25</name>
      <uri>https://old.reddit.com/user/hidai25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q2wny9/offline_agent_testing_chat_mode_using_ollama_as/"&gt; &lt;img alt="Offline agent testing chat mode using Ollama as the judge (EvalView)" src="https://external-preview.redd.it/OGQksTmq-Xi-DKXl6h0CyL7yaRb404yUr8mvZQNBiSU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a5fb41b62d23c04e5d8feaa779c16a229dd5ca2f" title="Offline agent testing chat mode using Ollama as the judge (EvalView)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick demo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1q2wny9/video/z75urjhci5bg1/player"&gt;https://reddit.com/link/1q2wny9/video/z75urjhci5bg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’ve been working on EvalView (pytest-style regression tests for tool-using agents) and just added an interactive chat mode that runs fully local with Ollama.&lt;/p&gt; &lt;p&gt;Instead of remembering commands or writing YAML up front, you can just ask:&lt;/p&gt; &lt;p&gt;“run my tests”&lt;/p&gt; &lt;p&gt;“why did checkout fail?”&lt;/p&gt; &lt;p&gt;“diff this run vs yesterday’s golden baseline”&lt;/p&gt; &lt;p&gt;It uses your local Ollama model for the chat + for LLM-as-judge grading. No tokens leave your machine, no API costs (unless you count electricity and emotional damage).&lt;/p&gt; &lt;p&gt;Setup:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama pull llama3.2&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install evalview&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;evalview chat --provider ollama --model llama3.2&lt;/code&gt;&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;p&gt;- Runs your agent test suite + diffs against baselines&lt;/p&gt; &lt;p&gt;- Grades outputs with the local model (LLM-as-judge)&lt;/p&gt; &lt;p&gt;- Shows tool-call / latency / token (and cost estimate) diffs between runs&lt;/p&gt; &lt;p&gt;- Lets you drill into failures conversationally&lt;/p&gt; &lt;p&gt;Repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/hidai25/eval-view"&gt;https://github.com/hidai25/eval-view&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Question for the Ollama crowd:&lt;/p&gt; &lt;p&gt;What models have you found work well for &amp;quot;reasoning about agent behavior&amp;quot; and judging tool calls?&lt;/p&gt; &lt;p&gt;I’ve been using llama3.2 but I’m curious if mistral or deepseek-coder style models do better for tool-use grading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hidai25"&gt; /u/hidai25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2wny9/offline_agent_testing_chat_mode_using_ollama_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2wny9/offline_agent_testing_chat_mode_using_ollama_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q2wny9/offline_agent_testing_chat_mode_using_ollama_as/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T15:00:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2isyx</id>
    <title>Run Claude Code with ollama without losing any single feature offered by Anthropic backend</title>
    <updated>2026-01-03T02:47:39+00:00</updated>
    <author>
      <name>/u/Dangerous-Dingo-5169</name>
      <uri>https://old.reddit.com/user/Dangerous-Dingo-5169</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! Sharing an open-source project that might be useful:&lt;/p&gt; &lt;p&gt;Lynkr connects AI coding tools (like Claude Code) to multiple LLM providers with intelligent routing.&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Route between multiple providers: Databricks, Azure Ai Foundry, OpenRouter, Ollama,llama.cpp, OpenAi&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Cost optimization through hierarchical routing, heavy prompt caching&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Production-ready: circuit breakers, load shedding, monitoring&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It supports all the features offered by claude code like sub agents, skills , mcp , plugins etc unlike other proxies which only supports basic tool callings and chat completions.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Great for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Reducing API costs as it supports hierarchical routing where you can route requstes to smaller local models and later switch to cloud LLMs automatically.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Using enterprise infrastructure (Azure)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;- Local LLM experimentation&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;npm install -g lynkr&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Fast-Editor/Lynkr"&gt;https://github.com/Fast-Editor/Lynkr&lt;/a&gt; (Apache 2.0)&lt;/p&gt; &lt;p&gt;Would love to get your feedback on this one. Please drop a star on the repo if you found it helpful&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous-Dingo-5169"&gt; /u/Dangerous-Dingo-5169 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2isyx/run_claude_code_with_ollama_without_losing_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2isyx/run_claude_code_with_ollama_without_losing_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q2isyx/run_claude_code_with_ollama_without_losing_any/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T02:47:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1q33op3</id>
    <title>Radxa Orion O6 LLM Benchmarks (Ollama, Debian 12 Headless, 64GB RAM) – 30B on ARM is actually usable</title>
    <updated>2026-01-03T19:30:06+00:00</updated>
    <author>
      <name>/u/RasPiBuilder</name>
      <uri>https://old.reddit.com/user/RasPiBuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent some time benchmarking the Radxa Orion O6 running Debian 12 + Ollama after sorting out early thermal issues. Sharing results in case they’re helpful for anyone considering this board for local LLM inference. One important note is that the official Radxa Debian 12 image for the Orion O6 only ships with a desktop environment. For these tests, I removed the desktop and ran the system headless, which helped reduce background load and thermals.&lt;/p&gt; &lt;h1&gt;Hardware / Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Radxa Orion O6&lt;/li&gt; &lt;li&gt;64 GB RAM&lt;/li&gt; &lt;li&gt;Powered over USB-C PD&lt;/li&gt; &lt;li&gt;Radxa AI Kit case (this significantly improved thermals)&lt;/li&gt; &lt;li&gt;Debian 12 (official Radxa image, desktop removed → headless)&lt;/li&gt; &lt;li&gt;Ollama (CPU-only)&lt;/li&gt; &lt;li&gt;CPU governor: &lt;code&gt;schedutil&lt;/code&gt; (performed better than forcing &lt;code&gt;performance&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Adequate cooling and airflow (critical on this board)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results (tokens/sec = eval rate)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Qwen3-Next&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;2.41 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 1203&lt;/li&gt; &lt;li&gt;Total time: ~8m25s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Nemotron-3-nano&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;6.04 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 836&lt;/li&gt; &lt;li&gt;Total time: ~2m21s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen3:30B (MoE)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;6.42 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 709&lt;/li&gt; &lt;li&gt;Total time: ~1m52s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen3:30B-Instruct (MoE)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: 6.81 &lt;strong&gt;tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 148&lt;/li&gt; &lt;li&gt;Total time: ~23s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen3:14B (dense)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;3.66 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 328&lt;/li&gt; &lt;li&gt;Total time: ~1m33s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GPT-OSS&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;3.01 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 543&lt;/li&gt; &lt;li&gt;Total time: ~3m09s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Llama3:8B&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;6.42 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 273&lt;/li&gt; &lt;li&gt;Total time: ~45s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-R1:1.5B&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;19.57 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 44&lt;/li&gt; &lt;li&gt;Total time: ~2.7s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Granite 3.1 MoE (3B)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;17.87 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 66&lt;/li&gt; &lt;li&gt;Total time: ~4.8s&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Observations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;30B-class models &lt;em&gt;do&lt;/em&gt; run on the Orion O6 — slow, but usable for experimentation.&lt;/li&gt; &lt;li&gt;Larger models (8B–30B) cluster around ~3–6 tok/s, suggesting a memory-bandwidth / ARM CPU ceiling, not a power or clock issue.&lt;/li&gt; &lt;li&gt;Smaller models (Granite 3B, DeepSeek 1.5B) feel very responsive.&lt;/li&gt; &lt;li&gt;schedutil governor consistently outperformed performance in testing.&lt;/li&gt; &lt;li&gt;Thermals matter a lot: moving to the Radxa AI Kit case and running headless eliminated thermal shutdowns seen earlier.&lt;/li&gt; &lt;li&gt;USB-C PD has been stable so far with adequate cooling.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;The Orion O6 isn’t a GPU replacement, but as a compact ARM server with 64 GB RAM that can genuinely run 30B MoE models, it exceeded my expectations. Running Debian headless and using the AI Kit case makes a real difference. With realistic performance expectations, it’s a solid platform for local LLM experimentation.&lt;/p&gt; &lt;p&gt;Happy to answer questions or run additional tests if people are interested.&lt;/p&gt; &lt;h1&gt;Update&lt;/h1&gt; &lt;p&gt;I was able to slightly increase perform by making a few more tweaks.&lt;br /&gt; 1. Changed CPU Governor to ondemand&lt;br /&gt; 2. Pruned Unnecessary background services (isp_app, avahi-daemon, cups, fwupd, upower, etc.)&lt;br /&gt; 3. OLLAMA_SCHED_SPREAD=false&lt;/p&gt; &lt;p&gt;For Qwen3:30b-instruct, this boosted performance from ~6.8t/s to ~7.4t/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RasPiBuilder"&gt; /u/RasPiBuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q33op3/radxa_orion_o6_llm_benchmarks_ollama_debian_12/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q33op3/radxa_orion_o6_llm_benchmarks_ollama_debian_12/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q33op3/radxa_orion_o6_llm_benchmarks_ollama_debian_12/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T19:30:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q36m42</id>
    <title>What model to use and how to disable using cloud.</title>
    <updated>2026-01-03T21:24:58+00:00</updated>
    <author>
      <name>/u/ItsWappers</name>
      <uri>https://old.reddit.com/user/ItsWappers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just don't want to use credits and want to know what model is the best for offline use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ItsWappers"&gt; /u/ItsWappers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q36m42/what_model_to_use_and_how_to_disable_using_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q36m42/what_model_to_use_and_how_to_disable_using_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q36m42/what_model_to_use_and_how_to_disable_using_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T21:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q47pg0</id>
    <title>Ollama Cloud?</title>
    <updated>2026-01-05T01:14:23+00:00</updated>
    <author>
      <name>/u/Natjoe64</name>
      <uri>https://old.reddit.com/user/Natjoe64</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, been using ollama as my main ai provider for a while, and it works great for smaller tasks with on device Qwen 3 vl, Ministral, and other models, but my 16 gb of unified memory on my M2 Pro Macbook Pro is getting a little cramped. 4b is plenty fast, and 8b is doable with quantization, but especially with bigger context lengths it's getting tight, and I don't want to cook my ssd alive with overusing swap. I was looking into a server build, but with ram prices being what they are combined with gpus that would make the endeavour worth the squeeze, it's looking very expensive. &lt;/p&gt; &lt;p&gt;With a yearly cost of 250, is ollama cloud the best way to use these massive 235b+ models without forking over data to openai, anthropic, or google? The whole reason I started to use ollama was the data collection and spooky ammounts of knowledge that these commercial models can learn about you. Ollama cloud seems to have a very &amp;quot;trust me bro&amp;quot; approach to privacy in their resources, which only really say &amp;quot;Ollama does not log prompt or response data&amp;quot;. I would trust them more than the frontier ai labs listed above, but I would like to see some evidence. If you do use ollama cloud, is it worth it? How do these massive models like mistral large 3 and the 235b parameter version of qwen 3 vl compare to the frontier models? &lt;/p&gt; &lt;p&gt;TL;DR: Privacy policy nonexistent, but I need more vram&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Natjoe64"&gt; /u/Natjoe64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q47pg0/ollama_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q47pg0/ollama_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q47pg0/ollama_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T01:14:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4l7sf</id>
    <title>AI pre code</title>
    <updated>2026-01-05T13:07:21+00:00</updated>
    <author>
      <name>/u/umutkrts</name>
      <uri>https://old.reddit.com/user/umutkrts</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umutkrts"&gt; /u/umutkrts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AI_Agents/comments/1q494al/ai_pre_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4l7sf/ai_pre_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4l7sf/ai_pre_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T13:07:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4x5ih</id>
    <title>I forked Andrej Karpathy's LLM Council and added Ollama support, a Modern UI &amp; Settings Page, multi-AI API support, and Ollama support web search providers</title>
    <updated>2026-01-05T20:31:57+00:00</updated>
    <author>
      <name>/u/KobyStam</name>
      <uri>https://old.reddit.com/user/KobyStam</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KobyStam"&gt; /u/KobyStam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/artificial/comments/1q4wuet/i_forked_andrej_karpathys_llm_council_and_added_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4x5ih/i_forked_andrej_karpathys_llm_council_and_added/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4x5ih/i_forked_andrej_karpathys_llm_council_and_added/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T20:31:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4lf3s</id>
    <title>Google's Coral chip not compatible? what's the next cheap hardware to run locally?</title>
    <updated>2026-01-05T13:16:44+00:00</updated>
    <author>
      <name>/u/Curious_Party_4683</name>
      <uri>https://old.reddit.com/user/Curious_Party_4683</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im kinda bummed out out Ollama not compat with this $50 Coral chip that i got.&lt;/p&gt; &lt;p&gt;what's the next best thing to run Ollama 100% locally? &lt;/p&gt; &lt;p&gt;i plan to use Ollama with Home Assistant to identify delivery people, boxes or packages left on my porch, read pressure gauges, and utility meters. so far, Google Gemini has been working flawlessly but i would like to get off the cloud if i can.... &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Curious_Party_4683"&gt; /u/Curious_Party_4683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4lf3s/googles_coral_chip_not_compatible_whats_the_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4lf3s/googles_coral_chip_not_compatible_whats_the_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4lf3s/googles_coral_chip_not_compatible_whats_the_next/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T13:16:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4slm1</id>
    <title>What GPU for lecture summarizing?</title>
    <updated>2026-01-05T17:50:23+00:00</updated>
    <author>
      <name>/u/dnielso5</name>
      <uri>https://old.reddit.com/user/dnielso5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;My GF is in collage and records her lectures, I was going to get something like Plaude to do AI transcribing and summarizing but the teachers forbid sending the audio to 3rd parties (they even need permission to share recordings with each-other)&lt;/p&gt; &lt;p&gt;I set up a small server as a test and run Scriberr + ollama.&lt;/p&gt; &lt;p&gt;Scriberr model: Small&lt;/p&gt; &lt;p&gt;Ollama model: llama3.2:3b&lt;/p&gt; &lt;p&gt;The specs for the proof of concept are:&lt;/p&gt; &lt;p&gt;CPU: 2600x&lt;/p&gt; &lt;p&gt;Ram: 16g&lt;/p&gt; &lt;p&gt;GPU: Thats my question!&lt;/p&gt; &lt;p&gt;Scribing a 32 minute lecture took about 14 minutes and a very small summary took about 15 minutes. Thats not horrible as they only need to run once, but if i try and use a chat window thats easy another 12 minutes per chat and usually times out. &lt;/p&gt; &lt;p&gt;I understand VRAM is way better than system RAM but I'm wondering what would be ideal.&lt;/p&gt; &lt;p&gt;I have a 1660 with 6G i can test with but im guessing ill need 8G+&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnielso5"&gt; /u/dnielso5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4slm1/what_gpu_for_lecture_summarizing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4slm1/what_gpu_for_lecture_summarizing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4slm1/what_gpu_for_lecture_summarizing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T17:50:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4p21j</id>
    <title>Model Running for 1 day</title>
    <updated>2026-01-05T15:43:48+00:00</updated>
    <author>
      <name>/u/Binary_Alpha</name>
      <uri>https://old.reddit.com/user/Binary_Alpha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q4p21j/model_running_for_1_day/"&gt; &lt;img alt="Model Running for 1 day" src="https://b.thumbs.redditmedia.com/T-9Mn1o4fT2U6cePFTg_kM58kO848-uItnhTMoXStGw.jpg" title="Model Running for 1 day" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/8fq1qo2rwjbg1.png?width=1638&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=693365be93fcdd8809028b0cbbf5ddf36deb869b"&gt;https://preview.redd.it/8fq1qo2rwjbg1.png?width=1638&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=693365be93fcdd8809028b0cbbf5ddf36deb869b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been running this model for one day and it's not even finished. For your guys information, I'm running it on a Raspberry Pi 5 overclocked at 2.8 gigahertz. 16 gigabytes of RAM. of course this computer is not meant to do this workload and it's not surprising that it's taking one whole day to do this. when it's finished I'll update you guys with the final tokens per second and time it ran everything.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Binary_Alpha"&gt; /u/Binary_Alpha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4p21j/model_running_for_1_day/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4p21j/model_running_for_1_day/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4p21j/model_running_for_1_day/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T15:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1q53r5h</id>
    <title>LLMs are so unreliable</title>
    <updated>2026-01-06T00:46:28+00:00</updated>
    <author>
      <name>/u/Armageddon_80</name>
      <uri>https://old.reddit.com/user/Armageddon_80</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Armageddon_80"&gt; /u/Armageddon_80 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1q53qlk/llms_are_so_unreliable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q53r5h/llms_are_so_unreliable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q53r5h/llms_are_so_unreliable/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-06T00:46:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4vzci</id>
    <title>Achieving 30x Real-Time Transcription on CPU . Multilingual STT Openai api endpoint compatible. Plug and play in Open-webui - Parakeet</title>
    <updated>2026-01-05T19:49:24+00:00</updated>
    <author>
      <name>/u/SlightPossibility331</name>
      <uri>https://old.reddit.com/user/SlightPossibility331</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlightPossibility331"&gt; /u/SlightPossibility331 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1q4vz16/achieving_30x_realtime_transcription_on_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4vzci/achieving_30x_realtime_transcription_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4vzci/achieving_30x_realtime_transcription_on_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T19:49:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4ov85</id>
    <title>Hardware Suggestions for Local LLM with RAG and MCP for Nonprofit</title>
    <updated>2026-01-05T15:37:08+00:00</updated>
    <author>
      <name>/u/Realistic-Foot8724</name>
      <uri>https://old.reddit.com/user/Realistic-Foot8724</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good morning.&lt;/p&gt; &lt;p&gt;Sorry in advance if I use any terms incorrectly, still a newb to much of this.&lt;/p&gt; &lt;p&gt;Looking for advice on building a PC for learning local LLM usage/deployment. I also have relationships with local non-profit organizations that are very interested in adding AI to their workflows and have major privacy concerns. &lt;/p&gt; &lt;p&gt;Usage:&lt;/p&gt; &lt;p&gt;For me; local home network with two users looking for inference/chat capabilities as well as developing skills in local AI implementation.&lt;/p&gt; &lt;p&gt;For non-profits; vectorizing a couple decades worth of documentation (reports in .doc, .pdf, .xls) for RAG, help with statistical analysis (they currently use SPSS), tool calling to search APIs for up-to-date information for literature reviews or adding context/examples to reports, day to day chat/inference.&lt;/p&gt; &lt;p&gt;Budget is $1500-2000 (could stretch this a bit if it will really improve the experience).&lt;/p&gt; &lt;p&gt;Concerns: having at least reasonable speed (conversational) with acceptable power consumption (say not drastically higher than a good quality PC workstation when idling).&lt;/p&gt; &lt;p&gt;Looks like a high capacity (2tb-4tb) NVMe is helpful for model storage/loading.&lt;/p&gt; &lt;p&gt;Budgeting $800 for a RTX 3090 as Nvidia seems to be the way to go and that is the least expensive way to get a decent amount of Vram. Also like the possibility of adding a second RTX 3090 in the future.&lt;/p&gt; &lt;p&gt;Shopping used as storage/RAM prices are what they are.&lt;/p&gt; &lt;p&gt;Where I am really stuck is CPU, motherboard, RAM combo. I see online builds using old HP Z440s, Z4 G4s, Lenovo P620s, or other older workstations with some success. Is Xeon/Threadripper/EPYC worth the power consumption penalty? What would they help with? Would I be better off with a newer (10th-12th gen) I5 or similar CPU? Is a high amount onboard RAM helpful? &lt;/p&gt; &lt;p&gt;Any direction is appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Realistic-Foot8724"&gt; /u/Realistic-Foot8724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4ov85/hardware_suggestions_for_local_llm_with_rag_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4ov85/hardware_suggestions_for_local_llm_with_rag_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4ov85/hardware_suggestions_for_local_llm_with_rag_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T15:37:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q519lf</id>
    <title>Models with a sense of humor?</title>
    <updated>2026-01-05T23:06:00+00:00</updated>
    <author>
      <name>/u/icebergelishious</name>
      <uri>https://old.reddit.com/user/icebergelishious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was trying some models and hit them the the &amp;quot;Who invented running?&amp;quot; prompt, and then I responded back with &amp;quot;False, running was invented by Thomas Running is 1748 he tried to walk twice at the same time&amp;quot;&lt;/p&gt; &lt;p&gt;Some of them got the joke, but others it went over their head and they thought I was stupid haha&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icebergelishious"&gt; /u/icebergelishious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q519lf/models_with_a_sense_of_humor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q519lf/models_with_a_sense_of_humor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q519lf/models_with_a_sense_of_humor/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T23:06:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4jxcj</id>
    <title>Use ollama to run lightweight, open-source, local agents as UNIX tools.</title>
    <updated>2026-01-05T12:02:51+00:00</updated>
    <author>
      <name>/u/Available_Pressure47</name>
      <uri>https://old.reddit.com/user/Available_Pressure47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q4jxcj/use_ollama_to_run_lightweight_opensource_local/"&gt; &lt;img alt="Use ollama to run lightweight, open-source, local agents as UNIX tools." src="https://b.thumbs.redditmedia.com/Sa0PA8SD9rCnAyIg5ukMTwDe_YIbmXh3B9sWvfSZ8jA.jpg" title="Use ollama to run lightweight, open-source, local agents as UNIX tools." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/dorcha-inc/orla"&gt;https://github.com/dorcha-inc/orla&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The current ecosystem around agents feels like a collection of bloated SaaS with expensive subscriptions and privacy concerns. Orla brings large language models to your terminal with a dead-simple, Unix-friendly interface. Everything runs 100% locally. You don't need any API keys or subscriptions, and your data never leaves your machine. Use it like any other command-line tool:&lt;/p&gt; &lt;p&gt;$ orla agent &amp;quot;summarize this code&amp;quot; &amp;lt; main.go&lt;/p&gt; &lt;p&gt;$ git status | orla agent &amp;quot;Draft a commit message for these changes.&amp;quot;&lt;/p&gt; &lt;p&gt;$ cat data.json | orla agent &amp;quot;extract all email addresses&amp;quot; | sort -u&lt;/p&gt; &lt;p&gt;It's built on the Unix philosophy and is pipe-friendly and easily extensible.&lt;/p&gt; &lt;p&gt;The README in the repo contains a quick demo.&lt;/p&gt; &lt;p&gt;Installation is a single command. The script installs Orla, sets up Ollama for local inference, and pulls a lightweight model to get you started.&lt;/p&gt; &lt;p&gt;You can use homebrew (on Mac OS or Linux)&lt;/p&gt; &lt;p&gt;$ brew install --cask dorcha-inc/orla/orla&lt;/p&gt; &lt;p&gt;Or use the shell installer:&lt;/p&gt; &lt;p&gt;$ curl -fsSL &lt;a href="https://raw.githubusercontent.com/dorcha-inc/orla/main/scrip"&gt;https://raw.githubusercontent.com/dorcha-inc/orla/main/scrip&lt;/a&gt;... | sh&lt;/p&gt; &lt;p&gt;Orla is written in Go and is completely free software (MIT licensed) built on other free software. We'd love your feedback.&lt;/p&gt; &lt;p&gt;Thank you! :-)&lt;/p&gt; &lt;p&gt;Side note: contributions to Orla are very welcome. Please see (&lt;a href="https://github.com/dorcha-inc/orla/blob/main/CONTRIBUTING.md"&gt;https://github.com/dorcha-inc/orla/blob/main/CONTRIBUTING.md&lt;/a&gt;) for a guide on how to contribute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available_Pressure47"&gt; /u/Available_Pressure47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q4jxcj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4jxcj/use_ollama_to_run_lightweight_opensource_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4jxcj/use_ollama_to_run_lightweight_opensource_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T12:02:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4p8aa</id>
    <title>Introducing MiroThinker 1.5 — the world’s leading search-based agent model!</title>
    <updated>2026-01-05T15:49:57+00:00</updated>
    <author>
      <name>/u/wuqiao</name>
      <uri>https://old.reddit.com/user/wuqiao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q4p8aa/introducing_mirothinker_15_the_worlds_leading/"&gt; &lt;img alt="Introducing MiroThinker 1.5 — the world’s leading search-based agent model!" src="https://external-preview.redd.it/cH2lE5iC3U5CuznHdVEsQrxsFQW9rX4gLlOCeNsa0eE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c8e5a6d9fb506d45e380a2b69000398cdfa1e84" title="Introducing MiroThinker 1.5 — the world’s leading search-based agent model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have officially released our self-developed flagship search-based agent model, MiroThinker 1.5.This release delivers significant performance improvements and explores as well as implements predictive use cases.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get started now:&lt;/strong&gt; &lt;a href="https://dr.miromind.ai/"&gt;&lt;strong&gt;https://dr.miromind.ai/&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Leading Performance:&lt;/strong&gt; MiroThinker 1.5 (235B) surpasses ChatGPT-Agent in BrowseComp, ranking among the world's top tier.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extreme Efficiency:&lt;/strong&gt; MiroThinker 1.5 (30B) costs only 1/20 of Kimi-K2, delivering faster inference and higher intelligence-to-cost ratio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Predict the Future:&lt;/strong&gt; Proprietary “Interactive Scaling” and “Temporal-Sensitive Training” enable forward-looking analysis of how macro events trigger chain reactions across the Nasdaq.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully Open-Source:&lt;/strong&gt; Model and code are fully open, immediately unlocking discovery-driven intelligence for free.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Sample Showcase&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Case 1:&lt;/strong&gt; What major events next week could affect the U.S. Nasdaq Index, and how might each of them impact it?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://dr.miromind.ai/share/85ebca56-20b4-431d-bd3a-9dbbce7a82ea"&gt;https://dr.miromind.ai/share/85ebca56-20b4-431d-bd3a-9dbbce7a82ea&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Case 2:&lt;/strong&gt; Which film is most likely to receive a Best Picture nomination at the 2026 Oscars?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://dr.miromind.ai/share/e1099047-4488-4642-b7a4-e001e6213b22"&gt;https://dr.miromind.ai/share/e1099047-4488-4642-b7a4-e001e6213b22&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Case 3:&lt;/strong&gt; Which team is most likely to make it to the Super Bowl in 2026?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://dr.miromind.ai/share/c5ee0db8-676a-4b75-b42d-fd5ef8a2e0db"&gt;https://dr.miromind.ai/share/c5ee0db8-676a-4b75-b42d-fd5ef8a2e0db&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub :&lt;/strong&gt; &lt;a href="https://github.com/MiroMindAI/MiroThinker"&gt;https://github.com/MiroMindAI/MiroThinker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/F7EQFnYscV"&gt;https://discord.gg/F7EQFnYscV&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;：&lt;a href="https://github.com/MiroMindAI/MiroThinker/discussions/64"&gt;https://github.com/MiroMindAI/MiroThinker/discussions/64&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wuqiao"&gt; /u/wuqiao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4p8aa/introducing_mirothinker_15_the_worlds_leading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4p8aa/introducing_mirothinker_15_the_worlds_leading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T15:49:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5zcyq</id>
    <title>JRVS Community Feedback</title>
    <updated>2026-01-06T23:50:47+00:00</updated>
    <author>
      <name>/u/Xthebuilder</name>
      <uri>https://old.reddit.com/user/Xthebuilder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q5zcyq/jrvs_community_feedback/"&gt; &lt;img alt="JRVS Community Feedback" src="https://preview.redd.it/v6c9mctugtbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d76fb6103736ca1624e94d1958d82d38c4ea418" title="JRVS Community Feedback" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys it’s creator or JRVS. I want to say thank you all for the effort and the time you guys put into my app . Some of you guys said you made something similar and I’m glad because in reality if we all can learn on thing from each other we all won. Now that JRVS has been public for some time I really want to know from the community who uses it. What’s next , what do you guys want to see out of this project what do you like that it has what do you not like , etc. if this is an app you want developed to a certain degree , this is your chance to help the development. So please comment below your experience with JRVS the more detail the better. AGAIN THANKYOU ALL .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xthebuilder"&gt; /u/Xthebuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v6c9mctugtbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q5zcyq/jrvs_community_feedback/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q5zcyq/jrvs_community_feedback/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-06T23:50:47+00:00</published>
  </entry>
</feed>
