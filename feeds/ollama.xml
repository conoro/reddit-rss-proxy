<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-04T21:23:54+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1lpchao</id>
    <title>TimeCapsule-SLM - Open Source AI Deep Research Platform That Runs 100% in Your Browser!</title>
    <updated>2025-07-01T20:27:45+00:00</updated>
    <author>
      <name>/u/adssidhu86</name>
      <uri>https://old.reddit.com/user/adssidhu86</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lpchao/timecapsuleslm_open_source_ai_deep_research/"&gt; &lt;img alt="TimeCapsule-SLM - Open Source AI Deep Research Platform That Runs 100% in Your Browser!" src="https://preview.redd.it/ma9l20u8obaf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9c105132c47fdebf61ac5c089af6603184d7003" title="TimeCapsule-SLM - Open Source AI Deep Research Platform That Runs 100% in Your Browser!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Heyüëã&lt;br /&gt; Just launched &lt;a href="https://timecapsule.bubblspace.com/"&gt;TimeCapsule-SLM&lt;/a&gt; - an open source AI research platform that I think you'll find interesting. The key differentiator? Everything runs locally in your browser with complete privacy.üî• What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In-Browser RAG: Upload PDFs/documents, get AI insights without sending data to servers&lt;/li&gt; &lt;li&gt;TimeCapsule Sharing: Export/import complete research sessions as .timecapsule.json files&lt;/li&gt; &lt;li&gt;Multi-LLM Support: Works with Ollama, LM Studio, OpenAI APIs&lt;/li&gt; &lt;li&gt;Two main tools: DeepResearch (for novel idea generation) + Playground (for visual coding)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üîí Privacy Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Zero server dependency after initial load&lt;/li&gt; &lt;li&gt;All processing happens locally&lt;/li&gt; &lt;li&gt;Your data never leaves your device&lt;/li&gt; &lt;li&gt;Works offline once models are loaded&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üéØ Perfect for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Researchers who need privacy-first AI tools&lt;/li&gt; &lt;li&gt;Teams wanting to share research sessions&lt;/li&gt; &lt;li&gt;Anyone building local AI workflows&lt;/li&gt; &lt;li&gt;People tired of cloud-dependent tools&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Live Demo: &lt;a href="https://timecapsule.bubblspace.com"&gt;https://timecapsule.bubblspace.com&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/thefirehacker/TimeCapsule-SLM"&gt;https://github.com/thefirehacker/TimeCapsule-SLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Ollama integration is particularly smooth - just enable CORS and you're ready to go with local models like qwen3:0.6b.Would love to hear your thoughts and feedback! Also happy to answer any technical questions about the implementation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adssidhu86"&gt; /u/adssidhu86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ma9l20u8obaf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpchao/timecapsuleslm_open_source_ai_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpchao/timecapsuleslm_open_source_ai_deep_research/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T20:27:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpxsxk</id>
    <title>DeepSeek R1 8b: was it supposed to support tools?</title>
    <updated>2025-07-02T14:48:45+00:00</updated>
    <author>
      <name>/u/Effective_Head_5020</name>
      <uri>https://old.reddit.com/user/Effective_Head_5020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to use DeepSeek R1 8b through the HTTP API, but it says that it does not support tools. Is that correct? Or am I doing something wrong? Let me know and I can share more details&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective_Head_5020"&gt; /u/Effective_Head_5020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpxsxk/deepseek_r1_8b_was_it_supposed_to_support_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpxsxk/deepseek_r1_8b_was_it_supposed_to_support_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpxsxk/deepseek_r1_8b_was_it_supposed_to_support_tools/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T14:48:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq09qy</id>
    <title>Hardware advice?</title>
    <updated>2025-07-02T16:24:41+00:00</updated>
    <author>
      <name>/u/Glittering-Role3913</name>
      <uri>https://old.reddit.com/user/Glittering-Role3913</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone, i hope this is the right place to ask this. &lt;/p&gt; &lt;p&gt;Recently I've gotten into using local llms and I foresee myself getting alot of utility out of local llms. With that said, I want to upgrade my rig to be able to run models like deepseek r1 32b with 8-bit quantization locally inside of a vm. &lt;/p&gt; &lt;p&gt;My setup is: Ryzen 5 7600 (6 core, 12 thread) 2x8gb ddr5 ram (4800mhz at cl40) rx 7800 xt (16gb gddr6) Rtx 3060 (12gb gddr6) Powered by a 1000w psu OS: debian 12 (server)&lt;/p&gt; &lt;p&gt;Because I run the llms in a vm, I allocate 6 threads to the llms with 8gb of memory (i have other vms that require the other 8gb). &lt;/p&gt; &lt;p&gt;Total RAM - 28gb gddr6 + 8gb ddr5&lt;/p&gt; &lt;p&gt;Due to limited system resources, I realize that I need more system RAM or more VRAM. Ram will cost me $250 CAD after tax (2x32gb ddr5, 6000mhz cl30) whereas I can spend $300 CAD and get another 3060 (12gb gddr6). &lt;/p&gt; &lt;p&gt;Option A - 40gb gddr6 + 8gb ddr5 (cl40, 4800mhz) Option B - 28gb gddr6 + 64gb ddr5 (cl30, 6000 mhz)&lt;/p&gt; &lt;p&gt;My question is which one should I go with? Given my requirements, which one makes more sense? Are my requirements too intense, would it require too much VRAM? What models will provide similar performance or atleast really good performance given my setup in your opinion. Advice is greatly appreciated. &lt;/p&gt; &lt;p&gt;As long as I can get around 4 tokens per second under 8-bit quantization with an accurate model, id say im pretty satisfied. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Role3913"&gt; /u/Glittering-Role3913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq09qy/hardware_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq09qy/hardware_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lq09qy/hardware_advice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T16:24:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqe0ap</id>
    <title>Gemma3 e series</title>
    <updated>2025-07-03T02:14:08+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can anyone give some insight on the new gemma3 with the matroshka learning model? It sounds like a highly powered network in network NIN &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqe0ap/gemma3_e_series/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqe0ap/gemma3_e_series/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqe0ap/gemma3_e_series/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T02:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqaff2</id>
    <title>Dumb question, but how do you choose an LLM that's most appropriate for your system in the event of restrictions (no / lightweight GPU, limited RAM, etc)?</title>
    <updated>2025-07-02T23:18:02+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqaff2/dumb_question_but_how_do_you_choose_an_llm_thats/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqaff2/dumb_question_but_how_do_you_choose_an_llm_thats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqaff2/dumb_question_but_how_do_you_choose_an_llm_thats/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T23:18:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqnwac</id>
    <title>Help!! Ollama on AMD</title>
    <updated>2025-07-03T12:07:40+00:00</updated>
    <author>
      <name>/u/nqdat1995</name>
      <uri>https://old.reddit.com/user/nqdat1995</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could someone help me run Ollama on my AMD Radeon 6800 GPU. I run Ollama but it always runs on CPU instead :((&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nqdat1995"&gt; /u/nqdat1995 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqnwac/help_ollama_on_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqnwac/help_ollama_on_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqnwac/help_ollama_on_amd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T12:07:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqphsw</id>
    <title>What's the difference between ollama.embeddings() and ollama.embed() ? Why do the methods return different embeddings for the same model (code in description)?</title>
    <updated>2025-07-03T13:23:21+00:00</updated>
    <author>
      <name>/u/LordTerminator</name>
      <uri>https://old.reddit.com/user/LordTerminator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am calling both methods to compare the embeddings they return.&lt;/p&gt; &lt;p&gt;&lt;code&gt;ll = ollama.embeddings(model='llama3.2',&lt;/code&gt;&lt;br /&gt; &lt;code&gt;prompt = 'The sky is blue because of rayleigh scattering'&lt;/code&gt;&lt;br /&gt; &lt;code&gt;)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;llm = dict(ll)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;llm['embedding']&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ll = ollama.embed(model='llama3.2',&lt;/code&gt;&lt;br /&gt; &lt;code&gt;input = 'The sky is blue because of rayleigh scattering'&lt;/code&gt;&lt;br /&gt; &lt;code&gt;)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;llm = dict(ll)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;llm['embeddings'][0]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;They return different embeddings for the same model. Why is that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LordTerminator"&gt; /u/LordTerminator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqphsw/whats_the_difference_between_ollamaembeddings_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqphsw/whats_the_difference_between_ollamaembeddings_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqphsw/whats_the_difference_between_ollamaembeddings_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T13:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq8yx7</id>
    <title>Best lightweight model for running on CPU with low RAM?</title>
    <updated>2025-07-02T22:13:36+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got an unRAID server and I've set up Open WebUI and Ollama on it. Problem is, I've only got 16gb of RAM and no GPU... I plan to upgrade eventually, but can't afford that right now. As a beginner, the sheer mass of options in Ollama is a bit overwhelming. What options would you recommend for lightweight hardware?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq8yx7/best_lightweight_model_for_running_on_cpu_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq8yx7/best_lightweight_model_for_running_on_cpu_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lq8yx7/best_lightweight_model_for_running_on_cpu_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T22:13:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqfy1w</id>
    <title>A little project to analyze stock trends and explain major movements</title>
    <updated>2025-07-03T03:55:32+00:00</updated>
    <author>
      <name>/u/m19990328</name>
      <uri>https://old.reddit.com/user/m19990328</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lqfy1w/a_little_project_to_analyze_stock_trends_and/"&gt; &lt;img alt="A little project to analyze stock trends and explain major movements" src="https://b.thumbs.redditmedia.com/-tJiAQz3zixSDJrySlOjJVjaLSvDEuXQ8cBOZ83hpYo.jpg" title="A little project to analyze stock trends and explain major movements" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a tool that tries to explain the market movements to better understand the risk of investing in any stocks. I'd love to hear your opinion.&lt;/p&gt; &lt;p&gt;üëâ&lt;a href="https://github.com/CyrusCKF/stock-gone-wrong"&gt;https://github.com/CyrusCKF/stock-gone-wrong&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m19990328"&gt; /u/m19990328 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lqfy1w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqfy1w/a_little_project_to_analyze_stock_trends_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqfy1w/a_little_project_to_analyze_stock_trends_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T03:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqor90</id>
    <title>Build a Multi-Agent AI Investment Advisor using Ollama, LangGraph, and Streamlit</title>
    <updated>2025-07-03T12:49:41+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lqor90/build_a_multiagent_ai_investment_advisor_using/"&gt; &lt;img alt="Build a Multi-Agent AI Investment Advisor using Ollama, LangGraph, and Streamlit" src="https://external-preview.redd.it/jrYQtvyJEJq6ekkw4MqwVzItNTy5yAMF5kFGArliMc8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10054d9e3a79cf2b91c7dfa3d5941441e8535236" title="Build a Multi-Agent AI Investment Advisor using Ollama, LangGraph, and Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/FXPYOq63eWY?si=W7L7eCU1Ad3mOUd3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqor90/build_a_multiagent_ai_investment_advisor_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqor90/build_a_multiagent_ai_investment_advisor_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T12:49:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrai9k</id>
    <title>Ollama and side Hussle</title>
    <updated>2025-07-04T05:10:58+00:00</updated>
    <author>
      <name>/u/penguinlinux</name>
      <uri>https://old.reddit.com/user/penguinlinux</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to drop in and say how much I genuinely love Ollama. I‚Äôm constantly amazed at the quality and range of models available, and the fact that I don‚Äôt even need a GPU to use it blows my mind. I‚Äôm running everything on a small PC with a Ryzen CPU and 32GB of RAM, and it‚Äôs been smooth sailing.&lt;/p&gt; &lt;p&gt;Over the last few months, I‚Äôve been using Ollama not just for fun, but as the foundation of a real side hustle. I‚Äôve been writing and publishing books on KDP, and before anyone rolls their eyes no, it‚Äôs not AI slop.&lt;/p&gt; &lt;p&gt;What makes the difference for me is how I approach it. I‚Äôve crafted a set of advanced prompts that I feed to models like gemma3n, phi4, and llama3.2. I‚Äôve also built some clever Python scripts to orchestrate the whole thing, and I don‚Äôt just stop at generating content. I run everything through layers of agents that review, expand, and refine the material. I‚Äôm often surprised by the quality myself it feels like these books come to life in a way I never imagined possible.&lt;/p&gt; &lt;p&gt;This hasn‚Äôt been an overnight success. It took weeks of trial and error, adjusting prompts, restructuring my workflows, and staying persistent when nothing seemed to work. But now I‚Äôve got over 70 books published, and after a slow start back in March, I'm consistently selling at least 5 books a day. No ads, no gimmicks. Just quietly working in the background, creating value.&lt;/p&gt; &lt;p&gt;I know there‚Äôs a lot of skepticism around AI generated books, and honestly I get it. But I‚Äôm really intentional with my process. I don‚Äôt treat this as a quick cash grab I treat it like real publishing. I want every book I release to actually help and provide value for the buyer like before I post a book i read it and think would i but this if it sucks i scrape it and refine it until I get something that i feel someone would get value from my book.&lt;/p&gt; &lt;p&gt;Huge thanks to the Ollama team and the whole open model ecosystem. This tool gave me the chance to do something creative, meaningful, and profitable all without needing a high-end machine. I‚Äôm excited to keep pushing the boundaries of what‚Äôs possible here. There are many other ideas I have and I am reinvesting money into buying more PC's to create more advanced workflows.&lt;/p&gt; &lt;p&gt;Curious if there are other people doing the same ! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/penguinlinux"&gt; /u/penguinlinux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrai9k/ollama_and_side_hussle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrai9k/ollama_and_side_hussle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrai9k/ollama_and_side_hussle/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T05:10:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq6a33</id>
    <title>It‚Äôs finally here. Thanks to the Ollama community, I'm launching Observer AI v1.0 this Friday üöÄ ‚Äì the open-source agent builder you helped shape.</title>
    <updated>2025-07-02T20:22:22+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community!,&lt;/p&gt; &lt;p&gt;Some of you might remember my earlier posts about a project I was building‚Äîan open-source way to create local AI agents. I've been tinkering, coding, and taking in all your amazing feedback for months. Today, I'm incredibly excited (and a little nervous!) to announce that &lt;strong&gt;Observer AI v1.0 is officially launching this Friday!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For anyone who missed it, &lt;strong&gt;Observer AI üëÅÔ∏è&lt;/strong&gt; is a privacy-first platform for building your own micro-agents that run locally on your machine.&lt;/p&gt; &lt;p&gt;The whole idea started because, like many of you, I was blown away by the power of local models but wanted a simple, powerful way to connect them to my own computer‚Äîto let them see my screen, react to events, and automate tasks without sending my screen data to cloud providers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This Project is a Love Letter to Ollama and This Community&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Observer AI would not exist without Ollama.&lt;/strong&gt; The sheer accessibility and power of what the Ollama team has built was what gave me the vision of this project.&lt;/p&gt; &lt;p&gt;And more importantly, it wouldn't be what it is today without &lt;strong&gt;YOU&lt;/strong&gt;. Every comment, suggestion, and bit of encouragement I've received from this community has directly shaped the features and direction of Observer. You told me what you wanted to see in a local agent platform, and I did my best to build it. So, from the bottom of my heart, &lt;strong&gt;thank you.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Launch This Friday&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The core Observer AI platform is, and will always be, &lt;strong&gt;free and open-source&lt;/strong&gt;. That's non-negotiable.&lt;/p&gt; &lt;p&gt;To help support the project's future development (I'm a solo dev, so server costs and coffee are my main fuel!), I'm also introducing an optional &lt;strong&gt;Observer Pro&lt;/strong&gt; subscription. This will give users unlimited access to the hosted Ob-Server models for those who might not be running a local instance 24/7. It's my way of trying to make the project sustainable long-term.&lt;/p&gt; &lt;p&gt;I'd be incredibly grateful if you'd take a look. Star the repo if you think it's cool, try building an agent, and let me know what you think. I'm building this for you, and your feedback is what will guide v1.1 and beyond.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;App Link:&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub (all the code is here!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Twitter/X:&lt;/strong&gt; &lt;a href="https://x.com/AppObserverAI"&gt;https://x.com/AppObserverAI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out here all day to answer any questions. Let's build some cool stuff together!&lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq6a33/its_finally_here_thanks_to_the_ollama_community/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq6a33/its_finally_here_thanks_to_the_ollama_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lq6a33/its_finally_here_thanks_to_the_ollama_community/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T20:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqlwli</id>
    <title>Best light llm for ocr summarize chat</title>
    <updated>2025-07-03T10:13:04+00:00</updated>
    <author>
      <name>/u/SuperMindHero</name>
      <uri>https://old.reddit.com/user/SuperMindHero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I would like to run a local model 32 ram i7 12g. The goal is OCR for small pdf files max 2pages, summarize of text, chat with limited context and rag logic for specialized knowedge&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperMindHero"&gt; /u/SuperMindHero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqlwli/best_light_llm_for_ocr_summarize_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqlwli/best_light_llm_for_ocr_summarize_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqlwli/best_light_llm_for_ocr_summarize_chat/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T10:13:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lr265k</id>
    <title>Serene Pub v0.3.0 Alpha Released ‚Äî Offline AI Roleplay Client w/ Lorebooks+</title>
    <updated>2025-07-03T22:01:03+00:00</updated>
    <author>
      <name>/u/doolijb</name>
      <uri>https://old.reddit.com/user/doolijb</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doolijb"&gt; /u/doolijb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lr18jg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lr265k/serene_pub_v030_alpha_released_offline_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lr265k/serene_pub_v030_alpha_released_offline_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T22:01:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lragx8</id>
    <title>Need some recommendations</title>
    <updated>2025-07-04T05:08:40+00:00</updated>
    <author>
      <name>/u/Muscle_Rabbit</name>
      <uri>https://old.reddit.com/user/Muscle_Rabbit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago I matched with someone on Tinder. I noticed just after a few hours that I was seemingly talking with an AI. The AI had both an Instagram and Snapchat account and photos that looked very realistic to me. It sparked some interest for me as I would like to tinker with AI as well but after reading about the system requirements I lost interest. &lt;/p&gt; &lt;p&gt;I have a modest gaming rig today with RTX 2060 Super 8GB vram i7 2600 and 12gb ram. It‚Äôs enough for me as most of my gaming is done on my PS5 and Switch and I don‚Äôt want to spend money on a new rig that I will barely use. And even if I was to get a newer rig I‚Äôm not interested in the heat and noise it would generate or the electricity bill for that matter.&lt;/p&gt; &lt;p&gt;I‚Äôm mostly interested in making personalized chatbots with distinct ‚Äùpersonalities‚Äù . That can generate photos of ‚Äùthemselves‚Äù I also want to know how other people can seemingly connect their AIs to various messaging services like instagram and Snapchat.&lt;/p&gt; &lt;p&gt;Also my last question is that I saw that renting is a thing for AI/Cloud services too. I tried finding someone on YouTube comparing them but I could not find so much. I saw that most services charge per hour but if possible I would like to access my AI on demand. I also saw some services having some free tiers/trial before committing. Which services do other people here recommend?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Muscle_Rabbit"&gt; /u/Muscle_Rabbit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lragx8/need_some_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lragx8/need_some_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lragx8/need_some_recommendations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T05:08:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqzzxp</id>
    <title>use ollama with browser</title>
    <updated>2025-07-03T20:28:37+00:00</updated>
    <author>
      <name>/u/RealFullMetal</name>
      <uri>https://old.reddit.com/user/RealFullMetal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lqzzxp/use_ollama_with_browser/"&gt; &lt;img alt="use ollama with browser" src="https://external-preview.redd.it/ICK3WGgddRobWxwgfzh4IdlTfyzsA9tf-6ErBoiqdM0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cab1b8518887baecef9354d7a843dcc3e755426" title="use ollama with browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to be able ask questions on website using local models, so added ollama support in browserOS - &lt;a href="https://github.com/browseros-ai/BrowserOS"&gt;https://github.com/browseros-ai/BrowserOS&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Quick demo :) wdyt?&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1lqzzxp/video/6d6fop82ypaf1/player"&gt;https://reddit.com/link/1lqzzxp/video/6d6fop82ypaf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RealFullMetal"&gt; /u/RealFullMetal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqzzxp/use_ollama_with_browser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqzzxp/use_ollama_with_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqzzxp/use_ollama_with_browser/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T20:28:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lraoso</id>
    <title>Question: Choosing Mac Studio for a "small" MVP project</title>
    <updated>2025-07-04T05:21:28+00:00</updated>
    <author>
      <name>/u/linnk87</name>
      <uri>https://old.reddit.com/user/linnk87</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm developing a small project involving image analysis using gemma3:27b. It looks like it could work, but for my MVP version I kinda need to run this model 24/7 for around 2 weeks.&lt;/p&gt; &lt;p&gt;If the MVP works, I'll need to run it way more (2 months to 1 year) for more experimentation and potentially first customers. &lt;/p&gt; &lt;p&gt;Remember: 24/7 doing inferences. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do you think a Mac Studio M3 Ultra can sustain it?&lt;/li&gt; &lt;li&gt;Or do you think it will burn? lmao&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have a gaming PC with a 4090 where I've been testing my development. It gets pretty hot after a few hours of inference and windows crashed at least once. The MacStudio is way more power efficient (which is also why I think it could be a good option), but for sustained work I'm not sure how stable would it be.&lt;/p&gt; &lt;p&gt;For an MVP the Mac Studio seems perfect: easy to manage, relatively cheap, power efficient, and powerful enough for production. Still, it's $10K I don't want to burn.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/linnk87"&gt; /u/linnk87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lraoso/question_choosing_mac_studio_for_a_small_mvp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lraoso/question_choosing_mac_studio_for_a_small_mvp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lraoso/question_choosing_mac_studio_for_a_small_mvp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T05:21:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqqocr</id>
    <title>Ollama Local AI Journaling App.</title>
    <updated>2025-07-03T14:14:51+00:00</updated>
    <author>
      <name>/u/Frosty-Cap-4282</name>
      <uri>https://old.reddit.com/user/Frosty-Cap-4282</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was born out of a personal need ‚Äî I journal daily , and I didn‚Äôt want to upload my thoughts to some cloud server and also wanted to use AI. So I built Vinaya to be:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Private&lt;/strong&gt;: Everything stays on your device. No servers, no cloud, no trackers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt;: Clean UI built with Electron + React. No bloat, just journaling.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Insightful&lt;/strong&gt;: Semantic search, mood tracking, and AI-assisted reflections (all offline).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Link to the app: &lt;a href="https://vinaya-journal.vercel.app/"&gt;https://vinaya-journal.vercel.app/&lt;/a&gt;&lt;br /&gt; Github: &lt;a href="https://github.com/BarsatKhadka/Vinaya-Journal"&gt;https://github.com/BarsatKhadka/Vinaya-Journal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm not trying to build a SaaS or chase growth metrics. I just wanted something I could trust and use daily. If this resonates with anyone else, I‚Äôd love feedback or thoughts.&lt;/p&gt; &lt;p&gt;If you like the idea or find it useful and want to encourage me to consistently refine it but don‚Äôt know me personally and feel shy to say it ‚Äî just drop a ‚≠ê on GitHub. That‚Äôll mean a lot :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frosty-Cap-4282"&gt; /u/Frosty-Cap-4282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqqocr/ollama_local_ai_journaling_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqqocr/ollama_local_ai_journaling_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqqocr/ollama_local_ai_journaling_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T14:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrfgy8</id>
    <title>Ollama hangs without timeout</title>
    <updated>2025-07-04T10:30:43+00:00</updated>
    <author>
      <name>/u/NaiveWonder4836</name>
      <uri>https://old.reddit.com/user/NaiveWonder4836</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lrfgy8/ollama_hangs_without_timeout/"&gt; &lt;img alt="Ollama hangs without timeout" src="https://b.thumbs.redditmedia.com/NXALozAqIZ1aW9VvUEyPKaSL8NzpqPwsWKy5QrMzNRM.jpg" title="Ollama hangs without timeout" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2d2si40j4uaf1.png?width=860&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=81876cc07e3da5cc2ab0008b0946db17c1b36aaf"&gt;&amp;lt;SOLVED&amp;gt; The port 127.0.0.1:11434 was running a process. After killing it and running this command again, it was solved&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NaiveWonder4836"&gt; /u/NaiveWonder4836 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrfgy8/ollama_hangs_without_timeout/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrfgy8/ollama_hangs_without_timeout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrfgy8/ollama_hangs_without_timeout/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T10:30:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrd7lq</id>
    <title>Two local LLM 4 newbie</title>
    <updated>2025-07-04T08:01:57+00:00</updated>
    <author>
      <name>/u/Powerful-Shine8690</name>
      <uri>https://old.reddit.com/user/Powerful-Shine8690</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wish to initialize my notebook to support two local LLMs (&lt;strong&gt;running NOT at the same time&lt;/strong&gt;).&lt;/p&gt; &lt;p&gt;First'll do:&lt;/p&gt; &lt;p&gt;- Work &lt;strong&gt;only in local&lt;/strong&gt;, w/out Internet access, throught my .md files (write for &lt;a href="http://Obsidian.MD"&gt;Obsidian.MD&lt;/a&gt; platform), about 1K files, in Italian language, then suggest me internal link and indexing datas;&lt;/p&gt; &lt;p&gt;- Trasform scanned text (Jpg, Pic, Jpeg, Png, Pdf and ePub) into text MD files. Scanned texts are writen in Italian, Latin and Ancient Greek;&lt;/p&gt; &lt;p&gt;Second'll do:&lt;/p&gt; &lt;p&gt;- Work locally (but also &lt;strong&gt;online&lt;/strong&gt; if necessary) to help me in JavaScript, CSS, Powershell and Python programming with Microsoft Visual Studio Code.&lt;/p&gt; &lt;p&gt;Here is my configuration:&lt;/p&gt; &lt;p&gt;PC: - Acer Predator PH317-56&lt;/p&gt; &lt;p&gt;CPU: - 12th Gen Intel i7-12700H&lt;/p&gt; &lt;p&gt;RAM: - 2x16Gb Samsung DDR5 x4800 (@2400MHz) + 2 slot free&lt;/p&gt; &lt;p&gt;Graph: - NVIDIA GeForce RTX 3070 Ti Laptop GPU 8Gb GDDR6&lt;/p&gt; &lt;p&gt;2x SSD: - Crucial P3 4TB M.2 2280 PCIe 4.0 NVMe (Os + Progr)&lt;/p&gt; &lt;pre&gt;&lt;code&gt; \- WD Black WDS800T2XHE 8 TB M.2 2280 PCIe 4.0 NVMe (Doc) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Os: - Win 11 Pro updated&lt;/p&gt; &lt;p&gt;What you expert can suggest me? Tnx in advance&lt;/p&gt; &lt;p&gt;Emanuele&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Shine8690"&gt; /u/Powerful-Shine8690 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrd7lq/two_local_llm_4_newbie/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrd7lq/two_local_llm_4_newbie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrd7lq/two_local_llm_4_newbie/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T08:01:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqpsjh</id>
    <title>Ollama based AI presentation generator and API - Gamma Alternative</title>
    <updated>2025-07-03T13:36:35+00:00</updated>
    <author>
      <name>/u/goodboydhrn</name>
      <uri>https://old.reddit.com/user/goodboydhrn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lqpsjh/ollama_based_ai_presentation_generator_and_api/"&gt; &lt;img alt="Ollama based AI presentation generator and API - Gamma Alternative" src="https://preview.redd.it/awcrxuqjwnaf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=aa54d3b167b836a81007137b327da2d5800fd272" title="Ollama based AI presentation generator and API - Gamma Alternative" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Me and my roommates are building Presenton, which is an AI presentation generator that can run entirely on your own device. It has Ollama built in so, all you need is add Pexels (free image provider) API Key and start generating high quality presentations which can be exported to PPTX and PDF. It even works on CPU(can generate professional presentation with as small as 3b models)!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Presentation Generation UI&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It has beautiful user-interface which can be used to create presentations.&lt;/li&gt; &lt;li&gt;7+ beautiful themes to choose from.&lt;/li&gt; &lt;li&gt;Can choose number of slides, languages and themes.&lt;/li&gt; &lt;li&gt;Can create presentation from PDF, PPTX, DOCX, etc files directly.&lt;/li&gt; &lt;li&gt;Export to PPTX, PDF.&lt;/li&gt; &lt;li&gt;Share presentation link.(if you host on public IP)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Presentation Generation over API&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can even host the instance to generation presentation over API. (1 endpoint for all above features)&lt;/li&gt; &lt;li&gt;All above features supported over API&lt;/li&gt; &lt;li&gt;You'll get two links; first the static presentation file (pptx/pdf) which you requested and editable link through which you can edit the presentation and export the file.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love for you to try it out! Very easy docker based setup and deployment.&lt;/p&gt; &lt;p&gt;Here's the github link: &lt;a href="https://github.com/presenton/presenton"&gt;https://github.com/presenton/presenton&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Also check out the docs here: &lt;a href="https://docs.presenton.ai/"&gt;https://docs.presenton.ai&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Feedbacks are very appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goodboydhrn"&gt; /u/goodboydhrn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/awcrxuqjwnaf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqpsjh/ollama_based_ai_presentation_generator_and_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqpsjh/ollama_based_ai_presentation_generator_and_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T13:36:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lri3xg</id>
    <title>nous-hermes2-mixtral asking for ssh access</title>
    <updated>2025-07-04T12:57:26+00:00</updated>
    <author>
      <name>/u/YetToBeTold</name>
      <uri>https://old.reddit.com/user/YetToBeTold</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I am new to this local AI self hosting, and i installed nous-hermes2-mixtral because chatgpt said its good with engineering, anyways i wanted to try a few models till i find the one that suits me, but what happened was I asked the model if it can access a pdf file in a certain directory, and it replied that it needs authority to do so, and asked me to generate an ssh key with ssh-keygen and shared its public key with me so i add it in authorized_keys under ~/.ssh.&lt;/p&gt; &lt;p&gt;Is this normal or dangerous? &lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YetToBeTold"&gt; /u/YetToBeTold &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lri3xg/noushermes2mixtral_asking_for_ssh_access/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lri3xg/noushermes2mixtral_asking_for_ssh_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lri3xg/noushermes2mixtral_asking_for_ssh_access/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T12:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrquyz</id>
    <title>Built an offline AI chat app for macOS that works with local LLMs via Ollama</title>
    <updated>2025-07-04T19:11:35+00:00</updated>
    <author>
      <name>/u/Disastrous-Parsnip93</name>
      <uri>https://old.reddit.com/user/Disastrous-Parsnip93</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a lightweight macOS desktop chat application that runs entirely offline and communicates with local LLMs through Ollama. No internet required once set up!&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;p&gt;- üß† Local LLM integration via Ollama&lt;/p&gt; &lt;p&gt;- üí¨ Clean, modern chat interface with real-time streaming&lt;/p&gt; &lt;p&gt;- üìù Full markdown support with syntax highlighting&lt;/p&gt; &lt;p&gt;- üïò Persistent chat history&lt;/p&gt; &lt;p&gt;- üîÑ Easy model switching&lt;/p&gt; &lt;p&gt;- üé® Auto dark/light theme&lt;/p&gt; &lt;p&gt;- üì¶ Under 20MB final app size&lt;/p&gt; &lt;p&gt;Built with Tauri, React, and Rust for optimal performance. The app automatically detects available Ollama models and provides a native macOS experience.&lt;/p&gt; &lt;p&gt;Perfect for anyone who wants to chat with AI models privately without sending data to external servers. Works great with llama3, codellama, and other Ollama models.&lt;/p&gt; &lt;p&gt;Available on GitHub with releases for macOS. Would love feedback from the community!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/abhijeetlokhande1996/local-chat-releases/releases/download/v0.1.0/Local.Chat_0.1.0_aarch64.dmg"&gt;https://github.com/abhijeetlokhande1996/local-chat-releases/releases/download/v0.1.0/Local.Chat_0.1.0_aarch64.dmg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Parsnip93"&gt; /u/Disastrous-Parsnip93 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrquyz/built_an_offline_ai_chat_app_for_macos_that_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrquyz/built_an_offline_ai_chat_app_for_macos_that_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrquyz/built_an_offline_ai_chat_app_for_macos_that_works/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T19:11:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrpjci</id>
    <title>Please... how can I set the reasoning effortüò≠üò≠</title>
    <updated>2025-07-04T18:14:56+00:00</updated>
    <author>
      <name>/u/Open-Flounder-7194</name>
      <uri>https://old.reddit.com/user/Open-Flounder-7194</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lrpjci/please_how_can_i_set_the_reasoning_effort/"&gt; &lt;img alt="Please... how can I set the reasoning effortüò≠üò≠" src="https://preview.redd.it/ckugsto8dwaf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9407005119d0bbe95bb2c6de4e1a2b491749d3a1" title="Please... how can I set the reasoning effortüò≠üò≠" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried setting it to &amp;quot;none&amp;quot; but it did not seem to work, does Deepseek R1 not support the reasoning effort API or is &amp;quot;none&amp;quot; not an accepted value and it defaulted to medium or something like high? If possible how could I include something like Thinkless to still get reasoning if I need it or at least a button at the prompt window to enable or disable rasoning?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Open-Flounder-7194"&gt; /u/Open-Flounder-7194 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ckugsto8dwaf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrpjci/please_how_can_i_set_the_reasoning_effort/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrpjci/please_how_can_i_set_the_reasoning_effort/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T18:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrtj0m</id>
    <title>Use all your favorite MCP servers in your meetings</title>
    <updated>2025-07-04T21:12:19+00:00</updated>
    <author>
      <name>/u/Square-Test-515</name>
      <uri>https://old.reddit.com/user/Square-Test-515</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lrtj0m/use_all_your_favorite_mcp_servers_in_your_meetings/"&gt; &lt;img alt="Use all your favorite MCP servers in your meetings" src="https://external-preview.redd.it/ZHpxdml1OXJheGFmMf7so8CSE-8PmjQuJPM-OgOW72CEju6_3gCE3GVMC0Pl.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2489d6fca27e489a49dd4b43863047cc47c4ac67" title="Use all your favorite MCP servers in your meetings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;We've been working on an open-source project called joinly for the last two months. The idea is that you can connect your favourite MCP servers (e.g. Asana, Notion and Linear) to an AI agent and send that agent to any browser-based video conference. This essentially allows you to create your own custom meeting assistant that can perform tasks in real time during the meeting.&lt;/p&gt; &lt;p&gt;So, how does it work? Ultimately, joinly is also just a MCP server that you can host yourself, providing your agent with essential meeting tools (such as speak_text and send_chat_message) alongside automatic real-time transcription. By the way, we've designed it so that you can select your own LLM (e.g., Ollama), TTS and STT providers. &lt;/p&gt; &lt;p&gt;We made a quick video to show how it works connecting it to the Tavily and GitHub MCP servers and let joinly explain how joinly works. Because we think joinly best speaks for itself.&lt;/p&gt; &lt;p&gt;We'd love to hear your feedback or ideas on which other MCP servers you'd like to use in your meetings. Or just try it out yourself üëâ &lt;a href="https://github.com/joinly-ai/joinly"&gt;https://github.com/joinly-ai/joinly&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Square-Test-515"&gt; /u/Square-Test-515 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p9inht9raxaf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrtj0m/use_all_your_favorite_mcp_servers_in_your_meetings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrtj0m/use_all_your_favorite_mcp_servers_in_your_meetings/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T21:12:19+00:00</published>
  </entry>
</feed>
