<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-10T20:08:14+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1orglym</id>
    <title>Improving accuracy when extracting structured data from OCR text using Gemma 3</title>
    <updated>2025-11-08T04:54:54+00:00</updated>
    <author>
      <name>/u/Weekly_Signature_510</name>
      <uri>https://old.reddit.com/user/Weekly_Signature_510</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on a project where I extract text from U.S. driver’s license images using OCR. The OCR text itself contains all the necessary information (name, address, license number, etc.), and I also provide a version of the image with bounding boxes for context.&lt;/p&gt; &lt;p&gt;However, even though the OCR output has everything, my LLM (Gemma 3 12B running via Ollama) still misses or misclassifies some fields when structuring the data into JSON.&lt;/p&gt; &lt;p&gt;What can I do to improve extraction accuracy? Would better prompt design, fine-tuning, or additional preprocessing (like spatial grouping or text reformatting) make the biggest difference here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weekly_Signature_510"&gt; /u/Weekly_Signature_510 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1orglym/improving_accuracy_when_extracting_structured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1orglym/improving_accuracy_when_extracting_structured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1orglym/improving_accuracy_when_extracting_structured/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-08T04:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqxqvx</id>
    <title>POC: Model Context Protocol integration for native Ollama app</title>
    <updated>2025-11-07T15:41:54+00:00</updated>
    <author>
      <name>/u/Plenty_Seesaw8878</name>
      <uri>https://old.reddit.com/user/Plenty_Seesaw8878</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oqxqvx/poc_model_context_protocol_integration_for_native/"&gt; &lt;img alt="POC: Model Context Protocol integration for native Ollama app" src="https://preview.redd.it/5bp232couuzf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=d5274f89b68b48ec4343e680e68088f0703597fe" title="POC: Model Context Protocol integration for native Ollama app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;I built a small poc that lets the native ollama app connect to external tools and data sources through the Model Context Protocol.&lt;/p&gt; &lt;p&gt;Made it for personal use and wanted to check if the community would value this before I open a PR.&lt;/p&gt; &lt;p&gt;It’s based on Anthropic’s Go SDK and integrates into the app lifecycle.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plenty_Seesaw8878"&gt; /u/Plenty_Seesaw8878 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5bp232couuzf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqxqvx/poc_model_context_protocol_integration_for_native/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oqxqvx/poc_model_context_protocol_integration_for_native/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-07T15:41:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1os3aou</id>
    <title>chaTTY - A fast AI chat for the terminal</title>
    <updated>2025-11-08T23:04:27+00:00</updated>
    <author>
      <name>/u/kekePower</name>
      <uri>https://old.reddit.com/user/kekePower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I just pushed a few updates to chaTTY to git. Added Sqlite3 on the backend to save chats that can be loaded in later. Also added liner so that you can use the left and right arrow keys to go back and forth to edit the text instead of having to delete everything as it was before.&lt;/p&gt; &lt;p&gt;Works with the Ollama OpenAI-compatible API.&lt;/p&gt; &lt;p&gt;Check it out at &lt;a href="https://labs.promptshield.io/experiments/chatty"&gt;https://labs.promptshield.io/experiments/chatty&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MIT License.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kekePower"&gt; /u/kekePower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1os3aou/chatty_a_fast_ai_chat_for_the_terminal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1os3aou/chatty_a_fast_ai_chat_for_the_terminal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1os3aou/chatty_a_fast_ai_chat_for_the_terminal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-08T23:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1os2jgk</id>
    <title>Enabling web search in a modelfile</title>
    <updated>2025-11-08T22:31:29+00:00</updated>
    <author>
      <name>/u/AdministrativeBlock0</name>
      <uri>https://old.reddit.com/user/AdministrativeBlock0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I use gpt-oss:20b in the GUI I can enable thinking and web search and everything works great. But if I make a modelfile with FROM gpt-oss:20b I don't have those options. Is there something I need to enable or a parameter I have to define in the modelfile? I can't see anything in the docs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdministrativeBlock0"&gt; /u/AdministrativeBlock0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1os2jgk/enabling_web_search_in_a_modelfile/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1os2jgk/enabling_web_search_in_a_modelfile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1os2jgk/enabling_web_search_in_a_modelfile/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-08T22:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oso3jb</id>
    <title>We made a multi-agent framework . Here’s the demo. Break it harder.</title>
    <updated>2025-11-09T16:45:55+00:00</updated>
    <author>
      <name>/u/wikkid_lizard</name>
      <uri>https://old.reddit.com/user/wikkid_lizard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oso3jb/we_made_a_multiagent_framework_heres_the_demo/"&gt; &lt;img alt="We made a multi-agent framework . Here’s the demo. Break it harder." src="https://external-preview.redd.it/4BEVVDWWk0p8l-rmKtxNg3NJqXG3x5Xaq8LtHSZqFmg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3a4d662253c1885f12d0b7f2ea6294a2977e1e3" title="We made a multi-agent framework . Here’s the demo. Break it harder." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since we dropped Laddr about a week ago, a bunch of people on our last post said “cool idea, but show it actually working.”&lt;br /&gt; So we put together a short demo of how to get started with Laddr.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo video:&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=ISeaVNfH4aM"&gt;https://www.youtube.com/watch?v=ISeaVNfH4aM&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/AgnetLabs/laddr"&gt;https://github.com/AgnetLabs/laddr&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Docs:&lt;/strong&gt; &lt;a href="https://laddr.agnetlabs.com"&gt;https://laddr.agnetlabs.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to try weird workflows, force edge cases, or just totally break the orchestration logic.&lt;br /&gt; We’re actively improving based on what hurts.&lt;/p&gt; &lt;p&gt;Also, tell us what you want to see Laddr do next.&lt;br /&gt; Browser agent? research assistant? something chaotic?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wikkid_lizard"&gt; /u/wikkid_lizard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=ISeaVNfH4aM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oso3jb/we_made_a_multiagent_framework_heres_the_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oso3jb/we_made_a_multiagent_framework_heres_the_demo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-09T16:45:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1osq7gs</id>
    <title>Memory architecture</title>
    <updated>2025-11-09T18:07:49+00:00</updated>
    <author>
      <name>/u/Equivalent-Ad-9798</name>
      <uri>https://old.reddit.com/user/Equivalent-Ad-9798</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. So ive been tinkering with a framework I built called SoulCore to see how far a local LLM can go with real persistence and self modeling. Instead of a stateless chat buffer, SoulCore keeps a structured autobiographical memory. It can recall people or schemas that the model created itself dynamically, through detectors, then reflects on them between sessions and updates its beliefs. The goal is to test whether continuity and reflection can make small local models feel more context aware. &lt;/p&gt; &lt;p&gt;It’s still early dev (lots of logging and clean up right now), but so far it maintains stable identity, recalls past sessions, and shows consistent personality over time. &lt;/p&gt; &lt;p&gt;I’m mainly sharing to compare notes. Has anyone here tried similar memory/ reflection setups for local models? Any big issues you’ve managed to overcome? &lt;/p&gt; &lt;p&gt;Sorry if this isn’t allowed. Oh, and I’ve been using Ollama models. I’ve tested it on a few other models as well but I’m currently using dolphin3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent-Ad-9798"&gt; /u/Equivalent-Ad-9798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1osq7gs/memory_architecture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1osq7gs/memory_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1osq7gs/memory_architecture/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-09T18:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ost8w6</id>
    <title>Ollama + Python project: myguru</title>
    <updated>2025-11-09T20:05:24+00:00</updated>
    <author>
      <name>/u/Germfreekai</name>
      <uri>https://old.reddit.com/user/Germfreekai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;br /&gt; Created the following project: &lt;a href="https://github.com/germfreekai/myguru"&gt;https://github.com/germfreekai/myguru&lt;/a&gt;&lt;/p&gt; &lt;p&gt;myguru aims to help developers work on projects that they are not familiarized with, or not even familiarized with the used language, by providing a guru assistant, which is an expert on any project.&lt;/p&gt; &lt;p&gt;You should be able to ask your guru things such as: &amp;quot;How are files being created?&amp;quot; or &amp;quot;Where is the request to this api done?&amp;quot;, etc.&lt;/p&gt; &lt;p&gt;It works integrating ollama and chromedb, 100% python.&lt;/p&gt; &lt;p&gt;Lmk any feedback, and if anyone finds it useful, I would be glad!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Germfreekai"&gt; /u/Germfreekai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ost8w6/ollama_python_project_myguru/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ost8w6/ollama_python_project_myguru/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ost8w6/ollama_python_project_myguru/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-09T20:05:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1osqpvq</id>
    <title>OpenwebUI from other PC</title>
    <updated>2025-11-09T18:27:46+00:00</updated>
    <author>
      <name>/u/paradoxunlimited2022</name>
      <uri>https://old.reddit.com/user/paradoxunlimited2022</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have ollama and openwebUI running in my localhost port 8080. using mostly mixtral amd codellema model. Tried to connect my PC running AI model from other PC in same network using the http:&amp;lt;ip&amp;gt;.8080; doesnt work. Any idea how can I achieve this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paradoxunlimited2022"&gt; /u/paradoxunlimited2022 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1osqpvq/openwebui_from_other_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1osqpvq/openwebui_from_other_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1osqpvq/openwebui_from_other_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-09T18:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1osrrxi</id>
    <title>CPU on self host ollama 1000%</title>
    <updated>2025-11-09T19:08:45+00:00</updated>
    <author>
      <name>/u/Super-Professor519</name>
      <uri>https://old.reddit.com/user/Super-Professor519</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I host a ollama app with gemma3:4b model in a server with 16gb ram. I use caddy as reserve proxy to the ollama port. What I send a request it takes 20+ seconds to respond. &lt;/p&gt; &lt;p&gt;Note I use the /chat endpoint with 2 messages one for system and one for user.&lt;/p&gt; &lt;p&gt;I set the OLLAMA_KEEP_ALLIVE to 86400 so it never sleeps.&lt;/p&gt; &lt;p&gt;How can I speed up the respond time? Any idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Super-Professor519"&gt; /u/Super-Professor519 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1osrrxi/cpu_on_self_host_ollama_1000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1osrrxi/cpu_on_self_host_ollama_1000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1osrrxi/cpu_on_self_host_ollama_1000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-09T19:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1otjeax</id>
    <title>Am i in danger?</title>
    <updated>2025-11-10T17:06:02+00:00</updated>
    <author>
      <name>/u/Minecraft-tlauncher</name>
      <uri>https://old.reddit.com/user/Minecraft-tlauncher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otjeax/am_i_in_danger/"&gt; &lt;img alt="Am i in danger?" src="https://preview.redd.it/e8oqcp7eog0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53d46605b5594f04b3972493e2d6d4154b02f46e" title="Am i in danger?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tinyllama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Minecraft-tlauncher"&gt; /u/Minecraft-tlauncher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e8oqcp7eog0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otjeax/am_i_in_danger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otjeax/am_i_in_danger/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T17:06:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ospgkw</id>
    <title>GPT 5 for Computer Use agents</title>
    <updated>2025-11-09T17:39:02+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ospgkw/gpt_5_for_computer_use_agents/"&gt; &lt;img alt="GPT 5 for Computer Use agents" src="https://external-preview.redd.it/OHA3ZWVjMW5wOTBnMRG6-oWujtVtwWnIPYQYAphLJPDZc9z94p-KY-4O-UR8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff9163fdb86010d07646c010c115ee643423195b" title="GPT 5 for Computer Use agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same tasks, same grounding model we just swapped GPT 4o with GPT 5 as the thinking model.&lt;/p&gt; &lt;p&gt;Left = 4o, right = 5.&lt;/p&gt; &lt;p&gt;Watch GPT 5 pull through.&lt;/p&gt; &lt;p&gt;Grounding model: Salesforce GTA1-7B&lt;/p&gt; &lt;p&gt;Action space: CUA Cloud Instances (macOS/Linux/Windows)&lt;/p&gt; &lt;p&gt;The task is: &amp;quot;Navigate to {random_url} and play the game until you reach a score of 5/5”....each task is set up by having claude generate a random app from a predefined list of prompts (multiple choice trivia, form filling, or color matching)&amp;quot;&lt;/p&gt; &lt;p&gt;Try it yourself here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs : &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agent"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agent&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.gg/cua-ai"&gt;https://discord.gg/cua-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ojia28enp90g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ospgkw/gpt_5_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ospgkw/gpt_5_for_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-09T17:39:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1otbs2d</id>
    <title>Which model is better to create notes from sample?</title>
    <updated>2025-11-10T11:53:43+00:00</updated>
    <author>
      <name>/u/Adventurous-Hunter98</name>
      <uri>https://old.reddit.com/user/Adventurous-Hunter98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I need to create lots of notes from a sample note with a note that has list of data.&lt;/p&gt; &lt;p&gt;Which model achieves to do this?&lt;/p&gt; &lt;p&gt;For example; note sample has&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Title:&lt;br /&gt; Date:&lt;br /&gt; Description:&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;and I have a list of these datas in a note like below&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;title1, date1, description1&lt;/p&gt; &lt;p&gt;title2, date2, description2&lt;/p&gt; &lt;p&gt;title3, date3, description3 ...&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Hunter98"&gt; /u/Adventurous-Hunter98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otbs2d/which_model_is_better_to_create_notes_from_sample/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otbs2d/which_model_is_better_to_create_notes_from_sample/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otbs2d/which_model_is_better_to_create_notes_from_sample/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T11:53:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1otgf7i</id>
    <title>Ollama not finishing thoughts/replies.</title>
    <updated>2025-11-10T15:16:09+00:00</updated>
    <author>
      <name>/u/New-Maintenance2371</name>
      <uri>https://old.reddit.com/user/New-Maintenance2371</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm new to Ollama. I've been using Ollama gpt-oss:120b-cloud for two days, primarily for assistance in programming. The last time I tried writing it a request, it thinks for 5-7 seconds and then it stops. It doesn't finish what it thinks and sometimes when it does, it doesn't finish the sentence when replying, it simply cuts out. I've decided to wait for a week to let it cool down but the issue persists. I did not run out of my Hourly/Weekly usages. The problem is still present and it's frustrating. Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New-Maintenance2371"&gt; /u/New-Maintenance2371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otgf7i/ollama_not_finishing_thoughtsreplies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otgf7i/ollama_not_finishing_thoughtsreplies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otgf7i/ollama_not_finishing_thoughtsreplies/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T15:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1otgiyg</id>
    <title>Wiredigg now integrates Ollama for AI-powered network analysis + new packet visualization engine!</title>
    <updated>2025-11-10T15:20:13+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otgiyg/wiredigg_now_integrates_ollama_for_aipowered/"&gt; &lt;img alt="Wiredigg now integrates Ollama for AI-powered network analysis + new packet visualization engine!" src="https://external-preview.redd.it/ii2qXwOt9q-3JoNEi3AZTjVgmUTUDvnX5B70rzCS6xA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c066983a82b9b18a5d4a22fc39abfcfd82be25a0" title="Wiredigg now integrates Ollama for AI-powered network analysis + new packet visualization engine!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Zrufy/wiredigg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otgiyg/wiredigg_now_integrates_ollama_for_aipowered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otgiyg/wiredigg_now_integrates_ollama_for_aipowered/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T15:20:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1otgj9y</id>
    <title>PolyMCP — Giving LLM Agents Real Multi-Tool Intelligence</title>
    <updated>2025-11-10T15:20:35+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otgj9y/polymcp_giving_llm_agents_real_multitool/"&gt; &lt;img alt="PolyMCP — Giving LLM Agents Real Multi-Tool Intelligence" src="https://external-preview.redd.it/Msm-QajuVHOOiFNkJqYJNVPFdKfyURY_aL6fbzgG9Vc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1c84385768301e5c01a6756534af82406a9b572" title="PolyMCP — Giving LLM Agents Real Multi-Tool Intelligence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/Polymcp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otgj9y/polymcp_giving_llm_agents_real_multitool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otgj9y/polymcp_giving_llm_agents_real_multitool/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T15:20:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1otgjmm</id>
    <title>You don’t need the biggest model: how LLM-Use helps humans solve complex problems</title>
    <updated>2025-11-10T15:20:59+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otgjmm/you_dont_need_the_biggest_model_how_llmuse_helps/"&gt; &lt;img alt="You don’t need the biggest model: how LLM-Use helps humans solve complex problems" src="https://external-preview.redd.it/RRYPD6RWnoPPKoEXXdxyjsRQ413b2PstPkKrMO7uaos.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c17d5626dd71809e0d938b9fafdadf8a3bf4450e" title="You don’t need the biggest model: how LLM-Use helps humans solve complex problems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/llm-use-agentic"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otgjmm/you_dont_need_the_biggest_model_how_llmuse_helps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otgjmm/you_dont_need_the_biggest_model_how_llmuse_helps/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T15:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1otiwgj</id>
    <title>Thinking Edge LLMS , are dumber for non thinking and reasoning tasks even with nothink mode</title>
    <updated>2025-11-10T16:48:14+00:00</updated>
    <author>
      <name>/u/Tan442</name>
      <uri>https://old.reddit.com/user/Tan442</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tan442"&gt; /u/Tan442 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LLMDevs/comments/1otiv47/thinking_edge_llms_are_dumber_for_non_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otiwgj/thinking_edge_llms_are_dumber_for_non_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otiwgj/thinking_edge_llms_are_dumber_for_non_thinking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T16:48:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ota5ou</id>
    <title>GLM-4.6-REAP any good for coding? Min VRAM+RAM?</title>
    <updated>2025-11-10T10:17:23+00:00</updated>
    <author>
      <name>/u/WaitformeBumblebee</name>
      <uri>https://old.reddit.com/user/WaitformeBumblebee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using mostly QWEN3 variants (&amp;lt;20GB) for python coding tasks. Would 16GB VRAM + 64GB RAM be able to &amp;quot;run&amp;quot; (I don't mind waiting some minutes if the answer is much better) 72GB model like &lt;a href="https://ollama.com/MichelRosselli/GLM-4.6-REAP-218B-A32B-FP8-mixed-AutoRound"&gt;https://ollama.com/MichelRosselli/GLM-4.6-REAP-218B-A32B-FP8-mixed-AutoRound&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and how good is it? Been hearing high praise for GLM-4.5-AIR, but don't want to download &amp;gt;70GB for nothing. Perhaps I'd be better of with GLM-4.5-Air:Q2_K at 45GB ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WaitformeBumblebee"&gt; /u/WaitformeBumblebee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ota5ou/glm46reap_any_good_for_coding_min_vramram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ota5ou/glm46reap_any_good_for_coding_min_vramram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ota5ou/glm46reap_any_good_for_coding_min_vramram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T10:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1otlpy9</id>
    <title>Mimir - OSS memory bank and file indexer + MCP http server ++ under MIT license.</title>
    <updated>2025-11-10T18:29:33+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/GithubCopilot/comments/1otlo8c/mimir_oss_memory_bank_and_file_indexer_mcp_http/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otlpy9/mimir_oss_memory_bank_and_file_indexer_mcp_http/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otlpy9/mimir_oss_memory_bank_and_file_indexer_mcp_http/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T18:29:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot3pp8</id>
    <title>Granite 4 micro-h doing great on my older pc</title>
    <updated>2025-11-10T03:47:18+00:00</updated>
    <author>
      <name>/u/newz2000</name>
      <uri>https://old.reddit.com/user/newz2000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"&gt; &lt;img alt="Granite 4 micro-h doing great on my older pc" src="https://a.thumbs.redditmedia.com/bbtxjoQlGAnti-gN1X_64JqMNQSAnCRZif4YFLrMqj0.jpg" title="Granite 4 micro-h doing great on my older pc" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/nars9rownc0g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2d27ce0136bd0fc08a26e32900ff809de4c39dd"&gt;https://preview.redd.it/nars9rownc0g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2d27ce0136bd0fc08a26e32900ff809de4c39dd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My older 7th Gen i5 gaming computer has been repurposed into my local llm workhorse for most of this year. I use it to automate tasks. In this example, extract key dates and information from an email producing the results in JSON format. &lt;/p&gt; &lt;p&gt;I have been using Qwen 3 and Gemma 3 and I'd say if I want to have a conversation, Qwen 3:8b is my favorite. But it's not good at instruction following. Gemma 3:4b really does great all around and is very quick on this computer. But for instruction following, Granite 4 micro-h is tough to beat.&lt;/p&gt; &lt;p&gt;I have not yet tested it with tool calling, but this is something I want to do and is what made me check out Granite.&lt;/p&gt; &lt;p&gt;Since you can kinda see my prompt through the translucent window, I'll save you the effort and put it in here. &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;You are an assistant that extracts litigation-relevant structured data from incoming court notices that arrive via email or plaintext.&lt;/p&gt; &lt;p&gt;Read the following email or document VERY CAREFULLY.&lt;/p&gt; &lt;p&gt;Then output ONLY JSON.&lt;/p&gt; &lt;p&gt;Do not summarize.&lt;/p&gt; &lt;p&gt;Do not infer beyond what is explicitly written.&lt;/p&gt; &lt;p&gt;If a field cannot be determined, return null — do NOT guess.&lt;/p&gt; &lt;p&gt;You must return ONLY this exact JSON structure (no explanation):&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;case_name&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;case_number&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;court&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;hearing_date&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;hearing_time&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;presiding_judge&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;filing_date_of_order&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;required_filing_deadlines&amp;quot;: [],&lt;/p&gt; &lt;p&gt;&amp;quot;parties_involved&amp;quot;: [],&lt;/p&gt; &lt;p&gt;&amp;quot;topic_or_subject_matter&amp;quot;: &amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;Return ONLY a JSON object with EXACTLY these keys:&lt;/p&gt; &lt;p&gt;case_name, case_number, court, hearing_date, hearing_time, presiding_judge,&lt;/p&gt; &lt;p&gt;filing_date_of_order, required_filing_deadlines, parties_involved, topic_or_subject_matter.&lt;/p&gt; &lt;p&gt;If a value is unknown, set null. Do NOT add any other keys or sections.&lt;/p&gt; &lt;p&gt;Dates = YYYY-MM-DD. Times = 24-hour local-to-court (e.g., 13:30).&lt;/p&gt; &lt;p&gt;Include ONLY human names in `parties_involved` (no emails). Remove HTML entities.&lt;/p&gt; &lt;p&gt;Rules:&lt;/p&gt; &lt;p&gt;hearing_date and hearing_time must be extracted if a hearing is set.&lt;/p&gt; &lt;p&gt;required_filing_deadlines must list ONLY dates that represent “something is due” by “a date certain.”&lt;/p&gt; &lt;p&gt;parties_involved should list all names referenced as parties, attorneys, or counsel receiving service.&lt;/p&gt; &lt;p&gt;The topic_or_subject_matter is a single short clause describing WHAT the order is about (motion type, hearing type, etc).&lt;/p&gt; &lt;p&gt;Dates must be formatted YYYY-MM-DD.&lt;/p&gt; &lt;p&gt;Times must be formatted 24 hour format, local to the court when stated (CST → convert to 24h).&lt;/p&gt; &lt;p&gt;DO NOT return anything not inside the JSON block.&lt;/p&gt; &lt;p&gt;EMAIL:&lt;/p&gt; &lt;p&gt;…&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newz2000"&gt; /u/newz2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T03:47:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1otnogr</id>
    <title>HOW DO I DO THIS</title>
    <updated>2025-11-10T19:40:08+00:00</updated>
    <author>
      <name>/u/Minecraft-tlauncher</name>
      <uri>https://old.reddit.com/user/Minecraft-tlauncher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otnogr/how_do_i_do_this/"&gt; &lt;img alt="HOW DO I DO THIS" src="https://external-preview.redd.it/0vxFDxo53p-qtsifbP0aBQuFAgn8-6d3UDzff95MmEM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4d823b7229949e6d21ed9ed57ad324369ffb1a7" title="HOW DO I DO THIS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im guessing this can be done with any local ai, in ollama aswell probably&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Minecraft-tlauncher"&gt; /u/Minecraft-tlauncher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/WP5_XJY_P0Q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otnogr/how_do_i_do_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otnogr/how_do_i_do_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T19:40:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot3u7f</id>
    <title>Speculative decoding: Faster inference for local LLMs over the network?</title>
    <updated>2025-11-10T03:53:54+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ot3u7f/speculative_decoding_faster_inference_for_local/"&gt; &lt;img alt="Speculative decoding: Faster inference for local LLMs over the network?" src="https://preview.redd.it/70p6li1poc0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6daaaeff166a74d20a883ce88ad7a5a9b3feaf6" title="Speculative decoding: Faster inference for local LLMs over the network?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am gearing up for a big release to add support for speculative decoding for LLMs and looking for early feedback.&lt;/p&gt; &lt;p&gt;First a bit of context, speculative decoding is a technique whereby a draft model (usually a smaller LLM) is engaged to produce tokens and the candidate set produced is verified by a target model (usually a larger model). The set of candidate tokens produced by a draft model must be verifiable via logits by the target model. While tokens produced are serial, verification can happen in parallel which can lead to significant improvements in speed.&lt;/p&gt; &lt;p&gt;This is what OpenAI uses to accelerate the speed of its responses especially in cases where outputs can be guaranteed to come from the same distribution, where:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;propose(x, k) → τ # Draft model proposes k tokens based on context x verify(x, τ) → m # Target verifies τ, returns accepted count m continue_from(x) # If diverged, resume from x with target model &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So I am thinking of adding support to &lt;a href="https://github.com/katanemo/archgw"&gt;arch&lt;/a&gt; (a models-native sidecar proxy for agents). And the developer experience could be something along the following lines:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;POST /v1/chat/completions { &amp;quot;model&amp;quot;: &amp;quot;target:gpt-large@2025-06&amp;quot;, &amp;quot;speculative&amp;quot;: { &amp;quot;draft_model&amp;quot;: &amp;quot;draft:small@v3&amp;quot;, &amp;quot;max_draft_window&amp;quot;: 8, &amp;quot;min_accept_run&amp;quot;: 2, &amp;quot;verify_logprobs&amp;quot;: false }, &amp;quot;messages&amp;quot;: [...], &amp;quot;stream&amp;quot;: true } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here the max_draft_window is the number of tokens to verify, the max_accept_run tells us after how many failed verifications should we give up and just send all the remaining traffic to the target model etc. Of course this work assumes a low RTT between the target and draft model so that speculative decoding is faster without compromising quality.&lt;/p&gt; &lt;p&gt;Question: how would you feel about this functionality? Could you see it being useful for your LLM-based applications? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/70p6li1poc0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ot3u7f/speculative_decoding_faster_inference_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ot3u7f/speculative_decoding_faster_inference_for_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T03:53:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1otbtp2</id>
    <title>Ollama-powered open source single-stock analysis tool with Python, including ratios/news analysis/LSTM forecast</title>
    <updated>2025-11-10T11:56:09+00:00</updated>
    <author>
      <name>/u/gruquilla</name>
      <uri>https://old.reddit.com/user/gruquilla</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otbtp2/ollamapowered_open_source_singlestock_analysis/"&gt; &lt;img alt="Ollama-powered open source single-stock analysis tool with Python, including ratios/news analysis/LSTM forecast" src="https://preview.redd.it/9hn8znm45f0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22404009b637970ab1e3ab5561cb65759c99e1c7" title="Ollama-powered open source single-stock analysis tool with Python, including ratios/news analysis/LSTM forecast" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good morning everyone,&lt;/p&gt; &lt;p&gt;I am currently a MSc Fintech student at Aston University (Birmingham, UK) and Audencia Business School (Nantes, France). Alongside my studies, I've started to develop a few personal Python projects.&lt;/p&gt; &lt;p&gt;My first big open-source project: A single-stock analysis tool that uses both market and financial statements informations. It also integrates news sentiment analysis (FinBert and Pygooglenews), as well as LSTM forecast for the stock price. You can also enable Ollama to get information complements using a local LLM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What my project (FinAPy) does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prologue: Ticker input collection and essential functions and data: &lt;em&gt;In this part, the program gets in input a ticker from the user, and asks wether or not he wants to enable the AI analysis. Then, it generates a short summary about the company fetching information from Yahoo Finance, so the user has something to read while the next step proceeds. It also fetches the main financial metrics and computes additional ones.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Step 1: Events and news fetching: &lt;em&gt;This part fetches stock events from Yahoo Finance and news from Google RSS feed. It also generates a sentiment analysis about the articles fetched using FinBERT.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 2: Forecast using Machine Learning LSTM: &lt;em&gt;This part creates a baseline scenario from a LSTM forecast. The forecast covers 60 days and is trained from 100 last values of close/ high/low prices. It is a quantiative model only. An optimistic and pessimistic scenario are then created by tweaking the main baseline to give a window of prediction. They do not integrate macroeconomic factors, specific metric variations nor Monte Carlo simulations for the moment.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 3: Market data restitution: &lt;em&gt;This part is dedicated to restitute graphically the previously computed data. It also computes CFA classical metrics (histogram of returns, skewness, kurtosis) and their explanation. The part concludes with an Ollama AI commentary of the analysis.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 4: Financial statement analysis: &lt;em&gt;This part is dedicated to the generation of the main ratios from the financial statements of the last 3 years of the company. Each part concludes with an Ollama AI commentary on the ratios. The analysis includes an overview of the variation, and highlights in color wether the change is positive or negative. Each ratio is commented so you can understand what they represent/ how they are calculated. The ratios include:&lt;/em&gt; &lt;ul&gt; &lt;li&gt;Profitability ratios: Profit margin, ROA, ROCE, ROE,...&lt;/li&gt; &lt;li&gt;Asset related ratios: Asset turnover, working capital.&lt;/li&gt; &lt;li&gt;Liquidity ratios: Current ratio, quick ratio, cash ratio.&lt;/li&gt; &lt;li&gt;Solvency ratios: debt to assets, debt to capital, financial leverage, coverage ratios,...&lt;/li&gt; &lt;li&gt;Operational ratios (cashflow related): CFI/ CFF/ CFO ratios, cash return on assets,...&lt;/li&gt; &lt;li&gt;Bankrupcy and financial health scores: Altman Z-score/ Ohlson O-score.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Appendix: Financial statements: &lt;em&gt;A summary of the financial statements scaled for better readability in case you want to push the manual analysis further.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Target audience:&lt;/strong&gt; Students, researchers,... For educational and research purpose only. However, it illustrates how local LLMs could be integrated into industry practices and workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Comparison:&lt;/strong&gt; The project enables both a market and statement analysis perspective, and showcases how a local LLM can run in a financial context while showing to which extent it can bring something to analysts.&lt;/p&gt; &lt;p&gt;At this point, I'm considering starting to work on industry metrics (for comparability of ratios) and portfolio construction. Thank you in advance for your insights, I’m keen to refine this further with input from the community!&lt;/p&gt; &lt;p&gt;The repository: &lt;a href="https://github.com/gruquilla/FinAPy"&gt;gruquilla/FinAPy: Single-stock analysis using Python and local machine learning/ AI tools (Ollama, LSTM).&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gruquilla"&gt; /u/gruquilla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9hn8znm45f0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otbtp2/ollamapowered_open_source_singlestock_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otbtp2/ollamapowered_open_source_singlestock_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T11:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1otbtrh</id>
    <title>Built a local chat UI for Ollama - thought I'd share</title>
    <updated>2025-11-10T11:56:14+00:00</updated>
    <author>
      <name>/u/neiellcare</name>
      <uri>https://old.reddit.com/user/neiellcare</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a web interface for Ollama that stores everything locally. No external servers, all conversations stay on your machine.&lt;/p&gt; &lt;p&gt;Main features: - Memory system so the AI remembers context between chats - Upload documents (PDFs, Word files) for the AI to reference - Web search integration when you need current information - Works with vision models like LLaVA - Live preview for code the AI generates&lt;/p&gt; &lt;p&gt;Everything runs in Docker or you can run it locally with Node. It uses React and TypeScript, stores data in IndexedDB in your browser.&lt;/p&gt; &lt;p&gt;I built it because I wanted something privacy-focused that also had RAG and conversation memory in one place. Works well for me, figured others might find it useful.&lt;/p&gt; &lt;p&gt;It's open source if anyone wants to check it out or contribute. Happy to answer questions about how it works.&lt;/p&gt; &lt;p&gt;Look for &lt;code&gt;Symchat&lt;/code&gt; in Github.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neiellcare"&gt; /u/neiellcare &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otbtrh/built_a_local_chat_ui_for_ollama_thought_id_share/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otbtrh/built_a_local_chat_ui_for_ollama_thought_id_share/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otbtrh/built_a_local_chat_ui_for_ollama_thought_id_share/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T11:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1otixju</id>
    <title>Ollama working well on the vs code</title>
    <updated>2025-11-10T16:49:23+00:00</updated>
    <author>
      <name>/u/Dry_Shower287</name>
      <uri>https://old.reddit.com/user/Dry_Shower287</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otixju/ollama_working_well_on_the_vs_code/"&gt; &lt;img alt="Ollama working well on the vs code" src="https://preview.redd.it/fotbt1hplg0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aff4a58a24ebef77a98f660883ac714cad18fdef" title="Ollama working well on the vs code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Shower287"&gt; /u/Dry_Shower287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fotbt1hplg0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otixju/ollama_working_well_on_the_vs_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otixju/ollama_working_well_on_the_vs_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T16:49:23+00:00</published>
  </entry>
</feed>
