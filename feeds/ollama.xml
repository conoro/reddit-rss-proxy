<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-10-19T17:05:44+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1o83ejq</id>
    <title>Ollama's cloud what‚Äôs the limits?</title>
    <updated>2025-10-16T11:17:16+00:00</updated>
    <author>
      <name>/u/CertainTime5947</name>
      <uri>https://old.reddit.com/user/CertainTime5947</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anybody paying for access to the cloud hosted models? This might be interesting depending on the limits, calls per hour, tokens per day etc, but I can for my life not find any info on this. In the docs they write &amp;quot;Ollama's cloud includes hourly and daily limits to avoid capacity issues&amp;quot; ok.. and they are?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CertainTime5947"&gt; /u/CertainTime5947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o83ejq/ollamas_cloud_whats_the_limits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o83ejq/ollamas_cloud_whats_the_limits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o83ejq/ollamas_cloud_whats_the_limits/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T11:17:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7z1y3</id>
    <title>How to pick the best ollama model for your use case.</title>
    <updated>2025-10-16T06:40:18+00:00</updated>
    <author>
      <name>/u/evalProtocol</name>
      <uri>https://old.reddit.com/user/evalProtocol</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o7z1y3/how_to_pick_the_best_ollama_model_for_your_use/"&gt; &lt;img alt="How to pick the best ollama model for your use case." src="https://external-preview.redd.it/5Yyi7FZfBglJXdTAq5ctyvLjdHtxUhbYkAAZztvAOSg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=555cf9a9171ccbc0dd2a187ee6851a61b8931671" title="How to pick the best ollama model for your use case." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey I am Benny, I have been working on &lt;a href="http://evalprotocol.io"&gt;evalprotocol.io&lt;/a&gt; for a while now, and we recently published a post on using evaluations to pick the best local model to get your job done &lt;a href="https://fireworks.ai/blog/llm-judge-eval-protocol-ollama"&gt;https://fireworks.ai/blog/llm-judge-eval-protocol-ollama&lt;/a&gt; . The SDK is here &lt;a href="https://github.com/eval-protocol/python-sdk"&gt;https://github.com/eval-protocol/python-sdk&lt;/a&gt; , totally open source, and would love to figure out how to best work together with everyone. Please give it a try and let me know if you have any feedback!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wez92nzz5fvf1.png?width=2454&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55544f5acd4332e6977c7be82e44f0fa9ea9edda"&gt;https://preview.redd.it/wez92nzz5fvf1.png?width=2454&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55544f5acd4332e6977c7be82e44f0fa9ea9edda&lt;/a&gt;&lt;/p&gt; &lt;p&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/evalProtocol"&gt; /u/evalProtocol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7z1y3/how_to_pick_the_best_ollama_model_for_your_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7z1y3/how_to_pick_the_best_ollama_model_for_your_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o7z1y3/how_to_pick_the_best_ollama_model_for_your_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T06:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o84ndg</id>
    <title>Configuring GPT OSS 20B for smaller systems</title>
    <updated>2025-10-16T12:20:54+00:00</updated>
    <author>
      <name>/u/Birdinhandandbush</name>
      <uri>https://old.reddit.com/user/Birdinhandandbush</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If this has been answered I've missed it so I apologise. When running GPT-OSS 20B on my LM Studio instance I can set number of experts and reasoning effort, so I can still run on a GTX1660ti and get about 15 tokens/sec with 6gb VRAM and 32gb system ram. &lt;/p&gt; &lt;p&gt;In Ollama and Open WebUI I can't see where I can make the same adjustments, the number of experts setting isn't in an obvious place IMO. &lt;/p&gt; &lt;p&gt;At present on the Ollama + Open WebUi is giving me 7 tokens/sec but I can't configure it from what I can see. &lt;/p&gt; &lt;p&gt;Any help appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Birdinhandandbush"&gt; /u/Birdinhandandbush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o84ndg/configuring_gpt_oss_20b_for_smaller_systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o84ndg/configuring_gpt_oss_20b_for_smaller_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o84ndg/configuring_gpt_oss_20b_for_smaller_systems/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T12:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8j7ne</id>
    <title>Inconsistent code generation and poor Python script updates with Local LLM</title>
    <updated>2025-10-16T21:43:16+00:00</updated>
    <author>
      <name>/u/BomaSanto</name>
      <uri>https://old.reddit.com/user/BomaSanto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What am I doing wrong?&lt;/p&gt; &lt;p&gt;I've been testing both &lt;em&gt;cline&lt;/em&gt; and &lt;em&gt;OpenCode&lt;/em&gt; inside VS Code to generate simple Python code. However, the results are highly inconsistent, lots of repetition, and updates to existing scripts often fail or get ignored.&lt;/p&gt; &lt;p&gt;What might I be doing wrong?&lt;/p&gt; &lt;p&gt;I've tried several Qwen-based models, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;qwen3-30b-a3b-python-coder-i1&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;opencodeedit-qwen3-8b@q8_0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen/qwen3-coder-30b&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also tested:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;openai/gpt-oss-20b&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any tips on improving reliability or reducing redundancy?&lt;/p&gt; &lt;p&gt;- I've already set the parametes like K, P etc according to the advice of Qwen model card&lt;br /&gt; - Tried different prompts&lt;/p&gt; &lt;p&gt;Also lots of these messages:&lt;br /&gt; Cline uses complex prompts and iterative task execution that may be challenging for less capable models. For best results, it's recommended to use Claude 4 Sonnet for its advanced agentic coding capabilities.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BomaSanto"&gt; /u/BomaSanto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o8j7ne/inconsistent_code_generation_and_poor_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o8j7ne/inconsistent_code_generation_and_poor_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o8j7ne/inconsistent_code_generation_and_poor_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T21:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o889pg</id>
    <title>Distil-PII: family of PII redaction SLMs</title>
    <updated>2025-10-16T14:52:18+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o889pg/distilpii_family_of_pii_redaction_slms/"&gt; &lt;img alt="Distil-PII: family of PII redaction SLMs" src="https://external-preview.redd.it/O_gL1U3L1p3-vSC-SY63BYIemJsMFDjZscM68KkUT8E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7099e0a2b4708e42b79cdffbd672a6712e564209" title="Distil-PII: family of PII redaction SLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We trained and released a family of small language models (SLMs) specialized for policy-aware PII redaction. The 1B model, which can be deployed locally with ollama, matches a frontier 600B+ LLM model (DeepSeek 3.1) in prediction accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/distil-labs/Distil-PII"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o889pg/distilpii_family_of_pii_redaction_slms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o889pg/distilpii_family_of_pii_redaction_slms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T14:52:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8rs69</id>
    <title>Brand new ollama install on Linux Mint - not accessible from another computer</title>
    <updated>2025-10-17T04:22:52+00:00</updated>
    <author>
      <name>/u/Punnalackakememumu</name>
      <uri>https://old.reddit.com/user/Punnalackakememumu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have loaded up ollama on a Linux Mint testbed. From the terminal window on the Mint system, &lt;strong&gt;it is functioning&lt;/strong&gt; and I have had brief conversations with it.&lt;/p&gt; &lt;p&gt;I want to expose it to other computers inside my home network (for security reasons, let's call it the &lt;a href="http://192.168.0.0/24"&gt;192.168.0.0/24&lt;/a&gt; network) so they can use the ollama AI from their web browsers.&lt;/p&gt; &lt;p&gt;I ran &lt;code&gt;sudo systemctl edit ollama.service&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I added the following in the upper portion of the file:&lt;br /&gt; &lt;code&gt;[Service]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;OLLAMA_HOST=0.0.0.0&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;OLLAMA_ORIGINS=*&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;and then exited the editor by hitting CTRL+X, told it &amp;quot;Y&amp;quot; to save the file.&lt;/p&gt; &lt;p&gt;Then I switched to another terminal window where I had previously stopped ollama with &lt;code&gt;/bye&lt;/code&gt; and I ran &lt;code&gt;sudo systemctl restart ollama&lt;/code&gt;. Finally, I executed &lt;code&gt;ollama run dolphin-mistral:7b-v2.8&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;When I try and access the ollama instance from a Windows system using Firefox, I get:&lt;br /&gt; &lt;code&gt;Firefox can‚Äôt establish a connection to the server at 192.168.0.100:11434.&lt;/code&gt; &lt;/p&gt; &lt;p&gt;If I test it on the Mint server in Firefox using &lt;a href="http://127.0.0.1:11434"&gt;127.0.0.1:11434&lt;/a&gt;, it reports &amp;quot;Ollama is running.&amp;quot; However, if I use &lt;a href="http://192.168.0.100:1134"&gt;192.168.0.100:1134&lt;/a&gt;, it displays the Firefox &amp;quot;Unable to connect&amp;quot; page.&lt;/p&gt; &lt;p&gt;Other possibly helpful facts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;UFW is not running on the Mint Server&lt;/li&gt; &lt;li&gt;&lt;code&gt;netstat -tuln&lt;/code&gt; reports that the Mint server is LISTENing on 127.0.0.1:11434.&lt;/li&gt; &lt;li&gt;The Linux Mint server is a DHCP client, but the router that issued the IP address has a MAC reservation for it so there's not a conflict.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm trying to learn how to do this to potentially use it later on in my career field, so I'd appreciate the assistance.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Punnalackakememumu"&gt; /u/Punnalackakememumu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o8rs69/brand_new_ollama_install_on_linux_mint_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o8rs69/brand_new_ollama_install_on_linux_mint_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o8rs69/brand_new_ollama_install_on_linux_mint_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-17T04:22:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o831a8</id>
    <title>Looking for a good agentic coding model that fits into Apple M1 Max, 32 GB</title>
    <updated>2025-10-16T10:56:53+00:00</updated>
    <author>
      <name>/u/ThingRexCom</name>
      <uri>https://old.reddit.com/user/ThingRexCom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o831a8/looking_for_a_good_agentic_coding_model_that_fits/"&gt; &lt;img alt="Looking for a good agentic coding model that fits into Apple M1 Max, 32 GB" src="https://preview.redd.it/urja35ayfgvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=728e6b36df2275f162e72765721dcdc07a926e66" title="Looking for a good agentic coding model that fits into Apple M1 Max, 32 GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a huge fan of agentic coding using CLI (i.e., Gemini CLI). I want to create a local setup on Apple M1 Max 32 GB providing similar experience.&lt;/p&gt; &lt;p&gt;Currently, my best setup is Opencode + llama.cpp + gpt-oss-20b.&lt;/p&gt; &lt;p&gt;I have tried other models from HF marked as compatible with my hardware, but most of them failed to start:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable) ggml_metal_synchronize: error: command buffer 0 failed with status 5 error: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory) /private/tmp/llama.cpp-20251013-5280-4lte0l/ggml/src/ggml-metal/ggml-metal-context.m:241: fatal error &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Any recommendation regarding the LLM and fine-tuning my setup is very welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThingRexCom"&gt; /u/ThingRexCom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/urja35ayfgvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o831a8/looking_for_a_good_agentic_coding_model_that_fits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o831a8/looking_for_a_good_agentic_coding_model_that_fits/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T10:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1o985o3</id>
    <title>Smollm:135m hallucinates</title>
    <updated>2025-10-17T17:49:39+00:00</updated>
    <author>
      <name>/u/AnaverageuserX</name>
      <uri>https://old.reddit.com/user/AnaverageuserX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's good but confusing, as without a system parameter it believes it's someone else. NSFW because I asked it a question with a cuss word.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnaverageuserX"&gt; /u/AnaverageuserX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mcbz562lmpvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o985o3/smollm135m_hallucinates/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o985o3/smollm135m_hallucinates/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-17T17:49:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8czoo</id>
    <title>AI chess showdown: comparing LLM vs LLM using Ollama ‚Äì check out this small project</title>
    <updated>2025-10-16T17:46:39+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o8czoo/ai_chess_showdown_comparing_llm_vs_llm_using/"&gt; &lt;img alt="AI chess showdown: comparing LLM vs LLM using Ollama ‚Äì check out this small project" src="https://external-preview.redd.it/aHzSussgql1XiPGJK_HAL4uPNcl6xyTxYMBIRF1BOHk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecadb06e9853de3808f39bee844562c0594d955b" title="AI chess showdown: comparing LLM vs LLM using Ollama ‚Äì check out this small project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I made a cool little open-source tool: &lt;strong&gt;chess-llm-vs-llm&lt;/strong&gt;. &lt;a href="https://github.com/Laszlobeer/chess-llm-vs-llm"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/g8nr15yjhivf1.gif"&gt;https://i.redd.it/g8nr15yjhivf1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;üß† What it does&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;It connects with &lt;strong&gt;Ollama&lt;/strong&gt; to let you pit two language models (LLMs) against each other in chess matches. &lt;a href="https://github.com/Laszlobeer/chess-llm-vs-llm"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;You can also play &lt;strong&gt;Human vs AI&lt;/strong&gt; or watch &lt;strong&gt;AI vs AI&lt;/strong&gt; duels. &lt;a href="https://github.com/Laszlobeer/chess-llm-vs-llm"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;It uses a clean PyQt5 interface (board, move highlighting, history, undo, etc.). &lt;a href="https://github.com/Laszlobeer/chess-llm-vs-llm"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;If a model fails to return a move, there‚Äôs a fallback to a random legal move. &lt;a href="https://github.com/Laszlobeer/chess-llm-vs-llm"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üîß How to try it&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;You need &lt;strong&gt;Python 3.7+&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Install &lt;strong&gt;Ollama&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Load at least two chess-capable models in Ollama&lt;/li&gt; &lt;li&gt;&lt;code&gt;pip install PyQt5 chess requests&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Run the &lt;a href="http://chess.py"&gt;&lt;code&gt;chess.py&lt;/code&gt;&lt;/a&gt; script and pick your mode / models &lt;a href="https://github.com/Laszlobeer/chess-llm-vs-llm"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;üí≠ Why this is interesting&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;It gives a &lt;strong&gt;hands-on way&lt;/strong&gt; to compare different LLMs in a structured game environment rather than just text tasks.&lt;/li&gt; &lt;li&gt;You can see &lt;em&gt;where&lt;/em&gt; model strengths/weaknesses emerge in planning, tactics, endgames, etc.&lt;/li&gt; &lt;li&gt;It‚Äôs lightweight and modular ‚Äî you can swap in new models or augment logic.&lt;/li&gt; &lt;li&gt;For folks into AI + games, it's a fun sandbox to experiment with.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o8czoo/ai_chess_showdown_comparing_llm_vs_llm_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o8czoo/ai_chess_showdown_comparing_llm_vs_llm_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o8czoo/ai_chess_showdown_comparing_llm_vs_llm_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T17:46:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8h138</id>
    <title>why no one is speaking about the ollama gui ?</title>
    <updated>2025-10-16T20:17:02+00:00</updated>
    <author>
      <name>/u/Constant-Fondant-178</name>
      <uri>https://old.reddit.com/user/Constant-Fondant-178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o8h138/why_no_one_is_speaking_about_the_ollama_gui/"&gt; &lt;img alt="why no one is speaking about the ollama gui ?" src="https://b.thumbs.redditmedia.com/cww06GGh2FJFtVra9--eZiDTx-YyW7WtbDZ9Bah1G9Y.jpg" title="why no one is speaking about the ollama gui ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ifhy27gy7jvf1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a009fc6b1517203fea4b14311809fa899fce2737"&gt;https://preview.redd.it/ifhy27gy7jvf1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a009fc6b1517203fea4b14311809fa899fce2737&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Constant-Fondant-178"&gt; /u/Constant-Fondant-178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o8h138/why_no_one_is_speaking_about_the_ollama_gui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o8h138/why_no_one_is_speaking_about_the_ollama_gui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o8h138/why_no_one_is_speaking_about_the_ollama_gui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T20:17:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o93mz0</id>
    <title>Ollama Cloud API Tool usage</title>
    <updated>2025-10-17T14:59:29+00:00</updated>
    <author>
      <name>/u/Content-Baby2782</name>
      <uri>https://old.reddit.com/user/Content-Baby2782</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been writing a connector for the Ollama cloud api, i've managed to get it connecting and running prompts but when it comes to toolcalls, the signature it returns is different to the OpenAI standard. Well i actually used OpenRouter first, OpenRouter when the LLm returns a function call it also returns an ID so that when you post the tool reply back to the LLM it can identifiy which tool result is for which tool call.&lt;/p&gt; &lt;p&gt;But Ollama cloud doesnt seem to send this back?&lt;/p&gt; &lt;p&gt;Can Ollama cloud do parallel toolcalls? is that possibly why it doesnt do that?&lt;/p&gt; &lt;p&gt;Also the stop reason is set to &amp;quot;stop&amp;quot; installed of &amp;quot;tool_calls&amp;quot;&lt;/p&gt; &lt;p&gt;Should i just ignore the function id and post it back without that? or am i missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Content-Baby2782"&gt; /u/Content-Baby2782 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o93mz0/ollama_cloud_api_tool_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o93mz0/ollama_cloud_api_tool_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o93mz0/ollama_cloud_api_tool_usage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-17T14:59:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o94mq8</id>
    <title>Anyone else getting this error on v0.12.6?</title>
    <updated>2025-10-17T15:36:11+00:00</updated>
    <author>
      <name>/u/veryhasselglad</name>
      <uri>https://old.reddit.com/user/veryhasselglad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o94mq8/anyone_else_getting_this_error_on_v0126/"&gt; &lt;img alt="Anyone else getting this error on v0.12.6?" src="https://preview.redd.it/5o6s2dhuxovf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05c4600cd19edf5f65e3369f3e6a4e9f1853e10c" title="Anyone else getting this error on v0.12.6?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just updated to v0.12.6 and I'm running into this error:&lt;/p&gt; &lt;p&gt;&amp;quot;500 Internal Server Error: load unmarshal encode response: json: cannot unmarshal number into Go struct field BackendMemory.Memory.InputWeights of type ml.Memory&amp;quot;&lt;/p&gt; &lt;p&gt;Is this happening to anyone else or just me?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/veryhasselglad"&gt; /u/veryhasselglad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5o6s2dhuxovf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o94mq8/anyone_else_getting_this_error_on_v0126/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o94mq8/anyone_else_getting_this_error_on_v0126/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-17T15:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o95ytc</id>
    <title>ADAM Project. Beta testing and feedback.</title>
    <updated>2025-10-17T16:26:42+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have created a chat bot for people who are interested to know about project management. Or people who are involved in managing projects. The chatbot will try answer your queries to best of its knowledge allowed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;ADAM = Agile Digital Assistance for Managers.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;You can try ADAMProject&lt;/strong&gt; &lt;a href="https://adamprojectv1.streamlit.app"&gt;&lt;strong&gt;here.&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Instructions&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;It is using Ollama cloud. So you need to key in your API key.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&lt;/p&gt; &lt;p&gt;Once you have test it out, &lt;strong&gt;please fill in the feedback form&lt;/strong&gt; &lt;a href="https://forms.gle/DfzHhzbYsMCpATzdA"&gt;&lt;strong&gt;here.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I like to here from you.&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;p&gt;#AI #OllamaCloud #ProjectManagementAI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o95ytc/adam_project_beta_testing_and_feedback/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o95ytc/adam_project_beta_testing_and_feedback/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o95ytc/adam_project_beta_testing_and_feedback/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-17T16:26:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9tohq</id>
    <title>Train Your Own AI Model with Ollama | Full Step-by-Step Tutorial</title>
    <updated>2025-10-18T11:49:22+00:00</updated>
    <author>
      <name>/u/amplifyabhi</name>
      <uri>https://old.reddit.com/user/amplifyabhi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o9tohq/train_your_own_ai_model_with_ollama_full/"&gt; &lt;img alt="Train Your Own AI Model with Ollama | Full Step-by-Step Tutorial" src="https://external-preview.redd.it/iuwZrUlAtOFQSZoGf94UeyiEgDCBL6YojK05bHIHfg0.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fbad4649faa4747963f09602eb98c3f65a38509" title="Train Your Own AI Model with Ollama | Full Step-by-Step Tutorial" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting with Ollama for a while now, and I‚Äôve finally created a clean, beginner-friendly tutorial on how anyone can train and run their &lt;strong&gt;own AI models locally&lt;/strong&gt; ‚Äî no cloud required üíª‚ö°&lt;/p&gt; &lt;p&gt;üëâ What I‚Äôve covered in the tutorial:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üß∞ Setting up Ollama step by step&lt;/li&gt; &lt;li&gt;üß† Running and customizing your own model&lt;/li&gt; &lt;li&gt;üß™ Training a simple AI locally&lt;/li&gt; &lt;li&gt;üõ°Ô∏è 100% private and offline&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amplifyabhi"&gt; /u/amplifyabhi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/7gXB0wnCvLY"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o9tohq/train_your_own_ai_model_with_ollama_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o9tohq/train_your_own_ai_model_with_ollama_full/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-18T11:49:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9byfp</id>
    <title>Ollama on Linux with swap enabled.</title>
    <updated>2025-10-17T20:16:22+00:00</updated>
    <author>
      <name>/u/GhostInThePudding</name>
      <uri>https://old.reddit.com/user/GhostInThePudding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just in case anyone else is having trouble with their device hard locking when frequently switching models in Ollama on Linux with an Nvidia GPU.&lt;/p&gt; &lt;p&gt;After giving up on trying to solve it and accepting it was either an obscure driver issue on my device, or maybe even a hardware fault, I happened to use Ollama after disabling my swap space, and suddenly it worked perfectly.&lt;/p&gt; &lt;p&gt;It seems there is some issue with memory management when swap is enabled, that if you switch models a lot, it can not only crash Ollama, but the entire system, forcing a hard reboot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhostInThePudding"&gt; /u/GhostInThePudding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o9byfp/ollama_on_linux_with_swap_enabled/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o9byfp/ollama_on_linux_with_swap_enabled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o9byfp/ollama_on_linux_with_swap_enabled/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-17T20:16:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9w6zv</id>
    <title>Opencode + Ollama Doesn't Work With Local LLMs on Windows 11</title>
    <updated>2025-10-18T13:43:44+00:00</updated>
    <author>
      <name>/u/SlideRuleFan</name>
      <uri>https://old.reddit.com/user/SlideRuleFan</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlideRuleFan"&gt; /u/SlideRuleFan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/opencodeCLI/comments/1o9emcc/opencode_ollama_doesnt_work_with_local_llms_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o9w6zv/opencode_ollama_doesnt_work_with_local_llms_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o9w6zv/opencode_ollama_doesnt_work_with_local_llms_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-18T13:43:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9l4vg</id>
    <title>Is Ollama slower on Windows, compared with Linux, when starting a model? (cold start from disk, the model files are not in the cache yet)</title>
    <updated>2025-10-18T03:16:26+00:00</updated>
    <author>
      <name>/u/florinandrei</name>
      <uri>https://old.reddit.com/user/florinandrei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same machine, dual boot, Windows 11 and Ubuntu 24.04&lt;/p&gt; &lt;p&gt;The system is reasonably fast, I can play recent games, fine-tune LLMs, write and run PyTorch code, etc. Each OS is on its own SSD drive, but the drives are nearly identical.&lt;/p&gt; &lt;p&gt;Starting a model from a cold start is fairly quick on Linux.&lt;/p&gt; &lt;p&gt;On Windows, I have to wait something like 30 seconds until gemma3:27b is loaded and I can start prompting it. The wait might be a bit even longer if I use Open WebUI as an interface to Ollama.&lt;/p&gt; &lt;p&gt;After stopping the model, and running it again, now the model files are cached, and the start process is as fast as on Linux.&lt;/p&gt; &lt;p&gt;Has anybody else seen this issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/florinandrei"&gt; /u/florinandrei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o9l4vg/is_ollama_slower_on_windows_compared_with_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o9l4vg/is_ollama_slower_on_windows_compared_with_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o9l4vg/is_ollama_slower_on_windows_compared_with_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-18T03:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa2psp</id>
    <title>Claude Haiku 4.5 for Computer Use</title>
    <updated>2025-10-18T18:05:14+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oa2psp/claude_haiku_45_for_computer_use/"&gt; &lt;img alt="Claude Haiku 4.5 for Computer Use" src="https://external-preview.redd.it/NGgxeHA0aTl1d3ZmMbN-OTJwTQPs45moaBiyBOFffvNmrIYfiiCYht-O6e2Z.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5532755cf7668522edea9a2c78e7434a196ae6d2" title="Claude Haiku 4.5 for Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude Haiku 4.5 on a computer-use task and it's faster + 3.5x cheaper than Sonnet 4.5:&lt;/p&gt; &lt;p&gt;Create a landing page of Cua and open it in browser&lt;/p&gt; &lt;p&gt;Haiku 4.5: 2 minutes, $0.04&lt;/p&gt; &lt;p&gt;Sonnet 4.5: 3 minutes, ~$0.14&lt;/p&gt; &lt;p&gt;Haiku shown here.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uaw73rq9uwvf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oa2psp/claude_haiku_45_for_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oa2psp/claude_haiku_45_for_computer_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-18T18:05:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oadgp4</id>
    <title>Hi, I hope this is not a dumb question, I have hard time getting thinking models (open ai open model, qwen) to send back a JSON and only a json. It keeps sending back the thinking tokens which messes up the parsing. I tried many suggestions from ChatGPT or claude to no avail. Thank you!</title>
    <updated>2025-10-19T01:44:28+00:00</updated>
    <author>
      <name>/u/Defiant_Watch9818</name>
      <uri>https://old.reddit.com/user/Defiant_Watch9818</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Defiant_Watch9818"&gt; /u/Defiant_Watch9818 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oadgp4/hi_i_hope_this_is_not_a_dumb_question_i_have_hard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oadgp4/hi_i_hope_this_is_not_a_dumb_question_i_have_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oadgp4/hi_i_hope_this_is_not_a_dumb_question_i_have_hard/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T01:44:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oadii0</id>
    <title>Ollama newbie seeking advice/tips</title>
    <updated>2025-10-19T01:47:07+00:00</updated>
    <author>
      <name>/u/CryptoNiight</name>
      <uri>https://old.reddit.com/user/CryptoNiight</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just ordered a mini pc for ollama. The specs are: Intel Core i5 with integrated graphics + 32 GB of memory. Do I absolutely need a dedicated graphics card to get started? Will it be too slow without one? Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CryptoNiight"&gt; /u/CryptoNiight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oadii0/ollama_newbie_seeking_advicetips/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oadii0/ollama_newbie_seeking_advicetips/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oadii0/ollama_newbie_seeking_advicetips/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T01:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1oani8z</id>
    <title>Download keeps resetting</title>
    <updated>2025-10-19T11:41:39+00:00</updated>
    <author>
      <name>/u/karrie0027</name>
      <uri>https://old.reddit.com/user/karrie0027</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to download other models in ollama I am in macbook m1 air Downloading gemma3:4b model and whenever my download reaches to like 90% it goes back to like 84%, currently stuck at 2.8gb/3.1gb , even though i have fast internet around 200mbps&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/karrie0027"&gt; /u/karrie0027 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oani8z/download_keeps_resetting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oani8z/download_keeps_resetting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oani8z/download_keeps_resetting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T11:41:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oak98t</id>
    <title>Model for organizing photos</title>
    <updated>2025-10-19T08:18:50+00:00</updated>
    <author>
      <name>/u/gregusmeus</name>
      <uri>https://old.reddit.com/user/gregusmeus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I‚Äôm seeking a recommendation please, I‚Äôd like to use a local model to organize my folder of photos - is there a model I can download via ollama that folks would recommend for this task‚Ä¶with no risk of my photos ending up in the wild?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gregusmeus"&gt; /u/gregusmeus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oak98t/model_for_organizing_photos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oak98t/model_for_organizing_photos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oak98t/model_for_organizing_photos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T08:18:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1oahq2c</id>
    <title>When you have little money but want to run big models</title>
    <updated>2025-10-19T05:39:04+00:00</updated>
    <author>
      <name>/u/alok_saurabh</name>
      <uri>https://old.reddit.com/user/alok_saurabh</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alok_saurabh"&gt; /u/alok_saurabh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oahpmx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oahq2c/when_you_have_little_money_but_want_to_run_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oahq2c/when_you_have_little_money_but_want_to_run_big/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T05:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oarkmc</id>
    <title>Ollama Conversation History</title>
    <updated>2025-10-19T14:50:56+00:00</updated>
    <author>
      <name>/u/PalSCentered</name>
      <uri>https://old.reddit.com/user/PalSCentered</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oarkmc/ollama_conversation_history/"&gt; &lt;img alt="Ollama Conversation History" src="https://b.thumbs.redditmedia.com/8w6kAPmVIFxE_hEdgrXeDkszDvIMQhezd1H9mrncxhQ.jpg" title="Ollama Conversation History" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where does ollama app chat history get saved. I'm trying to find it and can't find the exact location. &lt;/p&gt; &lt;p&gt;I tried to look in the Ollama folder and originally thought it was the history file but no this is only for when using terminal so that begs the question where is this history when you use the app.&lt;/p&gt; &lt;p&gt;I mean this is supposed to be local right so it has to be somewhere in my computer.&lt;/p&gt; &lt;p&gt;If you have the answer to this I would love to know. Thanks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/74ergpxby2wf1.png?width=1676&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9f055581c791a5154900f96c6bd0c01bd02395e"&gt;https://preview.redd.it/74ergpxby2wf1.png?width=1676&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9f055581c791a5154900f96c6bd0c01bd02395e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PalSCentered"&gt; /u/PalSCentered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oarkmc/ollama_conversation_history/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oarkmc/ollama_conversation_history/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oarkmc/ollama_conversation_history/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T14:50:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oanxsn</id>
    <title>I built Graphite: A visual, non-linear LLM interface that turns your local chats into a map of ideas (Python/Ollama)</title>
    <updated>2025-10-19T12:05:16+00:00</updated>
    <author>
      <name>/u/Ok-Function-7101</name>
      <uri>https://old.reddit.com/user/Ok-Function-7101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oanxsn/i_built_graphite_a_visual_nonlinear_llm_interface/"&gt; &lt;img alt="I built Graphite: A visual, non-linear LLM interface that turns your local chats into a map of ideas (Python/Ollama)" src="https://b.thumbs.redditmedia.com/h0XcLRwXsEHG2NPsYuol7pUeT-xOwVJz9V91tvTZerg.jpg" title="I built Graphite: A visual, non-linear LLM interface that turns your local chats into a map of ideas (Python/Ollama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Check out the live view:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jupyfcdn62wf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=33c2f3dacb8ba6a9d7970ba8ec5e0e186931b2a2"&gt;easily convert text to graphic charts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v30rybdn62wf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a29976c4743918b51203b361fa5859d4fa540e8"&gt;multiple thread directions from a single point on the graph&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been working on a side project called &lt;strong&gt;Graphite&lt;/strong&gt; for nearly a year, because I found standard LLM chat interfaces too restrictive. When you're trying to brainstorm, research, or trace complex logic, the linear scroll format is a massive blocker‚Äîideas get buried, and it‚Äôs impossible to track branches of thought.&lt;/p&gt; &lt;p&gt;Graphite solves this by treating every chat as a dynamic, visual graph on an infinite canvas.&lt;/p&gt; &lt;h1&gt;What it is&lt;/h1&gt; &lt;p&gt;Graphite is a desktop application built with Python (PyQt5) that integrates with your local LLMs via &lt;strong&gt;Ollama&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Non-Linear Conversations:&lt;/strong&gt; Every prompt and response is a movable, selectable node. If you want to revisit a question from 20 steps ago, you click that node, and your new query creates a branching path, allowing you to explore tangents without losing the original context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Workspace:&lt;/strong&gt; It's designed to be a workspace, not just a chat log. You can organize nodes into &lt;strong&gt;Frames&lt;/strong&gt;, add &lt;strong&gt;Notes&lt;/strong&gt; for external annotations, and drop &lt;strong&gt;Navigation Pins&lt;/strong&gt; to bookmark key moments.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Privacy:&lt;/strong&gt; Because it uses Ollama, all conversations and data processing stay local to your machine.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features I‚Äôm Excited About&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Chart Generation:&lt;/strong&gt; You can right-click any node containing structured data and ask the AI to generate a &lt;strong&gt;bar chart, pie chart, or even a Sankey diagram&lt;/strong&gt; directly on your canvas using Matplotlib.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Takeaways &amp;amp; Explainers:&lt;/strong&gt; The context menu lets you instantly generate key summaries or simplified &amp;quot;explain it like I'm five&amp;quot; notes from a complex AI response.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comprehensive Persistence:&lt;/strong&gt; It saves the entire workspace (nodes, connections, frames, notes, and pins) to a local SQLite database, managed via a &amp;quot;Chat Library&amp;quot; for session management.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'm currently using the qwen2.5:7b model, but it's designed to be model-agnostic as long as it runs on Ollama.&lt;/p&gt; &lt;p&gt;I'm looking for feedback from the community, especially around the usability of the non-linear graph metaphor and any potential features you'd find useful for this kind of visual AI interaction.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo Link:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fdovvnloading%2FGraphite"&gt;https://github.com/dovvnloading/Graphite&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for taking a look!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Function-7101"&gt; /u/Ok-Function-7101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oanxsn/i_built_graphite_a_visual_nonlinear_llm_interface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oanxsn/i_built_graphite_a_visual_nonlinear_llm_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oanxsn/i_built_graphite_a_visual_nonlinear_llm_interface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T12:05:16+00:00</published>
  </entry>
</feed>
