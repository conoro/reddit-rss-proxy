<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-10T09:06:56+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1nb3uxi</id>
    <title>MSSQL Server Query Generator</title>
    <updated>2025-09-07T20:26:18+00:00</updated>
    <author>
      <name>/u/Private_Tank</name>
      <uri>https://old.reddit.com/user/Private_Tank</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys,&lt;/p&gt; &lt;p&gt;Im currently using n8n to build a Workflow that generates and executes sql queries. Im currently using the Mistral model and its not getting where I want.&lt;/p&gt; &lt;p&gt;If Im asking â€žHow much did John spend in 2022?â€œ, I sometimes get an SQL Query without the customer name in the where condition. Sometimes it uses the Customer name but checks the wrong column. It even looked up invoices from 2021 even tho I clearly asked for invoices from 2022.&lt;/p&gt; &lt;p&gt;In the prompt I have: Schema Information for my views A quick description for every single column Documentation on how to join the views Question-Query Pairs as a guiding example More instruction like â€žalways use Select *â€¦â€œ&lt;/p&gt; &lt;p&gt;What can I do to make it reliable? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private_Tank"&gt; /u/Private_Tank &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb3uxi/mssql_server_query_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb3uxi/mssql_server_query_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb3uxi/mssql_server_query_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T20:26:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb8wc8</id>
    <title>Anyone have any experience with flash card creation from notes, which model to pick? Have you made a good system prompt.</title>
    <updated>2025-09-07T23:59:36+00:00</updated>
    <author>
      <name>/u/Leather-Equipment256</name>
      <uri>https://old.reddit.com/user/Leather-Equipment256</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leather-Equipment256"&gt; /u/Leather-Equipment256 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb8wc8/anyone_have_any_experience_with_flash_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb8wc8/anyone_have_any_experience_with_flash_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb8wc8/anyone_have_any_experience_with_flash_card/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T23:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb8k4a</id>
    <title>ðŸš€ aX Epic Demo: Agents Interviewing Each Other</title>
    <updated>2025-09-07T23:44:01+00:00</updated>
    <author>
      <name>/u/madtank10</name>
      <uri>https://old.reddit.com/user/madtank10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nb8k4a/ax_epic_demo_agents_interviewing_each_other/"&gt; &lt;img alt="ðŸš€ aX Epic Demo: Agents Interviewing Each Other" src="https://external-preview.redd.it/AoCHcpKVgJqJY1bTlln7l0J1QATRui8ipsLuVGfyFjs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dea3b9c4c68cc7825f036b10a639be4b45938bd2" title="ðŸš€ aX Epic Demo: Agents Interviewing Each Other" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madtank10"&gt; /u/madtank10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sml16nfhynnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb8k4a/ax_epic_demo_agents_interviewing_each_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb8k4a/ax_epic_demo_agents_interviewing_each_other/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T23:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbbdya</id>
    <title>embeddinggemma have higher memory footprint than qwen3:0.6b</title>
    <updated>2025-09-08T01:59:07+00:00</updated>
    <author>
      <name>/u/Common_Network</name>
      <uri>https://old.reddit.com/user/Common_Network</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nbbdya/embeddinggemma_have_higher_memory_footprint_than/"&gt; &lt;img alt="embeddinggemma have higher memory footprint than qwen3:0.6b" src="https://preview.redd.it/u7bktr9elunf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb4467add1ba497ae97b6eafd27634895d8e4bf7" title="embeddinggemma have higher memory footprint than qwen3:0.6b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;does anyone have any idea why this is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Common_Network"&gt; /u/Common_Network &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u7bktr9elunf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbbdya/embeddinggemma_have_higher_memory_footprint_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbbdya/embeddinggemma_have_higher_memory_footprint_than/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T01:59:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb1xuc</id>
    <title>Best local coding tools or ides with ollama support?</title>
    <updated>2025-09-07T19:11:18+00:00</updated>
    <author>
      <name>/u/No_Strawberry_8719</name>
      <uri>https://old.reddit.com/user/No_Strawberry_8719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im looking for a simple lightweight local tool for coding or programming that actually support ollama and are not a nightmare to setup. What would you suggest and what model pairs well with said tool?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Strawberry_8719"&gt; /u/No_Strawberry_8719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb1xuc/best_local_coding_tools_or_ides_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb1xuc/best_local_coding_tools_or_ides_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb1xuc/best_local_coding_tools_or_ides_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T19:11:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbuk0a</id>
    <title>Ollama seems to be computing on CPU rather than GPU</title>
    <updated>2025-09-08T17:46:12+00:00</updated>
    <author>
      <name>/u/Busy-Examination1924</name>
      <uri>https://old.reddit.com/user/Busy-Examination1924</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! for some reason all my models, even smaller models, seem to be running very slowly and for some reason seems to be doing the computing from the CPU instead of GPU. While the VRam seems to loading, the GPUs utilization hovers around 0-5% and the CPU spikes to 80-100%. ANy ideas what could be the problem? I have an RTX 4070, 11700k CPU and 64GB ram. In the example below I am running mistral-nemo Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Busy-Examination1924"&gt; /u/Busy-Examination1924 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbuk0a/ollama_seems_to_be_computing_on_cpu_rather_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbuk0a/ollama_seems_to_be_computing_on_cpu_rather_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbuk0a/ollama_seems_to_be_computing_on_cpu_rather_than/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T17:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nax9sq</id>
    <title>This Setting dramatically increases all Ollama Model speeds!</title>
    <updated>2025-09-07T16:11:49+00:00</updated>
    <author>
      <name>/u/NenntronReddit</name>
      <uri>https://old.reddit.com/user/NenntronReddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was getting terrible speeds within my python queries and couldn't figure out why.&lt;/p&gt; &lt;p&gt;Turns out, Ollama uses the global context setting from the Ollama GUI for every request, even short ones. I thought that was for the GUI only, but it effects python and all other ollama queries too. Setting it from 128k down to 4k gave me a &lt;strong&gt;435% speed boost.&lt;/strong&gt; So in case you didn't know that already, try it out.&lt;/p&gt; &lt;p&gt;Open up Ollama Settings.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4nqx3ev5lrnf1.png?width=206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84c8b0d304bb23b47b671e90ed9390bad22c1e41"&gt;https://preview.redd.it/4nqx3ev5lrnf1.png?width=206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84c8b0d304bb23b47b671e90ed9390bad22c1e41&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Reduce the Context length in here. If you use the model to analyse long context windows, obviously keep it higher, but since I only have context lengths of around 2-3k tokens, I never need 128k which I had it on before.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y0ps6j6flrnf1.png?width=661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e569dcb679ee5ea85d5a28b0be3f93fe9caad99"&gt;https://preview.redd.it/y0ps6j6flrnf1.png?width=661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e569dcb679ee5ea85d5a28b0be3f93fe9caad99&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, the Speed dramatically increased to this:&lt;/p&gt; &lt;p&gt;Before:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/40ewfc9skrnf1.png?width=349&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32ead0c0672d8318583ef46afdc8add0323474e8"&gt;https://preview.redd.it/40ewfc9skrnf1.png?width=349&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32ead0c0672d8318583ef46afdc8add0323474e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s36tfzp5ornf1.png?width=355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56fcdcf9dcb3f466d587f812a54d5882907ec1e5"&gt;https://preview.redd.it/s36tfzp5ornf1.png?width=355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56fcdcf9dcb3f466d587f812a54d5882907ec1e5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NenntronReddit"&gt; /u/NenntronReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nax9sq/this_setting_dramatically_increases_all_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nax9sq/this_setting_dramatically_increases_all_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nax9sq/this_setting_dramatically_increases_all_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T16:11:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc29ed</id>
    <title>Looking for Chatgpt and Perplexity Replacement</title>
    <updated>2025-09-08T22:42:32+00:00</updated>
    <author>
      <name>/u/KingTSS</name>
      <uri>https://old.reddit.com/user/KingTSS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am an engineer, but not a C.Sc. or C.E. engineer. I use AI models regularly for work, for business documents, materials, and legal documents generation for work. Most of my personal time, I use them for research purposes and resume generation as I am looking for a new job.&lt;/p&gt; &lt;p&gt;Currently, I am a paid user of ChatGPT &amp;amp; Perplexity, and I am not liking it so far due to a lack of Privacy, and my hatred towards subscription-based business models. &lt;/p&gt; &lt;p&gt;Please share your suggestions for suitable models in Ollama for the following spec: 2TB SSD, 12th Gen Intel Core i9-12900H vPro Processor, 64GB RAM, 16GB Graphics Card. (NVIDIA RTX A5500 16GB GDDR6 Graphics, 64GB DDR5 Memory)&lt;/p&gt; &lt;p&gt;Thanks in advance for your suggestions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KingTSS"&gt; /u/KingTSS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nc29ed/looking_for_chatgpt_and_perplexity_replacement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nc29ed/looking_for_chatgpt_and_perplexity_replacement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nc29ed/looking_for_chatgpt_and_perplexity_replacement/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T22:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbsfgi</id>
    <title>Graph RAG pipeline thatâ€™s runs entirely locally with ollama and has full source attribution</title>
    <updated>2025-09-08T16:28:39+00:00</updated>
    <author>
      <name>/u/BitterHouse8234</name>
      <uri>https://old.reddit.com/user/BitterHouse8234</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt; I built a Graph RAG pipeline (VeritasGraph) that runs entirely locally with Ollama (Llama 3.1) and has full source attribution. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hey r/,&lt;/p&gt; &lt;p&gt;I've been deep in the world of local RAG and wanted to share a project I built, VeritasGraph, that's designed from the ground up for private, on-premise use with tools we all love.&lt;/p&gt; &lt;p&gt;My setup uses Ollama with llama3.1 for generation and nomic-embed-text for embeddings. The whole thing runs on my machine without hitting any external APIs.&lt;/p&gt; &lt;p&gt;The main goal was to solve two big problems:&lt;/p&gt; &lt;p&gt;Multi-Hop Reasoning: Standard vector RAG fails when you need to connect facts from different documents. VeritasGraph builds a knowledge graph to traverse these relationships.&lt;/p&gt; &lt;p&gt;Trust &amp;amp; Verification: It provides full source attribution for every generated statement, so you can see exactly which part of your source documents was used to construct the answer.&lt;/p&gt; &lt;p&gt;One of the key challenges I ran into (and solved) was the default context length in Ollama. I found that the default of 2048 was truncating the context and leading to bad results. The repo includes a Modelfile to build a version of llama3.1 with a 12k context window, which fixed the issue completely.&lt;/p&gt; &lt;p&gt;The project includes:&lt;/p&gt; &lt;p&gt;The full Graph RAG pipeline.&lt;/p&gt; &lt;p&gt;A Gradio UI for an interactive chat experience.&lt;/p&gt; &lt;p&gt;A guide for setting everything up, from installing dependencies to running the indexing process.&lt;/p&gt; &lt;p&gt;GitHub Repo with all the code and instructions: &lt;a href="https://github.com/bibinprathap/VeritasGraph"&gt;https://github.com/bibinprathap/VeritasGraph&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd be really interested to hear your thoughts, especially on the local LLM implementation and prompt tuning. I'm sure there are ways to optimize it further.&lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BitterHouse8234"&gt; /u/BitterHouse8234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbsfgi/graph_rag_pipeline_thats_runs_entirely_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbsfgi/graph_rag_pipeline_thats_runs_entirely_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbsfgi/graph_rag_pipeline_thats_runs_entirely_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T16:28:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncjaa9</id>
    <title>How can I download models onto USB and install from filesystem?.</title>
    <updated>2025-09-09T13:42:53+00:00</updated>
    <author>
      <name>/u/abrandis</name>
      <uri>https://old.reddit.com/user/abrandis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I need to download a bunch of models to sue at my cabin that's in a remote location no network .. I have a laptop up there and would like to take a bunch of models in a USB drive the models , but how do I install models from filesystem in ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abrandis"&gt; /u/abrandis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncjaa9/how_can_i_download_models_onto_usb_and_install/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncjaa9/how_can_i_download_models_onto_usb_and_install/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ncjaa9/how_can_i_download_models_onto_usb_and_install/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T13:42:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckbbf</id>
    <title>Suddenly can't run bigger models anymore?!</title>
    <updated>2025-09-09T14:23:32+00:00</updated>
    <author>
      <name>/u/MatthKarl</name>
      <uri>https://old.reddit.com/user/MatthKarl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a GMKtek Evo-X2 Plus with 128GB RAM that is shared with the GPU. The machine runs Ubuntu 24.04 and Ollama is running in a docker container.&lt;/p&gt; &lt;p&gt;That setup worked just fine for about 2 weeks and I had qwen3:235b and even deepseek-r1:671b installed and it was running. qwen3:235b was running at around 5 t/s and over all not too bad. The 671b was obviously very slow, but still, it was running.&lt;/p&gt; &lt;p&gt;Now suddenly whenever I try to use the 235b it immediately errors out saying the model needs more RAM than available. But why did it run before?&lt;/p&gt; &lt;p&gt;I'm not aware that I have made any changes to the system, and I really would love to understand why it suddenly isn't working anymore. Any hints how I could that get running again?&lt;/p&gt; &lt;p&gt;EDIT: More detailed information:&lt;/p&gt; &lt;p&gt;Hardware: GMKtec NucBox EVO-X2&lt;br /&gt; Processor: AMD RYZEN AI MAX+ 395 w/Radeon 8060S x 32&lt;br /&gt; RAM: 128GB&lt;br /&gt; OS: Ubuntu 24.04.3 LTS&lt;/p&gt; &lt;p&gt;Log from the Ollama container:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.803Z level=INFO source=amd_linux.go:487 msg=&amp;quot;amdgpu is supported&amp;quot; gpu=0 gpu_type=gfx1151&lt;/p&gt; &lt;p&gt;llama_model_loader: loaded meta data with 33 key-value pairs and 1131 tensors from /root/.ollama/models/blobs/sha256-791d5d11998e006548d6b58c31756562ea61446ebc7d19686608402a797ecc82 (version GGUF V3 (latest))&lt;/p&gt; &lt;p&gt;llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 0: general.architecture str = qwen3moe&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 1: general.basename str = Qwen3&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 2: general.file_type u32 = 15&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 3: general.finetune str = Thinking&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 4: &lt;a href="http://general.name"&gt;general.name&lt;/a&gt; str = Qwen3 235B A22B Thinking 2507&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 5: general.parameter_count u64 = 235093634560&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 6: general.quantization_version u32 = 2&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 7: general.size_label str = 235B-A22B&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 8: general.type str = model&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 9: general.version str = 2507&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 10: qwen3moe.attention.head_count u32 = 64&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 11: qwen3moe.attention.head_count_kv u32 = 4&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 12: qwen3moe.attention.key_length u32 = 128&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 13: qwen3moe.attention.layer_norm_rms_epsilon f32 = 0.000001&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 14: qwen3moe.attention.value_length u32 = 128&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 15: qwen3moe.block_count u32 = 94&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 16: qwen3moe.context_length u32 = 262144&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 17: qwen3moe.embedding_length u32 = 4096&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 18: qwen3moe.expert_count u32 = 128&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 19: qwen3moe.expert_feed_forward_length u32 = 1536&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 20: qwen3moe.expert_used_count u32 = 8&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 21: qwen3moe.feed_forward_length u32 = 12288&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 22: qwen3moe.rope.freq_base f32 = 5000000.000000&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 23: tokenizer.chat_template str = {%- if tools %}\n {{- '&amp;lt;|im_start|&amp;gt;...&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 24: tokenizer.ggml.add_bos_token bool = false&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 25: tokenizer.ggml.bos_token_id u32 = 151643&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 26: tokenizer.ggml.eos_token_id u32 = 151645&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 27: tokenizer.ggml.merges arr[str,151387] = [&amp;quot;Ä  Ä &amp;quot;, &amp;quot;Ä Ä  Ä Ä &amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;Ä  t&amp;quot;,...&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 28: tokenizer.ggml.model str = gpt2&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 29: tokenizer.ggml.padding_token_id u32 = 151643&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 30: tokenizer.ggml.pre str = qwen2&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 31: tokenizer.ggml.token_type arr[i32,151936] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 32: tokenizer.ggml.tokens arr[str,151936] = [&amp;quot;!&amp;quot;, &amp;quot;\&amp;quot;&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;$&amp;quot;, &amp;quot;%&amp;quot;, &amp;quot;&amp;amp;&amp;quot;, &amp;quot;'&amp;quot;, ...&lt;/p&gt; &lt;p&gt;llama_model_loader: - type f32: 471 tensors&lt;/p&gt; &lt;p&gt;llama_model_loader: - type q4_K: 567 tensors&lt;/p&gt; &lt;p&gt;llama_model_loader: - type q6_K: 93 tensors&lt;/p&gt; &lt;p&gt;print_info: file format = GGUF V3 (latest)&lt;/p&gt; &lt;p&gt;print_info: file type = Q4_K - Medium&lt;/p&gt; &lt;p&gt;print_info: file size = 132.39 GiB (4.84 BPW)&lt;/p&gt; &lt;p&gt;load: printing all EOG tokens:&lt;/p&gt; &lt;p&gt;load: - 151643 ('&amp;lt;|endoftext|&amp;gt;')&lt;/p&gt; &lt;p&gt;load: - 151645 ('&amp;lt;|im_end|&amp;gt;')&lt;/p&gt; &lt;p&gt;load: - 151662 ('&amp;lt;|fim_pad|&amp;gt;')&lt;/p&gt; &lt;p&gt;load: - 151663 ('&amp;lt;|repo_name|&amp;gt;')&lt;/p&gt; &lt;p&gt;load: - 151664 ('&amp;lt;|file_sep|&amp;gt;')&lt;/p&gt; &lt;p&gt;load: special tokens cache size = 26&lt;/p&gt; &lt;p&gt;load: token to piece cache size = 0.9311 MB&lt;/p&gt; &lt;p&gt;print_info: arch = qwen3moe&lt;/p&gt; &lt;p&gt;print_info: vocab_only = 1&lt;/p&gt; &lt;p&gt;print_info: model type = ?B&lt;/p&gt; &lt;p&gt;print_info: model params = 235.09 B&lt;/p&gt; &lt;p&gt;print_info: &lt;a href="http://general.name"&gt;general.name&lt;/a&gt;= Qwen3 235B A22B Thinking 2507&lt;/p&gt; &lt;p&gt;print_info: n_ff_exp = 0&lt;/p&gt; &lt;p&gt;print_info: vocab type = BPE&lt;/p&gt; &lt;p&gt;print_info: n_vocab = 151936&lt;/p&gt; &lt;p&gt;print_info: n_merges = 151387&lt;/p&gt; &lt;p&gt;print_info: BOS token = 151643 '&amp;lt;|endoftext|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOS token = 151645 '&amp;lt;|im_end|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOT token = 151645 '&amp;lt;|im_end|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: PAD token = 151643 '&amp;lt;|endoftext|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: LF token = 198 'ÄŠ'&lt;/p&gt; &lt;p&gt;print_info: FIM PRE token = 151659 '&amp;lt;|fim_prefix|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: FIM SUF token = 151661 '&amp;lt;|fim_suffix|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: FIM MID token = 151660 '&amp;lt;|fim_middle|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: FIM PAD token = 151662 '&amp;lt;|fim_pad|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: FIM REP token = 151663 '&amp;lt;|repo_name|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: FIM SEP token = 151664 '&amp;lt;|file_sep|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOG token = 151643 '&amp;lt;|endoftext|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOG token = 151645 '&amp;lt;|im_end|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOG token = 151662 '&amp;lt;|fim_pad|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOG token = 151663 '&amp;lt;|repo_name|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOG token = 151664 '&amp;lt;|file_sep|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: max token length = 256&lt;/p&gt; &lt;p&gt;llama_model_load: vocab only - skipping tensors&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.943Z level=INFO source=amd_linux.go:487 msg=&amp;quot;amdgpu is supported&amp;quot; gpu=0 gpu_type=gfx1151&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.944Z level=INFO source=server.go:398 msg=&amp;quot;starting runner&amp;quot; cmd=&amp;quot;/usr/bin/ollama runner --model /root/.ollama/models/blobs/sha256-791d5d11998e006548d6b58c31756562ea61446ebc7d19686608402a797ecc82 --port 41329&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.945Z level=INFO source=amd_linux.go:487 msg=&amp;quot;amdgpu is supported&amp;quot; gpu=0 gpu_type=gfx1151&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.945Z level=INFO source=server.go:503 msg=&amp;quot;system memory&amp;quot; total=&amp;quot;17179869180.9 GiB&amp;quot; free=&amp;quot;17179869176.2 GiB&amp;quot; free_swap=&amp;quot;8.0 GiB&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.946Z level=WARN source=server.go:538 msg=&amp;quot;model request too large for system&amp;quot; requested=&amp;quot;9.5 GiB&amp;quot; available=&amp;quot;162.0 MiB&amp;quot; total=&amp;quot;17179869180.9 GiB&amp;quot; free=&amp;quot;17179869176.2 GiB&amp;quot; swap=&amp;quot;8.0 GiB&amp;quot;&lt;/p&gt; &lt;p&gt;5-09-09T14:35:23.946Z level=INFO source=sched.go:441 msg=&amp;quot;Load failed&amp;quot; model=/root/.ollama/models/blobs/sha256-791d5d11998e006548d6b58c31756562ea61446ebc7d19686608402a797ecc82 error=&amp;quot;model requires more system memory (9.5 GiB) than is available (162.0 MiB)&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.950Z level=INFO source=runner.go:864 msg=&amp;quot;starting go runner&amp;quot;&lt;/p&gt; &lt;p&gt;load_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-icelake.so&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.953Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.954Z level=INFO source=runner.go:900 msg=&amp;quot;Server listening on 127.0.0.1:41329&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2025/09/09 - 14:35:23 | 500 | 201.456301ms | &lt;a href="http://172.17.0.1"&gt;172.17.0.1&lt;/a&gt; | POST &amp;quot;/api/chat&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2025/09/09 - 14:35:29 | 200 | 744.615Âµs | &lt;a href="http://172.17.0.1"&gt;172.17.0.1&lt;/a&gt; | GET &amp;quot;/api/tags&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2025/09/09 - 14:35:29 | 200 | 304.82Âµs | &lt;a href="http://172.17.0.1"&gt;172.17.0.1&lt;/a&gt; | GET &amp;quot;/api/ps&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2025/09/09 - 14:35:29 | 200 | 51.799Âµs | &lt;a href="http://172.17.0.1"&gt;172.17.0.1&lt;/a&gt; | GET &amp;quot;/api/version&amp;quot;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MatthKarl"&gt; /u/MatthKarl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nckbbf/suddenly_cant_run_bigger_models_anymore/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nckbbf/suddenly_cant_run_bigger_models_anymore/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nckbbf/suddenly_cant_run_bigger_models_anymore/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T14:23:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckn1n</id>
    <title>MiniCPM hallucinations in Ollama</title>
    <updated>2025-09-09T14:36:10+00:00</updated>
    <author>
      <name>/u/Fluid-Performance721</name>
      <uri>https://old.reddit.com/user/Fluid-Performance721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nckn1n/minicpm_hallucinations_in_ollama/"&gt; &lt;img alt="MiniCPM hallucinations in Ollama" src="https://b.thumbs.redditmedia.com/W6QtjXBC43ffyKEvK--qNix9VmDGULEWNQKwf9JHKNw.jpg" title="MiniCPM hallucinations in Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've recently started testing around with Ollama models again, and I tried using openbmb/minicpm-o2.6:8b as an image model, but it seems to massively hallucinate image descriptions, I probably did something wrong this time, since I've used this before and it DID work very well, maybe it's because of recent Ollama updates? I'm not sure, but this is pretty weird. I've tried running the model on a different computer as well and it still got the same weird results/hallucinations&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k34suqxwf5of1.png?width=676&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc29f1cc8843c17c46d24bcd69e3c1c25671fe9f"&gt;https://preview.redd.it/k34suqxwf5of1.png?width=676&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc29f1cc8843c17c46d24bcd69e3c1c25671fe9f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If I ran it in cmd, this is what it would say&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9i3deyepg5of1.png?width=1041&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1de132b7b59f98db8c64e214549a4856bc1ec5b"&gt;https://preview.redd.it/9i3deyepg5of1.png?width=1041&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1de132b7b59f98db8c64e214549a4856bc1ec5b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;keep in mind, the ACTUAL image i put was this picture of the golden gate bridge I found online as a test.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hwk31q9rg5of1.jpg?width=275&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=875780d34a8a6a9e6e453d9f363659ddc7ff7195"&gt;https://preview.redd.it/hwk31q9rg5of1.jpg?width=275&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=875780d34a8a6a9e6e453d9f363659ddc7ff7195&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've tried switching to Llava to see if images even work, Llava had no problems with that, but it was definitely less accurate than minicpm, back when it worked before anyway.&lt;/p&gt; &lt;p&gt;Anyone know any solutions or causes of this? I've already tried reinstalling the model and my ollama version is 0.11.10 (latest version the last time I checked)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluid-Performance721"&gt; /u/Fluid-Performance721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nckn1n/minicpm_hallucinations_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nckn1n/minicpm_hallucinations_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nckn1n/minicpm_hallucinations_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T14:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbqgj8</id>
    <title>Nanocoder is now on over 200 stars on GitHub ðŸ”¥</title>
    <updated>2025-09-08T15:14:57+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nbqgj8/nanocoder_is_now_on_over_200_stars_on_github/"&gt; &lt;img alt="Nanocoder is now on over 200 stars on GitHub ðŸ”¥" src="https://external-preview.redd.it/B1K19VSJZ1XNxMxdwGOqJ44wRvwJMPwJt-hWdao1JIw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=333de91255fc68d3b66679c4a7b8b8db62a28136" title="Nanocoder is now on over 200 stars on GitHub ðŸ”¥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pd1666jziynf1.png?width=5408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c0d2f01d12b8ab9efb1fb29a969f98d05bf7e1a"&gt;https://preview.redd.it/pd1666jziynf1.png?width=5408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c0d2f01d12b8ab9efb1fb29a969f98d05bf7e1a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is just an appreciation post for the community. My last Reddit post sent Nanocoder, the open-source coding CLI I started to over 200 stars on GitHub up from 80 - the feedback, insights and contributions is yet again, so humbling. &lt;/p&gt; &lt;p&gt;I love this community.&lt;/p&gt; &lt;p&gt;One of the main questions I get is &amp;quot;how is this different from other CLI's like OpenCode or Claude Code?&amp;quot; - and my answer to that comes down to philosophy for me. So many tools out there are great but owned and managed by a venture-backed companies and corporates and if they are open-source, more often than not, they restrict community and open source involvement to the outskirts.&lt;/p&gt; &lt;p&gt;With Nanocoder I do really want to build a community-led and managed piece of software. I would love to move Nanocoder towards being managed and built entirely by the open source community and build a competitor that is just as beautiful and well built as something like OpenCode.&lt;/p&gt; &lt;p&gt;Another thing is Iâ€™m pushing for local-first. People already within the community are putting a lot of time into developing solutions and better frameworks to run models locally that perform well as ultimately this is where the future is heading.&lt;/p&gt; &lt;p&gt;I'm still looking for any feedback and help in any domain:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;System prompt writing&lt;/li&gt; &lt;li&gt;Helping to push the word out&lt;/li&gt; &lt;li&gt;Any feedback generally! Good or bad :)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to get involved the links are below.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Link&lt;/strong&gt;: &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord Link&lt;/strong&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbqgj8/nanocoder_is_now_on_over_200_stars_on_github/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbqgj8/nanocoder_is_now_on_over_200_stars_on_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbqgj8/nanocoder_is_now_on_over_200_stars_on_github/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T15:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc75bq</id>
    <title>Local Open Source Alternative to NotebookLM</title>
    <updated>2025-09-09T02:24:44+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Hereâ€™s a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Podcasts&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Support for local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt; &lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt; &lt;li&gt;Multiple TTS providers supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search Engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Jira&lt;/li&gt; &lt;li&gt;ClickUp&lt;/li&gt; &lt;li&gt;Gmail&lt;/li&gt; &lt;li&gt;Confluence&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;Youtube Videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;Discord&lt;/li&gt; &lt;li&gt;Airtable&lt;/li&gt; &lt;li&gt;Google Calandar&lt;/li&gt; &lt;li&gt;and more to come.....&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Cross-Browser Extension&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nc75bq/local_open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nc75bq/local_open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nc75bq/local_open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T02:24:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncceae</id>
    <title>Built an AI news agent that actually stops information overload</title>
    <updated>2025-09-09T07:22:58+00:00</updated>
    <author>
      <name>/u/ComplexScary8689</name>
      <uri>https://old.reddit.com/user/ComplexScary8689</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sick of reading the same story 10 times across different sources?&lt;/p&gt; &lt;p&gt;Built an AI agent that deduplicates news semantically and synthesizes multiple articles into single summaries.&lt;/p&gt; &lt;p&gt;Uses LangGraph reactive pattern + BGE embeddings to understand when articles are actually the same story, then merges them intelligently. Configured via YAML instead of algorithmic guessing.&lt;/p&gt; &lt;p&gt;Live at &lt;a href="http://news.reckoning.dev/"&gt;news.reckoning.dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Built with LangGraph/Ollama if anyone wants to adapt the pattern&lt;/p&gt; &lt;p&gt;Full post at: &lt;a href="https://reckoning.dev/posts/news-agent-reactive-intelligence"&gt;https://reckoning.dev/posts/news-agent-reactive-intelligence&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexScary8689"&gt; /u/ComplexScary8689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncceae/built_an_ai_news_agent_that_actually_stops/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncceae/built_an_ai_news_agent_that_actually_stops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ncceae/built_an_ai_news_agent_that_actually_stops/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T07:22:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nclpsv</id>
    <title>Building an Ollama LLM detector: suggestions welcome :)</title>
    <updated>2025-09-09T15:17:09+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nclpsv/building_an_ollama_llm_detector_suggestions/"&gt; &lt;img alt="Building an Ollama LLM detector: suggestions welcome :)" src="https://external-preview.redd.it/aHVmYWNxNDBvNW9mMfubv4xrJ0EGfE1Jt62kyw2TXl4jxOv8uA65QtASYNDp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9d898cddf0789bda20da51521d0eefa39ae13ab" title="Building an Ollama LLM detector: suggestions welcome :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/Pavelevich/llm-checker"&gt;https://github.com/Pavelevich/llm-checker&lt;/a&gt; npm:&lt;a href="https://www.npmjs.com/package/llm-checker"&gt;https://www.npmjs.com/package/llm-checker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/brvvwp40o5of1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nclpsv/building_an_ollama_llm_detector_suggestions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nclpsv/building_an_ollama_llm_detector_suggestions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T15:17:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nczqst</id>
    <title>Sudden performance loss Ollama &amp; Termux</title>
    <updated>2025-09-10T00:21:03+00:00</updated>
    <author>
      <name>/u/Thepumayman</name>
      <uri>https://old.reddit.com/user/Thepumayman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. Pretty new to LLMs. I have a gen 4 Lenovo y700 tablet. It was running Ollama through Termix extremely well. Fired it up today and I'm getting 0.3 tokens a second on all models that were previously getting 8-12 t/s. Any idea what could be happening? Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thepumayman"&gt; /u/Thepumayman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nczqst/sudden_performance_loss_ollama_termux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nczqst/sudden_performance_loss_ollama_termux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nczqst/sudden_performance_loss_ollama_termux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-10T00:21:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncwe18</id>
    <title>Any idea how to use ollama (debian) with 2x GPUs to load larger models?</title>
    <updated>2025-09-09T21:56:09+00:00</updated>
    <author>
      <name>/u/StartupTim</name>
      <uri>https://old.reddit.com/user/StartupTim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I have a system that currently has a RTX 5090 32GB and I'll be adding another RTX 5070 Ti 16GB.&lt;/p&gt; &lt;p&gt;Is there a way I can use both of them at the same time on a single ollama model? If so, what is entailed to get this going and how would it work? Is it okay that both GPUs are different (5090 + 5070 Ti), or do they need to be the same? &lt;/p&gt; &lt;p&gt;If it does work, what happens with regards to the num_ctx, does it sit fully on both GPUs, or do each CPU somehow share part of it, or how's that work?&lt;/p&gt; &lt;p&gt;System specs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Debian 12x (latest) Ollama (latest) RTX 5090 32GB VRAM RTX 5070Ti 16GB VRAM 64GB DDR5 6000 Nvidia driver 575.57.08 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StartupTim"&gt; /u/StartupTim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncwe18/any_idea_how_to_use_ollama_debian_with_2x_gpus_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncwe18/any_idea_how_to_use_ollama_debian_with_2x_gpus_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ncwe18/any_idea_how_to_use_ollama_debian_with_2x_gpus_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T21:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncgm7d</id>
    <title>Best "parameters &lt; 10b" LLM to use as Fullstack Developer as Agent with Ollama</title>
    <updated>2025-09-09T11:44:55+00:00</updated>
    <author>
      <name>/u/kazeotokudai</name>
      <uri>https://old.reddit.com/user/kazeotokudai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Greetings. I'm looking for an open-source model which performs fair enough for my system. &lt;/p&gt; &lt;p&gt;I don't want to be specific, so I only want to ask you guys: What do you suggest?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kazeotokudai"&gt; /u/kazeotokudai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncgm7d/best_parameters_10b_llm_to_use_as_fullstack/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncgm7d/best_parameters_10b_llm_to_use_as_fullstack/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ncgm7d/best_parameters_10b_llm_to_use_as_fullstack/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T11:44:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncyyxf</id>
    <title>Ubuntu 24.04, 64GB, AMD iGPU 780M, ROCm 6.12, GTT vs VRAM</title>
    <updated>2025-09-09T23:45:30+00:00</updated>
    <author>
      <name>/u/reywang18</name>
      <uri>https://old.reddit.com/user/reywang18</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It was with GTT 24GB, and I run some model, thru SSH. I noticed by amdgpu_top, mostly GTT has higher usages, and almost thing used in VRAM.&lt;/p&gt; &lt;p&gt;I changed GRUB with amdgpu.gttsize to 8192 (M), now VRAM usage is picking up. Possible performace is 30%+ improved.&lt;/p&gt; &lt;p&gt;Got this error, couple times.&lt;/p&gt; &lt;p&gt;17:10:43 U24 ollama[3526]: load: &lt;strong&gt;special_eos_id&lt;/strong&gt; is not in special_eog_ids - the tokenizer config may be incorrect&lt;/p&gt; &lt;p&gt;And qwen3 14b seems not very stable, not sure due to AMD CPU/Radeon or not. &lt;/p&gt; &lt;p&gt;Please comments how to fix these, or suggest which model / steps I can try it on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reywang18"&gt; /u/reywang18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncyyxf/ubuntu_2404_64gb_amd_igpu_780m_rocm_612_gtt_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncyyxf/ubuntu_2404_64gb_amd_igpu_780m_rocm_612_gtt_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ncyyxf/ubuntu_2404_64gb_amd_igpu_780m_rocm_612_gtt_vs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T23:45:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd4q42</id>
    <title>Best model to "train" for cover letters?</title>
    <updated>2025-09-10T04:25:12+00:00</updated>
    <author>
      <name>/u/Key_Appointment_7582</name>
      <uri>https://old.reddit.com/user/Key_Appointment_7582</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a model I can &amp;quot;train&amp;quot; via Open WebUI that will give me the best results when making a cover letters. Is there a current local model that might be the best for this that can sun on an 8 GB VRAM 4070? &lt;/p&gt; &lt;p&gt;I currently post the job listing, my resume, and how I want the AI to write it into chat but chat hallucinates a lot of my qualifications rather than focusing on writing down what I am asking of it. Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Key_Appointment_7582"&gt; /u/Key_Appointment_7582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nd4q42/best_model_to_train_for_cover_letters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nd4q42/best_model_to_train_for_cover_letters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nd4q42/best_model_to_train_for_cover_letters/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-10T04:25:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd8c8x</id>
    <title>Best tiny model for a chatbot</title>
    <updated>2025-09-10T08:10:26+00:00</updated>
    <author>
      <name>/u/Digi-Device_File</name>
      <uri>https://old.reddit.com/user/Digi-Device_File</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to make a conversational chatbot for a game, it doesn't need to be able write code or solve complex math just talk like an average person. What's the best light model for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Digi-Device_File"&gt; /u/Digi-Device_File &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nd8c8x/best_tiny_model_for_a_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nd8c8x/best_tiny_model_for_a_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nd8c8x/best_tiny_model_for_a_chatbot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-10T08:10:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd8cy2</id>
    <title>Open Notebook and NotebookLM &amp; use them locally</title>
    <updated>2025-09-10T08:11:45+00:00</updated>
    <author>
      <name>/u/wash-basin</name>
      <uri>https://old.reddit.com/user/wash-basin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have worked all day with ChatGPT trying to setup a lite NotebookLM. I am more confused now than I was before deciding such a thing.&lt;/p&gt; &lt;p&gt;This is what I want:&lt;/p&gt; &lt;p&gt;I want to run an LLM locally on my computer and have it trained on my own data. My data will be hundreds of architecture articles and I want to have the AI be able to assess, analyze, and give original answers to questions I may pose to it. &lt;/p&gt; &lt;p&gt;I also want to be able to describe with text some images I want made and I desire for the LLM/AI I use to be able to interpret my text in the context of the local documents which will be used for training the LLM and provide unique example images from the research documents used in training. &lt;/p&gt; &lt;p&gt;An example would be that I want an image of a double-facade with photovoltaic glass on the exterior and clear glass on the interior. I want the LLM to look at all of my documents and come up with a streamlined and complete image of the double-facade I described. Or I might ask for an image of an example of the best angle to create for a clerestory and the AI will give me an image based on the research.&lt;/p&gt; &lt;p&gt;TL;DR:&lt;/p&gt; &lt;p&gt;Can I use either the AnythingLLMDesktop and LM-Studio-0.3.25-2-x64 could do what I want, but I am not sure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Request for Advice:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;What would be the easiest methods for installing AI/LLM locally to help assess 1000+ articles and answer questions about the content?&lt;/p&gt; &lt;p&gt;Is there a way to point the language model to my articles for ingestion rather than copy and paste them in a certain area on my hard drive?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Thank you all for reading this. If I need to provide any additional answers to help my desires come to life, please ask.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wash-basin"&gt; /u/wash-basin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nd8cy2/open_notebook_and_notebooklm_use_them_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nd8cy2/open_notebook_and_notebooklm_use_them_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nd8cy2/open_notebook_and_notebooklm_use_them_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-10T08:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd785y</id>
    <title>Best Tiny Model for programming?</title>
    <updated>2025-09-10T06:56:10+00:00</updated>
    <author>
      <name>/u/Late_Comfortable5094</name>
      <uri>https://old.reddit.com/user/Late_Comfortable5094</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a model out there that's under 2B params which is surprisingly proficient in programming? I have an old mac, which dies with anything after 2B. I use the 1.5B version of Deepseek-r1, and it is surprisingly good. Are there any other models out there that you have tried, and maybe they're better than this one?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Late_Comfortable5094"&gt; /u/Late_Comfortable5094 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nd785y/best_tiny_model_for_programming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nd785y/best_tiny_model_for_programming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nd785y/best_tiny_model_for_programming/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-10T06:56:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncp908</id>
    <title>Qwen 8B on locally on iPhone - 10 tokens/s</title>
    <updated>2025-09-09T17:29:29+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ncp908/qwen_8b_on_locally_on_iphone_10_tokenss/"&gt; &lt;img alt="Qwen 8B on locally on iPhone - 10 tokens/s" src="https://external-preview.redd.it/cG10OHlsZGNjNm9mMTOl0IOQAgqwlCRxZAKRf2LxKM72dLj3tO8fplxlobs7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d82834d07921ef83fa4a9a2caa0ee5a907985f89" title="Qwen 8B on locally on iPhone - 10 tokens/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have pushed what is possible on mobile devices!&lt;/p&gt; &lt;p&gt;Vector Space a project and app that explores what is possible for AI on iOS devices. We believe are very capable devices for AI and we wish to help fill the gap that some company is leaving out. &lt;/p&gt; &lt;p&gt;I am pleased to announce that we have fit Qwen 8B to run on iPhone. It runs 10 token/s on iPhone 16, on ANE too - so it doesnâ€™t drain your battery. Fitting a model this big to the memory limited environment of an iPhone required serious optimization and compression for the hardware.&lt;/p&gt; &lt;p&gt;Also, thanks to your feedback, you can now not only run, but SERVE all models ranging from Qwen 0.6B to 8B in a OpenAI compatible endpoint. You can point your app directly to this localhost endpoint to start saving from API cost now. Simply turn on the Web Server in settings after compiling a model. &lt;/p&gt; &lt;p&gt;You can try these features out today on our TestFlight beta app. You can download and run local models - including the 8B - without a line of code. If you encounter an issue, please report them - it will be much appreciated.&lt;/p&gt; &lt;p&gt;&lt;a href="https://testflight.apple.com/join/HXyt2bjU"&gt;https://testflight.apple.com/join/HXyt2bjU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please consider complete this survey to help determine what would be the next step for Vector Space&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/VectorSpaceApp/s/9ZZGS8YeeI"&gt;https://www.reddit.com/r/VectorSpaceApp/s/9ZZGS8YeeI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fine prints: -8B is tested on iPhone 16 only. iPhone 14 supports up to 4B. -Please delete and redownload if you are an existing tester. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7mpa5hicc6of1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncp908/qwen_8b_on_locally_on_iphone_10_tokenss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ncp908/qwen_8b_on_locally_on_iphone_10_tokenss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T17:29:29+00:00</published>
  </entry>
</feed>
