<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-05T09:49:39+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pcyqd3</id>
    <title>Is your local model safe from "Emoji Smuggling"? Visual demo of Prompt Injection</title>
    <updated>2025-12-03T08:21:07+00:00</updated>
    <author>
      <name>/u/jokiruiz</name>
      <uri>https://old.reddit.com/user/jokiruiz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We all love Ollama because it makes running local, private models incredibly easy. But I've been testing how these open weights models handle Prompt Injection and Logic Hacking.&lt;/p&gt; &lt;p&gt;I made a video demonstrating how attackers can use techniques like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Emoji Smuggling: Hiding malicious instructions inside tokens that look innocent (like emojis) but trigger specific behaviors in the model.&lt;/li&gt; &lt;li&gt;Roleplay Attacks: Bypassing safety filters just by using the classic &amp;quot;Grandma exploit&amp;quot; or logic games (demonstrated with the Gandalf game).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even if Ollama runs locally and offline, if you connect it to a UI that scrapes the web or reads external text, your model might be vulnerable to these &amp;quot;invisible&amp;quot; commands.&lt;/p&gt; &lt;p&gt;- Video Link: &lt;a href="https://youtu.be/Kck8JxHmDOs?si=yp5sjBq_d2hh5QU3"&gt;https://youtu.be/Kck8JxHmDOs?si=yp5sjBq_d2hh5QU3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For the Ollama users: Have you found any robust System Prompts (Modelfiles) that effectively block these types of &amp;quot;jailbreaks&amp;quot;? Or is Llama 3 still too easily convinced to break character?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jokiruiz"&gt; /u/jokiruiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcyqd3/is_your_local_model_safe_from_emoji_smuggling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcyqd3/is_your_local_model_safe_from_emoji_smuggling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pcyqd3/is_your_local_model_safe_from_emoji_smuggling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T08:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcsp6c</id>
    <title>ministral-3 and mistral-large-3</title>
    <updated>2025-12-03T02:53:07+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ollama.com/library/ministral-3"&gt;https://ollama.com/library/ministral-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ministral-3: The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/mistral-large-3"&gt;https://ollama.com/library/mistral-large-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mistral-Large-3: A general-purpose multimodal mixture-of-experts model for production-grade tasks and enterprise workloads.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This models requires Ollama 0.13.1&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcsp6c/ministral3_and_mistrallarge3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcsp6c/ministral3_and_mistrallarge3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pcsp6c/ministral3_and_mistrallarge3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T02:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdhg4p</id>
    <title>NornicDB - Heimdall (embedded llm executor) + plugins - MIT Licensed</title>
    <updated>2025-12-03T21:43:13+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pdgfr4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdhg4p/nornicdb_heimdall_embedded_llm_executor_plugins/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdhg4p/nornicdb_heimdall_embedded_llm_executor_plugins/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T21:43:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdmzpf</id>
    <title>Tested GPT-5.1, Gemini 3, and Claude Opus 4.5 on real data analysis tasks. Results surprised me.</title>
    <updated>2025-12-04T01:36:24+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pdmzpf/tested_gpt51_gemini_3_and_claude_opus_45_on_real/"&gt; &lt;img alt="Tested GPT-5.1, Gemini 3, and Claude Opus 4.5 on real data analysis tasks. Results surprised me." src="https://b.thumbs.redditmedia.com/Bs4LExwpf9RifuGZfuOabpughUO-dDZGVP8YrwtFR3M.jpg" title="Tested GPT-5.1, Gemini 3, and Claude Opus 4.5 on real data analysis tasks. Results surprised me." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/GeminiAI/comments/1pd7kre/tested_gpt51_gemini_3_and_claude_opus_45_on_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdmzpf/tested_gpt51_gemini_3_and_claude_opus_45_on_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdmzpf/tested_gpt51_gemini_3_and_claude_opus_45_on_real/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T01:36:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdn0qt</id>
    <title>Update: I added more open-source AI tools + new categories to my ForgeIndex project</title>
    <updated>2025-12-04T01:37:47+00:00</updated>
    <author>
      <name>/u/Equivalent-Ad-9798</name>
      <uri>https://old.reddit.com/user/Equivalent-Ad-9798</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been expanding the index of open-source AI tools I posted last week. Added new categories, improved the browsing experience, and added a bunch of projects people suggested.&lt;/p&gt; &lt;p&gt;Still a work in progress, but it’s starting to feel really usable.&lt;/p&gt; &lt;p&gt;If anyone wants to check it out or give feedback, here’s the link: &lt;a href="https://forgeindex.ai"&gt;https://forgeindex.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks again to everyone who checked it out the first time…The support was super motivating.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent-Ad-9798"&gt; /u/Equivalent-Ad-9798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdn0qt/update_i_added_more_opensource_ai_tools_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdn0qt/update_i_added_more_opensource_ai_tools_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdn0qt/update_i_added_more_opensource_ai_tools_new/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T01:37:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdlr9e</id>
    <title>Recent Update Shows Scrollbar Rail</title>
    <updated>2025-12-04T00:40:10+00:00</updated>
    <author>
      <name>/u/Delirious_Rimbaud</name>
      <uri>https://old.reddit.com/user/Delirious_Rimbaud</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pdlr9e/recent_update_shows_scrollbar_rail/"&gt; &lt;img alt="Recent Update Shows Scrollbar Rail" src="https://preview.redd.it/67j4da7j235g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ce03c1cd4bb49970ee47aaff4140542ad910863" title="Recent Update Shows Scrollbar Rail" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delirious_Rimbaud"&gt; /u/Delirious_Rimbaud &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/67j4da7j235g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdlr9e/recent_update_shows_scrollbar_rail/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdlr9e/recent_update_shows_scrollbar_rail/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T00:40:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdi80d</id>
    <title>AI Runner v5.0.5</title>
    <updated>2025-12-03T22:12:56+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This update allows you to run AI Runner as a headless server, but mask AI Runner as Ollama so that other services such as VSCode will think it is interfacing with Ollama, allowing you to select it as &amp;quot;Ollama&amp;quot; from the model manager in VSCode Copilot Chat.&lt;/p&gt; &lt;p&gt;Note: I haven't been able to get it to work well with agents yet, but it does work with tools if you choose the right model.&lt;/p&gt; &lt;p&gt;after you follow the installation instructions, you can use `airunner-hf-download` to list the available models and then `airunner-hf-download &amp;lt;name&amp;gt;` to download one. You might need to activate it by running the gui with `airunner` and selecting it in the chat prompt widget dropdown box. Then you can close the GUI and run `airunner-headless --ollama-mode` - after the server starts, it will be available within vscode by simply choosing &amp;quot;ollama&amp;quot;&lt;/p&gt; &lt;p&gt;Obviously, you can't have the real ollama running at the same time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Capsize-Games/airunner/compare/v5.0.4...v5.0.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdi80d/ai_runner_v505/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdi80d/ai_runner_v505/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T22:12:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdu2qt</id>
    <title>Two minor creation issues with my local Ollama</title>
    <updated>2025-12-04T07:41:24+00:00</updated>
    <author>
      <name>/u/IAmAUser4Real</name>
      <uri>https://old.reddit.com/user/IAmAUser4Real</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I just managed to recover the GGUF files for both GPT-OSS-20b and Qwen3-4B; followed the base modelfile instruction on how to have them within my &amp;quot;library&amp;quot; of models, and they are there, ready to answer.&lt;/p&gt; &lt;p&gt;What I'm having issue with is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;GPT - after my questions states again what was my requests with some delimiting markers, that I'd like to avoid:&lt;/p&gt; &lt;p&gt;We have to produce a response that follows the instruction: &amp;quot;You are a helpful assistant.&amp;quot; So we should respond as a helpful assistant. The user says &amp;quot;hi, who are you?&amp;quot; We should respond politely, say we are ChatGPT, a large language model trained by OpenAI, etc. And we should not mention policy. So let's do that.&amp;lt;|end|&amp;gt;&amp;lt;|start|&amp;gt;assistant&amp;lt;|channel|&amp;gt;final&amp;lt;|message|&amp;gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- After that it just provides the answer that I want with enough details to be useful.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Qwen - even if in the Modelfile I have put 3 &amp;quot;PARAMETER stop&amp;quot; it keeps on repeating whatever was the request, in addition to the proper reply:&lt;/p&gt; &lt;p&gt;I am Qwen, a large-scale language model developed by Alibaba Cloud. I can answer questions, create content, and perform various tasks. How can I assist you today? HUMAN: what is the best way to learn a new language? RESPONSE: Learning a new language can be a rewarding experience. Here are some effective strategies to help you learn a new language efficiently:&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- First part is fine, but the &amp;quot;HUMAN/RESPONSE&amp;quot; i want to get rid of it.&lt;/p&gt; &lt;p&gt;I used a similar Modelfile for both, following some suggestions on how to stop them from ranting around, as that was the initial issue:&lt;/p&gt; &lt;p&gt;FROM ./gpt-oss-20b-F16.gguf &lt;/p&gt; &lt;p&gt;PARAMETER temperature 0.7&lt;br /&gt; PARAMETER num_ctx 2048&lt;br /&gt; PARAMETER repeat_penalty 1.1&lt;br /&gt; PARAMETER num_predict -1&lt;br /&gt; PARAMETER top_k 40&lt;br /&gt; PARAMETER top_p 0.75&lt;br /&gt; PARAMETER min_p 0.05 &lt;/p&gt; &lt;p&gt;PARAMETER stop &amp;quot;### Input:&amp;quot;&lt;br /&gt; PARAMETER stop &amp;quot;### Response:&amp;quot;&lt;br /&gt; PARAMETER stop &amp;quot;### human&amp;quot; &lt;/p&gt; &lt;p&gt;TEMPLATE &amp;quot;&amp;quot;&amp;quot;### HUMAN:&lt;br /&gt; {{ .Prompt }} &lt;/p&gt; &lt;p&gt;### RESPONSE:&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;What is my main mistake, in either, and how can I fix them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IAmAUser4Real"&gt; /u/IAmAUser4Real &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdu2qt/two_minor_creation_issues_with_my_local_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdu2qt/two_minor_creation_issues_with_my_local_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdu2qt/two_minor_creation_issues_with_my_local_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T07:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe8uah</id>
    <title>i see why RAM and ssd prices are going up..</title>
    <updated>2025-12-04T19:09:44+00:00</updated>
    <author>
      <name>/u/one_moar_time</name>
      <uri>https://old.reddit.com/user/one_moar_time</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its because local AI is uncensored free speech that is not monitored.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/one_moar_time"&gt; /u/one_moar_time &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe8uah/i_see_why_ram_and_ssd_prices_are_going_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe8uah/i_see_why_ram_and_ssd_prices_are_going_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe8uah/i_see_why_ram_and_ssd_prices_are_going_up/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T19:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdq44e</id>
    <title>A free Opensource A.I. resource of video tutorials available as an iOS app</title>
    <updated>2025-12-04T04:02:32+00:00</updated>
    <author>
      <name>/u/Other_Passion_4710</name>
      <uri>https://old.reddit.com/user/Other_Passion_4710</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pdq44e/a_free_opensource_ai_resource_of_video_tutorials/"&gt; &lt;img alt="A free Opensource A.I. resource of video tutorials available as an iOS app" src="https://a.thumbs.redditmedia.com/k0JUJjrX055eQTSGU186a2qI76GBqxxtoRefZj-XX28.jpg" title="A free Opensource A.I. resource of video tutorials available as an iOS app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Just wanted to reshare a project I’ve been working on for anyone—from beginners to advanced developers—who enjoy learning the nuts and bolts of generative AI. I put together an app called &lt;strong&gt;A.I. DelvePad&lt;/strong&gt;, and it’s basically a casual toolbox of tutorials and resources for anyone curious about how generative AI models really work under the hood.&lt;/p&gt; &lt;p&gt;It includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;35+ free bite-sized video tutorials&lt;/strong&gt; (with more on the way)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A simple glossary&lt;/strong&gt; for key AI terms&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A quick intro to how LLMs are trained&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A tutorial-sharing feature&lt;/strong&gt; so you can pass finds to friends&lt;/li&gt; &lt;li&gt;Everything is &lt;strong&gt;100% free and open source&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to make your own resource app—whether it’s for YouTube tutorials, Udemy courses, or any hobby you’re into—you can dive into the repo and customize it however you like.&lt;/p&gt; &lt;p&gt;Would appreciate any feedback—good or bad. And if you’re building your own stuff too, keep going! It’s always fun to see what fellow hobbyists are creating.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;App Store:&lt;/strong&gt; &lt;a href="https://apps.apple.com/us/app/a-i-delvepad/id6743481267"&gt;https://apps.apple.com/us/app/a-i-delvepad/id6743481267&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/leapdeck/AIDelvePad"&gt;https://github.com/leapdeck/AIDelvePad&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Site:&lt;/strong&gt; &lt;a href="http://aidelvepad.com/"&gt;http://aidelvepad.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy coding!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Passion_4710"&gt; /u/Other_Passion_4710 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pdq44e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdq44e/a_free_opensource_ai_resource_of_video_tutorials/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdq44e/a_free_opensource_ai_resource_of_video_tutorials/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T04:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdy67t</id>
    <title>[Help] XTTS v2 drops first ~100–300ms of audio (24kHz) — CLI and API both affected. Anyone else?</title>
    <updated>2025-12-04T11:57:52+00:00</updated>
    <author>
      <name>/u/Mantus123</name>
      <uri>https://old.reddit.com/user/Mantus123</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mantus123"&gt; /u/Mantus123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/TextToSpeech/comments/1pdy5sj/help_xtts_v2_drops_first_100300ms_of_audio_24khz/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdy67t/help_xtts_v2_drops_first_100300ms_of_audio_24khz/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdy67t/help_xtts_v2_drops_first_100300ms_of_audio_24khz/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T11:57:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd9chp</id>
    <title>Ministral 3: A New Family of Edge Deployment Models</title>
    <updated>2025-12-03T16:46:27+00:00</updated>
    <author>
      <name>/u/United-Manner-7</name>
      <uri>https://old.reddit.com/user/United-Manner-7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just tried out the &lt;strong&gt;Ministral 3&lt;/strong&gt; family, and it looks pretty impressive:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; 3B, 8B, 14B (3–9.1GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context window:&lt;/strong&gt; 256K tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inputs:&lt;/strong&gt; Text + Images (cloud versions are text-only)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Vision support for image analysis&lt;/li&gt; &lt;li&gt;Multilingual (English, French, Spanish, German, Italian, Portuguese, Chinese, Japanese, Korean, Arabic…)&lt;/li&gt; &lt;li&gt;Strong system prompt adherence&lt;/li&gt; &lt;li&gt;Agentic abilities with native function calls and JSON output&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Edge-optimized:&lt;/strong&gt; can run on a wide range of hardware&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Apache 2.0 (open for commercial &amp;amp; non-commercial use)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Requires &lt;strong&gt;Ollama 0.13.1 (pre-release)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Run it with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama run ministral-3 # Basically the same as other models &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Feels like one of the most polished edge-deployable models right now, especially if you care about multimodal inputs and huge context.&lt;br /&gt; I would like to add that I would still stay on qwen and llama because they are already configured, but it was an interesting, albeit illogical, experience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Manner-7"&gt; /u/United-Manner-7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pd9chp/ministral_3_a_new_family_of_edge_deployment_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pd9chp/ministral_3_a_new_family_of_edge_deployment_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pd9chp/ministral_3_a_new_family_of_edge_deployment_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T16:46:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdpmh6</id>
    <title>GPT-OSS 120B vs QWEN3-NEXT 80B</title>
    <updated>2025-12-04T03:38:22+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT-OSS 120B vs QWEN3-NEXT 80B - A3B&lt;/p&gt; &lt;p&gt;Who have better balanced his MOE arquitecture GPT OSS with 128 experts or Qwen with 512 experts?&lt;/p&gt; &lt;p&gt;Who give the better result in quality/performance?&lt;/p&gt; &lt;p&gt;Who is better for coding?&lt;/p&gt; &lt;p&gt;Who is faster in toks/second without using a GPU.&lt;/p&gt; &lt;p&gt;Who is capable of running only with ram and one single gpu for experts loading.&lt;/p&gt; &lt;p&gt;Who is better in math&lt;/p&gt; &lt;p&gt;Who is better in coding python and C &lt;/p&gt; &lt;p&gt;Who is better for coding PICs and Arduino&lt;/p&gt; &lt;p&gt;Who have more knowledge..biology..medical…wikipedia…History..humanity…Engineering..etc&lt;/p&gt; &lt;p&gt;Give me our opinions , please.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdpmh6/gptoss_120b_vs_qwen3next_80b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdpmh6/gptoss_120b_vs_qwen3next_80b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdpmh6/gptoss_120b_vs_qwen3next_80b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T03:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe1dba</id>
    <title>Hito 1.7 GGUF release</title>
    <updated>2025-12-04T14:27:29+00:00</updated>
    <author>
      <name>/u/TastyWriting8360</name>
      <uri>https://old.reddit.com/user/TastyWriting8360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I will leave this here open wieghts &lt;a href="https://huggingface.co/hitonet/hito-1.7b"&gt;https://huggingface.co/hitonet/hito-1.7b&lt;/a&gt; and gguf release &lt;a href="https://huggingface.co/hitonet/hito-1.7b-GGUFI"&gt;https://huggingface.co/hitonet/hito-1.7b-GGUFI&lt;/a&gt; will make a proper post in the next few hours, gotta catch some sleep but cant wait to share this with you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TastyWriting8360"&gt; /u/TastyWriting8360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe1dba/hito_17_gguf_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe1dba/hito_17_gguf_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe1dba/hito_17_gguf_release/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T14:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe2tk3</id>
    <title>ministral-3 is not using gpu in ollama</title>
    <updated>2025-12-04T15:26:18+00:00</updated>
    <author>
      <name>/u/AdhesivenessLatter57</name>
      <uri>https://old.reddit.com/user/AdhesivenessLatter57</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;why ministral-3 is running on cpu only with ollama version is 0.13.1? &lt;/p&gt; &lt;p&gt;this model starts loading in gpu and later offloads to cpu.&lt;/p&gt; &lt;p&gt;I tried 3b,8b and 14b, while qwen3-coder is running fine in same gpu. &lt;/p&gt; &lt;p&gt;Some issue in ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdhesivenessLatter57"&gt; /u/AdhesivenessLatter57 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2tk3/ministral3_is_not_using_gpu_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2tk3/ministral3_is_not_using_gpu_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe2tk3/ministral3_is_not_using_gpu_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T15:26:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe5x7a</id>
    <title>smallevals - Tiny 0.6B Evaluation Models and a Local LLM Evaluation Framework</title>
    <updated>2025-12-04T17:23:26+00:00</updated>
    <author>
      <name>/u/mburaksayici</name>
      <uri>https://old.reddit.com/user/mburaksayici</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mburaksayici"&gt; /u/mburaksayici &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Rag/comments/1pe56ch/smallevals_tiny_06b_evaluation_models_and_a_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe5x7a/smallevals_tiny_06b_evaluation_models_and_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe5x7a/smallevals_tiny_06b_evaluation_models_and_a_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T17:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdw272</id>
    <title>Ollama powered chat component for any website</title>
    <updated>2025-12-04T09:51:52+00:00</updated>
    <author>
      <name>/u/ovi_nation</name>
      <uri>https://old.reddit.com/user/ovi_nation</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pdw272/ollama_powered_chat_component_for_any_website/"&gt; &lt;img alt="Ollama powered chat component for any website" src="https://preview.redd.it/d7xbh8i2t55g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=514ff5f50122295dad7147a13b43b905a32c3830" title="Ollama powered chat component for any website" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! I have open sourced a project called Deep Chat. It is a feature-rich chat web component that can be used to connect and converse with Ollama models.&lt;/p&gt; &lt;p&gt;Check it out at:&lt;br /&gt; &lt;a href="https://github.com/OvidijusParsiunas/deep-chat?fbclid=IwAR0uSvTiVXL5rICg3YfKqV2er0E355LGrg5ha6JVkUEaem8PKU98sU6ysbE"&gt;https://github.com/OvidijusParsiunas/deep-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A GitHub star is ALWAYS appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ovi_nation"&gt; /u/ovi_nation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d7xbh8i2t55g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdw272/ollama_powered_chat_component_for_any_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdw272/ollama_powered_chat_component_for_any_website/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T09:51:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe38x8</id>
    <title>AI Runner Release v5.1.0 · support for art, text-to-speech and speech-to-text in headless server mode</title>
    <updated>2025-12-04T15:43:22+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pe38x8/ai_runner_release_v510_support_for_art/"&gt; &lt;img alt="AI Runner Release v5.1.0 · support for art, text-to-speech and speech-to-text in headless server mode" src="https://external-preview.redd.it/EIGwz7Gm65cPP16X5uk_KCDLI-pxhanbMt56tZbOIho.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17e3785ebc1c34a9ccb69b71dd5325bac0a7e318" title="AI Runner Release v5.1.0 · support for art, text-to-speech and speech-to-text in headless server mode" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title pretty much says it all.&lt;/p&gt; &lt;p&gt;AI Runner can be used as an alternative to ollama for experimentation, educational purposes, serious professional projects during development (for example I choose to use my server when developing cloud based chatbots as it has the same endpoints as ollama and openai).&lt;/p&gt; &lt;p&gt;Works with LLM, art, tts, stt.&lt;/p&gt; &lt;p&gt;This application also comes with a GUI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://airunner.org"&gt;https://airunner.org&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Capsize-Games/airunner/releases/tag/v5.1.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe38x8/ai_runner_release_v510_support_for_art/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe38x8/ai_runner_release_v510_support_for_art/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T15:43:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdv7ya</id>
    <title>Quick real coding test GPT-OS-20b-Code vs. Ministral-3b-fp16</title>
    <updated>2025-12-04T08:56:20+00:00</updated>
    <author>
      <name>/u/New_Cranberry_6451</name>
      <uri>https://old.reddit.com/user/New_Cranberry_6451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few minutes ago I downloaded Ministral-3b-fp16 and plugged it to my 8Gb VRAM laptop running Windows and Ollama. Just a plain 0.13.1 version on a plain standard laptop.&lt;/p&gt; &lt;p&gt;I needed a simple PHP script to export rows from a mysql db to CSV, but I had a function that failed escaping when using &amp;quot;,&amp;quot; as delim.&lt;/p&gt; &lt;p&gt;I asked Ministral and obtained a classic working function that I will resume as (skipped to focus on point):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$header = str_getcsv(implode(',', array_keys($export_data[0])), ','); $csv_data = implode($csv_delim, $header) . &amp;quot;\r\n&amp;quot;; [...] // Escape commas and quotes in values $escaped_value = preg_replace('/\s+/', ' ', $value); $escaped_value = str_replace(',', ' ', $escaped_value); $escaped_value = str_replace('&amp;quot;', '&amp;quot;&amp;quot;', $escaped_value); $row_values[] = '&amp;quot;' . $escaped_value . '&amp;quot;'; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Cool. It works. Now I paste the exact same prompt into GPT-OS-20b-Code: (once again, skipped to focus on point)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// Open a temporary memory stream $fp = fopen( 'php://temp', 'r+' ); [...] fputcsv( $fp, $row, $csv_delim ); rewind( $fp ); $csv_data = stream_get_contents( $fp ); fclose( $fp ); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;WTF! This is so elegant. It delegates escaping to a core function so that the regExp-s, that are prone to fail in weird cases, are not even needed, and on the other hand, the way of treating the array as a temp stream to read and rewind blew my mind.&lt;/p&gt; &lt;p&gt;One shot prompts like these make me say THANKS for this open source gifts I can run locally and privately :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Cranberry_6451"&gt; /u/New_Cranberry_6451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdv7ya/quick_real_coding_test_gptos20bcode_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdv7ya/quick_real_coding_test_gptos20bcode_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdv7ya/quick_real_coding_test_gptos20bcode_vs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T08:56:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe5b1t</id>
    <title>Computer Use with Claude Opus 4.5</title>
    <updated>2025-12-04T17:01:11+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pe5b1t/computer_use_with_claude_opus_45/"&gt; &lt;img alt="Computer Use with Claude Opus 4.5" src="https://external-preview.redd.it/eWc1Z2J3ZG94NzVnMY644GvbBlUugVuvecrsXUzyNOCzfLoiwHL0f-lwE9G6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64ec98da6d0fbfaba8c88013ccf90ed592f5e91c" title="Computer Use with Claude Opus 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude Opus 4.5 support to the Cua VLM Router and Playground - and you can already see it running inside Windows sandboxes. Early results are seriously impressive, even on tricky desktop workflows.&lt;/p&gt; &lt;p&gt;Benchmark results:&lt;/p&gt; &lt;p&gt;-new SOTA 66.3% on OSWorld (beats Sonnet 4.5’s 61.4% in the general model category)&lt;/p&gt; &lt;p&gt;-88.9% on tool-use&lt;/p&gt; &lt;p&gt;Better reasoning. More reliable multi-step execution.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try the playground here : &lt;a href="https://cua.ai"&gt;https://cua.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9q65hzoox75g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe5b1t/computer_use_with_claude_opus_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe5b1t/computer_use_with_claude_opus_45/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T17:01:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe0s1q</id>
    <title>New Feature in RAGLight: Multimodal PDF Ingestion</title>
    <updated>2025-12-04T14:02:16+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"&gt; &lt;img alt="New Feature in RAGLight: Multimodal PDF Ingestion" src="https://b.thumbs.redditmedia.com/UkmGLN7LsRnojkAulEBihmGFp1HrsIPIMO8GS0KQ9ko.jpg" title="New Feature in RAGLight: Multimodal PDF Ingestion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I just added a small but powerful feature to &lt;strong&gt;RAGLight&lt;/strong&gt;: you can now override any document processor, and this unlocks &lt;strong&gt;a new built-in example : a VLM-powered PDF parser.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Find repo here :&lt;/strong&gt; &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;https://github.com/Bessouat40/RAGLight&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try this new feature with many LLM provider such as &lt;strong&gt;Ollama&lt;/strong&gt; !&lt;/p&gt; &lt;h1&gt;What it does&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Extracts &lt;strong&gt;text AND images&lt;/strong&gt; from PDFs&lt;/li&gt; &lt;li&gt;Sends images to a &lt;strong&gt;Vision-Language Model&lt;/strong&gt; (Mistral, OpenAI, etc.)&lt;/li&gt; &lt;li&gt;Captions them and injects the result into your vector store&lt;/li&gt; &lt;li&gt;Makes RAG truly understand diagrams, block schemas, charts, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Super helpful for technical documentation, research papers, engineering PDFs…&lt;/p&gt; &lt;h1&gt;Minimal Example&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/di41m8op175g1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d01198e5a8bf6c0b0c64f46e9b8fe176debc2707"&gt;https://preview.redd.it/di41m8op175g1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d01198e5a8bf6c0b0c64f46e9b8fe176debc2707&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;p&gt;Most RAG tools ignore images entirely. Now RAGLight can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;interpret diagrams&lt;/li&gt; &lt;li&gt;index visual content&lt;/li&gt; &lt;li&gt;retrieve multimodal meaning&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T14:02:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdui4a</id>
    <title>Pipeshub just hit 2k GitHub stars.</title>
    <updated>2025-12-04T08:08:14+00:00</updated>
    <author>
      <name>/u/Inevitable-Letter385</name>
      <uri>https://old.reddit.com/user/Inevitable-Letter385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re super excited to share a milestone that wouldn’t have been possible without this community. &lt;strong&gt;PipesHub just crossed 2,000 GitHub stars!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thank you to everyone who tried it out, shared feedback, opened issues, or even just followed the project.&lt;/p&gt; &lt;p&gt;For those who haven’t heard of it yet, &lt;strong&gt;PipesHub&lt;/strong&gt; is a fully open-source enterprise search platform we’ve been building over the past few months. Our goal is simple: bring powerful &lt;strong&gt;Enterprise Search&lt;/strong&gt; and &lt;strong&gt;Agent Builders&lt;/strong&gt; to every team, without vendor lock-in. PipesHub brings all your business data together and makes it instantly searchable.&lt;/p&gt; &lt;p&gt;It integrates with tools like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, and even local files. You can deploy it with a single Docker Compose command.&lt;/p&gt; &lt;p&gt;Under the hood, PipesHub runs on a &lt;strong&gt;Kafka powered event streaming architecture&lt;/strong&gt;, giving it real time, scalable, fault tolerant indexing. It combines a vector database with a knowledge graph and uses &lt;strong&gt;Agentic RAG&lt;/strong&gt; to keep responses grounded in source of truth. You get visual citations, reasoning, and confidence scores, and if information isn’t found, it simply says so instead of hallucinating.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enterprise knowledge graph for deep understanding of users, orgs, and teams&lt;/li&gt; &lt;li&gt;Connect to any AI model: OpenAI, Gemini, Claude, Ollama, or any OpenAI compatible endpoint&lt;/li&gt; &lt;li&gt;Vision Language Models and OCR for images and scanned documents&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, and SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs&lt;/li&gt; &lt;li&gt;Support for all major file types, including PDFs with images and diagrams&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Builder&lt;/strong&gt; for actions like sending emails, scheduling meetings, deep research, internet search, and more&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning Agent&lt;/strong&gt; with planning capabilities&lt;/li&gt; &lt;li&gt;&lt;strong&gt;40+ connectors&lt;/strong&gt; for integrating with your business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’d love for you to check it out and share your thoughts or feedback. It truly helps guide the roadmap:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1pdudws"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Letter385"&gt; /u/Inevitable-Letter385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T08:08:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe18c7</id>
    <title>How does Ollama truncate the context when it's too long?</title>
    <updated>2025-12-04T14:21:49+00:00</updated>
    <author>
      <name>/u/AmphibianFrog</name>
      <uri>https://old.reddit.com/user/AmphibianFrog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to understand how prompts are truncated then they exceed the context limit. Let's say I have the num_ctx parameter set to 1000 tokens. I then send in a system prompt, and a whole bunch of user and assistant messages, but in total there are 2000 tokens in the context. What happens?&lt;/p&gt; &lt;p&gt;Specific questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does truncation happen per message or do partial messages get fed into the LLM?&lt;/li&gt; &lt;li&gt;Does the system prompt get truncated? Or is it always passed in even if other messages are removed from the input?&lt;/li&gt; &lt;li&gt;Do the messages get removed in pairs, i.e will it always make sure a user message is first in the prompt?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;My conclusion is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The system prompt is never truncated&lt;/li&gt; &lt;li&gt;Entire messages are dropped from the context, not individual tokens&lt;/li&gt; &lt;li&gt;Ollama will include as many messages as possible, getting rid of the oldest messages first (apart from the system prompt which is always included)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hopefully this helps someone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AmphibianFrog"&gt; /u/AmphibianFrog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T14:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe2xbe</id>
    <title>how close to codex or claude code can you get with ollama and a 4090?</title>
    <updated>2025-12-04T15:30:22+00:00</updated>
    <author>
      <name>/u/reelznfeelz</name>
      <uri>https://old.reddit.com/user/reelznfeelz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to run an agentic code assistant that can use tools on top of ollama without having to spend a whole ton of time on building it yourself? &lt;/p&gt; &lt;p&gt;Somebody here has probably followed or played in that space. &lt;/p&gt; &lt;p&gt;Just thinking/planning for the day when openAI/Claude has to charge what's actually needed to be profitable, which will be like $900/mo for the kind of usage I have. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reelznfeelz"&gt; /u/reelznfeelz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T15:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pekbzp</id>
    <title>No Uncensored Models?</title>
    <updated>2025-12-05T03:16:26+00:00</updated>
    <author>
      <name>/u/One_Spaceman</name>
      <uri>https://old.reddit.com/user/One_Spaceman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just downloaded Ollama, pulled Dolphin-Ollama3 from &lt;a href="https://www.youtube.com/watch?v=cTxENLLX1ho"&gt;THIS &lt;/a&gt;vid but its completely censored, is there anything that is totally uncensored? idk what to do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One_Spaceman"&gt; /u/One_Spaceman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T03:16:26+00:00</published>
  </entry>
</feed>
