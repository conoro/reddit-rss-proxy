<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-22T20:48:51+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1nlpwkg</id>
    <title>Goto mcps</title>
    <updated>2025-09-20T05:32:55+00:00</updated>
    <author>
      <name>/u/Humbrol2</name>
      <uri>https://old.reddit.com/user/Humbrol2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are everyone's goto mcps free or paid&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humbrol2"&gt; /u/Humbrol2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlpwkg/goto_mcps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlpwkg/goto_mcps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlpwkg/goto_mcps/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-20T05:32:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlemes</id>
    <title>Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp; Streamlit</title>
    <updated>2025-09-19T20:31:33+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nlemes/build_a_local_ai_agent_with_mcp_tools_using/"&gt; &lt;img alt="Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp;amp; Streamlit" src="https://external-preview.redd.it/rq8k6bkBVDqS3EaB-6PmZwrrp9mjAeoX2Tt37ubIdpg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76f91a095d5d6782424d54183f94d9fb060dd411" title="Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp;amp; Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/Baa-z7cum1g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlemes/build_a_local_ai_agent_with_mcp_tools_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlemes/build_a_local_ai_agent_with_mcp_tools_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T20:31:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nl8a7w</id>
    <title>[Project] I created an AI photo organizer that uses Ollama to sort photos, filter duplicates, and write Instagram captions.</title>
    <updated>2025-09-19T16:29:01+00:00</updated>
    <author>
      <name>/u/summitsc</name>
      <uri>https://old.reddit.com/user/summitsc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone at &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I wanted to share a Python project I've been working on called the &lt;strong&gt;AI Instagram Organizer&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; I had thousands of photos from a recent trip, and the thought of manually sorting them, finding the best ones, and thinking of captions was overwhelming. I wanted a way to automate this using local LLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution:&lt;/strong&gt; I built a script that uses a multimodal model via Ollama (like LLaVA, Gemma, or Llama 3.2 Vision) to do all the heavy lifting.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Chronological Sorting:&lt;/strong&gt; It reads EXIF data to organize posts by the date they were taken.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Duplicate Filtering:&lt;/strong&gt; It uses multiple perceptual hashes and a dynamic threshold to remove repetitive shots.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Caption &amp;amp; Hashtag Generation:&lt;/strong&gt; For each post folder it creates, it writes several descriptive caption options and a list of hashtags.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Handles HEIC Files:&lt;/strong&gt; It automatically converts Apple's HEIC format to JPG.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s been a really fun project and a great way to explore what's possible with local vision models. I'd love to get your feedback and see if it's useful to anyone else!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/summitsingh/ai-instagram-organizer"&gt;https://github.com/summitsingh/ai-instagram-organizer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since this is my first time building an open-source AI project, any feedback is welcome. And if you like it, a star on GitHub would really make my day! ⭐&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/summitsc"&gt; /u/summitsc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl8a7w/project_i_created_an_ai_photo_organizer_that_uses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl8a7w/project_i_created_an_ai_photo_organizer_that_uses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nl8a7w/project_i_created_an_ai_photo_organizer_that_uses/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T16:29:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nl0d45</id>
    <title>Ollama or LM Studio?</title>
    <updated>2025-09-19T10:55:22+00:00</updated>
    <author>
      <name>/u/Artaherzadeh</name>
      <uri>https://old.reddit.com/user/Artaherzadeh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to install and run it on my PC, which has a 12600k CPU, 6700XT AMD GPU 12G, and 32GB RAM. Which one is better in terms of features, UI, performance and etc?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Artaherzadeh"&gt; /u/Artaherzadeh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl0d45/ollama_or_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl0d45/ollama_or_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nl0d45/ollama_or_lm_studio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T10:55:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlo7pk</id>
    <title>MCP-servers http connection</title>
    <updated>2025-09-20T03:58:21+00:00</updated>
    <author>
      <name>/u/Few-Advance4363</name>
      <uri>https://old.reddit.com/user/Few-Advance4363</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm playing with MCP servers and local llm. I want to use a client other than terminal. &lt;/p&gt; &lt;p&gt;Currently, I can connect my client over network to ip:11434 and the model works.&lt;/p&gt; &lt;p&gt;I can also get my llm to connected to mcp-server using ollmcp. However, I have to be directly on the terminal of the llm machine.&lt;/p&gt; &lt;p&gt;How can I get ollama to connect to the mcp server AND then allow a client to have access to those MCP tools?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Advance4363"&gt; /u/Few-Advance4363 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlo7pk/mcpservers_http_connection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlo7pk/mcpservers_http_connection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlo7pk/mcpservers_http_connection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-20T03:58:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlinpi</id>
    <title>Wanna try an uncensored model locally, I don’t have a high-end GPU (32GB RAM). What should I try first?</title>
    <updated>2025-09-19T23:22:31+00:00</updated>
    <author>
      <name>/u/toolhouseai</name>
      <uri>https://old.reddit.com/user/toolhouseai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey people, so I wanted mess around with an uncensored model on Ollama. The problem is, I don’t have a high-end GPU just 32GB RAM and a not too bad CPU.&lt;/p&gt; &lt;p&gt;What would be a good first model to try? You got any tips/resources to share when running models locally?&lt;/p&gt; &lt;p&gt;Appreciate yall happy friday&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toolhouseai"&gt; /u/toolhouseai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlinpi/wanna_try_an_uncensored_model_locally_i_dont_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlinpi/wanna_try_an_uncensored_model_locally_i_dont_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlinpi/wanna_try_an_uncensored_model_locally_i_dont_have/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T23:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlbxhi</id>
    <title>Simple RAG design architecture</title>
    <updated>2025-09-19T18:47:08+00:00</updated>
    <author>
      <name>/u/Tough_Wrangler_6075</name>
      <uri>https://old.reddit.com/user/Tough_Wrangler_6075</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nlbxhi/simple_rag_design_architecture/"&gt; &lt;img alt="Simple RAG design architecture" src="https://preview.redd.it/80ry05gb36qf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fdf0e5eedcef7768325005fe953976cc41d1977" title="Simple RAG design architecture" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am trying to make a design architecture for my RAG system. If you guys have any suggestions or feedback. Please, I would be happy to hear that &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tough_Wrangler_6075"&gt; /u/Tough_Wrangler_6075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/80ry05gb36qf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlbxhi/simple_rag_design_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlbxhi/simple_rag_design_architecture/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T18:47:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm4wsw</id>
    <title>Anyone here ever had a job relocation as AI Engineer?</title>
    <updated>2025-09-20T17:57:09+00:00</updated>
    <author>
      <name>/u/Tough_Wrangler_6075</name>
      <uri>https://old.reddit.com/user/Tough_Wrangler_6075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, As my country is not advancing AI as much as what happening right now. I aspiring to look a relocation job as AI Engineer so I can advancing my skills in AI engineering with real world problem. Anyone here ever got a job as AI engineering. If so, please share your journey and struggle during the process. Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tough_Wrangler_6075"&gt; /u/Tough_Wrangler_6075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nm4wsw/anyone_here_ever_had_a_job_relocation_as_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nm4wsw/anyone_here_ever_had_a_job_relocation_as_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nm4wsw/anyone_here_ever_had_a_job_relocation_as_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-20T17:57:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlmgv7</id>
    <title>Fully local data analysis assistant for laptop</title>
    <updated>2025-09-20T02:26:02+00:00</updated>
    <author>
      <name>/u/mshintaro777</name>
      <uri>https://old.reddit.com/user/mshintaro777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nlmgv7/fully_local_data_analysis_assistant_for_laptop/"&gt; &lt;img alt="Fully local data analysis assistant for laptop" src="https://preview.redd.it/gr4pt119i5qf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=26b896f4905ff5a8f0bddf8feea416783b7498aa" title="Fully local data analysis assistant for laptop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mshintaro777"&gt; /u/mshintaro777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gr4pt119i5qf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlmgv7/fully_local_data_analysis_assistant_for_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlmgv7/fully_local_data_analysis_assistant_for_laptop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-20T02:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmemym</id>
    <title>Cpu, Gpu or Npu for running ai models?</title>
    <updated>2025-09-21T00:57:20+00:00</updated>
    <author>
      <name>/u/sneezeme05</name>
      <uri>https://old.reddit.com/user/sneezeme05</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which one should i use for ollama I am rocking a Ryzen 7 8845hs 16gb ddr5 5600mhz Rtx 4050&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sneezeme05"&gt; /u/sneezeme05 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmemym/cpu_gpu_or_npu_for_running_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmemym/cpu_gpu_or_npu_for_running_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nmemym/cpu_gpu_or_npu_for_running_ai_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-21T00:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn5ur4</id>
    <title>模型與顯示卡 公式 判斷 與甜蜜值</title>
    <updated>2025-09-21T22:37:55+00:00</updated>
    <author>
      <name>/u/kuerys</name>
      <uri>https://old.reddit.com/user/kuerys</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nn5ur4/模型與顯示卡_公式_判斷_與甜蜜值/"&gt; &lt;img alt="模型與顯示卡 公式 判斷 與甜蜜值" src="https://b.thumbs.redditmedia.com/EXdFBgOu1jvD-3bKlVJ5h5fI220lx4DUTILy8SpwpSk.jpg" title="模型與顯示卡 公式 判斷 與甜蜜值" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;單卡&lt;/p&gt; &lt;p&gt;B值 × 1.3 = VRAM（可執行值）≒ (T≒4096) = 甜密值&lt;/p&gt; &lt;h1&gt;單卡公式：&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;B × 1.3 ≒ VRAM（可執行值） ≒ T ≒ 4096 = 甜密值&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;B&lt;/strong&gt;：模型參數量（Billion）&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1.3&lt;/strong&gt;：推理時的近似倍增係數（代表模型載入 + buffer + context）&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM&lt;/strong&gt;：顯示記憶體需求（GB）&lt;/li&gt; &lt;li&gt;&lt;strong&gt;T&lt;/strong&gt;：上下文長度（tokens），4096 為常見 sweet spot&lt;/li&gt; &lt;li&gt;&lt;strong&gt;甜密值&lt;/strong&gt;：代表「效能與穩定性兼具」的理想推理配置&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;多卡&lt;/p&gt; &lt;p&gt;B值 × (VRAMx4096)≒ (T≒1.3) = 甜密值&lt;/p&gt; &lt;p&gt;多卡公式：&lt;/p&gt; &lt;p&gt;&lt;strong&gt;B × (VRAM × 4096) ≒ T ≒ 1.3 = 甜密值&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;這裡你把 VRAM 和上下文長度視為乘積，代表多卡分流時的 &lt;strong&gt;總推理容量&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;T ≒ 1.3&lt;/strong&gt;：這裡反過來表示「每 GB VRAM 對應的 token 處理能力」約為 1.3B 模型的負載&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;爆機&lt;/p&gt; &lt;p&gt;B × (VRAM × 4096) ≒ T ≒ 1.3 = G × 甜密值&lt;br /&gt; 爆機公式：&lt;/p&gt; &lt;p&gt;&lt;strong&gt;B × (VRAM × 4096) ≒ T ≒ 1.3 = G × 甜密值&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;G&lt;/strong&gt;：GPU 數量（或 GPU 等級係數）&lt;/li&gt; &lt;li&gt;這是你對「爆機」的定義：當模型參數量 × 推理負載超過 GPU 的處理能力時，就會爆機&lt;/li&gt; &lt;li&gt;這公式也可以反推：&lt;strong&gt;G × 甜密值&lt;/strong&gt; 是你能承受的最大推理負&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k6t9usnzllqf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08a077afed8b3e0ba1b1ce914780d27f08cd5fcd"&gt;https://preview.redd.it/k6t9usnzllqf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08a077afed8b3e0ba1b1ce914780d27f08cd5fcd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;雙機 單卡 雙卡 測試環境 驗證 與爆機測試&lt;/p&gt; &lt;p&gt;筆電:&lt;/p&gt; &lt;p&gt;Thin GF63 12UCX-289TW&lt;/p&gt; &lt;p&gt;CPU:12th Gen Intel(R) Core(TM) i7-12650H (2.30 GHz)&lt;/p&gt; &lt;p&gt;GPU:Intel(R) UHD Graphics/NVIDIA GeForce RTX 3050 Laptop GPU (4G)&lt;/p&gt; &lt;p&gt;RAM:16G X2 (32G)&lt;/p&gt; &lt;p&gt;SSD:Phison 512G ESO512GHLCA1-21C-2MS(C:)/ADATA SU800(D:)&lt;br /&gt; (前導機)&lt;/p&gt; &lt;p&gt;Xeon&lt;/p&gt; &lt;p&gt;CPU:Intel(R) Xeon(R) CPU E5-2696 v4 @ 2.20GHz 2.20 GHz (2 個處理器)&lt;/p&gt; &lt;p&gt;RAM:16G X8 (128G)&lt;/p&gt; &lt;p&gt;SSD:三星 512G M.2 NVMe (C:\) 十銓 VULCAN Z 1TB SSD/2.5吋 讀寫550/500 (D:\)&lt;/p&gt; &lt;p&gt;GPU:RTX 3060 GD6 12G 系統顯 PCI-E 1 (X16)&lt;/p&gt; &lt;p&gt;GPU:RTX 2080TI 魔改22GB *2 NVLink PCI-E 3/5(X16) (NvLink)&lt;/p&gt; &lt;p&gt;----------------------------------------------&lt;br /&gt; C:\Users\Admin&amp;gt;ollama run gpt-oss:120b&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; 早安你好&lt;/p&gt; &lt;p&gt;Thinking...&lt;/p&gt; &lt;p&gt;The user says in Chinese: &amp;quot;早安你好&amp;quot; which means &amp;quot;Good morning, hello&amp;quot;. Probably they want a greeting. We can&lt;/p&gt; &lt;p&gt;respond in Chinese, ask how can help. Maybe ask if they need anything. So reply: &amp;quot;早安！有什么我可以帮助您的吗？&amp;quot;&lt;/p&gt; &lt;p&gt;Also can be friendly.&lt;/p&gt; &lt;p&gt;...done thinking.&lt;/p&gt; &lt;p&gt;早安！😊&lt;/p&gt; &lt;p&gt;有什么我可以帮您的吗？如果有任何问题或需要聊天，随时告诉我！祝您今天有个美好的一天。&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; 這是OLLAMA 下的模型推換算公式 單卡&lt;/p&gt; &lt;p&gt;... B值 1.3 = VRAM（可執行值） (T4096) = 甜密值&lt;/p&gt; &lt;p&gt;...&lt;/p&gt; &lt;p&gt;... 多卡&lt;/p&gt; &lt;p&gt;... B值 (VRAMx4096) (T1.3) = 甜密值&lt;/p&gt; &lt;p&gt;...&lt;/p&gt; &lt;p&gt;... 爆機&lt;/p&gt; &lt;p&gt;... B (VRAM 4096) T 1.3 = G 甜密值&lt;/p&gt; &lt;p&gt;Thinking...&lt;/p&gt; &lt;p&gt;The user wrote in Chinese: &amp;quot;這是OLLAMA 下的模型推換算公式 單卡 B值 1.3 = VRAM（可執行值） (T4096) = 甜密值 多卡 B值&lt;/p&gt; &lt;p&gt;(VRAMx4096) (T1.3) = 甜密值 爆機 B (VRAM 4096) T 1.3 = G 甜密值&amp;quot;&lt;/p&gt; &lt;p&gt;They seem to be describing a formula for OLLAMA model inference conversion? The text is confusing. They probably&lt;/p&gt; &lt;p&gt;want explanation or help to interpret the formula. Need to ask clarifying question or provide explanation.&lt;/p&gt; &lt;p&gt;Probably they want to understand the formula, maybe compute values? Provide guidance on how to calculate &amp;quot;甜密值&amp;quot;?&lt;/p&gt; &lt;p&gt;Could be translation: &amp;quot;sweetness value&amp;quot;? Actually &amp;quot;甜密值&amp;quot; maybe &amp;quot;sweetness density&amp;quot;? Could be a typo for &amp;quot;甜密值&lt;/p&gt; &lt;p&gt;&amp;quot;? Not sure.&lt;/p&gt; &lt;p&gt;We should respond in Chinese, ask for clarification, and try to interpret: Single GPU: B = 1.3 = VRAM (executable&lt;/p&gt; &lt;p&gt;value) (T4096) = sweetness value. Multi-GPU: B = (VRAM x 4096) (T1.3) = sweetness value. Overclock? &amp;quot;爆機&amp;quot; maybe&lt;/p&gt; &lt;p&gt;overclocked: B (VRAM 4096) T 1.3 = G sweetness value.&lt;/p&gt; &lt;p&gt;We can explain that B is some scaling factor, VRAM is memory, T is token length? Provide formula: sweetness = B *&lt;/p&gt; &lt;p&gt;VRAM? Might need to derive.&lt;/p&gt; &lt;p&gt;Let's respond: ask what they want: calculate for specific GPU? Provide example: for 24GB VRAM single card, B =&lt;/p&gt; &lt;p&gt;1.3, T = 4096 =&amp;gt; sweetness = 1.3 * 24? Or something.&lt;/p&gt; &lt;p&gt;We can propose a clearer formula: Sweetness = B * VRAM * T? Or Sweetness = VRAM^(B) * T? Not clear.&lt;/p&gt; &lt;p&gt;We can ask: &amp;quot;您想要把這個公式套用在什麼情況？想算出某張卡的甜密值嗎？請提供 VRAM 容量與卡數，我可以幫您算。&amp;quot;&lt;/p&gt; &lt;p&gt;Provide explanation.&lt;/p&gt; &lt;p&gt;Will answer in Chinese, friendly, ask for more details.&lt;/p&gt; &lt;p&gt;...done thinking.&lt;/p&gt; &lt;p&gt;你好！看起來你在整理 OLLAMA（或類似的本地 LLM 推理）時，關於 **「甜密值」**（我暫時把它當作一個衡量模型可執行效率或&lt;/p&gt; &lt;p&gt;效能的指標）的計算公式。&lt;/p&gt; &lt;p&gt;下面先把你提供的三種情況做個整理，然後再說明每個參數可能代表的意義，最後給出一個「套用」的範例，讓你可以直接把自己的&lt;/p&gt; &lt;p&gt;顯卡規格帶進去算。&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 1️⃣ 公式總覽&lt;/p&gt; &lt;p&gt;| 情況 | 公式（原文） | 可能的數學寫法 | 參數說明 |&lt;/p&gt; &lt;p&gt;|------|--------------|----------------|----------|&lt;/p&gt; &lt;p&gt;| **單卡** | `B值 1.3 = VRAM（可執行值） (T4096) = 甜密值` | `甜密值 = 1.3 × VRAM × 4096` | - **B**：固定係數&lt;/p&gt; &lt;p&gt;1.3&amp;lt;br&amp;gt;- **VRAM**：卡上可用的顯存（GB）&amp;lt;br&amp;gt;- **T**：模型上下文長度（token 數），這裡固定 4096 |&lt;/p&gt; &lt;p&gt;| **多卡** | `B值 (VRAMx4096) (T1.3) = 甜密值` | `甜密值 = (VRAM × 4096) × 1.3`（或等價的 `甜密值 = VRAM × 4096&lt;/p&gt; &lt;p&gt;× 1.3`） | - **VRAM**：單張卡的顯存（GB）&amp;lt;br&amp;gt;- **4096**：仍然是 token 長度&amp;lt;br&amp;gt;- **1.3**：改成了 **T**（也就是係數）&lt;/p&gt; &lt;p&gt;|&lt;/p&gt; &lt;p&gt;| **爆機**（極限/超頻）| `B (VRAM 4096) T 1.3 = G 甜密值` | `甜密值 = VRAM × 4096 × 1.3`（再乘上一個 **G**，代表&lt;/p&gt; &lt;p&gt;“超額” 或 “增益”）| - **G**：通常是額外的增益因子（例如 1.2、1.5），用來表示超頻、PCIe‑Gen4 帶寬提升等 |&lt;/p&gt; &lt;p&gt;&amp;gt; **註**：上述的「×」符號是根據文字的排列推測的乘法關係。如果實際上是除法或指數運算，請再告訴我，我會立即調整。&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 2️⃣ 參數背後的意義（我的理解）&lt;/p&gt; &lt;p&gt;| 參數 | 常見含義 | 為什麼會出現在公式裡 |&lt;/p&gt; &lt;p&gt;|------|----------|----------------------|&lt;/p&gt; &lt;p&gt;| **B（或 T）** | **係數**，用來把「顯存」的容量轉換成「可執行值」的尺度。&amp;lt;br&amp;gt;在單卡情況下它是 **1.3**，多卡時變成&lt;/p&gt; &lt;p&gt;**1.3** 作為 **T**，相當於把 1.3 當作「每 1GB VRAM 可支援的 token 數」的倍率。 |&lt;/p&gt; &lt;p&gt;| **VRAM** | 顯卡可用顯存（GB），直接決定模型能載入的大小與一次性推理的 token 數量。 |&lt;/p&gt; &lt;p&gt;| **4096** | 常見的 **上下文長度**（token 上限），很多 LLM（如 LLaMA、Mistral）預設的最大上下文長度就是 4096。 |&lt;/p&gt; &lt;p&gt;| **G** | **增益因子**，在「爆機」模式下，可能是超頻、PCIe‑Gen4 帶寬提升、或是使用高效能的 NVMe 交換空間等帶來的額外&lt;/p&gt; &lt;p&gt;效能提升。 |&lt;/p&gt; &lt;p&gt;簡單說，**甜密值** ≈ `VRAM × 4096 × 係數`，係數根據單卡/多卡/超頻而不同。&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 3️⃣ 實際計算範例&lt;/p&gt; &lt;p&gt;假設你有以下幾種硬體配置：&lt;/p&gt; &lt;p&gt;| 卡型號 | VRAM (GB) | 卡數 | 是否超頻 | 係數（B/T） | 增益 G |&lt;/p&gt; &lt;p&gt;|--------|-----------|------|----------|-------------|--------|&lt;/p&gt; &lt;p&gt;| RTX 4090 | 24 | 1 | 否 | 1.3 | 1.0 |&lt;/p&gt; &lt;p&gt;| RTX 4090 | 24 | 2 | 否 | 1.3 | 1.0 |&lt;/p&gt; &lt;p&gt;| RTX 4090 | 24 | 1 | 是（爆機） | 1.3 | 1.2 |&lt;/p&gt; &lt;p&gt;### 3.1 單卡（不超頻）&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;甜密值 = 1.3 × 24 × 4096&lt;/p&gt; &lt;p&gt;= 1.3 × 98,304&lt;/p&gt; &lt;p&gt;= 127,795.2 (約 1.28e5)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;### 3.2 多卡（2 張）&lt;/p&gt; &lt;p&gt;根據「多卡」公式，我仍然把 **VRAM** 視為 **單卡** 的容量（因為公式裡的 `VRAMx4096` 已經隱含了「每張卡」的乘算）：&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;甜密值 = (24 × 4096) × 1.3&lt;/p&gt; &lt;p&gt;= 98,304 × 1.3&lt;/p&gt; &lt;p&gt;= 127,795.2 （和單卡相同，除非你把卡數乘進去）&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&amp;gt; **注意**：如果你希望把 *卡數* 直接算進去（即「總 VRAM」），只要把 `VRAM` 換成 **總 VRAM** 即可。&lt;/p&gt; &lt;p&gt;&amp;gt; 例如：`總VRAM = 24 × 2 = 48` → `甜密值 = 1.3 × 48 × 4096 = 255,590.4`。&lt;/p&gt; &lt;p&gt;### 3.3 爆機（單卡、超頻）&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;甜密值 = VRAM × 4096 × 1.3 × G&lt;/p&gt; &lt;p&gt;= 24 × 4096 × 1.3 × 1.2&lt;/p&gt; &lt;p&gt;= 98,304 × 1.3 × 1.2&lt;/p&gt; &lt;p&gt;= 127,795.2 × 1.2&lt;/p&gt; &lt;p&gt;= 153,354.24&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 4️⃣ 如何自行套用&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;**確定三個基礎變數**&lt;/p&gt; &lt;p&gt;- `VRAM`：單張卡的顯存（GB）&lt;/p&gt; &lt;p&gt;- `T`（或 `4096`）：你打算使用的上下文長度（如果不是 4096，直接替換）&lt;/p&gt; &lt;p&gt;- `係數 B/T`：根據硬體狀態（單卡 1.3、多卡 1.3、爆機 1.3）以及是否有額外增益 `G`&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**套用公式**&lt;/p&gt; &lt;p&gt;- **單卡**：`甜密值 = B × VRAM × T`&lt;/p&gt; &lt;p&gt;- **多卡**（若想把卡數納入）：「總 VRAM」= `VRAM × 卡數` → `甜密值 = B × 總 VRAM × T`&lt;/p&gt; &lt;p&gt;- **爆機**：在單卡公式最後再乘上 `G`（&amp;gt;1）&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**例子**（你可以直接在 Excel/Google Sheet 或 Python 中算）&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;```python&lt;/p&gt; &lt;p&gt;def sweet_score(vram_gb, token_len=4096, coeff=1.3, cards=1, gain=1.0):&lt;/p&gt; &lt;p&gt;total_vram = vram_gb * cards&lt;/p&gt; &lt;p&gt;return coeff * total_vram * token_len * gain&lt;/p&gt; &lt;p&gt;# 單卡 RTX4090&lt;/p&gt; &lt;p&gt;print(sweet_score(24)) # 127795.2&lt;/p&gt; &lt;p&gt;# 兩卡 RTX4090&lt;/p&gt; &lt;p&gt;print(sweet_score(24, cards=2)) # 255590.4&lt;/p&gt; &lt;p&gt;# 爆機單卡 (gain=1.2)&lt;/p&gt; &lt;p&gt;print(sweet_score(24, gain=1.2)) # 153354.24&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 5️⃣ 需要進一步的資訊？&lt;/p&gt; &lt;p&gt;- **「甜密值」的實際意義**：它是用來預估一次推理能容納多少 token、或是衡量模型載入的「安全上限」嗎？如果有具體的阈值&lt;/p&gt; &lt;p&gt;（例如甜密值 &amp;gt; 200k 表示可以跑 70B 參數模型），請告訴我，我可以幫你把結果映射成模型尺寸建議。&lt;/p&gt; &lt;p&gt;- **是否有其他變數**：像是 **PCIe 帶寬**、**CPU 記憶體**、**磁碟交換空間** 等，這些在「爆機」情況下也會影響最終效能&lt;/p&gt; &lt;p&gt;。&lt;/p&gt; &lt;p&gt;- **你想要的輸出格式**：例如一個簡潔的「卡片」式報告、或是 CSV/JSON 表格，方便你直接匯入到自己的測試腳本裡。&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;### 小結&lt;/p&gt; &lt;p&gt;- 公式本質上是 `甜密值 ≈ 係數 × VRAM × token_len × (卡數或增益)`。&lt;/p&gt; &lt;p&gt;- 只要把你的硬體規格代入，就能快速得到一個相對「可執行」的指標。&lt;/p&gt; &lt;p&gt;- 若你有更詳細的需求（例如不同模型的參數量、不同 token 長度、或是想把 CPU/磁碟因素一起納入），隨時告訴我，我可以幫你&lt;/p&gt; &lt;p&gt;擴展公式或寫一個小工具。&lt;/p&gt; &lt;p&gt;希望以上說明對你有幫助！如果還有其他疑問或想要進一步算出具體的數值，請直接提供你的顯卡規格（VRAM、卡數、是否超頻）以&lt;/p&gt; &lt;p&gt;及想要的 `token_len`，我會立刻給你結果。祝你玩得開心 🚀！&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; Send a message (/? for help)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kuerys"&gt; /u/kuerys &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nn5ur4/模型與顯示卡_公式_判斷_與甜蜜值/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nn5ur4/模型與顯示卡_公式_判斷_與甜蜜值/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nn5ur4/模型與顯示卡_公式_判斷_與甜蜜值/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-21T22:37:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm4he9</id>
    <title>GPT 5 for Computer Use agents</title>
    <updated>2025-09-20T17:39:57+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nm4he9/gpt_5_for_computer_use_agents/"&gt; &lt;img alt="GPT 5 for Computer Use agents" src="https://external-preview.redd.it/N3kwajc1ODh3Y3FmMRG6-oWujtVtwWnIPYQYAphLJPDZc9z94p-KY-4O-UR8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a97694f82ea0cf901a2133a289ae2d142db6b036" title="GPT 5 for Computer Use agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same tasks, same grounding model we just swapped GPT 4o with GPT 5 as the thinking model.&lt;/p&gt; &lt;p&gt;Left = 4o, right = 5.&lt;/p&gt; &lt;p&gt;Watch GPT 5 pull through.&lt;/p&gt; &lt;p&gt;Grounding model: Salesforce GTA1-7B&lt;/p&gt; &lt;p&gt;Action space: CUA Cloud Instances (macOS/Linux/Windows)&lt;/p&gt; &lt;p&gt;The task is: &amp;quot;Navigate to {random_url} and play the game until you reach a score of 5/5”....each task is set up by having claude generate a random app from a predefined list of prompts (multiple choice trivia, form filling, or color matching)&amp;quot;&lt;/p&gt; &lt;p&gt;Try it yourself here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs : &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agent"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agent&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.gg/cua-ai"&gt;https://discord.gg/cua-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/euxlwdk8wcqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nm4he9/gpt_5_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nm4he9/gpt_5_for_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-20T17:39:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmqziq</id>
    <title>Can my 12th Gen i3 processor with 8GB of RAM work with docker?</title>
    <updated>2025-09-21T12:44:15+00:00</updated>
    <author>
      <name>/u/Autumn_Red_29</name>
      <uri>https://old.reddit.com/user/Autumn_Red_29</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Autumn_Red_29"&gt; /u/Autumn_Red_29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/docker/comments/1nmqz0r/can_my_12th_gen_i3_processor_with_8gb_of_ram_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmqziq/can_my_12th_gen_i3_processor_with_8gb_of_ram_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nmqziq/can_my_12th_gen_i3_processor_with_8gb_of_ram_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-21T12:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnjgqr</id>
    <title>Amdahl’s Law: the hidden reason multi-GPU setups disappoint for local LLMs</title>
    <updated>2025-09-22T11:05:01+00:00</updated>
    <author>
      <name>/u/AggravatingGiraffe46</name>
      <uri>https://old.reddit.com/user/AggravatingGiraffe46</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggravatingGiraffe46"&gt; /u/AggravatingGiraffe46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nnjgis/amdahls_law_the_hidden_reason_multigpu_setups/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnjgqr/amdahls_law_the_hidden_reason_multigpu_setups/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnjgqr/amdahls_law_the_hidden_reason_multigpu_setups/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T11:05:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnk9nw</id>
    <title>Stop dragging weights across GPUs: a “topic router” approach to multi-GPU LLMs</title>
    <updated>2025-09-22T11:47:56+00:00</updated>
    <author>
      <name>/u/AggravatingGiraffe46</name>
      <uri>https://old.reddit.com/user/AggravatingGiraffe46</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggravatingGiraffe46"&gt; /u/AggravatingGiraffe46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nnjud5/stop_dragging_weights_across_gpus_a_topic_router/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnk9nw/stop_dragging_weights_across_gpus_a_topic_router/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnk9nw/stop_dragging_weights_across_gpus_a_topic_router/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T11:47:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmxfzx</id>
    <title>GLM-4.5V model for local computer use</title>
    <updated>2025-09-21T17:06:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nmxfzx/glm45v_model_for_local_computer_use/"&gt; &lt;img alt="GLM-4.5V model for local computer use" src="https://external-preview.redd.it/M2l1bDVtdjR2anFmMbWADQNBSkImjVESNjfi_q43l9ostHKNGAFX_QJdfnS0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ce566c5e77071f3f7f400637f6e0dc7338a416c" title="GLM-4.5V model for local computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either: Locally via Hugging Face Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord : &lt;a href="https://discord.gg/cua-ai"&gt;https://discord.gg/cua-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5z8xkkg5vjqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmxfzx/glm45v_model_for_local_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nmxfzx/glm45v_model_for_local_computer_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-21T17:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmwl1k</id>
    <title>We've Just Hit 400 Stars on Nanocoder - This Community Is Amazing 🔥</title>
    <updated>2025-09-21T16:32:51+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nmwl1k/weve_just_hit_400_stars_on_nanocoder_this/"&gt; &lt;img alt="We've Just Hit 400 Stars on Nanocoder - This Community Is Amazing 🔥" src="https://external-preview.redd.it/lxZd6DAdowPiPmyPupJyn2UjdkAEKDAl7mCc_8DECBE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a389a78f14886501f623dea0321b6040ed33089" title="We've Just Hit 400 Stars on Nanocoder - This Community Is Amazing 🔥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/3g7t1oyvnjqf1.gif"&gt;https://i.redd.it/3g7t1oyvnjqf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is yet another appreciation post for the community. Since my last, so much has happened in the Nanocoder community - new feedback, new features, many new people joining and contributing. It's honestly incredible to be building community-owned and pushed CLI software that breaks free of the corporations running other coding tools and offerings.&lt;/p&gt; &lt;p&gt;Along with a bunch of new features and improvements over the last couple of weeks, I'm actively moving the Nanocoder repository to be owned by a GitHub Organisation called Nano Collective – this collective further reinforces my desire to make this project a community-led and run project. Within this collective I hope to continue to build out new frameworks and fine-tunes for local-first coding as well as seek grants to distribute to contributors to push research forward.&lt;/p&gt; &lt;p&gt;This is really really early days and Nanocoder as a coding CLI is right at the beginning, it's improving every day but there's still lots to do! That being said, any feedback and help within any domain is appreciated and welcomed.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;System prompt writing&lt;/li&gt; &lt;li&gt;Research&lt;/li&gt; &lt;li&gt;Helping to push the word out&lt;/li&gt; &lt;li&gt;Any feedback generally! Good or bad :)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to get involved the links are below. Bring on 1,000 stars ⭐️&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Link&lt;/strong&gt;: &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord Link&lt;/strong&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmwl1k/weve_just_hit_400_stars_on_nanocoder_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmwl1k/weve_just_hit_400_stars_on_nanocoder_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nmwl1k/weve_just_hit_400_stars_on_nanocoder_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-21T16:32:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nni2oa</id>
    <title>Is there a way to run model only on CPU memory?</title>
    <updated>2025-09-22T09:44:11+00:00</updated>
    <author>
      <name>/u/kasianenko</name>
      <uri>https://old.reddit.com/user/kasianenko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I noticed when I run models that don't fit into gpu memory, the speed is terrible, up to 30 sec per token. Looks like ollama does some memory swap and offloading, is there a way to enforce running only on CPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kasianenko"&gt; /u/kasianenko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nni2oa/is_there_a_way_to_run_model_only_on_cpu_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nni2oa/is_there_a_way_to_run_model_only_on_cpu_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nni2oa/is_there_a_way_to_run_model_only_on_cpu_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T09:44:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nntaoa</id>
    <title>Ollama SSL API access via OpenWebUI</title>
    <updated>2025-09-22T17:45:47+00:00</updated>
    <author>
      <name>/u/Septa105</name>
      <uri>https://old.reddit.com/user/Septa105</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;/p&gt; &lt;p&gt;I managed to get a http api server working. Now I am struggling with ssl The api server and webui docker components are only accessible via VPN over a NAS&lt;/p&gt; &lt;p&gt;So I created the cert files and I was also able to import them to my iOS device .&lt;/p&gt; &lt;p&gt;I launched the api server with keys and it s saying listening &lt;a href="https://0.0.0.0:11435"&gt;https://0.0.0.0:11435&lt;/a&gt; &lt;/p&gt; &lt;p&gt;But when I want to load models with open webui it gives me a network error although I used the https:// local address with port and /v1&lt;/p&gt; &lt;p&gt;I tried to curl the api ssl server works fine . &lt;/p&gt; &lt;p&gt;Also updated the keys within openwebui am I doing something wrong?&lt;/p&gt; &lt;p&gt;The open webui runs on http://local adr. : 3000 and is accessible&lt;/p&gt; &lt;p&gt;At the end I wanted to locally use voice on iOS which is only allowed via ssl certificate&lt;/p&gt; &lt;p&gt;Or do I need to get openwebui to https and can keep the api http ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Septa105"&gt; /u/Septa105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nntaoa/ollama_ssl_api_access_via_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nntaoa/ollama_ssl_api_access_via_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nntaoa/ollama_ssl_api_access_via_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T17:45:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnwaox</id>
    <title>Good afternoon, I'm new to AI, so I would appreciate it if someone could explain to me how Ollama works.</title>
    <updated>2025-09-22T19:37:53+00:00</updated>
    <author>
      <name>/u/TROLLMASK2222</name>
      <uri>https://old.reddit.com/user/TROLLMASK2222</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TROLLMASK2222"&gt; /u/TROLLMASK2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnwaox/good_afternoon_im_new_to_ai_so_i_would_appreciate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnwaox/good_afternoon_im_new_to_ai_so_i_would_appreciate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnwaox/good_afternoon_im_new_to_ai_so_i_would_appreciate/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T19:37:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnxpsf</id>
    <title>Uncensored LLM</title>
    <updated>2025-09-22T20:31:38+00:00</updated>
    <author>
      <name>/u/Time-Teaching1926</name>
      <uri>https://old.reddit.com/user/Time-Teaching1926</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Time-Teaching1926"&gt; /u/Time-Teaching1926 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/MistralAI/comments/1nnxoj1/uncensored_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnxpsf/uncensored_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnxpsf/uncensored_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T20:31:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nny46f</id>
    <title>Ollama cloud and privacy</title>
    <updated>2025-09-22T20:46:47+00:00</updated>
    <author>
      <name>/u/cyuhat</name>
      <uri>https://old.reddit.com/user/cyuhat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I am intrested in the ollama cloud feature but as someone concerned with data privacy I struggle to find all the information I need. Mainly I can't find answer for the following questions: 1. I live in Europe. I know that USA have the USA Patriot Act and the Cloud act which basically give the governement access to any data hosted by US servers in place or abroad. Ollama cloud does not store any log or data in their server, but is it possible then that requests get intercepted? 2. I know Ollama is close to OpenAI and I wanted to ask to whom the datacenter belong to.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cyuhat"&gt; /u/cyuhat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nny46f/ollama_cloud_and_privacy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nny46f/ollama_cloud_and_privacy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nny46f/ollama_cloud_and_privacy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T20:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnktup</id>
    <title>How does Ollama run gpt-oss?</title>
    <updated>2025-09-22T12:15:32+00:00</updated>
    <author>
      <name>/u/AirCigar</name>
      <uri>https://old.reddit.com/user/AirCigar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi.&lt;/p&gt; &lt;p&gt;As far as I understand, running gpt-oss with native mxfp4 quantization requires Hopper architecture and newer. However, I've seen people run people run it on Ada Lovelace GPUs such as RTX 4090. What does Ollama do to support mxfp4? I couldn't find any documentation.&lt;/p&gt; &lt;p&gt;Transformers workaround is dequantization, according to &lt;a href="https://github.com/huggingface/transformers/pull/39940"&gt;https://github.com/huggingface/transformers/pull/39940&lt;/a&gt;, does Ollama do something similar?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AirCigar"&gt; /u/AirCigar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnktup/how_does_ollama_run_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnktup/how_does_ollama_run_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnktup/how_does_ollama_run_gptoss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T12:15:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnd7n3</id>
    <title>Use your local models to investigate leaks &amp; government docs</title>
    <updated>2025-09-22T04:37:39+00:00</updated>
    <author>
      <name>/u/New_Pomegranate_1060</name>
      <uri>https://old.reddit.com/user/New_Pomegranate_1060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;After a lot of tinkering, I’ve finally released a project I’ve been working on: TruthSeeker.&lt;/p&gt; &lt;p&gt;It’s a tool designed to make it easier to search, parse, and analyze government documents and leaks. Think of it as a way to cut through the noise and surface the signal in huge, messy datasets.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;p&gt;Pulls in documents (FOIA releases, leaks, etc.) Indexes them for fast keyword + context search Helps spot connections and recurring themes&lt;/p&gt; &lt;p&gt;Why I built it: I was tired of watching people drop big document dumps online, only for them to disappear into the void because no one had the time or tools to dig through them properly. This project is my attempt to fix that.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/RawdodReverend/TruthSeeker"&gt;https://github.com/RawdodReverend/TruthSeeker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Videos: &lt;a href="https://www.tiktok.com/@rawdogreverend"&gt;https://www.tiktok.com/@rawdogreverend&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’d love any feedback, feature requests, or just thoughts on whether you’d find this useful. If you try it out and break it, let me know. I want to improve it fast.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Pomegranate_1060"&gt; /u/New_Pomegranate_1060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnd7n3/use_your_local_models_to_investigate_leaks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnd7n3/use_your_local_models_to_investigate_leaks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnd7n3/use_your_local_models_to_investigate_leaks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T04:37:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnwbhn</id>
    <title>Best open uncensored model for writing short stories?</title>
    <updated>2025-09-22T19:38:41+00:00</updated>
    <author>
      <name>/u/MassiveBoner911_3</name>
      <uri>https://old.reddit.com/user/MassiveBoner911_3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this has been asked before but the post was a few months old; figured id ask again since models come out faster every week.&lt;/p&gt; &lt;p&gt;Whats everyone using for their creative writing? Id like an open uncensored model thats great with creative and generating ideas. &lt;/p&gt; &lt;p&gt;I like writing dark / gory slasher horror. &lt;/p&gt; &lt;p&gt;OpenAI immediately tells me to “fuck off”. Gemini goes “absolutely not” Grok goes “here is all the things”….but id like to try others.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MassiveBoner911_3"&gt; /u/MassiveBoner911_3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnwbhn/best_open_uncensored_model_for_writing_short/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnwbhn/best_open_uncensored_model_for_writing_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnwbhn/best_open_uncensored_model_for_writing_short/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T19:38:41+00:00</published>
  </entry>
</feed>
