<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-21T05:49:35+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1p0ijg7</id>
    <title>Local Kaggle Companion Experiment Using Ollama + MCP</title>
    <updated>2025-11-18T17:44:29+00:00</updated>
    <author>
      <name>/u/Impossible_Grand_552</name>
      <uri>https://old.reddit.com/user/Impossible_Grand_552</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1p0ijg7/video/v2t9f8t3y12g1/player"&gt;https://reddit.com/link/1p0ijg7/video/v2t9f8t3y12g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;br /&gt; I originally started this project just to learn MCP, but it ended up becoming something I actually enjoy working on, so I‚Äôm continuing to build it out. I‚Äôm calling it &lt;strong&gt;Pocket Data Scientist&lt;/strong&gt;. It runs entirely locally using Streamlit, Ollama, and the Model Context Protocol, and the idea is to have a conversational interface that can actually &lt;em&gt;use tools&lt;/em&gt; to explore and analyze CSV datasets.&lt;/p&gt; &lt;p&gt;One thing that pushed me forward is a pretty noticeable gap in current LLMs: they‚Äôre great at conversation, but they‚Äôre still inconsistent when it comes to selecting or sequencing the right analysis steps. I wanted to experiment with giving the model a proper toolset and letting MCP drive the interaction. It works with any Ollama model, but I stuck with &lt;strong&gt;Llama3 8B&lt;/strong&gt; because that‚Äôs what my hardware can handle without melting.&lt;/p&gt; &lt;p&gt;The bigger vision is to eventually turn this into something like a Kaggle companion: an assistant that can study a dataset, point out issues, suggest preprocessing, and guide you toward the right modeling approach for a given competition. I‚Äôm not there yet, but that‚Äôs the direction I‚Äôd love to take it.&lt;br /&gt; The full code is here if you want to try it or look around:&lt;br /&gt; &lt;a href="https://github.com/Real4LA/pocket-data-scientist"&gt;&lt;strong&gt;https://github.com/Real4LA/pocket-data-scientist&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd really appreciate any feedback. Let me know what features would make this genuinely useful, what it would need to move toward a real Kaggle-style companion, and which analysis tools you think are missing. I‚Äôm also looking for advice on improving tool-use reliability with Ollama and any tips to reduce inference latency, which is the main bottleneck right now (the vid has cuts BTW).&lt;/p&gt; &lt;p&gt;Thanks for checking it out, and I‚Äôm open to any ideas or critiques. üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impossible_Grand_552"&gt; /u/Impossible_Grand_552 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0ijg7/local_kaggle_companion_experiment_using_ollama_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0ijg7/local_kaggle_companion_experiment_using_ollama_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p0ijg7/local_kaggle_companion_experiment_using_ollama_mcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-18T17:44:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0ust1</id>
    <title>M.I.M.I.R - Multi-agent orchestration - drag and drop UI</title>
    <updated>2025-11-19T01:52:44+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1p0usmc/mimir_multiagent_orchestration_drag_and_drop_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0ust1/mimir_multiagent_orchestration_drag_and_drop_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p0ust1/mimir_multiagent_orchestration_drag_and_drop_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T01:52:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0p1pm</id>
    <title>Web front end for Ollama? Is llama.cpp what I'm looking for?</title>
    <updated>2025-11-18T21:48:36+00:00</updated>
    <author>
      <name>/u/JortsKitty</name>
      <uri>https://old.reddit.com/user/JortsKitty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm confused about the relationship between these two things. Is llama.cpp just a web front end for Ollama? I'd like to use ollama, but would like a UI that's better than just a command line.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JortsKitty"&gt; /u/JortsKitty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0p1pm/web_front_end_for_ollama_is_llamacpp_what_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0p1pm/web_front_end_for_ollama_is_llamacpp_what_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p0p1pm/web_front_end_for_ollama_is_llamacpp_what_im/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-18T21:48:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1fnit</id>
    <title>Google Antigravity and Local Models?</title>
    <updated>2025-11-19T18:31:18+00:00</updated>
    <author>
      <name>/u/valtor2</name>
      <uri>https://old.reddit.com/user/valtor2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know it's early days, but has anyone tried to load local models into Google Antigravity? Would it make sense to use a local version of gpt-oss-120B (In the process of getting a Strix Halo) with it? If possible, that could be a cool tool, particularly with the agent manager...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valtor2"&gt; /u/valtor2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1fnit/google_antigravity_and_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1fnit/google_antigravity_and_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1fnit/google_antigravity_and_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T18:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p110ju</id>
    <title>LLM Benchmark Tool</title>
    <updated>2025-11-19T07:17:56+00:00</updated>
    <author>
      <name>/u/Dry-Bandicoot9512</name>
      <uri>https://old.reddit.com/user/Dry-Bandicoot9512</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p110ju/llm_benchmark_tool/"&gt; &lt;img alt="LLM Benchmark Tool" src="https://b.thumbs.redditmedia.com/-h-QkdwM_g1OryC3KMKkNlsBx5R0FUI-4iZO_UtJOvY.jpg" title="LLM Benchmark Tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/CordatusAI/ollama-benchmark"&gt;https://github.com/CordatusAI/ollama-benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A lightweight, real‚Äëtime Streamlit application for benchmarking large language models (LLMs) that are hosted with &lt;strong&gt;Ollama&lt;/strong&gt;. The tool automatically detects your GPU, filters models by VRAM requirements, pulls missing models, runs a prompt‚Äëbased benchmark, and visualises the results with interactive Plotly charts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bkc9m3wwz52g1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=292a30629f7704806393ee4f499cf60a65a8832a"&gt;https://preview.redd.it/bkc9m3wwz52g1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=292a30629f7704806393ee4f499cf60a65a8832a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry-Bandicoot9512"&gt; /u/Dry-Bandicoot9512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p110ju/llm_benchmark_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p110ju/llm_benchmark_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p110ju/llm_benchmark_tool/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T07:17:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0zwio</id>
    <title>Is there an slm that supports Function calling on slm</title>
    <updated>2025-11-19T06:11:37+00:00</updated>
    <author>
      <name>/u/Due_Ad3126</name>
      <uri>https://old.reddit.com/user/Due_Ad3126</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tired of trying different models. Want to know if there is a model or a finetuned model I can run on pc that does very accurate function calling&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Ad3126"&gt; /u/Due_Ad3126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0zwio/is_there_an_slm_that_supports_function_calling_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0zwio/is_there_an_slm_that_supports_function_calling_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p0zwio/is_there_an_slm_that_supports_function_calling_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T06:11:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1cyqc</id>
    <title>How to disable thinking in Qwen3 VL models?</title>
    <updated>2025-11-19T16:53:48+00:00</updated>
    <author>
      <name>/u/eyueldk</name>
      <uri>https://old.reddit.com/user/eyueldk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to run qwen3-vl:2b on ollama and I want to disable thinking. I've tried all the suggested techniques such as: adding /no_think in prompt and system prompt; running /set nothink in interactive mode; and setting --think=false. Yet the Model ALWAYS thinks long and hard even for a simple &amp;quot;hello.&amp;quot;&lt;/p&gt; &lt;p&gt;Has anyone encountered this and found a solution? I'm stuck at the moment. Thanks.&lt;/p&gt; &lt;p&gt;Note: I looked into a previous &lt;a href="https://www.reddit.com/r/ollama/comments/1kcybew/qwen3_disable_thinking_in_ollama/https://www.reddit.com/r/ollama/comments/1kcybew/qwen3_disable_thinking_in_ollama/"&gt;post&lt;/a&gt; but none of the solutions worked.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eyueldk"&gt; /u/eyueldk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1cyqc/how_to_disable_thinking_in_qwen3_vl_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1cyqc/how_to_disable_thinking_in_qwen3_vl_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1cyqc/how_to_disable_thinking_in_qwen3_vl_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T16:53:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1157m</id>
    <title>Modest but reliably accurate LLM</title>
    <updated>2025-11-19T07:26:02+00:00</updated>
    <author>
      <name>/u/SignificanceFlat1460</name>
      <uri>https://old.reddit.com/user/SignificanceFlat1460</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I want to run LLM on my hardware which is a &lt;em&gt;bit&lt;/em&gt; old. My Laptop is a FX505DU&lt;/p&gt; &lt;p&gt;GTX 1660 Ti 6GB Ryzen 7 3750H 16 GB RAM&lt;/p&gt; &lt;p&gt;OK it's a bit more than just a bit old haha. But I wanted to run an LLM that can accurately answer questions related to my CV when applying for jobs. I know some will recommend readily available solutions like gpt-4/5 or Gemini but I want to do this for my own project to see if I can actually do it. Any help would be great. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SignificanceFlat1460"&gt; /u/SignificanceFlat1460 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1157m/modest_but_reliably_accurate_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1157m/modest_but_reliably_accurate_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1157m/modest_but_reliably_accurate_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T07:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1lngs</id>
    <title>Anyone noticed "Premium requests" within their usage tab? What is this for?</title>
    <updated>2025-11-19T22:14:42+00:00</updated>
    <author>
      <name>/u/Active-Shock-7739</name>
      <uri>https://old.reddit.com/user/Active-Shock-7739</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p1lngs/anyone_noticed_premium_requests_within_their/"&gt; &lt;img alt="Anyone noticed &amp;quot;Premium requests&amp;quot; within their usage tab? What is this for?" src="https://b.thumbs.redditmedia.com/aN3C9QQnWaslpPlrBlAPlTC5LHjVGecdhUCQT6ynxLg.jpg" title="Anyone noticed &amp;quot;Premium requests&amp;quot; within their usage tab? What is this for?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am subscribed to the pro plan and used to see just Hourly and Weekly usage, now i see Premium requests as well, but not sure what it is for.&lt;/p&gt; &lt;p&gt;I tried googling and looking up info in their docs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/is6o4ko7ga2g1.png?width=901&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f8bc61f37e553daf7f6f04042bc1a9974b7d427f"&gt;https://preview.redd.it/is6o4ko7ga2g1.png?width=901&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f8bc61f37e553daf7f6f04042bc1a9974b7d427f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Active-Shock-7739"&gt; /u/Active-Shock-7739 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1lngs/anyone_noticed_premium_requests_within_their/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1lngs/anyone_noticed_premium_requests_within_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1lngs/anyone_noticed_premium_requests_within_their/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T22:14:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1vauu</id>
    <title>Mimir - VSCode plugin - Multi-agent parallel studio, code intelligence, vector db search, chat participant - MIT licensed - can use ollama completely</title>
    <updated>2025-11-20T05:44:02+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p1v9fk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1vauu/mimir_vscode_plugin_multiagent_parallel_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1vauu/mimir_vscode_plugin_multiagent_parallel_studio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T05:44:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1izx2</id>
    <title>Ollama Not Using GPU on RTX 5070 Ti (Blackwell)</title>
    <updated>2025-11-19T20:33:42+00:00</updated>
    <author>
      <name>/u/deparko</name>
      <uri>https://old.reddit.com/user/deparko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt; community,&lt;/p&gt; &lt;p&gt;I'm experiencing an issue where Ollama 0.12.11 fails to use the GPU for local models on my RTX 5070 Ti. The GPU is functional and accessible (nvidia-smi works, other services use GPU successfully), but Ollama immediately falls back to CPU-only mode.&lt;/p&gt; &lt;h1&gt;System Details&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: NVIDIA GeForce RTX 5070 Ti (16GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU Compute Capability&lt;/strong&gt;: 12.0 (Blackwell architecture - very new)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU Driver&lt;/strong&gt;: 580.95.05&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CUDA Runtime&lt;/strong&gt;: 12.2.140&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: Ubuntu 25.04 (Linux 6.14.0-35-generic)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Version&lt;/strong&gt;: 0.12.11 (latest, clean install)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Installation&lt;/strong&gt;: Standalone binary via systemd service&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Symptoms&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;All local models show &lt;code&gt;size_vram: 0 MB&lt;/code&gt; in &lt;code&gt;ollama ps&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Logs show: &lt;code&gt;&amp;quot;discovering available GPUs...&amp;quot;&lt;/code&gt; ‚Üí &lt;code&gt;&amp;quot;inference compute&amp;quot; id=cpu library=cpu&lt;/code&gt; ‚Üí &lt;code&gt;&amp;quot;total vram&amp;quot;=&amp;quot;0 B&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Models run on CPU (slow - ~60+ seconds for simple queries)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No error messages&lt;/strong&gt; - Ollama silently falls back to CPU&lt;/li&gt; &lt;li&gt;GPU is functional: &lt;code&gt;nvidia-smi&lt;/code&gt; works, RAG service uses GPU for embeddings/reranking successfully&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What Worked Before&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;This worked before November 17, 2025.&lt;/strong&gt; Logs from Nov 17 show:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;ggml_cuda_init: found 1 CUDA devices&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Models successfully offloaded to GPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;After a system reboot on Nov 18, GPU detection stopped working.&lt;/p&gt; &lt;h1&gt;What I've Tried&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;‚úÖ Environment variables (&lt;code&gt;OLLAMA_NUM_GPU=1&lt;/code&gt;, &lt;code&gt;CUDA_VISIBLE_DEVICES=0&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;‚úÖ Reinstalled Ollama binary (v0.12.11 from GitHub releases)&lt;/li&gt; &lt;li&gt;‚úÖ Manual CUDA library path configuration (&lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;‚úÖ Symlinks for CUDA libraries&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Clean install&lt;/strong&gt; - complete removal of all Ollama files/configs + fresh install&lt;/li&gt; &lt;li&gt;‚úÖ Minimal configuration (removed all manual overrides, let Ollama auto-discover)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;: All attempts show the same behavior - GPU discovery runs but immediately falls back to CPU within ~13ms.&lt;/p&gt; &lt;h1&gt;Current Configuration&lt;/h1&gt; &lt;p&gt;Minimal systemd override (no manual library paths):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Service] Environment=OLLAMA_MODELS=/mnt/shared/ollama-models/models Environment=CUDA_VISIBLE_DEVICES=0 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Hypothesis&lt;/h1&gt; &lt;p&gt;I suspect &lt;strong&gt;Ollama 0.12.11 doesn't support Compute Capability 12.0 (Blackwell architecture)&lt;/strong&gt; yet. The RTX 5070 Ti is very new hardware, and Ollama's bundled CUDA runners may not include kernels compiled for CC 12.0. When initialization fails, Ollama gracefully falls back to CPU without error messages.&lt;/p&gt; &lt;h1&gt;Questions&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Has anyone else with RTX 50-series GPUs (Blackwell) experienced this?&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Is there a known issue or workaround for CC 12.0 support?&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Are there any debug flags or logs that would show why CUDA initialization fails?&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Should I try rolling back to an older Ollama version that worked before Nov 17?&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Additional Info&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Cloud models work fine (authenticated with Ollama Cloud)&lt;/li&gt; &lt;li&gt;RAG service successfully uses GPU for embeddings/reranking (confirms GPU is functional)&lt;/li&gt; &lt;li&gt;Models tested: &lt;code&gt;qwen3:14b&lt;/code&gt;, &lt;code&gt;llama3.1:8b&lt;/code&gt;, &lt;code&gt;qwen:14b&lt;/code&gt; - all show same behavior&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks in advance for any insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deparko"&gt; /u/deparko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1izx2/ollama_not_using_gpu_on_rtx_5070_ti_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1izx2/ollama_not_using_gpu_on_rtx_5070_ti_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1izx2/ollama_not_using_gpu_on_rtx_5070_ti_blackwell/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T20:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2akip</id>
    <title>Perplexity AI PRO - 1 YEAR at 90% Discount ‚Äì Don‚Äôt Miss Out!</title>
    <updated>2025-11-20T18:18:44+00:00</updated>
    <author>
      <name>/u/Verza-</name>
      <uri>https://old.reddit.com/user/Verza-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p2akip/perplexity_ai_pro_1_year_at_90_discount_dont_miss/"&gt; &lt;img alt="Perplexity AI PRO - 1 YEAR at 90% Discount ‚Äì Don‚Äôt Miss Out!" src="https://preview.redd.it/n3tnktqreg2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64e35a540fb60bb61d4a150741bf99c0cb7d9deb" title="Perplexity AI PRO - 1 YEAR at 90% Discount ‚Äì Don‚Äôt Miss Out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get Perplexity AI PRO (1-Year) ‚Äì at 90% OFF!&lt;/p&gt; &lt;p&gt;Order here: &lt;a href="https://cheapgpts.store/Perplexity"&gt;CHEAPGPT.STORE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Plan: 12 Months&lt;/p&gt; &lt;p&gt;üí≥ Pay with: PayPal or Revolut&lt;/p&gt; &lt;p&gt;Reddit reviews: &lt;a href="https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu"&gt;FEEDBACK POST&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TrustPilot: &lt;a href="https://www.trustpilot.com/review/cheapgpt.store"&gt;TrustPilot FEEDBACK&lt;/a&gt;&lt;br /&gt; Bonus: Apply code PROMO5 for $5 OFF your order! &lt;/p&gt; &lt;p&gt;BONUS!: Enjoy the AI Powered automated web browser. (Presented by Perplexity) included!&lt;/p&gt; &lt;p&gt;Trusted and the cheapest!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Verza-"&gt; /u/Verza- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n3tnktqreg2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2akip/perplexity_ai_pro_1_year_at_90_discount_dont_miss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2akip/perplexity_ai_pro_1_year_at_90_discount_dont_miss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T18:18:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1aurp</id>
    <title>An update to Nanocoder üî•</title>
    <updated>2025-11-19T15:35:49+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p1aurp/an_update_to_nanocoder/"&gt; &lt;img alt="An update to Nanocoder üî•" src="https://b.thumbs.redditmedia.com/o-J_tGC-BVWwO-0BEfEwLQLLmDRKaHlx0Lt83jP6NzI.jpg" title="An update to Nanocoder üî•" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/r4j2v8emc82g1.png?width=1734&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e09117983a322fd00410d747da2c1cff7cdda800"&gt;https://preview.redd.it/r4j2v8emc82g1.png?width=1734&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e09117983a322fd00410d747da2c1cff7cdda800&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;Just a quick update on Nanocoder - the open-source, open-community coding CLI that's built with privacy + local-first in mind. You may have seen posts on here before with updates!&lt;/p&gt; &lt;p&gt;One of the first comments on the last post was about starting a dedicated sub-reddit for those interested enough. We've now created this and will slowly phase to use it as an additional channel to provide updates and interact with the AI community over other sub-reddits.&lt;/p&gt; &lt;p&gt;We can't thank everyone enough though that has engaged so positively with the project on sub-reddits like &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;. It means a lot and the community we're building as grown hugely since we started in August.&lt;/p&gt; &lt;p&gt;If you want to join our sub-reddit, you can find it here: &lt;a href="/r/nanocoder"&gt;r/nanocoder&lt;/a&gt; - again, we'll breathe more life into this page as time goes along!&lt;/p&gt; &lt;p&gt;As for what's happening in the world of Nanocoder:&lt;/p&gt; &lt;p&gt;- We're almost at 1K stars!!!&lt;/p&gt; &lt;p&gt;- We've fully switched to use AI SDK now over LangGraph. This has been a fantastic change and one that allows us to expand capabilities of the agent.&lt;/p&gt; &lt;p&gt;- You can now tag files into context with `@`.&lt;/p&gt; &lt;p&gt;- You can no track context usage with the `/usage` command.&lt;/p&gt; &lt;p&gt;- One of our main goals is to make Nanocoder work well and reliably with smaller and smaller models. To do this, we've continued to work on everything from fine-tuned models to better tool orchestration and context management. &lt;/p&gt; &lt;p&gt;We're now at a point where models like `gpt-oss:20b` are reliably working well within the CLI for smaller coding tasks. This is ongoing but we're improving every week. The end vision is to be able to code using Nanocoder totally locally with no need for APIs if you don't want them!&lt;/p&gt; &lt;p&gt;- Continued work to build a small language model into &lt;a href="https://github.com/Nano-Collective/get-md"&gt;get-md&lt;/a&gt; for more accurate and context aware markdown generation for LLMs.&lt;/p&gt; &lt;p&gt;If you're interested in the project, we're a completely open collective building privacy-focused AI. We actively invite all contributions to help build a tool for the community by the community! I'd love for you to get involved :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;em&gt;GitHub Repo&lt;/em&gt;: &lt;a href="https://github.com/Nano-Collective/nanocoder"&gt;https://github.com/Nano-Collective/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Discord&lt;/em&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1aurp/an_update_to_nanocoder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1aurp/an_update_to_nanocoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1aurp/an_update_to_nanocoder/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T15:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1836v</id>
    <title>An Open-Source Agent Foundation Model with Interactive ScalingÔºÅMiroThinker V1.0 just launched!</title>
    <updated>2025-11-19T13:45:54+00:00</updated>
    <author>
      <name>/u/wuqiao</name>
      <uri>https://old.reddit.com/user/wuqiao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p1836v/an_opensource_agent_foundation_model_with/"&gt; &lt;img alt="An Open-Source Agent Foundation Model with Interactive ScalingÔºÅMiroThinker V1.0 just launched!" src="https://external-preview.redd.it/Jeli8vyNHpi1OW6VpCLC7sqFTicW7HMwR1zgB4aSLV4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c40f788dfb4b80413245504088417de6a745393" title="An Open-Source Agent Foundation Model with Interactive ScalingÔºÅMiroThinker V1.0 just launched!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiroThinker v1.0 just launched recently! We're back with a MASSIVE update that's gonna blow your mind!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Code&lt;/strong&gt;Ôºö&lt;a href="https://github.com/MiroMindAI/MiroThinker"&gt;https://github.com/MiroMindAI/MiroThinker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Paper&lt;/strong&gt;Ôºö&lt;a href="https://huggingface.co/papers/2511.11793"&gt;https://huggingface.co/papers/2511.11793&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're introducing the &amp;quot;Interactive Scaling&amp;quot; - a completely new dimension for AI scaling! Instead of just throwing more data/params at models, we let agents learn through deep environmental interaction. The more they practice &amp;amp; reflect, the smarter they get! &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;256K Context + 600-Turn Tool Interaction&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance That Slaps:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;BrowseComp: 47.1% accuracy (nearly matches OpenAI DeepResearch at 51.5%)&lt;/li&gt; &lt;li&gt;Chinese tasks (BrowseComp-ZH): 7.7pp better than DeepSeek-v3.2&lt;/li&gt; &lt;li&gt;First-tier performance across HLE, GAIA, xBench-DeepSearch, SEAL-0&lt;/li&gt; &lt;li&gt;Competing head-to-head with GPT, Grok, Claude&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% Open Source&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Full model weights ‚úÖ &lt;/li&gt; &lt;li&gt;Complete toolchains ‚úÖ &lt;/li&gt; &lt;li&gt;Interaction frameworks ‚úÖ&lt;/li&gt; &lt;li&gt;Because transparency &amp;gt; black boxes&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about the Interactive Scaling approach or benchmarks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wuqiao"&gt; /u/wuqiao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-72B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1836v/an_opensource_agent_foundation_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1836v/an_opensource_agent_foundation_model_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T13:45:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1rv91</id>
    <title>Browser extension Powered by Ollama for Code Reviews on Gitlab and Azure DO</title>
    <updated>2025-11-20T02:47:17+00:00</updated>
    <author>
      <name>/u/Brilliant-Vehicle994</name>
      <uri>https://old.reddit.com/user/Brilliant-Vehicle994</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p1rv91/browser_extension_powered_by_ollama_for_code/"&gt; &lt;img alt="Browser extension Powered by Ollama for Code Reviews on Gitlab and Azure DO" src="https://preview.redd.it/eki2xx0bsb2g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=0c9381ba8c12943fb74f536960db2c403d6897c8" title="Browser extension Powered by Ollama for Code Reviews on Gitlab and Azure DO" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello friends,&lt;/p&gt; &lt;p&gt;I just want to let the community know about my open source project ThinkReview that is now powered by Ollama&lt;br /&gt; It does code reviews for Pull and merge requests on Gitlab and Azure DO , summarize the changes , find security issues , best practices and provide scoring , in addition conversations to chat with your OR and dive deeper.&lt;/p&gt; &lt;p&gt;The project is open source under AGPL 3.0 license : &lt;a href="https://github.com/Thinkode/thinkreview-browser-extension"&gt;https://github.com/Thinkode/thinkreview-browser-extension&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and is available on chrome store &lt;a href="https://chromewebstore.google.com/detail/thinkreview-ai-code-revie/bpgkhgbchmlmpjjpmlaiejhnnbkdjdjn"&gt;https://chromewebstore.google.com/detail/thinkreview-ai-code-revie/bpgkhgbchmlmpjjpmlaiejhnnbkdjdjn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love for some of you to try and give me some feedback&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant-Vehicle994"&gt; /u/Brilliant-Vehicle994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eki2xx0bsb2g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1rv91/browser_extension_powered_by_ollama_for_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1rv91/browser_extension_powered_by_ollama_for_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T02:47:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p25jl7</id>
    <title>We trained an SLM assistants for assistance with commit messages on TypeScript codebases - Qwen 3 model (0.6B parameters) that you can run locally!</title>
    <updated>2025-11-20T15:08:26+00:00</updated>
    <author>
      <name>/u/kruszczynski</name>
      <uri>https://old.reddit.com/user/kruszczynski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p25jl7/we_trained_an_slm_assistants_for_assistance_with/"&gt; &lt;img alt="We trained an SLM assistants for assistance with commit messages on TypeScript codebases - Qwen 3 model (0.6B parameters) that you can run locally!" src="https://preview.redd.it/etw8u82jgf2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=404ecde2c4e912c91cbf66ac37ebd425fa6bbb15" title="We trained an SLM assistants for assistance with commit messages on TypeScript codebases - Qwen 3 model (0.6B parameters) that you can run locally!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kruszczynski"&gt; /u/kruszczynski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/etw8u82jgf2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p25jl7/we_trained_an_slm_assistants_for_assistance_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p25jl7/we_trained_an_slm_assistants_for_assistance_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T15:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1p264l9</id>
    <title>Host open-source LLM on a local server and access it Publicly</title>
    <updated>2025-11-20T15:31:30+00:00</updated>
    <author>
      <name>/u/ibjects</name>
      <uri>https://old.reddit.com/user/ibjects</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p264l9/host_opensource_llm_on_a_local_server_and_access/"&gt; &lt;img alt="Host open-source LLM on a local server and access it Publicly" src="https://external-preview.redd.it/aKJxI02wF_Cz77qgjK9fKjpcdmyKIzBmMYNw1dRgElo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3de5bdd2442b26d9f880ac7a730349c5ee8e0369" title="Host open-source LLM on a local server and access it Publicly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ibjects"&gt; /u/ibjects &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ibjects.medium.com/950f48c6858e?source=friends_link&amp;amp;sk=9caf3c3738c9538cd7b0a54def6c6b18"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p264l9/host_opensource_llm_on_a_local_server_and_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p264l9/host_opensource_llm_on_a_local_server_and_access/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T15:31:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p219ok</id>
    <title>Webui agent model in vscode</title>
    <updated>2025-11-20T11:56:47+00:00</updated>
    <author>
      <name>/u/TheRealFAG69</name>
      <uri>https://old.reddit.com/user/TheRealFAG69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it possible to use a custom webui model with a knowledge base in vscode? It would be very handy for VHDL coding &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealFAG69"&gt; /u/TheRealFAG69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p219ok/webui_agent_model_in_vscode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p219ok/webui_agent_model_in_vscode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p219ok/webui_agent_model_in_vscode/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T11:56:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1mnra</id>
    <title>DeepSeek-OCR</title>
    <updated>2025-11-19T22:55:37+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek-OCR is a vision-language model that can perform token-efficient optical character recognition (OCR).&lt;/p&gt; &lt;p&gt;DeepSeek-OCR requires Ollama v0.13.0 or later.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/deepseek-ocr"&gt;https://ollama.com/library/deepseek-ocr&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1mnra/deepseekocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1mnra/deepseekocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1mnra/deepseekocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T22:55:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1p27i64</id>
    <title>Ollama signin docker compose</title>
    <updated>2025-11-20T16:23:36+00:00</updated>
    <author>
      <name>/u/Brilliant_Anxiety_36</name>
      <uri>https://old.reddit.com/user/Brilliant_Anxiety_36</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I have a question im trying to build a stack on docker compose with openwebui and ollama but when i access the container of ollama and i run &lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I get this: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use &amp;quot;ollama [command] --help&amp;quot; for more information about a command. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wich is normal but when i run&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama singin&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Nothing happens. When i do it on ubuntu i get the link to access to ollama and give access to the machine. &lt;/p&gt; &lt;p&gt;I'm ok using ollama directly on the machine but i will like to use it on the stack. Im guessing ollama signin is not availeable yet for containers? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant_Anxiety_36"&gt; /u/Brilliant_Anxiety_36 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p27i64/ollama_signin_docker_compose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p27i64/ollama_signin_docker_compose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p27i64/ollama_signin_docker_compose/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T16:23:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2evio</id>
    <title>Delta Dialogue for local model conversations with report drafting</title>
    <updated>2025-11-20T20:57:28+00:00</updated>
    <author>
      <name>/u/spreader123</name>
      <uri>https://old.reddit.com/user/spreader123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I rediscovered a project I built a few months ago to help me flesh out ideas, draft designs, and rapidly explore concepts. It‚Äôs called Delta Dialogue, and it‚Äôs a local conversation framework for Ollama models that lets them collaborate on any topic you throw at them.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;What it does&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Core idea: Treat each exchange as a ‚Äúdelta‚Äù (a change-state) so conversations build coherently over rounds.&lt;/li&gt; &lt;li&gt;Local-first: Runs with Ollama models only; no web search. Results depend on model training, but are generally solid.&lt;/li&gt; &lt;li&gt;Flexible use: Works for brainstorming, instructions, design ideas, foreign concepts, you name it, they will give it the old college try. &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;How it works&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model setup: You can run multiple distinct models (one of each), or a single model that chains off its own messages.&lt;/li&gt; &lt;li&gt;Rounds: Set 1‚Äì10 rounds. Example: 2 models √ó 10 rounds = 20 replies in a single chain.&lt;/li&gt; &lt;li&gt;Parallel reporting: Each model updates an executive report before finishing its main turn. It starts as a copy of the live feed, then gets refined as the discussion deepens. The report evolves in parallel to the raw dialogue.&lt;/li&gt; &lt;li&gt;Open-ended: There‚Äôs no automatic conclusion step; you can stop any time.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Setup and usage instructions&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Install requirements: &lt;ul&gt; &lt;li&gt;Ollama, Python, and project dependencies.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Launch: &lt;ul&gt; &lt;li&gt;Run Launchfluidoracle.bat.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Select models: &lt;ul&gt; &lt;li&gt;Click the first model to populate the ‚Äúmodel theater‚Äù staging area.&lt;/li&gt; &lt;li&gt;Add more models: Ctrl+click additional models to include them in the discourse.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Prime the session: &lt;ul&gt; &lt;li&gt;Click Think Mode, then click Activate All beneath the staging area.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Set rounds: &lt;ul&gt; &lt;li&gt;Choose a count from 1‚Äì10 (recommend &amp;gt;1).&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Example: 5 models √ó 10 rounds = 50 messages total.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Enter your prompt: &lt;ul&gt; &lt;li&gt;Type into the Fluid Prompt input.&lt;/li&gt; &lt;li&gt;Important: Do not highlight text at this stage; highlighting clears the staging area.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Start the report: &lt;ul&gt; &lt;li&gt;Click Start Report to activate the executive report drafting process.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Initiate dialogue: &lt;ul&gt; &lt;li&gt;Click Initiate Fluid Dialogue.&lt;/li&gt; &lt;li&gt;You can resize the layout; after initiating, highlighting won‚Äôt affect the queued run (models may visually disappear from staging, but the queue is already loaded).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Wait for responses: &lt;ul&gt; &lt;li&gt;Depending on your machine, expect the first reply after a short delay.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Known limits and tips&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt length: There‚Äôs a character limit. I typically keep inputs under ~20 paragraphs. &lt;ul&gt; &lt;li&gt;Symptom: Empty, instant responses mean the prompt was too long‚Äîshorten it.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;UI quirk: Highlighting text before initiating can clear selected models from staging.&lt;/li&gt; &lt;li&gt;Performance: On lower-spec machines, choose lighter Ollama models to avoid slowdowns.&lt;/li&gt; &lt;li&gt;Portability: Built pre‚Äìturbo/cloud; should be straightforward to port to a cloud setup if you want bigger conversations.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Looking for feedback!!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spreader123"&gt; /u/spreader123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Yufok1/Delta_Dialogue"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2evio/delta_dialogue_for_local_model_conversations_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2evio/delta_dialogue_for_local_model_conversations_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T20:57:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2954y</id>
    <title>Evaluating 5090 Desktops for running LLMs locally/ollama</title>
    <updated>2025-11-20T17:24:29+00:00</updated>
    <author>
      <name>/u/Excellent_Composer42</name>
      <uri>https://old.reddit.com/user/Excellent_Composer42</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Composer42"&gt; /u/Excellent_Composer42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1p294oo/evaluating_5090_desktops_for_running_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2954y/evaluating_5090_desktops_for_running_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2954y/evaluating_5090_desktops_for_running_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T17:24:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1p22trj</id>
    <title>Computer Use with Gemini 3 pro</title>
    <updated>2025-11-20T13:13:17+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p22trj/computer_use_with_gemini_3_pro/"&gt; &lt;img alt="Computer Use with Gemini 3 pro" src="https://external-preview.redd.it/Ynh4bWs0aTl3ZTJnMUjWqlAeWHcDEffTfb8EVeAbSnSLMt2a5iQQQ8LfY2lH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b2a472e07bbd2da794d06bc33de06e735e9298d" title="Computer Use with Gemini 3 pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemini 3 pro for Computer Use.&lt;/p&gt; &lt;p&gt;Built with the new windows sandboxes.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs : &lt;a href="https://cua.ai/docs/example-usecases/gemini-complex-ui-navigation"&gt;https://cua.ai/docs/example-usecases/gemini-complex-ui-navigation&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4g3t80r9we2g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p22trj/computer_use_with_gemini_3_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p22trj/computer_use_with_gemini_3_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T13:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2gzxk</id>
    <title>Cortex got a massive update! (ollama UI desktop ap)</title>
    <updated>2025-11-20T22:18:14+00:00</updated>
    <author>
      <name>/u/Ok-Function-7101</name>
      <uri>https://old.reddit.com/user/Ok-Function-7101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p2gzxk/cortex_got_a_massive_update_ollama_ui_desktop_ap/"&gt; &lt;img alt="Cortex got a massive update! (ollama UI desktop ap)" src="https://a.thumbs.redditmedia.com/twAZvPomgU64duwvRIJ4q3p_WaI91rdw3r3eSX7-LB0.jpg" title="Cortex got a massive update! (ollama UI desktop ap)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its entirely open-source and you're invited to come try it out! &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://dovvnloading.github.io/Cortex/"&gt;https://dovvnloading.github.io/Cortex/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8fkh4lhclh2g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9dd2214c5ff9b533cdc596a7217359ed0d07689"&gt;https://preview.redd.it/8fkh4lhclh2g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9dd2214c5ff9b533cdc596a7217359ed0d07689&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ht1e8mhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b86ab8106804a525f3690e85f42c8c2b8612d0cf"&gt;https://preview.redd.it/ht1e8mhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b86ab8106804a525f3690e85f42c8c2b8612d0cf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6jvmrmhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eee7558190f4d4564fdc4284b7e207fb08d4ae7c"&gt;https://preview.redd.it/6jvmrmhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eee7558190f4d4564fdc4284b7e207fb08d4ae7c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j3kzilhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a374639836dc6abdf736458797d3d328c32df70"&gt;https://preview.redd.it/j3kzilhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a374639836dc6abdf736458797d3d328c32df70&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7mpmalhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb9ac99722711d9d4a0981247c3bf883fc88bd0"&gt;https://preview.redd.it/7mpmalhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb9ac99722711d9d4a0981247c3bf883fc88bd0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xm3kklhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b72d44f1aa1495b585e2cc5a08148bf91f53e19"&gt;https://preview.redd.it/xm3kklhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b72d44f1aa1495b585e2cc5a08148bf91f53e19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hza6cnhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85ab8a70697092d5fb0c13e1446f2033fefb707e"&gt;https://preview.redd.it/hza6cnhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85ab8a70697092d5fb0c13e1446f2033fefb707e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kvbxrlhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e538e902ef4db617145140d9ffa90f6df440302"&gt;https://preview.redd.it/kvbxrlhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e538e902ef4db617145140d9ffa90f6df440302&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Function-7101"&gt; /u/Ok-Function-7101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2gzxk/cortex_got_a_massive_update_ollama_ui_desktop_ap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2gzxk/cortex_got_a_massive_update_ollama_ui_desktop_ap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2gzxk/cortex_got_a_massive_update_ollama_ui_desktop_ap/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T22:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2mro7</id>
    <title>Best &lt; $20k Configuration</title>
    <updated>2025-11-21T02:30:03+00:00</updated>
    <author>
      <name>/u/JMWTech</name>
      <uri>https://old.reddit.com/user/JMWTech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What would you build with $20k to train a model(s) and operate a on-prem chat bot for document and policy retrieval?&lt;/p&gt; &lt;p&gt;I've received quotes from &amp;quot;workstations&amp;quot; with 5090s to rack mounted servers running either four L4s (ewww) to a dual proc single RTX Pro 6000. Just want to make sure we're not wasting money and getting the most bang for the buck.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JMWTech"&gt; /u/JMWTech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2mro7/best_20k_configuration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2mro7/best_20k_configuration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2mro7/best_20k_configuration/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T02:30:03+00:00</published>
  </entry>
</feed>
