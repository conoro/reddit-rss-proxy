<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-17T15:34:31+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1r5eyo9</id>
    <title>Causal Ability Injectors - Deterministic Behavioural Override (During Runtime)</title>
    <updated>2026-02-15T13:43:51+00:00</updated>
    <author>
      <name>/u/frank_brsrk</name>
      <uri>https://old.reddit.com/user/frank_brsrk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r5eyo9/causal_ability_injectors_deterministic/"&gt; &lt;img alt="Causal Ability Injectors - Deterministic Behavioural Override (During Runtime)" src="https://preview.redd.it/ifj3miyywnjg1.png?width=140&amp;amp;height=93&amp;amp;auto=webp&amp;amp;s=0d8d938ad3fdc60831283dee7ba2c26bcace46cd" title="Causal Ability Injectors - Deterministic Behavioural Override (During Runtime)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been spending a lot of time lately trying to fix agent's drift or get lost in long loops. While most everyone just feeds them more text, I wanted to build the rules that actually command how they think. Today, I am open sourcing the Causal Ability Injectors. A way to switch the AI's mindset in real-time based on what's happening while in the flow.&lt;/p&gt; &lt;p&gt;[ Example:&lt;br /&gt; during a critical question the input goes through lightweight rag node that dynamically corresponds to the query style and that picks up the most confident way of thinking to enforce to the model and keeping it on track and prohibit model drifting]&lt;/p&gt; &lt;p&gt;[ integrate as retrieval step before agent, OR upsert in your existing doc db for opportunistical retrieval, OR &lt;strong&gt;best case&lt;/strong&gt; add in an isolated namespace and use as behavioral contstraint retrieval]&lt;/p&gt; &lt;p&gt;[Data is already graph-augmented and ready for upsertion]&lt;/p&gt; &lt;p&gt;You can find the registry here: &lt;a href="https://huggingface.co/datasets/frankbrsrk/causal-ability-injectors"&gt;https://huggingface.co/datasets/frankbrsrk/causal-ability-injectors&lt;/a&gt; And the source is here: &lt;a href="https://github.com/frankbrsrkagentarium/causal-ability-injectors-csv"&gt;https://github.com/frankbrsrkagentarium/causal-ability-injectors-csv&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;How it works:&lt;/h1&gt; &lt;p&gt;The registry contains specific mindsets, like reasoning for root causes or checking for logic errors. When the agent hits a bottleneck, it pulls the exact injector it needs. I added columns for things like graph instructions, so each row is a command the machine can actually execute. It's like programming a nervous system instead of just chatting with a bot.&lt;/p&gt; &lt;p&gt;This is the next link in the &lt;em&gt;Architecture of Why&lt;/em&gt;. Build it and you will feel how the information moves once you start using it. Please check it out; I am sure it‚Äôs going to help if you are building complex RAG systems.&lt;/p&gt; &lt;h1&gt;Agentarium | Causal Ability Injectors Walkthrough&lt;/h1&gt; &lt;h1&gt;1. What this is&lt;/h1&gt; &lt;p&gt;Think of this as a blueprint for instructions. It's structured in rows, so each row is the embedding text you want to match against specific situations. I added columns for logic commands that tell the system exactly how to modify the context.&lt;/p&gt; &lt;h1&gt;2. Logic clusters&lt;/h1&gt; &lt;p&gt;I grouped these into four domains. Some are for checking errors, some are for analyzing big systems, and others are for ethics or safety. For example, CA001 is for challenging causal claims and CA005 is for red-teaming a plan.&lt;/p&gt; &lt;h1&gt;3. How to trigger it&lt;/h1&gt; &lt;p&gt;You use the &lt;/p&gt; &lt;pre&gt;&lt;code&gt;trigger_condition &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If the agent is stuck or evaluating a plan, it knows exactly which ability to inject. This keeps the transformer's attention focused on the right constraint at the right time.&lt;/p&gt; &lt;h1&gt;4. Standalone design&lt;/h1&gt; &lt;p&gt;I encoded each row to have everything it needs. Each one has a full JSON payload, so you don't have to look up other files. It's meant to be portable and easy to drop into a vector DB namespace like &lt;/p&gt; &lt;pre&gt;&lt;code&gt;causal-abilities &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;5. Why it's valuable&lt;/h1&gt; &lt;p&gt;It's not just the knowledge; it's the procedures. Instead of a massive 4k-token prompt, you just pull exactly what the AI needs for that one step. It stops the agent from drifting and keeps the reasoning sharp.&lt;/p&gt; &lt;p&gt;It turns ai vibes, to adaptive thought , through retrieved hard-coded instruction set.&lt;/p&gt; &lt;p&gt;State A always pulls Rule B.&lt;br /&gt; Fixed hierarchy resolves every conflict.&lt;br /&gt; Commands the system instead of just adding text.&lt;/p&gt; &lt;p&gt;Repeatable, traceable reasoning that works every single time.&lt;/p&gt; &lt;p&gt;Take Dataset and Use It, Just Download It and Give It To Ur LLM for Analysis&lt;/p&gt; &lt;p&gt;I designed it for power users, and If u like it, give me some feedback report,&lt;/p&gt; &lt;p&gt;This is my work's broader vision, applying cognition when needed, through my personal attention on data driven ability.&lt;/p&gt; &lt;p&gt;frank_brsrk&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ifj3miyywnjg1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce2e010cd3106d92f8823ac7445df4b2cbf78641"&gt;dataset containing forced rules instructions , machine runnable .&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frank_brsrk"&gt; /u/frank_brsrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5eyo9/causal_ability_injectors_deterministic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5eyo9/causal_ability_injectors_deterministic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5eyo9/causal_ability_injectors_deterministic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T13:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5dae9</id>
    <title>Enrichissement corpus et entra√Ænement IA open source</title>
    <updated>2026-02-15T12:20:06+00:00</updated>
    <author>
      <name>/u/Master_Way4672</name>
      <uri>https://old.reddit.com/user/Master_Way4672</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bonjour √† tous, j‚Äôai le projet d'installer un mod√®le IA en Opensource (oLlama ou autre), et je voudrais l‚Äôenrichir et l‚Äôentra√Æner sur un m√©tier particulier.&lt;/p&gt; &lt;p&gt;quelqu‚Äôun pour me guider ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Master_Way4672"&gt; /u/Master_Way4672 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5dae9/enrichissement_corpus_et_entra√Ænement_ia_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5dae9/enrichissement_corpus_et_entra√Ænement_ia_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5dae9/enrichissement_corpus_et_entra√Ænement_ia_open/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T12:20:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5wysb</id>
    <title>Does anybody know how to get llama.cpp? Thanks!</title>
    <updated>2026-02-16T02:06:12+00:00</updated>
    <author>
      <name>/u/Massive-Farm-3410</name>
      <uri>https://old.reddit.com/user/Massive-Farm-3410</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Massive-Farm-3410"&gt; /u/Massive-Farm-3410 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5wysb/does_anybody_know_how_to_get_llamacpp_thanks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5wysb/does_anybody_know_how_to_get_llamacpp_thanks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5wysb/does_anybody_know_how_to_get_llamacpp_thanks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T02:06:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5u9at</id>
    <title>Openclaw on cloud models</title>
    <updated>2026-02-16T00:01:10+00:00</updated>
    <author>
      <name>/u/cfipilot715</name>
      <uri>https://old.reddit.com/user/cfipilot715</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running ollama on a separate server.&lt;/p&gt; &lt;p&gt;Created a openclaw bot and using the cloud models on the ollama server, but it‚Äôs so so slow. Is there a way to go direct to ollama cloud without having to hit my local ollama? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cfipilot715"&gt; /u/cfipilot715 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5u9at/openclaw_on_cloud_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5u9at/openclaw_on_cloud_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5u9at/openclaw_on_cloud_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T00:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5gkph</id>
    <title>Opencode Agent Swarms!</title>
    <updated>2026-02-15T14:55:17+00:00</updated>
    <author>
      <name>/u/Available-Craft-5795</name>
      <uri>https://old.reddit.com/user/Available-Craft-5795</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r5gkph/opencode_agent_swarms/"&gt; &lt;img alt="Opencode Agent Swarms!" src="https://preview.redd.it/j7ipb4qp9ojg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=e0dbc6737089eabf63dc85e0cf89855d7ac3d6d7" title="Opencode Agent Swarms!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/lanefiedler731-gif/OpencodeSwarms"&gt;https://github.com/lanefiedler731-gif/OpencodeSwarms&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I vibecoded this with opencode btw.&lt;/p&gt; &lt;p&gt;This fork emulates Kimi K2.5 Agent Swarms, any model, up to 100 agents at a time.&lt;br /&gt; You will have to build this yourself.&lt;br /&gt; (Press tab until you see &amp;quot;Swarm_manager&amp;quot; mode enabled)&lt;br /&gt; All of them run in parallel.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j7ipb4qp9ojg1.png?width=447&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0eddc72b57bee16dd9ea6f3e30947e9d77523c70"&gt;https://preview.redd.it/j7ipb4qp9ojg1.png?width=447&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0eddc72b57bee16dd9ea6f3e30947e9d77523c70&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available-Craft-5795"&gt; /u/Available-Craft-5795 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5gkph/opencode_agent_swarms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5gkph/opencode_agent_swarms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5gkph/opencode_agent_swarms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T14:55:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5bkwf</id>
    <title>Can I pull models from Huggingface?</title>
    <updated>2026-02-15T10:39:56+00:00</updated>
    <author>
      <name>/u/Keensworth</name>
      <uri>https://old.reddit.com/user/Keensworth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm new to Ollama and just finished installing it and integrating it with OpenWebUI. I'm wondering if it was possible to pull models from Huggingface via a API call or a plugin?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Keensworth"&gt; /u/Keensworth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5bkwf/can_i_pull_models_from_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5bkwf/can_i_pull_models_from_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5bkwf/can_i_pull_models_from_huggingface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T10:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5xruc</id>
    <title>Has anyone gotten AMD Radeon 780M to work with Ollama?</title>
    <updated>2026-02-16T02:44:36+00:00</updated>
    <author>
      <name>/u/Famous-Weight2271</name>
      <uri>https://old.reddit.com/user/Famous-Weight2271</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alas, I'm giving up at this point, concluding that it is not possible for me to run Ollama on the GPU with my 780M. The main go to reference for this is &lt;a href="https://github.com/likelovewant/ollama-for-amd"&gt;https://github.com/likelovewant/ollama-for-amd&lt;/a&gt;, which may have worked in the past, or may just not like my specific machine. The instructions I followed were for HIP SDK 6.4.2 and gfx1103 / &lt;a href="https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU"&gt;&lt;strong&gt;ROCmLibs-for-gfx1103-AMD780M-APU&lt;/strong&gt;&lt;/a&gt; matching the HIP SDK (&lt;a href="https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU/releases/download/v0.6.4.2/rocm.gfx1103.for.hip.6.4.2.7z"&gt;rocm.gfx1103.for.hip.6.4.2.7z&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I also tried &lt;a href="https://github.com/ByronLeeeee/Ollama-For-AMD-Installer"&gt;https://github.com/ByronLeeeee/Ollama-For-AMD-Installer&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The only time Ollama works on my machine &amp;quot;ollama ps&amp;quot; reports 100% CPU. Very slow with llama 3.1. I have configured my dedicated GPU RAM in the BIOS to be 8 GB (the max for my machine) of the 32GB available on my GMKtex K4.&lt;/p&gt; &lt;p&gt;When playing with ROCM libs above, the worst case (like using the no-longer-maintained BrysonLee installer) give the dreaded &amp;quot;500 Internal Server Error: llama runner process has terminated: ROCm error&amp;quot;.&lt;/p&gt; &lt;p&gt;When running on CPU, I see the usual logs the others running Ollama on AMD GPUs see: that ollama can't find a suitable GPU.&lt;/p&gt; &lt;p&gt;If anyone has gotten this to work, there's many of us out there who'd love to know what the trick is.&lt;/p&gt; &lt;p&gt;(If your suggestion is to buy another PC, please consider being more helpful than that. My dev machine has a RTX 4080, but I'd like to run zeroclaw on this little NUC box, which should be perfect for it. And zeroclaw works fine, of course, with a paid API (Grok, OpenAI, Gemini), but for obvious reasons would like an open source model running locally, too.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Famous-Weight2271"&gt; /u/Famous-Weight2271 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5xruc/has_anyone_gotten_amd_radeon_780m_to_work_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5xruc/has_anyone_gotten_amd_radeon_780m_to_work_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5xruc/has_anyone_gotten_amd_radeon_780m_to_work_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T02:44:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6qte3</id>
    <title>Three months ago I didn‚Äôt know how to code. Today I‚Äôm running a multi-agent AI system locally.</title>
    <updated>2026-02-17T00:17:08+00:00</updated>
    <author>
      <name>/u/Decent-Freedom5374</name>
      <uri>https://old.reddit.com/user/Decent-Freedom5374</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r6qte3/three_months_ago_i_didnt_know_how_to_code_today/"&gt; &lt;img alt="Three months ago I didn‚Äôt know how to code. Today I‚Äôm running a multi-agent AI system locally." src="https://preview.redd.it/r6eb6mjx6yjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dab11baac416319d97da40dae4b85746f279c8b1" title="Three months ago I didn‚Äôt know how to code. Today I‚Äôm running a multi-agent AI system locally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two months ago I didn‚Äôt know how to code. I couldn‚Äôt even find the terminal on my computer. I didn‚Äôt understand APIs, Docker, ports, or what a daemon was. AI to me was just ChatGPT in a browser.&lt;/p&gt; &lt;p&gt;Today I‚Äôm running local models through Ollama, orchestrating multiple AI agents (CEO, CTO, CFO, COO roles), spinning daemons, debugging backend routers, testing health endpoints, and integrating marketplace modules ‚Äî all running on local hardware.&lt;/p&gt; &lt;p&gt;I broke everything at least 200 times. I stared at ‚Äúcommand not recognized‚Äù errors like they were written in another language. I had to learn what ports do, how environment variables work, how services talk to each other, and why one missing slash can ruin your night.&lt;/p&gt; &lt;p&gt;I didn‚Äôt take a bootcamp. I didn‚Äôt wait until I felt ‚Äúready.‚Äù I just started building something real and forced myself to understand every failure.&lt;/p&gt; &lt;p&gt;I‚Äôm building HaleES ‚Äî a local-first hospitality intelligence system that runs inside the venue, owns its data, and doesn‚Äôt collapse when WiFi does. It‚Äôs not a SaaS wrapper. It‚Äôs infrastructure.&lt;/p&gt; &lt;p&gt;The biggest lesson? The gap between ‚ÄúI‚Äôm not technical‚Äù and ‚ÄúI run systems‚Äù is mostly tolerance for frustration. If you‚Äôre willing to keep going when nothing works, eventually it does.&lt;/p&gt; &lt;p&gt;Stop waiting to feel qualified. Build something slightly too ambitious and grow into it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Decent-Freedom5374"&gt; /u/Decent-Freedom5374 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r6eb6mjx6yjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6qte3/three_months_ago_i_didnt_know_how_to_code_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6qte3/three_months_ago_i_didnt_know_how_to_code_today/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T00:17:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r63th5</id>
    <title>Liquid LFM2-VL 450M (Q4_0) running in-browser via WebGPU (local inference)</title>
    <updated>2026-02-16T08:05:07+00:00</updated>
    <author>
      <name>/u/New_Inflation_6927</name>
      <uri>https://old.reddit.com/user/New_Inflation_6927</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r63th5/liquid_lfm2vl_450m_q4_0_running_inbrowser_via/"&gt; &lt;img alt="Liquid LFM2-VL 450M (Q4_0) running in-browser via WebGPU (local inference)" src="https://external-preview.redd.it/NVlVHryhjyeP5-OGKDEiV_ex2BfxuDpWKO79stNO5RU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=034922f8ce7d38b2b351cd03b76eba3bcaf114fb" title="Liquid LFM2-VL 450M (Q4_0) running in-browser via WebGPU (local inference)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Inflation_6927"&gt; /u/New_Inflation_6927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hds7nl76dtjg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r63th5/liquid_lfm2vl_450m_q4_0_running_inbrowser_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r63th5/liquid_lfm2vl_450m_q4_0_running_inbrowser_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T08:05:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6n635</id>
    <title>New to this need some help.</title>
    <updated>2026-02-16T21:52:53+00:00</updated>
    <author>
      <name>/u/Sensitive-Tip-3669</name>
      <uri>https://old.reddit.com/user/Sensitive-Tip-3669</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so I recently started my AS in Computer Science and I ended up getting a pretty nice rig. I started looking into ai's long story short I ended up with dolphin3. I need a killer prompt for this thing so it will operate like i need it to.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive-Tip-3669"&gt; /u/Sensitive-Tip-3669 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6n635/new_to_this_need_some_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6n635/new_to_this_need_some_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6n635/new_to_this_need_some_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T21:52:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6cgfw</id>
    <title>How to integrate local ollama into vs code?</title>
    <updated>2026-02-16T15:23:34+00:00</updated>
    <author>
      <name>/u/elixon</name>
      <uri>https://old.reddit.com/user/elixon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r6cgfw/how_to_integrate_local_ollama_into_vs_code/"&gt; &lt;img alt="How to integrate local ollama into vs code?" src="https://preview.redd.it/bqtub1ngjvjg1.png?width=140&amp;amp;height=139&amp;amp;auto=webp&amp;amp;s=772ba358c46068f37cb3a852d1f2a330ced77190" title="How to integrate local ollama into vs code?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elixon"&gt; /u/elixon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/vscode/comments/1r6cg6r/how_to_integrate_local_ollama_into_vs_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6cgfw/how_to_integrate_local_ollama_into_vs_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6cgfw/how_to_integrate_local_ollama_into_vs_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T15:23:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6dsun</id>
    <title>New to the space - Hey guys üëã</title>
    <updated>2026-02-16T16:12:52+00:00</updated>
    <author>
      <name>/u/notalentwasted</name>
      <uri>https://old.reddit.com/user/notalentwasted</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So let me start with that I'm broke rn so don't just. I feel I should brag about my set up though since it's necromancy. &lt;/p&gt; &lt;p&gt;I have a Asus Maximus IV gene Z i7 3770 - 24gb ram (1333) , M10 32gb (4 x 8gb GM107 dies) plus a GTX750TI 2gb for embeddings models. OS is Ubuntu on 128gb ssd. Model storage is on pcie to m.2 256gb.1.4gb read - I use 8b or less and get sub 4 second cold swap. The m10 is in the first x16 slot. Pcie to m.2 is in x8 slot (space doesn't allow) then I got the gtx750ti to run on the x4 slot. 1tb hdd auxiliary storage &lt;/p&gt; &lt;p&gt;I run 18 ish services and have a 300 plus line docker compose file. (Changes frequently) &lt;/p&gt; &lt;p&gt;I only started this journey last may after finishing school. Kinda just finding the limits of the architecture and seeing what I can squeeze out of this gpu set up.&lt;/p&gt; &lt;p&gt;I'm new to the community and looking for my tribe! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notalentwasted"&gt; /u/notalentwasted &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6dsun/new_to_the_space_hey_guys/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6dsun/new_to_the_space_hey_guys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6dsun/new_to_the_space_hey_guys/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T16:12:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6fqfs</id>
    <title>Building a local RAG system for academic</title>
    <updated>2026-02-16T17:22:09+00:00</updated>
    <author>
      <name>/u/kejitoo</name>
      <uri>https://old.reddit.com/user/kejitoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I‚Äôd like to know if it‚Äôs possible to use Ollama to create a search system for dissertations and theses from a local graduate program. What would be the costs and hardware requirements?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kejitoo"&gt; /u/kejitoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6fqfs/building_a_local_rag_system_for_academic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6fqfs/building_a_local_rag_system_for_academic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6fqfs/building_a_local_rag_system_for_academic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T17:22:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5wlwk</id>
    <title>What kind of models can I run on my M5 MBP with 24GB RAM?</title>
    <updated>2026-02-16T01:49:32+00:00</updated>
    <author>
      <name>/u/saintforlife1</name>
      <uri>https://old.reddit.com/user/saintforlife1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got a new M5 MacBook Pro with 24GB RAM. What kind of models are best suited to run on such a machine?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/saintforlife1"&gt; /u/saintforlife1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5wlwk/what_kind_of_models_can_i_run_on_my_m5_mbp_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5wlwk/what_kind_of_models_can_i_run_on_my_m5_mbp_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5wlwk/what_kind_of_models_can_i_run_on_my_m5_mbp_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T01:49:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6gyv6</id>
    <title>How do you decide to choose between fine tuning an LLM model or using RAG?</title>
    <updated>2026-02-16T18:05:55+00:00</updated>
    <author>
      <name>/u/degr8sid</name>
      <uri>https://old.reddit.com/user/degr8sid</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/degr8sid"&gt; /u/degr8sid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LLM/comments/1r6gy86/how_do_you_decide_to_choose_between_fine_tuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6gyv6/how_do_you_decide_to_choose_between_fine_tuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6gyv6/how_do_you_decide_to_choose_between_fine_tuning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T18:05:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6h59q</id>
    <title>PolyClaw ‚Äì An Autonomous Docker-First MCP Agent for PolyMCP</title>
    <updated>2026-02-16T18:12:17+00:00</updated>
    <author>
      <name>/u/Just_Vugg_PolyMCP</name>
      <uri>https://old.reddit.com/user/Just_Vugg_PolyMCP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r6h59q/polyclaw_an_autonomous_dockerfirst_mcp_agent_for/"&gt; &lt;img alt="PolyClaw ‚Äì An Autonomous Docker-First MCP Agent for PolyMCP" src="https://external-preview.redd.it/od8MSIgruh5Q6AVcijBeXw-iTs44_Suwgl-5tJxhnnM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=311527a31504769db78a4645ad608cd000b27422" title="PolyClaw ‚Äì An Autonomous Docker-First MCP Agent for PolyMCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Vugg_PolyMCP"&gt; /u/Just_Vugg_PolyMCP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/poly-mcp/PolyMCP"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6h59q/polyclaw_an_autonomous_dockerfirst_mcp_agent_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6h59q/polyclaw_an_autonomous_dockerfirst_mcp_agent_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T18:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5r9tp</id>
    <title>Why Ollama and not ChatGPT?</title>
    <updated>2026-02-15T21:54:40+00:00</updated>
    <author>
      <name>/u/Humble_Ad_7053</name>
      <uri>https://old.reddit.com/user/Humble_Ad_7053</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Asking out of curiosity because I‚Äôm new to Ollama. I do understand the local storage and privacy concerns, and working offline. But I want to know what other reasons people go for it. Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humble_Ad_7053"&gt; /u/Humble_Ad_7053 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5r9tp/why_ollama_and_not_chatgpt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5r9tp/why_ollama_and_not_chatgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5r9tp/why_ollama_and_not_chatgpt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T21:54:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6lgl6</id>
    <title>openclaw locally?</title>
    <updated>2026-02-16T20:48:08+00:00</updated>
    <author>
      <name>/u/Efficient-Level1944</name>
      <uri>https://old.reddit.com/user/Efficient-Level1944</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Efficient-Level1944"&gt; /u/Efficient-Level1944 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1r6ldr1/openclaw_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6lgl6/openclaw_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6lgl6/openclaw_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T20:48:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6l3vr</id>
    <title>Anything usable for everyday nonenglish chat on limited mac HW?</title>
    <updated>2026-02-16T20:35:00+00:00</updated>
    <author>
      <name>/u/librewolf</name>
      <uri>https://old.reddit.com/user/librewolf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there, got recently a Macbook Air M4 with just 16gb ram and 256gb ssd. To end my hopes faster- is there any model to chat general purpose ways with any of the current model usable enough with my specs, for czech langauge?&lt;/p&gt; &lt;p&gt;For coding Im very happy sith Codex, im just looking for chat with private stuff I dont wanr leaving my computer&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/librewolf"&gt; /u/librewolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6l3vr/anything_usable_for_everyday_nonenglish_chat_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6l3vr/anything_usable_for_everyday_nonenglish_chat_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6l3vr/anything_usable_for_everyday_nonenglish_chat_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T20:35:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r71lx5</id>
    <title>CodeSolver Pro - Browser extension for ollama</title>
    <updated>2026-02-17T09:33:32+00:00</updated>
    <author>
      <name>/u/Fun-Zookeepergame700</name>
      <uri>https://old.reddit.com/user/Fun-Zookeepergame700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just built CodeSolver Pro ‚Äì a browser extension that automatically detects coding problems from LeetCode, HackerRank, and other platforms, then uses local AI running entirely on your machine to generate complete solutions with approach explanations, time complexity analysis, and code. Your problems never leave your computer ‚Äì no cloud API calls, no privacy concerns, works offline. It runs in a side panel for seamless workflow, supports Ollama and LM Studio, and includes focus protection for platforms that detect extensions. Free, open-source, Chrome/Firefox. Would love feedback from fellow devs who value privacy! &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/sourjatilak/CodeSolverPro"&gt;https://github.com/sourjatilak/CodeSolverPro&lt;/a&gt;&lt;br /&gt; Youtube: &lt;a href="https://www.youtube.com/watch?v=QX0T8DcmDpw"&gt;https://www.youtube.com/watch?v=QX0T8DcmDpw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Zookeepergame700"&gt; /u/Fun-Zookeepergame700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r71lx5/codesolver_pro_browser_extension_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r71lx5/codesolver_pro_browser_extension_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r71lx5/codesolver_pro_browser_extension_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T09:33:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r71km5</id>
    <title>Hey everyone! Ollama llm-checker has been growing and i need help</title>
    <updated>2026-02-17T09:31:12+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r71km5/hey_everyone_ollama_llmchecker_has_been_growing/"&gt; &lt;img alt="Hey everyone! Ollama llm-checker has been growing and i need help" src="https://preview.redd.it/l6lzywpqx0kg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06ad9e939f017f018c335c60461294a517db62cd" title="Hey everyone! Ollama llm-checker has been growing and i need help" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;I created a Discord server to build it together with the community. Your feedback and ideas matter the most whether it's new&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;model&lt;/em&gt; &lt;em&gt;support,&lt;/em&gt; &lt;em&gt;hardware&lt;/em&gt; &lt;em&gt;detection,&lt;/em&gt; &lt;em&gt;scoring&lt;/em&gt; &lt;em&gt;improvements,&lt;/em&gt; &lt;em&gt;or&lt;/em&gt; &lt;em&gt;anything&lt;/em&gt; &lt;em&gt;else.&lt;/em&gt; &lt;em&gt;Come&lt;/em&gt; &lt;em&gt;join&lt;/em&gt; &lt;em&gt;and&lt;/em&gt; &lt;em&gt;help&lt;/em&gt; &lt;em&gt;shape&lt;/em&gt; &lt;em&gt;the&lt;/em&gt; &lt;em&gt;tool:&lt;/em&gt; &lt;a href="https://discord.com/invite/mnmYrA7T"&gt;&lt;em&gt;https://discord.com/invite/mnmYrA7T&lt;/em&gt;&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l6lzywpqx0kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r71km5/hey_everyone_ollama_llmchecker_has_been_growing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r71km5/hey_everyone_ollama_llmchecker_has_been_growing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T09:31:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6tj0j</id>
    <title>Lots of pain, finally a small breakthrough, is it enough? Sharing what I've done</title>
    <updated>2026-02-17T02:17:04+00:00</updated>
    <author>
      <name>/u/Minimum-Two-8093</name>
      <uri>https://old.reddit.com/user/Minimum-Two-8093</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r6tj0j/lots_of_pain_finally_a_small_breakthrough_is_it/"&gt; &lt;img alt="Lots of pain, finally a small breakthrough, is it enough? Sharing what I've done" src="https://preview.redd.it/jgt5b0hmnyjg1.png?width=140&amp;amp;height=81&amp;amp;auto=webp&amp;amp;s=c0ee1c4b62e6541654901aef808a587bc5986976" title="Lots of pain, finally a small breakthrough, is it enough? Sharing what I've done" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not overly knowledgeable about self-hosting models, but I do have a software engineering background and time on my hands to try to make it work, as well as an employer that's encouraging us to figure shit out (read: I'm being paid to do this).&lt;/p&gt; &lt;p&gt;Comparing cloud agents with local agents is never a good idea, but that's been my only frame of reference. I have an absolutely huge solution for an economic simulation that I've been building, recently in conjunction with cloud agents.&lt;/p&gt; &lt;p&gt;Up until now, I've been using Chat-GPT (web) as a designer and prompt engineer, and Opus 4.5 as a coding model through Claude Code. I've been treating this approach as if I'm the architect, and the agents are my junior to middling developers. This has been working very well.&lt;/p&gt; &lt;p&gt;But, I've wanted to find a use-case where my local machine is used for some of the work - I'm sick of paying for cloud agents, running out of quota continually, and sharing my information freely.&lt;/p&gt; &lt;p&gt;For the past 3 weeks, I've been struggling with Ollama hosted models, trying to find the right use-cases. I have an RTX4090 and have been dancing between Qwen, GPT-OSS, and DeepSeek derivatives of Qwen. I have docker running on an old 1U server in my garage, currently only hosting Open WebUI, this is exposing Ollama hosted models to all of my devices via Tailscale.&lt;/p&gt; &lt;p&gt;I'm using &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; in VS Code.&lt;/p&gt; &lt;p&gt;That's the background, now the problem statement:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;When trying to use my self-hosted &amp;quot;agents&amp;quot; for coding tasks, nothing &lt;em&gt;felt&lt;/em&gt; good. Attempting to edit files and failing to reference them properly just felt like &lt;em&gt;friction&lt;/em&gt;. Unintended files ended up overwritten.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I have the following folder structure for Continue to load when VS Code is opened:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;./.continue/rules/&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And inside that folder are the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;00-path-grounding .md&lt;/li&gt; &lt;li&gt;00-project-context .md&lt;/li&gt; &lt;li&gt;08-path-grounding-hard-stop .md&lt;/li&gt; &lt;li&gt;08b-no-fake-tools .md&lt;/li&gt; &lt;li&gt;09-repomap-maintenance .md&lt;/li&gt; &lt;li&gt;10-determinism .md&lt;/li&gt; &lt;li&gt;11-simulation-boundaries .md&lt;/li&gt; &lt;li&gt;12-contract-invariants .md&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've also got another repo with my project agnostic rules files and a script which copies them into the same &lt;em&gt;rules&lt;/em&gt; folder. This is so that I can keep all of my projects consistent with each other if they're using my local models.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;shared-01-general .md&lt;/li&gt; &lt;li&gt;shared-02-safe-edits .md&lt;/li&gt; &lt;li&gt;shared-03-tests-first .md&lt;/li&gt; &lt;li&gt;shared-04-diff-discipline .md&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This has been hit and miss, mainly because tool usage has also been hit and miss. What's improved this however are the built-in custom providers for Continue.&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.continue.dev/customize/deep-dives/custom-providers#built-in-context-providers"&gt;https://docs.continue.dev/customize/deep-dives/custom-providers#built-in-context-providers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's the Continue config.yaml file I've settled on, including the providers I've chosen:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// config.yaml name: Local Config version: 1.0.0 schema: v1 context: - provider: file - provider: code - provider: open params: onlyPinned: true - provider: clipboard - provider: tree - provider: repo-map params: includeSignatures: false # default true models: - name: GPT-OSS Chat provider: ollama apiBase: http://localhost:11434 model: gpt-oss:20b roles: - chat - name: Qwen3 Coder provider: ollama apiBase: http://localhost:11434 model: qwen3-coder:30b roles: - chat - edit - apply - name: Qwen2.5 Autocomplete provider: ollama apiBase: http://localhost:11434 model: qwen2.5-coder:1.5b-base roles: - autocomplete - name: Nomic Embed provider: ollama apiBase: http://localhost:11434 model: nomic-embed-text:latest roles: - embed &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The end result seems to be that I have finally settled on something &lt;em&gt;kinda&lt;/em&gt; useful.&lt;/p&gt; &lt;p&gt;This may look simple (it is), but it's the first scoped refactor of an existing piece of code where the agent hasn't screwed &lt;em&gt;something&lt;/em&gt; up.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jgt5b0hmnyjg1.png?width=2728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=25e57ca54e0d0d35116c47df872e7d7dc7f1e18a"&gt;https://preview.redd.it/jgt5b0hmnyjg1.png?width=2728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=25e57ca54e0d0d35116c47df872e7d7dc7f1e18a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As mentioned, this code base is quite large and set to get bigger.&lt;/p&gt; &lt;p&gt;I've been used to cloud agents achieving quite amazing things while significantly boosting throughput (at least 20x what I'm capable of, probably more).&lt;/p&gt; &lt;p&gt;I'd wanted to have my local models do the same, but in reality that was completely unrealistic; a 4090 cannot compete with cloud inference.&lt;/p&gt; &lt;p&gt;Where I &lt;em&gt;think&lt;/em&gt; I've settled now (especially since a significant portion of my simulation is complete) is that my local agents can probably help to augment me more now that I am moving onto front-end implementation. I think that if I can constrain myself to only expect the local agents to help me with &lt;em&gt;the boring shit&lt;/em&gt; like boilerplate and mass data entry, I will still save myself significant amounts of time; e.g. that edit would have taken me a couple of minutes of data entry, the prompt took a few seconds to write, and a few seconds to execute. I think it's likely that I'll keep using cloud agents for gnarly work, and local agents for the simpler things. That's not bad.&lt;/p&gt; &lt;p&gt;Perhaps that's the sweet-spot.&lt;/p&gt; &lt;p&gt;I don't really know what I want to get from this post, perhaps just to start a conversation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Are you working on large projects with locally hosted models?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How has your experience been?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If I'm missing anything obvious, let me know.&lt;/p&gt; &lt;p&gt;Edit: The markdown filenames attempted to be clickable links, edited to add a space to stop it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Minimum-Two-8093"&gt; /u/Minimum-Two-8093 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6tj0j/lots_of_pain_finally_a_small_breakthrough_is_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6tj0j/lots_of_pain_finally_a_small_breakthrough_is_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6tj0j/lots_of_pain_finally_a_small_breakthrough_is_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T02:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r77o7d</id>
    <title>REASONING AUGMENTED RETRIEVAL (RAR) is the production-grade successor to single-pass RAG.</title>
    <updated>2026-02-17T14:37:05+00:00</updated>
    <author>
      <name>/u/frank_brsrk</name>
      <uri>https://old.reddit.com/user/frank_brsrk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Single-pass rag retrieves once and hopes the model stitches fragments into coherent reasoning. It fails on multi-hop questions, contradictions, temporal dependencies, or cases needing follow-up fetches.Rar puts reasoning first. The system decomposes the problem, identifies gaps, issues precise (often multiple, reformulated, or negated) retrievals.&lt;br /&gt; integrates results into an ongoing chain-of-thought, discards noise or conflicts, and loops until the logic closes with high confidence.&lt;/p&gt; &lt;p&gt;Measured gains in production:&lt;/p&gt; &lt;p&gt;-35‚Äì60% accuracy lift on multi-hop, regulatory, and long-document tasks&lt;br /&gt; -far fewer confident-but-wrong answers&lt;br /&gt; -built-in uncertainty detection and gap admission&lt;br /&gt; -traceable retrieval decisions&lt;/p&gt; &lt;p&gt;Training data must include:&lt;br /&gt; -interleaved reasoning + retrieval + reflection traces&lt;br /&gt; -negative examples forcing rejection of misleading chunks&lt;br /&gt; -synthetic trajectories with hidden multi-hop needs&lt;br /&gt; -confidence rules that trigger extra cycles&lt;/p&gt; &lt;p&gt;Rar turns retrieval into an active part of thinking instead of a one time lookup. Systems still using single pass dense retrieval in 2026 accept unnecessary limits on depth, reliability, and explainability. RAR is the necessary direction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frank_brsrk"&gt; /u/frank_brsrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r77o7d/reasoning_augmented_retrieval_rar_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r77o7d/reasoning_augmented_retrieval_rar_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r77o7d/reasoning_augmented_retrieval_rar_is_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T14:37:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r735eo</id>
    <title>Self Hosted Alternative to NotebookLM</title>
    <updated>2026-02-17T11:06:53+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, SurfSense is an open-source alternative to NotebookLM, Perplexity, and Glean.&lt;/p&gt; &lt;p&gt;It connects any LLM to your internal knowledge sources, then lets teams chat, comment, and collaborate in real time. Think of it as a team-first research workspace with citations, connectors, and agentic workflows.&lt;/p&gt; &lt;p&gt;I‚Äôm looking for contributors. If you‚Äôre into AI agents, RAG, search, browser extensions, or open-source research tooling, would love your help.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Self-hostable (Docker)&lt;/li&gt; &lt;li&gt;25+ external connectors (search engines, Drive, Slack, Teams, Jira, Notion, GitHub, Discord, and more)&lt;/li&gt; &lt;li&gt;Realtime Group Chats&lt;/li&gt; &lt;li&gt;Hybrid retrieval (semantic + full-text) with cited answers&lt;/li&gt; &lt;li&gt;Deep agent architecture (planning + subagents + filesystem access)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs and 6000+ embedding models (via OpenAI-compatible APIs + LiteLLM)&lt;/li&gt; &lt;li&gt;50+ file formats (including Docling/local parsing options)&lt;/li&gt; &lt;li&gt;Podcast generation (multiple TTS providers)&lt;/li&gt; &lt;li&gt;Cross-browser extension to save dynamic/authenticated web pages&lt;/li&gt; &lt;li&gt;RBAC roles for teams&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Slide creation support&lt;/li&gt; &lt;li&gt;Multilingual podcast support&lt;/li&gt; &lt;li&gt;Video creation agent&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r735eo/self_hosted_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r735eo/self_hosted_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r735eo/self_hosted_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T11:06:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r72my6</id>
    <title>My experience with running small scale open source models on my own PC.</title>
    <updated>2026-02-17T10:36:47+00:00</updated>
    <author>
      <name>/u/Dibru9109_4259</name>
      <uri>https://old.reddit.com/user/Dibru9109_4259</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got exposed to &lt;strong&gt;Ollama&lt;/strong&gt; and the realization that I could take the 2 Billion 3 Billion parameter models and run them locally in my small pc with limited capacity of &lt;strong&gt;8 GB RAM&lt;/strong&gt; and just an &lt;strong&gt;Intel i3&lt;/strong&gt; CPU and without any GPU made me so excited and amazed. &lt;/p&gt; &lt;p&gt;Though the experience of running such Billions parameter models with 2-5 GB RAM consumption was not a smooth experience. Firstly I run the &amp;quot;&lt;strong&gt;Mistral 7B&lt;/strong&gt;&amp;quot; model in my ollama. The response was well structured and the reasoning was good but given the limitations of my hardwares, it took about &lt;strong&gt;3-4 minutes&lt;/strong&gt; in generating every response.&lt;/p&gt; &lt;p&gt;For a smoother expereience, I decided to run a smaller model. I choose Microsoft's &lt;strong&gt;phi3:mini&lt;/strong&gt; model which was trained on around &lt;strong&gt;3.8 Billion&lt;/strong&gt; parameters. The experience with this model was quite smoother compared to the pervious Minstral 7B model. phi3:mini took about 7-8 secods for the cold start and once it was started, it was generating responses with &lt;strong&gt;less than 0.5 seconds&lt;/strong&gt; of prompting. I tried to measure the token generating speed using my phone's stopwatch and the number of words generated by the model (NOTE: &lt;strong&gt;1 token = 0.75 word&lt;/strong&gt;, on average). I found out that this model was generating 7.5 tokens per second on my PC. The experience was pretty smooth with such a speed and it was also able to do all kinds of basic chat and reasoning.&lt;/p&gt; &lt;p&gt;After this I decided to test the limits so I downloaded two even smaller models - &lt;strong&gt;tinyLLama&lt;/strong&gt;. While the model was much compact with just &lt;strong&gt;1.1 Billion&lt;/strong&gt; parameters and just 0.67GB download size for the &lt;strong&gt;4-bit (Q4_K_M) version&lt;/strong&gt;, its performance deteriorated sharply.&lt;/p&gt; &lt;p&gt;When I first gave a simple Hi to this model it responded with a random unrelated texts about &amp;quot;nothingness&amp;quot; and the paradox of nothingness. I tried to make it talk to me but it kept elaborating in its own cilo about the great philosophies around the concept of nothingness thereby not responding to whatever prompt I gave to it. Afterwards I also tried my hand at the &lt;strong&gt;smoLlm&lt;/strong&gt; and this one also hallucinated massively.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Conclusion :&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My &lt;em&gt;hardware&lt;/em&gt; capacity affected the &lt;em&gt;speed&lt;/em&gt; of Token generated by the different models. While the 7B parameter Mistral model took several minutes to respond each time, &lt;em&gt;this problem was eliminated entirely once I went 3.8 Billion parameters and less.&lt;/em&gt; All of the phi3:mini and even the ones that hallucinated heavily - smolLm and tinyLlama generated tokens instantly.&lt;/p&gt; &lt;p&gt;The &lt;em&gt;number of parameters determines the extent of intelligence&lt;/em&gt; of the LLM. Going below the 3.8 Billion parameter phi3:mini f, all the tiny models hallucinated excessively even though they were generating those rubbish responses very quickly and almost instantly.&lt;/p&gt; &lt;p&gt;There was &lt;em&gt;a tradeoff between&lt;/em&gt; &lt;strong&gt;&lt;em&gt;speed&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;and&lt;/em&gt; &lt;strong&gt;&lt;em&gt;accuracy.&lt;/em&gt;&lt;/strong&gt; Given the limited hardware capacity of my pc, going below 3.8 Billion parameter model gave instant speed but extremely bad accuracy while going above it gave slow speed but higher accuracy.&lt;/p&gt; &lt;p&gt;So this was my experience about experimenting with Edge AI and various open source models. &lt;strong&gt;Please feel free to correct me whereever you think I might be wrong. Questions are absolutely welcome!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dibru9109_4259"&gt; /u/Dibru9109_4259 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r72my6/my_experience_with_running_small_scale_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r72my6/my_experience_with_running_small_scale_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r72my6/my_experience_with_running_small_scale_open/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T10:36:47+00:00</published>
  </entry>
</feed>
