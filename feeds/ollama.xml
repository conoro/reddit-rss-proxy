<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-16T11:34:31+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ne3nks</id>
    <title>I tested an uncensored LLAMA model...</title>
    <updated>2025-09-11T08:49:20+00:00</updated>
    <author>
      <name>/u/Flax19</name>
      <uri>https://old.reddit.com/user/Flax19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ne3nks/i_tested_an_uncensored_llama_model/"&gt; &lt;img alt="I tested an uncensored LLAMA model..." src="https://b.thumbs.redditmedia.com/kikxB11-qllLLoGh8j_89ksTTQrUe5ROvuaRz0jmS9s.jpg" title="I tested an uncensored LLAMA model..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/i5nosmpi1iof1.png?width=828&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb7d78da292c63e4c77d5bf93ccd4c737a9bbd95"&gt;https://preview.redd.it/i5nosmpi1iof1.png?width=828&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb7d78da292c63e4c77d5bf93ccd4c737a9bbd95&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Who even comes up with something like this?ðŸ˜‚&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flax19"&gt; /u/Flax19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ne3nks/i_tested_an_uncensored_llama_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ne3nks/i_tested_an_uncensored_llama_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ne3nks/i_tested_an_uncensored_llama_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-11T08:49:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nell96</id>
    <title>Thoughts on Memory Pooling with Multiple GPUs vs. Going With a Single Big Card</title>
    <updated>2025-09-11T21:46:32+00:00</updated>
    <author>
      <name>/u/AggravatingGiraffe46</name>
      <uri>https://old.reddit.com/user/AggravatingGiraffe46</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggravatingGiraffe46"&gt; /u/AggravatingGiraffe46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nelky7/thoughts_on_memory_pooling_with_multiple_gpus_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nell96/thoughts_on_memory_pooling_with_multiple_gpus_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nell96/thoughts_on_memory_pooling_with_multiple_gpus_vs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-11T21:46:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nembeg</id>
    <title>did MCP become usefull?</title>
    <updated>2025-09-11T22:17:28+00:00</updated>
    <author>
      <name>/u/rhaastt-ai</name>
      <uri>https://old.reddit.com/user/rhaastt-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ive stopped working with llms about 9 months ago. i use to use ollama as my main way to inference llms. last i was working on them MCP was becoming the new way to have models connect with the real world. from my understanding it organized API calls but with a lot more usability. long story short. is MCP the standard for llms making api calls? it seemed promising at the time. any info would be greatly appreciated, thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rhaastt-ai"&gt; /u/rhaastt-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nembeg/did_mcp_become_usefull/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nembeg/did_mcp_become_usefull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nembeg/did_mcp_become_usefull/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-11T22:17:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1negg4w</id>
    <title>Introducing Ally, an open source CLI assistant</title>
    <updated>2025-09-11T18:24:40+00:00</updated>
    <author>
      <name>/u/YassinK97</name>
      <uri>https://old.reddit.com/user/YassinK97</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1negg4w/introducing_ally_an_open_source_cli_assistant/"&gt; &lt;img alt="Introducing Ally, an open source CLI assistant" src="https://external-preview.redd.it/uwkmfNcDLcZDlQ8FYiWmiighX4Q13I5okEpaYg1NwcY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=726d52086d0a6f2f9f545a19b8caab3e4fb43a58" title="Introducing Ally, an open source CLI assistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d9f9kw2uvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a11406fa1b82a2c11d87a83923206dc663f3dcaf"&gt;https://preview.redd.it/d9f9kw2uvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a11406fa1b82a2c11d87a83923206dc663f3dcaf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/YassWorks/Ally"&gt;Ally&lt;/a&gt; is a CLI multi-agent assistant that can assist with coding, searching and running commands.&lt;/p&gt; &lt;p&gt;I made this tool because I wanted to make agents with Ollama models but then added support for OpenAI, Anthropic, Gemini (Google Gen AI) and Cerebras for more flexibility.&lt;/p&gt; &lt;p&gt;What makes Ally special is that It can be 100% local and private. A law firm or a lab could run this on a server and benefit from all the things tools like Claude Code and Gemini Code have to offer. Itâ€™s also designed to understand context (by not feeding entire history and irrelevant tool calls to the LLM) and use tokens efficiently, providing a reliable, hallucination-free experience even on smaller models.&lt;/p&gt; &lt;p&gt;While still in its early stages, Ally provides a vibe coding framework that goes through brainstorming and coding phases with all under human supervision.&lt;/p&gt; &lt;p&gt;I intend to more features (one coming soon is RAG) but preferred to post about it at this stage for some feedback and visibility.&lt;/p&gt; &lt;p&gt;Give it a go: &lt;a href="https://github.com/YassWorks/Ally"&gt;https://github.com/YassWorks/Ally&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More screenshots:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zyl96inuvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de9cf20053ff2e5f890ea2e7fc9dc668600a263a"&gt;https://preview.redd.it/zyl96inuvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de9cf20053ff2e5f890ea2e7fc9dc668600a263a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8wp9awvuvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b7104092cd3020b43162082000ce2d8f77dabe5"&gt;https://preview.redd.it/8wp9awvuvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b7104092cd3020b43162082000ce2d8f77dabe5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YassinK97"&gt; /u/YassinK97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1negg4w/introducing_ally_an_open_source_cli_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1negg4w/introducing_ally_an_open_source_cli_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1negg4w/introducing_ally_an_open_source_cli_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-11T18:24:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1neupwp</id>
    <title>Recommended model for lightweight text tagging</title>
    <updated>2025-09-12T05:17:37+00:00</updated>
    <author>
      <name>/u/xegoba7006</name>
      <uri>https://old.reddit.com/user/xegoba7006</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I don't know much about LLMS so I'm looking for somebody with more experience to recommend me a model for a side project of mine.&lt;/p&gt; &lt;p&gt;I need something super lightweight (as it's running on a cheap hetzner VPS). &lt;/p&gt; &lt;p&gt;The use case is also pretty simple: I want to feed it some text (Just a couple sentences, nothing long) and get some recommended categories/labels/tags for the given text.&lt;/p&gt; &lt;p&gt;What would you recommend?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xegoba7006"&gt; /u/xegoba7006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neupwp/recommended_model_for_lightweight_text_tagging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neupwp/recommended_model_for_lightweight_text_tagging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1neupwp/recommended_model_for_lightweight_text_tagging/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T05:17:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf18q3</id>
    <title>Llama Builds is now in beta! PcPartPicker for Local AI Builds</title>
    <updated>2025-09-12T11:58:42+00:00</updated>
    <author>
      <name>/u/Vegetable_Low2907</name>
      <uri>https://old.reddit.com/user/Vegetable_Low2907</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nf18q3/llama_builds_is_now_in_beta_pcpartpicker_for/"&gt; &lt;img alt="Llama Builds is now in beta! PcPartPicker for Local AI Builds" src="https://b.thumbs.redditmedia.com/piGiXthvayvHA3TydCUURjfU2oa57pJxlMBT0HnK9oo.jpg" title="Llama Builds is now in beta! PcPartPicker for Local AI Builds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Low2907"&gt; /u/Vegetable_Low2907 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nersq0/llama_builds_is_now_in_beta_pcpartpicker_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nf18q3/llama_builds_is_now_in_beta_pcpartpicker_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nf18q3/llama_builds_is_now_in_beta_pcpartpicker_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T11:58:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf3pbh</id>
    <title>Recommendations On Model For Journal Style Writing</title>
    <updated>2025-09-12T13:46:57+00:00</updated>
    <author>
      <name>/u/Extra_Upstairs4075</name>
      <uri>https://old.reddit.com/user/Extra_Upstairs4075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All, found some time today to do something I've been wanting to do for a while now. Download and setup MSTY and also Ollama now it has a UI. So far so good. One of the main tasks I was wanting to complete was to take many, many pages of daily notes, written in dot points, and run them through AI to turn them into paragraph style notes / journal entries.&lt;/p&gt; &lt;p&gt;I tested this with with ChatGPT some time ago and was surprised how well it worked, though, I would like to complete this on a local AI. So - I have Qwen3 and DeepSeek R1 models running. I gave both of these a daily section of dot points to write into a paragraph style journal entry, they both seemed relatively average, they both completely added in bits that didn't exist in the summary I provided.&lt;/p&gt; &lt;p&gt;My question, as somebody new to this - there's so many models available, is there any that could be recommended for my use case? Is there any recommendations I could try to improve the answers I receive?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extra_Upstairs4075"&gt; /u/Extra_Upstairs4075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nf3pbh/recommendations_on_model_for_journal_style_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nf3pbh/recommendations_on_model_for_journal_style_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nf3pbh/recommendations_on_model_for_journal_style_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T13:46:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfgehx</id>
    <title>Small model recommendation for evaluating web data</title>
    <updated>2025-09-12T22:06:55+00:00</updated>
    <author>
      <name>/u/Busy_Satisfaction791</name>
      <uri>https://old.reddit.com/user/Busy_Satisfaction791</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Guys, I've been looking for some small models that can run on my MBA M1 16GB Ram with Browser use to play around with AI test automation. &lt;/p&gt; &lt;p&gt;So far, the ones that gives hope are Qwen2.5-Coder-3B it and Qwen2.5-Coder-7B it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Busy_Satisfaction791"&gt; /u/Busy_Satisfaction791 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfgehx/small_model_recommendation_for_evaluating_web_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfgehx/small_model_recommendation_for_evaluating_web_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nfgehx/small_model_recommendation_for_evaluating_web_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T22:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nezvor</id>
    <title>I made script to allow an Ollama server to be ran off of kaggle with a Ngrok domain.</title>
    <updated>2025-09-12T10:46:18+00:00</updated>
    <author>
      <name>/u/jam06452</name>
      <uri>https://old.reddit.com/user/jam06452</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I made a Kaggle script that sets up an Ollama server with GPU acceleration, this is amazing for 30 hours/week of Kaggle GPU time for free.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Installs CUDA + dependencies&lt;/li&gt; &lt;li&gt;Downloads the latest Ollama (since it doesnâ€™t persist on Kaggle)&lt;/li&gt; &lt;li&gt;Serves the API with ngrok&lt;/li&gt; &lt;li&gt;Installs two models: &lt;code&gt;deepseek-r1:14b&lt;/code&gt; and &lt;code&gt;qwen3-coder:30b&lt;/code&gt; &lt;em&gt;(You can swap these outâ€”just keep total size under ~30GB for 2Ã—T4s)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Once itâ€™s running, you can use the API from your terminal or even connect it to an Open-webui instance in the cloud, like myself.&lt;/p&gt; &lt;p&gt;It uses an ngrok tunnel since kaggle provides random IPv4s every time. It's easier to use with a static domain&lt;/p&gt; &lt;p&gt;GitHub link: &lt;a href="https://github.com/jam06452/Ollama-Server-on-Kaggle"&gt;https://github.com/jam06452/Ollama-Server-on-Kaggle&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback or ideas for other models to try!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jam06452"&gt; /u/jam06452 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nezvor/i_made_script_to_allow_an_ollama_server_to_be_ran/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nezvor/i_made_script_to_allow_an_ollama_server_to_be_ran/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nezvor/i_made_script_to_allow_an_ollama_server_to_be_ran/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T10:46:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfdj7j</id>
    <title>Is there a way to enable mfa on OpenWebUI?</title>
    <updated>2025-09-12T20:13:13+00:00</updated>
    <author>
      <name>/u/Dense-Land-5927</name>
      <uri>https://old.reddit.com/user/Dense-Land-5927</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I am in the process of seeing about implementing Ollama where I work. However, after messing around with Ollama and the OpenWebUI, I cannot for the life of me find where you can activate mfa easily. &lt;/p&gt; &lt;p&gt;I saw another post on another website where someone said &amp;quot;It's in the settings&amp;quot; but no matter where I go in OpenWebUI, I don't have a setting where it says &amp;quot;turn on MFA.&amp;quot;&lt;/p&gt; &lt;p&gt;Any help would be nice. Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense-Land-5927"&gt; /u/Dense-Land-5927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfdj7j/is_there_a_way_to_enable_mfa_on_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfdj7j/is_there_a_way_to_enable_mfa_on_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nfdj7j/is_there_a_way_to_enable_mfa_on_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T20:13:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1neyrkj</id>
    <title>Best LLM for my laptop</title>
    <updated>2025-09-12T09:38:53+00:00</updated>
    <author>
      <name>/u/Silly_Bad_7692</name>
      <uri>https://old.reddit.com/user/Silly_Bad_7692</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys! I've a thinkpad x1 carbon G9 (i7 1165G7, 32GB ram) and I was wondering what's the best LLM I can run on my pc. I'm new to local LLM and ollama so please be kind with me!&lt;/p&gt; &lt;p&gt;Also I would like to run it with a GUI. How can I do it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silly_Bad_7692"&gt; /u/Silly_Bad_7692 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neyrkj/best_llm_for_my_laptop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neyrkj/best_llm_for_my_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1neyrkj/best_llm_for_my_laptop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T09:38:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfbba3</id>
    <title>Ollama integration!!</title>
    <updated>2025-09-12T18:45:49+00:00</updated>
    <author>
      <name>/u/Direct_Effort_4892</name>
      <uri>https://old.reddit.com/user/Direct_Effort_4892</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nfbba3/ollama_integration/"&gt; &lt;img alt="Ollama integration!!" src="https://external-preview.redd.it/6YKpG1RrqJFlYrGZQcvKrQrg8zYisa5ZdVMLcKEnEJg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ce98aaadd3050590046aac15357e304aa66ed43" title="Ollama integration!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Direct_Effort_4892"&gt; /u/Direct_Effort_4892 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Animesh-Varma/Mythryl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfbba3/ollama_integration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nfbba3/ollama_integration/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T18:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1neupyc</id>
    <title>Gpt oss 20b ft 3090 in proxmox</title>
    <updated>2025-09-12T05:17:40+00:00</updated>
    <author>
      <name>/u/LeftelfinX</name>
      <uri>https://old.reddit.com/user/LeftelfinX</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1neupyc/gpt_oss_20b_ft_3090_in_proxmox/"&gt; &lt;img alt="Gpt oss 20b ft 3090 in proxmox" src="https://external-preview.redd.it/YW5vNDZsYmM0b29mMSNQ3BfJaOBXHJDEKG-O492PgD5xrAutyBU3r3wb6pnX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b212bd16cc2038bb8d505e8035b4b599832749b0" title="Gpt oss 20b ft 3090 in proxmox" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just installed a 3090 which I got for 450$ into my proxmox server and viola that's another tier of perfomance unlocked. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeftelfinX"&gt; /u/LeftelfinX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dx9re49c4oof1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neupyc/gpt_oss_20b_ft_3090_in_proxmox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1neupyc/gpt_oss_20b_ft_3090_in_proxmox/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T05:17:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng02h7</id>
    <title>So many models...confused how to pick the right one. Need one to help fix English grammar and text.</title>
    <updated>2025-09-13T15:16:25+00:00</updated>
    <author>
      <name>/u/MoChuang</name>
      <uri>https://old.reddit.com/user/MoChuang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am working on a project that needs a step to fix some closed captioning text to make it more coherent. Example input and output text below. I have a laptop with an RTX 3050 4GB so the models I can run are pretty limited but I think it is still sufficient for what I need. I've tried qwen2.5:1.5b-instruct-q4_K_M and qwen2.5:3b-instruct-q4_K_M mostly so far. I am going to start testing some phi, gemma, and llama models as well. But there are so many versions, sizes, and quantizations its kind of overwhelming.&lt;/p&gt; &lt;p&gt;For example, Gemma3 is newer and better than Gemma2, but on my GPU I have to choose between Gemma3:1b and Gemma2:2b, and generally 2b is better than 1b...so in my case which option is actually better? I know ultimately I need to test things myself to see which I am more satisfied with, but is there some logical reasoning I can do to at least narrow down the possible options to a handful that should work better before embarking on all this testing?&lt;/p&gt; &lt;p&gt;Example input text:&lt;/p&gt; &lt;p&gt;|| || |All right, I'm goingAllAll right, I'm going to get started with a question for the three of our panelists who are older and You've all been in the field You've all You've all been in the field for a lifetime. Here's Here's my question, because there's a lot of younger people in this room. What Expected What are the things that you thought? Expect|&lt;/p&gt; &lt;p&gt;Prompt used for qwen2.5:3b-instruct-q4_K_M:&lt;/p&gt; &lt;p&gt;|| || |Remove repeated words and phrases from the following sentences. Make the sentences grammatically correct, but do not add, remove, or change the meaning of the text: {text}|&lt;/p&gt; &lt;p&gt;Corrected output:&lt;/p&gt; &lt;p&gt;|| || |All right, I'm going to get started with a question for the three of our older panelists. You've all been in the field for a lifetime. Here's my question, because there are a lot of younger people in this room. What are the things that you expected and believed?|&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoChuang"&gt; /u/MoChuang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ng02h7/so_many_modelsconfused_how_to_pick_the_right_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ng02h7/so_many_modelsconfused_how_to_pick_the_right_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ng02h7/so_many_modelsconfused_how_to_pick_the_right_one/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-13T15:16:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ngrits</id>
    <title>Comment utiliser le GPU ?</title>
    <updated>2025-09-14T13:37:17+00:00</updated>
    <author>
      <name>/u/Maleficent-Hotel8207</name>
      <uri>https://old.reddit.com/user/Maleficent-Hotel8207</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ngrits/comment_utiliser_le_gpu/"&gt; &lt;img alt="Comment utiliser le GPU ?" src="https://preview.redd.it/xb4qwochv4pf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e16ed4fdbfab7be2cfc1832b6e804dfecad39491" title="Comment utiliser le GPU ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Comment utiliser le GPU sur ollama jâ€™ai une GTX 1050 et je nâ€™arrive pas Ã  lâ€™utiliser pour exÃ©cuter des modÃ¨les &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Hotel8207"&gt; /u/Maleficent-Hotel8207 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xb4qwochv4pf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ngrits/comment_utiliser_le_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ngrits/comment_utiliser_le_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-14T13:37:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfytlv</id>
    <title>Gemma 3 12B versus GPT 5 Nano</title>
    <updated>2025-09-13T14:25:54+00:00</updated>
    <author>
      <name>/u/fundal_alb</name>
      <uri>https://old.reddit.com/user/fundal_alb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is just me or that Gemma version is better or equal to GPT 5 Nano?&lt;/p&gt; &lt;p&gt;In my case...:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nano is responding with the first token after 6-10 seconds&lt;/li&gt; &lt;li&gt;Gemma has better language understanding than 5 Nano.&lt;/li&gt; &lt;li&gt;Gemma is structuring the output in a more readable way&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fundal_alb"&gt; /u/fundal_alb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfytlv/gemma_3_12b_versus_gpt_5_nano/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfytlv/gemma_3_12b_versus_gpt_5_nano/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nfytlv/gemma_3_12b_versus_gpt_5_nano/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-13T14:25:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng9tdq</id>
    <title>Ollama start all models on CPU instead GPU [Arch/Nvidia]</title>
    <updated>2025-09-13T21:52:04+00:00</updated>
    <author>
      <name>/u/MrDoc79</name>
      <uri>https://old.reddit.com/user/MrDoc79</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ng9tdq/ollama_start_all_models_on_cpu_instead_gpu/"&gt; &lt;img alt="Ollama start all models on CPU instead GPU [Arch/Nvidia]" src="https://b.thumbs.redditmedia.com/t2wX7z3e3ylFI8LXPaCJf9wbkIBJekACEeCKiiZ4G4Y.jpg" title="Ollama start all models on CPU instead GPU [Arch/Nvidia]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Idk why, but all models, what i started, are running on CPU, and, had small speed for generate answer. However, nvidia-smi works, and driver is available. I'm on EndeavourOS (Arch-based), with RTX 2060 on 6gb. All screenshots pinned&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrDoc79"&gt; /u/MrDoc79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ng9tdq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ng9tdq/ollama_start_all_models_on_cpu_instead_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ng9tdq/ollama_start_all_models_on_cpu_instead_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-13T21:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhhdac</id>
    <title>GPT-OSS-120B Performance Benchmarks and Provider Trade-Offs</title>
    <updated>2025-09-15T09:40:37+00:00</updated>
    <author>
      <name>/u/Sumanth_077</name>
      <uri>https://old.reddit.com/user/Sumanth_077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking at the latest Artificial Analysis benchmarks for GPT-OSS-120B and noticed some interesting differences between providers, especially for those using it in production.&lt;/p&gt; &lt;p&gt;Time to first token (TTFT) ranges from under 0.3 seconds to nearly a second depending on the provider. That can be significant for applications where responsiveness matters. Throughput also varies, from under 200 tokens per second to over 400.&lt;/p&gt; &lt;p&gt;Cost per million tokens adds another consideration. Some providers offer high throughput at a higher cost, while others like CompactifAI are cheaper but very slower. Clarifai, for example, delivers low TTFT, solid throughput, and relatively low cost.&lt;/p&gt; &lt;p&gt;The takeaway is that no single metric tells the full story. Latency affects responsiveness, throughput matters for larger tasks, and cost impacts scaling. The best provider depends on which of these factors is most important for your use case.&lt;/p&gt; &lt;p&gt;For those using GPT-OSS-120B in production, which of these do you find the hardest to manage: step latency, throughput, or cost?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/92e9yu2ctapf1.png?width=3408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ff0bd76c8ac202aad4c9e453dc975c7e67cfa6d7"&gt;https://preview.redd.it/92e9yu2ctapf1.png?width=3408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ff0bd76c8ac202aad4c9e453dc975c7e67cfa6d7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sumanth_077"&gt; /u/Sumanth_077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhhdac/gptoss120b_performance_benchmarks_and_provider/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhhdac/gptoss120b_performance_benchmarks_and_provider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nhhdac/gptoss120b_performance_benchmarks_and_provider/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-15T09:40:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhl7h0</id>
    <title>Advice on building an enterprise-scale, privacy-first conversational assistant (local LLMs with Ollama vs fine-tuning)</title>
    <updated>2025-09-15T13:01:05+00:00</updated>
    <author>
      <name>/u/jamalhassouni</name>
      <uri>https://old.reddit.com/user/jamalhassouni</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Iâ€™m working on a project to design a &lt;strong&gt;conversational AI assistant for employee well-being and productivity&lt;/strong&gt; inside a large enterprise (think thousands of staff, high compliance/security requirements). The assistant should provide personalized nudges, lightweight recommendations, and track anonymized engagement data â€” without sending sensitive data outside the organization.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key constraints:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Must be &lt;strong&gt;privacy-first&lt;/strong&gt; (local deployment or private cloud â€” no SaaS APIs).&lt;/li&gt; &lt;li&gt;Needs to support &lt;strong&gt;personalized recommendations&lt;/strong&gt; and &lt;strong&gt;ongoing employee state tracking&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Must handle &lt;strong&gt;enterprise scale&lt;/strong&gt; (hundredsâ€“thousands of concurrent users).&lt;/li&gt; &lt;li&gt;Regulatory requirements: &lt;strong&gt;PII protection, anonymization, auditability&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What Iâ€™d love advice on:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Local LLM deployment&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Is using &lt;strong&gt;Ollama with models like Gemma/MedGemma&lt;/strong&gt; a solid foundation for production at enterprise scale?&lt;/li&gt; &lt;li&gt;What are the pros/cons of Ollama vs more MLOps-oriented solutions (vLLM, TGI, LM Studio, custom Dockerized serving)?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model strategy: RAG vs fine-tuning&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;For delivering contextual, evolving guidance: would you start with &lt;strong&gt;RAG (vector DB + retrieval)&lt;/strong&gt; or jump straight into &lt;strong&gt;fine-tuning a domain model&lt;/strong&gt;?&lt;/li&gt; &lt;li&gt;Any rule of thumb on when fine-tuning becomes necessary in real-world enterprise use cases?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model choice&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Experiences with &lt;strong&gt;Gemma/MedGemma&lt;/strong&gt; or other open-source models for well-being / health-adjacent guidance?&lt;/li&gt; &lt;li&gt;Alternatives youâ€™d recommend (Mistral, LLaMA 3, Phi-3, Qwen, etc.) in terms of reasoning, safety, and multilingual support?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Infrastructure &amp;amp; scaling&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Minimum GPU/CPU/RAM targets to support &lt;strong&gt;hundreds of concurrent chats&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Vector DB choices: FAISS, Milvus, Weaviate, Pinecone â€” what works best at enterprise scale?&lt;/li&gt; &lt;li&gt;Monitoring, evaluation, and safe deployment patterns (A/B testing, hallucination mitigation, guardrails).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Security &amp;amp; compliance&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Best practices to prevent &lt;strong&gt;PII leakage into embeddings/prompts&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Recommended architectures for &lt;strong&gt;GDPR/HIPAA-like compliance&lt;/strong&gt; when dealing with well-being data.&lt;/li&gt; &lt;li&gt;Any proven strategies to balance personalization with strict privacy requirements?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation &amp;amp; KPIs&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;How to measure assistant effectiveness (safety checks, employee satisfaction, retention impact).&lt;/li&gt; &lt;li&gt;Tooling for anonymized analytics dashboards at the org level.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jamalhassouni"&gt; /u/jamalhassouni &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhl7h0/advice_on_building_an_enterprisescale/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhl7h0/advice_on_building_an_enterprisescale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nhl7h0/advice_on_building_an_enterprisescale/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-15T13:01:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhkjk8</id>
    <title>What are the ways to use Ollama 120B without breaking the bank?</title>
    <updated>2025-09-15T12:32:02+00:00</updated>
    <author>
      <name>/u/Significant_Loss_541</name>
      <uri>https://old.reddit.com/user/Significant_Loss_541</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hello, i have been looking into running the ollama 120b model for a project, but honestly the hardware/hosting side looks kinda tough to setup for me. i really dont want to set up big servers or spend a lot initially just to try it out.&lt;/p&gt; &lt;p&gt;are there any ways people here are running it cheaper? like cloud setups, colab hacks, lighter quantized versions, or anything similar?&lt;/p&gt; &lt;p&gt;also curious if it even makes sense to skip self-hosting and just use a service that already runs it (saw deepinfra has it with an api, and itâ€™s way less than openai prices but still not free). has anyone tried going that route vs rolling your own?&lt;/p&gt; &lt;p&gt;whatâ€™s the most practical way for someone who doesnâ€™t want to melt their credit card on gpu rentals?&lt;/p&gt; &lt;p&gt;thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Loss_541"&gt; /u/Significant_Loss_541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhkjk8/what_are_the_ways_to_use_ollama_120b_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhkjk8/what_are_the_ways_to_use_ollama_120b_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nhkjk8/what_are_the_ways_to_use_ollama_120b_without/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-15T12:32:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1nid7wt</id>
    <title>Model recommendation</title>
    <updated>2025-09-16T09:57:56+00:00</updated>
    <author>
      <name>/u/Honest_Adagio_1629</name>
      <uri>https://old.reddit.com/user/Honest_Adagio_1629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Suggest me a model or even a few which I can run on my laptop. The social science and literature use case is optional (just wanna try it out). Also how would you suggest I myself learn to assess a model? The day or two I've been using ollama with webui and cli, all I learnt was that I can't run mixtral on my system.&lt;/p&gt; &lt;p&gt;Tech spec: 16GB ram, Ryzen 7, rtx 3050 4GB and external harddrive for storage &lt;/p&gt; &lt;p&gt;Requirement: Uncensored (necessary) Use Case: - coding - engineering/science/math for assignments and stuff - creative writing (just wanna try it out) - social sciences (if there exists such a model &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest_Adagio_1629"&gt; /u/Honest_Adagio_1629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nid7wt/model_recommendation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nid7wt/model_recommendation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nid7wt/model_recommendation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T09:57:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1niendu</id>
    <title>how to make custom chatbot for my website</title>
    <updated>2025-09-16T11:19:31+00:00</updated>
    <author>
      <name>/u/Comfortable-Fan-8931</name>
      <uri>https://old.reddit.com/user/Comfortable-Fan-8931</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i am student ,&lt;br /&gt; how to make custom chatbot for my website . &lt;/p&gt; &lt;p&gt;when i ask question related to my website then, chatbot gives answer .&lt;br /&gt; And please suggest best approach and steps to create this chatbot&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Fan-8931"&gt; /u/Comfortable-Fan-8931 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1niendu/how_to_make_custom_chatbot_for_my_website/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1niendu/how_to_make_custom_chatbot_for_my_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1niendu/how_to_make_custom_chatbot_for_my_website/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T11:19:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhiixf</id>
    <title>Was working in RAG recently got to know how well Gemma3 4B performs</title>
    <updated>2025-09-15T10:49:31+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nhiixf/was_working_in_rag_recently_got_to_know_how_well/"&gt; &lt;img alt="Was working in RAG recently got to know how well Gemma3 4B performs" src="https://preview.redd.it/91u5300g6bpf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=fc7c57a6e1b897ac550462d08b77359bb65c2b95" title="Was working in RAG recently got to know how well Gemma3 4B performs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got this working and had to share because wtf, this tiny model is way better than expected.&lt;/p&gt; &lt;p&gt;Built a RAG system that renders docs as a knowledge graph you can actually navigate through. Using Gemma3 4B via Ollama and honestly shocked at how well it clusters related content.&lt;/p&gt; &lt;p&gt;The crazy part? Sub-200ms responses and the semantic relationships actually make sense. Running smooth on small GPU&lt;/p&gt; &lt;p&gt;Anyone else trying local models for RAG? Kinda nice not sending everything to OpenAI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/91u5300g6bpf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhiixf/was_working_in_rag_recently_got_to_know_how_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nhiixf/was_working_in_rag_recently_got_to_know_how_well/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-15T10:49:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nicqtd</id>
    <title>Need a simple UI/UX for chat (similar to OpenAI Chatgpt) using Ollama</title>
    <updated>2025-09-16T09:28:13+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Appreciate any advice. I ask chatgpt to create but not getting the right look. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nicqtd/need_a_simple_uiux_for_chat_similar_to_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nicqtd/need_a_simple_uiux_for_chat_similar_to_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nicqtd/need_a_simple_uiux_for_chat_similar_to_openai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T09:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni5cfx</id>
    <title>Fully local data analysis assistant (plus new Model)</title>
    <updated>2025-09-16T02:17:40+00:00</updated>
    <author>
      <name>/u/mshintaro777</name>
      <uri>https://old.reddit.com/user/mshintaro777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ni5cfx/fully_local_data_analysis_assistant_plus_new_model/"&gt; &lt;img alt="Fully local data analysis assistant (plus new Model)" src="https://preview.redd.it/ifula3tiqfpf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=e334cd9bdf8f0d006bb34767d9f02bebb39206c1" title="Fully local data analysis assistant (plus new Model)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mshintaro777"&gt; /u/mshintaro777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ifula3tiqfpf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ni5cfx/fully_local_data_analysis_assistant_plus_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ni5cfx/fully_local_data_analysis_assistant_plus_new_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T02:17:40+00:00</published>
  </entry>
</feed>
