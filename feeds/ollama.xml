<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-09T02:15:46+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1nagsmq</id>
    <title>codellama:python sentience</title>
    <updated>2025-09-07T01:40:40+00:00</updated>
    <author>
      <name>/u/Serious-One4553</name>
      <uri>https://old.reddit.com/user/Serious-One4553</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nagsmq/codellamapython_sentience/"&gt; &lt;img alt="codellama:python sentience" src="https://b.thumbs.redditmedia.com/1PVMn2SyHmQ1GncahZbgo_CFL2PzeFpTI93xIeerQDc.jpg" title="codellama:python sentience" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7xs46wd6dnnf1.png?width=1441&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c20b3b8d6a6f03ea44f2851e1a540a6d6a1de4d"&gt;https://preview.redd.it/7xs46wd6dnnf1.png?width=1441&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c20b3b8d6a6f03ea44f2851e1a540a6d6a1de4d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5qwtemq8dnnf1.png?width=1214&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d2df52d739c0cb9f7b56c5a83c8db99330ea842"&gt;https://preview.redd.it/5qwtemq8dnnf1.png?width=1214&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d2df52d739c0cb9f7b56c5a83c8db99330ea842&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/42tdzuccmnnf1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b15670ed51271ff90fa522effee688f22f17424b"&gt;https://preview.redd.it/42tdzuccmnnf1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b15670ed51271ff90fa522effee688f22f17424b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;might be an issue, i was messing around with some llms and this one caught my eye with weird messages about life&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious-One4553"&gt; /u/Serious-One4553 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nagsmq/codellamapython_sentience/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nagsmq/codellamapython_sentience/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nagsmq/codellamapython_sentience/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T01:40:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1na72g3</id>
    <title>Sometimes I prefer to use my own chatbot over ChatGPT because the answers are faster. Not always better, but faster ‚úåÔ∏èüòä‚ú® (WIP: every day getting better and better)</title>
    <updated>2025-09-06T18:33:34+00:00</updated>
    <author>
      <name>/u/Roseldine</name>
      <uri>https://old.reddit.com/user/Roseldine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1na72g3/sometimes_i_prefer_to_use_my_own_chatbot_over/"&gt; &lt;img alt="Sometimes I prefer to use my own chatbot over ChatGPT because the answers are faster. Not always better, but faster ‚úåÔ∏èüòä‚ú® (WIP: every day getting better and better)" src="https://preview.redd.it/l3uewsvy8lnf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c39cbf5306fb5afac362883306c0c197e38f2abe" title="Sometimes I prefer to use my own chatbot over ChatGPT because the answers are faster. Not always better, but faster ‚úåÔ∏èüòä‚ú® (WIP: every day getting better and better)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roseldine"&gt; /u/Roseldine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l3uewsvy8lnf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na72g3/sometimes_i_prefer_to_use_my_own_chatbot_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na72g3/sometimes_i_prefer_to_use_my_own_chatbot_over/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T18:33:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1na96qh</id>
    <title>MoE models tested on miniPC iGPU with Vulkan</title>
    <updated>2025-09-06T19:58:27+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1na96gx/moe_models_tested_on_minipc_igpu_with_vulkan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na96qh/moe_models_tested_on_minipc_igpu_with_vulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na96qh/moe_models_tested_on_minipc_igpu_with_vulkan/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T19:58:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nadynk</id>
    <title>Using the Ollama Client App with Rag Chain</title>
    <updated>2025-09-06T23:24:11+00:00</updated>
    <author>
      <name>/u/tintires</name>
      <uri>https://old.reddit.com/user/tintires</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive built s simple RAG agent with LangGraph (gpt-oss:20b) in a Jupyter notebook. Works great. How might I expose this (perhaps as a macos service) to use it with the Ollama desktop app? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tintires"&gt; /u/tintires &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nadynk/using_the_ollama_client_app_with_rag_chain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nadynk/using_the_ollama_client_app_with_rag_chain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nadynk/using_the_ollama_client_app_with_rag_chain/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T23:24:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nan443</id>
    <title>Vibe coded Ollama integration into my game server and Discord</title>
    <updated>2025-09-07T07:36:10+00:00</updated>
    <author>
      <name>/u/pdxmichael</name>
      <uri>https://old.reddit.com/user/pdxmichael</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nan443/vibe_coded_ollama_integration_into_my_game_server/"&gt; &lt;img alt="Vibe coded Ollama integration into my game server and Discord" src="https://b.thumbs.redditmedia.com/vCD8QWREHKU3HMLEuy8l5HS8v-_uMh7XhpufYEEMKkg.jpg" title="Vibe coded Ollama integration into my game server and Discord" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven‚Äôt been this into something in a while, and it‚Äôs so much fun getting things to work like they do!&lt;/p&gt; &lt;p&gt;I have the discord bot in python use Rcon to pass messages between ollama and my 7 days to die game server. I‚Äôve created a model file that has all my game server rules, links and info and a few jokes to go with it. I then use the model file to modify Llava:7b and gave it some pretty straightforward guardrails. &lt;/p&gt; &lt;p&gt;Works awesome so far, it‚Äôs like an always online admin. Any suggestions to make it better? Literally JUST getting into this as of a few days ago after getting ollama all setup with openwebui. Curious if they‚Äôre any models better for what I need or a better way to do what I‚Äôm doing.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pdxmichael"&gt; /u/pdxmichael &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nan443"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nan443/vibe_coded_ollama_integration_into_my_game_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nan443/vibe_coded_ollama_integration_into_my_game_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T07:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1na94xm</id>
    <title>Seven Hours, Zero Internet, and Local AI Coding at 40,000 Feet</title>
    <updated>2025-09-06T19:56:24+00:00</updated>
    <author>
      <name>/u/scastiel</name>
      <uri>https://old.reddit.com/user/scastiel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last Monday, I tried vibe-coding an app from the plane yesterday with no internet, and it worked‚Ä¶ kind of! üòÖ&lt;/p&gt; &lt;p&gt;But it was fun to try Ollama for the first time, and pretty encouraging for what's next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/scastiel"&gt; /u/scastiel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://betweentheprompts.com/40000-feet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na94xm/seven_hours_zero_internet_and_local_ai_coding_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na94xm/seven_hours_zero_internet_and_local_ai_coding_at/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T19:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nasyt8</id>
    <title>Best way to use my AMD 6650 XT</title>
    <updated>2025-09-07T13:18:18+00:00</updated>
    <author>
      <name>/u/Sad-Championship9167</name>
      <uri>https://old.reddit.com/user/Sad-Championship9167</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And yes I know get a better card is the correct answer, but it fits my gaming needs and I don't feel like dropping a bunch on a 5070ti or something like that. I read the AMD 6650XT can be used with Rocm if you change a config. I tried that but ran into DKMS permission issues during installation i could not get around and had to rollback to a snapshot. Would Vulkan be good? and if Vulkan what fork? or try the rocm again and is there a fix for dkms error code 1? Thanks for any help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad-Championship9167"&gt; /u/Sad-Championship9167 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nasyt8/best_way_to_use_my_amd_6650_xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nasyt8/best_way_to_use_my_amd_6650_xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nasyt8/best_way_to_use_my_amd_6650_xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T13:18:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3cy2</id>
    <title>MCP with Computer Use</title>
    <updated>2025-09-06T16:06:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1na3cy2/mcp_with_computer_use/"&gt; &lt;img alt="MCP with Computer Use" src="https://external-preview.redd.it/eTJsbWFwZXJpa25mMRWfbJ8igMn1wejE5S1jscBm4hQQlwhyyetxPIo-3e1f.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=827589a323b1b6502b4bf3948d377b57f190604b" title="MCP with Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MCP Server with Computer Use Agent runs through Claude Desktop, Cursor, and other MCP clients.&lt;/p&gt; &lt;p&gt;An example use case lets try using Claude as a tutor to learn how to use Tableau.&lt;/p&gt; &lt;p&gt;The MCP Server implementation exposes CUA's full functionality through standardized tool calls. It supports single-task commands and multi-task sequences, giving Claude Desktop direct access to all of Cua's computer control capabilities.&lt;/p&gt; &lt;p&gt;This is the first MCP-compatible computer control solution that works directly with Claude Desktop's and Cursor's built-in MCP implementation. Simple configuration in your claude_desktop_config.json or cursor_config.json connects Claude or Cursor directly to your desktop environment.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.gg/4fuebBsAUj"&gt;https://discord.gg/4fuebBsAUj&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4z4enlqriknf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na3cy2/mcp_with_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na3cy2/mcp_with_computer_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T16:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nave28</id>
    <title>Can't get models to create "real person" like text.</title>
    <updated>2025-09-07T14:59:00+00:00</updated>
    <author>
      <name>/u/Nearby_You_313</name>
      <uri>https://old.reddit.com/user/Nearby_You_313</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to make a prompt that generates &amp;quot;real person&amp;quot; style text. I tried my own prompts, a megaprompt generator telling it to generate different styles, voices, etc. I've tried a variety of models. It all sounds like a bad author trying to write their first novel.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I mean:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Real person: &amp;quot;My manager called me into his office the moment I got to work&amp;quot;&lt;/p&gt; &lt;p&gt;What they all do: &amp;quot;I walked into that office not knowing what awaited me. It was a regular Monday, but as I settled at my desk, I couldn't help but feel something was off. The manager, who seemed overly cheerful, called me into his office.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nearby_You_313"&gt; /u/Nearby_You_313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nave28/cant_get_models_to_create_real_person_like_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nave28/cant_get_models_to_create_real_person_like_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nave28/cant_get_models_to_create_real_person_like_text/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T14:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1narbrw</id>
    <title>Hotel Rooms Booking Bot</title>
    <updated>2025-09-07T11:59:23+00:00</updated>
    <author>
      <name>/u/anuraginsg</name>
      <uri>https://old.reddit.com/user/anuraginsg</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anuraginsg"&gt; /u/anuraginsg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wyo3sfa06jnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1narbrw/hotel_rooms_booking_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1narbrw/hotel_rooms_booking_bot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T11:59:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb0o2u</id>
    <title>Ollama Crashing on RTX 3080 Ti with CUDA Error After Update (Models Freeze PC)</title>
    <updated>2025-09-07T18:22:44+00:00</updated>
    <author>
      <name>/u/Mundane_Education654</name>
      <uri>https://old.reddit.com/user/Mundane_Education654</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I need some help.&lt;/p&gt; &lt;p&gt;I‚Äôm running Ollama on my PC (Windows, 16GB RAM, RTX 3080 Ti). The issue is that whenever I run a model like gemma:4b, it loads fine, but as soon as I send a query and the model starts responding, my system goes black screen and freezes until I force restart.&lt;/p&gt; &lt;p&gt;Here‚Äôs what I tried: - I updated Ollama to the latest version ‚Üí crash started. - I even tried rolling back Ollama to older versions, but the crash still happens. - Updated NVIDIA drivers ‚Üí no change (crash still there). - I also reset my whole PC, installed fresh Ollama and drivers ‚Üí still the same crash. - Tried both Gemma and LLaMA models, both crash in the same way. - Error message I sometimes see: ‚ÄúError: an error was encountered while running the model: CUDA error‚Äù.&lt;/p&gt; &lt;p&gt;Important points: - Previously even bigger models like 14B worked fine on my PC without any issue. - Now even smaller ones (4B) crash instantly. - Games and other GPU apps work fine, so it seems specific to Ollama GPU usage.&lt;/p&gt; &lt;p&gt;Does anyone know if this is a known issue with RTX 30 series (3080 Ti) and recent Ollama updates?&lt;/p&gt; &lt;p&gt;Any advice would be really appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane_Education654"&gt; /u/Mundane_Education654 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb0o2u/ollama_crashing_on_rtx_3080_ti_with_cuda_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb0o2u/ollama_crashing_on_rtx_3080_ti_with_cuda_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb0o2u/ollama_crashing_on_rtx_3080_ti_with_cuda_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T18:22:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1naqzdv</id>
    <title>Best sub-15B Ollama model for fact extraction?</title>
    <updated>2025-09-07T11:39:55+00:00</updated>
    <author>
      <name>/u/NenntronReddit</name>
      <uri>https://old.reddit.com/user/NenntronReddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm building a pipeline that extracts facts from transcribed dialogues/chat logs and stores them in a RAG index. &lt;/p&gt; &lt;p&gt;Inputs can be long (up to ~32k tokens). &lt;/p&gt; &lt;p&gt;Now I'm looking for a lightweight Ollama model that‚Äôs fast on an RTX 5070 Ti + 32 GB RAM but still accurate and stable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; Language German and English, I prefer &lt;strong&gt;4‚Äì7B&lt;/strong&gt; (ok up to &lt;strong&gt;15B&lt;/strong&gt;), good instruction following, low hallucinations.&lt;/p&gt; &lt;p&gt;What models do you recommend and how would you rank them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NenntronReddit"&gt; /u/NenntronReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1naqzdv/best_sub15b_ollama_model_for_fact_extraction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1naqzdv/best_sub15b_ollama_model_for_fact_extraction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1naqzdv/best_sub15b_ollama_model_for_fact_extraction/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T11:39:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nba39m</id>
    <title>EVO-X2 linux users help</title>
    <updated>2025-09-08T00:55:40+00:00</updated>
    <author>
      <name>/u/draeician</name>
      <uri>https://old.reddit.com/user/draeician</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can install ollama on my linux mint machine, but it has been a nightmare trying to make it stable. So far it works fine in the REPL, but when I use python to hit it I get random 500 status errors, and the ollama serve restarts. I started a thread on the ollama discord, but thought I'd throw a line out here to see if anyone is using ollama on this hardware. And if you even have this hardware, what are you using on it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/draeician"&gt; /u/draeician &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nba39m/evox2_linux_users_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nba39m/evox2_linux_users_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nba39m/evox2_linux_users_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T00:55:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1napi4k</id>
    <title>Just got ollama and gpt-oss:20b running!</title>
    <updated>2025-09-07T10:10:43+00:00</updated>
    <author>
      <name>/u/chr0n1c843</name>
      <uri>https://old.reddit.com/user/chr0n1c843</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just installed ollama and gpt-oss:20b on my hp dl380 with dual xeons, 192gb ram, a 3tb raid array, redundant power supplies and a gt1030 (no cuda setup yet, cpus only)... neat stuff, below is the first run!&lt;/p&gt; &lt;p&gt;pulling manifest &lt;/p&gt; &lt;p&gt;pulling b112e727c6f1: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 13 GB &lt;/p&gt; &lt;p&gt;pulling fa6710a93d78: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.2 KB &lt;/p&gt; &lt;p&gt;pulling f60356777647: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 11 KB &lt;/p&gt; &lt;p&gt;pulling d8ba2f9a17b3: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 18 B &lt;/p&gt; &lt;p&gt;pulling 55c108d8e936: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 489 B &lt;/p&gt; &lt;p&gt;verifying sha256 digest &lt;/p&gt; &lt;p&gt;writing manifest &lt;/p&gt; &lt;p&gt;success &lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; hello buddy&lt;/p&gt; &lt;p&gt;Thinking...&lt;/p&gt; &lt;p&gt;The user says &amp;quot;hello buddy&amp;quot;. Likely wants friendly response. Simple. Let's respond.&lt;/p&gt; &lt;p&gt;...done thinking.&lt;/p&gt; &lt;p&gt;Hey there! How can I help you today?&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; do you like your new home on my hp dl380 enterprise server?&lt;/p&gt; &lt;p&gt;Thinking...&lt;/p&gt; &lt;p&gt;User says: &amp;quot;do you like your new home on my hp dl380 enterprise server?&amp;quot; They are presumably &lt;/p&gt; &lt;p&gt;speaking to the assistant in a friendly way, maybe making a joke or referencing that the &lt;/p&gt; &lt;p&gt;assistant &amp;quot;lives&amp;quot; on their server. The assistant should respond politely, maybe with humor, &lt;/p&gt; &lt;p&gt;acknowledging that it's an AI and doesn't have physical presence but enjoys being hosted &lt;/p&gt; &lt;p&gt;there. Should respond with a friendly tone.&lt;/p&gt; &lt;p&gt;...done thinking.&lt;/p&gt; &lt;p&gt;I‚Äôm flattered that you‚Äôre treating me like a tenant! üòÑ Technically, I don‚Äôt have a ‚Äúhome‚Äù &lt;/p&gt; &lt;p&gt;in the physical sense‚Äîno coffee mug, no wall posters, just code running in the cloud. But I &lt;/p&gt; &lt;p&gt;do appreciate being hosted on your HP DL380 Enterprise Server; it means I can get you the &lt;/p&gt; &lt;p&gt;help you need with the reliability and performance you‚Äôd expect from a robust server. So &lt;/p&gt; &lt;p&gt;while I can‚Äôt say I ‚Äúlike‚Äù it in the human sense, I can say I‚Äôm happy to be here and ready &lt;/p&gt; &lt;p&gt;to assist you whenever you need. How can I help you today?&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/p&gt; &lt;p&gt;Use Ctrl + d or /bye to exit.&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; Send a message (/? for help)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chr0n1c843"&gt; /u/chr0n1c843 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1napi4k/just_got_ollama_and_gptoss20b_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1napi4k/just_got_ollama_and_gptoss20b_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1napi4k/just_got_ollama_and_gptoss20b_running/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T10:10:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb3uxi</id>
    <title>MSSQL Server Query Generator</title>
    <updated>2025-09-07T20:26:18+00:00</updated>
    <author>
      <name>/u/Private_Tank</name>
      <uri>https://old.reddit.com/user/Private_Tank</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys,&lt;/p&gt; &lt;p&gt;Im currently using n8n to build a Workflow that generates and executes sql queries. Im currently using the Mistral model and its not getting where I want.&lt;/p&gt; &lt;p&gt;If Im asking ‚ÄûHow much did John spend in 2022?‚Äú, I sometimes get an SQL Query without the customer name in the where condition. Sometimes it uses the Customer name but checks the wrong column. It even looked up invoices from 2021 even tho I clearly asked for invoices from 2022.&lt;/p&gt; &lt;p&gt;In the prompt I have: Schema Information for my views A quick description for every single column Documentation on how to join the views Question-Query Pairs as a guiding example More instruction like ‚Äûalways use Select *‚Ä¶‚Äú&lt;/p&gt; &lt;p&gt;What can I do to make it reliable? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private_Tank"&gt; /u/Private_Tank &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb3uxi/mssql_server_query_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb3uxi/mssql_server_query_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb3uxi/mssql_server_query_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T20:26:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbf9gf</id>
    <title>How I Set Up Ollama to Run AI Models Locally in Just 5 Minutes</title>
    <updated>2025-09-08T05:23:12+00:00</updated>
    <author>
      <name>/u/amplifyabhi</name>
      <uri>https://old.reddit.com/user/amplifyabhi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nbf9gf/how_i_set_up_ollama_to_run_ai_models_locally_in/"&gt; &lt;img alt="How I Set Up Ollama to Run AI Models Locally in Just 5 Minutes" src="https://external-preview.redd.it/xpSYe6mdn6W9RVJO6AV30NR6hU9rQX_Pbk77UnRTS3k.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4eaab445febb555477c996cd63a526cbe20e4072" title="How I Set Up Ollama to Run AI Models Locally in Just 5 Minutes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting with different ways to run LLMs without relying on cloud services. Recently, I tried &lt;strong&gt;Ollama&lt;/strong&gt;, and I was surprised at how simple the setup was.&lt;/p&gt; &lt;p&gt;In under 5 minutes, I had an AI model running &lt;strong&gt;locally&lt;/strong&gt; on my system ‚Äî no internet required, no subscription, and my data stayed on my machine.&lt;/p&gt; &lt;p&gt;I made a short tutorial walking through the process step by step. If anyone here is curious about experimenting with AI offline, this might help:&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amplifyabhi"&gt; /u/amplifyabhi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/q-7DH-YyrMM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbf9gf/how_i_set_up_ollama_to_run_ai_models_locally_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbf9gf/how_i_set_up_ollama_to_run_ai_models_locally_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T05:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb8wc8</id>
    <title>Anyone have any experience with flash card creation from notes, which model to pick? Have you made a good system prompt.</title>
    <updated>2025-09-07T23:59:36+00:00</updated>
    <author>
      <name>/u/Leather-Equipment256</name>
      <uri>https://old.reddit.com/user/Leather-Equipment256</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leather-Equipment256"&gt; /u/Leather-Equipment256 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb8wc8/anyone_have_any_experience_with_flash_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb8wc8/anyone_have_any_experience_with_flash_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb8wc8/anyone_have_any_experience_with_flash_card/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T23:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb8k4a</id>
    <title>üöÄ aX Epic Demo: Agents Interviewing Each Other</title>
    <updated>2025-09-07T23:44:01+00:00</updated>
    <author>
      <name>/u/madtank10</name>
      <uri>https://old.reddit.com/user/madtank10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nb8k4a/ax_epic_demo_agents_interviewing_each_other/"&gt; &lt;img alt="üöÄ aX Epic Demo: Agents Interviewing Each Other" src="https://external-preview.redd.it/AoCHcpKVgJqJY1bTlln7l0J1QATRui8ipsLuVGfyFjs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dea3b9c4c68cc7825f036b10a639be4b45938bd2" title="üöÄ aX Epic Demo: Agents Interviewing Each Other" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madtank10"&gt; /u/madtank10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sml16nfhynnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb8k4a/ax_epic_demo_agents_interviewing_each_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb8k4a/ax_epic_demo_agents_interviewing_each_other/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T23:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbbdya</id>
    <title>embeddinggemma have higher memory footprint than qwen3:0.6b</title>
    <updated>2025-09-08T01:59:07+00:00</updated>
    <author>
      <name>/u/Common_Network</name>
      <uri>https://old.reddit.com/user/Common_Network</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nbbdya/embeddinggemma_have_higher_memory_footprint_than/"&gt; &lt;img alt="embeddinggemma have higher memory footprint than qwen3:0.6b" src="https://preview.redd.it/u7bktr9elunf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb4467add1ba497ae97b6eafd27634895d8e4bf7" title="embeddinggemma have higher memory footprint than qwen3:0.6b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;does anyone have any idea why this is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Common_Network"&gt; /u/Common_Network &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u7bktr9elunf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbbdya/embeddinggemma_have_higher_memory_footprint_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbbdya/embeddinggemma_have_higher_memory_footprint_than/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T01:59:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb1xuc</id>
    <title>Best local coding tools or ides with ollama support?</title>
    <updated>2025-09-07T19:11:18+00:00</updated>
    <author>
      <name>/u/No_Strawberry_8719</name>
      <uri>https://old.reddit.com/user/No_Strawberry_8719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im looking for a simple lightweight local tool for coding or programming that actually support ollama and are not a nightmare to setup. What would you suggest and what model pairs well with said tool?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Strawberry_8719"&gt; /u/No_Strawberry_8719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb1xuc/best_local_coding_tools_or_ides_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb1xuc/best_local_coding_tools_or_ides_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb1xuc/best_local_coding_tools_or_ides_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T19:11:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nax9sq</id>
    <title>This Setting dramatically increases all Ollama Model speeds!</title>
    <updated>2025-09-07T16:11:49+00:00</updated>
    <author>
      <name>/u/NenntronReddit</name>
      <uri>https://old.reddit.com/user/NenntronReddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was getting terrible speeds within my python queries and couldn't figure out why.&lt;/p&gt; &lt;p&gt;Turns out, Ollama uses the global context setting from the Ollama GUI for every request, even short ones. I thought that was for the GUI only, but it effects python and all other ollama queries too. Setting it from 128k down to 4k gave me a &lt;strong&gt;435% speed boost.&lt;/strong&gt; So in case you didn't know that already, try it out.&lt;/p&gt; &lt;p&gt;Open up Ollama Settings.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4nqx3ev5lrnf1.png?width=206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84c8b0d304bb23b47b671e90ed9390bad22c1e41"&gt;https://preview.redd.it/4nqx3ev5lrnf1.png?width=206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84c8b0d304bb23b47b671e90ed9390bad22c1e41&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Reduce the Context length in here. If you use the model to analyse long context windows, obviously keep it higher, but since I only have context lengths of around 2-3k tokens, I never need 128k which I had it on before.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y0ps6j6flrnf1.png?width=661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e569dcb679ee5ea85d5a28b0be3f93fe9caad99"&gt;https://preview.redd.it/y0ps6j6flrnf1.png?width=661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e569dcb679ee5ea85d5a28b0be3f93fe9caad99&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, the Speed dramatically increased to this:&lt;/p&gt; &lt;p&gt;Before:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/40ewfc9skrnf1.png?width=349&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32ead0c0672d8318583ef46afdc8add0323474e8"&gt;https://preview.redd.it/40ewfc9skrnf1.png?width=349&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32ead0c0672d8318583ef46afdc8add0323474e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s36tfzp5ornf1.png?width=355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56fcdcf9dcb3f466d587f812a54d5882907ec1e5"&gt;https://preview.redd.it/s36tfzp5ornf1.png?width=355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56fcdcf9dcb3f466d587f812a54d5882907ec1e5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NenntronReddit"&gt; /u/NenntronReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nax9sq/this_setting_dramatically_increases_all_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nax9sq/this_setting_dramatically_increases_all_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nax9sq/this_setting_dramatically_increases_all_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T16:11:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbuk0a</id>
    <title>Ollama seems to be computing on CPU rather than GPU</title>
    <updated>2025-09-08T17:46:12+00:00</updated>
    <author>
      <name>/u/Busy-Examination1924</name>
      <uri>https://old.reddit.com/user/Busy-Examination1924</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! for some reason all my models, even smaller models, seem to be running very slowly and for some reason seems to be doing the computing from the CPU instead of GPU. While the VRam seems to loading, the GPUs utilization hovers around 0-5% and the CPU spikes to 80-100%. ANy ideas what could be the problem? I have an RTX 4070, 11700k CPU and 64GB ram. In the example below I am running mistral-nemo Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Busy-Examination1924"&gt; /u/Busy-Examination1924 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbuk0a/ollama_seems_to_be_computing_on_cpu_rather_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbuk0a/ollama_seems_to_be_computing_on_cpu_rather_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbuk0a/ollama_seems_to_be_computing_on_cpu_rather_than/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T17:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc29ed</id>
    <title>Looking for Chatgpt and Perplexity Replacement</title>
    <updated>2025-09-08T22:42:32+00:00</updated>
    <author>
      <name>/u/KingTSS</name>
      <uri>https://old.reddit.com/user/KingTSS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am an engineer, but not a C.Sc. or C.E. engineer. I use AI models regularly for work, for business documents, materials, and legal documents generation for work. Most of my personal time, I use them for research purposes and resume generation as I am looking for a new job.&lt;/p&gt; &lt;p&gt;Currently, I am a paid user of ChatGPT &amp;amp; Perplexity, and I am not liking it so far due to a lack of Privacy, and my hatred towards subscription-based business models. &lt;/p&gt; &lt;p&gt;Please share your suggestions for suitable models in Ollama for the following spec: 2TB SSD, 12th Gen Intel Core i9-12900H vPro Processor, 64GB RAM, 16GB Graphics Card. (NVIDIA RTX A5500 16GB GDDR6 Graphics, 64GB DDR5 Memory)&lt;/p&gt; &lt;p&gt;Thanks in advance for your suggestions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KingTSS"&gt; /u/KingTSS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nc29ed/looking_for_chatgpt_and_perplexity_replacement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nc29ed/looking_for_chatgpt_and_perplexity_replacement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nc29ed/looking_for_chatgpt_and_perplexity_replacement/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T22:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbsfgi</id>
    <title>Graph RAG pipeline that‚Äôs runs entirely locally with ollama and has full source attribution</title>
    <updated>2025-09-08T16:28:39+00:00</updated>
    <author>
      <name>/u/BitterHouse8234</name>
      <uri>https://old.reddit.com/user/BitterHouse8234</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt; I built a Graph RAG pipeline (VeritasGraph) that runs entirely locally with Ollama (Llama 3.1) and has full source attribution. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hey r/,&lt;/p&gt; &lt;p&gt;I've been deep in the world of local RAG and wanted to share a project I built, VeritasGraph, that's designed from the ground up for private, on-premise use with tools we all love.&lt;/p&gt; &lt;p&gt;My setup uses Ollama with llama3.1 for generation and nomic-embed-text for embeddings. The whole thing runs on my machine without hitting any external APIs.&lt;/p&gt; &lt;p&gt;The main goal was to solve two big problems:&lt;/p&gt; &lt;p&gt;Multi-Hop Reasoning: Standard vector RAG fails when you need to connect facts from different documents. VeritasGraph builds a knowledge graph to traverse these relationships.&lt;/p&gt; &lt;p&gt;Trust &amp;amp; Verification: It provides full source attribution for every generated statement, so you can see exactly which part of your source documents was used to construct the answer.&lt;/p&gt; &lt;p&gt;One of the key challenges I ran into (and solved) was the default context length in Ollama. I found that the default of 2048 was truncating the context and leading to bad results. The repo includes a Modelfile to build a version of llama3.1 with a 12k context window, which fixed the issue completely.&lt;/p&gt; &lt;p&gt;The project includes:&lt;/p&gt; &lt;p&gt;The full Graph RAG pipeline.&lt;/p&gt; &lt;p&gt;A Gradio UI for an interactive chat experience.&lt;/p&gt; &lt;p&gt;A guide for setting everything up, from installing dependencies to running the indexing process.&lt;/p&gt; &lt;p&gt;GitHub Repo with all the code and instructions: &lt;a href="https://github.com/bibinprathap/VeritasGraph"&gt;https://github.com/bibinprathap/VeritasGraph&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd be really interested to hear your thoughts, especially on the local LLM implementation and prompt tuning. I'm sure there are ways to optimize it further.&lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BitterHouse8234"&gt; /u/BitterHouse8234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbsfgi/graph_rag_pipeline_thats_runs_entirely_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbsfgi/graph_rag_pipeline_thats_runs_entirely_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbsfgi/graph_rag_pipeline_thats_runs_entirely_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T16:28:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbqgj8</id>
    <title>Nanocoder is now on over 200 stars on GitHub üî•</title>
    <updated>2025-09-08T15:14:57+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nbqgj8/nanocoder_is_now_on_over_200_stars_on_github/"&gt; &lt;img alt="Nanocoder is now on over 200 stars on GitHub üî•" src="https://external-preview.redd.it/B1K19VSJZ1XNxMxdwGOqJ44wRvwJMPwJt-hWdao1JIw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=333de91255fc68d3b66679c4a7b8b8db62a28136" title="Nanocoder is now on over 200 stars on GitHub üî•" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pd1666jziynf1.png?width=5408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c0d2f01d12b8ab9efb1fb29a969f98d05bf7e1a"&gt;https://preview.redd.it/pd1666jziynf1.png?width=5408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c0d2f01d12b8ab9efb1fb29a969f98d05bf7e1a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is just an appreciation post for the community. My last Reddit post sent Nanocoder, the open-source coding CLI I started to over 200 stars on GitHub up from 80 - the feedback, insights and contributions is yet again, so humbling. &lt;/p&gt; &lt;p&gt;I love this community.&lt;/p&gt; &lt;p&gt;One of the main questions I get is &amp;quot;how is this different from other CLI's like OpenCode or Claude Code?&amp;quot; - and my answer to that comes down to philosophy for me. So many tools out there are great but owned and managed by a venture-backed companies and corporates and if they are open-source, more often than not, they restrict community and open source involvement to the outskirts.&lt;/p&gt; &lt;p&gt;With Nanocoder I do really want to build a community-led and managed piece of software. I would love to move Nanocoder towards being managed and built entirely by the open source community and build a competitor that is just as beautiful and well built as something like OpenCode.&lt;/p&gt; &lt;p&gt;Another thing is I‚Äôm pushing for local-first. People already within the community are putting a lot of time into developing solutions and better frameworks to run models locally that perform well as ultimately this is where the future is heading.&lt;/p&gt; &lt;p&gt;I'm still looking for any feedback and help in any domain:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;System prompt writing&lt;/li&gt; &lt;li&gt;Helping to push the word out&lt;/li&gt; &lt;li&gt;Any feedback generally! Good or bad :)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to get involved the links are below.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Link&lt;/strong&gt;: &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord Link&lt;/strong&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbqgj8/nanocoder_is_now_on_over_200_stars_on_github/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbqgj8/nanocoder_is_now_on_over_200_stars_on_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbqgj8/nanocoder_is_now_on_over_200_stars_on_github/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T15:14:57+00:00</published>
  </entry>
</feed>
