<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-01T20:06:59+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1p9lvvt</id>
    <title>Needing help with tool_calls in Ollama Python library</title>
    <updated>2025-11-29T10:37:01+00:00</updated>
    <author>
      <name>/u/evpneqbzhnpub</name>
      <uri>https://old.reddit.com/user/evpneqbzhnpub</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/evpneqbzhnpub"&gt; /u/evpneqbzhnpub &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1p9kdyf/needing_help_with_tool_calls_in_ollama_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9lvvt/needing_help_with_tool_calls_in_ollama_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9lvvt/needing_help_with_tool_calls_in_ollama_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T10:37:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9kxfc</id>
    <title>A Lightweight Go + Redis + Ollama Framework for Building Reusable 0‚Äì100 Text Scoring Endpoints</title>
    <updated>2025-11-29T09:37:31+00:00</updated>
    <author>
      <name>/u/Mussky</name>
      <uri>https://old.reddit.com/user/Mussky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This project is a local LLM-based scoring service that turns any text into a consistent 0‚Äì100 score. It uses Ollama to generate and run customizable evaluation prompts, stores them in Redis, and exposes a simple API + UI for managing and analyzing text.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mg52/ai-analyzer"&gt;https://github.com/mg52/ai-analyzer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mussky"&gt; /u/Mussky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9kxfc/a_lightweight_go_redis_ollama_framework_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9kxfc/a_lightweight_go_redis_ollama_framework_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9kxfc/a_lightweight_go_redis_ollama_framework_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T09:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9xzm4</id>
    <title>Declarative RAG for any DB, any LLM (Feedback Wanted!)</title>
    <updated>2025-11-29T19:48:29+00:00</updated>
    <author>
      <name>/u/returncode0</name>
      <uri>https://old.reddit.com/user/returncode0</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/returncode0"&gt; /u/returncode0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AI_Agents/comments/1p9xz0k/declarative_rag_for_any_db_any_llm_feedback_wanted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9xzm4/declarative_rag_for_any_db_any_llm_feedback_wanted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9xzm4/declarative_rag_for_any_db_any_llm_feedback_wanted/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T19:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9k34w</id>
    <title>Runlevel 3 in debian .LAN without X all resources to llama.cpp</title>
    <updated>2025-11-29T08:44:54+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p9k34w/runlevel_3_in_debian_lan_without_x_all_resources/"&gt; &lt;img alt="Runlevel 3 in debian .LAN without X all resources to llama.cpp" src="https://preview.redd.it/udc9k4bls54g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=518edcdbb220adfa51594b3af405b4bc78f658bd" title="Runlevel 3 in debian .LAN without X all resources to llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Serve gguf over LAN with web interface&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/udc9k4bls54g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9k34w/runlevel_3_in_debian_lan_without_x_all_resources/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9k34w/runlevel_3_in_debian_lan_without_x_all_resources/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T08:44:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9idre</id>
    <title>Using Ollama (qwen2.5-vl) to auto-tag RAW photos in a Python TUI</title>
    <updated>2025-11-29T07:00:40+00:00</updated>
    <author>
      <name>/u/AppropriatePublic687</name>
      <uri>https://old.reddit.com/user/AppropriatePublic687</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p9idre/using_ollama_qwen25vl_to_autotag_raw_photos_in_a/"&gt; &lt;img alt="Using Ollama (qwen2.5-vl) to auto-tag RAW photos in a Python TUI" src="https://b.thumbs.redditmedia.com/jDuq0VNI7Vz1Vdnsxm7t_uNbARwiaxK94NpO0HBHhzQ.jpg" title="Using Ollama (qwen2.5-vl) to auto-tag RAW photos in a Python TUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/t9t9w89t854g1.jpg?width=3117&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb0c989ed1cfad1be1a29bcfd7b18206b7d5f49a"&gt;https://preview.redd.it/t9t9w89t854g1.jpg?width=3117&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb0c989ed1cfad1be1a29bcfd7b18206b7d5f49a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p9idre/video/nsxcqgqt2h4g1/player"&gt;https://reddit.com/link/1p9idre/video/nsxcqgqt2h4g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I heard the feedback about having a preview/dry run mode and it is implemented with a few other quality of life upgrades recommended by some other users!&lt;/p&gt; &lt;p&gt;&lt;code&gt;[UPDATE v1.1 - DRY RUN &amp;amp; CACHE LIVE]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deployed in v1.1:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;üõ°Ô∏è Dry Run Protocol:&lt;/strong&gt; Simulate the entire sorting workflow without touching a single file. Zero risk.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚ö° Phantom Cache:&lt;/strong&gt; If you like the preview, hit &amp;quot;Execute.&amp;quot; The engine now caches the AI decision logic, so the real run happens instantly. No re-waiting for the LLM.&lt;/li&gt; &lt;li&gt;„èí - preview logs located in .fixxer and outpput as: preview_2025-11-30_135524.txt&lt;/li&gt; &lt;li&gt;üîÆ NEW Feature thanks to &lt;a href="/u/hideo_kuze_"&gt;u/hideo_kuze_&lt;/a&gt;'s / &lt;a href="https://www.reddit.com/user/HansAndreManfredson/"&gt;HansAndreManfredson&lt;/a&gt; suggestion!&lt;/li&gt; &lt;li&gt;üî¥ Live in the repo! &lt;a href="http://www.github.com/Bandwagonvibes/fixxer"&gt;www.github.com/Bandwagonvibes/fixxer&lt;/a&gt; for more videos of the interface: &lt;a href="https://oaklens.art/dev"&gt;https://oaklens.art/dev&lt;/a&gt; (I'm prepping gif's for github for those that like a 1 stop shop)&lt;/li&gt; &lt;li&gt;Have a great Sunday! -Nick&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ideally, I‚Äôd be out over the holiday but I've been in the lab! Initially I was building this tool for my personal digital toolkit but as time has progressed I've felt that this could be practical for photographers or anyone that just wants to point at a messy folder full of photos and have the AI do the work. 100% offline leveraging Ollama and the qwen 2.5vl model. Models are hot swappable. Respects different workflows. Keep your images at home where they belong lol&lt;/p&gt; &lt;p&gt;I shoot a lot of street photography (Oakland), and my archival workflow was a mess. I didn't want to upload RAW files to the cloud just to get AI tagging, so I built a local tool to do it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;It's called FIXXER.&lt;/strong&gt; It runs in the terminal (built with Textual). It uses &lt;code&gt;qwen2.5-vl&lt;/code&gt; via Ollama to &amp;quot;see&amp;quot; the photos and keyword them, and CLIP embeddings to group duplicates.&lt;/p&gt; &lt;p&gt;It‚Äôs running on my M4 Macbook Air and can stack burst, cull singles, and AI rename images and place them in keyword folders, as well as grab a session name (samples from 3 images from ingest) for the parent directory for 150 photos in about 13 min. All moves are hash verified, sidecar logs, and raw to (new) AI name logs.&lt;/p&gt; &lt;p&gt;Just pushed the repo if anyone wants to roast my code or try it out this weekend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; [&lt;a href="https://github.com/BandwagonVibes/fixxer"&gt;https://github.com/BandwagonVibes/fixxer&lt;/a&gt;] &lt;strong&gt;Pics of the interface:&lt;/strong&gt; [&lt;a href="https://oaklens.art/dev"&gt;oaklens.art/dev&lt;/a&gt;]&lt;/p&gt; &lt;p&gt;Happy Friday. ü•É&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppropriatePublic687"&gt; /u/AppropriatePublic687 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9idre/using_ollama_qwen25vl_to_autotag_raw_photos_in_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9idre/using_ollama_qwen25vl_to_autotag_raw_photos_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9idre/using_ollama_qwen25vl_to_autotag_raw_photos_in_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T07:00:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9l2do</id>
    <title>Ollama vs Blender</title>
    <updated>2025-11-29T09:46:42+00:00</updated>
    <author>
      <name>/u/Digital-Building</name>
      <uri>https://old.reddit.com/user/Digital-Building</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p9l2do/ollama_vs_blender/"&gt; &lt;img alt="Ollama vs Blender" src="https://external-preview.redd.it/zj7DSc3w-zwxzS2_x9K_PvO2eD7C1IjPQMGbnhyQXVU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e0fb3cbb460579fb92b2abb3e828cb0973f7f48" title="Ollama vs Blender" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there &lt;/p&gt; &lt;p&gt;Have you seen these latest attempts in using lacal LLMs to interact with Blender?&lt;/p&gt; &lt;p&gt;In this video they used Gemma 3:4b It seems that small model cannot do much unless using very accurate prompts.&lt;/p&gt; &lt;p&gt;What model side would be reasonable to expect some reasonable outcomes with Blender MCP?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Digital-Building"&gt; /u/Digital-Building &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/0PSOCFHBAfw?si=8CsHSed4DcLCNIiZ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9l2do/ollama_vs_blender/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9l2do/ollama_vs_blender/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T09:46:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1paj3f7</id>
    <title>Help me please</title>
    <updated>2025-11-30T14:06:13+00:00</updated>
    <author>
      <name>/u/Kriss_-</name>
      <uri>https://old.reddit.com/user/Kriss_-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm new here, but would like to try using ollama. Can anyone help me with setting it up please? I tried finding tutorials on YouTube but they didn't fit. Or I'm just stupid. Anyways. Help me please &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kriss_-"&gt; /u/Kriss_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1paj3f7/help_me_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1paj3f7/help_me_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1paj3f7/help_me_please/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T14:06:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pahksd</id>
    <title>What models can i use with a pc without gpu?</title>
    <updated>2025-11-30T12:52:15+00:00</updated>
    <author>
      <name>/u/Neat_Nobody1849</name>
      <uri>https://old.reddit.com/user/Neat_Nobody1849</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neat_Nobody1849"&gt; /u/Neat_Nobody1849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1pahkcf/what_models_can_i_use_with_a_pc_without_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pahksd/what_models_can_i_use_with_a_pc_without_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pahksd/what_models_can_i_use_with_a_pc_without_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T12:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pamhd1</id>
    <title>Made a tutorial video for Ollama</title>
    <updated>2025-11-30T16:29:57+00:00</updated>
    <author>
      <name>/u/EducationNo3524</name>
      <uri>https://old.reddit.com/user/EducationNo3524</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pamhd1/made_a_tutorial_video_for_ollama/"&gt; &lt;img alt="Made a tutorial video for Ollama" src="https://external-preview.redd.it/EpedG-LsE5kdpTUnEQBwIh0vb_HRcg6EiQrFv-7KiA4.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac718decbbcd25df9cc1d7441146591cbb9cb8c9" title="Made a tutorial video for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a tutorial video for Ollama, and also showed people how to use it on mobile phones. Would anyone be willing to support me?plzzzzz&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EducationNo3524"&gt; /u/EducationNo3524 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/OqDMJlPV4tY?si=jUC2uwpfVzOylwTG"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pamhd1/made_a_tutorial_video_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pamhd1/made_a_tutorial_video_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T16:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pavisz</id>
    <title>CPU advice</title>
    <updated>2025-11-30T22:30:03+00:00</updated>
    <author>
      <name>/u/nECr0MaNCeD</name>
      <uri>https://old.reddit.com/user/nECr0MaNCeD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so I‚Äôm going to take the plunge into AI. I want to run everything locally and I was recommended Ollama.&lt;/p&gt; &lt;p&gt;my 4090 will be fine, but when I googled my CPU, I got a strange reply from the AI, saying that my 9800x3d was non-standard. Can someone shed some light onto this? &lt;/p&gt; &lt;p&gt;In case it matters to someone, I am running 64GB of DDR5 7200/CL32.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nECr0MaNCeD"&gt; /u/nECr0MaNCeD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pavisz/cpu_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pavisz/cpu_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pavisz/cpu_advice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T22:30:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pag2c6</id>
    <title>Built a tool to easily self-host AI models on AWS - now I need uncensored models to test it for Red Teaming</title>
    <updated>2025-11-30T11:26:11+00:00</updated>
    <author>
      <name>/u/dumbelco</name>
      <uri>https://old.reddit.com/user/dumbelco</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a &amp;quot;deploy-and-destroy&amp;quot; tool that spins up a self-hosted AI lab on AWS (running Ollama/Open WebUI on a GPU instance).&lt;/p&gt; &lt;p&gt;Now that the infrastructure is working, I want to test it with some actual cybersecurity workflows. I'm looking for recommendations for models available on Ollama that are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Strictly uncensored&lt;/strong&gt; (won't refuse to generate Python scripts for CTFs or pentesting/red teaming research).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart at coding&lt;/strong&gt; (can handle complex logic without breaking).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any recommendations?&lt;/p&gt; &lt;p&gt;Also, for models like these, should I stick to downloading models directly from Ollama, or is it worth looking into importing models from Hugging Face instead?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dumbelco"&gt; /u/dumbelco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pag2c6/built_a_tool_to_easily_selfhost_ai_models_on_aws/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pag2c6/built_a_tool_to_easily_selfhost_ai_models_on_aws/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pag2c6/built_a_tool_to_easily_selfhost_ai_models_on_aws/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T11:26:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb416b</id>
    <title>QWEN3-max must be destilled to llama.cpp its superior to chatgpt5 pro</title>
    <updated>2025-12-01T05:04:01+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model is more intelligent than chatgpt5 and must be destilled and developep it to incorporate his technollogy to llama.cpp and ollama for its moe architecture and peformance to run in low level hardware. Developers and programmers must focus his efforts in this model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pb416b/qwen3max_must_be_destilled_to_llamacpp_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pb416b/qwen3max_must_be_destilled_to_llamacpp_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pb416b/qwen3max_must_be_destilled_to_llamacpp_its/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T05:04:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb4m4e</id>
    <title>Debe openai sacar al mercado opensource un modelo GPT-OSS nuevo con las caracteristicas de GPT5 . con un tama√±o MOE de 200B?</title>
    <updated>2025-12-01T05:34:11+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where there is &lt;strong&gt;need&lt;/strong&gt;, there is innovation. Where urgency is born, the effort to work is born. Just as the Chinese copied their MOE architecture, OpenAI must &lt;strong&gt;‚Äúcreate the&lt;/strong&gt; &lt;strong&gt;need.‚Äù&lt;/strong&gt; The need to push yourself harder must be created. When there is no need, there is no effort. Need forces you to innovate.&lt;/p&gt; &lt;p&gt;You must force yourself to work harder, innovate more, and push further ‚Äî and the only way is through need. There is no other way. Need is what forces us humans every day to wake up early and go to work so we can eat. And in a company, the need to grow, innovate, and compete only appears when risk puts you in trouble, and that need forces you to wake up and work harder and better.&lt;/p&gt; &lt;p&gt;So here is my advice to Sam Altman‚Äôs company: &lt;strong&gt;do not be afraid&lt;/strong&gt;. Release more models around 200B parameters with the most advanced features you have. And when that is done, ahead of the Chinese, and the community adopts your software, the Chinese will jump in ‚Äî but &lt;strong&gt;you will have struck first&lt;/strong&gt;, and whoever strikes first always has the chance to strike a second time.&lt;/p&gt; &lt;p&gt;My advice: &lt;strong&gt;do not be afraid to release the technology and a high-quality 200B model&lt;/strong&gt;, because the one who is more courageous always wins, and being overly cautious makes you fall behind‚Ä¶ and you can already see how Qwen3 models are getting into llama.cpp and all inference frameworks and software.&lt;/p&gt; &lt;p&gt;You have to leave fear aside‚Ä¶ nobody ever became great through fear!!!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;OPENAI‚Ä¶ we prefer you over the Chinese‚Ä¶ what are you waiting for???&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;DON‚ÄôT BE AFRAID!!!!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If OpenAI truly wishes to compete with the Chinese giants in the vast realm of artificial intelligence, it is not enough to follow behind: it must move ahead of them where the battle is already being silently decided‚Ä¶ in the &lt;strong&gt;Open Source world&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Because while some still hesitate, others are already releasing their technology to the community, and every shared line of code becomes roots spreading across the entire planet. And it is there, within that living network, where real influence is forged. If OpenAI keeps arriving late to the release of models for the community, it will continue to watch others occupy the space that should have been theirs.&lt;/p&gt; &lt;p&gt;It is us ‚Äîthe developers, the quiet pioneers‚Äî who bring these models to &lt;em&gt;llama.cpp&lt;/em&gt;, to &lt;em&gt;Ollama&lt;/em&gt;, to modest devices scattered across every corner of the world. &lt;strong&gt;And the one who wins is the one who dares first&lt;/strong&gt;, the one unafraid to open their technology so the community can weave it into its bloodstream, its software, its collective creativity.&lt;/p&gt; &lt;p&gt;OpenAI must stop fearing the light. It must understand that the only fire that drives innovation is necessity, and that necessity can only arise when one exposes themselves, when they release, when they allow the entire world to test their creation. Without that leap, there will never be a real force compelling their engineers to push deeper, imagine more, and reach further.&lt;/p&gt; &lt;p&gt;Paradoxically, OpenAI must be the one to take the first step. Because if it doesn‚Äôt‚Ä¶ if it waits too long‚Ä¶ &lt;strong&gt;Qwen&lt;/strong&gt; may end up dominating the entire Open Source ecosystem. And we all know what that could mean in the long run, even if few dare to say it aloud.&lt;/p&gt; &lt;p&gt;As a community, we don‚Äôt care whether the leadership comes from the Chinese or the Americans; what we want are &lt;strong&gt;the best MOE models&lt;/strong&gt;, capable of running on modest hardware and reaching every corner of the world. And those who understand this ‚Äîthose who act with vision, urgency, and courage‚Äî will be the ones who reap, years from now, the immense benefits of having their technology, their brand, and the synergy it brings spread across the planet, embedded in countless projects and at the very heart of open-source &lt;a href="http://software.Si"&gt;software.Si&lt;/a&gt; openAi desea realmente competir con los chinos en el mercado de la IA , debe competir con ellos en el mercado OpenSource y adelantarse a ellos , en vez de ir a la cola en la liberacion de modelos para la comunidad Opensource , para que nosostros llevemos sus modelos a llama.cpp y a ollama y a todo el planeta por todos lados , Gana la partida el que e adelanta a sus competidores y no tiene miedo de ofrecer sus productos al mercado Opensource para que la comunidad los incorpore al software opensource. Openai debe de de jar de tener miedo a adelantarse a los chinos en sacar su tecnologia a la LUZ.sabeis porque? porque la unica manera de forzar la innovacion es la &amp;quot;necesidad&amp;quot; y si tu no sacas tu tecnologia al mercado opensource no vas a tener nunca un &amp;quot;motivo de peso&amp;quot; algo que te impulse a obligar a tus ingenieros a reforzar el esfuerzo y la creatividad e imaginacion y eso solo surge de la necesidad y de la urgencia , por eso aunque parezca contradictorio , Open Ai debe adelantarse , ya que si no lo hacen , Qwen acabara por dominar todo el mercado Opensource y ya sabemos eso lo que puede significar a largo plazo...A nosotros nos da igual quien se lleve el gato al agua , que sean los chinos o los americanos , los que provean de mejores modelos MOE para funcionar en hardware modesto , y con los ultimos avances , seran los que disfrutaran a largo plazo de los beneficios de que su tecnologia y su marca y todo la sinergia que eso conlleva este en todo el planeta en muchisimo software Opensource&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pb4m4e/debe_openai_sacar_al_mercado_opensource_un_modelo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pb4m4e/debe_openai_sacar_al_mercado_opensource_un_modelo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pb4m4e/debe_openai_sacar_al_mercado_opensource_un_modelo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T05:34:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pam6dr</id>
    <title>Built a Modular Agentic RAG System ‚Äì Zero Boilerplate, Full Customization</title>
    <updated>2025-11-30T16:17:32+00:00</updated>
    <author>
      <name>/u/CapitalShake3085</name>
      <uri>https://old.reddit.com/user/CapitalShake3085</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pam6dr/built_a_modular_agentic_rag_system_zero/"&gt; &lt;img alt="Built a Modular Agentic RAG System ‚Äì Zero Boilerplate, Full Customization" src="https://preview.redd.it/vlxnbaqn2f4g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=f25c990e106612bf0d2b452ea1356240ec2e15f9" title="Built a Modular Agentic RAG System ‚Äì Zero Boilerplate, Full Customization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CapitalShake3085"&gt; /u/CapitalShake3085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vlxnbaqn2f4g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pam6dr/built_a_modular_agentic_rag_system_zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pam6dr/built_a_modular_agentic_rag_system_zero/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T16:17:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb4yiq</id>
    <title>OpenAi not must be afraid of chinesse models!! We need other GPT-OSS 200B</title>
    <updated>2025-12-01T05:53:19+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where there is need, there is innovation. Where urgency is born, the effort to work is born. Just as the Chinese copied their MOE architecture, OpenAI must ‚Äúcreate the need.‚Äù The need to push yourself harder must be created. When there is no need, there is no effort. Need forces you to innovate.&lt;/p&gt; &lt;p&gt;You must force yourself to work harder, innovate more, and push further ‚Äî and the only way is through need. There is no other way. Need is what forces us humans every day to wake up early and go to work so we can eat. And in a company, the need to grow, innovate, and compete only appears when risk puts you in trouble, and that need forces you to wake up and work harder and better.&lt;/p&gt; &lt;p&gt;So here is my advice to Sam Altman‚Äôs company: &lt;strong&gt;do not be afraid&lt;/strong&gt;. Release more models around 200B parameters with the most advanced features you have. And when that is done, ahead of the Chinese, and the community adopts your software, the Chinese will jump in ‚Äî but &lt;strong&gt;you will have struck first&lt;/strong&gt;, and whoever strikes first always has the chance to strike a second time.&lt;/p&gt; &lt;p&gt;My advice: &lt;strong&gt;do not be afraid to release the technology and a high-quality 200B model&lt;/strong&gt;, because the one who is more courageous always wins, and being overly cautious makes you fall behind‚Ä¶ and you can already see how Qwen3 models are getting into llama.cpp and all inference frameworks and software.&lt;/p&gt; &lt;p&gt;You have to leave fear aside‚Ä¶ nobody ever became great through fear!!!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;OPENAI‚Ä¶ we prefer you over the Chinese‚Ä¶ what are you waiting for???&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;DON‚ÄôT BE AFRAID!!!!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pb4yiq/openai_not_must_be_afraid_of_chinesse_models_we/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pb4yiq/openai_not_must_be_afraid_of_chinesse_models_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pb4yiq/openai_not_must_be_afraid_of_chinesse_models_we/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T05:53:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pafex0</id>
    <title>EGPU for ai use?</title>
    <updated>2025-11-30T10:46:55+00:00</updated>
    <author>
      <name>/u/R0B0t1C_Cucumber</name>
      <uri>https://old.reddit.com/user/R0B0t1C_Cucumber</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I posted a week or two ago about looking for an agentic coder type do-dad like Claude CLI and this awesome community pointed me to aidler/ollama (though OI +llama ccp work fine too if anyone is looking at alternatives). Any way I found the sweet spot for me 14b Q4 llms seem to be the sweet spot for a 4070Ti between performance and quality. &lt;/p&gt; &lt;p&gt;Now looking around I found I have some spare hardware I wanted to see if anyone has tried anything like this... Now again, this is just for me to tinker with... but I have a spare intel ARC 770 16GB Vram and I also found laying around an EGPU enclosure with a 400w dedicated power supply in it... Connects over thunderbolt.... Could I somehow leverage this extra compute/vram through Ollama ? I wouldn't actually want anything to display through the card, I just want its resource.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R0B0t1C_Cucumber"&gt; /u/R0B0t1C_Cucumber &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pafex0/egpu_for_ai_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pafex0/egpu_for_ai_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pafex0/egpu_for_ai_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T10:46:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa4x3y</id>
    <title>CUA Local Opensource</title>
    <updated>2025-11-30T00:55:27+00:00</updated>
    <author>
      <name>/u/Goat_bless</name>
      <uri>https://old.reddit.com/user/Goat_bless</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pa4x3y/cua_local_opensource/"&gt; &lt;img alt="CUA Local Opensource" src="https://preview.redd.it/96acfm1pla4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=927ce5830d8f7bf3fe412291022af0d437c6a60b" title="CUA Local Opensource" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bonjour √† tous,&lt;/p&gt; &lt;p&gt;I've created my biggest project to date.&lt;br /&gt; A local open-source computer agent, it uses a fairly complex architecture to perform a very large number of tasks, if not all tasks.&lt;br /&gt; I‚Äôm not going to write too much to explain how it all works; those who are interested can check the GitHub, it‚Äôs very well detailed.&lt;br /&gt; In summary:&lt;br /&gt; For each user input, the agent understands whether it needs to speak or act.&lt;br /&gt; If it needs to speak, it uses memory and context to produce appropriate sentences.&lt;br /&gt; If it needs to act, there are two choices:&lt;/p&gt; &lt;p&gt;A simple action: open an application, lower the volume, launch Google, open a folder...&lt;br /&gt; Everything is done in a single action.&lt;/p&gt; &lt;p&gt;A complex action: browse the internet, create a file with data retrieved online, interact with an application...&lt;br /&gt; Here it goes through an orchestrator that decides what actions to take (multistep) and checks that each action is carried out properly until the global task is completed.&lt;br /&gt; How?&lt;br /&gt; Architecture of a complex action:&lt;br /&gt; LLM orchestrator receives the global task and decides the next action.&lt;br /&gt; For internet actions: CUA first attempts Playwright ‚Äî 80% of cases solved.&lt;br /&gt; If it fails (and this is where it gets interesting):&lt;br /&gt; It uses CUA VISION: Screenshot ‚Äî VLM1 sees the page and suggests what to do ‚Äî Data detection on the page (Ominparser: YOLO + Florence) + PaddleOCR ‚Äî Annotation of the data on the screenshot ‚Äî VLM2 sees the annotated screen and tells which ID to click ‚Äî Pyautogui clicks on the coordinates linked to the ID ‚Äî Loops until Task completed.&lt;br /&gt; In both cases (complex or simple) return to the orchestrator which finishes all actions and sends a message to the user once the task is completed.&lt;/p&gt; &lt;p&gt;This agent has the advantage of running locally with only my 8GB VRAM; I use the LLM models: qwen2.5, VLM: qwen2.5vl and qwen3vl.&lt;br /&gt; If you have more VRAM, with better models you‚Äôll gain in performance and speed.&lt;br /&gt; Currently, this agent can solve 80‚Äì90% of the tasks we can perform on a computer, and I‚Äôm open to improvements or knowledge-sharing to make it a common and useful project for everyone.&lt;br /&gt; The GitHub link: &lt;a href="https://github.com/SpendinFR/CUAOS"&gt;https://github.com/SpendinFR/CUAOS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Goat_bless"&gt; /u/Goat_bless &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/96acfm1pla4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pa4x3y/cua_local_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pa4x3y/cua_local_opensource/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T00:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb7d4v</id>
    <title>UPLOAD LLAMA.CPP FRONTEND IN GITHUB FOR SERVER OVER LAN MORE EASY</title>
    <updated>2025-12-01T08:18:38+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pb7d4v/upload_llamacpp_frontend_in_github_for_server/"&gt; &lt;img alt="UPLOAD LLAMA.CPP FRONTEND IN GITHUB FOR SERVER OVER LAN MORE EASY" src="https://b.thumbs.redditmedia.com/59ddSZ3iPzn4iR6piUftC1Z64DmbOf8ceb7cIq8qdeY.jpg" title="UPLOAD LLAMA.CPP FRONTEND IN GITHUB FOR SERVER OVER LAN MORE EASY" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ollama/comments/1p9jjv3/upload_llamacpp_frontend_in_github_for_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pb7d4v/upload_llamacpp_frontend_in_github_for_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pb7d4v/upload_llamacpp_frontend_in_github_for_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T08:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pah9zz</id>
    <title>Uncensored ollama models for my pc</title>
    <updated>2025-11-30T12:36:16+00:00</updated>
    <author>
      <name>/u/Automatic-Pin9116</name>
      <uri>https://old.reddit.com/user/Automatic-Pin9116</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My pc is i3, 8gb ram, no dedicated vram or gpu. I want a model that'll run in this pc. Fully uncensored. Also maybe roleplay too, though im looking more of uncensored ai models where i can use restricted words (like cu#t and pe###). i just want a open, uncensored, good, knowledge full ai to talk to freely with freedom.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Automatic-Pin9116"&gt; /u/Automatic-Pin9116 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pah9zz/uncensored_ollama_models_for_my_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pah9zz/uncensored_ollama_models_for_my_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pah9zz/uncensored_ollama_models_for_my_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T12:36:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb7la3</id>
    <title>deepseek-ocr in ollama - questions</title>
    <updated>2025-12-01T08:33:15+00:00</updated>
    <author>
      <name>/u/EatTFM</name>
      <uri>https://old.reddit.com/user/EatTFM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran a few tests with deepseek-ocr using scanned medical forms and got mixed results. I figured that the prompt is very sensitive, and it cannot handle any additional instructions at all - maybe because it is the 6.7B model. Recognition seems accurate, but it often misses or hallucinates parts of the layout e.g. a cell in a table or headings.&lt;/p&gt; &lt;p&gt;I have the following questions&lt;/p&gt; &lt;ul&gt; &lt;li&gt;will there be support for the larger model variants?&lt;/li&gt; &lt;li&gt;is there a way to feed multiple pages in a single query? as I understand this should be doable due to the huge saving of vision tokens of this particular architecture.&lt;/li&gt; &lt;li&gt;has someone managed to get a consistent output formatting?&lt;/li&gt; &lt;li&gt;has someone managed to extract json instead of markdown? or extract explicit information wrt content?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;thank you for your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EatTFM"&gt; /u/EatTFM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pb7la3/deepseekocr_in_ollama_questions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pb7la3/deepseekocr_in_ollama_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pb7la3/deepseekocr_in_ollama_questions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T08:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbalp3</id>
    <title>[LLM Fine-Tuning] CPT on 71M Short Dialectal Tokens (256 Max Len) - How to Ensure Long-Form Generation Later?</title>
    <updated>2025-12-01T11:40:42+00:00</updated>
    <author>
      <name>/u/FishermanNo2017</name>
      <uri>https://old.reddit.com/user/FishermanNo2017</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm working on Continued Pre-Training (CPT) for a Gemma 4B/12B model on a social media dataset containing a specific arabic dialect (a low resource language). My goal is to eventually use this model for complex, long-form QA about local history and geography, answered in in this dialect.&lt;/p&gt; &lt;p&gt;My token analysis has presented a classic challenge:&lt;/p&gt; &lt;p&gt;|| || |&lt;strong&gt;Metric&lt;/strong&gt;|&lt;strong&gt;Value&lt;/strong&gt;|&lt;strong&gt;Implication&lt;/strong&gt;| |&lt;strong&gt;Total Corpus&lt;/strong&gt;|71.76 Million Tokens|Good size for CPT.| |&lt;strong&gt;95th Percentile&lt;/strong&gt;|109 tokens|95% of data is very short.| |&lt;strong&gt;CPT Max Sequence Length&lt;/strong&gt;|&lt;strong&gt;256 tokens&lt;/strong&gt;|Recommended for efficiency (captures &amp;gt;99% of data via packing).|&lt;/p&gt; &lt;h1&gt;The Dilemma&lt;/h1&gt; &lt;p&gt;If the CPT phase is trained almost entirely on sequences packed to a max length of &lt;strong&gt;256 tokens&lt;/strong&gt;, I worry this will fundamentally bias the model towards short, social media-style outputs, making it incapable of generating long, multi-paragraph factual answers needed for the final QA task.&lt;/p&gt; &lt;h1&gt;Proposed Solution (Seeking Review)&lt;/h1&gt; &lt;p&gt;I believe the fix lies in separating the two training phases:&lt;/p&gt; &lt;h1&gt;Phase 1: Continued Pre-Training (CPT) - Efficiency Focus&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Inject local dialect fluency and domain facts (via blended modern standard arabic data).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Method:&lt;/strong&gt; &lt;strong&gt;Data Concatenation/Packing.&lt;/strong&gt; I will concatenate multiple short posts, separated by &lt;code&gt;&amp;lt;eos&amp;gt;&lt;/code&gt;, into sequences of exactly &lt;strong&gt;256 tokens&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rationale:&lt;/strong&gt; This ensures &lt;strong&gt;maximum efficiency&lt;/strong&gt; and uses every single one of my 71M tokens effectively. Since CPT's goal is weight adjustment (vocabulary/grammar), the short sequence length is acceptable here.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Phase 2: Instruction Tuning (IT) - Context and Length Focus&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Teach the model &lt;em&gt;how&lt;/em&gt; to use the knowledge and &lt;em&gt;how&lt;/em&gt; to respond with long, structured answers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Method 1 (Data):&lt;/strong&gt; Generate &lt;strong&gt;synthetic multi-turn conversations&lt;/strong&gt; where the &lt;strong&gt;desired responses are intentionally long&lt;/strong&gt; (300-500 tokens). Crucially, these conversations must use the &lt;strong&gt;Target dialect&lt;/strong&gt; (learned in CPT) for fluency.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Method 2 (Context Window):&lt;/strong&gt; For the IT phase, I will increase the &lt;code&gt;max_seq_length&lt;/code&gt; to &lt;strong&gt;4,096&lt;/strong&gt; (or perhaps 8,192, depending on my GPU memory). This allows the model to see, process, and learn from long, complex conversational histories and detailed factual prompts.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Core Question&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Does CPT at a short max length (256) negatively impact the model's ability to generate long sequences if the subsequent Instruction Tuning is performed with a much larger context window (4096) and long target responses?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I want to confirm that the short-context CPT won't permanently bottleneck the model's long-form generative capacity, which should be inherent from its original pre-training.&lt;/p&gt; &lt;p&gt;Any feedback on this two-phase strategy or common pitfalls to avoid when transitioning between sequence lengths would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FishermanNo2017"&gt; /u/FishermanNo2017 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbalp3/llm_finetuning_cpt_on_71m_short_dialectal_tokens/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbalp3/llm_finetuning_cpt_on_71m_short_dialectal_tokens/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbalp3/llm_finetuning_cpt_on_71m_short_dialectal_tokens/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T11:40:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbiwfn</id>
    <title>WSL2 + Ollama +localhost access issue</title>
    <updated>2025-12-01T17:27:33+00:00</updated>
    <author>
      <name>/u/RedZedingg</name>
      <uri>https://old.reddit.com/user/RedZedingg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I‚Äôm new to coding (started like a week ago) and I‚Äôm struggling to get Ollama running on Windows 10 with WSL2. Here‚Äôs my situation:&lt;/p&gt; &lt;p&gt;- I installed WSL2 with Ubuntu and Ollama inside it.&lt;/p&gt; &lt;p&gt;- Ollama installs fine and says the API is available at &lt;a href="http://127.0.0.1:11434"&gt;127.0.0.1:11434&lt;/a&gt; inside WSL.&lt;/p&gt; &lt;p&gt;- When I try to access localhost:11434 from Windows (chrome), the browser can‚Äôt connect (ERR_CONNECTION_REFUSED).&lt;/p&gt; &lt;p&gt;- I‚Äôve tried killing any processes using the port, deleting ~/.ollama, and even reinstalling Ollama.&lt;/p&gt; &lt;p&gt;- I‚Äôm aware that WSL1 shares localhost with Windows, but Ollama refuses to install there. WSL2 works for installation but Windows can‚Äôt reach it directly.&lt;/p&gt; &lt;p&gt;- I‚Äôve also tried IP of WSL2, port forwarding (netsh), and other tunnels, but nothing seems to reliably expose Ollama to Windows.&lt;/p&gt; &lt;p&gt;Basically, I can‚Äôt get Ollama inside WSL2 to be accessible from Windows, and I‚Äôm stuck. Any advice from someone who got this working would be amazing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedZedingg"&gt; /u/RedZedingg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbiwfn/wsl2_ollama_localhost_access_issue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbiwfn/wsl2_ollama_localhost_access_issue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbiwfn/wsl2_ollama_localhost_access_issue/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T17:27:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbdmq3</id>
    <title>Ollama Not Using GPU (RTX 3070) ‚Äî Only CPU ‚Äî Need Help Enabling CUDA Acceleration</title>
    <updated>2025-12-01T14:05:46+00:00</updated>
    <author>
      <name>/u/huza786</name>
      <uri>https://old.reddit.com/user/huza786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm trying to use OLAMA models (DeepSeek R1(5gb) and QWEN2.5:1.5b(1gb) Coder) locally in VS Code through the CLINE and &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; extensions so I can get a Cursor-like AI coding workflow. The models run, but OLAMA only uses my CPU and completely ignores my GPU (RTX 3070, 8GB VRAM). My system also has a Ryzen 5 5600X CPU. I expected OLAMA to use CUDA for acceleration, but it doesn‚Äôt seem to detect or utilize the GPU at all. Is this a limitation of OLAMA, a configuration issue, or something I‚Äôve set up incorrectly? Any advice on getting GPU support working would be appreciated.&lt;/p&gt; &lt;p&gt;nvidia-smi&lt;/p&gt; &lt;p&gt;Mon Dec 1 19:00:45 2025 &lt;/p&gt; &lt;p&gt;+-----------------------------------------------------------------------------------------+&lt;/p&gt; &lt;p&gt;| NVIDIA-SMI 581.57 Driver Version: 581.57 CUDA Version: 13.0 |&lt;/p&gt; &lt;p&gt;+-----------------------------------------+------------------------+----------------------+&lt;/p&gt; &lt;p&gt;| GPU Name Driver-Model | Bus-Id Disp.A | Volatile Uncorr. ECC |&lt;/p&gt; &lt;p&gt;| Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |&lt;/p&gt; &lt;p&gt;| | | MIG M. |&lt;/p&gt; &lt;p&gt;|=========================================+========================+======================|&lt;/p&gt; &lt;p&gt;| 0 NVIDIA GeForce RTX 3070 WDDM | 00000000:05:00.0 On | N/A |&lt;/p&gt; &lt;p&gt;| 0% 35C P8 24W / 270W | 1627MiB / 8192MiB | 7% Default |&lt;/p&gt; &lt;p&gt;| | | N/A |&lt;/p&gt; &lt;p&gt;+-----------------------------------------+------------------------+----------------------+&lt;/p&gt; &lt;p&gt;+-----------------------------------------------------------------------------------------+&lt;/p&gt; &lt;p&gt;| Processes: |&lt;/p&gt; &lt;p&gt;| GPU GI CI PID Type Process name GPU Memory |&lt;/p&gt; &lt;p&gt;| ID ID Usage |&lt;/p&gt; &lt;p&gt;|=========================================================================================|&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 2228 C+G ....0.3595.94\msedgewebview2.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 3844 C+G ...8bbwe\PhoneExperienceHost.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 4100 C+G ...indows\System32\ShellHost.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 7580 C+G ...y\StartMenuExperienceHost.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 7756 C+G F:\Microsoft VS Code\Code.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 8228 C+G ...5n1h2txyewy\TextInputHost.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 11164 C+G ...2txyewy\CrossDeviceResume.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 12464 C+G ...ntrolPanel\SystemSettings.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 13332 C+G ...xyewy\ShellExperienceHost.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 14160 C+G ...em32\ApplicationFrameHost.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 14460 C+G ....0.3595.94\msedgewebview2.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 15884 C+G ..._cw5n1h2txyewy\SearchHost.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 17164 C+G ...s\Mozilla Firefox\firefox.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 17992 C+G ...4__cv1g1gvanyjgm\WhatsApp.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 18956 C+G C:\Windows\explorer.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 19076 C+G ...lare WARP\Cloudflare WARP.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 22612 C+G ...s\Mozilla Firefox\firefox.exe N/A |&lt;/p&gt; &lt;p&gt;+-----------------------------------------------------------------------------------------+&lt;/p&gt; &lt;p&gt;nvcc --version&lt;/p&gt; &lt;p&gt;nvcc: NVIDIA (R) Cuda compiler driver&lt;/p&gt; &lt;p&gt;Copyright (c) 2005-2025 NVIDIA Corporation&lt;/p&gt; &lt;p&gt;Built on Wed_Apr__9_19:29:17_Pacific_Daylight_Time_2025&lt;/p&gt; &lt;p&gt;Cuda compilation tools, release 12.9, V12.9.41&lt;/p&gt; &lt;p&gt;Build cuda_12.9.r12.9/compiler.35813241_0&lt;/p&gt; &lt;p&gt;ollama ps&lt;/p&gt; &lt;p&gt;NAME ID SIZE PROCESSOR CONTEXT UNTIL &lt;/p&gt; &lt;p&gt;qwen2.5-coder:1.5b d7372fd82851 1.9 GB 100% CPU 32768 Stopping...&lt;/p&gt; &lt;p&gt;I‚Äôm trying to use Ollama models locally in VS Code through the &lt;strong&gt;Cline&lt;/strong&gt; and &lt;a href="http://Continue.dev"&gt;&lt;strong&gt;Continue.dev&lt;/strong&gt;&lt;/a&gt; extensions to get something similar to Cursor‚Äôs AI-assisted coding workflow. The models work, but &lt;strong&gt;Ollama only uses my CPU and completely ignores my GPU&lt;/strong&gt;, even though I have an RTX 3070 with 8GB VRAM. I expected CUDA acceleration to kick in, but it looks like Ollama isn‚Äôt detecting or using the GPU at all.&lt;/p&gt; &lt;h1&gt;My setup:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Ryzen 5 5600X&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; NVIDIA GeForce RTX 3070 (8GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Drivers:&lt;/strong&gt; NVIDIA 581.57&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CUDA:&lt;/strong&gt; Installed (nvcc 12.9)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Models I‚Äôm running:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;DeepSeek R1 (~5GB)&lt;/li&gt; &lt;li&gt;Qwen2.5-Coder 1.5B (~1GB)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Run Ollama models locally with GPU acceleration inside VS Code (Cline / Continue.dev)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;Ollama is &lt;em&gt;only&lt;/em&gt; using the CPU:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL qwen2.5-coder:1.5b d7372fd82851 1.9 GB 100% CPU 32768 Stopping... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There is &lt;strong&gt;no GPU usage at all&lt;/strong&gt; when models load or run.&lt;/p&gt; &lt;h1&gt;NVIDIA-SMI Output&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;+-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 581.57 Driver Version: 581.57 CUDA Version: 13.0 | | GPU Name Driver-Model | Bus-Id Disp.A | Volatile Uncorr. ECC | | 0 NVIDIA GeForce RTX 3070 WDDM | Memory-Usage: 1627MiB / 8192MiB | Util: 7% | +-----------------------------------------------------------------------------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No Ollama process appears in the GPU process list.&lt;/p&gt; &lt;h1&gt;nvcc --version&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Cuda compilation tools, release 12.9, V12.9.41 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So CUDA toolkit is installed and working.&lt;/p&gt; &lt;h1&gt;What I Want to Know&lt;/h1&gt; &lt;p&gt;Is this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A &lt;strong&gt;known limitation&lt;/strong&gt; of Ollama on Windows?&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;config issue&lt;/strong&gt; (env vars, WSL2, driver mode, etc.)?&lt;/li&gt; &lt;li&gt;Something I set up incorrectly?&lt;/li&gt; &lt;li&gt;Or do some models not support GPU on Windows yet?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any advice on getting Ollama to actually use the GPU (especially for VS Code integrations) would be super appreciated.I‚Äôm trying to use Ollama models locally in VS Code through the Cline and &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; extensions to get something similar to Cursor‚Äôs AI-assisted coding workflow. The models work, but Ollama only uses my CPU and completely ignores my GPU, even though I have an RTX 3070 with 8GB VRAM. I expected CUDA acceleration to kick in, but it looks like Ollama isn‚Äôt detecting or using the GPU at all.&lt;br /&gt; My setup: &lt;/p&gt; &lt;p&gt;CPU: Ryzen 5 5600X &lt;/p&gt; &lt;p&gt;GPU: NVIDIA GeForce RTX 3070 (8GB VRAM) &lt;/p&gt; &lt;p&gt;Drivers: NVIDIA 581.57 &lt;/p&gt; &lt;p&gt;CUDA: Installed (nvcc 12.9) &lt;/p&gt; &lt;p&gt;Models I‚Äôm running: &lt;/p&gt; &lt;p&gt;DeepSeek R1 (~5GB) &lt;/p&gt; &lt;p&gt;Qwen2.5-Coder 1.5B (~1GB) &lt;/p&gt; &lt;p&gt;Goal: Run Ollama models locally with GPU acceleration inside VS Code (Cline / Continue.dev) &lt;/p&gt; &lt;p&gt;The Problem&lt;br /&gt; Ollama is only using the CPU:&lt;br /&gt; ollama ps&lt;br /&gt; NAME ID SIZE PROCESSOR CONTEXT UNTIL&lt;br /&gt; qwen2.5-coder:1.5b d7372fd82851 1.9 GB 100% CPU 32768 Stopping... &lt;/p&gt; &lt;p&gt;There is no GPU usage at all when models load or run. &lt;/p&gt; &lt;p&gt;NVIDIA-SMI Output&lt;br /&gt; +-----------------------------------------------------------------------------------------+&lt;br /&gt; | NVIDIA-SMI 581.57 Driver Version: 581.57 CUDA Version: 13.0 |&lt;br /&gt; | GPU Name Driver-Model | Bus-Id Disp.A | Volatile Uncorr. ECC |&lt;br /&gt; | 0 NVIDIA GeForce RTX 3070 WDDM | Memory-Usage: 1627MiB / 8192MiB | Util: 7% |&lt;br /&gt; +-----------------------------------------------------------------------------------------+ &lt;/p&gt; &lt;p&gt;No Ollama process appears in the GPU process list. &lt;/p&gt; &lt;p&gt;nvcc --version&lt;br /&gt; Cuda compilation tools, release 12.9, V12.9.41 &lt;/p&gt; &lt;p&gt;So CUDA toolkit is installed and working. &lt;/p&gt; &lt;p&gt;What I Want to Know&lt;br /&gt; Is this: &lt;/p&gt; &lt;p&gt;A known limitation of Ollama on Windows? &lt;/p&gt; &lt;p&gt;A config issue (env vars, WSL2, driver mode, etc.)? &lt;/p&gt; &lt;p&gt;Something I set up incorrectly? &lt;/p&gt; &lt;p&gt;Or do some models not support GPU on Windows yet? &lt;/p&gt; &lt;p&gt;Any advice on getting Ollama to actually use the GPU (especially for VS Code integrations) would be super appreciated.&lt;/p&gt; &lt;p&gt;I‚Äôm trying to use Ollama models locally in VS Code through the Cline and &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; extensions to get something similar to Cursor‚Äôs AI-assisted coding workflow. The models work, but Ollama only uses my CPU and completely ignores my GPU, even though I have an RTX 3070 with 8GB VRAM. I expected CUDA acceleration to kick in, but it looks like Ollama isn‚Äôt detecting or using the GPU at all.&lt;/p&gt; &lt;h1&gt;My setup:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;CPU: Ryzen 5 5600X&lt;/li&gt; &lt;li&gt;GPU: NVIDIA GeForce RTX 3070 (8GB VRAM)&lt;/li&gt; &lt;li&gt;Drivers: NVIDIA 581.57&lt;/li&gt; &lt;li&gt;CUDA: Installed (nvcc 12.9)&lt;/li&gt; &lt;li&gt;Models I‚Äôm running: &lt;ul&gt; &lt;li&gt;DeepSeek R1 (~5GB)&lt;/li&gt; &lt;li&gt;Qwen2.5-Coder 1.5B (~1GB)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Goal: Run Ollama models locally with GPU acceleration inside VS Code (Cline / Continue.dev)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;Ollama is only using the CPU:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL qwen2.5-coder:1.5b d7372fd82851 1.9 GB 100% CPU 32768 Stopping... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There is no GPU usage at all when models load or run.&lt;/p&gt; &lt;h1&gt;NVIDIA-SMI Output&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;+-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 581.57 Driver Version: 581.57 CUDA Version: 13.0 | | GPU Name Driver-Model | Bus-Id Disp.A | Volatile Uncorr. ECC | | 0 NVIDIA GeForce RTX 3070 WDDM | Memory-Usage: 1627MiB / 8192MiB | Util: 7% | +-----------------------------------------------------------------------------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No Ollama process appears in the GPU process list.&lt;/p&gt; &lt;h1&gt;nvcc --version&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Cuda compilation tools, release 12.9, V12.9.41 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So CUDA toolkit is installed and working.&lt;/p&gt; &lt;h1&gt;What I Want to Know&lt;/h1&gt; &lt;p&gt;Is this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A known limitation of Ollama on Windows?&lt;/li&gt; &lt;li&gt;A config issue (env vars, WSL2, driver mode, etc.)?&lt;/li&gt; &lt;li&gt;Something I set up incorrectly?&lt;/li&gt; &lt;li&gt;Or do some models not support GPU on Windows yet?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any advice on getting Ollama to actually use the GPU (especially for VS Code integrations) would be super appreciated.I‚Äôm trying to use Ollama models locally in VS Code through the Cline and &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; extensions to get something similar to Cursor‚Äôs AI-assisted coding workflow. The models work, but Ollama only uses my CPU and completely ignores my GPU, even though I have an RTX 3070 with 8GB VRAM. I expected CUDA acceleration to kick in, but it looks like Ollama isn‚Äôt detecting or using the GPU at all.&lt;br /&gt; My setup: &lt;/p&gt; &lt;p&gt;CPU: Ryzen 5 5600X &lt;/p&gt; &lt;p&gt;GPU: NVIDIA GeForce RTX 3070 (8GB VRAM) &lt;/p&gt; &lt;p&gt;Drivers: NVIDIA 581.57 &lt;/p&gt; &lt;p&gt;CUDA: Installed (nvcc 12.9) &lt;/p&gt; &lt;p&gt;Models I‚Äôm running: &lt;/p&gt; &lt;p&gt;DeepSeek R1 (~5GB) &lt;/p&gt; &lt;p&gt;Qwen2.5-Coder 1.5B (~1GB) &lt;/p&gt; &lt;p&gt;Goal: Run Ollama models locally with GPU acceleration inside VS Code (Cline / Continue.dev) &lt;/p&gt; &lt;p&gt;The Problem&lt;br /&gt; Ollama is only using the CPU:&lt;br /&gt; ollama ps&lt;br /&gt; NAME ID SIZE PROCESSOR CONTEXT UNTIL&lt;br /&gt; qwen2.5-coder:1.5b d7372fd82851 1.9 GB 100% CPU 32768 Stopping... &lt;/p&gt; &lt;p&gt;There is no GPU usage at all when models load or run. &lt;/p&gt; &lt;p&gt;NVIDIA-SMI Output&lt;br /&gt; +-----------------------------------------------------------------------------------------+&lt;br /&gt; | NVIDIA-SMI 581.57 Driver Version: 581.57 CUDA Version: 13.0 |&lt;br /&gt; | GPU Name Driver-Model | Bus-Id Disp.A | Volatile Uncorr. ECC |&lt;br /&gt; | 0 NVIDIA GeForce RTX 3070 WDDM | Memory-Usage: 1627MiB / 8192MiB | Util: 7% |&lt;br /&gt; +-----------------------------------------------------------------------------------------+ &lt;/p&gt; &lt;p&gt;No Ollama process appears in the GPU process list. &lt;/p&gt; &lt;p&gt;nvcc --version&lt;br /&gt; Cuda compilation tools, release 12.9, V12.9.41 &lt;/p&gt; &lt;p&gt;So CUDA toolkit is installed and working. &lt;/p&gt; &lt;p&gt;What I Want to Know&lt;br /&gt; Is this: &lt;/p&gt; &lt;p&gt;A known limitation of Ollama on Windows? &lt;/p&gt; &lt;p&gt;A config issue (env vars, WSL2, driver mode, etc.)? &lt;/p&gt; &lt;p&gt;Something I set up incorrectly? &lt;/p&gt; &lt;p&gt;Or do some models not support GPU on Windows yet? &lt;/p&gt; &lt;p&gt;Any advice on getting Ollama to actually use the GPU (especially for VS Code integrations) would be super appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/huza786"&gt; /u/huza786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbdmq3/ollama_not_using_gpu_rtx_3070_only_cpu_need_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbdmq3/ollama_not_using_gpu_rtx_3070_only_cpu_need_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbdmq3/ollama_not_using_gpu_rtx_3070_only_cpu_need_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T14:05:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbeesy</id>
    <title>We built a **3B local Git agent** that turns plain English into correct git commands ‚Äî matches GPT-OSS 120B accuracy (gitara)</title>
    <updated>2025-12-01T14:38:30+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pbeesy/we_built_a_3b_local_git_agent_that_turns_plain/"&gt; &lt;img alt="We built a **3B local Git agent** that turns plain English into correct git commands ‚Äî matches GPT-OSS 120B accuracy (gitara)" src="https://preview.redd.it/s72uc6fusl4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55e186276e8f039add893c488cace0821219c3ce" title="We built a **3B local Git agent** that turns plain English into correct git commands ‚Äî matches GPT-OSS 120B accuracy (gitara)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s72uc6fusl4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbeesy/we_built_a_3b_local_git_agent_that_turns_plain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbeesy/we_built_a_3b_local_git_agent_that_turns_plain/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T14:38:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbbht5</id>
    <title>AI Agent from scratch: Django + Ollama + Pydantic AI - A Step-by-Step Guide</title>
    <updated>2025-12-01T12:28:31+00:00</updated>
    <author>
      <name>/u/tom-mart</name>
      <uri>https://old.reddit.com/user/tom-mart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey-up Reddit. I‚Äôm excited to share my latest project with you, a detailed, step-by-step guide on building a basic AI agent using Django, Ollama, and Pydantic AI.&lt;/p&gt; &lt;p&gt;I‚Äôve broken down the entire process, making it accessible even if you‚Äôre just starting with Python. In the first part I'll show you how to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Set up a Django project with Django Ninja for rapid API development.&lt;/li&gt; &lt;li&gt;Integrate your local Ollama engine.&lt;/li&gt; &lt;li&gt;Use Pydantic AI to manage your agent‚Äôs context and tool calls.&lt;/li&gt; &lt;li&gt;Build a functional AI agent in just a few lines of code!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is a great starting point for anyone wanting to experiment with local LLMs and build their own AI agents from scratch.&lt;/p&gt; &lt;p&gt;Read the full article &lt;a href="https://medium.com/@tom.mart/build-self-hosted-ai-agent-with-ollama-pydantic-ai-and-django-ninja-53c6b3f14a1d"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In the next part I'll be diving into memory management ‚Äì giving your agent the ability to remember past conversations and interactions.&lt;/p&gt; &lt;p&gt;Looking forward to your comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tom-mart"&gt; /u/tom-mart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbbht5/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbbht5/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbbht5/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T12:28:31+00:00</published>
  </entry>
</feed>
