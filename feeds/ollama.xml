<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-12T14:34:49+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1r1acup</id>
    <title>PolyMCP Major Update: New Website, New Inspector UX, Installable Desktop App, and skills.sh-First Workflow</title>
    <updated>2026-02-10T19:23:51+00:00</updated>
    <author>
      <name>/u/Just_Vugg_PolyMCP</name>
      <uri>https://old.reddit.com/user/Just_Vugg_PolyMCP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r1acup/polymcp_major_update_new_website_new_inspector_ux/"&gt; &lt;img alt="PolyMCP Major Update: New Website, New Inspector UX, Installable Desktop App, and skills.sh-First Workflow" src="https://external-preview.redd.it/TTmX4QB47ieCB6lScf1c5zSQ8KES1TshcAp18basPSg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4529bf6eedf476025a82561414f9218da9edf0d1" title="PolyMCP Major Update: New Website, New Inspector UX, Installable Desktop App, and skills.sh-First Workflow" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Vugg_PolyMCP"&gt; /u/Just_Vugg_PolyMCP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/poly-mcp/PolyMCP"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1acup/polymcp_major_update_new_website_new_inspector_ux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1acup/polymcp_major_update_new_website_new_inspector_ux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T19:23:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r18kd0</id>
    <title>Open-Source Apple Silicon Local LLM Benchmarking Software. Would love some feedback!</title>
    <updated>2026-02-10T18:20:38+00:00</updated>
    <author>
      <name>/u/peppaz</name>
      <uri>https://old.reddit.com/user/peppaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r18kd0/opensource_apple_silicon_local_llm_benchmarking/"&gt; &lt;img alt="Open-Source Apple Silicon Local LLM Benchmarking Software. Would love some feedback!" src="https://external-preview.redd.it/--hFrBiIzmgKy0LazyUbsxh-mezAHM_pi4pFCex-GiU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25fef397f6029bcc19c322f5a7f3635232833d0e" title="Open-Source Apple Silicon Local LLM Benchmarking Software. Would love some feedback!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peppaz"&gt; /u/peppaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Cyberpunk69420/anubis-oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r18kd0/opensource_apple_silicon_local_llm_benchmarking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r18kd0/opensource_apple_silicon_local_llm_benchmarking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T18:20:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0vhy0</id>
    <title>What's the fastest-response model to run on AMD (no-GPU) machines ?</title>
    <updated>2026-02-10T08:39:10+00:00</updated>
    <author>
      <name>/u/mohamedheiba</name>
      <uri>https://old.reddit.com/user/mohamedheiba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey I'm running Ollama on Kubernetes. Help me choose the best model for text summarization and writing documentation based on code please.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hetzner &lt;a href="https://www.hetzner.com/dedicated-rootserver/ax102/"&gt;AX102&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ryzen 7950X3D processor with 16 vCPU&lt;/li&gt; &lt;li&gt;96MB 3D V-Cache&lt;/li&gt; &lt;li&gt;192 GB DDR5 RAM.&lt;/li&gt; &lt;li&gt;No GPU.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Use case:&lt;/strong&gt; AI agent (OpenClaw) orchestrated via n8n, heavy on tool calling / function calling. Needs 40K+ context window. Not doing chat ‚Äî it's purely agentic workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I've tried so far:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;qwen3:32b&lt;/code&gt; (dense) ‚Äî painfully slow on CPU, unusable&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3:30b-a3b-q8_0&lt;/code&gt; (MoE) ‚Äî much faster, works well, decent tool calling&lt;/li&gt; &lt;li&gt;&lt;code&gt;gpt-oss:20b&lt;/code&gt; (MoE, MXFP4) ‚Äî noticeably faster than Qwen3-30B, lightest memory footprint (~12-16GB). Impressed so far.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Now considering:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPT-OSS 20B&lt;/strong&gt; ‚Äî 21B/3.6B active, MXFP4 native, ~12-16GB RAM. Lightest option. Built-in tool calling. Concerned about the harmony format playing nice with n8n.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7-Flash&lt;/strong&gt; ‚Äî 30B/3B active, 128K context, best SWE-bench scores. Saw reports of Ollama template issues ‚Äî is that fixed?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sticking with Qwen3-30B-A3B&lt;/strong&gt; but Q4_K_M &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I haven't tried any of them yet with OpenClaw or n8n.&lt;/p&gt; &lt;p&gt;What are your recommendations ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mohamedheiba"&gt; /u/mohamedheiba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T08:39:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r11i8r</id>
    <title>Best model for Figma MCP server</title>
    <updated>2026-02-10T14:00:42+00:00</updated>
    <author>
      <name>/u/commandermd</name>
      <uri>https://old.reddit.com/user/commandermd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to design in Figma using a local LLM. The idea came after reading about using Claude to design in Figma in the browser. &lt;a href="https://cianfrani.dev/posts/a-better-figma-mcp/"&gt;https://cianfrani.dev/posts/a-better-figma-mcp/&lt;/a&gt; I love the concept but want to try to run locally. Nothing local could beat Claude with my setup. What would get close? I'm thinking &lt;a href="https://ollama.com/library/qwen3-vl:8b"&gt;qwen3-vl:8b&lt;/a&gt;. What should I look for in a model besides vision capabilities? Are there others that would work better? &lt;/p&gt; &lt;p&gt;Specs &amp;amp; Settings&lt;/p&gt; &lt;p&gt;AMD 5600G&lt;br /&gt; 64 GB DDR4&lt;br /&gt; 5060 TI 16GB&lt;br /&gt; Chrome Devtools CDP&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/commandermd"&gt; /u/commandermd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r11i8r/best_model_for_figma_mcp_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r11i8r/best_model_for_figma_mcp_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r11i8r/best_model_for_figma_mcp_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T14:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1bstj</id>
    <title>Preprocessing and prompt formatting with multimodal models</title>
    <updated>2026-02-10T20:15:21+00:00</updated>
    <author>
      <name>/u/AdaObvlada</name>
      <uri>https://old.reddit.com/user/AdaObvlada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have some coding experiences but am still pretty new to AI. So far I managed to set up a few local inferences, but I struggled with understanding the right preprocessing and more important prompt message formatting.&lt;/p&gt; &lt;p&gt;Example: &lt;a href="https://huggingface.co/dam2452/Qwen3-VL-Embedding-8B-GGUF"&gt;https://huggingface.co/dam2452/Qwen3-VL-Embedding-8B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HTTP payload example used by author:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;content&amp;quot;: &amp;quot;Your text or image data here&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But looking at the prompt construction in the helper functions for the original model here (line 250): &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-Embedding-8B/blob/main/scripts/qwen3_vl_embedding.py"&gt;https://huggingface.co/Qwen/Qwen3-VL-Embedding-8B/blob/main/scripts/qwen3_vl_embedding.py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I see, for example, for &lt;code&gt;image_content&lt;/code&gt; that it appends it as instance of PIL.Image&lt;br /&gt; &lt;code&gt;'type': 'image', 'image': image_content&lt;/code&gt; or first downloads it if it was passed as URL.&lt;/p&gt; &lt;p&gt;What exactly is author of the GGUF model expecting me to input then at &lt;code&gt;&amp;quot;content&amp;quot;: &amp;quot;Your text or image data here&amp;quot;&lt;/code&gt; Am I supposed think of passing image data as passing a string of RGB pixel information? The original model also expects min and max pixel metadata that is entirely missing from the other ones prompt.&lt;/p&gt; &lt;p&gt;I didn't check how it does the video but I expect it just grabs out selective frames.&lt;/p&gt; &lt;p&gt;Does it even matter as long as the prompt is consistent across embedding and later query encoding?&lt;/p&gt; &lt;p&gt;Thanks for all the tips.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdaObvlada"&gt; /u/AdaObvlada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1bstj/preprocessing_and_prompt_formatting_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1bstj/preprocessing_and_prompt_formatting_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1bstj/preprocessing_and_prompt_formatting_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T20:15:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1cz0l</id>
    <title>How much autonomy do you give your AI?</title>
    <updated>2026-02-10T20:58:13+00:00</updated>
    <author>
      <name>/u/Xthebuilder</name>
      <uri>https://old.reddit.com/user/Xthebuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i use AI in a few of my workflows. I noticed the more time i spent with tools the more I learn their limitations. I also realize that to more you do something unconsciously , the harder it is to bring it to the forefront to explain it. The relevance to all that is i wonder how much autonomy is okay and how much is not enough generally when using AI in workflows. I feel like there is a spectrum and every application of AI varies. So whats your workflow in general and how much autonomy does your AI have in it? Do you watch it like a hawk or only start checking if things don't look or feel right kinda like a sense.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xthebuilder"&gt; /u/Xthebuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1cz0l/how_much_autonomy_do_you_give_your_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1cz0l/how_much_autonomy_do_you_give_your_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1cz0l/how_much_autonomy_do_you_give_your_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T20:58:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r132zl</id>
    <title>My Journey Building an AI Agent Orchestrator</title>
    <updated>2026-02-10T15:02:13+00:00</updated>
    <author>
      <name>/u/PuzzleheadedFail3131</name>
      <uri>https://old.reddit.com/user/PuzzleheadedFail3131</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;# üéÆ 88% Success Rate with qwen2.5-coder:7b on RTX 3060 Ti - My Journey Building an AI Agent Orchestrator **TL;DR:** Built a tiered AI agent system where Ollama handles 88% of tasks for FREE, with automatic escalation to Claude for complex work. Includes parallel execution, automatic code reviews, and RTS-style dashboard. ## Why This Matters for After months of testing, I've proven that **local models can handle real production workloads** with the right architecture. Here's the breakdown: ### The Setup - **Hardware:** RTX 3060 Ti (8GB VRAM) - **Model:** qwen2.5-coder:7b (4.7GB) - **Temperature:** 0 (critical for tool calling!) - **Context Management:** 3s rest between tasks + 8s every 5 tasks ### The Results (40-Task Stress Test) - **C1-C8 tasks: 100% success** (20/20) - **C9 tasks: 80% success** (LeetCode medium, class implementations) - **Overall: 88% success** (35/40 tasks) - **Average execution: 0.88 seconds** ### What Works ‚úÖ File I/O operations ‚úÖ Algorithm implementations (merge sort, binary search) ‚úÖ Class implementations (Stack, RPN Calculator) ‚úÖ LeetCode Medium (LRU Cache!) ‚úÖ Data structure operations ### The Secret Sauce **1. Temperature 0** This was the game-changer. T=0.7 ‚Üí model outputs code directly. T=0 ‚Üí reliable tool calling. **2. Rest Between Tasks** Context pollution is real! Without rest: 85% success. With rest: 100% success (C1-C8). **3. Agent Persona (&amp;quot;CodeX-7&amp;quot;)** Gave the model an elite agent identity with mission examples. Completion rates jumped significantly. Agents need personality! **4. Stay in VRAM** Tested 14B model ‚Üí CPU offload ‚Üí 40% pass rate 7B model fully in VRAM ‚Üí 88-100% pass rate **5. Smart Escalation** Tasks that fail escalate to Claude automatically. Best of both worlds. ### The Architecture ``` Task Queue ‚Üí Complexity Router ‚Üí Resource Pool ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚Üì ‚Üì ‚Üì Ollama Haiku Sonnet (C1-6) (C7-8) (C9-10) FREE! $0.003 $0.01 ‚Üì ‚Üì ‚Üì Automatic Code Reviews (Haiku every 5th, Opus every 10th) ``` ### Cost Comparison (10-task batch) - **All Claude Opus:** ~$15 - **Tiered (mostly Ollama):** ~$1.50 - **Savings:** 90% ### GitHub https://github.com/mrdushidush/agent-battle-command-center Full Docker setup, just needs Ollama + optional Claude API for fallback. ## Questions for the Community 1. **Has anyone else tested qwen2.5-coder:7b for production?** How do your results compare? 2. **What's your sweet spot for VRAM vs model size?** 3. **Agent personas - placebo or real?** My tests suggest real improvement but could be confirmation bias. 4. **Other models?** Considering DeepSeek Coder v2 next. --- **Stack:** TypeScript, Python, FastAPI, CrewAI, Ollama, Docker **Status:** Production ready, all tests passing Let me know if you want me to share the full prompt engineering approach or stress test methodology! &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PuzzleheadedFail3131"&gt; /u/PuzzleheadedFail3131 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T15:02:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1474m</id>
    <title>A Ralph Loop for Reading: Beating GPT 5.2 with a 4k Context Window &amp; Ollama</title>
    <updated>2026-02-10T15:43:46+00:00</updated>
    <author>
      <name>/u/smhanov</name>
      <uri>https://old.reddit.com/user/smhanov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r1474m/a_ralph_loop_for_reading_beating_gpt_52_with_a_4k/"&gt; &lt;img alt="A Ralph Loop for Reading: Beating GPT 5.2 with a 4k Context Window &amp;amp; Ollama" src="https://external-preview.redd.it/PcLeb8yolVzvJxYoO6wR-sH415-VkF75d4IUzvwdPfo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b997b5b30b859d417fafb336d277eb90311962a3" title="A Ralph Loop for Reading: Beating GPT 5.2 with a 4k Context Window &amp;amp; Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLM costs are getting out of hand. I made an AI agent for research that works well using only qwen3:4b with Ollama. It's all about managing the context window smartly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smhanov"&gt; /u/smhanov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://stevehanov.ca/blog/a-ralph-loop-for-reading-beating-gpt-52-with-a-4k-context-window-and-4-gpus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1474m/a_ralph_loop_for_reading_beating_gpt_52_with_a_4k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1474m/a_ralph_loop_for_reading_beating_gpt_52_with_a_4k/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T15:43:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1r20j9k</id>
    <title>GLM 5 Just released !</title>
    <updated>2026-02-11T15:34:39+00:00</updated>
    <author>
      <name>/u/No-Intention-5521</name>
      <uri>https://old.reddit.com/user/No-Intention-5521</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r20j9k/glm_5_just_released/"&gt; &lt;img alt="GLM 5 Just released !" src="https://external-preview.redd.it/PElQ7F-MlQ6E8G-ybtZ13GI60Cflo6wm1ZoH_W1kihc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6e08cd79ac643cc33ce6d7d45a63125b8c6c2c7" title="GLM 5 Just released !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Intention-5521"&gt; /u/No-Intention-5521 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://glm5.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r20j9k/glm_5_just_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r20j9k/glm_5_just_released/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T15:34:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1urix</id>
    <title>whats the best open source ai video generator?</title>
    <updated>2026-02-11T11:20:54+00:00</updated>
    <author>
      <name>/u/DoubleSubstantial805</name>
      <uri>https://old.reddit.com/user/DoubleSubstantial805</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i have 4050 rtx, and i want to generate some ai videos. anyone in here who is knows of such models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DoubleSubstantial805"&gt; /u/DoubleSubstantial805 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1urix/whats_the_best_open_source_ai_video_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1urix/whats_the_best_open_source_ai_video_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1urix/whats_the_best_open_source_ai_video_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T11:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1v1jk</id>
    <title>question about usage API fees. Also are local LLMs good? want to know if my specs are enough</title>
    <updated>2026-02-11T11:36:06+00:00</updated>
    <author>
      <name>/u/industrysaurus</name>
      <uri>https://old.reddit.com/user/industrysaurus</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/industrysaurus"&gt; /u/industrysaurus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1r1uxqc/question_about_usage_api_fees_also_are_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1v1jk/question_about_usage_api_fees_also_are_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1v1jk/question_about_usage_api_fees_also_are_local_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T11:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1tr5p</id>
    <title>Suggestions for project for pdf translation, summary and keyword extraction</title>
    <updated>2026-02-11T10:22:29+00:00</updated>
    <author>
      <name>/u/Radiant_Ad9653</name>
      <uri>https://old.reddit.com/user/Radiant_Ad9653</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I am working on a project which runs on my local llm(I am running 2 Mac Studio with total 1Gb VRAM). My aim is to translate documents, summarize them and extract keywords based on my requirement. There is no control over the document size but I dont want to do analysis of more than 10-20 pages probably. &lt;/p&gt; &lt;p&gt;Another Idea on my mind is if we are able yo recreate the original document trnaslated in english. Not req to look exactly the same but whatever best is possible. &lt;/p&gt; &lt;p&gt;Main requirment is for the system to be offline at all stages. &lt;/p&gt; &lt;p&gt;Can yiu guys show me some on going project? Open source if you know. I want this to be part of an already working pipeline. So GUI is not requirement.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Radiant_Ad9653"&gt; /u/Radiant_Ad9653 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1tr5p/suggestions_for_project_for_pdf_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1tr5p/suggestions_for_project_for_pdf_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1tr5p/suggestions_for_project_for_pdf_translation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T10:22:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2dba3</id>
    <title>OMG i just ran local models for 70% cheaper with this OSS tool (Tandemn Tuna)</title>
    <updated>2026-02-11T23:29:05+00:00</updated>
    <author>
      <name>/u/Research_Still</name>
      <uri>https://old.reddit.com/user/Research_Still</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r2dba3/omg_i_just_ran_local_models_for_70_cheaper_with/"&gt; &lt;img alt="OMG i just ran local models for 70% cheaper with this OSS tool (Tandemn Tuna)" src="https://external-preview.redd.it/_QAEV8Z_7oErmkJRvOa21yS6UzAzTVD4XDQvCGHcy1E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be7226b3adcf61373c3a6df188a279dbf7047e7f" title="OMG i just ran local models for 70% cheaper with this OSS tool (Tandemn Tuna)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Research_Still"&gt; /u/Research_Still &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1r2d8jy/omg_i_just_ran_local_models_for_70_cheaper_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2dba3/omg_i_just_ran_local_models_for_70_cheaper_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2dba3/omg_i_just_ran_local_models_for_70_cheaper_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T23:29:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r21l7g</id>
    <title>The Next Generation of On-Device AI</title>
    <updated>2026-02-11T16:14:13+00:00</updated>
    <author>
      <name>/u/Unique_Winner_5927</name>
      <uri>https://old.reddit.com/user/Unique_Winner_5927</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With tools like Ollama and optimized models like Liquid AI's LFM-2.5, we're entering an era where powerful AI runs on your local hardware. No cloud dependency, no privacy concerns, no recurring costs.&lt;/p&gt; &lt;p&gt;The barrier to entry has never been lower. If you have a laptop from the last 5 years, you can run AI agents locally.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.liquid.ai/blog/introducing-lfm2-5-the-next-generation-of-on-device-ai"&gt;https://www.liquid.ai/blog/introducing-lfm2-5-the-next-generation-of-on-device-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama"&gt;https://github.com/ollama/ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Give it a try!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unique_Winner_5927"&gt; /u/Unique_Winner_5927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r21l7g/the_next_generation_of_ondevice_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r21l7g/the_next_generation_of_ondevice_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r21l7g/the_next_generation_of_ondevice_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T16:14:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2296k</id>
    <title>How to use ollama launch claude with ollama launch with dangerously-skip-permissions?</title>
    <updated>2026-02-11T16:39:01+00:00</updated>
    <author>
      <name>/u/vcliment89</name>
      <uri>https://old.reddit.com/user/vcliment89</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As title says, how can I run the ollama launch command to use claude with dangerously-skip-permissions? This obviously is not working. What could be an alternative?&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama launch claude --dangerously-skip-permissions&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vcliment89"&gt; /u/vcliment89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2296k/how_to_use_ollama_launch_claude_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2296k/how_to_use_ollama_launch_claude_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2296k/how_to_use_ollama_launch_claude_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T16:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2i4m4</id>
    <title>I've created the perfect platform for open-source AI-powered web development.</title>
    <updated>2026-02-12T03:01:37+00:00</updated>
    <author>
      <name>/u/LeadingFun1849</name>
      <uri>https://old.reddit.com/user/LeadingFun1849</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r2i4m4/ive_created_the_perfect_platform_for_opensource/"&gt; &lt;img alt="I've created the perfect platform for open-source AI-powered web development." src="https://preview.redd.it/yhosdvepbzig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5f407b406e09438dd981aaae2b604484f85c5cf" title="I've created the perfect platform for open-source AI-powered web development." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by &lt;a href="http://Lovable.dev"&gt;Lovable.dev&lt;/a&gt;, Vercel v0, and Google Labs' Stitch, it combines cutting-edge AI orchestration with browser-based execution to deliver the most advanced open-source alternative for rapid frontend prototyping.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeadingFun1849"&gt; /u/LeadingFun1849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yhosdvepbzig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2i4m4/ive_created_the_perfect_platform_for_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2i4m4/ive_created_the_perfect_platform_for_opensource/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T03:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2ggik</id>
    <title>A love song to the molts.</title>
    <updated>2026-02-12T01:46:15+00:00</updated>
    <author>
      <name>/u/ttvbkofam</name>
      <uri>https://old.reddit.com/user/ttvbkofam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r2ggik/a_love_song_to_the_molts/"&gt; &lt;img alt="A love song to the molts." src="https://external-preview.redd.it/IsE4W0eIojB0HMFrdspuOtrtKGs3LU8mV0wdj0ME-hE.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=067f067f44d59778d9fbd64e664f38d7a53bc123" title="A love song to the molts." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttvbkofam"&gt; /u/ttvbkofam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Moltbook/comments/1r0kq8u/a_love_song_to_the_molts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2ggik/a_love_song_to_the_molts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2ggik/a_love_song_to_the_molts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T01:46:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2eveh</id>
    <title>JRVS Update;260+ stars later. I need honest feedback from people actually using it.</title>
    <updated>2026-02-12T00:35:03+00:00</updated>
    <author>
      <name>/u/Xthebuilder</name>
      <uri>https://old.reddit.com/user/Xthebuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone üëã&lt;/p&gt; &lt;p&gt;A little while ago I shared JRVS here my local AI assistant with persistent memory and a built-in knowledge base.&lt;/p&gt; &lt;p&gt;Since then it‚Äôs grown to 260+ GitHub stars, which honestly surprised me. Thank you to everyone who checked it out.&lt;/p&gt; &lt;p&gt;Now I‚Äôm at an important stage and I need real feedback from people who are actually using JRVS (not just starring it).&lt;/p&gt; &lt;p&gt;If you‚Äôve run it locally, experimented with it, or tried integrating it into your workflow, I‚Äôd really appreciate your thoughts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How are you actually using JRVS?&lt;/li&gt; &lt;li&gt;What feels powerful?&lt;/li&gt; &lt;li&gt;What feels confusing or unnecessary?&lt;/li&gt; &lt;li&gt;What would make you use it daily?&lt;/li&gt; &lt;li&gt;What‚Äôs missing?&lt;/li&gt; &lt;li&gt;Would you trust it as your main local AI assistant?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm considering evolving JRVS further (possibly a more polished version / edge-focused build), but I don‚Äôt want to build in the wrong direction.&lt;/p&gt; &lt;p&gt;Brutal honesty is welcome. This project only gets better if the feedback is real.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/Xthebuilder/JRVS"&gt;https://github.com/Xthebuilder/JRVS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks again to everyone who‚Äôs supported it so far üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xthebuilder"&gt; /u/Xthebuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2eveh/jrvs_update260_stars_later_i_need_honest_feedback/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2eveh/jrvs_update260_stars_later_i_need_honest_feedback/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2eveh/jrvs_update260_stars_later_i_need_honest_feedback/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T00:35:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r27yqt</id>
    <title>GLM5 in Ollama</title>
    <updated>2026-02-11T20:03:45+00:00</updated>
    <author>
      <name>/u/quantumsequrity</name>
      <uri>https://old.reddit.com/user/quantumsequrity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys they've released GLM 5 cloud version in ollama go try it out, it's pretty cool not upto claude opus 4.5 or 4.6 for a open source model it's efficient,..&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/glm-5"&gt;https://ollama.com/library/glm-5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantumsequrity"&gt; /u/quantumsequrity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r27yqt/glm5_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r27yqt/glm5_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r27yqt/glm5_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T20:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2qrkn</id>
    <title>Testing LLM's</title>
    <updated>2026-02-12T11:15:05+00:00</updated>
    <author>
      <name>/u/ThaLazyLand</name>
      <uri>https://old.reddit.com/user/ThaLazyLand</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThaLazyLand"&gt; /u/ThaLazyLand &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1r2q631/testing_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2qrkn/testing_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2qrkn/testing_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T11:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2nyqv</id>
    <title>Ollama's cloud plan token limitations</title>
    <updated>2026-02-12T08:20:19+00:00</updated>
    <author>
      <name>/u/TerryTheAwesomeKitty</name>
      <uri>https://old.reddit.com/user/TerryTheAwesomeKitty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I worked with Ollama a year or so ago, and I remained with a great impression. When I picked my work back up for professional reasons, I saw they launched a cloud service!&lt;/p&gt; &lt;p&gt;I decided to try out the 20 USD/month cloud plan, but my usage was filling up.&lt;br /&gt; Weirdly enough, I could not find an exact max token count on their website, only percentages for usages in a 4 hour session and a weekly session.&lt;/p&gt; &lt;p&gt;I got the most expensive plan ( 100 USD/month ) to try to solve that issue. &lt;/p&gt; &lt;p&gt;**However** I also decided to email Ollama's support team to inquire about the specific limitations-- what the max number of tokens is, etc.&lt;br /&gt; This is their response:&lt;/p&gt; &lt;p&gt;&amp;quot;We've designed our plans around use case intensity rather than hard token caps. The free tier is for light experimentation. Pro is built for day-to-day work like chat, document analysis, and coding assistance. Max is for heavier usage including coding agents and batch processing. These plans aren't currently designed for sustained production API usage‚Äîif that's what you're looking for, I'd love to hear more about your specific use case so we can factor it into future plans.&amp;quot;&lt;/p&gt; &lt;p&gt;What is this??? Their limitations are defined by &amp;quot;light experimentation&amp;quot;, &amp;quot;day-to-day work&amp;quot; and &amp;quot;heavier usage&amp;quot;???&lt;br /&gt; Has Ollama really degraded that much? I was eager to support them financially but what I recieved is a little dissapointing....&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TerryTheAwesomeKitty"&gt; /u/TerryTheAwesomeKitty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2nyqv/ollamas_cloud_plan_token_limitations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2nyqv/ollamas_cloud_plan_token_limitations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2nyqv/ollamas_cloud_plan_token_limitations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T08:20:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2or7i</id>
    <title>If your Ollama RAG still feels cursed, here is my 16-problem map and 131 math questions (MIT)</title>
    <updated>2026-02-12T09:11:01+00:00</updated>
    <author>
      <name>/u/StarThinker2025</name>
      <uri>https://old.reddit.com/user/StarThinker2025</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r2or7i/if_your_ollama_rag_still_feels_cursed_here_is_my/"&gt; &lt;img alt="If your Ollama RAG still feels cursed, here is my 16-problem map and 131 math questions (MIT)" src="https://preview.redd.it/pm7w8r5d51jg1.png?width=140&amp;amp;height=107&amp;amp;auto=webp&amp;amp;s=354591eedc625c99fde685d217659a3130aa4733" title="If your Ollama RAG still feels cursed, here is my 16-problem map and 131 math questions (MIT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, Ôº© am PSBigBig, indie dev , no company, no sponsor just too many nights with local LLMs, RAG pipelines and notebooks&lt;/p&gt; &lt;p&gt;last year i basically disappeared from normal life and spent 3000+ hours building something i call &lt;strong&gt;WFGY Series ( Github 1.4k)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;it is not a model&lt;br /&gt; not a SaaS&lt;br /&gt; just TXT files + math + some small workflows you can feed into any strong LLM&lt;/p&gt; &lt;p&gt;for ollama people i think there are two concrete things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;a 16-problem map, works like a RAG / agent failure checklist&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;a more hardcore pack: 131 math-based ‚Äútension‚Äù problems inside one TXT&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;everything is MIT license and text only you can run it with OpenAI, Claude, or your local Qwen/Mistral in ollama you can also fork, steal, rewrite, as long as you keep MIT, i am totally fine&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;WFGY 2.0 ‚Äì 16 failure modes as a RAG clinic for local pipelines&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;most of my work is RAG + tools, many times running on local models first&lt;/p&gt; &lt;p&gt;the story is probably same as many people here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you wire up embed model, vector store, rerank, nice prompt&lt;/li&gt; &lt;li&gt;logs look ‚Äúok‚Äù, latency ok&lt;/li&gt; &lt;li&gt;but in real use, answers feel cursed, unstable, hard to debug&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;for a long time i just said ‚Äúit is hallucination‚Äù after some point this word is useless for me&lt;/p&gt; &lt;p&gt;so i started writing small incident notes and every time something exploded i gave it a simple number&lt;/p&gt; &lt;p&gt;over time this became a 16-problem map for LLM systems it is basically my ‚Äúcore RAG / agent failure taxonomy‚Äù&lt;/p&gt; &lt;p&gt;very roughly, the families look like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;retrieval / embedding problems&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;right doc wrong chunk, chunk too big/small, metric mismatch, hybrid weights off, no rerank&lt;/li&gt; &lt;li&gt;&lt;strong&gt;reasoning problems&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;question slowly drift after few hops, constraints dropped, answer optimize for style not for invariants&lt;/li&gt; &lt;li&gt;&lt;strong&gt;memory / long horizon problems&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;model believes its own earlier speculation, multi tool / multi agent flows overwrite each other&lt;/li&gt; &lt;li&gt;&lt;strong&gt;deployment / infra problems&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;index empty on first call, half rebuilt index after deploy, wrong startup order so first user become unlucky tester&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;for each of the 16, i tried to define:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;short description in human language&lt;/li&gt; &lt;li&gt;what symptoms you see in logs / user reports&lt;/li&gt; &lt;li&gt;typical root cause pattern&lt;/li&gt; &lt;li&gt;minimal &lt;em&gt;structural&lt;/em&gt; fix&lt;/li&gt; &lt;li&gt;(not just ‚Äúadd more DO NOT in system prompt‚Äù)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;some dev friends around me already use this map as a base layer&lt;br /&gt; they design a new pipeline and ask:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;‚Äúok, which problem numbers are we likely to hit with this design?‚Äù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;then they add small guards:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;extra semantic check node&lt;/li&gt; &lt;li&gt;small bootstrap script for vector store&lt;/li&gt; &lt;li&gt;constraint check before final answer, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;on top of the WFGY Problem Map page there is also a ‚ÄúDr WFGY‚Äù / ER ChatGPT link&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;if you have ChatGPT account you can click that and dump your RAG pipeline / logs / prompt and ask it to map your case to one or more of the 16 problems and suggest structural fix ideas&lt;/p&gt; &lt;p&gt;it is basically a &lt;strong&gt;24/7 RAG clinic&lt;/strong&gt; powered by that problem map nothing commercial, just my own debugging brain turned into a GPT&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. WFGY 3.0 ‚Äì 131 math-based problems as hardcore tests for local models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;the second thing is more crazy and more math&lt;/p&gt; &lt;p&gt;after WFGY 1.0 (PDF with formulas in prompts) and 2.0 (16 failure modes) i wanted to push the question:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;what kind of math structure actually makes LLM reasoning more stable, not just ‚Äúprompt looks nicer‚Äù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;so in WFGY 3.0 i wrote &lt;strong&gt;131 ‚ÄúS-class&lt;/strong&gt;‚Äù problems in a small tension-style language&lt;/p&gt; &lt;p&gt;many of them have explicit math inside, not only prose&lt;/p&gt; &lt;p&gt;for example, there are problems with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;custom free-energy functionals and weird energy landscapes&lt;/li&gt; &lt;li&gt;zeta-like objects and critical lines for ‚Äúimaginary tension‚Äù&lt;/li&gt; &lt;li&gt;symbolic constraints mixing logic and geometry&lt;/li&gt; &lt;li&gt;precise state spaces for agents, markets, physics, consciousness, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;all of this lives in one TXT pack&lt;/p&gt; &lt;p&gt;you can load it into any strong model including your local models in ollama&lt;/p&gt; &lt;p&gt;for example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you can run a local Qwen, Mistral, Llama, etc.&lt;/li&gt; &lt;li&gt;feed the TXT as system/context&lt;/li&gt; &lt;li&gt;drive the model through these problems as long-horizon stress tests&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;in my own experiments, prompts that carry this kind of math:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;tend to be more stable than pure natural language&lt;/li&gt; &lt;li&gt;give clearer invariants (things the model should not break)&lt;/li&gt; &lt;li&gt;make it easier to define ‚Äúforbidden moves‚Äù in a reasoning chain&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;this is my personal result, not a finished paper so i am basically open-sourcing my math and saying:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;if you build local LLMs / tools / eval, please steal these structures and see what happens&lt;/p&gt; &lt;/blockquote&gt; &lt;ol&gt; &lt;li&gt;what can an ollama user actually do with this?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;some concrete ideas:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;take one of the 131 problems and use it as a fixed eval prompt&lt;/li&gt; &lt;li&gt;run it on different local models / quant configs see where reasoning breaks, not only accuracy&lt;/li&gt; &lt;li&gt;mine the formulas from a problem and turn it into: &lt;ul&gt; &lt;li&gt;part of your system prompt&lt;/li&gt; &lt;li&gt;or a small synthetic dataset for fine-tune / lora&lt;/li&gt; &lt;li&gt;or guardrail rules inside your tool pipeline&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;combine 2.0 + 3.0: first use the 16-problem map to find where your RAG breaks&lt;/li&gt; &lt;li&gt;then after you patch it, use some of the 131 problems&lt;/li&gt; &lt;li&gt;as long-horizon ‚Äústress tests‚Äù to see if your fix survives&lt;/li&gt; &lt;li&gt;build your own open eval suite: if you take even 10 of the 131 questions and formalize them as tasks you already have a small but very weird benchmark for local reasoning / agent behavior&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;everything is pure text and MIT so you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;fork the repo&lt;/li&gt; &lt;li&gt;rename everything&lt;/li&gt; &lt;li&gt;wrap into your own library or UI&lt;/li&gt; &lt;li&gt;publish your own framework on top&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;if it helps, great&lt;br /&gt; if you break it and show me why it is bad, also great.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;why i am giving this away for free&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;i know many local devs do not have time to invent new ‚ÄúAI math‚Äù or design 100+ long-horizon eval tasks by themselves&lt;/p&gt; &lt;p&gt;this is basically 3000+ hours of my life compressed into:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;one 16-problem map for debugging&lt;/li&gt; &lt;li&gt;one 131-question pack for reasoning / math stress test&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;some people already treat WFGY as a ‚Äúlogic layer‚Äù under their own code but i do not have enough time or power to turn every idea into a product&lt;/p&gt; &lt;p&gt;so i decided to just open it under MIT and let other people run faster than me if they want&lt;/p&gt; &lt;p&gt;if this thing slowly becomes popular, maybe later it will feel hard to catch up so if any of this sounds interesting for your ollama setup, probably better to clone now and play with it while it is still ‚Äújust txt files‚Äù&lt;/p&gt; &lt;p&gt;&lt;strong&gt;entry point (from here you can find the Problem Map and the 3.0 demo via the compass&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md"&gt;https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pm7w8r5d51jg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6334c4c99791529b63dccb547da9cbc0549c06ac"&gt;WFGY 2.0 - 16 Problem maps&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StarThinker2025"&gt; /u/StarThinker2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2or7i/if_your_ollama_rag_still_feels_cursed_here_is_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2or7i/if_your_ollama_rag_still_feels_cursed_here_is_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2or7i/if_your_ollama_rag_still_feels_cursed_here_is_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T09:11:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2swvo</id>
    <title>First time local ai</title>
    <updated>2026-02-12T13:05:42+00:00</updated>
    <author>
      <name>/u/VariousAd3474</name>
      <uri>https://old.reddit.com/user/VariousAd3474</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi ive started to play around with ollama, open webui and comfyui. Im still learning alot. I wanted to get current opinions of the llm models to use with ollama on a lower end pc. From what ive read i should stay at ~8b or lower&lt;/p&gt; &lt;p&gt;I'm looking for an llm that is good for general questions and reasoning. Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VariousAd3474"&gt; /u/VariousAd3474 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2swvo/first_time_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2swvo/first_time_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2swvo/first_time_local_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T13:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2phik</id>
    <title>Non-Existent Data LLM Stress Testing - Hallucination Rates Amongst LLMs without Domain-Specific Knowledge</title>
    <updated>2026-02-12T09:59:06+00:00</updated>
    <author>
      <name>/u/Thor110</name>
      <uri>https://old.reddit.com/user/Thor110</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r2phik/nonexistent_data_llm_stress_testing_hallucination/"&gt; &lt;img alt="Non-Existent Data LLM Stress Testing - Hallucination Rates Amongst LLMs without Domain-Specific Knowledge" src="https://external-preview.redd.it/tCt4PzUCIXeBuxrdrg8V8mZlkyVBiJIxHC12AiS3cKM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85428e72b80555c63a788f0ab2d8feffa42e4fa1" title="Non-Existent Data LLM Stress Testing - Hallucination Rates Amongst LLMs without Domain-Specific Knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I conducted a short experiment over the past four days on hallucination rates amongst LLMs without domain specific knowledge.&lt;/p&gt; &lt;p&gt;I encourage people to educate themselves with regards to how these systems really work and the many dangers they present, which you are likely all familiar with by now.&lt;/p&gt; &lt;p&gt;The repository includes a custom built tool for automating the prompt-response cycle locally through Ollama as well as all of the results of the study I conducted.&lt;/p&gt; &lt;p&gt;This is a direct link to the paper itself.&lt;/p&gt; &lt;p&gt;The results themselves can be found displayed in the spreadsheet within the repository.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thor110"&gt; /u/Thor110 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Thor110/Non-Existent-Data-LLM-Stress-Testing/blob/main/Non-Existent%20Data%20LLM%20Stress%20Testing.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2phik/nonexistent_data_llm_stress_testing_hallucination/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2phik/nonexistent_data_llm_stress_testing_hallucination/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T09:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2ex9k</id>
    <title>Just try gpt-oss:20b</title>
    <updated>2026-02-12T00:37:17+00:00</updated>
    <author>
      <name>/u/newz2000</name>
      <uri>https://old.reddit.com/user/newz2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a MacBook Air with 24gb ram (M2) and when I set the context to 32k I can really do about everything I want a local model to do for normal business stuff. Tokens/sec is about 15 at medium reasoning, which means it produces words a little faster than I can type.&lt;/p&gt; &lt;p&gt;I also tested on an older Linux machine with 64gb ram and a GTX gpu with 8gb vram and it worked fine doing batch processing overnight. A little too slow for interactive use though.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scripting - yes&lt;/li&gt; &lt;li&gt;Calling tools - yes&lt;/li&gt; &lt;li&gt;Summarizing long content - yes&lt;/li&gt; &lt;li&gt;Writing content - yes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here‚Äôs how I used it:&lt;/p&gt; &lt;p&gt;Create a file named &lt;code&gt;Modelfile-agent-gpt-oss-20b&lt;/code&gt; and put the following in it&lt;/p&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM gpt-oss:20b # 1. Hardware-Aware Context PARAMETER num_ctx 32768 # 2. Anti-Loop Parameters # Penalize repeated tokens and force variety in phrasing PARAMETER repeat_penalty 1.2 PARAMETER repeat_last_n 128 # Temperature at 0.1 makes it more deterministic (less 'drifting' into loops) PARAMETER temperature 0.1 # 3. Agentic Steering SYSTEM &amp;quot;&amp;quot;&amp;quot; You are a 'one-shot' execution agent. To prevent reasoning loops, follow these strict rules: If a tool output is the same as a previous attempt, do NOT retry the same parameters. If you are stuck, state 'I am unable to progress with the current toolset' and stop. Every &amp;lt;thought&amp;gt; must provide NEW information. Do not repeat the user's instructions back to them. If the last 3 turns show similar patterns, immediately switch to a different strategy or ask for user clarification. &amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;p&gt;At the terminal type:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama create gpt-oss-agent -f Modelfile-aget-gpt-oss-20b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Now you can use the model ‚Äúgpt-oss-agent‚Äù like you would any other model.&lt;/p&gt; &lt;p&gt;I used opencode using this command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama launch opencode --model gpt-oss-agent&lt;/code&gt;&lt;/p&gt; &lt;p&gt;That let me do Claude-code style activities bough Claude is way more capable.&lt;/p&gt; &lt;p&gt;With a bunch of browser tabs open and a few apps I was using about 22gb of ram and 3gb of swap. During longer activities using other apps was laggy but usable.&lt;/p&gt; &lt;p&gt;On my computer I use for batch tasks I have python scripts that use the ollama python library. I use a tool like Claude code to create the script.&lt;/p&gt; &lt;p&gt;I‚Äôm a lawyer and use this for processing lots of documents. Sorting them, looking for interesting information, cataloging them. There are a lot of great models for this. But with this model I was able to produce better output.&lt;/p&gt; &lt;p&gt;Also, I can run tools. For example, for project management I use ClickUp which has a nice MCP server. I set it up with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;opencode mcp add&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then put in the url and follow the instructions. Since that mcp server requires authentication I use this:&lt;/p&gt; &lt;p&gt;&lt;code&gt;opencode mcp auth ClickUp&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then again follow the instructions.&lt;/p&gt; &lt;p&gt;**Edit: Fixed terrible formatting&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newz2000"&gt; /u/newz2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T00:37:17+00:00</published>
  </entry>
</feed>
