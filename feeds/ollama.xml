<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-21T23:48:49+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1nkk0s5</id>
    <title>Offline Ollama GUI Help</title>
    <updated>2025-09-18T20:52:30+00:00</updated>
    <author>
      <name>/u/ExplorerOk996</name>
      <uri>https://old.reddit.com/user/ExplorerOk996</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying to get the Ollama GUI working on an offline windows 10 pc with no luck. It works fine with the command prompt as far as I know. If I try to use ollama app.exe, it just &amp;quot;hangs&amp;quot;. &lt;/p&gt; &lt;p&gt;I downloaded the ollama windows installer from the ollama website on my laptop. I then copied that installer onto the pc and ran it. After that, I copied models from my laptop over to the pc. I feel like I might be missing some additional required files. Downloading files on my laptop and copying them over is the only method I currently have to update the pc (the pc is more powerful than the laptop). I'm not too worried about it working, but it would be nice to have.&lt;/p&gt; &lt;p&gt;Any help would be appreciated. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExplorerOk996"&gt; /u/ExplorerOk996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkk0s5/offline_ollama_gui_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkk0s5/offline_ollama_gui_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nkk0s5/offline_ollama_gui_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-18T20:52:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkqvqx</id>
    <title>Is 1070ti no longer supported?</title>
    <updated>2025-09-19T01:53:04+00:00</updated>
    <author>
      <name>/u/allknowing2012</name>
      <uri>https://old.reddit.com/user/allknowing2012</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Went to use ollama today and it was no longer working. (did a quick search after updating).&lt;br /&gt; From my googling - it appears 1070ti is not CUDA arch 6.1 or better?&lt;br /&gt; From log...&lt;br /&gt; C:\a\ollama\ollama\ml\backend\ggml\ggml\src\ggml-cuda\common.cuh:106: ggml was not compiled with any CUDA arch &amp;lt;= 610&lt;/p&gt; &lt;p&gt;Am I pooched for even doing the simplest queries?&lt;/p&gt; &lt;p&gt;Update: Thanks for the comments - installing the 0.12.0 and it is working again!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/allknowing2012"&gt; /u/allknowing2012 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkqvqx/is_1070ti_no_longer_supported/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkqvqx/is_1070ti_no_longer_supported/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nkqvqx/is_1070ti_no_longer_supported/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T01:53:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk1qkf</id>
    <title>LLM VRAM/RAM Calculator</title>
    <updated>2025-09-18T07:22:22+00:00</updated>
    <author>
      <name>/u/SmilingGen</name>
      <uri>https://old.reddit.com/user/SmilingGen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a simple tool to estimate how much memory is needed to run GGUF models locally, based on your desired maximum context size.&lt;/p&gt; &lt;p&gt;You just paste the direct download URL of a GGUF model (for example, from Hugging Face), enter the context length you plan to use, and it will give you an approximate memory requirement.&lt;/p&gt; &lt;p&gt;Itâ€™s especially useful if you're trying to figure out whether a model will fit in your available VRAM or RAM, or when comparing different quantization levels like Q4_K_M vs Q8_0.&lt;/p&gt; &lt;p&gt;The tool is completely free and open-source. You can try it here: &lt;a href="https://www.kolosal.ai/memory-calculator"&gt;https://www.kolosal.ai/memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And check out the code on GitHub: &lt;a href="https://github.com/KolosalAI/model-memory-calculator"&gt;https://github.com/KolosalAI/model-memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd really appreciate any feedback, suggestions, or bug reports if you decide to give it a try.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SmilingGen"&gt; /u/SmilingGen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk1qkf/llm_vramram_calculator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk1qkf/llm_vramram_calculator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nk1qkf/llm_vramram_calculator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-18T07:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlbqmj</id>
    <title>An Ollama user seeking uncensored models that can generate images</title>
    <updated>2025-09-19T18:39:45+00:00</updated>
    <author>
      <name>/u/Ghostone89</name>
      <uri>https://old.reddit.com/user/Ghostone89</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been liking the privacy and freedom of running models locally. I've primarily been doing it for roleplay and creative writing, but I'm looking to take things further&lt;/p&gt; &lt;p&gt;My goal is to find a model that is:&lt;/p&gt; &lt;p&gt;Uncensored: I need something with minimal to no filters for creative, long-form roleplay.&lt;/p&gt; &lt;p&gt;Image-capable: The key is a model that can actually generate and send images within the chat, not just analyze them.&lt;/p&gt; &lt;p&gt;I know that multimodal models like LLaVA exist, but I'm looking for specific recommendations from people who have used these models for this particular purpose. Which model do you recommend for combining uncensored roleplay with in-chat image generation? Are there any specific workflows or UIs that make this seamless?&lt;/p&gt; &lt;p&gt;Currently I know some sites are able to do this but I want to know if there are open-source ones too&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ghostone89"&gt; /u/Ghostone89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlbqmj/an_ollama_user_seeking_uncensored_models_that_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlbqmj/an_ollama_user_seeking_uncensored_models_that_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlbqmj/an_ollama_user_seeking_uncensored_models_that_can/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T18:39:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nl3wnx</id>
    <title>ADAM - First Agile Digital Assistant for Managers</title>
    <updated>2025-09-19T13:41:21+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ADAM is a personal project based on ollama LLM that really tackles Agile Project management issues. Ask ADAM about Agile and Traditional project management practices. &lt;/p&gt; &lt;p&gt;For sneak peak, visit the site. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://adam-showcase.vercel.app/#videos"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl3wnx/adam_first_agile_digital_assistant_for_managers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nl3wnx/adam_first_agile_digital_assistant_for_managers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T13:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkpg8z</id>
    <title>Integrating LLM with enterprise DB</title>
    <updated>2025-09-19T00:45:22+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw lots of ads about how you could integrate LLM to pull data from your own database. For example, an AI can get data from your CRM db etc. I want to do similar but not sure where to start. &lt;/p&gt; &lt;p&gt;any suggestion or sample project as reference are most welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkpg8z/integrating_llm_with_enterprise_db/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nkpg8z/integrating_llm_with_enterprise_db/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nkpg8z/integrating_llm_with_enterprise_db/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T00:45:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nl7ro3</id>
    <title>Arch Dolphin 3 stuck</title>
    <updated>2025-09-19T16:09:44+00:00</updated>
    <author>
      <name>/u/Representative-Gur71</name>
      <uri>https://old.reddit.com/user/Representative-Gur71</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nl7ro3/arch_dolphin_3_stuck/"&gt; &lt;img alt="Arch Dolphin 3 stuck" src="https://preview.redd.it/vtfexfo3b5qf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18109aa7946325075ac9bb7ee61371c6ec6e55e5" title="Arch Dolphin 3 stuck" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to install ollama and dolphin but my console gets stuck here, doesnt move.&lt;/p&gt; &lt;p&gt;Any solutions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Representative-Gur71"&gt; /u/Representative-Gur71 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vtfexfo3b5qf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl7ro3/arch_dolphin_3_stuck/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nl7ro3/arch_dolphin_3_stuck/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T16:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlccd8</id>
    <title>Which Linux for my app</title>
    <updated>2025-09-19T19:02:37+00:00</updated>
    <author>
      <name>/u/trucmuch83</name>
      <uri>https://old.reddit.com/user/trucmuch83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, Iâ€™ve been experimenting with app development on a Raspberry Pi 5 ğŸ˜…, but now Iâ€™m looking to upgrade to a new computer so I can run larger models. Iâ€™m planning to get a decent GPU and set up my LLM on Linux â€” any recommendations for which distro works best? Thanks a lot for the help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trucmuch83"&gt; /u/trucmuch83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlccd8/which_linux_for_my_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlccd8/which_linux_for_my_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlccd8/which_linux_for_my_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T19:02:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlw6yt</id>
    <title>How A.I might work the things we don't know yet about ai</title>
    <updated>2025-09-20T11:52:51+00:00</updated>
    <author>
      <name>/u/Adventurous-Lunch332</name>
      <uri>https://old.reddit.com/user/Adventurous-Lunch332</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI Parameters Theory â€“ A Detailed Report&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Introduction&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The power of modern AI systems does not come from storing explicit data or memorizing facts. Instead, it emerges from parametersâ€”the numerical weights and biases that define how information flows through a neural network. These parameters shape the networkâ€™s high-dimensional geometry, enabling it to learn statistical patterns, build representations, and generate reasoning abilities far beyond the scale of individual examples.&lt;/p&gt; &lt;p&gt;This theory explains what parameters really are, how they function, and why emergent intelligence arises when they are scaled up.&lt;/p&gt; &lt;hr /&gt; &lt;ol&gt; &lt;li&gt;Parameters as Functions, Not Memory&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;A common misconception is that parameters â€œstoreâ€ knowledge the way a hard drive stores files. This is false. Parameters are functions, not databases.&lt;/p&gt; &lt;p&gt;Each parameter is a coefficient in a mathematical function.&lt;/p&gt; &lt;p&gt;When multiplied and composed across millions or billions of layers, they form a geometry of transformations.&lt;/p&gt; &lt;p&gt;Input data is not retrieved; it is transformed step by step into outputs.&lt;/p&gt; &lt;p&gt;Example: in a 2-layer model, the forward pass looks like:&lt;/p&gt; &lt;p&gt;\hat{y} = \text{softmax}(W_2 f(W_1 x + b_1) + b_2)&lt;/p&gt; &lt;hr /&gt; &lt;ol&gt; &lt;li&gt;Geometry of Parameters&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Parameters exist in high-dimensional space. Training sculpts this space so that:&lt;/p&gt; &lt;p&gt;Similar inputs are projected to nearby points.&lt;/p&gt; &lt;p&gt;Important features are magnified, irrelevant ones are suppressed.&lt;/p&gt; &lt;p&gt;Non-linear layers bend and fold space, making complex relationships linearly separable.&lt;/p&gt; &lt;p&gt;Think of parameters as knobs on a multidimensional machine. Each knob slightly reshapes the landscape until the system aligns with the statistical structure of the world.&lt;/p&gt; &lt;hr /&gt; &lt;ol&gt; &lt;li&gt;Training: Shaping the Landscape&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;During training, parameters start as random numbers. Backpropagation adjusts them using gradients:&lt;/p&gt; &lt;p&gt;Loss function: Measures how wrong the output is.&lt;/p&gt; &lt;p&gt;Gradient: Tells how to adjust each parameter to reduce error.&lt;/p&gt; &lt;p&gt;Update rule: Moves parameters in small steps toward better alignment.&lt;/p&gt; &lt;p&gt;This process is not about saving examples but about reshaping the parameter landscape so that general rules emerge.&lt;/p&gt; &lt;hr /&gt; &lt;ol&gt; &lt;li&gt;Emergent Abilities from Composition&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The magic comes from scale and composition:&lt;/p&gt; &lt;p&gt;Small networks can classify digits or words, but scaling parameters (millions â†’ billions) allows new abilities.&lt;/p&gt; &lt;p&gt;These abilities are not directly programmedâ€”they emerge naturally from the interaction of many layers and nonlinearities.&lt;/p&gt; &lt;p&gt;Example: reasoning, translation, summarization, even abstract problem-solving.&lt;/p&gt; &lt;p&gt;This is why parameter count mattersâ€”not because more storage = more facts, but because more functional capacity = richer geometry = more emergent skills.&lt;/p&gt; &lt;hr /&gt; &lt;ol&gt; &lt;li&gt;Inference: Function Application&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;At inference time, no learning occurs. Parameters are frozen. The process is simply applying the learned function to new inputs.&lt;/p&gt; &lt;p&gt;Input â†’ Transformed through parameterized layers â†’ Output.&lt;/p&gt; &lt;p&gt;The system does not look up an answerâ€”it computes one.&lt;/p&gt; &lt;p&gt;This makes neural networks fundamentally different from databases or rule-based systems.&lt;/p&gt; &lt;hr /&gt; &lt;ol&gt; &lt;li&gt;Scaling Laws and Limits&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Scaling parameters increases emergent ability, but with trade-offs:&lt;/p&gt; &lt;p&gt;More parameters = higher capacity for complex reasoning.&lt;/p&gt; &lt;p&gt;Training costs (compute, energy, time) grow superlinearly.&lt;/p&gt; &lt;p&gt;Beyond a point, diminishing returns appear unless paired with better data and architectures.&lt;/p&gt; &lt;p&gt;This suggests parameters are necessary but not sufficientâ€”intelligence is a balance of scale, data quality, and structure.&lt;/p&gt; &lt;hr /&gt; &lt;ol&gt; &lt;li&gt;Parameters as Knowledge Geometry&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The final insight: parameters encode knowledge not as symbols, but as geometry.&lt;/p&gt; &lt;p&gt;Each parameter nudges the networkâ€™s internal map of the world.&lt;/p&gt; &lt;p&gt;Together, billions form a statistical mirror of reality.&lt;/p&gt; &lt;p&gt;Reasoning emerges not from stored facts but from navigating this geometry.&lt;/p&gt; &lt;p&gt;In short: Parameters are not memoryâ€”they are the fabric of learned intelligence.&lt;/p&gt; &lt;hr /&gt; &lt;ol&gt; &lt;li&gt;Conclusion&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The AI Parameters Theory reframes how we view large neural networks:&lt;/p&gt; &lt;p&gt;They are not storage devices.&lt;/p&gt; &lt;p&gt;They are mathematical landscapes sculpted by data.&lt;/p&gt; &lt;p&gt;Intelligence arises when the parameter space grows large and structured enough to represent complex patterns of the world.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Lunch332"&gt; /u/Adventurous-Lunch332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlw6yt/how_ai_might_work_the_things_we_dont_know_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlw6yt/how_ai_might_work_the_things_we_dont_know_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlw6yt/how_ai_might_work_the_things_we_dont_know_yet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-20T11:52:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlpvw4</id>
    <title>Any small model 4b - 8b that is both vision and tool calling?</title>
    <updated>2025-09-20T05:31:50+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for small model that support both tool calling and vision.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlpvw4/any_small_model_4b_8b_that_is_both_vision_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlpvw4/any_small_model_4b_8b_that_is_both_vision_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlpvw4/any_small_model_4b_8b_that_is_both_vision_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-20T05:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlpwkg</id>
    <title>Goto mcps</title>
    <updated>2025-09-20T05:32:55+00:00</updated>
    <author>
      <name>/u/Humbrol2</name>
      <uri>https://old.reddit.com/user/Humbrol2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are everyone's goto mcps free or paid&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humbrol2"&gt; /u/Humbrol2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlpwkg/goto_mcps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlpwkg/goto_mcps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlpwkg/goto_mcps/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-20T05:32:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlemes</id>
    <title>Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp; Streamlit</title>
    <updated>2025-09-19T20:31:33+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nlemes/build_a_local_ai_agent_with_mcp_tools_using/"&gt; &lt;img alt="Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp;amp; Streamlit" src="https://external-preview.redd.it/rq8k6bkBVDqS3EaB-6PmZwrrp9mjAeoX2Tt37ubIdpg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76f91a095d5d6782424d54183f94d9fb060dd411" title="Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp;amp; Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/Baa-z7cum1g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlemes/build_a_local_ai_agent_with_mcp_tools_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlemes/build_a_local_ai_agent_with_mcp_tools_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T20:31:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nl8a7w</id>
    <title>[Project] I created an AI photo organizer that uses Ollama to sort photos, filter duplicates, and write Instagram captions.</title>
    <updated>2025-09-19T16:29:01+00:00</updated>
    <author>
      <name>/u/summitsc</name>
      <uri>https://old.reddit.com/user/summitsc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone at &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I wanted to share a Python project I've been working on called the &lt;strong&gt;AI Instagram Organizer&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; I had thousands of photos from a recent trip, and the thought of manually sorting them, finding the best ones, and thinking of captions was overwhelming. I wanted a way to automate this using local LLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution:&lt;/strong&gt; I built a script that uses a multimodal model via Ollama (like LLaVA, Gemma, or Llama 3.2 Vision) to do all the heavy lifting.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Chronological Sorting:&lt;/strong&gt; It reads EXIF data to organize posts by the date they were taken.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Duplicate Filtering:&lt;/strong&gt; It uses multiple perceptual hashes and a dynamic threshold to remove repetitive shots.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Caption &amp;amp; Hashtag Generation:&lt;/strong&gt; For each post folder it creates, it writes several descriptive caption options and a list of hashtags.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Handles HEIC Files:&lt;/strong&gt; It automatically converts Apple's HEIC format to JPG.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Itâ€™s been a really fun project and a great way to explore what's possible with local vision models. I'd love to get your feedback and see if it's useful to anyone else!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/summitsingh/ai-instagram-organizer"&gt;https://github.com/summitsingh/ai-instagram-organizer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since this is my first time building an open-source AI project, any feedback is welcome. And if you like it, a star on GitHub would really make my day! â­&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/summitsc"&gt; /u/summitsc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl8a7w/project_i_created_an_ai_photo_organizer_that_uses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl8a7w/project_i_created_an_ai_photo_organizer_that_uses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nl8a7w/project_i_created_an_ai_photo_organizer_that_uses/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T16:29:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nl0d45</id>
    <title>Ollama or LM Studio?</title>
    <updated>2025-09-19T10:55:22+00:00</updated>
    <author>
      <name>/u/Artaherzadeh</name>
      <uri>https://old.reddit.com/user/Artaherzadeh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to install and run it on my PC, which has a 12600k CPU, 6700XT AMD GPU 12G, and 32GB RAM. Which one is better in terms of features, UI, performance and etc?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Artaherzadeh"&gt; /u/Artaherzadeh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl0d45/ollama_or_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nl0d45/ollama_or_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nl0d45/ollama_or_lm_studio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T10:55:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlinpi</id>
    <title>Wanna try an uncensored model locally, I donâ€™t have a high-end GPU (32GB RAM). What should I try first?</title>
    <updated>2025-09-19T23:22:31+00:00</updated>
    <author>
      <name>/u/toolhouseai</name>
      <uri>https://old.reddit.com/user/toolhouseai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey people, so I wanted mess around with an uncensored model on Ollama. The problem is, I donâ€™t have a high-end GPU just 32GB RAM and a not too bad CPU.&lt;/p&gt; &lt;p&gt;What would be a good first model to try? You got any tips/resources to share when running models locally?&lt;/p&gt; &lt;p&gt;Appreciate yall happy friday&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toolhouseai"&gt; /u/toolhouseai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlinpi/wanna_try_an_uncensored_model_locally_i_dont_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlinpi/wanna_try_an_uncensored_model_locally_i_dont_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlinpi/wanna_try_an_uncensored_model_locally_i_dont_have/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T23:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlo7pk</id>
    <title>MCP-servers http connection</title>
    <updated>2025-09-20T03:58:21+00:00</updated>
    <author>
      <name>/u/Few-Advance4363</name>
      <uri>https://old.reddit.com/user/Few-Advance4363</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm playing with MCP servers and local llm. I want to use a client other than terminal. &lt;/p&gt; &lt;p&gt;Currently, I can connect my client over network to ip:11434 and the model works.&lt;/p&gt; &lt;p&gt;I can also get my llm to connected to mcp-server using ollmcp. However, I have to be directly on the terminal of the llm machine.&lt;/p&gt; &lt;p&gt;How can I get ollama to connect to the mcp server AND then allow a client to have access to those MCP tools?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Advance4363"&gt; /u/Few-Advance4363 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlo7pk/mcpservers_http_connection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlo7pk/mcpservers_http_connection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlo7pk/mcpservers_http_connection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-20T03:58:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlbxhi</id>
    <title>Simple RAG design architecture</title>
    <updated>2025-09-19T18:47:08+00:00</updated>
    <author>
      <name>/u/Tough_Wrangler_6075</name>
      <uri>https://old.reddit.com/user/Tough_Wrangler_6075</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nlbxhi/simple_rag_design_architecture/"&gt; &lt;img alt="Simple RAG design architecture" src="https://preview.redd.it/80ry05gb36qf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fdf0e5eedcef7768325005fe953976cc41d1977" title="Simple RAG design architecture" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am trying to make a design architecture for my RAG system. If you guys have any suggestions or feedback. Please, I would be happy to hear that &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tough_Wrangler_6075"&gt; /u/Tough_Wrangler_6075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/80ry05gb36qf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlbxhi/simple_rag_design_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlbxhi/simple_rag_design_architecture/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-19T18:47:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm4wsw</id>
    <title>Anyone here ever had a job relocation as AI Engineer?</title>
    <updated>2025-09-20T17:57:09+00:00</updated>
    <author>
      <name>/u/Tough_Wrangler_6075</name>
      <uri>https://old.reddit.com/user/Tough_Wrangler_6075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, As my country is not advancing AI as much as what happening right now. I aspiring to look a relocation job as AI Engineer so I can advancing my skills in AI engineering with real world problem. Anyone here ever got a job as AI engineering. If so, please share your journey and struggle during the process. Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tough_Wrangler_6075"&gt; /u/Tough_Wrangler_6075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nm4wsw/anyone_here_ever_had_a_job_relocation_as_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nm4wsw/anyone_here_ever_had_a_job_relocation_as_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nm4wsw/anyone_here_ever_had_a_job_relocation_as_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-20T17:57:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlmgv7</id>
    <title>Fully local data analysis assistant for laptop</title>
    <updated>2025-09-20T02:26:02+00:00</updated>
    <author>
      <name>/u/mshintaro777</name>
      <uri>https://old.reddit.com/user/mshintaro777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nlmgv7/fully_local_data_analysis_assistant_for_laptop/"&gt; &lt;img alt="Fully local data analysis assistant for laptop" src="https://preview.redd.it/gr4pt119i5qf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=26b896f4905ff5a8f0bddf8feea416783b7498aa" title="Fully local data analysis assistant for laptop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mshintaro777"&gt; /u/mshintaro777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gr4pt119i5qf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nlmgv7/fully_local_data_analysis_assistant_for_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nlmgv7/fully_local_data_analysis_assistant_for_laptop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-20T02:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmemym</id>
    <title>Cpu, Gpu or Npu for running ai models?</title>
    <updated>2025-09-21T00:57:20+00:00</updated>
    <author>
      <name>/u/sneezeme05</name>
      <uri>https://old.reddit.com/user/sneezeme05</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which one should i use for ollama I am rocking a Ryzen 7 8845hs 16gb ddr5 5600mhz Rtx 4050&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sneezeme05"&gt; /u/sneezeme05 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmemym/cpu_gpu_or_npu_for_running_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmemym/cpu_gpu_or_npu_for_running_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nmemym/cpu_gpu_or_npu_for_running_ai_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-21T00:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn5ur4</id>
    <title>æ¨¡å‹èˆ‡é¡¯ç¤ºå¡ å…¬å¼ åˆ¤æ–· èˆ‡ç”œèœœå€¼</title>
    <updated>2025-09-21T22:37:55+00:00</updated>
    <author>
      <name>/u/kuerys</name>
      <uri>https://old.reddit.com/user/kuerys</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nn5ur4/æ¨¡å‹èˆ‡é¡¯ç¤ºå¡_å…¬å¼_åˆ¤æ–·_èˆ‡ç”œèœœå€¼/"&gt; &lt;img alt="æ¨¡å‹èˆ‡é¡¯ç¤ºå¡ å…¬å¼ åˆ¤æ–· èˆ‡ç”œèœœå€¼" src="https://b.thumbs.redditmedia.com/EXdFBgOu1jvD-3bKlVJ5h5fI220lx4DUTILy8SpwpSk.jpg" title="æ¨¡å‹èˆ‡é¡¯ç¤ºå¡ å…¬å¼ åˆ¤æ–· èˆ‡ç”œèœœå€¼" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;å–®å¡&lt;/p&gt; &lt;p&gt;Bå€¼ Ã— 1.3 = VRAMï¼ˆå¯åŸ·è¡Œå€¼ï¼‰â‰’ (Tâ‰’4096) = ç”œå¯†å€¼&lt;/p&gt; &lt;h1&gt;å–®å¡å…¬å¼ï¼š&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;B Ã— 1.3 â‰’ VRAMï¼ˆå¯åŸ·è¡Œå€¼ï¼‰ â‰’ T â‰’ 4096 = ç”œå¯†å€¼&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;B&lt;/strong&gt;ï¼šæ¨¡å‹åƒæ•¸é‡ï¼ˆBillionï¼‰&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1.3&lt;/strong&gt;ï¼šæ¨ç†æ™‚çš„è¿‘ä¼¼å€å¢ä¿‚æ•¸ï¼ˆä»£è¡¨æ¨¡å‹è¼‰å…¥ + buffer + contextï¼‰&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM&lt;/strong&gt;ï¼šé¡¯ç¤ºè¨˜æ†¶é«”éœ€æ±‚ï¼ˆGBï¼‰&lt;/li&gt; &lt;li&gt;&lt;strong&gt;T&lt;/strong&gt;ï¼šä¸Šä¸‹æ–‡é•·åº¦ï¼ˆtokensï¼‰ï¼Œ4096 ç‚ºå¸¸è¦‹ sweet spot&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ç”œå¯†å€¼&lt;/strong&gt;ï¼šä»£è¡¨ã€Œæ•ˆèƒ½èˆ‡ç©©å®šæ€§å…¼å…·ã€çš„ç†æƒ³æ¨ç†é…ç½®&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;å¤šå¡&lt;/p&gt; &lt;p&gt;Bå€¼ Ã— (VRAMx4096)â‰’ (Tâ‰’1.3) = ç”œå¯†å€¼&lt;/p&gt; &lt;p&gt;å¤šå¡å…¬å¼ï¼š&lt;/p&gt; &lt;p&gt;&lt;strong&gt;B Ã— (VRAM Ã— 4096) â‰’ T â‰’ 1.3 = ç”œå¯†å€¼&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;é€™è£¡ä½ æŠŠ VRAM å’Œä¸Šä¸‹æ–‡é•·åº¦è¦–ç‚ºä¹˜ç©ï¼Œä»£è¡¨å¤šå¡åˆ†æµæ™‚çš„ &lt;strong&gt;ç¸½æ¨ç†å®¹é‡&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;T â‰’ 1.3&lt;/strong&gt;ï¼šé€™è£¡åéä¾†è¡¨ç¤ºã€Œæ¯ GB VRAM å°æ‡‰çš„ token è™•ç†èƒ½åŠ›ã€ç´„ç‚º 1.3B æ¨¡å‹çš„è² è¼‰&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;çˆ†æ©Ÿ&lt;/p&gt; &lt;p&gt;B Ã— (VRAM Ã— 4096) â‰’ T â‰’ 1.3 = G Ã— ç”œå¯†å€¼&lt;br /&gt; çˆ†æ©Ÿå…¬å¼ï¼š&lt;/p&gt; &lt;p&gt;&lt;strong&gt;B Ã— (VRAM Ã— 4096) â‰’ T â‰’ 1.3 = G Ã— ç”œå¯†å€¼&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;G&lt;/strong&gt;ï¼šGPU æ•¸é‡ï¼ˆæˆ– GPU ç­‰ç´šä¿‚æ•¸ï¼‰&lt;/li&gt; &lt;li&gt;é€™æ˜¯ä½ å°ã€Œçˆ†æ©Ÿã€çš„å®šç¾©ï¼šç•¶æ¨¡å‹åƒæ•¸é‡ Ã— æ¨ç†è² è¼‰è¶…é GPU çš„è™•ç†èƒ½åŠ›æ™‚ï¼Œå°±æœƒçˆ†æ©Ÿ&lt;/li&gt; &lt;li&gt;é€™å…¬å¼ä¹Ÿå¯ä»¥åæ¨ï¼š&lt;strong&gt;G Ã— ç”œå¯†å€¼&lt;/strong&gt; æ˜¯ä½ èƒ½æ‰¿å—çš„æœ€å¤§æ¨ç†è² &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k6t9usnzllqf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08a077afed8b3e0ba1b1ce914780d27f08cd5fcd"&gt;https://preview.redd.it/k6t9usnzllqf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08a077afed8b3e0ba1b1ce914780d27f08cd5fcd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;é›™æ©Ÿ å–®å¡ é›™å¡ æ¸¬è©¦ç’°å¢ƒ é©—è­‰ èˆ‡çˆ†æ©Ÿæ¸¬è©¦&lt;/p&gt; &lt;p&gt;ç­†é›»:&lt;/p&gt; &lt;p&gt;Thin GF63 12UCX-289TW&lt;/p&gt; &lt;p&gt;CPU:12th Gen Intel(R) Core(TM) i7-12650H (2.30 GHz)&lt;/p&gt; &lt;p&gt;GPU:Intel(R) UHD Graphics/NVIDIA GeForce RTX 3050 Laptop GPU (4G)&lt;/p&gt; &lt;p&gt;RAM:16G X2 (32G)&lt;/p&gt; &lt;p&gt;SSD:Phison 512G ESO512GHLCA1-21C-2MS(C:)/ADATA SU800(D:)&lt;br /&gt; (å‰å°æ©Ÿ)&lt;/p&gt; &lt;p&gt;Xeon&lt;/p&gt; &lt;p&gt;CPU:Intel(R) Xeon(R) CPU E5-2696 v4 @ 2.20GHz 2.20 GHz (2 å€‹è™•ç†å™¨)&lt;/p&gt; &lt;p&gt;RAM:16G X8 (128G)&lt;/p&gt; &lt;p&gt;SSD:ä¸‰æ˜Ÿ 512G M.2 NVMe (C:\) åéŠ“ VULCAN Z 1TB SSD/2.5å‹ è®€å¯«550/500 (D:\)&lt;/p&gt; &lt;p&gt;GPU:RTX 3060 GD6 12G ç³»çµ±é¡¯ PCI-E 1 (X16)&lt;/p&gt; &lt;p&gt;GPU:RTX 2080TI é­”æ”¹22GB *2 NVLink PCI-E 3/5(X16) (NvLink)&lt;/p&gt; &lt;p&gt;----------------------------------------------&lt;br /&gt; C:\Users\Admin&amp;gt;ollama run gpt-oss:120b&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; æ—©å®‰ä½ å¥½&lt;/p&gt; &lt;p&gt;Thinking...&lt;/p&gt; &lt;p&gt;The user says in Chinese: &amp;quot;æ—©å®‰ä½ å¥½&amp;quot; which means &amp;quot;Good morning, hello&amp;quot;. Probably they want a greeting. We can&lt;/p&gt; &lt;p&gt;respond in Chinese, ask how can help. Maybe ask if they need anything. So reply: &amp;quot;æ—©å®‰ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ&amp;quot;&lt;/p&gt; &lt;p&gt;Also can be friendly.&lt;/p&gt; &lt;p&gt;...done thinking.&lt;/p&gt; &lt;p&gt;æ—©å®‰ï¼ğŸ˜Š&lt;/p&gt; &lt;p&gt;æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿå¦‚æœæœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦èŠå¤©ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼ç¥æ‚¨ä»Šå¤©æœ‰ä¸ªç¾å¥½çš„ä¸€å¤©ã€‚&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; é€™æ˜¯OLLAMA ä¸‹çš„æ¨¡å‹æ¨æ›ç®—å…¬å¼ å–®å¡&lt;/p&gt; &lt;p&gt;... Bå€¼ 1.3 = VRAMï¼ˆå¯åŸ·è¡Œå€¼ï¼‰ (T4096) = ç”œå¯†å€¼&lt;/p&gt; &lt;p&gt;...&lt;/p&gt; &lt;p&gt;... å¤šå¡&lt;/p&gt; &lt;p&gt;... Bå€¼ (VRAMx4096) (T1.3) = ç”œå¯†å€¼&lt;/p&gt; &lt;p&gt;...&lt;/p&gt; &lt;p&gt;... çˆ†æ©Ÿ&lt;/p&gt; &lt;p&gt;... B (VRAM 4096) T 1.3 = G ç”œå¯†å€¼&lt;/p&gt; &lt;p&gt;Thinking...&lt;/p&gt; &lt;p&gt;The user wrote in Chinese: &amp;quot;é€™æ˜¯OLLAMA ä¸‹çš„æ¨¡å‹æ¨æ›ç®—å…¬å¼ å–®å¡ Bå€¼ 1.3 = VRAMï¼ˆå¯åŸ·è¡Œå€¼ï¼‰ (T4096) = ç”œå¯†å€¼ å¤šå¡ Bå€¼&lt;/p&gt; &lt;p&gt;(VRAMx4096) (T1.3) = ç”œå¯†å€¼ çˆ†æ©Ÿ B (VRAM 4096) T 1.3 = G ç”œå¯†å€¼&amp;quot;&lt;/p&gt; &lt;p&gt;They seem to be describing a formula for OLLAMA model inference conversion? The text is confusing. They probably&lt;/p&gt; &lt;p&gt;want explanation or help to interpret the formula. Need to ask clarifying question or provide explanation.&lt;/p&gt; &lt;p&gt;Probably they want to understand the formula, maybe compute values? Provide guidance on how to calculate &amp;quot;ç”œå¯†å€¼&amp;quot;?&lt;/p&gt; &lt;p&gt;Could be translation: &amp;quot;sweetness value&amp;quot;? Actually &amp;quot;ç”œå¯†å€¼&amp;quot; maybe &amp;quot;sweetness density&amp;quot;? Could be a typo for &amp;quot;ç”œå¯†å€¼&lt;/p&gt; &lt;p&gt;&amp;quot;? Not sure.&lt;/p&gt; &lt;p&gt;We should respond in Chinese, ask for clarification, and try to interpret: Single GPU: B = 1.3 = VRAM (executable&lt;/p&gt; &lt;p&gt;value) (T4096) = sweetness value. Multi-GPU: B = (VRAM x 4096) (T1.3) = sweetness value. Overclock? &amp;quot;çˆ†æ©Ÿ&amp;quot; maybe&lt;/p&gt; &lt;p&gt;overclocked: B (VRAM 4096) T 1.3 = G sweetness value.&lt;/p&gt; &lt;p&gt;We can explain that B is some scaling factor, VRAM is memory, T is token length? Provide formula: sweetness = B *&lt;/p&gt; &lt;p&gt;VRAM? Might need to derive.&lt;/p&gt; &lt;p&gt;Let's respond: ask what they want: calculate for specific GPU? Provide example: for 24GB VRAM single card, B =&lt;/p&gt; &lt;p&gt;1.3, T = 4096 =&amp;gt; sweetness = 1.3 * 24? Or something.&lt;/p&gt; &lt;p&gt;We can propose a clearer formula: Sweetness = B * VRAM * T? Or Sweetness = VRAM^(B) * T? Not clear.&lt;/p&gt; &lt;p&gt;We can ask: &amp;quot;æ‚¨æƒ³è¦æŠŠé€™å€‹å…¬å¼å¥—ç”¨åœ¨ä»€éº¼æƒ…æ³ï¼Ÿæƒ³ç®—å‡ºæŸå¼µå¡çš„ç”œå¯†å€¼å—ï¼Ÿè«‹æä¾› VRAM å®¹é‡èˆ‡å¡æ•¸ï¼Œæˆ‘å¯ä»¥å¹«æ‚¨ç®—ã€‚&amp;quot;&lt;/p&gt; &lt;p&gt;Provide explanation.&lt;/p&gt; &lt;p&gt;Will answer in Chinese, friendly, ask for more details.&lt;/p&gt; &lt;p&gt;...done thinking.&lt;/p&gt; &lt;p&gt;ä½ å¥½ï¼çœ‹èµ·ä¾†ä½ åœ¨æ•´ç† OLLAMAï¼ˆæˆ–é¡ä¼¼çš„æœ¬åœ° LLM æ¨ç†ï¼‰æ™‚ï¼Œé—œæ–¼ **ã€Œç”œå¯†å€¼ã€**ï¼ˆæˆ‘æš«æ™‚æŠŠå®ƒç•¶ä½œä¸€å€‹è¡¡é‡æ¨¡å‹å¯åŸ·è¡Œæ•ˆç‡æˆ–&lt;/p&gt; &lt;p&gt;æ•ˆèƒ½çš„æŒ‡æ¨™ï¼‰çš„è¨ˆç®—å…¬å¼ã€‚&lt;/p&gt; &lt;p&gt;ä¸‹é¢å…ˆæŠŠä½ æä¾›çš„ä¸‰ç¨®æƒ…æ³åšå€‹æ•´ç†ï¼Œç„¶å¾Œå†èªªæ˜æ¯å€‹åƒæ•¸å¯èƒ½ä»£è¡¨çš„æ„ç¾©ï¼Œæœ€å¾Œçµ¦å‡ºä¸€å€‹ã€Œå¥—ç”¨ã€çš„ç¯„ä¾‹ï¼Œè®“ä½ å¯ä»¥ç›´æ¥æŠŠè‡ªå·±çš„&lt;/p&gt; &lt;p&gt;é¡¯å¡è¦æ ¼å¸¶é€²å»ç®—ã€‚&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 1ï¸âƒ£ å…¬å¼ç¸½è¦½&lt;/p&gt; &lt;p&gt;| æƒ…æ³ | å…¬å¼ï¼ˆåŸæ–‡ï¼‰ | å¯èƒ½çš„æ•¸å­¸å¯«æ³• | åƒæ•¸èªªæ˜ |&lt;/p&gt; &lt;p&gt;|------|--------------|----------------|----------|&lt;/p&gt; &lt;p&gt;| **å–®å¡** | `Bå€¼ 1.3 = VRAMï¼ˆå¯åŸ·è¡Œå€¼ï¼‰ (T4096) = ç”œå¯†å€¼` | `ç”œå¯†å€¼ = 1.3 Ã— VRAM Ã— 4096` | - **B**ï¼šå›ºå®šä¿‚æ•¸&lt;/p&gt; &lt;p&gt;1.3&amp;lt;br&amp;gt;- **VRAM**ï¼šå¡ä¸Šå¯ç”¨çš„é¡¯å­˜ï¼ˆGBï¼‰&amp;lt;br&amp;gt;- **T**ï¼šæ¨¡å‹ä¸Šä¸‹æ–‡é•·åº¦ï¼ˆtoken æ•¸ï¼‰ï¼Œé€™è£¡å›ºå®š 4096 |&lt;/p&gt; &lt;p&gt;| **å¤šå¡** | `Bå€¼ (VRAMx4096) (T1.3) = ç”œå¯†å€¼` | `ç”œå¯†å€¼ = (VRAM Ã— 4096) Ã— 1.3`ï¼ˆæˆ–ç­‰åƒ¹çš„ `ç”œå¯†å€¼ = VRAM Ã— 4096&lt;/p&gt; &lt;p&gt;Ã— 1.3`ï¼‰ | - **VRAM**ï¼šå–®å¼µå¡çš„é¡¯å­˜ï¼ˆGBï¼‰&amp;lt;br&amp;gt;- **4096**ï¼šä»ç„¶æ˜¯ token é•·åº¦&amp;lt;br&amp;gt;- **1.3**ï¼šæ”¹æˆäº† **T**ï¼ˆä¹Ÿå°±æ˜¯ä¿‚æ•¸ï¼‰&lt;/p&gt; &lt;p&gt;|&lt;/p&gt; &lt;p&gt;| **çˆ†æ©Ÿ**ï¼ˆæ¥µé™/è¶…é »ï¼‰| `B (VRAM 4096) T 1.3 = G ç”œå¯†å€¼` | `ç”œå¯†å€¼ = VRAM Ã— 4096 Ã— 1.3`ï¼ˆå†ä¹˜ä¸Šä¸€å€‹ **G**ï¼Œä»£è¡¨&lt;/p&gt; &lt;p&gt;â€œè¶…é¡â€ æˆ– â€œå¢ç›Šâ€ï¼‰| - **G**ï¼šé€šå¸¸æ˜¯é¡å¤–çš„å¢ç›Šå› å­ï¼ˆä¾‹å¦‚ 1.2ã€1.5ï¼‰ï¼Œç”¨ä¾†è¡¨ç¤ºè¶…é »ã€PCIeâ€‘Gen4 å¸¶å¯¬æå‡ç­‰ |&lt;/p&gt; &lt;p&gt;&amp;gt; **è¨»**ï¼šä¸Šè¿°çš„ã€ŒÃ—ã€ç¬¦è™Ÿæ˜¯æ ¹æ“šæ–‡å­—çš„æ’åˆ—æ¨æ¸¬çš„ä¹˜æ³•é—œä¿‚ã€‚å¦‚æœå¯¦éš›ä¸Šæ˜¯é™¤æ³•æˆ–æŒ‡æ•¸é‹ç®—ï¼Œè«‹å†å‘Šè¨´æˆ‘ï¼Œæˆ‘æœƒç«‹å³èª¿æ•´ã€‚&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 2ï¸âƒ£ åƒæ•¸èƒŒå¾Œçš„æ„ç¾©ï¼ˆæˆ‘çš„ç†è§£ï¼‰&lt;/p&gt; &lt;p&gt;| åƒæ•¸ | å¸¸è¦‹å«ç¾© | ç‚ºä»€éº¼æœƒå‡ºç¾åœ¨å…¬å¼è£¡ |&lt;/p&gt; &lt;p&gt;|------|----------|----------------------|&lt;/p&gt; &lt;p&gt;| **Bï¼ˆæˆ– Tï¼‰** | **ä¿‚æ•¸**ï¼Œç”¨ä¾†æŠŠã€Œé¡¯å­˜ã€çš„å®¹é‡è½‰æ›æˆã€Œå¯åŸ·è¡Œå€¼ã€çš„å°ºåº¦ã€‚&amp;lt;br&amp;gt;åœ¨å–®å¡æƒ…æ³ä¸‹å®ƒæ˜¯ **1.3**ï¼Œå¤šå¡æ™‚è®Šæˆ&lt;/p&gt; &lt;p&gt;**1.3** ä½œç‚º **T**ï¼Œç›¸ç•¶æ–¼æŠŠ 1.3 ç•¶ä½œã€Œæ¯ 1GB VRAM å¯æ”¯æ´çš„ token æ•¸ã€çš„å€ç‡ã€‚ |&lt;/p&gt; &lt;p&gt;| **VRAM** | é¡¯å¡å¯ç”¨é¡¯å­˜ï¼ˆGBï¼‰ï¼Œç›´æ¥æ±ºå®šæ¨¡å‹èƒ½è¼‰å…¥çš„å¤§å°èˆ‡ä¸€æ¬¡æ€§æ¨ç†çš„ token æ•¸é‡ã€‚ |&lt;/p&gt; &lt;p&gt;| **4096** | å¸¸è¦‹çš„ **ä¸Šä¸‹æ–‡é•·åº¦**ï¼ˆtoken ä¸Šé™ï¼‰ï¼Œå¾ˆå¤š LLMï¼ˆå¦‚ LLaMAã€Mistralï¼‰é è¨­çš„æœ€å¤§ä¸Šä¸‹æ–‡é•·åº¦å°±æ˜¯ 4096ã€‚ |&lt;/p&gt; &lt;p&gt;| **G** | **å¢ç›Šå› å­**ï¼Œåœ¨ã€Œçˆ†æ©Ÿã€æ¨¡å¼ä¸‹ï¼Œå¯èƒ½æ˜¯è¶…é »ã€PCIeâ€‘Gen4 å¸¶å¯¬æå‡ã€æˆ–æ˜¯ä½¿ç”¨é«˜æ•ˆèƒ½çš„ NVMe äº¤æ›ç©ºé–“ç­‰å¸¶ä¾†çš„é¡å¤–&lt;/p&gt; &lt;p&gt;æ•ˆèƒ½æå‡ã€‚ |&lt;/p&gt; &lt;p&gt;ç°¡å–®èªªï¼Œ**ç”œå¯†å€¼** â‰ˆ `VRAM Ã— 4096 Ã— ä¿‚æ•¸`ï¼Œä¿‚æ•¸æ ¹æ“šå–®å¡/å¤šå¡/è¶…é »è€Œä¸åŒã€‚&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 3ï¸âƒ£ å¯¦éš›è¨ˆç®—ç¯„ä¾‹&lt;/p&gt; &lt;p&gt;å‡è¨­ä½ æœ‰ä»¥ä¸‹å¹¾ç¨®ç¡¬é«”é…ç½®ï¼š&lt;/p&gt; &lt;p&gt;| å¡å‹è™Ÿ | VRAM (GB) | å¡æ•¸ | æ˜¯å¦è¶…é » | ä¿‚æ•¸ï¼ˆB/Tï¼‰ | å¢ç›Š G |&lt;/p&gt; &lt;p&gt;|--------|-----------|------|----------|-------------|--------|&lt;/p&gt; &lt;p&gt;| RTX 4090 | 24 | 1 | å¦ | 1.3 | 1.0 |&lt;/p&gt; &lt;p&gt;| RTX 4090 | 24 | 2 | å¦ | 1.3 | 1.0 |&lt;/p&gt; &lt;p&gt;| RTX 4090 | 24 | 1 | æ˜¯ï¼ˆçˆ†æ©Ÿï¼‰ | 1.3 | 1.2 |&lt;/p&gt; &lt;p&gt;### 3.1 å–®å¡ï¼ˆä¸è¶…é »ï¼‰&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;ç”œå¯†å€¼ = 1.3 Ã— 24 Ã— 4096&lt;/p&gt; &lt;p&gt;= 1.3 Ã— 98,304&lt;/p&gt; &lt;p&gt;= 127,795.2 (ç´„ 1.28e5)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;### 3.2 å¤šå¡ï¼ˆ2 å¼µï¼‰&lt;/p&gt; &lt;p&gt;æ ¹æ“šã€Œå¤šå¡ã€å…¬å¼ï¼Œæˆ‘ä»ç„¶æŠŠ **VRAM** è¦–ç‚º **å–®å¡** çš„å®¹é‡ï¼ˆå› ç‚ºå…¬å¼è£¡çš„ `VRAMx4096` å·²ç¶“éš±å«äº†ã€Œæ¯å¼µå¡ã€çš„ä¹˜ç®—ï¼‰ï¼š&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;ç”œå¯†å€¼ = (24 Ã— 4096) Ã— 1.3&lt;/p&gt; &lt;p&gt;= 98,304 Ã— 1.3&lt;/p&gt; &lt;p&gt;= 127,795.2 ï¼ˆå’Œå–®å¡ç›¸åŒï¼Œé™¤éä½ æŠŠå¡æ•¸ä¹˜é€²å»ï¼‰&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&amp;gt; **æ³¨æ„**ï¼šå¦‚æœä½ å¸Œæœ›æŠŠ *å¡æ•¸* ç›´æ¥ç®—é€²å»ï¼ˆå³ã€Œç¸½ VRAMã€ï¼‰ï¼Œåªè¦æŠŠ `VRAM` æ›æˆ **ç¸½ VRAM** å³å¯ã€‚&lt;/p&gt; &lt;p&gt;&amp;gt; ä¾‹å¦‚ï¼š`ç¸½VRAM = 24 Ã— 2 = 48` â†’ `ç”œå¯†å€¼ = 1.3 Ã— 48 Ã— 4096 = 255,590.4`ã€‚&lt;/p&gt; &lt;p&gt;### 3.3 çˆ†æ©Ÿï¼ˆå–®å¡ã€è¶…é »ï¼‰&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;ç”œå¯†å€¼ = VRAM Ã— 4096 Ã— 1.3 Ã— G&lt;/p&gt; &lt;p&gt;= 24 Ã— 4096 Ã— 1.3 Ã— 1.2&lt;/p&gt; &lt;p&gt;= 98,304 Ã— 1.3 Ã— 1.2&lt;/p&gt; &lt;p&gt;= 127,795.2 Ã— 1.2&lt;/p&gt; &lt;p&gt;= 153,354.24&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 4ï¸âƒ£ å¦‚ä½•è‡ªè¡Œå¥—ç”¨&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;**ç¢ºå®šä¸‰å€‹åŸºç¤è®Šæ•¸**&lt;/p&gt; &lt;p&gt;- `VRAM`ï¼šå–®å¼µå¡çš„é¡¯å­˜ï¼ˆGBï¼‰&lt;/p&gt; &lt;p&gt;- `T`ï¼ˆæˆ– `4096`ï¼‰ï¼šä½ æ‰“ç®—ä½¿ç”¨çš„ä¸Šä¸‹æ–‡é•·åº¦ï¼ˆå¦‚æœä¸æ˜¯ 4096ï¼Œç›´æ¥æ›¿æ›ï¼‰&lt;/p&gt; &lt;p&gt;- `ä¿‚æ•¸ B/T`ï¼šæ ¹æ“šç¡¬é«”ç‹€æ…‹ï¼ˆå–®å¡ 1.3ã€å¤šå¡ 1.3ã€çˆ†æ©Ÿ 1.3ï¼‰ä»¥åŠæ˜¯å¦æœ‰é¡å¤–å¢ç›Š `G`&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**å¥—ç”¨å…¬å¼**&lt;/p&gt; &lt;p&gt;- **å–®å¡**ï¼š`ç”œå¯†å€¼ = B Ã— VRAM Ã— T`&lt;/p&gt; &lt;p&gt;- **å¤šå¡**ï¼ˆè‹¥æƒ³æŠŠå¡æ•¸ç´å…¥ï¼‰ï¼šã€Œç¸½ VRAMã€= `VRAM Ã— å¡æ•¸` â†’ `ç”œå¯†å€¼ = B Ã— ç¸½ VRAM Ã— T`&lt;/p&gt; &lt;p&gt;- **çˆ†æ©Ÿ**ï¼šåœ¨å–®å¡å…¬å¼æœ€å¾Œå†ä¹˜ä¸Š `G`ï¼ˆ&amp;gt;1ï¼‰&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**ä¾‹å­**ï¼ˆä½ å¯ä»¥ç›´æ¥åœ¨ Excel/Google Sheet æˆ– Python ä¸­ç®—ï¼‰&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;```python&lt;/p&gt; &lt;p&gt;def sweet_score(vram_gb, token_len=4096, coeff=1.3, cards=1, gain=1.0):&lt;/p&gt; &lt;p&gt;total_vram = vram_gb * cards&lt;/p&gt; &lt;p&gt;return coeff * total_vram * token_len * gain&lt;/p&gt; &lt;p&gt;# å–®å¡ RTX4090&lt;/p&gt; &lt;p&gt;print(sweet_score(24)) # 127795.2&lt;/p&gt; &lt;p&gt;# å…©å¡ RTX4090&lt;/p&gt; &lt;p&gt;print(sweet_score(24, cards=2)) # 255590.4&lt;/p&gt; &lt;p&gt;# çˆ†æ©Ÿå–®å¡ (gain=1.2)&lt;/p&gt; &lt;p&gt;print(sweet_score(24, gain=1.2)) # 153354.24&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 5ï¸âƒ£ éœ€è¦é€²ä¸€æ­¥çš„è³‡è¨Šï¼Ÿ&lt;/p&gt; &lt;p&gt;- **ã€Œç”œå¯†å€¼ã€çš„å¯¦éš›æ„ç¾©**ï¼šå®ƒæ˜¯ç”¨ä¾†é ä¼°ä¸€æ¬¡æ¨ç†èƒ½å®¹ç´å¤šå°‘ tokenã€æˆ–æ˜¯è¡¡é‡æ¨¡å‹è¼‰å…¥çš„ã€Œå®‰å…¨ä¸Šé™ã€å—ï¼Ÿå¦‚æœæœ‰å…·é«”çš„é˜ˆå€¼&lt;/p&gt; &lt;p&gt;ï¼ˆä¾‹å¦‚ç”œå¯†å€¼ &amp;gt; 200k è¡¨ç¤ºå¯ä»¥è·‘ 70B åƒæ•¸æ¨¡å‹ï¼‰ï¼Œè«‹å‘Šè¨´æˆ‘ï¼Œæˆ‘å¯ä»¥å¹«ä½ æŠŠçµæœæ˜ å°„æˆæ¨¡å‹å°ºå¯¸å»ºè­°ã€‚&lt;/p&gt; &lt;p&gt;- **æ˜¯å¦æœ‰å…¶ä»–è®Šæ•¸**ï¼šåƒæ˜¯ **PCIe å¸¶å¯¬**ã€**CPU è¨˜æ†¶é«”**ã€**ç£ç¢Ÿäº¤æ›ç©ºé–“** ç­‰ï¼Œé€™äº›åœ¨ã€Œçˆ†æ©Ÿã€æƒ…æ³ä¸‹ä¹Ÿæœƒå½±éŸ¿æœ€çµ‚æ•ˆèƒ½&lt;/p&gt; &lt;p&gt;ã€‚&lt;/p&gt; &lt;p&gt;- **ä½ æƒ³è¦çš„è¼¸å‡ºæ ¼å¼**ï¼šä¾‹å¦‚ä¸€å€‹ç°¡æ½”çš„ã€Œå¡ç‰‡ã€å¼å ±å‘Šã€æˆ–æ˜¯ CSV/JSON è¡¨æ ¼ï¼Œæ–¹ä¾¿ä½ ç›´æ¥åŒ¯å…¥åˆ°è‡ªå·±çš„æ¸¬è©¦è…³æœ¬è£¡ã€‚&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;### å°çµ&lt;/p&gt; &lt;p&gt;- å…¬å¼æœ¬è³ªä¸Šæ˜¯ `ç”œå¯†å€¼ â‰ˆ ä¿‚æ•¸ Ã— VRAM Ã— token_len Ã— (å¡æ•¸æˆ–å¢ç›Š)`ã€‚&lt;/p&gt; &lt;p&gt;- åªè¦æŠŠä½ çš„ç¡¬é«”è¦æ ¼ä»£å…¥ï¼Œå°±èƒ½å¿«é€Ÿå¾—åˆ°ä¸€å€‹ç›¸å°ã€Œå¯åŸ·è¡Œã€çš„æŒ‡æ¨™ã€‚&lt;/p&gt; &lt;p&gt;- è‹¥ä½ æœ‰æ›´è©³ç´°çš„éœ€æ±‚ï¼ˆä¾‹å¦‚ä¸åŒæ¨¡å‹çš„åƒæ•¸é‡ã€ä¸åŒ token é•·åº¦ã€æˆ–æ˜¯æƒ³æŠŠ CPU/ç£ç¢Ÿå› ç´ ä¸€èµ·ç´å…¥ï¼‰ï¼Œéš¨æ™‚å‘Šè¨´æˆ‘ï¼Œæˆ‘å¯ä»¥å¹«ä½ &lt;/p&gt; &lt;p&gt;æ“´å±•å…¬å¼æˆ–å¯«ä¸€å€‹å°å·¥å…·ã€‚&lt;/p&gt; &lt;p&gt;å¸Œæœ›ä»¥ä¸Šèªªæ˜å°ä½ æœ‰å¹«åŠ©ï¼å¦‚æœé‚„æœ‰å…¶ä»–ç–‘å•æˆ–æƒ³è¦é€²ä¸€æ­¥ç®—å‡ºå…·é«”çš„æ•¸å€¼ï¼Œè«‹ç›´æ¥æä¾›ä½ çš„é¡¯å¡è¦æ ¼ï¼ˆVRAMã€å¡æ•¸ã€æ˜¯å¦è¶…é »ï¼‰ä»¥&lt;/p&gt; &lt;p&gt;åŠæƒ³è¦çš„ `token_len`ï¼Œæˆ‘æœƒç«‹åˆ»çµ¦ä½ çµæœã€‚ç¥ä½ ç©å¾—é–‹å¿ƒ ğŸš€ï¼&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; Send a message (/? for help)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kuerys"&gt; /u/kuerys &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nn5ur4/æ¨¡å‹èˆ‡é¡¯ç¤ºå¡_å…¬å¼_åˆ¤æ–·_èˆ‡ç”œèœœå€¼/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nn5ur4/æ¨¡å‹èˆ‡é¡¯ç¤ºå¡_å…¬å¼_åˆ¤æ–·_èˆ‡ç”œèœœå€¼/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nn5ur4/æ¨¡å‹èˆ‡é¡¯ç¤ºå¡_å…¬å¼_åˆ¤æ–·_èˆ‡ç”œèœœå€¼/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-21T22:37:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm4he9</id>
    <title>GPT 5 for Computer Use agents</title>
    <updated>2025-09-20T17:39:57+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nm4he9/gpt_5_for_computer_use_agents/"&gt; &lt;img alt="GPT 5 for Computer Use agents" src="https://external-preview.redd.it/N3kwajc1ODh3Y3FmMRG6-oWujtVtwWnIPYQYAphLJPDZc9z94p-KY-4O-UR8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a97694f82ea0cf901a2133a289ae2d142db6b036" title="GPT 5 for Computer Use agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same tasks, same grounding model we just swapped GPT 4o with GPT 5 as the thinking model.&lt;/p&gt; &lt;p&gt;Left = 4o, right = 5.&lt;/p&gt; &lt;p&gt;Watch GPT 5 pull through.&lt;/p&gt; &lt;p&gt;Grounding model: Salesforce GTA1-7B&lt;/p&gt; &lt;p&gt;Action space: CUA Cloud Instances (macOS/Linux/Windows)&lt;/p&gt; &lt;p&gt;The task is: &amp;quot;Navigate to {random_url} and play the game until you reach a score of 5/5â€....each task is set up by having claude generate a random app from a predefined list of prompts (multiple choice trivia, form filling, or color matching)&amp;quot;&lt;/p&gt; &lt;p&gt;Try it yourself here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs : &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agent"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agent&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.gg/cua-ai"&gt;https://discord.gg/cua-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/euxlwdk8wcqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nm4he9/gpt_5_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nm4he9/gpt_5_for_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-20T17:39:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmqziq</id>
    <title>Can my 12th Gen i3 processor with 8GB of RAM work with docker?</title>
    <updated>2025-09-21T12:44:15+00:00</updated>
    <author>
      <name>/u/Autumn_Red_29</name>
      <uri>https://old.reddit.com/user/Autumn_Red_29</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Autumn_Red_29"&gt; /u/Autumn_Red_29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/docker/comments/1nmqz0r/can_my_12th_gen_i3_processor_with_8gb_of_ram_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmqziq/can_my_12th_gen_i3_processor_with_8gb_of_ram_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nmqziq/can_my_12th_gen_i3_processor_with_8gb_of_ram_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-21T12:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmxfzx</id>
    <title>GLM-4.5V model for local computer use</title>
    <updated>2025-09-21T17:06:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nmxfzx/glm45v_model_for_local_computer_use/"&gt; &lt;img alt="GLM-4.5V model for local computer use" src="https://external-preview.redd.it/M2l1bDVtdjR2anFmMbWADQNBSkImjVESNjfi_q43l9ostHKNGAFX_QJdfnS0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ce566c5e77071f3f7f400637f6e0dc7338a416c" title="GLM-4.5V model for local computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either: Locally via Hugging Face Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord : &lt;a href="https://discord.gg/cua-ai"&gt;https://discord.gg/cua-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5z8xkkg5vjqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmxfzx/glm45v_model_for_local_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nmxfzx/glm45v_model_for_local_computer_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-21T17:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmwl1k</id>
    <title>We've Just Hit 400 Stars on Nanocoder - This Community Is Amazing ğŸ”¥</title>
    <updated>2025-09-21T16:32:51+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nmwl1k/weve_just_hit_400_stars_on_nanocoder_this/"&gt; &lt;img alt="We've Just Hit 400 Stars on Nanocoder - This Community Is Amazing ğŸ”¥" src="https://external-preview.redd.it/lxZd6DAdowPiPmyPupJyn2UjdkAEKDAl7mCc_8DECBE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a389a78f14886501f623dea0321b6040ed33089" title="We've Just Hit 400 Stars on Nanocoder - This Community Is Amazing ğŸ”¥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/3g7t1oyvnjqf1.gif"&gt;https://i.redd.it/3g7t1oyvnjqf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is yet another appreciation post for the community. Since my last, so much has happened in the Nanocoder community - new feedback, new features, many new people joining and contributing. It's honestly incredible to be building community-owned and pushed CLI software that breaks free of the corporations running other coding tools and offerings.&lt;/p&gt; &lt;p&gt;Along with a bunch of new features and improvements over the last couple of weeks, I'm actively moving the Nanocoder repository to be owned by a GitHub Organisation called Nano Collective â€“ this collective further reinforces my desire to make this project a community-led and run project. Within this collective I hope to continue to build out new frameworks and fine-tunes for local-first coding as well as seek grants to distribute to contributors to push research forward.&lt;/p&gt; &lt;p&gt;This is really really early days and Nanocoder as a coding CLI is right at the beginning, it's improving every day but there's still lots to do! That being said, any feedback and help within any domain is appreciated and welcomed.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;System prompt writing&lt;/li&gt; &lt;li&gt;Research&lt;/li&gt; &lt;li&gt;Helping to push the word out&lt;/li&gt; &lt;li&gt;Any feedback generally! Good or bad :)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to get involved the links are below. Bring on 1,000 stars â­ï¸&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Link&lt;/strong&gt;: &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord Link&lt;/strong&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmwl1k/weve_just_hit_400_stars_on_nanocoder_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmwl1k/weve_just_hit_400_stars_on_nanocoder_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nmwl1k/weve_just_hit_400_stars_on_nanocoder_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-21T16:32:51+00:00</published>
  </entry>
</feed>
