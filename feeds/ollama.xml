<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-18T06:47:11+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mr1ga3</id>
    <title>Bringing Computer Use to the Web</title>
    <updated>2025-08-15T15:40:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mr1ga3/bringing_computer_use_to_the_web/"&gt; &lt;img alt="Bringing Computer Use to the Web" src="https://external-preview.redd.it/anRqMTh2ZjJlN2pmMRcxEnlpDBBJVNjXlCDC4HUtgXjfB5ufLszRpp9PEi0H.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bdfe3f9648d7ccca224923687bacd117f3c3d894" title="Bringing Computer Use to the Web" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are bringing Computer Use to the web, you can now control cloud desktops from JavaScript right in the browser.&lt;/p&gt; &lt;p&gt;Until today computer use was Python only shutting out web devs. Now you can automate real UIs without servers, VMs, or any weird work arounds.&lt;/p&gt; &lt;p&gt;What you can now build : Pixel-perfect UI tests,Live AI demos,In app assistants that actually move the cursor, or parallel automation streams for heavy workloads.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Read more here : &lt;a href="https://www.trycua.com/blog/bringing-computer-use-to-the-web"&gt;https://www.trycua.com/blog/bringing-computer-use-to-the-web&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zrulwgp2e7jf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mr1ga3/bringing_computer_use_to_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mr1ga3/bringing_computer_use_to_the_web/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T15:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrsst2</id>
    <title>How can I generate ANSYS models directly by prompting an LLM?</title>
    <updated>2025-08-16T11:39:55+00:00</updated>
    <author>
      <name>/u/omarshoaib</name>
      <uri>https://old.reddit.com/user/omarshoaib</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm curious if anyone here has experimented with using &lt;strong&gt;large language models (LLMs)&lt;/strong&gt; to generate &lt;strong&gt;ANSYS models&lt;/strong&gt; directly from natural language prompts.&lt;/p&gt; &lt;p&gt;The idea would be:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You type something like &lt;em&gt;‚ÄúCreate a 1m x 0.1m cantilever beam, mesh at 0.01m, apply a tip load of 1000 N, solve for displacement‚Äù&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;The LLM then produces the correct &lt;strong&gt;ANSYS input&lt;/strong&gt; (APDL script, Mechanical Python script, Fluent journal, or PyAnsys code).&lt;/li&gt; &lt;li&gt;That script is then fed into ANSYS to actually build and solve the model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So instead of manually writing APDL or going step by step in Workbench, you just describe the setup in plain language and the LLM handles the code generation.&lt;/p&gt; &lt;h1&gt;Questions for the community&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Has anyone here tried prompting an LLM this way to build or solve models in ANSYS?&lt;/li&gt; &lt;li&gt;What‚Äôs the most practical route‚Äî&lt;strong&gt;APDL scripts&lt;/strong&gt;, &lt;strong&gt;Workbench journal files&lt;/strong&gt;, or &lt;strong&gt;PyAnsys (Python APIs)&lt;/strong&gt;?&lt;/li&gt; &lt;li&gt;Are there good practices for making sure the generated input is valid before running it in ANSYS?&lt;/li&gt; &lt;li&gt;Do you think this workflow is realistic for production use, or mainly a research/demo tool?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear if anyone has given this a shot (or has thoughts on how feasible it is).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omarshoaib"&gt; /u/omarshoaib &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mrsst2/how_can_i_generate_ansys_models_directly_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mrsst2/how_can_i_generate_ansys_models_directly_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mrsst2/how_can_i_generate_ansys_models_directly_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T11:39:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr9rke</id>
    <title>Is this possible or even the right tool?</title>
    <updated>2025-08-15T20:43:40+00:00</updated>
    <author>
      <name>/u/NervousMood8071</name>
      <uri>https://old.reddit.com/user/NervousMood8071</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote 1000 words a day for over 6 years and exported it all to plain ascii text files -- no markup -- no tags etc.&lt;/p&gt; &lt;p&gt;I want to know if getting an LLM to digest all of my journal entries is feasible and doable on a local PC with an I9 12th-gen CPU, 64gb RAM, and an Nvidia GPU with 16gb VRAM? &lt;/p&gt; &lt;p&gt;If so, where do I begin? I want to be able to query the resulting LLM for stuff I've written. I was terribly organized and haphazard in my writing. For example I'd start reminiscing about events in the past interspersed with chores to do this week, plot outlines for stories, aborted first chapters etc. I would love to be able to query the LLM afterward to pullout topics at will.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NervousMood8071"&gt; /u/NervousMood8071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mr9rke/is_this_possible_or_even_the_right_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mr9rke/is_this_possible_or_even_the_right_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mr9rke/is_this_possible_or_even_the_right_tool/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T20:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrrbsh</id>
    <title>RTX 5090 @ 200W | Capped Core &amp; Power | AI Inference Efficiency</title>
    <updated>2025-08-16T10:25:42+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/nvidia/comments/1mrrb7e/rtx_5090_200w_capped_core_power_ai_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mrrbsh/rtx_5090_200w_capped_core_power_ai_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mrrbsh/rtx_5090_200w_capped_core_power_ai_inference/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T10:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mruik7</id>
    <title>Is web search tool calling only available with the got-oss models?</title>
    <updated>2025-08-16T12:56:54+00:00</updated>
    <author>
      <name>/u/Equivalent-Win-1294</name>
      <uri>https://old.reddit.com/user/Equivalent-Win-1294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will the web search tool and path be available to other compatible models as well?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent-Win-1294"&gt; /u/Equivalent-Win-1294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mruik7/is_web_search_tool_calling_only_available_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mruik7/is_web_search_tool_calling_only_available_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mruik7/is_web_search_tool_calling_only_available_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T12:56:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrawvt</id>
    <title>Open Moxie - Fully Offline (ollama option) and XAI Grok API</title>
    <updated>2025-08-15T21:26:23+00:00</updated>
    <author>
      <name>/u/Over-Mix7071</name>
      <uri>https://old.reddit.com/user/Over-Mix7071</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mrawvt/open_moxie_fully_offline_ollama_option_and_xai/"&gt; &lt;img alt="Open Moxie - Fully Offline (ollama option) and XAI Grok API" src="https://external-preview.redd.it/a1zSbUhyWKbtVHaGz5nbMQPuqamAd8HDk_nVmyFL_-4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4aae6540bec8fcaa94d2dcf38e781bec38212860" title="Open Moxie - Fully Offline (ollama option) and XAI Grok API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Over-Mix7071"&gt; /u/Over-Mix7071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l7mikg3609jf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mrawvt/open_moxie_fully_offline_ollama_option_and_xai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mrawvt/open_moxie_fully_offline_ollama_option_and_xai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T21:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrwsyx</id>
    <title>Smart assist Home Assistant.</title>
    <updated>2025-08-16T14:27:50+00:00</updated>
    <author>
      <name>/u/Original-Chapter-112</name>
      <uri>https://old.reddit.com/user/Original-Chapter-112</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Original-Chapter-112"&gt; /u/Original-Chapter-112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/homeassistant/comments/1mrws8z/smart_assist_home_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mrwsyx/smart_assist_home_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mrwsyx/smart_assist_home_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T14:27:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms59k7</id>
    <title>A Guide to GRPO Fine-Tuning on Windows Using the TRL Library</title>
    <updated>2025-08-16T19:33:04+00:00</updated>
    <author>
      <name>/u/oridnary_artist</name>
      <uri>https://old.reddit.com/user/oridnary_artist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ms59k7/a_guide_to_grpo_finetuning_on_windows_using_the/"&gt; &lt;img alt="A Guide to GRPO Fine-Tuning on Windows Using the TRL Library" src="https://preview.redd.it/3o9tbazgofjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecf02796e74bfbbb6661da8cb781ff654a0af2a2" title="A Guide to GRPO Fine-Tuning on Windows Using the TRL Library" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wrote a hands-on guide for fine-tuning LLMs with GRPO (Group-Relative PPO) locally on Windows, using Hugging Face's TRL library. My goal was to create a practical workflow that doesn't require Colab or Linux.&lt;/p&gt; &lt;p&gt;The guide and the accompanying script focus on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;A TRL-based implementation&lt;/strong&gt; that runs on consumer GPUs (with LoRA and optional 4-bit quantization).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A verifiable reward system&lt;/strong&gt; that uses numeric, format, and boilerplate checks to create a more reliable training signal.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Automatic data mapping&lt;/strong&gt; for most Hugging Face datasets to simplify preprocessing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Practical troubleshooting&lt;/strong&gt; and configuration notes for local setups.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is for anyone looking to experiment with reinforcement learning techniques on their own machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Read the blog post:&lt;/strong&gt; &lt;a href="https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323"&gt;&lt;code&gt;https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get the code:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/trl-ppo-fine-tuning"&gt;Reinforcement-learning-with-verifable-rewards-Learnings/projects/trl-ppo-fine-tuning at main ¬∑ Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm open to any feedback. Thanks!&lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. I'm currently looking for my next role in the LLM / Computer Vision space and would love to connect about any opportunities&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Portfolio:&lt;/em&gt; &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;Pavan Kunchala - AI Engineer &amp;amp; Full-Stack Developer&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oridnary_artist"&gt; /u/oridnary_artist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3o9tbazgofjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms59k7/a_guide_to_grpo_finetuning_on_windows_using_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ms59k7/a_guide_to_grpo_finetuning_on_windows_using_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T19:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms5cjg</id>
    <title>üöÄ New Feature in RAGLight: Effortless MCP Integration for Agentic RAG Pipelines! üîå</title>
    <updated>2025-08-16T19:36:10+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I just shipped a new feature in &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;RAGLight&lt;/a&gt;, my lightweight and modular Python framework for Retrieval-Augmented Generation, and it's a big one: &lt;strong&gt;easy MCP Server integration&lt;/strong&gt; for Agentic RAG workflows. üß†üíª&lt;/p&gt; &lt;h1&gt;What's new?&lt;/h1&gt; &lt;p&gt;You can now plug in external tools directly into your agent's reasoning process using an MCP server. No boilerplate required. Whether you're building code assistants, tool-augmented LLM agents, or just want your LLM to interact with a live backend, it's now just a few lines of config.&lt;/p&gt; &lt;h1&gt;Example:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;config = AgenticRAGConfig( provider = Settings.OPENAI, model = &amp;quot;gpt-4o&amp;quot;, k = 10, mcp_config = [ {&amp;quot;url&amp;quot;: &amp;quot;http://127.0.0.1:8001/sse&amp;quot;} # Your MCP server URL ], ... ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This automatically injects all MCP tools into the agent's toolset.&lt;/p&gt; &lt;p&gt;üìö If you're curious how to write your own MCP tool or server, you can check the &lt;code&gt;MCPClient.server_parameters&lt;/code&gt; doc from &lt;a href="https://huggingface.co/docs/smolagents/en/reference/tools#smolagents.MCPClient.server_parameters"&gt;smolagents&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;üëâ Try it out and let me know what you think: &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;https://github.com/Bessouat40/RAGLight&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms5cjg/new_feature_in_raglight_effortless_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms5cjg/new_feature_in_raglight_effortless_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ms5cjg/new_feature_in_raglight_effortless_mcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T19:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1msclnz</id>
    <title>Phi 3 Mini needing 50gb??</title>
    <updated>2025-08-17T00:22:40+00:00</updated>
    <author>
      <name>/u/Fun-Tangerine5264</name>
      <uri>https://old.reddit.com/user/Fun-Tangerine5264</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1msclnz/phi_3_mini_needing_50gb/"&gt; &lt;img alt="Phi 3 Mini needing 50gb??" src="https://a.thumbs.redditmedia.com/ITn7NYWmJ_MOxmF9GA6WyOx8eGtT06QxvF8ZTULN108.jpg" title="Phi 3 Mini needing 50gb??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/64fvg8f44hjf1.png?width=1825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e38d036375ba95767abce394acc2cef1eeb7aff8"&gt;https://preview.redd.it/64fvg8f44hjf1.png?width=1825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e38d036375ba95767abce394acc2cef1eeb7aff8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Erm, I don't think this is supposed to happen on a mini model no?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Tangerine5264"&gt; /u/Fun-Tangerine5264 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msclnz/phi_3_mini_needing_50gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msclnz/phi_3_mini_needing_50gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msclnz/phi_3_mini_needing_50gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T00:22:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mso19g</id>
    <title>How to maximize qwen-coder-30b TPS on a 4060 Ti (8 GB)?</title>
    <updated>2025-08-17T11:04:52+00:00</updated>
    <author>
      <name>/u/Overall-Branch-1496</name>
      <uri>https://old.reddit.com/user/Overall-Branch-1496</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Overall-Branch-1496"&gt; /u/Overall-Branch-1496 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1msnxpy/how_to_maximize_qwencoder30b_tps_on_a_4060_ti_8_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mso19g/how_to_maximize_qwencoder30b_tps_on_a_4060_ti_8_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mso19g/how_to_maximize_qwencoder30b_tps_on_a_4060_ti_8_gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T11:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1msarft</id>
    <title>Why does gpt-oss 120b run slower in ollama than in LM Studio in my setup?</title>
    <updated>2025-08-16T23:03:33+00:00</updated>
    <author>
      <name>/u/Southern-Chain-6485</name>
      <uri>https://old.reddit.com/user/Southern-Chain-6485</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My hardware is an RTX 3090 + 64gb of ddr4 ram. LM Studio runs it at something about 10-12 tokens per second (I don't have the actual measure at hand) while ollama runs it at half the speed, at best. I'm using the lm studio community version in LM Studio and the version downloaded from ollama's site with ollama - basically, the recommended versions. Are there flags that need to be run in Ollama to match LM Studio performance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Southern-Chain-6485"&gt; /u/Southern-Chain-6485 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msarft/why_does_gptoss_120b_run_slower_in_ollama_than_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msarft/why_does_gptoss_120b_run_slower_in_ollama_than_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msarft/why_does_gptoss_120b_run_slower_in_ollama_than_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T23:03:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1msflr9</id>
    <title>Anyone have tokens per second results for gpt-oss 20-b on an ada 2000?</title>
    <updated>2025-08-17T02:50:33+00:00</updated>
    <author>
      <name>/u/RasPiBuilder</name>
      <uri>https://old.reddit.com/user/RasPiBuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for something with relatively low power draw and decent inference speeds. I don't need it to be blazing fast, but it does need to be responsive at reasonable speeds (hoping for around 7-10t/s). &lt;/p&gt; &lt;p&gt;For this particular setup power draw is the bottleneck, where my absolute max is 100w. Cost is less of an issue, though I'd lean towards the least expensive on comparable speed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RasPiBuilder"&gt; /u/RasPiBuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msflr9/anyone_have_tokens_per_second_results_for_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msflr9/anyone_have_tokens_per_second_results_for_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msflr9/anyone_have_tokens_per_second_results_for_gptoss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T02:50:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms1v2p</id>
    <title>Ollama interface with memory</title>
    <updated>2025-08-16T17:31:04+00:00</updated>
    <author>
      <name>/u/mrdougwright</name>
      <uri>https://old.reddit.com/user/mrdougwright</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;Ollama is so cool, it inspired me to do some open source!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mrdougwright/yak"&gt;https://github.com/mrdougwright/yak&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.npmjs.com/package/yak-llm"&gt;https://www.npmjs.com/package/yak-llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yak is a CLI interface with persistent chat sessions for local LLMs. Instead of losing context every time you restart, it remembers your conversations across sessions and lets you organize them by topic. &lt;/p&gt; &lt;p&gt;Key features:&lt;br /&gt; - Multiple chat sessions (work, personal, coding help, etc.)&lt;br /&gt; - Persistent memory using simple JSONL files&lt;br /&gt; - Auto-starts Ollama if needed&lt;br /&gt; - Switch models from the CLI&lt;br /&gt; - Zero config for new users &lt;/p&gt; &lt;p&gt;Install: `npm install -g yak-llm`&lt;br /&gt; Usage: `yak start` &lt;/p&gt; &lt;p&gt;Built this because I wanted something lightweight that actually remembers context and doesn't slow down with long conversations. Plus you can directly edit the chat files if needed! &lt;/p&gt; &lt;p&gt;Would love feedback from the Ollama community! ü¶ß&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrdougwright"&gt; /u/mrdougwright &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms1v2p/ollama_interface_with_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms1v2p/ollama_interface_with_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ms1v2p/ollama_interface_with_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T17:31:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1msvir4</id>
    <title>Getting a memory error when downloading a model</title>
    <updated>2025-08-17T16:32:23+00:00</updated>
    <author>
      <name>/u/LaPreparando</name>
      <uri>https://old.reddit.com/user/LaPreparando</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1msvir4/getting_a_memory_error_when_downloading_a_model/"&gt; &lt;img alt="Getting a memory error when downloading a model" src="https://a.thumbs.redditmedia.com/NzO-T4fMtBWXQHMWdkyvR2WoSj2Ico-GoNBllptue00.jpg" title="Getting a memory error when downloading a model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made sure that the folder for the models is under D:, and it really downloads it to D:. But then I get a memory error, basically because C: doesn't have enough space (which I am aware of). Any idea how to avoid/fix that? &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/iyku9u96xljf1.png?width=813&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa19a05fd216ceaf378f045d736e8c4f1d418a5d"&gt;https://preview.redd.it/iyku9u96xljf1.png?width=813&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa19a05fd216ceaf378f045d736e8c4f1d418a5d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LaPreparando"&gt; /u/LaPreparando &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msvir4/getting_a_memory_error_when_downloading_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msvir4/getting_a_memory_error_when_downloading_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msvir4/getting_a_memory_error_when_downloading_a_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T16:32:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1msy9w5</id>
    <title>Looking for most optimal llms for ollama</title>
    <updated>2025-08-17T18:17:25+00:00</updated>
    <author>
      <name>/u/biggerbuiltbody</name>
      <uri>https://old.reddit.com/user/biggerbuiltbody</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just downloaded Ollama yesterday, and the list of all the models is a bit overwhelming, lol. i got a 300gb hard drive and an RTX 3060, and i am looking for an llm to help with some coding, general questions, maybe some math, idek, but if anyone's got any recs or even a google drive or something, I'd appreciate any help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/biggerbuiltbody"&gt; /u/biggerbuiltbody &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msy9w5/looking_for_most_optimal_llms_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msy9w5/looking_for_most_optimal_llms_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msy9w5/looking_for_most_optimal_llms_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T18:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1msw3k2</id>
    <title>how to force qwen3 to stop thinking? or even just how to limit the thinking effort?</title>
    <updated>2025-08-17T16:55:15+00:00</updated>
    <author>
      <name>/u/YaBoiGPT</name>
      <uri>https://old.reddit.com/user/YaBoiGPT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey yall so im using qwen3 0.6b and it seems to ignore my no_think tags and idk how to control the reasoning effort. any tips? thx&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YaBoiGPT"&gt; /u/YaBoiGPT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msw3k2/how_to_force_qwen3_to_stop_thinking_or_even_just/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msw3k2/how_to_force_qwen3_to_stop_thinking_or_even_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msw3k2/how_to_force_qwen3_to_stop_thinking_or_even_just/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T16:55:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms8ghv</id>
    <title>I made a no-install-needed web-GUI for Ollama</title>
    <updated>2025-08-16T21:32:51+00:00</updated>
    <author>
      <name>/u/DarkTom21</name>
      <uri>https://old.reddit.com/user/DarkTom21</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ms8ghv/i_made_a_noinstallneeded_webgui_for_ollama/"&gt; &lt;img alt="I made a no-install-needed web-GUI for Ollama" src="https://b.thumbs.redditmedia.com/fUb2ZooOQRQelk4szZCBNb_brztbxvoPSTkbeSXzDhk.jpg" title="I made a no-install-needed web-GUI for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;For the last while I've been working on a solution to a problem I've had ever since getting into Ollama, that being a GUI that is both powerful and easy to set up across multiple devices. Firstly I tried using OpenWebUI, however quickly dropped it due to needing to install either Python or Docker just to run it, and I didn't want to install more runtimes just to run a GUI. I looked at alternatives, but none seemed to quite fit what I wanted.&lt;/p&gt; &lt;p&gt;That's why I decided to make LlamaPen, LlamaPen is an open-source, no-download-required web app/GUI that lets you easily interface with your local instance of Ollama without needing to download anything extra. It contains all the basics you would expect from a GUI such as chats, conversations, and model selection, but also contains additional features, such as model management, downloading, mobile, PWA &amp;amp; offline support, formatting markdown and think text, icons for each model, and more, all without needing to go through a lengthy download and setup process.&lt;/p&gt; &lt;p&gt;It is currently available live at &lt;a href="https://llamapen.app/"&gt;https://llamapen.app/&lt;/a&gt; with a GitHub repo going further into the specifics and features at &lt;a href="https://github.com/ImDarkTom/LlamaPen"&gt;https://github.com/ImDarkTom/LlamaPen&lt;/a&gt;. If you have any questions or would like to know more feel free to leave a comment here and I will try to reply as soon as possible, and if you encounter any issues you can either comment here or I recommend opening an issue on the GitHub repo for faster support.&lt;/p&gt; &lt;p&gt;Thanks for reading and I hope at least one other person than me finds this useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkTom21"&gt; /u/DarkTom21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ms8ghv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms8ghv/i_made_a_noinstallneeded_webgui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ms8ghv/i_made_a_noinstallneeded_webgui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T21:32:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mte3wt</id>
    <title>RPG Game engine running on qwen3</title>
    <updated>2025-08-18T06:14:21+00:00</updated>
    <author>
      <name>/u/Humbrol2</name>
      <uri>https://old.reddit.com/user/Humbrol2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building an rpg game engine built on ollama models, use qwen3 with 128k tokens in settings but its running a tad slow. ALways looking for feedback&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/humbrol2/RPG-Assistant"&gt;humbrol2/RPG-Assistant: An AI assisted RPG game engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;RPG Assistant&lt;/p&gt; &lt;p&gt;An AI-powered RPG Game Master using Ollama local models for immersive tabletop gaming experiences.&lt;/p&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-Agent System&lt;/strong&gt;: Game Master, World Builder, Character Manager, and Narrative Engine&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Persistent Storage&lt;/strong&gt;: JSON-based world and character data management&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Command-Line Interface&lt;/strong&gt;: Easy-to-use CLI for game management&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open World Sandbox&lt;/strong&gt;: Dynamic world generation and storytelling&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humbrol2"&gt; /u/Humbrol2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mte3wt/rpg_game_engine_running_on_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mte3wt/rpg_game_engine_running_on_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mte3wt/rpg_game_engine_running_on_qwen3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T06:14:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1msobrw</id>
    <title>Chat Box: Open-Source Browser Extension</title>
    <updated>2025-08-17T11:21:41+00:00</updated>
    <author>
      <name>/u/MinhxThanh</name>
      <uri>https://old.reddit.com/user/MinhxThanh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1msobrw/chat_box_opensource_browser_extension/"&gt; &lt;img alt="Chat Box: Open-Source Browser Extension" src="https://external-preview.redd.it/eGN6eTcxOG9ka2pmMVQLTLeRVkAMK-J22uN-yMath0VT_Z9kdFkV696qxezn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f15eeef2e915c27a053a58c45bde7ef6fa8155a9" title="Chat Box: Open-Source Browser Extension" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I wanted to share this open-source project I've come across called Chat Box. It's a browser extension that brings AI chat, advanced web search, document interaction, and other handy tools right into a sidebar in your browser. It's designed to make your online workflow smoother without needing to switch tabs or apps constantly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What It Does&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;At its core, Chat Box gives you a persistent AI-powered chat interface that you can access with a quick shortcut (Ctrl+E or Cmd+E). It supports a bunch of AI providers like OpenAI, DeepSeek, Claude, and even local LLMs via Ollama. You just configure your API keys in the settings, and you're good to go.&lt;/p&gt; &lt;p&gt;It's all open-source under GPL-3.0, so you can tweak it if you want.&lt;/p&gt; &lt;p&gt;If you run into any errors, issues, or want to suggest a new feature, please create a new Issue on GitHub and describe it in detail ‚Äì I'll respond ASAP!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/MinhxThanh/Chat-Box"&gt;https://github.com/MinhxThanh/Chat-Box&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chrome Web Store: &lt;a href="https://chromewebstore.google.com/detail/chat-box-chat-with-all-ai/hhaaoibkigonnoedcocnkehipecgdodm"&gt;https://chromewebstore.google.com/detail/chat-box-chat-with-all-ai/hhaaoibkigonnoedcocnkehipecgdodm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Firefox Add-Ons: &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/chat-box-chat-with-all-ai/"&gt;https://addons.mozilla.org/en-US/firefox/addon/chat-box-chat-with-all-ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MinhxThanh"&gt; /u/MinhxThanh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uv7l508odkjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msobrw/chat_box_opensource_browser_extension/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msobrw/chat_box_opensource_browser_extension/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T11:21:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1msjayu</id>
    <title>Knowledge graph using gemma3</title>
    <updated>2025-08-17T06:13:22+00:00</updated>
    <author>
      <name>/u/cython_boy</name>
      <uri>https://old.reddit.com/user/cython_boy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1msjayu/knowledge_graph_using_gemma3/"&gt; &lt;img alt="Knowledge graph using gemma3" src="https://preview.redd.it/fviiip42uijf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1890c5ec0cd777145b90a7dff123fc028801135" title="Knowledge graph using gemma3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;strong&gt;Streamlit web app&lt;/strong&gt; that generates interactive &lt;strong&gt;knowledge graphs&lt;/strong&gt; from plain text using &lt;strong&gt;Ollama's open sourcw models.(geema3 , grnaite , llama3 , gpt-oss ,....)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Two input methods&lt;/strong&gt;: Upload &lt;code&gt;.txt&lt;/code&gt; file or paste text directly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama model integration&lt;/strong&gt;: Select from available local models (e.g., Gemma, Mistral, LLaMA).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Automatic graph storage&lt;/strong&gt;: Generated graphs are saved and can be reloaded anytime.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive visualization&lt;/strong&gt;: Zoom, drag, and explore relationships between concepts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimized for speed&lt;/strong&gt;: Uses hashed filenames to prevent regenerating the same graph.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/ganeshnikhil/Kgraph"&gt;kgraph&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cython_boy"&gt; /u/cython_boy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fviiip42uijf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msjayu/knowledge_graph_using_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msjayu/knowledge_graph_using_gemma3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T06:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt4fjp</id>
    <title>Connected Chrome's AI API to ollama, enabling ANY web app built for Chrome's local Gemini to seamlessly work with open-source LLMs</title>
    <updated>2025-08-17T22:19:07+00:00</updated>
    <author>
      <name>/u/andrei0david</name>
      <uri>https://old.reddit.com/user/andrei0david</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andrei0david"&gt; /u/andrei0david &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/AndreiDavid/status/1956692594878038046"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mt4fjp/connected_chromes_ai_api_to_ollama_enabling_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mt4fjp/connected_chromes_ai_api_to_ollama_enabling_any/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T22:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mslsvy</id>
    <title>Qwen 4B on iPhone Neural Engine runs at 20t/s</title>
    <updated>2025-08-17T08:46:29+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mslsvy/qwen_4b_on_iphone_neural_engine_runs_at_20ts/"&gt; &lt;img alt="Qwen 4B on iPhone Neural Engine runs at 20t/s" src="https://external-preview.redd.it/cHhxMXV5aTJtampmMdanLIb7eGjfE0jGaRLozRDSee5XNaS_ENo8Z9uouVRD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ee59b94928cc5ff9aff58431a6e1d4fbdc8158b" title="Qwen 4B on iPhone Neural Engine runs at 20t/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am excited to finally bring 4B models to iPhone!&lt;/p&gt; &lt;p&gt;Vector Space is a framework that makes it possible to run LLM on iPhones &lt;strong&gt;locally on the Neural Engine&lt;/strong&gt;. This translates to:&lt;/p&gt; &lt;p&gt;‚ö°Ô∏èFaster inference. Qwen 4B runs at &lt;strong&gt;~20 token/s&lt;/strong&gt; in short context.&lt;/p&gt; &lt;p&gt;üîã Low Energy. Energy consumption is 1/5 compared to CPU, which means your iPhone will stay cool and it will not drain your battery. &lt;/p&gt; &lt;p&gt;Vector Space also comes with an app üì≤ that allows you to download models and try out the framework with 0 code. Try it now on TestFlight:&lt;/p&gt; &lt;p&gt;&lt;a href="https://testflight.apple.com/join/HXyt2bjU"&gt;https://testflight.apple.com/join/HXyt2bjU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fine prints: 1. The app all does not guarantee the persistence of data. 2. Currently only supports hardware released on or after 2022 (&amp;gt;= iPhone 14) 3. First time model compilation will take several minutes. Subsequent loads will be instant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xewf9vn2mjjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mslsvy/qwen_4b_on_iphone_neural_engine_runs_at_20ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mslsvy/qwen_4b_on_iphone_neural_engine_runs_at_20ts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T08:46:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtclvn</id>
    <title>Qwen3 models - cannot disable thinking now?</title>
    <updated>2025-08-18T04:47:15+00:00</updated>
    <author>
      <name>/u/derSchwamm11</name>
      <uri>https://old.reddit.com/user/derSchwamm11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried a few Qwen3 models like 14b with ollama. Using /no_think in the system or user prompt has no effect, neither does setting &amp;quot;think&amp;quot;: false in the JSON payload to /chat/completions. Firing up the model in the terminal and using /set nothink also does nothing.&lt;/p&gt; &lt;p&gt;This all seems to fly in the face of documentation in the model and from ollama and I am going crazy here. Am I missing something? I am on ollama 0.11.4, linux&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/derSchwamm11"&gt; /u/derSchwamm11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtclvn/qwen3_models_cannot_disable_thinking_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtclvn/qwen3_models_cannot_disable_thinking_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtclvn/qwen3_models_cannot_disable_thinking_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T04:47:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt9fop</id>
    <title>Serene Pub 0.4 Release - Ollama Manager, Accessability &amp; Tags</title>
    <updated>2025-08-18T02:07:10+00:00</updated>
    <author>
      <name>/u/doolijb</name>
      <uri>https://old.reddit.com/user/doolijb</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doolijb"&gt; /u/doolijb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mt8hd1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mt9fop/serene_pub_04_release_ollama_manager/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mt9fop/serene_pub_04_release_ollama_manager/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T02:07:10+00:00</published>
  </entry>
</feed>
