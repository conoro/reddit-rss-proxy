<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-03T10:52:45+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pam6dr</id>
    <title>Built a Modular Agentic RAG System – Zero Boilerplate, Full Customization</title>
    <updated>2025-11-30T16:17:32+00:00</updated>
    <author>
      <name>/u/CapitalShake3085</name>
      <uri>https://old.reddit.com/user/CapitalShake3085</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pam6dr/built_a_modular_agentic_rag_system_zero/"&gt; &lt;img alt="Built a Modular Agentic RAG System – Zero Boilerplate, Full Customization" src="https://preview.redd.it/vlxnbaqn2f4g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=f25c990e106612bf0d2b452ea1356240ec2e15f9" title="Built a Modular Agentic RAG System – Zero Boilerplate, Full Customization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CapitalShake3085"&gt; /u/CapitalShake3085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vlxnbaqn2f4g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pam6dr/built_a_modular_agentic_rag_system_zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pam6dr/built_a_modular_agentic_rag_system_zero/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T16:17:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pag2c6</id>
    <title>Built a tool to easily self-host AI models on AWS - now I need uncensored models to test it for Red Teaming</title>
    <updated>2025-11-30T11:26:11+00:00</updated>
    <author>
      <name>/u/dumbelco</name>
      <uri>https://old.reddit.com/user/dumbelco</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a &amp;quot;deploy-and-destroy&amp;quot; tool that spins up a self-hosted AI lab on AWS (running Ollama/Open WebUI on a GPU instance).&lt;/p&gt; &lt;p&gt;Now that the infrastructure is working, I want to test it with some actual cybersecurity workflows. I'm looking for recommendations for models available on Ollama that are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Strictly uncensored&lt;/strong&gt; (won't refuse to generate Python scripts for CTFs or pentesting/red teaming research).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart at coding&lt;/strong&gt; (can handle complex logic without breaking).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any recommendations?&lt;/p&gt; &lt;p&gt;Also, for models like these, should I stick to downloading models directly from Ollama, or is it worth looking into importing models from Hugging Face instead?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dumbelco"&gt; /u/dumbelco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pag2c6/built_a_tool_to_easily_selfhost_ai_models_on_aws/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pag2c6/built_a_tool_to_easily_selfhost_ai_models_on_aws/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pag2c6/built_a_tool_to_easily_selfhost_ai_models_on_aws/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T11:26:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pafex0</id>
    <title>EGPU for ai use?</title>
    <updated>2025-11-30T10:46:55+00:00</updated>
    <author>
      <name>/u/R0B0t1C_Cucumber</name>
      <uri>https://old.reddit.com/user/R0B0t1C_Cucumber</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I posted a week or two ago about looking for an agentic coder type do-dad like Claude CLI and this awesome community pointed me to aidler/ollama (though OI +llama ccp work fine too if anyone is looking at alternatives). Any way I found the sweet spot for me 14b Q4 llms seem to be the sweet spot for a 4070Ti between performance and quality. &lt;/p&gt; &lt;p&gt;Now looking around I found I have some spare hardware I wanted to see if anyone has tried anything like this... Now again, this is just for me to tinker with... but I have a spare intel ARC 770 16GB Vram and I also found laying around an EGPU enclosure with a 400w dedicated power supply in it... Connects over thunderbolt.... Could I somehow leverage this extra compute/vram through Ollama ? I wouldn't actually want anything to display through the card, I just want its resource.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R0B0t1C_Cucumber"&gt; /u/R0B0t1C_Cucumber &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pafex0/egpu_for_ai_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pafex0/egpu_for_ai_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pafex0/egpu_for_ai_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T10:46:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb7d4v</id>
    <title>UPLOAD LLAMA.CPP FRONTEND IN GITHUB FOR SERVER OVER LAN MORE EASY</title>
    <updated>2025-12-01T08:18:38+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pb7d4v/upload_llamacpp_frontend_in_github_for_server/"&gt; &lt;img alt="UPLOAD LLAMA.CPP FRONTEND IN GITHUB FOR SERVER OVER LAN MORE EASY" src="https://b.thumbs.redditmedia.com/59ddSZ3iPzn4iR6piUftC1Z64DmbOf8ceb7cIq8qdeY.jpg" title="UPLOAD LLAMA.CPP FRONTEND IN GITHUB FOR SERVER OVER LAN MORE EASY" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ollama/comments/1p9jjv3/upload_llamacpp_frontend_in_github_for_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pb7d4v/upload_llamacpp_frontend_in_github_for_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pb7d4v/upload_llamacpp_frontend_in_github_for_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T08:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa4x3y</id>
    <title>CUA Local Opensource</title>
    <updated>2025-11-30T00:55:27+00:00</updated>
    <author>
      <name>/u/Goat_bless</name>
      <uri>https://old.reddit.com/user/Goat_bless</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pa4x3y/cua_local_opensource/"&gt; &lt;img alt="CUA Local Opensource" src="https://preview.redd.it/96acfm1pla4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=927ce5830d8f7bf3fe412291022af0d437c6a60b" title="CUA Local Opensource" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bonjour à tous,&lt;/p&gt; &lt;p&gt;I've created my biggest project to date.&lt;br /&gt; A local open-source computer agent, it uses a fairly complex architecture to perform a very large number of tasks, if not all tasks.&lt;br /&gt; I’m not going to write too much to explain how it all works; those who are interested can check the GitHub, it’s very well detailed.&lt;br /&gt; In summary:&lt;br /&gt; For each user input, the agent understands whether it needs to speak or act.&lt;br /&gt; If it needs to speak, it uses memory and context to produce appropriate sentences.&lt;br /&gt; If it needs to act, there are two choices:&lt;/p&gt; &lt;p&gt;A simple action: open an application, lower the volume, launch Google, open a folder...&lt;br /&gt; Everything is done in a single action.&lt;/p&gt; &lt;p&gt;A complex action: browse the internet, create a file with data retrieved online, interact with an application...&lt;br /&gt; Here it goes through an orchestrator that decides what actions to take (multistep) and checks that each action is carried out properly until the global task is completed.&lt;br /&gt; How?&lt;br /&gt; Architecture of a complex action:&lt;br /&gt; LLM orchestrator receives the global task and decides the next action.&lt;br /&gt; For internet actions: CUA first attempts Playwright — 80% of cases solved.&lt;br /&gt; If it fails (and this is where it gets interesting):&lt;br /&gt; It uses CUA VISION: Screenshot — VLM1 sees the page and suggests what to do — Data detection on the page (Ominparser: YOLO + Florence) + PaddleOCR — Annotation of the data on the screenshot — VLM2 sees the annotated screen and tells which ID to click — Pyautogui clicks on the coordinates linked to the ID — Loops until Task completed.&lt;br /&gt; In both cases (complex or simple) return to the orchestrator which finishes all actions and sends a message to the user once the task is completed.&lt;/p&gt; &lt;p&gt;This agent has the advantage of running locally with only my 8GB VRAM; I use the LLM models: qwen2.5, VLM: qwen2.5vl and qwen3vl.&lt;br /&gt; If you have more VRAM, with better models you’ll gain in performance and speed.&lt;br /&gt; Currently, this agent can solve 80–90% of the tasks we can perform on a computer, and I’m open to improvements or knowledge-sharing to make it a common and useful project for everyone.&lt;br /&gt; The GitHub link: &lt;a href="https://github.com/SpendinFR/CUAOS"&gt;https://github.com/SpendinFR/CUAOS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Goat_bless"&gt; /u/Goat_bless &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/96acfm1pla4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pa4x3y/cua_local_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pa4x3y/cua_local_opensource/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T00:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pah9zz</id>
    <title>Uncensored ollama models for my pc</title>
    <updated>2025-11-30T12:36:16+00:00</updated>
    <author>
      <name>/u/Automatic-Pin9116</name>
      <uri>https://old.reddit.com/user/Automatic-Pin9116</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My pc is i3, 8gb ram, no dedicated vram or gpu. I want a model that'll run in this pc. Fully uncensored. Also maybe roleplay too, though im looking more of uncensored ai models where i can use restricted words (like cu#t and pe###). i just want a open, uncensored, good, knowledge full ai to talk to freely with freedom.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Automatic-Pin9116"&gt; /u/Automatic-Pin9116 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pah9zz/uncensored_ollama_models_for_my_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pah9zz/uncensored_ollama_models_for_my_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pah9zz/uncensored_ollama_models_for_my_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T12:36:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb7la3</id>
    <title>deepseek-ocr in ollama - questions</title>
    <updated>2025-12-01T08:33:15+00:00</updated>
    <author>
      <name>/u/EatTFM</name>
      <uri>https://old.reddit.com/user/EatTFM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran a few tests with deepseek-ocr using scanned medical forms and got mixed results. I figured that the prompt is very sensitive, and it cannot handle any additional instructions at all - maybe because it is the 6.7B model. Recognition seems accurate, but it often misses or hallucinates parts of the layout e.g. a cell in a table or headings.&lt;/p&gt; &lt;p&gt;I have the following questions&lt;/p&gt; &lt;ul&gt; &lt;li&gt;will there be support for the larger model variants?&lt;/li&gt; &lt;li&gt;is there a way to feed multiple pages in a single query? as I understand this should be doable due to the huge saving of vision tokens of this particular architecture.&lt;/li&gt; &lt;li&gt;has someone managed to get a consistent output formatting?&lt;/li&gt; &lt;li&gt;has someone managed to extract json instead of markdown? or extract explicit information wrt content?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;thank you for your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EatTFM"&gt; /u/EatTFM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pb7la3/deepseekocr_in_ollama_questions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pb7la3/deepseekocr_in_ollama_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pb7la3/deepseekocr_in_ollama_questions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T08:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbalp3</id>
    <title>[LLM Fine-Tuning] CPT on 71M Short Dialectal Tokens (256 Max Len) - How to Ensure Long-Form Generation Later?</title>
    <updated>2025-12-01T11:40:42+00:00</updated>
    <author>
      <name>/u/FishermanNo2017</name>
      <uri>https://old.reddit.com/user/FishermanNo2017</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm working on Continued Pre-Training (CPT) for a Gemma 4B/12B model on a social media dataset containing a specific arabic dialect (a low resource language). My goal is to eventually use this model for complex, long-form QA about local history and geography, answered in in this dialect.&lt;/p&gt; &lt;p&gt;My token analysis has presented a classic challenge:&lt;/p&gt; &lt;p&gt;|| || |&lt;strong&gt;Metric&lt;/strong&gt;|&lt;strong&gt;Value&lt;/strong&gt;|&lt;strong&gt;Implication&lt;/strong&gt;| |&lt;strong&gt;Total Corpus&lt;/strong&gt;|71.76 Million Tokens|Good size for CPT.| |&lt;strong&gt;95th Percentile&lt;/strong&gt;|109 tokens|95% of data is very short.| |&lt;strong&gt;CPT Max Sequence Length&lt;/strong&gt;|&lt;strong&gt;256 tokens&lt;/strong&gt;|Recommended for efficiency (captures &amp;gt;99% of data via packing).|&lt;/p&gt; &lt;h1&gt;The Dilemma&lt;/h1&gt; &lt;p&gt;If the CPT phase is trained almost entirely on sequences packed to a max length of &lt;strong&gt;256 tokens&lt;/strong&gt;, I worry this will fundamentally bias the model towards short, social media-style outputs, making it incapable of generating long, multi-paragraph factual answers needed for the final QA task.&lt;/p&gt; &lt;h1&gt;Proposed Solution (Seeking Review)&lt;/h1&gt; &lt;p&gt;I believe the fix lies in separating the two training phases:&lt;/p&gt; &lt;h1&gt;Phase 1: Continued Pre-Training (CPT) - Efficiency Focus&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Inject local dialect fluency and domain facts (via blended modern standard arabic data).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Method:&lt;/strong&gt; &lt;strong&gt;Data Concatenation/Packing.&lt;/strong&gt; I will concatenate multiple short posts, separated by &lt;code&gt;&amp;lt;eos&amp;gt;&lt;/code&gt;, into sequences of exactly &lt;strong&gt;256 tokens&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rationale:&lt;/strong&gt; This ensures &lt;strong&gt;maximum efficiency&lt;/strong&gt; and uses every single one of my 71M tokens effectively. Since CPT's goal is weight adjustment (vocabulary/grammar), the short sequence length is acceptable here.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Phase 2: Instruction Tuning (IT) - Context and Length Focus&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Teach the model &lt;em&gt;how&lt;/em&gt; to use the knowledge and &lt;em&gt;how&lt;/em&gt; to respond with long, structured answers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Method 1 (Data):&lt;/strong&gt; Generate &lt;strong&gt;synthetic multi-turn conversations&lt;/strong&gt; where the &lt;strong&gt;desired responses are intentionally long&lt;/strong&gt; (300-500 tokens). Crucially, these conversations must use the &lt;strong&gt;Target dialect&lt;/strong&gt; (learned in CPT) for fluency.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Method 2 (Context Window):&lt;/strong&gt; For the IT phase, I will increase the &lt;code&gt;max_seq_length&lt;/code&gt; to &lt;strong&gt;4,096&lt;/strong&gt; (or perhaps 8,192, depending on my GPU memory). This allows the model to see, process, and learn from long, complex conversational histories and detailed factual prompts.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Core Question&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Does CPT at a short max length (256) negatively impact the model's ability to generate long sequences if the subsequent Instruction Tuning is performed with a much larger context window (4096) and long target responses?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I want to confirm that the short-context CPT won't permanently bottleneck the model's long-form generative capacity, which should be inherent from its original pre-training.&lt;/p&gt; &lt;p&gt;Any feedback on this two-phase strategy or common pitfalls to avoid when transitioning between sequence lengths would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FishermanNo2017"&gt; /u/FishermanNo2017 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbalp3/llm_finetuning_cpt_on_71m_short_dialectal_tokens/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbalp3/llm_finetuning_cpt_on_71m_short_dialectal_tokens/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbalp3/llm_finetuning_cpt_on_71m_short_dialectal_tokens/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T11:40:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbiwfn</id>
    <title>WSL2 + Ollama +localhost access issue</title>
    <updated>2025-12-01T17:27:33+00:00</updated>
    <author>
      <name>/u/RedZedingg</name>
      <uri>https://old.reddit.com/user/RedZedingg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I’m new to coding (started like a week ago) and I’m struggling to get Ollama running on Windows 10 with WSL2. Here’s my situation:&lt;/p&gt; &lt;p&gt;- I installed WSL2 with Ubuntu and Ollama inside it.&lt;/p&gt; &lt;p&gt;- Ollama installs fine and says the API is available at &lt;a href="http://127.0.0.1:11434"&gt;127.0.0.1:11434&lt;/a&gt; inside WSL.&lt;/p&gt; &lt;p&gt;- When I try to access localhost:11434 from Windows (chrome), the browser can’t connect (ERR_CONNECTION_REFUSED).&lt;/p&gt; &lt;p&gt;- I’ve tried killing any processes using the port, deleting ~/.ollama, and even reinstalling Ollama.&lt;/p&gt; &lt;p&gt;- I’m aware that WSL1 shares localhost with Windows, but Ollama refuses to install there. WSL2 works for installation but Windows can’t reach it directly.&lt;/p&gt; &lt;p&gt;- I’ve also tried IP of WSL2, port forwarding (netsh), and other tunnels, but nothing seems to reliably expose Ollama to Windows.&lt;/p&gt; &lt;p&gt;Basically, I can’t get Ollama inside WSL2 to be accessible from Windows, and I’m stuck. Any advice from someone who got this working would be amazing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedZedingg"&gt; /u/RedZedingg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbiwfn/wsl2_ollama_localhost_access_issue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbiwfn/wsl2_ollama_localhost_access_issue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbiwfn/wsl2_ollama_localhost_access_issue/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T17:27:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbq6av</id>
    <title>How to move from Ollama + Page Assist to Alpaca?</title>
    <updated>2025-12-01T21:57:11+00:00</updated>
    <author>
      <name>/u/TrableZ</name>
      <uri>https://old.reddit.com/user/TrableZ</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sup. I'm on Arch Linux. I've installed Ollama-Cuda (arch repositories) and use Page Assist (Firefox extension) as its frontend. Saw Alpaca on Flathub and wanted to give it a try. It couldn't find my locally installed files of Ollama and the Deepseek models I got, so I had to install another Ollama package from Flathub specifically made for alpaca. Through which I had to install my LLM models again as well. Tinkering around with Alpaca on Flatseal didn't do it either. Is there any way for Alpaca to just locate my actual locally installed files of Ollama without me having to additionally install it over Flathub?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TrableZ"&gt; /u/TrableZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbq6av/how_to_move_from_ollama_page_assist_to_alpaca/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbq6av/how_to_move_from_ollama_page_assist_to_alpaca/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbq6av/how_to_move_from_ollama_page_assist_to_alpaca/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T21:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbqmvv</id>
    <title>New to LocalLlama – whats the best model for medical documentation / text generation? (RTX 5090 + 64GB RAM)</title>
    <updated>2025-12-01T22:14:49+00:00</updated>
    <author>
      <name>/u/xchris1337xy</name>
      <uri>https://old.reddit.com/user/xchris1337xy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xchris1337xy"&gt; /u/xchris1337xy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pbqmoi/new_to_localllama_whats_the_best_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbqmvv/new_to_localllama_whats_the_best_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbqmvv/new_to_localllama_whats_the_best_model_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T22:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbeesy</id>
    <title>We built a **3B local Git agent** that turns plain English into correct git commands — matches GPT-OSS 120B accuracy (gitara)</title>
    <updated>2025-12-01T14:38:30+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pbeesy/we_built_a_3b_local_git_agent_that_turns_plain/"&gt; &lt;img alt="We built a **3B local Git agent** that turns plain English into correct git commands — matches GPT-OSS 120B accuracy (gitara)" src="https://preview.redd.it/s72uc6fusl4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55e186276e8f039add893c488cace0821219c3ce" title="We built a **3B local Git agent** that turns plain English into correct git commands — matches GPT-OSS 120B accuracy (gitara)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s72uc6fusl4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbeesy/we_built_a_3b_local_git_agent_that_turns_plain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbeesy/we_built_a_3b_local_git_agent_that_turns_plain/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T14:38:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbdmq3</id>
    <title>Ollama Not Using GPU (RTX 3070) — Only CPU — Need Help Enabling CUDA Acceleration</title>
    <updated>2025-12-01T14:05:46+00:00</updated>
    <author>
      <name>/u/huza786</name>
      <uri>https://old.reddit.com/user/huza786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to use OLAMA models (DeepSeek R1(5gb) and QWEN2.5:1.5b(1gb) Coder) locally in VS Code through the CLINE and &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; extensions so I can get a Cursor-like AI coding workflow. The models run, but OLAMA only uses my CPU and completely ignores my GPU (RTX 3070, 8GB VRAM). My system also has a Ryzen 5 5600X CPU. I expected OLAMA to use CUDA for acceleration, but it doesn’t seem to detect or utilize the GPU at all. Is this a limitation of OLAMA, a configuration issue, or something I’ve set up incorrectly? Any advice on getting GPU support working would be appreciated.&lt;/p&gt; &lt;p&gt;nvidia-smi&lt;/p&gt; &lt;p&gt;Mon Dec 1 19:00:45 2025 &lt;/p&gt; &lt;p&gt;+-----------------------------------------------------------------------------------------+&lt;/p&gt; &lt;p&gt;| NVIDIA-SMI 581.57 Driver Version: 581.57 CUDA Version: 13.0 |&lt;/p&gt; &lt;p&gt;+-----------------------------------------+------------------------+----------------------+&lt;/p&gt; &lt;p&gt;| GPU Name Driver-Model | Bus-Id Disp.A | Volatile Uncorr. ECC |&lt;/p&gt; &lt;p&gt;| Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |&lt;/p&gt; &lt;p&gt;| | | MIG M. |&lt;/p&gt; &lt;p&gt;|=========================================+========================+======================|&lt;/p&gt; &lt;p&gt;| 0 NVIDIA GeForce RTX 3070 WDDM | 00000000:05:00.0 On | N/A |&lt;/p&gt; &lt;p&gt;| 0% 35C P8 24W / 270W | 1627MiB / 8192MiB | 7% Default |&lt;/p&gt; &lt;p&gt;| | | N/A |&lt;/p&gt; &lt;p&gt;+-----------------------------------------+------------------------+----------------------+&lt;/p&gt; &lt;p&gt;+-----------------------------------------------------------------------------------------+&lt;/p&gt; &lt;p&gt;| Processes: |&lt;/p&gt; &lt;p&gt;| GPU GI CI PID Type Process name GPU Memory |&lt;/p&gt; &lt;p&gt;| ID ID Usage |&lt;/p&gt; &lt;p&gt;|=========================================================================================|&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 2228 C+G ....0.3595.94\msedgewebview2.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 3844 C+G ...8bbwe\PhoneExperienceHost.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 4100 C+G ...indows\System32\ShellHost.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 7580 C+G ...y\StartMenuExperienceHost.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 7756 C+G F:\Microsoft VS Code\Code.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 8228 C+G ...5n1h2txyewy\TextInputHost.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 11164 C+G ...2txyewy\CrossDeviceResume.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 12464 C+G ...ntrolPanel\SystemSettings.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 13332 C+G ...xyewy\ShellExperienceHost.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 14160 C+G ...em32\ApplicationFrameHost.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 14460 C+G ....0.3595.94\msedgewebview2.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 15884 C+G ..._cw5n1h2txyewy\SearchHost.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 17164 C+G ...s\Mozilla Firefox\firefox.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 17992 C+G ...4__cv1g1gvanyjgm\WhatsApp.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 18956 C+G C:\Windows\explorer.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 19076 C+G ...lare WARP\Cloudflare WARP.exe N/A |&lt;/p&gt; &lt;p&gt;| 0 N/A N/A 22612 C+G ...s\Mozilla Firefox\firefox.exe N/A |&lt;/p&gt; &lt;p&gt;+-----------------------------------------------------------------------------------------+&lt;/p&gt; &lt;p&gt;nvcc --version&lt;/p&gt; &lt;p&gt;nvcc: NVIDIA (R) Cuda compiler driver&lt;/p&gt; &lt;p&gt;Copyright (c) 2005-2025 NVIDIA Corporation&lt;/p&gt; &lt;p&gt;Built on Wed_Apr__9_19:29:17_Pacific_Daylight_Time_2025&lt;/p&gt; &lt;p&gt;Cuda compilation tools, release 12.9, V12.9.41&lt;/p&gt; &lt;p&gt;Build cuda_12.9.r12.9/compiler.35813241_0&lt;/p&gt; &lt;p&gt;ollama ps&lt;/p&gt; &lt;p&gt;NAME ID SIZE PROCESSOR CONTEXT UNTIL &lt;/p&gt; &lt;p&gt;qwen2.5-coder:1.5b d7372fd82851 1.9 GB 100% CPU 32768 Stopping...&lt;/p&gt; &lt;p&gt;I’m trying to use Ollama models locally in VS Code through the &lt;strong&gt;Cline&lt;/strong&gt; and &lt;a href="http://Continue.dev"&gt;&lt;strong&gt;Continue.dev&lt;/strong&gt;&lt;/a&gt; extensions to get something similar to Cursor’s AI-assisted coding workflow. The models work, but &lt;strong&gt;Ollama only uses my CPU and completely ignores my GPU&lt;/strong&gt;, even though I have an RTX 3070 with 8GB VRAM. I expected CUDA acceleration to kick in, but it looks like Ollama isn’t detecting or using the GPU at all.&lt;/p&gt; &lt;h1&gt;My setup:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Ryzen 5 5600X&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; NVIDIA GeForce RTX 3070 (8GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Drivers:&lt;/strong&gt; NVIDIA 581.57&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CUDA:&lt;/strong&gt; Installed (nvcc 12.9)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Models I’m running:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;DeepSeek R1 (~5GB)&lt;/li&gt; &lt;li&gt;Qwen2.5-Coder 1.5B (~1GB)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Run Ollama models locally with GPU acceleration inside VS Code (Cline / Continue.dev)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;Ollama is &lt;em&gt;only&lt;/em&gt; using the CPU:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL qwen2.5-coder:1.5b d7372fd82851 1.9 GB 100% CPU 32768 Stopping... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There is &lt;strong&gt;no GPU usage at all&lt;/strong&gt; when models load or run.&lt;/p&gt; &lt;h1&gt;NVIDIA-SMI Output&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;+-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 581.57 Driver Version: 581.57 CUDA Version: 13.0 | | GPU Name Driver-Model | Bus-Id Disp.A | Volatile Uncorr. ECC | | 0 NVIDIA GeForce RTX 3070 WDDM | Memory-Usage: 1627MiB / 8192MiB | Util: 7% | +-----------------------------------------------------------------------------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No Ollama process appears in the GPU process list.&lt;/p&gt; &lt;h1&gt;nvcc --version&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Cuda compilation tools, release 12.9, V12.9.41 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So CUDA toolkit is installed and working.&lt;/p&gt; &lt;h1&gt;What I Want to Know&lt;/h1&gt; &lt;p&gt;Is this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A &lt;strong&gt;known limitation&lt;/strong&gt; of Ollama on Windows?&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;config issue&lt;/strong&gt; (env vars, WSL2, driver mode, etc.)?&lt;/li&gt; &lt;li&gt;Something I set up incorrectly?&lt;/li&gt; &lt;li&gt;Or do some models not support GPU on Windows yet?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any advice on getting Ollama to actually use the GPU (especially for VS Code integrations) would be super appreciated.I’m trying to use Ollama models locally in VS Code through the Cline and &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; extensions to get something similar to Cursor’s AI-assisted coding workflow. The models work, but Ollama only uses my CPU and completely ignores my GPU, even though I have an RTX 3070 with 8GB VRAM. I expected CUDA acceleration to kick in, but it looks like Ollama isn’t detecting or using the GPU at all.&lt;br /&gt; My setup: &lt;/p&gt; &lt;p&gt;CPU: Ryzen 5 5600X &lt;/p&gt; &lt;p&gt;GPU: NVIDIA GeForce RTX 3070 (8GB VRAM) &lt;/p&gt; &lt;p&gt;Drivers: NVIDIA 581.57 &lt;/p&gt; &lt;p&gt;CUDA: Installed (nvcc 12.9) &lt;/p&gt; &lt;p&gt;Models I’m running: &lt;/p&gt; &lt;p&gt;DeepSeek R1 (~5GB) &lt;/p&gt; &lt;p&gt;Qwen2.5-Coder 1.5B (~1GB) &lt;/p&gt; &lt;p&gt;Goal: Run Ollama models locally with GPU acceleration inside VS Code (Cline / Continue.dev) &lt;/p&gt; &lt;p&gt;The Problem&lt;br /&gt; Ollama is only using the CPU:&lt;br /&gt; ollama ps&lt;br /&gt; NAME ID SIZE PROCESSOR CONTEXT UNTIL&lt;br /&gt; qwen2.5-coder:1.5b d7372fd82851 1.9 GB 100% CPU 32768 Stopping... &lt;/p&gt; &lt;p&gt;There is no GPU usage at all when models load or run. &lt;/p&gt; &lt;p&gt;NVIDIA-SMI Output&lt;br /&gt; +-----------------------------------------------------------------------------------------+&lt;br /&gt; | NVIDIA-SMI 581.57 Driver Version: 581.57 CUDA Version: 13.0 |&lt;br /&gt; | GPU Name Driver-Model | Bus-Id Disp.A | Volatile Uncorr. ECC |&lt;br /&gt; | 0 NVIDIA GeForce RTX 3070 WDDM | Memory-Usage: 1627MiB / 8192MiB | Util: 7% |&lt;br /&gt; +-----------------------------------------------------------------------------------------+ &lt;/p&gt; &lt;p&gt;No Ollama process appears in the GPU process list. &lt;/p&gt; &lt;p&gt;nvcc --version&lt;br /&gt; Cuda compilation tools, release 12.9, V12.9.41 &lt;/p&gt; &lt;p&gt;So CUDA toolkit is installed and working. &lt;/p&gt; &lt;p&gt;What I Want to Know&lt;br /&gt; Is this: &lt;/p&gt; &lt;p&gt;A known limitation of Ollama on Windows? &lt;/p&gt; &lt;p&gt;A config issue (env vars, WSL2, driver mode, etc.)? &lt;/p&gt; &lt;p&gt;Something I set up incorrectly? &lt;/p&gt; &lt;p&gt;Or do some models not support GPU on Windows yet? &lt;/p&gt; &lt;p&gt;Any advice on getting Ollama to actually use the GPU (especially for VS Code integrations) would be super appreciated.&lt;/p&gt; &lt;p&gt;I’m trying to use Ollama models locally in VS Code through the Cline and &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; extensions to get something similar to Cursor’s AI-assisted coding workflow. The models work, but Ollama only uses my CPU and completely ignores my GPU, even though I have an RTX 3070 with 8GB VRAM. I expected CUDA acceleration to kick in, but it looks like Ollama isn’t detecting or using the GPU at all.&lt;/p&gt; &lt;h1&gt;My setup:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;CPU: Ryzen 5 5600X&lt;/li&gt; &lt;li&gt;GPU: NVIDIA GeForce RTX 3070 (8GB VRAM)&lt;/li&gt; &lt;li&gt;Drivers: NVIDIA 581.57&lt;/li&gt; &lt;li&gt;CUDA: Installed (nvcc 12.9)&lt;/li&gt; &lt;li&gt;Models I’m running: &lt;ul&gt; &lt;li&gt;DeepSeek R1 (~5GB)&lt;/li&gt; &lt;li&gt;Qwen2.5-Coder 1.5B (~1GB)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Goal: Run Ollama models locally with GPU acceleration inside VS Code (Cline / Continue.dev)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;Ollama is only using the CPU:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL qwen2.5-coder:1.5b d7372fd82851 1.9 GB 100% CPU 32768 Stopping... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There is no GPU usage at all when models load or run.&lt;/p&gt; &lt;h1&gt;NVIDIA-SMI Output&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;+-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 581.57 Driver Version: 581.57 CUDA Version: 13.0 | | GPU Name Driver-Model | Bus-Id Disp.A | Volatile Uncorr. ECC | | 0 NVIDIA GeForce RTX 3070 WDDM | Memory-Usage: 1627MiB / 8192MiB | Util: 7% | +-----------------------------------------------------------------------------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No Ollama process appears in the GPU process list.&lt;/p&gt; &lt;h1&gt;nvcc --version&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Cuda compilation tools, release 12.9, V12.9.41 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So CUDA toolkit is installed and working.&lt;/p&gt; &lt;h1&gt;What I Want to Know&lt;/h1&gt; &lt;p&gt;Is this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A known limitation of Ollama on Windows?&lt;/li&gt; &lt;li&gt;A config issue (env vars, WSL2, driver mode, etc.)?&lt;/li&gt; &lt;li&gt;Something I set up incorrectly?&lt;/li&gt; &lt;li&gt;Or do some models not support GPU on Windows yet?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any advice on getting Ollama to actually use the GPU (especially for VS Code integrations) would be super appreciated.I’m trying to use Ollama models locally in VS Code through the Cline and &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; extensions to get something similar to Cursor’s AI-assisted coding workflow. The models work, but Ollama only uses my CPU and completely ignores my GPU, even though I have an RTX 3070 with 8GB VRAM. I expected CUDA acceleration to kick in, but it looks like Ollama isn’t detecting or using the GPU at all.&lt;br /&gt; My setup: &lt;/p&gt; &lt;p&gt;CPU: Ryzen 5 5600X &lt;/p&gt; &lt;p&gt;GPU: NVIDIA GeForce RTX 3070 (8GB VRAM) &lt;/p&gt; &lt;p&gt;Drivers: NVIDIA 581.57 &lt;/p&gt; &lt;p&gt;CUDA: Installed (nvcc 12.9) &lt;/p&gt; &lt;p&gt;Models I’m running: &lt;/p&gt; &lt;p&gt;DeepSeek R1 (~5GB) &lt;/p&gt; &lt;p&gt;Qwen2.5-Coder 1.5B (~1GB) &lt;/p&gt; &lt;p&gt;Goal: Run Ollama models locally with GPU acceleration inside VS Code (Cline / Continue.dev) &lt;/p&gt; &lt;p&gt;The Problem&lt;br /&gt; Ollama is only using the CPU:&lt;br /&gt; ollama ps&lt;br /&gt; NAME ID SIZE PROCESSOR CONTEXT UNTIL&lt;br /&gt; qwen2.5-coder:1.5b d7372fd82851 1.9 GB 100% CPU 32768 Stopping... &lt;/p&gt; &lt;p&gt;There is no GPU usage at all when models load or run. &lt;/p&gt; &lt;p&gt;NVIDIA-SMI Output&lt;br /&gt; +-----------------------------------------------------------------------------------------+&lt;br /&gt; | NVIDIA-SMI 581.57 Driver Version: 581.57 CUDA Version: 13.0 |&lt;br /&gt; | GPU Name Driver-Model | Bus-Id Disp.A | Volatile Uncorr. ECC |&lt;br /&gt; | 0 NVIDIA GeForce RTX 3070 WDDM | Memory-Usage: 1627MiB / 8192MiB | Util: 7% |&lt;br /&gt; +-----------------------------------------------------------------------------------------+ &lt;/p&gt; &lt;p&gt;No Ollama process appears in the GPU process list. &lt;/p&gt; &lt;p&gt;nvcc --version&lt;br /&gt; Cuda compilation tools, release 12.9, V12.9.41 &lt;/p&gt; &lt;p&gt;So CUDA toolkit is installed and working. &lt;/p&gt; &lt;p&gt;What I Want to Know&lt;br /&gt; Is this: &lt;/p&gt; &lt;p&gt;A known limitation of Ollama on Windows? &lt;/p&gt; &lt;p&gt;A config issue (env vars, WSL2, driver mode, etc.)? &lt;/p&gt; &lt;p&gt;Something I set up incorrectly? &lt;/p&gt; &lt;p&gt;Or do some models not support GPU on Windows yet? &lt;/p&gt; &lt;p&gt;Any advice on getting Ollama to actually use the GPU (especially for VS Code integrations) would be super appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/huza786"&gt; /u/huza786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbdmq3/ollama_not_using_gpu_rtx_3070_only_cpu_need_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbdmq3/ollama_not_using_gpu_rtx_3070_only_cpu_need_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbdmq3/ollama_not_using_gpu_rtx_3070_only_cpu_need_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T14:05:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbpuwm</id>
    <title>Qwen3 Vision is a *fantastic* local model for Home Assistant (with one fix)</title>
    <updated>2025-12-01T21:44:49+00:00</updated>
    <author>
      <name>/u/eXntrc</name>
      <uri>https://old.reddit.com/user/eXntrc</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eXntrc"&gt; /u/eXntrc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/homeassistant/comments/1pbpub7/qwen3_vision_is_a_fantastic_local_model_for_ha/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbpuwm/qwen3_vision_is_a_fantastic_local_model_for_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbpuwm/qwen3_vision_is_a_fantastic_local_model_for_home/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T21:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbbht5</id>
    <title>AI Agent from scratch: Django + Ollama + Pydantic AI - A Step-by-Step Guide</title>
    <updated>2025-12-01T12:28:31+00:00</updated>
    <author>
      <name>/u/tom-mart</name>
      <uri>https://old.reddit.com/user/tom-mart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey-up Reddit. I’m excited to share my latest project with you, a detailed, step-by-step guide on building a basic AI agent using Django, Ollama, and Pydantic AI.&lt;/p&gt; &lt;p&gt;I’ve broken down the entire process, making it accessible even if you’re just starting with Python. In the first part I'll show you how to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Set up a Django project with Django Ninja for rapid API development.&lt;/li&gt; &lt;li&gt;Integrate your local Ollama engine.&lt;/li&gt; &lt;li&gt;Use Pydantic AI to manage your agent’s context and tool calls.&lt;/li&gt; &lt;li&gt;Build a functional AI agent in just a few lines of code!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is a great starting point for anyone wanting to experiment with local LLMs and build their own AI agents from scratch.&lt;/p&gt; &lt;p&gt;Read the full article &lt;a href="https://medium.com/@tom.mart/build-self-hosted-ai-agent-with-ollama-pydantic-ai-and-django-ninja-53c6b3f14a1d"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In the next part I'll be diving into memory management – giving your agent the ability to remember past conversations and interactions.&lt;/p&gt; &lt;p&gt;Looking forward to your comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tom-mart"&gt; /u/tom-mart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbbht5/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbbht5/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbbht5/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T12:28:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbsy1n</id>
    <title>New to ollama</title>
    <updated>2025-12-01T23:49:55+00:00</updated>
    <author>
      <name>/u/Leading_Jury_6868</name>
      <uri>https://old.reddit.com/user/Leading_Jury_6868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everybody I’m new to the ollama world and in just want to know about hardware requirements for running a local a.I model. The thing is that I don’t know what I need something like a bot to help with my python coding. Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leading_Jury_6868"&gt; /u/Leading_Jury_6868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbsy1n/new_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pbsy1n/new_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pbsy1n/new_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-01T23:49:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc61qv</id>
    <title>Anyone tried a local model that can get UI element coordinates from a screenshot?</title>
    <updated>2025-12-02T11:25:43+00:00</updated>
    <author>
      <name>/u/Efficient_Weight3313</name>
      <uri>https://old.reddit.com/user/Efficient_Weight3313</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Has anyone here tried a &lt;strong&gt;local model&lt;/strong&gt; where you upload a &lt;strong&gt;UI screenshot&lt;/strong&gt; (like a dashboard or app UI) and the model can return the &lt;strong&gt;coordinates/bounding box&lt;/strong&gt; of elements such as &lt;em&gt;Login&lt;/em&gt;, &lt;em&gt;Signup&lt;/em&gt;, buttons, inputs, etc.?&lt;/p&gt; &lt;p&gt;Just want to know if anyone in the community has experimented with this.&lt;br /&gt; Any model name or experience would help.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Efficient_Weight3313"&gt; /u/Efficient_Weight3313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pc61qv/anyone_tried_a_local_model_that_can_get_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pc61qv/anyone_tried_a_local_model_that_can_get_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pc61qv/anyone_tried_a_local_model_that_can_get_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-02T11:25:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcdil0</id>
    <title>Working on structured task planning for Nanocoder - helping smaller local models tackle bigger tasks</title>
    <updated>2025-12-02T16:49:45+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/nanocoder/comments/1pcdifq/working_on_structured_task_planning_for_nanocoder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcdil0/working_on_structured_task_planning_for_nanocoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pcdil0/working_on_structured_task_planning_for_nanocoder/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-02T16:49:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc6087</id>
    <title>Built a local MCP Hub + Memory Engine for Ollama — looking for testers</title>
    <updated>2025-12-02T11:23:24+00:00</updated>
    <author>
      <name>/u/danny_094</name>
      <uri>https://old.reddit.com/user/danny_094</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey zusammen&lt;/p&gt; &lt;p&gt;Ich habe die letzten Monate an einem lokalen AI-Projekt gebastelt, das irgendwie „etwas größer“ geworden ist als geplant – und jetzt brauche ich Beta-Tester, damit es nicht nur auf meinem Server läuft. &lt;/p&gt; &lt;p&gt;Ich habe das project schon vor ein paar Tagen vorgestellt. Ich will nicht spammen. Aber ich denke es ist effektiver explizit noch mal einen Beitrag zu eröffnen um Tester zu suchen. &lt;/p&gt; &lt;p&gt;Was es ist:&lt;/p&gt; &lt;p&gt;➡️ Ein MCP-Hub, der mehrere AI-Tools / MCP-Server bündelt ➡️ Ein SQL-Memory-Server mit VectorStore + Graph + Layering (STM/MTM/LTM) ➡️ Ein AI-Gateway, das LobeChat / OpenWebUI / AnythingLLM mit Ollama + Tools verbinden kann ➡️ Mehrere kleine Module (Sequential Thinking, Validator-Service usw.)&lt;/p&gt; &lt;p&gt;Alles ist Dockerized, komplett lokal, ohne externen Cloud-Kram.&lt;/p&gt; &lt;p&gt;Was es kann: • echte Erinnerungen speichern • Facts extrahieren • Wissen als Graph verknüpfen • MCP-Tools automatisch routen • Multi-LLM Orchestrierung • Meta-Decision Layer • Plugins / Server beliebig erweitern&lt;/p&gt; &lt;p&gt;Repo: GitHub &lt;a href="https://github.com/danny094/ai-proxybridge"&gt;https://github.com/danny094/ai-proxybridge&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Was ich brauche: • Leute, die testen, ob der Stack auf anderen Geräten sauber läuft • Feedback über: • Memory-Qualität • Geschwindigkeit • MCP-Stabilität • Ideen für neue Tools • Setup-Probleme • Optional: Bugreports + Logs&lt;/p&gt; &lt;p&gt;Für wen ist das was? • Selbsthoster • Devs • Leute mit Ollama • LLM-Nerds • Menschen die Spaß haben am Rumprobieren&lt;/p&gt; &lt;p&gt;Wenn jemand Bock hat: Einfach antworten oder eine DM schreiben. Freue mich über jedes Feedback – das Projekt soll 100% open &amp;amp; kostenlos bleiben.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danny_094"&gt; /u/danny_094 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pc6087/built_a_local_mcp_hub_memory_engine_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pc6087/built_a_local_mcp_hub_memory_engine_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pc6087/built_a_local_mcp_hub_memory_engine_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-02T11:23:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcm31k</id>
    <title>fine tuning/doing prototype on ML model on mac and then testing it - How?</title>
    <updated>2025-12-02T22:06:01+00:00</updated>
    <author>
      <name>/u/Chachachaudhary123</name>
      <uri>https://old.reddit.com/user/Chachachaudhary123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to be able to do as a Data Scientist do some prototyping/eval for a particular Ml use case on my Mac(with large unified memory). What tools and ecosystems are available to do this effectively? Once I complete the prototype/eval, then I would deploy it on Nvidia GPU machines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chachachaudhary123"&gt; /u/Chachachaudhary123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcm31k/fine_tuningdoing_prototype_on_ml_model_on_mac_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcm31k/fine_tuningdoing_prototype_on_ml_model_on_mac_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pcm31k/fine_tuningdoing_prototype_on_ml_model_on_mac_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-02T22:06:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcvpd9</id>
    <title>What user mostly want in AI</title>
    <updated>2025-12-03T05:21:42+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rhe wqnt small models 7B-32B models? What really we need. The question is not we want…must be we need…&lt;/p&gt; &lt;p&gt;WE NEED MEDIUM MODELS 128B-256B MOE MODELS.&lt;/p&gt; &lt;p&gt;Enthusiasts come to buy and invest in ram memory and try the industry to freely liberate medium size models!! &lt;/p&gt; &lt;p&gt;We must start to try thinking more in pass the next step…adapt our hardware to medium models and forget small models!!!&lt;/p&gt; &lt;p&gt;Medium models are afordable and makes the difference with small models.&lt;/p&gt; &lt;p&gt;Industry!!! Liberate more medium moe models 128B-256B we need it!!!&lt;/p&gt; &lt;p&gt;Qwen 235 is a example…but we need more…more at 128GB ram affordable and more at 256GB ram affordable.&lt;/p&gt; &lt;p&gt;Quality models we need!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcvpd9/what_user_mostly_want_in_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcvpd9/what_user_mostly_want_in_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pcvpd9/what_user_mostly_want_in_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T05:21:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc0nd4</id>
    <title>I made a friendlier UI to manage ollama models</title>
    <updated>2025-12-02T05:49:22+00:00</updated>
    <author>
      <name>/u/ComfyTightwad</name>
      <uri>https://old.reddit.com/user/ComfyTightwad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pc0nd4/i_made_a_friendlier_ui_to_manage_ollama_models/"&gt; &lt;img alt="I made a friendlier UI to manage ollama models" src="https://external-preview.redd.it/OWgwaTNkMXhicTRnMfUiUzAnmNREsHt8dOXHrmeg5B6ZHRDEfM5KWhHunQFO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eae295535409f30cb5a15fe3c38d2c850491f516" title="I made a friendlier UI to manage ollama models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;OllaMan&lt;/strong&gt;(Ollama Manager) is a visual management interface for Ollama, with the following main features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Manage models on multiple remote or local Ollama servers simultaneously, with support for Basic Auth security authentication.&lt;/li&gt; &lt;li&gt;Built-in model marketplace for one-click online model installation, saying goodbye to command-line operations.&lt;/li&gt; &lt;li&gt;View currently running models and unload them with a single click to free up memory.&lt;/li&gt; &lt;li&gt;Chat functionality to test model performance.&lt;/li&gt; &lt;li&gt;Cross-platform support: MacOS, Windows, Linux&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComfyTightwad"&gt; /u/ComfyTightwad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/usrybf0xbq4g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pc0nd4/i_made_a_friendlier_ui_to_manage_ollama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pc0nd4/i_made_a_friendlier_ui_to_manage_ollama_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-02T05:49:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcyqd3</id>
    <title>Is your local model safe from "Emoji Smuggling"? Visual demo of Prompt Injection</title>
    <updated>2025-12-03T08:21:07+00:00</updated>
    <author>
      <name>/u/jokiruiz</name>
      <uri>https://old.reddit.com/user/jokiruiz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We all love Ollama because it makes running local, private models incredibly easy. But I've been testing how these open weights models handle Prompt Injection and Logic Hacking.&lt;/p&gt; &lt;p&gt;I made a video demonstrating how attackers can use techniques like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Emoji Smuggling: Hiding malicious instructions inside tokens that look innocent (like emojis) but trigger specific behaviors in the model.&lt;/li&gt; &lt;li&gt;Roleplay Attacks: Bypassing safety filters just by using the classic &amp;quot;Grandma exploit&amp;quot; or logic games (demonstrated with the Gandalf game).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even if Ollama runs locally and offline, if you connect it to a UI that scrapes the web or reads external text, your model might be vulnerable to these &amp;quot;invisible&amp;quot; commands.&lt;/p&gt; &lt;p&gt;- Video Link: &lt;a href="https://youtu.be/Kck8JxHmDOs?si=yp5sjBq_d2hh5QU3"&gt;https://youtu.be/Kck8JxHmDOs?si=yp5sjBq_d2hh5QU3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For the Ollama users: Have you found any robust System Prompts (Modelfiles) that effectively block these types of &amp;quot;jailbreaks&amp;quot;? Or is Llama 3 still too easily convinced to break character?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jokiruiz"&gt; /u/jokiruiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcyqd3/is_your_local_model_safe_from_emoji_smuggling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcyqd3/is_your_local_model_safe_from_emoji_smuggling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pcyqd3/is_your_local_model_safe_from_emoji_smuggling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T08:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcw61m</id>
    <title>Necesitamos que la industria de la IA libere modelos MEDIANOS</title>
    <updated>2025-12-03T05:46:36+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;La industria de la IA esta permanentemente liberando modelos pequeños y muy pequeños con muy poco conocimiento…el conocimiento es necesario para hacer tareas y resolver problemas de la vida real…por eso las empresas de IA rara vez liberando sus modelos medianos…Liberan los grandes porque saben que la comunidad opensource no tiene el hardware necesario para ejecutarlos y liberan los pequeños porque en la mayoria de los casos estan bien para ser probados y buscar implementar su tecnologia…pero porque no liberan modelos medianos en torno a 128B y 256B que suele ser la cantidad de ram que tienen los servidores obsoletos que se venden en ebay y un buen afficionado puede invertir en ello para trabajar con modelos medios con tecnologia moe.Pues simplemente porque a la industria no le interesa que nos adueñemos de esos modelos porque marcan la diferencia y tienen dentro conocimiento como para empezar a ser utiles para resolver ciertos problemas…Qwen se atrevio a liberar su 235B pero deepseek lo hizo con el 671B (imposible para un usuario medio de ejecutar) La industria deberia empezar a pensar que los entusiastas e investigadores y amateurs que le estan cogiendo el gusto y han convertido la IA en su hobbie estan dispuestos a gastar en maquinas con ram hasta 256 gigas y no tienen modelos moe con los que experimentar.Modelos con mucho conocimiento pero con muchos modelos expertos moe pequeños de forma que puedan ser usados con mucha ram y una sola grafica…y ahi ya empezamos a marcar la diferencia de un modelo para trastear o jugar o uno para realmente trabajar…y hacer cosas realmente interesantes.Hay que animar a las empresas a dar el paso de liberar modelos MOE medianos!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcw61m/necesitamos_que_la_industria_de_la_ia_libere/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcw61m/necesitamos_que_la_industria_de_la_ia_libere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pcw61m/necesitamos_que_la_industria_de_la_ia_libere/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T05:46:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcsp6c</id>
    <title>ministral-3 and mistral-large-3</title>
    <updated>2025-12-03T02:53:07+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ollama.com/library/ministral-3"&gt;https://ollama.com/library/ministral-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ministral-3: The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/mistral-large-3"&gt;https://ollama.com/library/mistral-large-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mistral-Large-3: A general-purpose multimodal mixture-of-experts model for production-grade tasks and enterprise workloads.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This models requires Ollama 0.13.1&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcsp6c/ministral3_and_mistrallarge3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pcsp6c/ministral3_and_mistrallarge3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pcsp6c/ministral3_and_mistrallarge3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-03T02:53:07+00:00</published>
  </entry>
</feed>
