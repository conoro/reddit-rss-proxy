<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-23T04:07:33+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1rakz0o</id>
    <title>Best structure and models for invoice data extraction</title>
    <updated>2026-02-21T07:30:28+00:00</updated>
    <author>
      <name>/u/Fickle-Bluebird-367</name>
      <uri>https://old.reddit.com/user/Fickle-Bluebird-367</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to build an invoice processing tool that can extract data such as supplier, total, ,vat net amount etc from pdf invoices with a range of layouts and styles. So far I have been using pytesseract to perform OCR then feeding the result into a local LLM using Ollama. with this I can get to about 85-90% accuracy but want to improve. can anyone suggest an improvement structure (for example skipping the OCR and passing the invoice straight into a multimodal model?) or which models are best for this task. I am currently running locally on a mac with 8gb RAM but could run on another set up with 16gb. &lt;/p&gt; &lt;p&gt;Would be great to get any tips from anyone who has worked on something similar. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fickle-Bluebird-367"&gt; /u/Fickle-Bluebird-367 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rakz0o/best_structure_and_models_for_invoice_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rakz0o/best_structure_and_models_for_invoice_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rakz0o/best_structure_and_models_for_invoice_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T07:30:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ra9wsv</id>
    <title>my portable ollama now has persistent memory</title>
    <updated>2026-02-20T22:39:02+00:00</updated>
    <author>
      <name>/u/VaguneBob</name>
      <uri>https://old.reddit.com/user/VaguneBob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ra9wsv/my_portable_ollama_now_has_persistent_memory/"&gt; &lt;img alt="my portable ollama now has persistent memory" src="https://preview.redd.it/0mwkg27n7qkg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=2125242ea136b89e636be8e0c012c543a9a1c392" title="my portable ollama now has persistent memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This instance of ollama you can just copy onto any machine without installing anything (no docker needed), everything ollama needs is contained within a folder, and all the memories can be reset with one click. This is work in progress.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;F:\MyOllama\ ‚îú‚îÄ‚îÄ Start-Ollama-Dark-Memory.bat ‚îú‚îÄ‚îÄ ollama-dark-memory-final.html ‚îú‚îÄ‚îÄ ollama.exe ‚îú‚îÄ‚îÄ ollama-portable.bat ‚îú‚îÄ‚îÄ models\ ‚îî‚îÄ‚îÄ memory\ ‚îú‚îÄ‚îÄ memory_server.py ‚îî‚îÄ‚îÄ memories.json &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VaguneBob"&gt; /u/VaguneBob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ra9wsv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ra9wsv/my_portable_ollama_now_has_persistent_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ra9wsv/my_portable_ollama_now_has_persistent_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T22:39:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1raui9d</id>
    <title>Computron 9000: The Autonomous AI That Redefines Digital Intelligence</title>
    <updated>2026-02-21T15:52:22+00:00</updated>
    <author>
      <name>/u/larz01larz</name>
      <uri>https://old.reddit.com/user/larz01larz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1raui9d/computron_9000_the_autonomous_ai_that_redefines/"&gt; &lt;img alt="Computron 9000: The Autonomous AI That Redefines Digital Intelligence" src="https://external-preview.redd.it/fztGDgUm_rlUEpoDco3Z49Bt8hogEjaDCtScZ3bcYJA.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ffeae09c0c1e9e0083d73139c266df20d43357ff" title="Computron 9000: The Autonomous AI That Redefines Digital Intelligence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/lefoulkrod/computron_9000"&gt;https://github.com/lefoulkrod/computron_9000&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/larz01larz"&gt; /u/larz01larz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=BG1px6cmwp4&amp;amp;si=d2sOr9Yr0mHSf4gU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1raui9d/computron_9000_the_autonomous_ai_that_redefines/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1raui9d/computron_9000_the_autonomous_ai_that_redefines/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T15:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rauy6b</id>
    <title>70B llm on 4gb android phone !</title>
    <updated>2026-02-21T16:09:44+00:00</updated>
    <author>
      <name>/u/Vast_Lingonberry7259</name>
      <uri>https://old.reddit.com/user/Vast_Lingonberry7259</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rauy6b/70b_llm_on_4gb_android_phone/"&gt; &lt;img alt="70B llm on 4gb android phone !" src="https://external-preview.redd.it/oy08tsZOgcbAmrxT3DWi6J-5B2GiEpwBGCRypU_3Wpg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c755c3a24868f6032710875f987082a1b3f71993" title="70B llm on 4gb android phone !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An 32b q4km on 4gb ram based on virtualization.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Lingonberry7259"&gt; /u/Vast_Lingonberry7259 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/78l5uaosfvkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rauy6b/70b_llm_on_4gb_android_phone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rauy6b/70b_llm_on_4gb_android_phone/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T16:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb95yc</id>
    <title>Got roasted a few weeks ago for "not knowing how to code." Here's what I was actually building....</title>
    <updated>2026-02-22T01:53:56+00:00</updated>
    <author>
      <name>/u/Decent-Freedom5374</name>
      <uri>https://old.reddit.com/user/Decent-Freedom5374</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rb95yc/got_roasted_a_few_weeks_ago_for_not_knowing_how/"&gt; &lt;img alt="Got roasted a few weeks ago for &amp;quot;not knowing how to code.&amp;quot; Here's what I was actually building...." src="https://preview.redd.it/mkktb226bykg1.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=a7a16515a6a16113936847ae7374b686f87bb4a3" title="Got roasted a few weeks ago for &amp;quot;not knowing how to code.&amp;quot; Here's what I was actually building...." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Decent-Freedom5374"&gt; /u/Decent-Freedom5374 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rb95yc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb95yc/got_roasted_a_few_weeks_ago_for_not_knowing_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb95yc/got_roasted_a_few_weeks_ago_for_not_knowing_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T01:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ravwue</id>
    <title>Help with hardware for OpenClaw/Ollama combo</title>
    <updated>2026-02-21T16:47:30+00:00</updated>
    <author>
      <name>/u/sp0okymuffin</name>
      <uri>https://old.reddit.com/user/sp0okymuffin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello - hope I‚Äôm posting this in the appropriate place. &lt;/p&gt; &lt;p&gt;I‚Äôm reasonably far down an agentic rabbit hole with OpenClaw running on an Proxmox VM and am concluding it‚Äôs time to invest in a set up that can scale and provide me with utility for at least a year. I also want to feed the beast more sensitive information, where I‚Äôd love to do local processing.&lt;/p&gt; &lt;p&gt;My plan is to buy a Mac Mini, where OpenClaw would run and have more power including desktop interaction. I‚Äôm also thinking I‚Äôd get a Mac Studio to serve as my primary PC, on which I‚Äôd love to run a beefy local LLM with good performance for sensitive document processing (think bank statements, business financials, etc.). &lt;/p&gt; &lt;p&gt;I envisage OpenClaw using a combination of the cloud LLMs (primarily Claude) and the local LLM when told to, and for heartbeats, etc. &lt;/p&gt; &lt;p&gt;I‚Äôm trying to gauge what the appropriate horsepower is to throw at this setup. Juggling between M4 16/24GB on the Mac Mini and perhaps even all the way up to 256GB unified memory on the Mac Studio. &lt;/p&gt; &lt;p&gt;But I‚Äôm also wondering if this is overkill; I am not a coder or engineer, and while I‚Äôm an experienced self hoster, I‚Äôm new to Ollama. I‚Äòd be very grateful for some pointers here ‚Äî e.g. Would I be just as well served getting an M4 Pro Mac Mini with 64GB memory for my use case? LLM would then run on the Mac Mini alongside OpenClaw and I‚Äôd hold off getting a primary PC upgrade for a while (and save some money!)&lt;/p&gt; &lt;p&gt;I‚Äôd also like to do text to speech and give my OpenClaw agent a voice. I‚Äôd love to process this locally with some push-to-talk wifi mics that can connect to speakers via AirPlay. speech should be transcribed locally and then prompts could be processed with a cloud provider if needed, just as long as the voice itself doesn‚Äôt get sent to Sam Altman‚Äôs beast (figuratively speaking) &lt;/p&gt; &lt;p&gt;I do care about reasoning models and make quite extensive use of ChatGPT 5.2 and Opus 4.6. &lt;/p&gt; &lt;p&gt;Any guidance much appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sp0okymuffin"&gt; /u/sp0okymuffin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ravwue/help_with_hardware_for_openclawollama_combo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ravwue/help_with_hardware_for_openclawollama_combo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ravwue/help_with_hardware_for_openclawollama_combo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T16:47:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1rawjod</id>
    <title>Ollama will not bind 11434 to any IPv4 interface. Works fine for localhost and IPv6. This is driving me *nuts*</title>
    <updated>2026-02-21T17:11:21+00:00</updated>
    <author>
      <name>/u/Big_Wave9732</name>
      <uri>https://old.reddit.com/user/Big_Wave9732</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rawjod/ollama_will_not_bind_11434_to_any_ipv4_interface/"&gt; &lt;img alt="Ollama will not bind 11434 to any IPv4 interface. Works fine for localhost and IPv6. This is driving me *nuts*" src="https://preview.redd.it/1kek8e77rvkg1.png?width=140&amp;amp;height=27&amp;amp;auto=webp&amp;amp;s=b1bc48b44cc6557c1d6ba76b4c8eb212f7f4a7da" title="Ollama will not bind 11434 to any IPv4 interface. Works fine for localhost and IPv6. This is driving me *nuts*" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit 3: I'm fuckng dumb. I thought the firewall service was disabled because I was able to ssh in remotely. It wasn't.&lt;/p&gt; &lt;p&gt;systemctl stop firewalld.service&lt;br /&gt; systemctl disable firewalld.service&lt;/p&gt; &lt;p&gt;------&lt;/p&gt; &lt;p&gt;Newly installed RHEL 9.7.&lt;/p&gt; &lt;p&gt;I had Ollama working on this machine previously before reinstalling. Now, however, Ollama absolutely will not listen on IPv4.&lt;/p&gt; &lt;p&gt;Attached is the systemctl file, a netstat showing the service listening on IPv6, the output from journalctl -u ollama --no-pager --follow, and the output in /var/log/messages when the service is started.&lt;/p&gt; &lt;p&gt;Anyone? What the hell is up here??&lt;/p&gt; &lt;p&gt;Edit: I have tried the line &amp;quot;Environment=&amp;quot;OLLAMA_HOST=0.0.0.0&amp;quot; with and without port 11434. Didn't work either way.&lt;/p&gt; &lt;p&gt;Things I have also tried:&lt;br /&gt; Selinux is disabled;&lt;br /&gt; Have completely uninstalled and removed all remnants and config files;&lt;br /&gt; Have disabled IPv6 everywhere that I can find it (kernel, OS, deleted the IPv6 interface);&lt;/p&gt; &lt;p&gt;Edit 2:&lt;br /&gt; &amp;quot;OLLAMA_HOST=&lt;a href="http://0.0.0.0:11434"&gt;http://0.0.0.0:11434&lt;/a&gt; ollama serve&amp;quot; gets the same result.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big_Wave9732"&gt; /u/Big_Wave9732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rawjod"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rawjod/ollama_will_not_bind_11434_to_any_ipv4_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rawjod/ollama_will_not_bind_11434_to_any_ipv4_interface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T17:11:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1razv53</id>
    <title>Version Mismatch help</title>
    <updated>2026-02-21T19:18:03+00:00</updated>
    <author>
      <name>/u/UpYourQuality</name>
      <uri>https://old.reddit.com/user/UpYourQuality</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;Not sure whats going on but the only ollama.exe i have is showing that its the wrong version.&lt;/p&gt; &lt;p&gt;I'm using the most recent installer direct from ollama.com. Even tried the winget version and same error.&lt;/p&gt; &lt;p&gt;On Windows 11&lt;/p&gt; &lt;p&gt;No ollama services or task anywhere&lt;/p&gt; &lt;p&gt;Deleted all ollama folders.&lt;/p&gt; &lt;p&gt;Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UpYourQuality"&gt; /u/UpYourQuality &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1razv53/version_mismatch_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1razv53/version_mismatch_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1razv53/version_mismatch_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T19:18:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rara7u</id>
    <title>Faster &amp; Cheaper LLM Apps with Semantic Caching</title>
    <updated>2026-02-21T13:36:10+00:00</updated>
    <author>
      <name>/u/Special_Community179</name>
      <uri>https://old.reddit.com/user/Special_Community179</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rara7u/faster_cheaper_llm_apps_with_semantic_caching/"&gt; &lt;img alt="Faster &amp;amp; Cheaper LLM Apps with Semantic Caching" src="https://external-preview.redd.it/oOZJdgmTkHw77V31kL7N1jm08j8e9Y-FJAqFULVQOvI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f8d267e8b0a6fddff7d4ecde829bd01253f232f" title="Faster &amp;amp; Cheaper LLM Apps with Semantic Caching" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special_Community179"&gt; /u/Special_Community179 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/NrqvtsnjIHU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rara7u/faster_cheaper_llm_apps_with_semantic_caching/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rara7u/faster_cheaper_llm_apps_with_semantic_caching/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T13:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1raw6z8</id>
    <title>qwen3-coder-next on desktop</title>
    <updated>2026-02-21T16:58:15+00:00</updated>
    <author>
      <name>/u/BobcatLegitimate1497</name>
      <uri>https://old.reddit.com/user/BobcatLegitimate1497</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"&gt; &lt;img alt="qwen3-coder-next on desktop" src="https://preview.redd.it/2t8c4wugnvkg1.png?width=140&amp;amp;height=112&amp;amp;auto=webp&amp;amp;s=b4c92b7d5d4f3747b4fa2068f7da52d3878b28fa" title="qwen3-coder-next on desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Old computer with low-end GPUs (RTX 4060 Ti 16 Gb and 1660 6 Gb)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2t8c4wugnvkg1.png?width=1202&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f426423c92b99569186820894b7fa3a3557b5d82"&gt;https://preview.redd.it/2t8c4wugnvkg1.png?width=1202&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f426423c92b99569186820894b7fa3a3557b5d82&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CPU half-loaded, GPU loaded about 20%. Are there any ways to optimize it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BobcatLegitimate1497"&gt; /u/BobcatLegitimate1497 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T16:58:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb5t3c</id>
    <title>I have a substantial codebase that I want to analyse and build a proof-of-concept around for demonstration purposes</title>
    <updated>2026-02-21T23:22:44+00:00</updated>
    <author>
      <name>/u/eufemiapiccio77</name>
      <uri>https://old.reddit.com/user/eufemiapiccio77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;which local LLM options would allow me to work without the usage restrictions imposed by mainstream hosted providers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eufemiapiccio77"&gt; /u/eufemiapiccio77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb5t3c/i_have_a_substantial_codebase_that_i_want_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb5t3c/i_have_a_substantial_codebase_that_i_want_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb5t3c/i_have_a_substantial_codebase_that_i_want_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T23:22:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1rarsuo</id>
    <title>TIFU/PSA: didn‚Äôt check which GPU ollama was using and was stuck wondering why so slow</title>
    <updated>2026-02-21T13:59:43+00:00</updated>
    <author>
      <name>/u/IAmANobodyAMA</name>
      <uri>https://old.reddit.com/user/IAmANobodyAMA</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure where to tell this story (megathread?) but it made me laugh and has a teachable moment so I thought I would share.&lt;/p&gt; &lt;p&gt;TL;DR: I was running ollama on an old GPU by mistake ü§¶‚Äç‚ôÇÔ∏è&lt;/p&gt; &lt;p&gt;I have two local machines running ollama. My gaming rig has a 5070ti 16gb but isn‚Äôt always on, and my ‚Äúdedicated‚Äù unraid server **had** a 3060ti 8gb that was going to be my AI workhorse.&lt;/p&gt; &lt;p&gt;I chose models that would kick ass on the 5070 when online and more conservative models for the 3060 otherwise.&lt;/p&gt; &lt;p&gt;This is all still experimental/for learning so this setup is fines for me ‚Ä¶ except the unraid server was painfully slow. Took me way too long to figure out that‚Äôs because it was hitting an old GTX 1650 4gb card!! I forgot I swapped out the cards because I was going to build a gaming rig for my kid with the 3060.&lt;/p&gt; &lt;p&gt;I spent way too long researching models and trying to figure out why my ‚Äú3060‚Äù was offloading over 50% of qwen3:4b to my CPU. Since this is hosted on unraid I was convinced that another service (plex?) was using my GPU without permission. Nope, I‚Äôm just a doofus.&lt;/p&gt; &lt;p&gt;It wasn‚Äôt until running `nvidia-smi` in terminal that I realized my error.&lt;/p&gt; &lt;p&gt;Anyways, hope this makes someone chuckle as much as me. Anyone else have some fun ‚Äúdoh‚Äù moments?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IAmANobodyAMA"&gt; /u/IAmANobodyAMA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T13:59:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb8q6p</id>
    <title>Your Model Doesn't Know What It's About to Do. Mine Does. Introducing The Cradle. (V1.0)</title>
    <updated>2026-02-22T01:33:25+00:00</updated>
    <author>
      <name>/u/BiscottiDisastrous19</name>
      <uri>https://old.reddit.com/user/BiscottiDisastrous19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rb8q6p/your_model_doesnt_know_what_its_about_to_do_mine/"&gt; &lt;img alt="Your Model Doesn't Know What It's About to Do. Mine Does. Introducing The Cradle. (V1.0)" src="https://preview.redd.it/jeeinozfmwkg1.png?width=140&amp;amp;height=58&amp;amp;auto=webp&amp;amp;s=0bb84b45e3850cd11ca230d83904e5ff34b94573" title="Your Model Doesn't Know What It's About to Do. Mine Does. Introducing The Cradle. (V1.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BiscottiDisastrous19"&gt; /u/BiscottiDisastrous19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/u_BiscottiDisastrous19/comments/1rb12vw/your_model_doesnt_know_what_its_about_to_do_mine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb8q6p/your_model_doesnt_know_what_its_about_to_do_mine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb8q6p/your_model_doesnt_know_what_its_about_to_do_mine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T01:33:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb1wzf</id>
    <title>RX 9060 XT 16GB</title>
    <updated>2026-02-21T20:41:26+00:00</updated>
    <author>
      <name>/u/ButterscotchTop4598</name>
      <uri>https://old.reddit.com/user/ButterscotchTop4598</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Leute,&lt;/p&gt; &lt;p&gt;Gibt es irgendwo eine Anleitung f√ºr die Verwendung der oben genannten Grafikkarte unter Olama? Leider finde ich dazu nichts. Zudem w√ºrde mich interessieren welches Modell ihr f√ºr die Grafikkarte empfehlen k√∂nnt.&lt;/p&gt; &lt;p&gt;Vielen Dank f√ºr eure Hilfe!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ButterscotchTop4598"&gt; /u/ButterscotchTop4598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb1wzf/rx_9060_xt_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb1wzf/rx_9060_xt_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb1wzf/rx_9060_xt_16gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T20:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb8w1n</id>
    <title>NPU for local models?</title>
    <updated>2026-02-22T01:40:59+00:00</updated>
    <author>
      <name>/u/MemeGLS</name>
      <uri>https://old.reddit.com/user/MemeGLS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm running Qwen 3 on my laptop which has an NPU but I have no idea on how to use it (I was hoping that I could use a bigger model if I‚Äôm able to find a way to use the NPU).&lt;/p&gt; &lt;p&gt;Thanks a lot &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MemeGLS"&gt; /u/MemeGLS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb8w1n/npu_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb8w1n/npu_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb8w1n/npu_for_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T01:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbha7b</id>
    <title>Unable to pull model from ollama</title>
    <updated>2026-02-22T09:12:51+00:00</updated>
    <author>
      <name>/u/badasssravikumae</name>
      <uri>https://old.reddit.com/user/badasssravikumae</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to pull models from ollama but I am unable to do &lt;/p&gt; &lt;p&gt;I did ollama serve&lt;br /&gt; I deleted the cache and checked if the models is available and tried pulling the model even though I see this error: &lt;/p&gt; &lt;p&gt;pulling manifest &lt;/p&gt; &lt;p&gt;Error: pull model manifest: file does not exist&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badasssravikumae"&gt; /u/badasssravikumae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T09:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbdq0m</id>
    <title>Agent architectures for SLMs</title>
    <updated>2026-02-22T05:43:51+00:00</updated>
    <author>
      <name>/u/PangolinPossible7674</name>
      <uri>https://old.reddit.com/user/PangolinPossible7674</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;What kind of agent architectures are generally used with Small Language Models? In the past, I had tried ReAct with some 8B param models, and they failed. Recently, I have been trying out tool calling models via Ollama. Even with function calling, Qwen 3 8B appears to somewhat work, but some other 8B models don't seem to be so great.&lt;/p&gt; &lt;p&gt;Therefore, I was wondering what SLM-agent gas worked for the others. Does verbose docstrings for tools affect performance with SLMs? Alternatively, what smallest model size generally allows diverse tool usage in a reliable way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PangolinPossible7674"&gt; /u/PangolinPossible7674 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T05:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc0r8o</id>
    <title>Got $800 of credits on a cloud platform (for GPU usage). Anyone here that's into AI training and inference and could make use of it?</title>
    <updated>2026-02-22T23:09:21+00:00</updated>
    <author>
      <name>/u/DocumentFun9077</name>
      <uri>https://old.reddit.com/user/DocumentFun9077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have around 800 bucks worth of GPU usage credits on one of the major platform, those can be used specifically for GPU and clusters. So if any individual or hobbyist or anyone out here is training models or inference, or anything else, please contact! (not free btw, but selling at way less price)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocumentFun9077"&gt; /u/DocumentFun9077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0r8o/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0r8o/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc0r8o/got_800_of_credits_on_a_cloud_platform_for_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T23:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc0sqn</id>
    <title>Got $800 of credits on a cloud platform (for GPU usage). Anyone here that's into AI training and inference and could make use of it?</title>
    <updated>2026-02-22T23:11:05+00:00</updated>
    <author>
      <name>/u/DocumentFun9077</name>
      <uri>https://old.reddit.com/user/DocumentFun9077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have around 800 bucks worth of GPU usage credits on one of the major platform, those can be used specifically for GPU and clusters. So if any individual or hobbyist or anyone out here is training models or inference, or anything else, please contact! (not free btw, but selling at way less price)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocumentFun9077"&gt; /u/DocumentFun9077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0sqn/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0sqn/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc0sqn/got_800_of_credits_on_a_cloud_platform_for_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T23:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1rajqj6</id>
    <title>15,000+ tok/s on ChatJimmy: Is the "Model-on-Silicon" era finally starting?</title>
    <updated>2026-02-21T06:19:08+00:00</updated>
    <author>
      <name>/u/Significant-Topic433</name>
      <uri>https://old.reddit.com/user/Significant-Topic433</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"&gt; &lt;img alt="15,000+ tok/s on ChatJimmy: Is the &amp;quot;Model-on-Silicon&amp;quot; era finally starting?" src="https://preview.redd.it/bq69s0n5jskg1.jpg?width=140&amp;amp;height=66&amp;amp;auto=webp&amp;amp;s=fa3f690c9b529f18075dc6e27d8b984f7fcc4fcd" title="15,000+ tok/s on ChatJimmy: Is the &amp;quot;Model-on-Silicon&amp;quot; era finally starting?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôve been discussing local inference for years, but chatjimmy.ai just moved the goalposts. They are hitting 15,414 tokens per second using what they call &amp;quot;mask ROM recall fabric&amp;quot;‚Äîbasically etching the model weights directly into the silicon logic.&lt;/p&gt; &lt;p&gt;‚ÄãThis is a massive shift from our current setups. We‚Äôre used to general-purpose compute, but this is a dedicated ASIC. No HBM, no VRAM bottlenecks, just raw, hardcoded inference.&lt;/p&gt; &lt;p&gt;‚Äã I just invested in two Gigabyte AI TOP ATOM units (the ones based on the NVIDIA Spark / Grace Blackwell architecture). They are absolute beasts for training and fine-tuning with 128GB of unified memory, but seeing a dedicated chip do 15k tok/s makes me wonder: &lt;/p&gt; &lt;p&gt;‚ÄãDid I make the right call with the AI TOP Spark units for local dev, or are we going to see these specialized ASIC cards hit the market soon and make general-purpose desktop AI look like dial-up?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant-Topic433"&gt; /u/Significant-Topic433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rajqj6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T06:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbt4di</id>
    <title>Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation</title>
    <updated>2026-02-22T18:17:48+00:00</updated>
    <author>
      <name>/u/dorbeats</name>
      <uri>https://old.reddit.com/user/dorbeats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"&gt; &lt;img alt="Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation" src="https://preview.redd.it/vj71i95883lg1.jpg?width=140&amp;amp;height=140&amp;amp;auto=webp&amp;amp;s=253fed1c03e9709c1d506fd05f1a1a3300ef8eda" title="Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this feasible on modest hardware?&lt;/p&gt; &lt;p&gt;Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.boomspot.com/llama-3-1-70b-on-rtx-3090-bypassing-cpu-for-ai-innovation"&gt;https://www.boomspot.com/llama-3-1-70b-on-rtx-3090-bypassing-cpu-for-ai-innovation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vj71i95883lg1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c0859a6b570010f0108ab4369475820f191095ed"&gt;https://preview.redd.it/vj71i95883lg1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c0859a6b570010f0108ab4369475820f191095ed&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dorbeats"&gt; /u/dorbeats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T18:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc3fkr</id>
    <title>Model requires more system memory (eventhough I have enough)</title>
    <updated>2026-02-23T01:03:23+00:00</updated>
    <author>
      <name>/u/gfejer</name>
      <uri>https://old.reddit.com/user/gfejer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have two Tesla P40s passed through as vGPU profiles to a Ubuntu 24.04 VM. As an example I can run gpt-oss just fine and the GPUs get recognized by the system, but when it comes to running Llama 3.3 which is a 43GB model (my 2*24GB VRAM should be enough right?) gives me an error that I don‚Äôt have enough system memory.&lt;/p&gt; &lt;p&gt;I am guessing that for some reason it tries to run the model on the CPU, but I don‚Äôt understand why‚Ä¶ Are there any possible fixes for this issue?&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gfejer"&gt; /u/gfejer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:03:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbq3dy</id>
    <title>Ollama for Dummies</title>
    <updated>2026-02-22T16:24:37+00:00</updated>
    <author>
      <name>/u/catbutchie</name>
      <uri>https://old.reddit.com/user/catbutchie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone needs to write a book. Right now my mentor is ChatGPT. There are so many parameters i just tell it my issue and it tells me what to change. Some small tweaks are significant performance adjustment. I‚Äôm new obviously. I‚Äôd like to know why I‚Äôm doing what I‚Äôm doing so I can be more in control. I‚Äôm using silly tavern and ollama for self hosted chat. Suggestions needed. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/catbutchie"&gt; /u/catbutchie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T16:24:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc4l9j</id>
    <title>16GB VRAM for mode agent</title>
    <updated>2026-02-23T01:56:50+00:00</updated>
    <author>
      <name>/u/ColdTransition5828</name>
      <uri>https://old.reddit.com/user/ColdTransition5828</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was hoping to enjoy something similar to Cursor on my PC. I even bought what was supposed to be a mid-range card. But the results are disappointing.&lt;/p&gt; &lt;p&gt;After studying it, I realized I'm missing the core agent and a better Ollama model that accepts tools. But honestly, I'm bored. What do you recommend I do to get the most out of local models with my 16GB of VRAM?&lt;/p&gt; &lt;p&gt;I mostly do full-track coding and Java.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ColdTransition5828"&gt; /u/ColdTransition5828 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:56:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc3srb</id>
    <title>I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review</title>
    <updated>2026-02-23T01:19:58+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"&gt; &lt;img alt="I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review" src="https://external-preview.redd.it/anBvOHE4ZWxiNWxnMYmmpi9UU3yP9yrC87ePDCyv5Mn4iZk_AUHCQZq2TOQ_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=452fe5fc9cf576221ea71aff1d15b07c8fa36f35" title="I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all ‚Äî been experimenting with running a local AI agent using ClawBot + Ollama and wanted to share what actually happened.&lt;/p&gt; &lt;p&gt;Link to full tutorial: &lt;a href="https://www.youtube.com/watch?v=FxyQkj95VXs"&gt;https://www.youtube.com/watch?v=FxyQkj95VXs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yes, ClawBot + Ollama works on Mac. Does it work well? Depends on what you mean by &amp;quot;work&amp;quot;&lt;/li&gt; &lt;li&gt;With an 8B model, agentic tasks are limited. Basic Q&amp;amp;A? Fine. Anything complex? It'll humble you real quick&lt;/li&gt; &lt;li&gt;Should you expect ChatGPT-level speed? Absolutely not. Go make a coffee while you wait üòÖ&lt;/li&gt; &lt;li&gt;Is it worth it for learning the stack and experimenting locally for free? Honestly yes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What worked&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Setup is cleaner than expected - VS Code, JSON config, localhost dashboard, done. I have no luck setting up ollama using their onboarding. So...I went straight to config file.&lt;/li&gt; &lt;li&gt;Ollama model switching is straightforward once you understand the config structure&lt;/li&gt; &lt;li&gt;Great for understanding how local AI agent setups actually work under the hood&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What didn't&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Speed is rough on anything under 32GB RAM&lt;/li&gt; &lt;li&gt;8B models hit their ceiling fast on multi-step reasoning and real agentic workflows. Keep context window low.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1ee663elb5lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:19:58+00:00</published>
  </entry>
</feed>
