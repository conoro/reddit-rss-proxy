<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-12T23:06:47+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ou02ob</id>
    <title>Please reply ASAP I really need your thoughts on this</title>
    <updated>2025-11-11T04:25:05+00:00</updated>
    <author>
      <name>/u/Acceptable-Buyer-184</name>
      <uri>https://old.reddit.com/user/Acceptable-Buyer-184</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can I run ollama qwen3-vl: 8b on my local computer? it will be use for ai generation from materials like summary, quiz, and flashcards? Or should i stick to 4b?&lt;/p&gt; &lt;p&gt;PC Specs:&lt;br /&gt; Ryzen 7 5700x&lt;br /&gt; RTX 5060 8GB&lt;br /&gt; 16GB RAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable-Buyer-184"&gt; /u/Acceptable-Buyer-184 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou02ob/please_reply_asap_i_really_need_your_thoughts_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou02ob/please_reply_asap_i_really_need_your_thoughts_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ou02ob/please_reply_asap_i_really_need_your_thoughts_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T04:25:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot3pp8</id>
    <title>Granite 4 micro-h doing great on my older pc</title>
    <updated>2025-11-10T03:47:18+00:00</updated>
    <author>
      <name>/u/newz2000</name>
      <uri>https://old.reddit.com/user/newz2000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"&gt; &lt;img alt="Granite 4 micro-h doing great on my older pc" src="https://a.thumbs.redditmedia.com/bbtxjoQlGAnti-gN1X_64JqMNQSAnCRZif4YFLrMqj0.jpg" title="Granite 4 micro-h doing great on my older pc" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/nars9rownc0g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2d27ce0136bd0fc08a26e32900ff809de4c39dd"&gt;https://preview.redd.it/nars9rownc0g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2d27ce0136bd0fc08a26e32900ff809de4c39dd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My older 7th Gen i5 gaming computer has been repurposed into my local llm workhorse for most of this year. I use it to automate tasks. In this example, extract key dates and information from an email producing the results in JSON format. &lt;/p&gt; &lt;p&gt;I have been using Qwen 3 and Gemma 3 and I'd say if I want to have a conversation, Qwen 3:8b is my favorite. But it's not good at instruction following. Gemma 3:4b really does great all around and is very quick on this computer. But for instruction following, Granite 4 micro-h is tough to beat.&lt;/p&gt; &lt;p&gt;I have not yet tested it with tool calling, but this is something I want to do and is what made me check out Granite.&lt;/p&gt; &lt;p&gt;Since you can kinda see my prompt through the translucent window, I'll save you the effort and put it in here. &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;You are an assistant that extracts litigation-relevant structured data from incoming court notices that arrive via email or plaintext.&lt;/p&gt; &lt;p&gt;Read the following email or document VERY CAREFULLY.&lt;/p&gt; &lt;p&gt;Then output ONLY JSON.&lt;/p&gt; &lt;p&gt;Do not summarize.&lt;/p&gt; &lt;p&gt;Do not infer beyond what is explicitly written.&lt;/p&gt; &lt;p&gt;If a field cannot be determined, return null ‚Äî do NOT guess.&lt;/p&gt; &lt;p&gt;You must return ONLY this exact JSON structure (no explanation):&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;case_name&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;case_number&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;court&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;hearing_date&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;hearing_time&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;presiding_judge&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;filing_date_of_order&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;required_filing_deadlines&amp;quot;: [],&lt;/p&gt; &lt;p&gt;&amp;quot;parties_involved&amp;quot;: [],&lt;/p&gt; &lt;p&gt;&amp;quot;topic_or_subject_matter&amp;quot;: &amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;Return ONLY a JSON object with EXACTLY these keys:&lt;/p&gt; &lt;p&gt;case_name, case_number, court, hearing_date, hearing_time, presiding_judge,&lt;/p&gt; &lt;p&gt;filing_date_of_order, required_filing_deadlines, parties_involved, topic_or_subject_matter.&lt;/p&gt; &lt;p&gt;If a value is unknown, set null. Do NOT add any other keys or sections.&lt;/p&gt; &lt;p&gt;Dates = YYYY-MM-DD. Times = 24-hour local-to-court (e.g., 13:30).&lt;/p&gt; &lt;p&gt;Include ONLY human names in `parties_involved` (no emails). Remove HTML entities.&lt;/p&gt; &lt;p&gt;Rules:&lt;/p&gt; &lt;p&gt;hearing_date and hearing_time must be extracted if a hearing is set.&lt;/p&gt; &lt;p&gt;required_filing_deadlines must list ONLY dates that represent ‚Äúsomething is due‚Äù by ‚Äúa date certain.‚Äù&lt;/p&gt; &lt;p&gt;parties_involved should list all names referenced as parties, attorneys, or counsel receiving service.&lt;/p&gt; &lt;p&gt;The topic_or_subject_matter is a single short clause describing WHAT the order is about (motion type, hearing type, etc).&lt;/p&gt; &lt;p&gt;Dates must be formatted YYYY-MM-DD.&lt;/p&gt; &lt;p&gt;Times must be formatted 24 hour format, local to the court when stated (CST ‚Üí convert to 24h).&lt;/p&gt; &lt;p&gt;DO NOT return anything not inside the JSON block.&lt;/p&gt; &lt;p&gt;EMAIL:&lt;/p&gt; &lt;p&gt;‚Ä¶&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newz2000"&gt; /u/newz2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T03:47:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot3u7f</id>
    <title>Speculative decoding: Faster inference for local LLMs over the network?</title>
    <updated>2025-11-10T03:53:54+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ot3u7f/speculative_decoding_faster_inference_for_local/"&gt; &lt;img alt="Speculative decoding: Faster inference for local LLMs over the network?" src="https://preview.redd.it/70p6li1poc0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6daaaeff166a74d20a883ce88ad7a5a9b3feaf6" title="Speculative decoding: Faster inference for local LLMs over the network?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am gearing up for a big release to add support for speculative decoding for LLMs and looking for early feedback.&lt;/p&gt; &lt;p&gt;First a bit of context, speculative decoding is a technique whereby a draft model (usually a smaller LLM) is engaged to produce tokens and the candidate set produced is verified by a target model (usually a larger model). The set of candidate tokens produced by a draft model must be verifiable via logits by the target model. While tokens produced are serial, verification can happen in parallel which can lead to significant improvements in speed.&lt;/p&gt; &lt;p&gt;This is what OpenAI uses to accelerate the speed of its responses especially in cases where outputs can be guaranteed to come from the same distribution, where:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;propose(x, k) ‚Üí œÑ # Draft model proposes k tokens based on context x verify(x, œÑ) ‚Üí m # Target verifies œÑ, returns accepted count m continue_from(x) # If diverged, resume from x with target model &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So I am thinking of adding support to &lt;a href="https://github.com/katanemo/archgw"&gt;arch&lt;/a&gt; (a models-native sidecar proxy for agents). And the developer experience could be something along the following lines:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;POST /v1/chat/completions { &amp;quot;model&amp;quot;: &amp;quot;target:gpt-large@2025-06&amp;quot;, &amp;quot;speculative&amp;quot;: { &amp;quot;draft_model&amp;quot;: &amp;quot;draft:small@v3&amp;quot;, &amp;quot;max_draft_window&amp;quot;: 8, &amp;quot;min_accept_run&amp;quot;: 2, &amp;quot;verify_logprobs&amp;quot;: false }, &amp;quot;messages&amp;quot;: [...], &amp;quot;stream&amp;quot;: true } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here the max_draft_window is the number of tokens to verify, the max_accept_run tells us after how many failed verifications should we give up and just send all the remaining traffic to the target model etc. Of course this work assumes a low RTT between the target and draft model so that speculative decoding is faster without compromising quality.&lt;/p&gt; &lt;p&gt;Question: how would you feel about this functionality? Could you see it being useful for your LLM-based applications? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/70p6li1poc0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ot3u7f/speculative_decoding_faster_inference_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ot3u7f/speculative_decoding_faster_inference_for_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T03:53:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ota5ou</id>
    <title>GLM-4.6-REAP any good for coding? Min VRAM+RAM?</title>
    <updated>2025-11-10T10:17:23+00:00</updated>
    <author>
      <name>/u/WaitformeBumblebee</name>
      <uri>https://old.reddit.com/user/WaitformeBumblebee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using mostly QWEN3 variants (&amp;lt;20GB) for python coding tasks. Would 16GB VRAM + 64GB RAM be able to &amp;quot;run&amp;quot; (I don't mind waiting some minutes if the answer is much better) 72GB model like &lt;a href="https://ollama.com/MichelRosselli/GLM-4.6-REAP-218B-A32B-FP8-mixed-AutoRound"&gt;https://ollama.com/MichelRosselli/GLM-4.6-REAP-218B-A32B-FP8-mixed-AutoRound&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and how good is it? Been hearing high praise for GLM-4.5-AIR, but don't want to download &amp;gt;70GB for nothing. Perhaps I'd be better of with GLM-4.5-Air:Q2_K at 45GB ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WaitformeBumblebee"&gt; /u/WaitformeBumblebee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ota5ou/glm46reap_any_good_for_coding_min_vramram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ota5ou/glm46reap_any_good_for_coding_min_vramram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ota5ou/glm46reap_any_good_for_coding_min_vramram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T10:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1otzvk3</id>
    <title>Built a Terminal-Based AI Chatbot Powered by Ollama Cloud</title>
    <updated>2025-11-11T04:15:17+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I‚Äôve been experimenting with &lt;strong&gt;Ollama&lt;/strong&gt; and just released a &lt;strong&gt;terminal-based AI Chatbot&lt;/strong&gt; that runs completely on your local machine ‚Äî &lt;strong&gt;no browser, no cloud dependency&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;üí¨ I‚Äôd love your help testing it out and sharing feedback (UX, response flow, model handling, or new feature ideas).&lt;/p&gt; &lt;p&gt;üì¶ &lt;strong&gt;Download the latest release:&lt;/strong&gt;&lt;br /&gt; üëâ &lt;a href="https://github.com/yasniy97/aichatbot/releases/tag/ollama"&gt;Releases URL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what the Ollama community thinks ‚Äî how it performs on your setup, any compatibility notes, or prompts you find interesting! üôå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otzvk3/built_a_terminalbased_ai_chatbot_powered_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otzvk3/built_a_terminalbased_ai_chatbot_powered_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otzvk3/built_a_terminalbased_ai_chatbot_powered_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T04:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1otbtp2</id>
    <title>Ollama-powered open source single-stock analysis tool with Python, including ratios/news analysis/LSTM forecast</title>
    <updated>2025-11-10T11:56:09+00:00</updated>
    <author>
      <name>/u/gruquilla</name>
      <uri>https://old.reddit.com/user/gruquilla</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otbtp2/ollamapowered_open_source_singlestock_analysis/"&gt; &lt;img alt="Ollama-powered open source single-stock analysis tool with Python, including ratios/news analysis/LSTM forecast" src="https://preview.redd.it/9hn8znm45f0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22404009b637970ab1e3ab5561cb65759c99e1c7" title="Ollama-powered open source single-stock analysis tool with Python, including ratios/news analysis/LSTM forecast" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good morning everyone,&lt;/p&gt; &lt;p&gt;I am currently a MSc Fintech student at Aston University (Birmingham, UK) and Audencia Business School (Nantes, France). Alongside my studies, I've started to develop a few personal Python projects.&lt;/p&gt; &lt;p&gt;My first big open-source project: A single-stock analysis tool that uses both market and financial statements informations. It also integrates news sentiment analysis (FinBert and Pygooglenews), as well as LSTM forecast for the stock price. You can also enable Ollama to get information complements using a local LLM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What my project (FinAPy) does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prologue: Ticker input collection and essential functions and data: &lt;em&gt;In this part, the program gets in input a ticker from the user, and asks wether or not he wants to enable the AI analysis. Then, it generates a short summary about the company fetching information from Yahoo Finance, so the user has something to read while the next step proceeds. It also fetches the main financial metrics and computes additional ones.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Step 1: Events and news fetching: &lt;em&gt;This part fetches stock events from Yahoo Finance and news from Google RSS feed. It also generates a sentiment analysis about the articles fetched using FinBERT.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 2: Forecast using Machine Learning LSTM: &lt;em&gt;This part creates a baseline scenario from a LSTM forecast. The forecast covers 60 days and is trained from 100 last values of close/ high/low prices. It is a quantiative model only. An optimistic and pessimistic scenario are then created by tweaking the main baseline to give a window of prediction. They do not integrate macroeconomic factors, specific metric variations nor Monte Carlo simulations for the moment.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 3: Market data restitution: &lt;em&gt;This part is dedicated to restitute graphically the previously computed data. It also computes CFA classical metrics (histogram of returns, skewness, kurtosis) and their explanation. The part concludes with an Ollama AI commentary of the analysis.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 4: Financial statement analysis: &lt;em&gt;This part is dedicated to the generation of the main ratios from the financial statements of the last 3 years of the company. Each part concludes with an Ollama AI commentary on the ratios. The analysis includes an overview of the variation, and highlights in color wether the change is positive or negative. Each ratio is commented so you can understand what they represent/ how they are calculated. The ratios include:&lt;/em&gt; &lt;ul&gt; &lt;li&gt;Profitability ratios: Profit margin, ROA, ROCE, ROE,...&lt;/li&gt; &lt;li&gt;Asset related ratios: Asset turnover, working capital.&lt;/li&gt; &lt;li&gt;Liquidity ratios: Current ratio, quick ratio, cash ratio.&lt;/li&gt; &lt;li&gt;Solvency ratios: debt to assets, debt to capital, financial leverage, coverage ratios,...&lt;/li&gt; &lt;li&gt;Operational ratios (cashflow related): CFI/ CFF/ CFO ratios, cash return on assets,...&lt;/li&gt; &lt;li&gt;Bankrupcy and financial health scores: Altman Z-score/ Ohlson O-score.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Appendix: Financial statements: &lt;em&gt;A summary of the financial statements scaled for better readability in case you want to push the manual analysis further.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Target audience:&lt;/strong&gt; Students, researchers,... For educational and research purpose only. However, it illustrates how local LLMs could be integrated into industry practices and workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Comparison:&lt;/strong&gt; The project enables both a market and statement analysis perspective, and showcases how a local LLM can run in a financial context while showing to which extent it can bring something to analysts.&lt;/p&gt; &lt;p&gt;At this point, I'm considering starting to work on industry metrics (for comparability of ratios) and portfolio construction. Thank you in advance for your insights, I‚Äôm keen to refine this further with input from the community!&lt;/p&gt; &lt;p&gt;The repository: &lt;a href="https://github.com/gruquilla/FinAPy"&gt;gruquilla/FinAPy: Single-stock analysis using Python and local machine learning/ AI tools (Ollama, LSTM).&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gruquilla"&gt; /u/gruquilla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9hn8znm45f0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otbtp2/ollamapowered_open_source_singlestock_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otbtp2/ollamapowered_open_source_singlestock_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T11:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou2hzw</id>
    <title>Smarter Agents, Fewer Integrations: How PolyMCP Is Changing Multi-Tool AI Workflows</title>
    <updated>2025-11-11T06:42:26+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ou2hzw/smarter_agents_fewer_integrations_how_polymcp_is/"&gt; &lt;img alt="Smarter Agents, Fewer Integrations: How PolyMCP Is Changing Multi-Tool AI Workflows" src="https://external-preview.redd.it/4WW_W4p9ydeJXDr9T_Om3KiQJYlfiVcevEHUJWwfFd4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f1c5ae24902278d025d41c1773334a9c3d6de5f" title="Smarter Agents, Fewer Integrations: How PolyMCP Is Changing Multi-Tool AI Workflows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/Polymcp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou2hzw/smarter_agents_fewer_integrations_how_polymcp_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ou2hzw/smarter_agents_fewer_integrations_how_polymcp_is/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T06:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1otbtrh</id>
    <title>Built a local chat UI for Ollama - thought I'd share</title>
    <updated>2025-11-10T11:56:14+00:00</updated>
    <author>
      <name>/u/neiellcare</name>
      <uri>https://old.reddit.com/user/neiellcare</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a web interface for Ollama that stores everything locally. No external servers, all conversations stay on your machine.&lt;/p&gt; &lt;p&gt;Main features: - Memory system so the AI remembers context between chats - Upload documents (PDFs, Word files) for the AI to reference - Web search integration when you need current information - Works with vision models like LLaVA - Live preview for code the AI generates&lt;/p&gt; &lt;p&gt;Everything runs in Docker or you can run it locally with Node. It uses React and TypeScript, stores data in IndexedDB in your browser.&lt;/p&gt; &lt;p&gt;I built it because I wanted something privacy-focused that also had RAG and conversation memory in one place. Works well for me, figured others might find it useful.&lt;/p&gt; &lt;p&gt;It's open source if anyone wants to check it out or contribute. Happy to answer questions about how it works.&lt;/p&gt; &lt;p&gt;Look for &lt;code&gt;Symchat&lt;/code&gt; in Github.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neiellcare"&gt; /u/neiellcare &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otbtrh/built_a_local_chat_ui_for_ollama_thought_id_share/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otbtrh/built_a_local_chat_ui_for_ollama_thought_id_share/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otbtrh/built_a_local_chat_ui_for_ollama_thought_id_share/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T11:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou7kuh</id>
    <title>Sheet / Data Analyst Tools, Partial Functionality Achieved</title>
    <updated>2025-11-11T11:58:31+00:00</updated>
    <author>
      <name>/u/eworker8888</name>
      <uri>https://old.reddit.com/user/eworker8888</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eworker8888"&gt; /u/eworker8888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ou71yg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou7kuh/sheet_data_analyst_tools_partial_functionality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ou7kuh/sheet_data_analyst_tools_partial_functionality/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T11:58:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1otsq4j</id>
    <title>RAG. Embedding model. What do u prefer ?</title>
    <updated>2025-11-10T22:52:33+00:00</updated>
    <author>
      <name>/u/apolorotov</name>
      <uri>https://old.reddit.com/user/apolorotov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm doing some research on real-world RAG setups and I‚Äôm curious which embedding models people actually use in production (or serious side projects).&lt;/p&gt; &lt;p&gt;There are dozens of options now ‚Äî OpenAI text-embedding-3, BGE-M3, Voyage, Cohere, Qwen3, local MiniLM, etc. But despite all the talk about ‚Äúdomain-specific embeddings‚Äù, I almost never see anyone training or fine-tuning their own.&lt;/p&gt; &lt;p&gt;So I‚Äôd love to hear from you: 1. Which embedding model(s) are you using, and for what kind of data/tasks? 2. Have you ever tried to fine-tune your own? Why or why not?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apolorotov"&gt; /u/apolorotov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otsq4j/rag_embedding_model_what_do_u_prefer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otsq4j/rag_embedding_model_what_do_u_prefer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otsq4j/rag_embedding_model_what_do_u_prefer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T22:52:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou1x4c</id>
    <title>Built a CLI tool to reuse AI instructions for specific tasks</title>
    <updated>2025-11-11T06:07:04+00:00</updated>
    <author>
      <name>/u/Revolutionary-Judge9</name>
      <uri>https://old.reddit.com/user/Revolutionary-Judge9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ou1x4c/built_a_cli_tool_to_reuse_ai_instructions_for/"&gt; &lt;img alt="Built a CLI tool to reuse AI instructions for specific tasks" src="https://external-preview.redd.it/bGQxbG05aDRpazBnMUXN_YYfWjInCTvbB8XSnnOq8UmtvCL2MyTJG86DAUjb.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bcf65b87e5e6aff456c7e4867e6af1c06677a893" title="Built a CLI tool to reuse AI instructions for specific tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Before creating Askimo, I used to keep my prompt templates in notes - things like &amp;quot;summarize this document concisely&amp;quot; or &amp;quot;write documentation that lists only a class‚Äôs responsibilities, not its implementation details.&amp;quot;&lt;br /&gt; Every time, I‚Äôd copy and paste those instructions into chat to get the same kind of result.&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://github.com/haiphucnguyen/askimo"&gt;Askimo&lt;/a&gt;, a CLI tool that lets you turn those instructions into reusable ‚Äúrecipes.‚Äù&lt;br /&gt; Now I can just run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;askimo -r summarize &amp;lt;file_path&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;User can write their own recipe to instruct the AI response&lt;/p&gt; &lt;p&gt;The CLI tool can also run in interactive mode, just like a regular chat program. It works locally with Ollama, and can also connect to other LLM providers.&lt;/p&gt; &lt;p&gt;If you have the problems like mine, this tool is worth for checkout and you know why I made it :D. &lt;/p&gt; &lt;p&gt;Github Repo: &lt;a href="https://github.com/haiphucnguyen/askimo"&gt;https://github.com/haiphucnguyen/askimo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Revolutionary-Judge9"&gt; /u/Revolutionary-Judge9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q315f9h4ik0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou1x4c/built_a_cli_tool_to_reuse_ai_instructions_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ou1x4c/built_a_cli_tool_to_reuse_ai_instructions_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T06:07:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou6ami</id>
    <title>Enterpise prices license</title>
    <updated>2025-11-11T10:43:43+00:00</updated>
    <author>
      <name>/u/LowTerrible9453</name>
      <uri>https://old.reddit.com/user/LowTerrible9453</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, we're working on an on-premise AI solution for an enterprise client and planning to integrate Ollama with a Web UI. Could you please share the pricing details or licensing requirements for enterprise use? Or if theres any consultant or business professionals who can support us let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LowTerrible9453"&gt; /u/LowTerrible9453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou6ami/enterpise_prices_license/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou6ami/enterpise_prices_license/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ou6ami/enterpise_prices_license/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T10:43:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1otixju</id>
    <title>Ollama working well on the vs code</title>
    <updated>2025-11-10T16:49:23+00:00</updated>
    <author>
      <name>/u/Dry_Shower287</name>
      <uri>https://old.reddit.com/user/Dry_Shower287</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otixju/ollama_working_well_on_the_vs_code/"&gt; &lt;img alt="Ollama working well on the vs code" src="https://preview.redd.it/fotbt1hplg0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aff4a58a24ebef77a98f660883ac714cad18fdef" title="Ollama working well on the vs code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Shower287"&gt; /u/Dry_Shower287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fotbt1hplg0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otixju/ollama_working_well_on_the_vs_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otixju/ollama_working_well_on_the_vs_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T16:49:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oubxxp</id>
    <title>model using 100% GPU in idle, is this normal?</title>
    <updated>2025-11-11T15:12:28+00:00</updated>
    <author>
      <name>/u/iMaexx_Backup</name>
      <uri>https://old.reddit.com/user/iMaexx_Backup</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oubxxp/model_using_100_gpu_in_idle_is_this_normal/"&gt; &lt;img alt="model using 100% GPU in idle, is this normal?" src="https://a.thumbs.redditmedia.com/LXyPa4BX87i_btgRKtZWhbozYyuaVqZLJEe8a0REts4.jpg" title="model using 100% GPU in idle, is this normal?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I'm new to this, so I apologize if the question is stupid. &lt;/p&gt; &lt;p&gt;I installed Ollama and Rocm on my Fedora distro, downloaded the latest gpt-oss:20b model and started it. &lt;/p&gt; &lt;p&gt;I noted that, as soon as I start the model, my GPU is and stays 100% utilized until I stop the model, even though I'm taking no inputs at all and the model is just idling. &lt;/p&gt; &lt;p&gt;Is this behavior normal? Shouldn't the utilization only go up after I did an input and the LLM is generating the response?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/olo09xm38n0g1.png?width=1168&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5a2966dad16985173674566b5bfe363312610cd"&gt;llm Idle&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/33oej42z8n0g1.png?width=1172&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d83596b58f27d4df12c1d8ea28d5d1df189b8ca9"&gt;llm off&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iMaexx_Backup"&gt; /u/iMaexx_Backup &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oubxxp/model_using_100_gpu_in_idle_is_this_normal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oubxxp/model_using_100_gpu_in_idle_is_this_normal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oubxxp/model_using_100_gpu_in_idle_is_this_normal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T15:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1our2h2</id>
    <title>Claudette Chatmode + Mimir memory bank integration</title>
    <updated>2025-11-12T00:51:29+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/GithubCopilot/comments/1our2aq/claudette_chatmode_mimir_memory_bank_integration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1our2h2/claudette_chatmode_mimir_memory_bank_integration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1our2h2/claudette_chatmode_mimir_memory_bank_integration/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T00:51:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1otwqev</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-11-11T01:44:51+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Note Management&lt;/li&gt; &lt;li&gt;Multi Collaborative Notebooks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otwqev/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otwqev/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otwqev/open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T01:44:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouxypo</id>
    <title>I cant get any of the uncensored abliterated gpt oss models to work</title>
    <updated>2025-11-12T06:34:49+00:00</updated>
    <author>
      <name>/u/Frogy_mcfrogyface</name>
      <uri>https://old.reddit.com/user/Frogy_mcfrogyface</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, Im having an issue with pretty much every uncensored gpt oss model. When I run it, it loads up and then nothing. When I load it up from CMD, it gets to the &amp;quot;Send a message (/? for help)&amp;quot;. I type in a message, it does its little spinny thing, then goes back to &amp;quot;Send a message (/? for help)&amp;quot;&lt;/p&gt; &lt;p&gt;current one im trying is Huihui-gpt-oss-20b-BF16-abliterated:Q4_K_M and no dice. The standard versions work just fine, as do other uncensored LLM's Any ideas? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frogy_mcfrogyface"&gt; /u/Frogy_mcfrogyface &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ouxypo/i_cant_get_any_of_the_uncensored_abliterated_gpt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ouxypo/i_cant_get_any_of_the_uncensored_abliterated_gpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ouxypo/i_cant_get_any_of_the_uncensored_abliterated_gpt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T06:34:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovcqwx</id>
    <title>Ollama just making shit up?</title>
    <updated>2025-11-12T18:13:14+00:00</updated>
    <author>
      <name>/u/groundserver</name>
      <uri>https://old.reddit.com/user/groundserver</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ovcqwx/ollama_just_making_shit_up/"&gt; &lt;img alt="Ollama just making shit up?" src="https://preview.redd.it/7c8nyux3av0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee2c701811da0212a4d75aa0dcea292f4df94441" title="Ollama just making shit up?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I asked Ollama 2 uncensored to provide a synopsis of a pdf and provided the link. The synopsis provided was not the subject of the linked PDF. Then I asked it again but didn't provide the link, and it just made shit up. Everytime I asked for a synopsis of a link and did not provide a link, it acted like I did, and returned different results every time. Wierd.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/groundserver"&gt; /u/groundserver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7c8nyux3av0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovcqwx/ollama_just_making_shit_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovcqwx/ollama_just_making_shit_up/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T18:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1our0hc</id>
    <title>What's Stopping you from using local AI models more?</title>
    <updated>2025-11-12T00:48:56+00:00</updated>
    <author>
      <name>/u/ButterscotchNo102</name>
      <uri>https://old.reddit.com/user/ButterscotchNo102</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ButterscotchNo102"&gt; /u/ButterscotchNo102 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1oukfvf/whats_stopping_you_from_using_local_ai_models_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1our0hc/whats_stopping_you_from_using_local_ai_models_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1our0hc/whats_stopping_you_from_using_local_ai_models_more/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T00:48:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov7kqc</id>
    <title>Masking the connection error in Ollama</title>
    <updated>2025-11-12T15:05:45+00:00</updated>
    <author>
      <name>/u/chirchan91</name>
      <uri>https://old.reddit.com/user/chirchan91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a simple way of masking the connection errors in Ollama when it failed to connect to the Model Server.&lt;/p&gt; &lt;p&gt;for example: Head &amp;quot;http://&amp;lt;internal‚Äëip&amp;gt;:11434/&amp;quot;: dial tcp &amp;lt;internal‚Äëip&amp;gt;:11434: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.&lt;/p&gt; &lt;p&gt;instead i should get &amp;quot;connection failed due host not responding. Please contact support&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chirchan91"&gt; /u/chirchan91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7kqc/masking_the_connection_error_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7kqc/masking_the_connection_error_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov7kqc/masking_the_connection_error_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T15:05:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov7t8l</id>
    <title>MCP Server for Blender - Built for PolyMCP Agent Orchestration</title>
    <updated>2025-11-12T15:14:21+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ov7t8l/mcp_server_for_blender_built_for_polymcp_agent/"&gt; &lt;img alt="MCP Server for Blender - Built for PolyMCP Agent Orchestration" src="https://external-preview.redd.it/JIrsfAZEJFJcSC_-Ows_UR3v_W3m64JAE2m0e3rAoNc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1896a8509f0f381f92c8934d66e06ad80e365455" title="MCP Server for Blender - Built for PolyMCP Agent Orchestration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/Blender-MCP-Server"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7t8l/mcp_server_for_blender_built_for_polymcp_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov7t8l/mcp_server_for_blender_built_for_polymcp_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T15:14:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovdpch</id>
    <title>Is anyone from London?</title>
    <updated>2025-11-12T18:47:26+00:00</updated>
    <author>
      <name>/u/Dry_Music_7160</name>
      <uri>https://old.reddit.com/user/Dry_Music_7160</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Music_7160"&gt; /u/Dry_Music_7160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1ovdp64/is_anyone_from_london/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovdpch/is_anyone_from_london/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovdpch/is_anyone_from_london/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T18:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov16ju</id>
    <title>Someone wrote an article about my library PolyMCP</title>
    <updated>2025-11-12T09:58:43+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ov16ju/someone_wrote_an_article_about_my_library_polymcp/"&gt; &lt;img alt="Someone wrote an article about my library PolyMCP" src="https://external-preview.redd.it/y0dgtZ0gVjnRje7BRhie_7gJWNn7zzoil-XFJAfPX90.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80e455756943072492fbf6c6135f659142eced5f" title="Someone wrote an article about my library PolyMCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://levelup.gitconnected.com/why-your-python-functions-arent-ai-tools-yet-and-how-polymcp-fixes-it-in-one-line-d8e62550ac53"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov16ju/someone_wrote_an_article_about_my_library_polymcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov16ju/someone_wrote_an_article_about_my_library_polymcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T09:58:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov62o7</id>
    <title>I taught my AI vtuber how to play osu! Here's how it went...</title>
    <updated>2025-11-12T14:07:41+00:00</updated>
    <author>
      <name>/u/imfstr</name>
      <uri>https://old.reddit.com/user/imfstr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ov62o7/i_taught_my_ai_vtuber_how_to_play_osu_heres_how/"&gt; &lt;img alt="I taught my AI vtuber how to play osu! Here's how it went..." src="https://external-preview.redd.it/kfbIE7cVIUZ7rju-fJnsEvS2hqkafGslwwkxU3XNQtc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb693bfc94522d852a0f35b056c144ca0965d28f" title="I taught my AI vtuber how to play osu! Here's how it went..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;some of you might remember my last post where i showed my AI, &lt;em&gt;Eris&lt;/em&gt;, picking out her dream PC setup on amazon. since then, i‚Äôve been working on something a bit crazier, i decided to teach her how to &lt;strong&gt;play osu!&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;in the video, she chats with me for a bit and then actually plays through &lt;strong&gt;two&lt;/strong&gt; osu! maps using a neural network I integrated into her system. It was a big leap from where she was before, and i learned a ton about AI decision-making, timing, and visual input in the process. &lt;/p&gt; &lt;p&gt;i‚Äôm always open to feedback, whether it‚Äôs about how she looks/it's animated, all the way to how she should respond, possible improvements to her interactivity, or just general advice for the project. &lt;/p&gt; &lt;p&gt;thanks again to everyone who gave feedback last time, it really helped a lot! :D&lt;/p&gt; &lt;p&gt;(she plays in an offline environment, and none of her scores get uploaded publicly incase it seems like this is cheating)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/imfstr"&gt; /u/imfstr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=R778HLEGeWg&amp;amp;t=2s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov62o7/i_taught_my_ai_vtuber_how_to_play_osu_heres_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov62o7/i_taught_my_ai_vtuber_how_to_play_osu_heres_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T14:07:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov7wch</id>
    <title>How high of a spec do you have to have in order to install ollama in local environment?</title>
    <updated>2025-11-12T15:17:29+00:00</updated>
    <author>
      <name>/u/SnooRegrets3378</name>
      <uri>https://old.reddit.com/user/SnooRegrets3378</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently work in a virtual machine environment where any website is unavailable. Is it possible to bring ollama into this type of setting? Exasperated by having to do everything with excel when you can use ai models and i work with sensitive datas so i would have to do the work locally.&lt;/p&gt; &lt;p&gt;Sorry in advance for the possible inaccurate word choice im not a computer guy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooRegrets3378"&gt; /u/SnooRegrets3378 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7wch/how_high_of_a_spec_do_you_have_to_have_in_order/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7wch/how_high_of_a_spec_do_you_have_to_have_in_order/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov7wch/how_high_of_a_spec_do_you_have_to_have_in_order/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T15:17:29+00:00</published>
  </entry>
</feed>
