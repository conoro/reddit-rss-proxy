<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-28T19:13:42+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qo69l7</id>
    <title>When trying to run latest ollama using zip ollama-windows-arm64.zip from GitHub facing issue. Can Someone please tell me what the issue is.</title>
    <updated>2026-01-27T06:33:01+00:00</updated>
    <author>
      <name>/u/FINALISHERE</name>
      <uri>https://old.reddit.com/user/FINALISHERE</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qo69l7/when_trying_to_run_latest_ollama_using_zip/"&gt; &lt;img alt="When trying to run latest ollama using zip ollama-windows-arm64.zip from GitHub facing issue. Can Someone please tell me what the issue is." src="https://preview.redd.it/ry4lgcgu6ufg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfc0b1d6f647b9c2cd1cc7c784682e2e3c7770c5" title="When trying to run latest ollama using zip ollama-windows-arm64.zip from GitHub facing issue. Can Someone please tell me what the issue is." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can someone please tell me what the issue is.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FINALISHERE"&gt; /u/FINALISHERE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ry4lgcgu6ufg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qo69l7/when_trying_to_run_latest_ollama_using_zip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qo69l7/when_trying_to_run_latest_ollama_using_zip/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T06:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnpy7y</id>
    <title>Fine tuning open models and prep for robust deployment</title>
    <updated>2026-01-26T19:21:32+00:00</updated>
    <author>
      <name>/u/codes_astro</name>
      <uri>https://old.reddit.com/user/codes_astro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was at a tech event recently and lots of devs mentioned about problem with ML projects, and most common was deployments and production issues.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;note:&lt;/strong&gt; I'm part of the KitOps community&lt;/p&gt; &lt;p&gt;Training a model is crucial but usually the easy part due to tools like Unsloth and lots of other options. You fine-tune it, it works, results look good. But when you start building a product, everything gets messy:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;model files in notebooks&lt;/li&gt; &lt;li&gt;configs and prompts not tracked properly&lt;/li&gt; &lt;li&gt;deployment steps that only work on one machine&lt;/li&gt; &lt;li&gt;datasets or other assets are lying somewhere else&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even when training is clean, moving the model forward feels challenging with real products.&lt;/p&gt; &lt;p&gt;So I tried a full train ‚Üí push ‚Üí pull ‚Üí run flow to see if it could actually be simple.&lt;/p&gt; &lt;p&gt;I fine-tuned a model using Unsloth.&lt;/p&gt; &lt;p&gt;It was fast, becasue I kept it simple for testing purpose, and ran fine using official cookbook. Nothing fancy, just a real dataset and a IBM-Granite-4.0 model.&lt;/p&gt; &lt;p&gt;Training wasn‚Äôt the issue though. What mattered was what came next.&lt;/p&gt; &lt;p&gt;Instead of manually moving files around, I pushed the fine-tuned model to Hugging Face, then imported it into Jozu ML. Jozu treats models like proper versioned artifacts, not random folders.&lt;/p&gt; &lt;p&gt;From there, I used KitOps to pull the model locally. One command and I had everything - weights, configs, metadata in the right place.&lt;/p&gt; &lt;p&gt;After that, running inference or deploying was straightforward.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Now, let me give context on why Jozu or KitOps?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Kitops is only open-source AIML tool for packaging and versioning for ML and it follows best practices for Devops while taking care of AI usecases.&lt;/p&gt; &lt;p&gt;- Jozu is enterprise platform which can be run on-prem on any existing infra and when it comes to problems like hot reload and cold start or pods going offline when making changes in large scale application, it's 7x faster then other in terms of GPU optimization.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The main takeaway for me:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most ML pain isn‚Äôt about training better models.&lt;br /&gt; It‚Äôs about keeping things clean at scale.&lt;/p&gt; &lt;p&gt;Unsloth made training easy.&lt;br /&gt; KitOps kept things organized with versioning and packaging.&lt;br /&gt; Jozu handled production side things like tracking, security and deployment.&lt;/p&gt; &lt;p&gt;I wrote a detailed article &lt;a href="https://mranand.substack.com/p/from-training-to-deployment-push"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codes_astro"&gt; /u/codes_astro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnpy7y/fine_tuning_open_models_and_prep_for_robust/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnpy7y/fine_tuning_open_models_and_prep_for_robust/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnpy7y/fine_tuning_open_models_and_prep_for_robust/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T19:21:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo7hhm</id>
    <title>Are my Pc requirements enough to run Ollama?</title>
    <updated>2026-01-27T07:43:37+00:00</updated>
    <author>
      <name>/u/SupermarketLost7854</name>
      <uri>https://old.reddit.com/user/SupermarketLost7854</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I been hearing that I need a top end pc to run ai models. Ollama peek my interest and I want to ask if my specs are enough. &lt;/p&gt; &lt;p&gt;AMD Ryzen 7 5700x 8 core processor, Memory DDR4 32Gbytes, NVDIA GeForce TRX 3070 8GBytes. &lt;/p&gt; &lt;p&gt;Thank you all and have a good day. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SupermarketLost7854"&gt; /u/SupermarketLost7854 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qo7hhm/are_my_pc_requirements_enough_to_run_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qo7hhm/are_my_pc_requirements_enough_to_run_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qo7hhm/are_my_pc_requirements_enough_to_run_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T07:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnpc7a</id>
    <title>Vulkan vs ROCm on RX 9070XT (RDNA4): 9% faster, 50% less power</title>
    <updated>2026-01-26T19:01:07+00:00</updated>
    <author>
      <name>/u/Due_Pea_372</name>
      <uri>https://old.reddit.com/user/Due_Pea_372</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Benchmarked Ollama 0.15.1 with qwen3-coder:30b on my RX 9070 XT (gfx1201, 16GB VRAM).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Vulkan&lt;/th&gt; &lt;th align="left"&gt;ROCm&lt;/th&gt; &lt;th align="left"&gt;Difference&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Tokens/s&lt;/td&gt; &lt;td align="left"&gt;52.5&lt;/td&gt; &lt;td align="left"&gt;48.2&lt;/td&gt; &lt;td align="left"&gt;+8.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Power&lt;/td&gt; &lt;td align="left"&gt;68 W&lt;/td&gt; &lt;td align="left"&gt;149 W&lt;/td&gt; &lt;td align="left"&gt;-54%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;VRAM&lt;/td&gt; &lt;td align="left"&gt;16.1 GB&lt;/td&gt; &lt;td align="left"&gt;15.8 GB&lt;/td&gt; &lt;td align="left"&gt;+2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TTFT&lt;/td&gt; &lt;td align="left"&gt;27.8 ms&lt;/td&gt; &lt;td align="left"&gt;22.9 ms&lt;/td&gt; &lt;td align="left"&gt;+21%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Temp&lt;/td&gt; &lt;td align="left"&gt;51¬∞C&lt;/td&gt; &lt;td align="left"&gt;47¬∞C&lt;/td&gt; &lt;td align="left"&gt;+8%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Key takeaway:&lt;/strong&gt; Vulkan is not only faster but dramatically more power efficient on RDNA4. ROCm draws 2x the power for worse performance.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt; Cachyos, ollama-vulkan / ollama-rocm from AUR, ~35 runs each.&lt;/p&gt; &lt;p&gt;Script used for benchmarking: &lt;a href="https://github.com/maeddesg/spielwiese"&gt;https://github.com/maeddesg/spielwiese&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone else seeing similar results on RDNA4? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Pea_372"&gt; /u/Due_Pea_372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnpc7a/vulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnpc7a/vulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnpc7a/vulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T19:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnjk4p</id>
    <title>SHELLper üêö: Qwen3 0.6B for More Reliable Multi-Turn Function Calling</title>
    <updated>2026-01-26T15:44:43+00:00</updated>
    <author>
      <name>/u/gabucz</name>
      <uri>https://old.reddit.com/user/gabucz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We fine-tuned a 0.6B model for converting English to executable bash commands. It's small enough to run locally on your laptop, giving you full data privacy.&lt;/p&gt; &lt;p&gt;Multi-turn tool calling is incredibly challenging for small models - before fine-tuning, Qwen3-0.6B had 84% single-call accuracy, which collapses to &lt;strong&gt;only 42% across 5 turns&lt;/strong&gt;! After our tuning, it reaches 100% on our test set, providing dependable multi-turn capabilities.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Parameters&lt;/th&gt; &lt;th align="left"&gt;Tool call accuracy (test set)&lt;/th&gt; &lt;th align="left"&gt;=&amp;gt; 5-turn tool call accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B Instruct (teacher)&lt;/td&gt; &lt;td align="left"&gt;235B&lt;/td&gt; &lt;td align="left"&gt;99%&lt;/td&gt; &lt;td align="left"&gt;95%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 0.6B (base)&lt;/td&gt; &lt;td align="left"&gt;0.6B&lt;/td&gt; &lt;td align="left"&gt;84%&lt;/td&gt; &lt;td align="left"&gt;42%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3 0.6B (tuned)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.6B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;100%&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;100%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/distil-labs/distil-SHELLper"&gt;https://github.com/distil-labs/distil-SHELLper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface model: &lt;a href="https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper"&gt;https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Quick Start&lt;/h1&gt; &lt;p&gt;Set up the environment:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Set up environment python -m venv .venv . .venv/bin/activate pip install openai huggingface_hub &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Dowload the model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;hf download distil-labs/distil-qwen3-0.6b-SHELLper --local-dir distil_model cd distil_model ollama create distil_model -f Modelfile cd .. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the assistant:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python filesystem_demo.py &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The demo asks for permission before executing commands (for safety) and restricts dangerous operations (like &lt;code&gt;rm -r /&lt;/code&gt;), so don't hesitate to try it!&lt;/p&gt; &lt;h1&gt;How We Trained SHELLper&lt;/h1&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;Multi-turn tool calling is exceptionally hard for small models - performance breaks down as tool calls chain, degrading with each turn. If prediction errors are independent (e.g. due to bad parameter values), an 80% accurate model has just a 33% chance of succeeding across 5 turns.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Single tool call accuracy&lt;/th&gt; &lt;th align="left"&gt;5-turn tool call accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;80%&lt;/td&gt; &lt;td align="left"&gt;33%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;90%&lt;/td&gt; &lt;td align="left"&gt;59%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;95%&lt;/td&gt; &lt;td align="left"&gt;77%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;99%&lt;/td&gt; &lt;td align="left"&gt;95%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;In this demo, we explored whether we could substantially boost a small model's multi-turn performance. We picked a task from the &lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html"&gt;Berkeley function calling leaderboard&lt;/a&gt; - the &lt;a href="https://github.com/ShishirPatil/gorilla/blob/main/berkeley-function-call-leaderboard/bfcl_eval/data/BFCL_v4_multi_turn_base.json"&gt;gorilla file system tool calling task&lt;/a&gt;. Our modifications were:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The original allows multiple tool calls per assistant turn ‚Üí we permit only one&lt;/li&gt; &lt;li&gt;Maximum 5 turns&lt;/li&gt; &lt;li&gt;Commands map to real bash (not gorilla filesystem functions)&lt;/li&gt; &lt;li&gt;Tool call outputs aren't added to the conversation history&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In short, an identical tool set, but simpler &lt;a href="https://github.com/distil-labs/distil-SHELLper/tree/main/data"&gt;train/test data.&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Training Pipeline&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Seed Data:&lt;/strong&gt; We wrote 20 simplified training conversations that span the available tools while remaining reasonably realistic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Synthetic Expansion:&lt;/strong&gt; Through our &lt;a href="https://www.distillabs.ai/blog/small-expert-agents-from-10-examples/?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=shellper"&gt;data synthesis pipeline&lt;/a&gt;, we scaled to thousands of examples.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For handling variable conversation lengths, we split each conversation into subsets ending with a tool call. For instance:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Input] User: List all files =&amp;gt; Model: ls -al =&amp;gt; User: go to directory models [Output] Model: cd models &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;... becomes 2 training points:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Input] User: List all files [Output] Model: ls -al [Input] User: List all files =&amp;gt; Model: ls -al =&amp;gt; User: go to directory models [Output] Model: cd models` &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Fine-tuning:&lt;/strong&gt; We went with &lt;strong&gt;Qwen3-0.6B&lt;/strong&gt; as the &lt;a href="https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning"&gt;best fine-tunable sub-1B&lt;/a&gt; model on our platform that handles tool calling.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Usage Examples&lt;/h1&gt; &lt;p&gt;The assistant takes natural language input, generates bash commands, and optionally runs them (after Y/N confirmation).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Basic filesystem operations&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; python filesystem_demo.py USER: List all files in the current directory COMMAND: ls USER: Create a new directory called test_folder COMMAND: mkdir test_folder USER: Navigate to test_folder COMMAND: cd test_folder &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Limitations and Next Steps&lt;/h1&gt; &lt;p&gt;Right now, we're limited to a basic bash tool set:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;no pipes, compound commands, or multiple tool calls per turn&lt;/li&gt; &lt;li&gt;no validation of invalid commands/parameters&lt;/li&gt; &lt;li&gt;5-turn conversation maximum&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We prioritized getting the simple case right before expanding to more complex scenarios. Up next: support for multiple tool calls (enabling more sophisticated agent workflows) and benchmarking on &lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html"&gt;BFCL&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you're using this in your bash workflows, track failing commands, append them to &lt;code&gt;data/train.jsonl&lt;/code&gt;, and retrain with the updated dataset (or experiment with a larger student model!).&lt;/p&gt; &lt;h1&gt;Discussion&lt;/h1&gt; &lt;p&gt;Interested to hear from the community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is anyone else working on fine-tuning small models for multi-turn tool calling?&lt;/li&gt; &lt;li&gt;What other &amp;quot;narrow but valuable&amp;quot; tasks could benefit from local, privacy-preserving models?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let us know your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gabucz"&gt; /u/gabucz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnjk4p/shellper_qwen3_06b_for_more_reliable_multiturn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnjk4p/shellper_qwen3_06b_for_more_reliable_multiturn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnjk4p/shellper_qwen3_06b_for_more_reliable_multiturn/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T15:44:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnersl</id>
    <title>I built a "Spatial" website for Ollama because I hate linear chats. (Local-first, no DB)</title>
    <updated>2026-01-26T12:31:18+00:00</updated>
    <author>
      <name>/u/yibie</name>
      <uri>https://old.reddit.com/user/yibie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qnersl/i_built_a_spatial_website_for_ollama_because_i/"&gt; &lt;img alt="I built a &amp;quot;Spatial&amp;quot; website for Ollama because I hate linear chats. (Local-first, no DB)" src="https://external-preview.redd.it/YjUzczlrM29ub2ZnMc1v4660SmLmGdaCwdxTOqPLhJLKWmMLYGebzhszHCXS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c84014d04ec9dff478c02a4e070e3416870f2d7" title="I built a &amp;quot;Spatial&amp;quot; website for Ollama because I hate linear chats. (Local-first, no DB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running Llama 3 locally via Ollama for a while, but I kept getting frustrated with the standard &amp;quot;ChatGPT-style&amp;quot; linear interface. My brain doesn't work in a straight line. I'm usually debugging code in one thread, writing docs in another, and brainstorming ideas in a third. In a linear chat, context gets polluted constantly.&lt;/p&gt; &lt;p&gt;So I built a tool called Project Nodal. It's a &amp;quot;Spatial Thinking OS&amp;quot; for your local LLMs.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Infinite Canvas: Drag and drop chat windows (Sticky Notes) anywhere.&lt;/li&gt; &lt;li&gt;Context Isolation: Group backend notes separate from frontend notes.&lt;/li&gt; &lt;li&gt;Forking: This is the big one. Click a message to &amp;quot;fork&amp;quot; it into a new branch/note. Great for &amp;quot;what if&amp;quot; scenarios without ruining the main thread.&lt;/li&gt; &lt;li&gt;100% Local: It uses IndexedDB. No backend database. Connects directly to your Ollama endpoint (or OpenAI if you want).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's open source and I just deployed a demo. &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/yibie/project-nodal"&gt;https://github.com/yibie/project-nodal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://project-nodal-ai.vercel.app/"&gt;https://project-nodal-ai.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;‚ö†Ô∏è A Note on Web Deployment (Vercel/Netlify)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/yibie/project-nodal#%EF%B8%8F-a-note-on-web-deployment-vercelnetlify"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are viewing this demo online (HTTPS), you &lt;strong&gt;cannot&lt;/strong&gt; connect to a local Ollama instance (HTTP) due to browser security policies (Mixed Content Blocking).&lt;/p&gt; &lt;p&gt;To use Local Ollama: Please &lt;a href="https://github.com/yibie/project-nodal"&gt;clone this repo&lt;/a&gt; and run it locally:&lt;/p&gt; &lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/yibie/project-nodal.git"&gt;&lt;code&gt;https://github.com/yibie/project-nodal.git&lt;/code&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;cd project-nodal&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;npm install&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;npm run dev&lt;/code&gt;&lt;/p&gt; &lt;p&gt;To use the Online Demo: Please use an OpenAI or DeepSeek API Key in the settings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yibie"&gt; /u/yibie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/utanig3onofg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qnersl/i_built_a_spatial_website_for_ollama_because_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qnersl/i_built_a_spatial_website_for_ollama_because_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-26T12:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qodhdc</id>
    <title>Ollama and Kokoro to test TTS on n8n</title>
    <updated>2026-01-27T13:13:20+00:00</updated>
    <author>
      <name>/u/Unique_Winner_5927</name>
      <uri>https://old.reddit.com/user/Unique_Winner_5927</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to hear Qwen3 answer using Kokoro TTS locally.&lt;/p&gt; &lt;p&gt;Everything run locally:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ThomasPlantain/n8n/blob/main/assets/tts/readme.md"&gt;https://github.com/ThomasPlantain/n8n/blob/main/assets/tts/readme.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unique_Winner_5927"&gt; /u/Unique_Winner_5927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qodhdc/ollama_and_kokoro_to_test_tts_on_n8n/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qodhdc/ollama_and_kokoro_to_test_tts_on_n8n/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qodhdc/ollama_and_kokoro_to_test_tts_on_n8n/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T13:13:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoeksi</id>
    <title>Use Ollama to Test Multiple Code Generation Models With Koyeb Sandboxes</title>
    <updated>2026-01-27T13:59:07+00:00</updated>
    <author>
      <name>/u/Plus_Ad7909</name>
      <uri>https://old.reddit.com/user/Plus_Ad7909</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qoeksi/use_ollama_to_test_multiple_code_generation/"&gt; &lt;img alt="Use Ollama to Test Multiple Code Generation Models With Koyeb Sandboxes" src="https://external-preview.redd.it/Xb9rtzGFTk3Zt-MFQ-VopbgY1_WeYce_sBo8AKo3HWs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d78ec5326e405991cefc1a2e47385c4d8e7518bd" title="Use Ollama to Test Multiple Code Generation Models With Koyeb Sandboxes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plus_Ad7909"&gt; /u/Plus_Ad7909 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.koyeb.com/tutorials/use-ollama-to-test-multiple-code-generation-models-with-koyeb-sandboxes"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qoeksi/use_ollama_to_test_multiple_code_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qoeksi/use_ollama_to_test_multiple_code_generation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T13:59:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qokj2n</id>
    <title>Streaming with Ollama on serverless GPUs seems fundamentally broken . curious how folks think about this</title>
    <updated>2026-01-27T17:34:40+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks , I‚Äôve been seeing a lot of Ollama users trying to run it on ‚Äúserverless‚Äù GPU platforms and hitting the same issue: streaming works great locally, but once Ollama is wrapped behind serverless APIs, streaming disappears.&lt;/p&gt; &lt;p&gt;From what I can tell, this isn‚Äôt really an Ollama limitation. It seems more like an execution-model mismatch. Most serverless GPU platforms treat inference as a batch job (run ‚Üí compute ‚Üí return), which kills long-lived processes and connections ‚Äî exactly what streaming depends on.&lt;/p&gt; &lt;p&gt;Curious how you guys are thinking about this tradeoff. Has anyone found a clean way to preserve token streaming and scale-to-zero semantics, or does this require runtime-level support rather than API tweaks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qokj2n/streaming_with_ollama_on_serverless_gpus_seems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qokj2n/streaming_with_ollama_on_serverless_gpus_seems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qokj2n/streaming_with_ollama_on_serverless_gpus_seems/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T17:34:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qotp4q</id>
    <title>I can't run deepseek-coder-v2 with Ollama. I suspect it has something to do with RAM. Is there any way around this?</title>
    <updated>2026-01-27T23:00:38+00:00</updated>
    <author>
      <name>/u/warpanomaly</name>
      <uri>https://old.reddit.com/user/warpanomaly</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/warpanomaly"&gt; /u/warpanomaly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1qotog1/i_cant_run_deepseekcoderv2_with_ollama_i_suspect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qotp4q/i_cant_run_deepseekcoderv2_with_ollama_i_suspect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qotp4q/i_cant_run_deepseekcoderv2_with_ollama_i_suspect/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T23:00:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoosmm</id>
    <title>Prompt -&gt; Offline Voice AI App in ~11 mins. We forked Expo to bundle native on-device AI runtimes ‚Äî Replit agent builds a fully offline voice assistant</title>
    <updated>2026-01-27T20:01:36+00:00</updated>
    <author>
      <name>/u/thecoder12322</name>
      <uri>https://old.reddit.com/user/thecoder12322</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qoosmm/prompt_offline_voice_ai_app_in_11_mins_we_forked/"&gt; &lt;img alt="Prompt -&amp;gt; Offline Voice AI App in ~11 mins. We forked Expo to bundle native on-device AI runtimes ‚Äî Replit agent builds a fully offline voice assistant" src="https://external-preview.redd.it/I4kx3t6tsAxUZU8Mfm9wDRDqbpUQ0BpsglmKIWPB04Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93eabd046e94e6a2e3e11cdff1a27329133969e5" title="Prompt -&amp;gt; Offline Voice AI App in ~11 mins. We forked Expo to bundle native on-device AI runtimes ‚Äî Replit agent builds a fully offline voice assistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecoder12322"&gt; /u/thecoder12322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h4frf0uk3yfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qoosmm/prompt_offline_voice_ai_app_in_11_mins_we_forked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qoosmm/prompt_offline_voice_ai_app_in_11_mins_we_forked/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T20:01:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp0hex</id>
    <title>Claude code model</title>
    <updated>2026-01-28T03:46:31+00:00</updated>
    <author>
      <name>/u/PropertyLoover</name>
      <uri>https://old.reddit.com/user/PropertyLoover</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What model can I use Linux Ubuntu with 128 GB RAM DDR5 and CPU AMD 8600G? I expect a medium speed and can afford to make a pause to coffee.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PropertyLoover"&gt; /u/PropertyLoover &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp0hex/claude_code_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp0hex/claude_code_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qp0hex/claude_code_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T03:46:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qochnc</id>
    <title>Do ollama models access the internet?</title>
    <updated>2026-01-27T12:28:07+00:00</updated>
    <author>
      <name>/u/Fancy_Purchase_9400</name>
      <uri>https://old.reddit.com/user/Fancy_Purchase_9400</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am new to ollama and I just wanted to know if the models downloaded locally (like mistral:7b) access the internet at all while using them (even if it is for maintaing any kind of logs). I have noticed a small spike in network usage (both for upload and download) while using the model, but I'm not sure if it is due to the usage of local ollama model, so I'm curious to know if it actually accesses the internet quietly. If so, how do I restrict it completely from accessing the internet? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fancy_Purchase_9400"&gt; /u/Fancy_Purchase_9400 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qochnc/do_ollama_models_access_the_internet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qochnc/do_ollama_models_access_the_internet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qochnc/do_ollama_models_access_the_internet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T12:28:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoi1op</id>
    <title>Tiny Ollama-powered CLI: Natural Language to shell commands (qwen2.5-coder default, easy model switch)</title>
    <updated>2026-01-27T16:08:37+00:00</updated>
    <author>
      <name>/u/ykushch</name>
      <uri>https://old.reddit.com/user/ykushch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/"&gt; &lt;img alt="Tiny Ollama-powered CLI: Natural Language to shell commands (qwen2.5-coder default, easy model switch)" src="https://external-preview.redd.it/JnNJEcF3jbp7PevNxANU0riqBFifG0zNxzy_XGtEtCw.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=3d94c3882da0b59d8f2197ce3c151ee720e44138" title="Tiny Ollama-powered CLI: Natural Language to shell commands (qwen2.5-coder default, easy model switch)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CLI that uses Ollama locally to translate natural language into shell commands. Supports &lt;code&gt;--model&lt;/code&gt; / &lt;code&gt;ASK_MODEL&lt;/code&gt; and &lt;code&gt;OLLAMA_HOST&lt;/code&gt;.&lt;br /&gt; Repo: &lt;a href="https://github.com/ykushch/ask"&gt;https://github.com/ykushch/ask&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/k5ldp45d1xfg1.gif"&gt;ask - natural language to shell commands&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ykushch"&gt; /u/ykushch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T16:08:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qousvs</id>
    <title>GPU advice for entry level AI</title>
    <updated>2026-01-27T23:44:30+00:00</updated>
    <author>
      <name>/u/fulefesi</name>
      <uri>https://old.reddit.com/user/fulefesi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My current desktop pc: h77ds3h mobo pcie gen 3, xeon e3 1275v2 4c/8t ivy bridge, 24gb ddr3 1600mhz bundled in old atx case with side vents at bottom and only 1 fan (80mm rear fan)&lt;/p&gt; &lt;p&gt;Purpose: learning, experimenting with entry-level AI, 1‚Äì3B or 7b (if possible) coding LLMs 4-bit quantized + LoRA inference. I only work with Python for data analysis, libraries like pandas, short scripts mainly. Hopefully upgrade entire system + new architecture GPU in 2028&lt;/p&gt; &lt;p&gt;Because of budget constrains and local availability where i'm currently stationed, i have very few contenders (listed as new): rtx 3050 8gb asus tuf (250$), rtx 5060 8gb msi ventus (320$), rtx 3060 12gb asus dual geforce v2 OC (320$)&lt;/p&gt; &lt;p&gt;What/how would you recommend to start with?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fulefesi"&gt; /u/fulefesi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qousvs/gpu_advice_for_entry_level_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qousvs/gpu_advice_for_entry_level_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qousvs/gpu_advice_for_entry_level_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T23:44:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo9tsi</id>
    <title>NotebookLM For Teams</title>
    <updated>2026-01-27T10:04:14+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qo9tsi/notebooklm_for_teams/"&gt; &lt;img alt="NotebookLM For Teams" src="https://external-preview.redd.it/dmgyMjVieGg4dmZnMSLy8o5ur8LGz7971UKmCZkldIebkAvR30ypzPlMaeND.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0496edb208cda2f95e1b3739aef3d3d21e3a42e3" title="NotebookLM For Teams" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.&lt;/p&gt; &lt;p&gt;In short, it is NotebookLM for teams, as it connects any LLM to your internal knowledge sources (search engines, Drive, Calendar, Notion, Obsidian, and 15+ other connectors) and lets you chat with it in real time alongside your team.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here's a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Self-Hostable (with docker support)&lt;/li&gt; &lt;li&gt;Real Time Collaborative Chats&lt;/li&gt; &lt;li&gt;Real Time Commenting&lt;/li&gt; &lt;li&gt;Deep Agentic Agent&lt;/li&gt; &lt;li&gt;RBAC (Role Based Access for Teams Members)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs (OpenAI spec with LiteLLM)&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Local TTS/STT support.&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Slide Creation Support&lt;/li&gt; &lt;li&gt;Multilingual Podcast Support&lt;/li&gt; &lt;li&gt;Video Creation Agent&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zxqevbwh8vfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qo9tsi/notebooklm_for_teams/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qo9tsi/notebooklm_for_teams/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T10:04:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp15k3</id>
    <title>Why does using ollama run claude with glm-4.7-flash have zero memory?</title>
    <updated>2026-01-28T04:17:41+00:00</updated>
    <author>
      <name>/u/ElRayoPeronizador</name>
      <uri>https://old.reddit.com/user/ElRayoPeronizador</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I started claude code with &lt;code&gt;ollama launch claude&lt;/code&gt;, it start and I have access to the prompt, and it will reply when I ask something, but I'm confused about how ollama / claude code handles conversation memory when using the glm-4.7-flash model. It seems like each new command resets the context completely, regardless of previous messages.&lt;/p&gt; &lt;p&gt;Example of the issue:&lt;/p&gt; &lt;p&gt;‚ùØ calculate 1+1&lt;/p&gt; &lt;p&gt;‚óè 1 + 1 = 2&lt;/p&gt; &lt;p&gt;‚ùØ add 4 to that&lt;/p&gt; &lt;p&gt;‚óè I'm not sure what you're referring to. Could you clarify what you'd like me to add 4 to?&lt;/p&gt; &lt;p&gt;Even though I just got &amp;quot;2&amp;quot; in the previous message, the model has no memory of it.&lt;/p&gt; &lt;p&gt;Is this expected behavior? Am I configuring something wrong, or is the glm-4.7-flash model simply not designed for multi-turn conversations? I've been trying to use Claude with local models for longer sessions but the memory seems to be completely reset on every new prompt.&lt;/p&gt; &lt;p&gt;Any insight would be appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElRayoPeronizador"&gt; /u/ElRayoPeronizador &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp15k3/why_does_using_ollama_run_claude_with_glm47flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp15k3/why_does_using_ollama_run_claude_with_glm47flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qp15k3/why_does_using_ollama_run_claude_with_glm47flash/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T04:17:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp0mjn</id>
    <title>I made a open source CLI ollama into terminal</title>
    <updated>2026-01-28T03:53:15+00:00</updated>
    <author>
      <name>/u/Loud-Description-460</name>
      <uri>https://old.reddit.com/user/Loud-Description-460</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qp0mjn/i_made_a_open_source_cli_ollama_into_terminal/"&gt; &lt;img alt="I made a open source CLI ollama into terminal" src="https://external-preview.redd.it/0e9XDVAmFcgNkfKVuVMILaTerqoxbi7p92dyaNqELF8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac1bffbca1b66136acbd5d0bfd9be02470d42ba0" title="I made a open source CLI ollama into terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud-Description-460"&gt; /u/Loud-Description-460 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/guirguispierre/Llaminal"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp0mjn/i_made_a_open_source_cli_ollama_into_terminal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qp0mjn/i_made_a_open_source_cli_ollama_into_terminal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T03:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp390m</id>
    <title>[Update] RX 9070 XT Vulkan vs ROCm: Detailed follow-up with more metrics</title>
    <updated>2026-01-28T06:02:19+00:00</updated>
    <author>
      <name>/u/Due_Pea_372</name>
      <uri>https://old.reddit.com/user/Due_Pea_372</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Follow-up to my &lt;a href="https://www.reddit.com/r/cachyos/comments/1qnpcm1/vulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50/"&gt;previous post&lt;/a&gt;. Improved methodology and more detailed measurements.&lt;/p&gt; &lt;p&gt;**Setup**&lt;/p&gt; &lt;p&gt;- GPU: AMD RX 9070 XT (gfx1201, RDNA4, 16GB)&lt;/p&gt; &lt;p&gt;- Model: qwen3-coder:30b&lt;/p&gt; &lt;p&gt;- Ollama: 0.15.1 (ollama-vulkan / ollama-rocm from AUR)&lt;/p&gt; &lt;p&gt;- OS: Arch Linux&lt;/p&gt; &lt;p&gt;**Methodology**&lt;/p&gt; &lt;p&gt;- Model stopped and reloaded before each session for clean VRAM state&lt;/p&gt; &lt;p&gt;- Baseline VRAM/GTT measured before model load&lt;/p&gt; &lt;p&gt;- First run marked as warmup and excluded from averages&lt;/p&gt; &lt;p&gt;- Power, clock, GTT, GPU-Busy, MEM-Busy sampled every 100ms during inference&lt;/p&gt; &lt;p&gt;**Results**&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Vulkan&lt;/th&gt; &lt;th align="left"&gt;ROCm&lt;/th&gt; &lt;th align="left"&gt;Diff&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Gen t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;51.58&lt;/td&gt; &lt;td align="left"&gt;48.20&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+7.0%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt t/s&lt;/td&gt; &lt;td align="left"&gt;855.96&lt;/td&gt; &lt;td align="left"&gt;865.39&lt;/td&gt; &lt;td align="left"&gt;-1.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Power&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;65.3 W&lt;/td&gt; &lt;td align="left"&gt;149.4 W&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;-56%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.79 t/W&lt;/td&gt; &lt;td align="left"&gt;0.32 t/W&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+145%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU-Clock&lt;/td&gt; &lt;td align="left"&gt;1988 MHz&lt;/td&gt; &lt;td align="left"&gt;3621 MHz&lt;/td&gt; &lt;td align="left"&gt;-45%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU-Busy&lt;/td&gt; &lt;td align="left"&gt;33%&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;-67%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MEM-Busy&lt;/td&gt; &lt;td align="left"&gt;12%&lt;/td&gt; &lt;td align="left"&gt;8%&lt;/td&gt; &lt;td align="left"&gt;+50%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;VRAM (actual)&lt;/td&gt; &lt;td align="left"&gt;15703 MB&lt;/td&gt; &lt;td align="left"&gt;16024 MB&lt;/td&gt; &lt;td align="left"&gt;-2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Temp&lt;/td&gt; &lt;td align="left"&gt;49¬∞C&lt;/td&gt; &lt;td align="left"&gt;47¬∞C&lt;/td&gt; &lt;td align="left"&gt;+4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TTFT (warm)&lt;/td&gt; &lt;td align="left"&gt;25.8 ms&lt;/td&gt; &lt;td align="left"&gt;25.4 ms&lt;/td&gt; &lt;td align="left"&gt;+2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TTFT (cold)&lt;/td&gt; &lt;td align="left"&gt;89.9 ms&lt;/td&gt; &lt;td align="left"&gt;77.9 ms&lt;/td&gt; &lt;td align="left"&gt;+15%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;**Key findings**&lt;/p&gt; &lt;p&gt;**1. ROCm works harder, not smarter**&lt;br /&gt; ROCm runs at nearly 2x the clock speed (3621 vs 1988 MHz) with 100% GPU utilization yet delivers worse performance than Vulkan at 33% utilization. This suggests the ROCm compute kernels are not optimized for RDNA4.&lt;/p&gt; &lt;p&gt;**2. Power efficiency is dramatic**&lt;br /&gt; Vulkan delivers 0.79 tokens per watt vs ROCm's 0.32 t/W ‚Äì that's 145% better efficiency.&lt;/p&gt; &lt;p&gt;**3. Prompt processing is identical**&lt;br /&gt; Both backends process prompts at ~860 t/s, so the inefficiency is specifically in token generation.&lt;/p&gt; &lt;p&gt;**Conclusion**&lt;br /&gt; For RDNA4 users: use Vulkan. You get better performance while your GPU runs cooler and quieter.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Pea_372"&gt; /u/Due_Pea_372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp390m/update_rx_9070_xt_vulkan_vs_rocm_detailed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp390m/update_rx_9070_xt_vulkan_vs_rocm_detailed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qp390m/update_rx_9070_xt_vulkan_vs_rocm_detailed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T06:02:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpffvq</id>
    <title>How to Prevent Context Degradation on Local LLM?</title>
    <updated>2026-01-28T16:05:27+00:00</updated>
    <author>
      <name>/u/Ok_Most9659</name>
      <uri>https://old.reddit.com/user/Ok_Most9659</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have run a few local LLMs, which start off great, but inevitably go off the rails as the context window grows resulting in illogical nonsense answers to my prompts. My understanding is this is Context Degradation and is related to the system running out of RAM to hold the contents of the context window. &lt;/p&gt; &lt;p&gt;What I don't understand is why does the system/LLM not simply off-load the prior context window into a file to not consume RAM, then reference that file via RAG when the next prompt is entered within that same context window?&lt;br /&gt; Would this not alleviate the RAM issue and let you continue within the same context window until your harddrive fills? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Most9659"&gt; /u/Ok_Most9659 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpffvq/how_to_prevent_context_degradation_on_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpffvq/how_to_prevent_context_degradation_on_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qpffvq/how_to_prevent_context_degradation_on_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T16:05:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qphrp7</id>
    <title>Ollama cloud / Kimi 2.5 / MoltBolt</title>
    <updated>2026-01-28T17:26:34+00:00</updated>
    <author>
      <name>/u/ayoubhak</name>
      <uri>https://old.reddit.com/user/ayoubhak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;Please excuse my basic understanding, but I‚Äôm looking for an efficient way to use an agent to breakdown my idea into actionable products.&lt;/p&gt; &lt;p&gt;Since I own a MacBook m4, I won‚Äôt be able to run the kimi 2.5 in this machine as a model with moltbolt, however I can subscribe to ollama cloud and have it use this model and I can basically just text it or share product ideas from Pinterest.&lt;/p&gt; &lt;p&gt;Please let me know if my reasoning is correct, and also I still have no idea if I‚Äôll hit quickly a limit with 20$ subscription using this model. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayoubhak"&gt; /u/ayoubhak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qphrp7/ollama_cloud_kimi_25_moltbolt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qphrp7/ollama_cloud_kimi_25_moltbolt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qphrp7/ollama_cloud_kimi_25_moltbolt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T17:26:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpj5nd</id>
    <title>I need help better undestanding how Ollama works</title>
    <updated>2026-01-28T18:12:59+00:00</updated>
    <author>
      <name>/u/Super_Nova02</name>
      <uri>https://old.reddit.com/user/Super_Nova02</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm new to the world of LLM and I'm currently trying to create a simple chatbot. The chatbot needs to follow the mcp and I've chosen Ollama with gemma3 as model. &lt;/p&gt; &lt;p&gt;I have an index.js file in which I manage the request forwarded by my client. The whole project is managed by Docker, and in the code below I pasted the setup. &lt;/p&gt; &lt;p&gt;Right now my whole system goes up. I manually check for an healthy service by Ollama and then I send the request &amp;quot;get sensors&amp;quot;. I receive this error: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Error chatting with Ollama: TypeError: fetch failed&lt;br /&gt; at node:internal/deps/undici/undici:12637:11&lt;br /&gt; at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&lt;br /&gt; at async post (file:///usr/src/app/node_modules/ollama/dist/browser.mjs:150:20)&lt;br /&gt; at async Ollama.processStreamableRequest (file:///usr/src/app/node_modules/ollama/dist/browser.mjs:297:22)&lt;br /&gt; at async file:///usr/src/app/index.js:22:22 {&lt;br /&gt; cause: Error: connect ECONNREFUSED &lt;a href="http://127.0.0.1:11434"&gt;127.0.0.1:11434&lt;/a&gt;&lt;br /&gt; at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1555:16) {&lt;br /&gt; errno: -111,&lt;br /&gt; code: 'ECONNREFUSED',&lt;br /&gt; syscall: 'connect',&lt;br /&gt; address: '127.0.0.1',&lt;br /&gt; port: 11434&lt;br /&gt; }&lt;br /&gt; }&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I don't really understand what I'm doing wrog, if you have an idea and want to share, thank you!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;//docker-compose.yml ollama: image: ollama/ollama:latest container_name: ollama ports: - 11434:11434 networks: - app-network mcp-server: image: node:18-alpine container_name: air-quality-map-mcp ports: - &amp;quot;4000:4000&amp;quot; depends_on: server: condition: service_healthy mongodb: condition: service_started environment: BACKEND_URL: http://server:3000 MONGO_URI: mongodb://mongodb:27017 OLLAMA_API_URL: http://ollama:11434 networks: - app-network working_dir: /usr/src/app volumes: - ./services/mcp-server:/usr/src/app command: &amp;gt; sh -c &amp;quot; npm install --legacy-peer-deps &amp;amp;&amp;amp; node index.js &amp;quot; healthcheck: test: [&amp;quot;CMD&amp;quot;, &amp;quot;wget&amp;quot;, &amp;quot;-qO-&amp;quot;, &amp;quot;http://localhost:4000/health&amp;quot;] interval: 10s timeout: 5s retries: 10 //index.js import express from &amp;quot;express&amp;quot;; import fetch from &amp;quot;node-fetch&amp;quot;; import ollama from &amp;quot;ollama&amp;quot;; const app = express(); app.use(express.json()); const OLLAMA_API_URL = process.env.OLLAMA_API_URL || &amp;quot;http://ollama:11434&amp;quot;; const BACKEND_URL = process.env.BACKEND_URL || &amp;quot;http://server:3000&amp;quot;; const ollamaClient = ollama; app.post(&amp;quot;/chat&amp;quot;, async (req, res) =&amp;gt; { const userMessage = req.body.message; console.log(&amp;quot;User message:&amp;quot;, userMessage); console.log(&amp;quot;Using Ollama API URL:&amp;quot;, OLLAMA_API_URL); try { const response = await ollamaClient.chat({ model: &amp;quot;gemma3&amp;quot;, messages: [{ role: &amp;quot;user&amp;quot;, content: userMessage }], baseUrl: OLLAMA_API_URL, }); const message = response.message?.content || &amp;quot;&amp;quot;; if (message.toLowerCase().includes(&amp;quot;get sensors&amp;quot;)) { const r = await fetch(`${BACKEND_URL}/tools/getSensors`, { method: &amp;quot;POST&amp;quot;, headers: { &amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot; }, body: JSON.stringify({}), }); const data = await r.json(); return res.json({ answer: &amp;quot;Here are the sensors from the system.&amp;quot;, data: data.result, }); } res.json({ answer: message }); } catch (err) { console.error(&amp;quot;Error chatting with Ollama:&amp;quot;, err); res.status(500).json({ error: &amp;quot;Failed to get response from model&amp;quot; }); } }); app.get(&amp;quot;/health&amp;quot;, (req, res) =&amp;gt; res.send(&amp;quot;OK&amp;quot;)); app.listen(4000, () =&amp;gt; { console.log(&amp;quot;MCP server running on port 4000&amp;quot;); }); &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Super_Nova02"&gt; /u/Super_Nova02 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpj5nd/i_need_help_better_undestanding_how_ollama_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpj5nd/i_need_help_better_undestanding_how_ollama_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qpj5nd/i_need_help_better_undestanding_how_ollama_works/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T18:12:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpkcd6</id>
    <title>What is the best model that I can run</title>
    <updated>2026-01-28T18:54:05+00:00</updated>
    <author>
      <name>/u/Legitimate_Worry5069</name>
      <uri>https://old.reddit.com/user/Legitimate_Worry5069</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a base 5060 8gb and was wondering which is the best model.i can run. I've heard terms like quantized and unquantized and 8b and 70b and so on. (I'm doing this just for fun and it does seem cool)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Legitimate_Worry5069"&gt; /u/Legitimate_Worry5069 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpkcd6/what_is_the_best_model_that_i_can_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpkcd6/what_is_the_best_model_that_i_can_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qpkcd6/what_is_the_best_model_that_i_can_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T18:54:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp7m9z</id>
    <title>Kimi K2.5 just blew my mind</title>
    <updated>2026-01-28T10:19:36+00:00</updated>
    <author>
      <name>/u/ComfyTightwad</name>
      <uri>https://old.reddit.com/user/ComfyTightwad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qp7m9z/kimi_k25_just_blew_my_mind/"&gt; &lt;img alt="Kimi K2.5 just blew my mind" src="https://external-preview.redd.it/W-LGhFmxh1k5FgyDxkGaY8p5gAgHMHyYGe0H34d64kM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5347aa6b8161bbfd06d77d438d3da9013013168a" title="Kimi K2.5 just blew my mind" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComfyTightwad"&gt; /u/ComfyTightwad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8o6c6wace2gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp7m9z/kimi_k25_just_blew_my_mind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qp7m9z/kimi_k25_just_blew_my_mind/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T10:19:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp9c9x</id>
    <title>AI started speaking in Russian out of nowhere</title>
    <updated>2026-01-28T11:54:45+00:00</updated>
    <author>
      <name>/u/No-Sky2462</name>
      <uri>https://old.reddit.com/user/No-Sky2462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qp9c9x/ai_started_speaking_in_russian_out_of_nowhere/"&gt; &lt;img alt="AI started speaking in Russian out of nowhere" src="https://preview.redd.it/jckp73fxv2gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3eaf242a017b1f3ab7b8bb92952977659fe2d5e" title="AI started speaking in Russian out of nowhere" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I am a hobbyist, who is interested in AI and LLM. I don't know much about how these LLM function but i was interested so i installed one using terminal. The installation completed successfully but the ai started speaking in russian out of nowhere. Is this intentional behaviour for a LLM or did i do something wrong? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Sky2462"&gt; /u/No-Sky2462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jckp73fxv2gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp9c9x/ai_started_speaking_in_russian_out_of_nowhere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qp9c9x/ai_started_speaking_in_russian_out_of_nowhere/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T11:54:45+00:00</published>
  </entry>
</feed>
