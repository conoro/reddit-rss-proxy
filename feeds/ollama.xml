<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-10-14T21:23:02+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1o3w4mk</id>
    <title>Split GGUF Files</title>
    <updated>2025-10-11T13:49:28+00:00</updated>
    <author>
      <name>/u/crossijinn</name>
      <uri>https://old.reddit.com/user/crossijinn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it now possible to import split gguf files into Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crossijinn"&gt; /u/crossijinn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3w4mk/split_gguf_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3w4mk/split_gguf_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o3w4mk/split_gguf_files/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-11T13:49:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4jixb</id>
    <title>LM Studio has launched on iOS—that's awesome</title>
    <updated>2025-10-12T08:14:04+00:00</updated>
    <author>
      <name>/u/Few-Independence-234</name>
      <uri>https://old.reddit.com/user/Few-Independence-234</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o4jixb/lm_studio_has_launched_on_iosthats_awesome/"&gt; &lt;img alt="LM Studio has launched on iOS—that's awesome" src="https://b.thumbs.redditmedia.com/-oiKnH8ZLAfa6k3PXH_5wwgK_mfyG6p7sjBrleGdZxw.jpg" title="LM Studio has launched on iOS—that's awesome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think I saw that LM Studio is now available on iPhone—that's absolutely fantastic!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z9kf4l5a3nuf1.png?width=1660&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fe0eb30e7dc28a12c590a17bf197fca517357cac"&gt;https://preview.redd.it/z9kf4l5a3nuf1.png?width=1660&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fe0eb30e7dc28a12c590a17bf197fca517357cac&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Independence-234"&gt; /u/Few-Independence-234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4jixb/lm_studio_has_launched_on_iosthats_awesome/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4jixb/lm_studio_has_launched_on_iosthats_awesome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o4jixb/lm_studio_has_launched_on_iosthats_awesome/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-12T08:14:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3vahv</id>
    <title>Fighting Email Spam on Your Mail Server with LLMs — Privately</title>
    <updated>2025-10-11T13:11:49+00:00</updated>
    <author>
      <name>/u/unixf0x</name>
      <uri>https://old.reddit.com/user/unixf0x</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unixf0x"&gt; /u/unixf0x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cybercarnet.eu/posts/email-spam-llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3vahv/fighting_email_spam_on_your_mail_server_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o3vahv/fighting_email_spam_on_your_mail_server_with_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-11T13:11:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o42jnm</id>
    <title>Built an MCP tool that connects Qwen, Claude Code, Codex, Gemini to a shared memory database.</title>
    <updated>2025-10-11T18:11:11+00:00</updated>
    <author>
      <name>/u/Alone-Biscotti6145</name>
      <uri>https://old.reddit.com/user/Alone-Biscotti6145</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1o42jnm/video/roi91aiwviuf1/player"&gt;https://reddit.com/link/1o42jnm/video/roi91aiwviuf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(TL:DR) I know the mcp tool I built is useful and can help a ton of people with their workflow and I'm looking for marketing/promoting advice.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Repo - &lt;a href="https://github.com/Lyellr88/MARM-Systems"&gt;https://github.com/Lyellr88/MARM-System&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Stats - 176 stars, 32 forks, 398 docker pulls and 2026 pip installs&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;What is MARM?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MARM is a production-ready Universal MCP Server that gives AI agents persistent, cross-platform memory. It's built on SQLite with vector embeddings for semantic search, meaning your AI can find information by meaning, not just keywords.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Universal memory layer - Works with Claude, Gemini, Qwen, any MCP client&lt;/li&gt; &lt;li&gt;Persistent cross-session - Memories survive restarts and container rebuilds&lt;/li&gt; &lt;li&gt;Semantic search - Sentence transformers (all-MiniLM-L6-v2) for intelligent recall&lt;/li&gt; &lt;li&gt;Cross-platform - One database, multiple AIs can read/write&lt;/li&gt; &lt;li&gt;Production architecture - WAL mode, connection pooling, rate limiting&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;5-Table Schema:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;memories - Core memory storage with vector embeddings&lt;/li&gt; &lt;li&gt;sessions - Session management and MARM activation state&lt;/li&gt; &lt;li&gt;Log_entries - Structured session logs with auto-dating&lt;/li&gt; &lt;li&gt;notebook_entries - Reusable instructions with semantic search&lt;/li&gt; &lt;li&gt;User_settings - Configuration and preferences &lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;18 complete MCP tools. FastAPI backend. Docker-ready. 8 months of building.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;What Users Are Saying:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;&lt;em&gt;MARM successfully handles our industrial automation workflows in production. Validated session management, persistent logging, and smart recall across container restarts. Reliably tracks complex technical decisions through deployment cycles&lt;/em&gt;.&amp;quot; — &lt;a href="/u/Ophy21"&gt;u/Ophy21&lt;/a&gt; (Industrial Automation Engineer)&lt;/p&gt; &lt;p&gt;&amp;quot;&lt;em&gt;100% memory accuracy across 46 services. Semantic search and automated session logs made solving async and infrastructure issues far easier. Value Rating: 9.5/10 - indispensable for enterprise-grade memory&lt;/em&gt;.&amp;quot; — &lt;a href="/u/joe_nyc"&gt;u/joe_nyc&lt;/a&gt; (DevOps/Infrastructure Engineer)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Problem:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm a builder, not an experienced marketer. I've spent 6 months building MARM into production-ready infrastructure (2,500+ lines, Pip and Docker deployment, semantic search working), but I have no idea how to get users.&lt;/p&gt; &lt;p&gt;I've tried:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Reddit posts&lt;/li&gt; &lt;li&gt;Twitter&lt;/li&gt; &lt;li&gt;Waiting for organic discovery (doesn't happen)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How do you market technical tools without it feeling spammy?&lt;/p&gt; &lt;p&gt;Also i am open to finding a marketing co-founder who can help take this to the next level. I can build, but this project deserves better visibility than I can give it alone.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker pull lyellr88/marm-mcp-server:latest docker run -d -p 8001:8001 -v ~/.marm:/home/marm/.marm lyellr88/marm-mcp-server:latest claude mcp add --transport http marm-memory http://localhost:8001/mcp pip install marm-mcp-server==2.2.6 marm-mcp-server claude mcp add --transport http marm-memory http://localhost:8001/mcp &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Biscotti6145"&gt; /u/Alone-Biscotti6145 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o42jnm/built_an_mcp_tool_that_connects_qwen_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o42jnm/built_an_mcp_tool_that_connects_qwen_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o42jnm/built_an_mcp_tool_that_connects_qwen_claude_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-11T18:11:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4ea93</id>
    <title>Worthwhile using Ollama without nVidia?</title>
    <updated>2025-10-12T03:02:05+00:00</updated>
    <author>
      <name>/u/inspector71</name>
      <uri>https://old.reddit.com/user/inspector71</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Update: shortly after this post, I noticed Mozilla has a benchmarking tool, site for helping to answer this question.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.localscore.ai/"&gt;https://www.localscore.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;...&lt;/p&gt; &lt;p&gt;I see the installer takes quite a while to download and install cuda.&lt;/p&gt; &lt;p&gt;I want to run Ollama to give me access to free models I can run locally to power a VS Code developer scenario. Agentic analysis, chat, code suggestion / completion, that sort of thing.&lt;/p&gt; &lt;p&gt;Is it worthwhile running Ollama on an AMD laptop without a discrete GPU ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inspector71"&gt; /u/inspector71 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4ea93/worthwhile_using_ollama_without_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4ea93/worthwhile_using_ollama_without_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o4ea93/worthwhile_using_ollama_without_nvidia/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-12T03:02:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3waak</id>
    <title>Anyone here building Agentic AI into their office workflow? How’s it going so far?</title>
    <updated>2025-10-11T13:56:32+00:00</updated>
    <author>
      <name>/u/Savings-Internal-297</name>
      <uri>https://old.reddit.com/user/Savings-Internal-297</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, is anyone here integrating Agentic AI into their office workflow or internal operations? If yes, how successful has it been so far?&lt;/p&gt; &lt;p&gt;Would like to hear what kind of use cases you are focusing on (automation, document handling, task management,) and what challenges or success you have seen.&lt;/p&gt; &lt;p&gt;Trying to get some real world insights before we start experimenting with it in our company.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savings-Internal-297"&gt; /u/Savings-Internal-297 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3waak/anyone_here_building_agentic_ai_into_their_office/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3waak/anyone_here_building_agentic_ai_into_their_office/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o3waak/anyone_here_building_agentic_ai_into_their_office/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-11T13:56:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4e004</id>
    <title>I want to create an AI tools that can create and manage project. See scenario below</title>
    <updated>2025-10-12T02:46:55+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to have something like this.&lt;/p&gt; &lt;p&gt;At the AI prompt, I will type in - create a new project&lt;/p&gt; &lt;p&gt;AI will response - please enter the project name, description, name of sponsor, name of manager, planned start and finish date.&lt;/p&gt; &lt;p&gt;User will enter the data and &amp;lt;enter&amp;gt;&lt;/p&gt; &lt;p&gt;AI will ask again - enter the project budget&lt;/p&gt; &lt;p&gt;Use will response with data&lt;/p&gt; &lt;p&gt;AI will then say - your project has been created with with project id zxxx&lt;/p&gt; &lt;p&gt;So user can create as many projects as desired and the AI will assign project id accordingly.&lt;/p&gt; &lt;p&gt;Next, user can create status report for the project.&lt;/p&gt; &lt;p&gt;User will type create status report&lt;/p&gt; &lt;p&gt;AI will ask - please enter the project id. If not sure type 'List projects'&lt;/p&gt; &lt;p&gt;User will enter project id&lt;/p&gt; &lt;p&gt;AI will ask - do you want to pull the last report you submitted or this is a new report&lt;/p&gt; &lt;p&gt;User will answer - new report&lt;/p&gt; &lt;p&gt;AI will response enter the followings - general health of project - issues - deliverables completed this month - deliverables to be completed next month - deliverables pending&lt;/p&gt; &lt;p&gt;And so on..&lt;/p&gt; &lt;p&gt;So you get the idea..&lt;/p&gt; &lt;p&gt;What is my best option to develop such AI tools?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4e004/i_want_to_create_an_ai_tools_that_can_create_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4e004/i_want_to_create_an_ai_tools_that_can_create_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o4e004/i_want_to_create_an_ai_tools_that_can_create_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-12T02:46:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4n8t1</id>
    <title>Is the GPU compatibility list up to date?</title>
    <updated>2025-10-12T12:01:04+00:00</updated>
    <author>
      <name>/u/PeterShowFull</name>
      <uri>https://old.reddit.com/user/PeterShowFull</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Greetings.&lt;/p&gt; &lt;p&gt;I noticed that, according to the latest version of &lt;a href="https://github.com/ollama/ollama/blob/main/docs/gpu.md"&gt;docs/gpu.md&lt;/a&gt; (from &lt;a href="https://github.com/ollama/ollama/pull/12567"&gt;this PR&lt;/a&gt;), the AMD list only lists models from the RX 6000 and RX 7000 series, apart from the Radeon Pro and Radeon Instinct series.&lt;/p&gt; &lt;p&gt;Is this up to date? Is there currently no support for the RX 9000 series?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeterShowFull"&gt; /u/PeterShowFull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4n8t1/is_the_gpu_compatibility_list_up_to_date/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4n8t1/is_the_gpu_compatibility_list_up_to_date/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o4n8t1/is_the_gpu_compatibility_list_up_to_date/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-12T12:01:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4sm2e</id>
    <title>How to create AI application with own data</title>
    <updated>2025-10-12T15:52:16+00:00</updated>
    <author>
      <name>/u/Rough_Knowledge7</name>
      <uri>https://old.reddit.com/user/Rough_Knowledge7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to know about AI Application Development with Own data as PDF,CSV,XLSX,..Documents to understand and maintenance of data processing and response with chat and other services. I was downloaded Ollama3 &amp;amp; SEA LION model to my MacBook Pro 19 with Ram 16 and Intel chips to use for development it. What I need to know and how to use free open source AI model to integrate with application to use for personal application.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rough_Knowledge7"&gt; /u/Rough_Knowledge7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4sm2e/how_to_create_ai_application_with_own_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4sm2e/how_to_create_ai_application_with_own_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o4sm2e/how_to_create_ai_application_with_own_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-12T15:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4yoi9</id>
    <title>Mac mini plus MacBook Pro</title>
    <updated>2025-10-12T19:44:52+00:00</updated>
    <author>
      <name>/u/Key_Distribution_167</name>
      <uri>https://old.reddit.com/user/Key_Distribution_167</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all I am new to local LLMs and I am wondering if I can connect my Mac mini to my MacBook Pro to be able to utilize more ram to run larger models. For context I have a Mac mini with a m4 pro chip and 64gb of ram and have a MacBook Pro also with the M4 pro chip with 24gb of ram. The reason I am inquiring about this is because I would like to have more power when I travel without having to pack a monitor keyboard etc. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Key_Distribution_167"&gt; /u/Key_Distribution_167 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4yoi9/mac_mini_plus_macbook_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4yoi9/mac_mini_plus_macbook_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o4yoi9/mac_mini_plus_macbook_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-12T19:44:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4eql9</id>
    <title>Why You Should Build AI Agents with Ollama First</title>
    <updated>2025-10-12T03:26:17+00:00</updated>
    <author>
      <name>/u/Previous_Comfort_447</name>
      <uri>https://old.reddit.com/user/Previous_Comfort_447</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: Distinguishing between AI model limitations and engineering limitations can be hard for AI services. Build AI Agents with Ollama first to understand the architecture risks in the early stage.&lt;/p&gt; &lt;h2&gt;The AI PoC Paradox: High Effort, Low ROI&lt;/h2&gt; &lt;p&gt;Building AI Proofs of Concept (PoCs) has become routine in many DX departments. With the rapid evolution of LLM models, more and more AI agents with new capabilities come every day. But Return on Investment (ROI) doesn’t change in the same way. Why is that?&lt;/p&gt; &lt;p&gt;One reason might be that while LLM capabilities are advancing at breakneck speed, our AI engineering techniques for bridging these powerful models with real-world problems are lagging. We get excited about new features and use cases enabled by the latest models, but real-world returns remains unimproved due to a lack of robust engineering practices.&lt;/p&gt; &lt;h2&gt;Simulating Real-World Constraints with Ollama&lt;/h2&gt; &lt;p&gt;So, how can we estimate the real-world accuracy of our AI PoCs? One easy approach is to start building your AI agents with Ollama. Ollama allows you to run a selection of LLM models locally with limited resource requirements. By beginning with Ollama, you face the challenges of difficult input from users in the early stage. Those challenges may remain hidden when a powerful LLM is used.&lt;/p&gt; &lt;p&gt;The limitation made visible are context window size (input being too long) and scalability (ignored small overheads become innegligible):&lt;/p&gt; &lt;h3&gt;Realistic Context Handling&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Realistic Context Handling:&lt;/strong&gt; Ollama's local execution has a default 4K context window size. Unlike cloud-based models with infinite contexts that can hide over-size retrieved context, Ollama exposes the out-of-size issue early. This helps developers understand what are the possible pitfalls in Retrieval Augmented Generation (RAG), ensures that an AI agent delivers good results even when some accidents happens. &lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Confronting Improper Workflow&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Confronting Improper Workflow:&lt;/strong&gt; The inference speeds on Ollama, around 20 tokens/second for a 4B model on a powerful CPU-only PC. Generating a summary take tens of seconds, which is just right. You won’t feel slow if LLM workflow is as you expected. And you will immediately feel strange if the agent gets into unnecessary loops or side tasks. Cloud services like ChatGPT and Claude infer so rapidly that bad workflow loops may only feel like a 10-second pause. Average PCs expose slow parts in apps. And average LLMs expose slow workflows.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Navigating Production Transition and Migration&lt;/h2&gt; &lt;p&gt;Even if you're persuaded by the benefits, you might worry about the cost of migrating an Ollama AI service to OpenAI LLMs and cloud platforms like AWS. You can start with local AWS to reduce costs. Standard cloud components like S3 and Lambda have readily available local alternatives, such as those provided by LocalStack. &lt;/p&gt; &lt;p&gt;However, if your architecture relies on specific cloud provider tweaks or runs on platforms like Azure, the migration might require more effort. Ollama may not be a good option for you.&lt;/p&gt; &lt;p&gt;Nevertheless, even without using Ollama, limiting your model choice to under 14B parameters can be beneficial for accurately assessing PoC efficacy early on.&lt;/p&gt; &lt;p&gt;Have fun experimenting with your AI PoCs!&lt;/p&gt; &lt;p&gt;Original Blog: &lt;a href="https://alroborol.github.io/en/blog/post-3/"&gt;https://alroborol.github.io/en/blog/post-3/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And my other blogs: &lt;a href="https://alroborol.github.io/en/blog"&gt;https://alroborol.github.io/en/blog&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Previous_Comfort_447"&gt; /u/Previous_Comfort_447 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4eql9/why_you_should_build_ai_agents_with_ollama_first/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4eql9/why_you_should_build_ai_agents_with_ollama_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o4eql9/why_you_should_build_ai_agents_with_ollama_first/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-12T03:26:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4wdgu</id>
    <title>Announcing Llamazing: Your Ollama and ComfyUI server on IOS!</title>
    <updated>2025-10-12T18:16:02+00:00</updated>
    <author>
      <name>/u/mandrak4</name>
      <uri>https://old.reddit.com/user/mandrak4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o4wdgu/announcing_llamazing_your_ollama_and_comfyui/"&gt; &lt;img alt="Announcing Llamazing: Your Ollama and ComfyUI server on IOS!" src="https://b.thumbs.redditmedia.com/8aoLLo4IeKUR_UTP4CRsnHcZlVDu72mHuUTke2VDz3w.jpg" title="Announcing Llamazing: Your Ollama and ComfyUI server on IOS!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mandrak4"&gt; /u/mandrak4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1o4wae8/announcing_llamazing_your_ollama_and_comfyui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o4wdgu/announcing_llamazing_your_ollama_and_comfyui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o4wdgu/announcing_llamazing_your_ollama_and_comfyui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-12T18:16:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5503i</id>
    <title>I Need a Very Simple Setup</title>
    <updated>2025-10-13T00:10:46+00:00</updated>
    <author>
      <name>/u/booknerdcarp</name>
      <uri>https://old.reddit.com/user/booknerdcarp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to use local Ollama models in my terminal to do some coding. I need read/write capabilities to my project folder in a chat type interface. I'm new to this so just need some guidance. I tried Ollama moles in Roo and Kilo in VSC but they just throw errors all the time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/booknerdcarp"&gt; /u/booknerdcarp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o5503i/i_need_a_very_simple_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o5503i/i_need_a_very_simple_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o5503i/i_need_a_very_simple_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-13T00:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o59owp</id>
    <title>0.12.2 and later are MUCH slower on prompt evaluation</title>
    <updated>2025-10-13T04:02:13+00:00</updated>
    <author>
      <name>/u/Maltz42</name>
      <uri>https://old.reddit.com/user/Maltz42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever since Qwen3 has switched to the new engine in 0.12.2, the prompt evaluation seems to be happening on the CPU instead of the GPU on models too big to fit in VRAM alone. Is this intended behavior for the new engine, trading prompt evaluation performance for improved inference? From my testing, that's only a good tradeoff when the prompt/context is quite small.&lt;/p&gt; &lt;p&gt;Under 0.12.1:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;VRAM allocation has more free space reserved for the context window. The larger the context window, the more space is reserved&lt;/li&gt; &lt;li&gt;During prompt evaluation, only one CPU core is used.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Under 0.12.2 through 0.12.5:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;VRAM is nearly fully allocated, leaving no space for the context window.&lt;/li&gt; &lt;li&gt;During prompt evaluation all CPU cores are pegged.&lt;/li&gt; &lt;li&gt;Prompt evaluation time in my specific case take 5x longer, taking total response time from 4 minutes to over 20.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've tried setting OLLAMA_NEW_ENGINE=0, but it seems to have no effect. If I also turn off ollama_new_estimates and ollama_flash_attention, it helps, but it's still primarily CPU and still much slower. Anyone have some ideas, other than reverting to 0.12.1? I don't imagine that will be a good option forever.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maltz42"&gt; /u/Maltz42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o59owp/0122_and_later_are_much_slower_on_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o59owp/0122_and_later_are_much_slower_on_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o59owp/0122_and_later_are_much_slower_on_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-13T04:02:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5mw4f</id>
    <title>The models I downloaded don't load</title>
    <updated>2025-10-13T15:29:32+00:00</updated>
    <author>
      <name>/u/starburstgamma</name>
      <uri>https://old.reddit.com/user/starburstgamma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two days ago I downloaded Ollama on Windows and I downloaded llama2 and dolphin phi, but when I enter a prompt it doesn't respond. The Ollama interface just freezes, while on my terminal only a loading icon appears. I waited for 20 minutes but it still doesn't work. Does anyone know why this happens?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/starburstgamma"&gt; /u/starburstgamma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o5mw4f/the_models_i_downloaded_dont_load/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o5mw4f/the_models_i_downloaded_dont_load/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o5mw4f/the_models_i_downloaded_dont_load/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-13T15:29:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5pwrb</id>
    <title>Retrieval-Augmented Generation with LangChain and Ollama: Generating SQL Queries from Natural Language</title>
    <updated>2025-10-13T17:17:29+00:00</updated>
    <author>
      <name>/u/ciazo-4942</name>
      <uri>https://old.reddit.com/user/ciazo-4942</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;br /&gt; I’m currently building a chatbot for my company that interfaces with our structured SQL database. The idea is to take user questions, generate SQL queries using LangChain, retrieve data, and then convert those results back into natural language answers with an LLM.&lt;/p&gt; &lt;p&gt;I’ve tested this workflow with Google Gemini’s API, and it works really well—responses are fast and accurate, which makes sense since it’s a powerful cloud service. But when I try using Ollama, which we run on our own server (64GB RAM, 12 CPU cores), the results are disappointing: it takes 5-6 minutes to respond, and more often than not it fails to generate a correct SQL query or returns no useful results at all.&lt;/p&gt; &lt;p&gt;We’ve tried tweaking prompts, adjusting context size, and even different Ollama models, but nothing really helps. I’m curious if anyone here has successfully used Ollama for similar tasks, especially SQL query generation or chatbot workflows involving structured data? How does it hold up in production scenarios where speed and reliability matter?&lt;/p&gt; &lt;p&gt;Any insights or recommendations would be really appreciated!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ciazo-4942"&gt; /u/ciazo-4942 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o5pwrb/retrievalaugmented_generation_with_langchain_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o5pwrb/retrievalaugmented_generation_with_langchain_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o5pwrb/retrievalaugmented_generation_with_langchain_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-13T17:17:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5knq8</id>
    <title>Which open source model is best for content writing?</title>
    <updated>2025-10-13T14:06:22+00:00</updated>
    <author>
      <name>/u/Green_Ad6024</name>
      <uri>https://old.reddit.com/user/Green_Ad6024</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Everyone, Could anyone suggest best open source model for content writing.?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Green_Ad6024"&gt; /u/Green_Ad6024 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o5knq8/which_open_source_model_is_best_for_content/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o5knq8/which_open_source_model_is_best_for_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o5knq8/which_open_source_model_is_best_for_content/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-13T14:06:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6bvd6</id>
    <title>IBM Graphite 4 thinks it's developed by OpenAI. LoL</title>
    <updated>2025-10-14T10:31:45+00:00</updated>
    <author>
      <name>/u/tusharkant15</name>
      <uri>https://old.reddit.com/user/tusharkant15</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o6bvd6/ibm_graphite_4_thinks_its_developed_by_openai_lol/"&gt; &lt;img alt="IBM Graphite 4 thinks it's developed by OpenAI. LoL" src="https://b.thumbs.redditmedia.com/S0TDqr1lMKa9XBdxCBuXvbJWACcNKm-ZGvRkZXROumg.jpg" title="IBM Graphite 4 thinks it's developed by OpenAI. LoL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/qnovip2o12vf1.png?width=1414&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8288713be8c23a7dc44006ad7da741bddfc84e6"&gt;https://preview.redd.it/qnovip2o12vf1.png?width=1414&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8288713be8c23a7dc44006ad7da741bddfc84e6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tusharkant15"&gt; /u/tusharkant15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6bvd6/ibm_graphite_4_thinks_its_developed_by_openai_lol/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6bvd6/ibm_graphite_4_thinks_its_developed_by_openai_lol/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o6bvd6/ibm_graphite_4_thinks_its_developed_by_openai_lol/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-14T10:31:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5uqzp</id>
    <title>I love Ollama, but why all the hate from other frontends?</title>
    <updated>2025-10-13T20:10:40+00:00</updated>
    <author>
      <name>/u/Atari-Katana</name>
      <uri>https://old.reddit.com/user/Atari-Katana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love Ollama, but it seems to get a lot of hate. What's up with that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Atari-Katana"&gt; /u/Atari-Katana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o5uqzp/i_love_ollama_but_why_all_the_hate_from_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o5uqzp/i_love_ollama_but_why_all_the_hate_from_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o5uqzp/i_love_ollama_but_why_all_the_hate_from_other/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-13T20:10:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6gx1q</id>
    <title>Ollama stops responding after an hour or so</title>
    <updated>2025-10-14T14:23:52+00:00</updated>
    <author>
      <name>/u/trefster</name>
      <uri>https://old.reddit.com/user/trefster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m using gpt-oss:120b as a coding assistant through Roo Code and Ollama. It’s works great for an hour or so and then just stops responding. I Ctrl-C out of Ollama thinking I’ll just reload it, but it doesn’t release my vram, so when I try to load it up again it will spin forever, never giving me an error. I’m running it on Linux with 512GB of DDR5 and an RTX PRO 6000. It’s using only 66 of the 96GB of VRAM so I’m not running into any resource issues. Is it just bad? Should I go back to LLM Studio or try vLLM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trefster"&gt; /u/trefster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6gx1q/ollama_stops_responding_after_an_hour_or_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6gx1q/ollama_stops_responding_after_an_hour_or_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o6gx1q/ollama_stops_responding_after_an_hour_or_so/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-14T14:23:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6i8r1</id>
    <title>AI assisted suite - Doubt about n_gpu layer test</title>
    <updated>2025-10-14T15:14:12+00:00</updated>
    <author>
      <name>/u/3Dpolycraft</name>
      <uri>https://old.reddit.com/user/3Dpolycraft</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o6i8r1/ai_assisted_suite_doubt_about_n_gpu_layer_test/"&gt; &lt;img alt="AI assisted suite - Doubt about n_gpu layer test" src="https://b.thumbs.redditmedia.com/fESlZjLtS8EK0N2I4DthVBsJnAmxej6exXelTlFOruU.jpg" title="AI assisted suite - Doubt about n_gpu layer test" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi community!&lt;br /&gt; First and please don't spit at me if I say something wrong, I'm a neophyte on the subject. That being said, I'm developing (by vibe coding, so... Claude is developing for me) an AI assistant suite that proposes several modules: text summarizer, web search, D&amp;amp;D story teller, chat, etc.&lt;br /&gt; I'm now testing the GPU layer optimizer. I took gemma3:27b-it-qat model and I run sequential prompts by varying the &amp;quot;number of GPU layers&amp;quot; in order to maximize speed of the inference.&lt;br /&gt; I observed that when I exceed a given limit (here the ~15800 MB VRAM, i.e. my 16 Gb VRAM graphic card) the inference time increases significantly. Does this mean that I need to stay below the optimized value if I want to increase my context length?&lt;br /&gt; Currently it's running in its default length, by for &amp;quot;normal use&amp;quot; of the suite I can change this value up to 128k, for this LLM model.&lt;/p&gt; &lt;p&gt;Sys specs: 32 GB RAM, AMD 9700X, RTX 5070 Ti (16 GB VRAM).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jt7n4ohqe3vf1.png?width=1435&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=775e90bf68f01737f71810646a660c7e042828b6"&gt;n_gpu layers optimization test, 2 layers step&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cnjqdhiif3vf1.png?width=1435&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec6dc67c38710ef14df4642fe41aa7a56c40aecc"&gt;n_gpu layers optimization test, 1 layer step&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3Dpolycraft"&gt; /u/3Dpolycraft &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6i8r1/ai_assisted_suite_doubt_about_n_gpu_layer_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6i8r1/ai_assisted_suite_doubt_about_n_gpu_layer_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o6i8r1/ai_assisted_suite_doubt_about_n_gpu_layer_test/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-14T15:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6k2rz</id>
    <title>Private Server Recommendations?</title>
    <updated>2025-10-14T16:21:33+00:00</updated>
    <author>
      <name>/u/coldfisherman</name>
      <uri>https://old.reddit.com/user/coldfisherman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's my situation: I've got a company that does construction work for power companies. The regulations are simply nuts. The crew foreman is supposed to carry the hard-copy of them in his truck and if you stacked the binder up, it would be like 5' tall. &lt;/p&gt; &lt;p&gt;I've got the PDFs and have been breaking them down and putting them in a Qdrant db. Right now, we can call the results and post to openai with no problem, BUT.... these regulations are specific to the jobs the crews are working on. We wrote an ipad app, so the guys in the field could take pictures for the inspectors and have them auto-uploaded to our servers and matched with job files, etc.... The goal here is for the crew member to say, &amp;quot;what kind of insulator should I use here?&amp;quot; and the iPad posts the GPS cooordinates, the crew id and the date. With that, we can say what job he's on. So, we can say, &amp;quot;I'm at Lat/Lon working on this job (break down of job documents). What kind of insulator should I use here?&amp;quot; So that would search the vector DB and then we can post to Ollama (or whichever local LLM we can use) and say, &amp;quot;I'm at Lat/Lon working on this job (break down of job documents). Based upon the regulations below, What kind of insulator should I use here? Return the results with the document references in the meta data&amp;quot;&lt;/p&gt; &lt;p&gt;Basically, I need a local LLM now because we can't send the job information to OpenAI. &lt;/p&gt; &lt;p&gt;There is going to be VERY little traffic here. I'd be willing to bet there'd never be more than one person at a time. &lt;/p&gt; &lt;p&gt;So, the question is..... Can I just get a little nuc in house, or colo some gaming machine or what do I really need to make this stable. &lt;/p&gt; &lt;p&gt;Also, this seems pretty simple so far. I mean, I've already set up stuff like this on my laptop. But I may be missing something. Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coldfisherman"&gt; /u/coldfisherman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6k2rz/private_server_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6k2rz/private_server_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o6k2rz/private_server_recommendations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-14T16:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6c81r</id>
    <title>I built a fully automated AI podcast generator that connects to ollama</title>
    <updated>2025-10-14T10:51:46+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey everyone,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’ve been working on a fun side project — an &lt;strong&gt;AI-powered podcast generator&lt;/strong&gt; built entirely with &lt;strong&gt;Ollama (for the LLM)&lt;/strong&gt; and &lt;strong&gt;Piper (for TTS)&lt;/strong&gt;. 🎙️&lt;/p&gt; &lt;p&gt;The system takes any topic and automatically:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Write a complete script&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generates the audio&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’ve &lt;strong&gt;open-sourced the full project&lt;/strong&gt; on GitHub so anyone can explore, use, or contribute to it. If you’re into AI, audio, or automation, I’d love your feedback and ideas!&lt;/p&gt; &lt;p&gt;🔗 &lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/Laszlobeer/AI-podcast"&gt;https://github.com/Laszlobeer/AI-podcast&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6c81r/i_built_a_fully_automated_ai_podcast_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6c81r/i_built_a_fully_automated_ai_podcast_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o6c81r/i_built_a_fully_automated_ai_podcast_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-14T10:51:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o65dni</id>
    <title>Open Source Alternative to Perplexity</title>
    <updated>2025-10-14T03:54:51+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here’s a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mergeable MindMaps.&lt;/li&gt; &lt;li&gt;Note Management&lt;/li&gt; &lt;li&gt;Multi Collaborative Notebooks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o65dni/open_source_alternative_to_perplexity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o65dni/open_source_alternative_to_perplexity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o65dni/open_source_alternative_to_perplexity/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-14T03:54:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6oo5y</id>
    <title>Nvidia DGX Spark, is it worth ?</title>
    <updated>2025-10-14T19:09:57+00:00</updated>
    <author>
      <name>/u/Appropriate-Camp7981</name>
      <uri>https://old.reddit.com/user/Appropriate-Camp7981</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o6oo5y/nvidia_dgx_spark_is_it_worth/"&gt; &lt;img alt="Nvidia DGX Spark, is it worth ?" src="https://preview.redd.it/jpm27ri6m4vf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3f612ec7a46fdd4e276c69a7e6e429e1635c47c" title="Nvidia DGX Spark, is it worth ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just received an email with a window to buy nvidia Dgx Spark. Is it worth against cloud platforms ?&lt;/p&gt; &lt;p&gt;I could ask ChatGPT but for a change wanted to involve my dear fellow humans to figure this out. &lt;/p&gt; &lt;p&gt;I am using &amp;lt; 30B models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Appropriate-Camp7981"&gt; /u/Appropriate-Camp7981 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jpm27ri6m4vf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6oo5y/nvidia_dgx_spark_is_it_worth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o6oo5y/nvidia_dgx_spark_is_it_worth/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-14T19:09:57+00:00</published>
  </entry>
</feed>
