<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-20T09:13:38+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qeatiw</id>
    <title>New version of Raspberry Pie Generative AI card (HAT+ 2)</title>
    <updated>2026-01-16T08:43:42+00:00</updated>
    <author>
      <name>/u/Unique_Winner_5927</name>
      <uri>https://old.reddit.com/user/Unique_Winner_5927</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perfect for private assistants, industrial equipment, proof of concept, ...&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/"&gt;https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;#RaspberryPi #DataSovereignty #EmbeddedAI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unique_Winner_5927"&gt; /u/Unique_Winner_5927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qeatiw/new_version_of_raspberry_pie_generative_ai_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qeatiw/new_version_of_raspberry_pie_generative_ai_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qeatiw/new_version_of_raspberry_pie_generative_ai_card/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T08:43:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qef38r</id>
    <title>Prompt tool I built/use with Ollama daily - render prompt variations without worrying about text files</title>
    <updated>2026-01-16T12:49:45+00:00</updated>
    <author>
      <name>/u/springwasser</name>
      <uri>https://old.reddit.com/user/springwasser</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted this to another subreddit, but should have posted it here.. sorry if you've seen it.&lt;/p&gt; &lt;p&gt;This is a tool I built because I use it in local development. I know there are solutions for these things mixed into other software, but this is standalone and does just one thing really well for me.&lt;/p&gt; &lt;p&gt;- create/version/store prompts.. don't have to worry about text files unless I want to&lt;br /&gt; - runs from command line, can pipe stdout into anything.. eg Ollama, ci, git hooks&lt;br /&gt; - easily render variations of prompts on the fly, inject {{variables}} or inject files.. e.g. git diffs or documents&lt;br /&gt; - can store prompts globally or in projects, run anywhere&lt;/p&gt; &lt;p&gt;Basic usage:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Create a prompt.. paste in text $ promptg prompt new my-prompt # -or- $ echo &amp;quot;Create a prompt with pipe&amp;quot; | promptg prompt save hello # Then.. $ promptg get my-prompt | ollama run deepseek-r1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or more advanced, render with dynamic variables and insert files..&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# before.. cat prompt.txt | sed &amp;quot;s/{{lang}}/Python/g; s/{{code}}/$(cat myfile.py)/g&amp;quot; | ollama run mistral # now, replace dynamic {{templateValue}} and insert code/file. promptg get code-review --var lang=Python --var code@myfile.py | ollama run mistral &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Install:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;npm install -g @promptg/cli &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/springwasser"&gt; /u/springwasser &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qef38r/prompt_tool_i_builtuse_with_ollama_daily_render/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qef38r/prompt_tool_i_builtuse_with_ollama_daily_render/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qef38r/prompt_tool_i_builtuse_with_ollama_daily_render/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T12:49:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeixz8</id>
    <title>Preventing hallucinations - what's working for me</title>
    <updated>2026-01-16T15:27:04+00:00</updated>
    <author>
      <name>/u/Financial-Local-5543</name>
      <uri>https://old.reddit.com/user/Financial-Local-5543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run several Facebook groups in which we explore academic articles; I found Claude and perplexity helpful for summarizing them so readers can get a quick overview. Hallucinations, of course can be a problem. In the article I share what has been working for me too minimize and prevent this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial-Local-5543"&gt; /u/Financial-Local-5543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai-consciousness.org/reducing-hallucinations-in-ai-how-to-get-reliable-ai-summaries-of-complex-articles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qeixz8/preventing_hallucinations_whats_working_for_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qeixz8/preventing_hallucinations_whats_working_for_me/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T15:27:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qersd1</id>
    <title>Suggestion on Renting an AI server for a month</title>
    <updated>2026-01-16T20:50:07+00:00</updated>
    <author>
      <name>/u/100yearsofhappiness</name>
      <uri>https://old.reddit.com/user/100yearsofhappiness</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/100yearsofhappiness"&gt; /u/100yearsofhappiness &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalAIServers/comments/1qerrtn/suggestion_on_renting_an_ai_server_for_a_month/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qersd1/suggestion_on_renting_an_ai_server_for_a_month/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qersd1/suggestion_on_renting_an_ai_server_for_a_month/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T20:50:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qemwau</id>
    <title>Best Compute Per Dollar for AI?</title>
    <updated>2026-01-16T17:49:11+00:00</updated>
    <author>
      <name>/u/NetTechMan</name>
      <uri>https://old.reddit.com/user/NetTechMan</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NetTechMan"&gt; /u/NetTechMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/homelab/comments/1qemvsi/best_compute_per_dollar_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qemwau/best_compute_per_dollar_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qemwau/best_compute_per_dollar_for_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T17:49:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qen6w1</id>
    <title>The Preprocessing Gap Between RAG and Agentic</title>
    <updated>2026-01-16T17:59:44+00:00</updated>
    <author>
      <name>/u/OnyxProyectoUno</name>
      <uri>https://old.reddit.com/user/OnyxProyectoUno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RAG is the standard way to connect documents to LLMs. Most people building RAGs know the steps by now: parse documents, chunk them, embed, store vectors, retrieve at query time. But something different happens when you're building systems that act rather than answer.&lt;/p&gt; &lt;h3&gt;The RAG mental model&lt;/h3&gt; &lt;p&gt;RAG preprocessing optimizes for retrieval. Someone asks a question, you find relevant chunks, you synthesize an answer. The whole pipeline is designed around that interaction pattern.&lt;/p&gt; &lt;p&gt;The work happens before anyone asks anything. Documents get parsed into text, extracting content from PDFs, Word docs, HTML, whatever format you're working with. Then chunking splits that text into pieces sized for context windows. You choose a strategy based on your content: split on paragraphs, headings, or fixed token counts. Overlap between chunks preserves context across boundaries. Finally, embedding converts each chunk into a vector where similar meanings cluster together. &amp;quot;The contract expires in December&amp;quot; ends up near &amp;quot;Agreement termination date: 12/31/2024&amp;quot; even though they share few words. That's what makes semantic search work.&lt;/p&gt; &lt;p&gt;Retrieval is similarity search over those vectors. Query comes in, gets embedded, you find the nearest chunks in vector space. For Q&amp;amp;A, this works well. You ask a question, the system finds relevant passages, an LLM synthesizes an answer. The whole architecture assumes a query-response pattern.&lt;/p&gt; &lt;p&gt;The requirements shift when you're building systems that act instead of answer.&lt;/p&gt; &lt;h3&gt;What agentic actually needs&lt;/h3&gt; &lt;p&gt;Consider a contract monitoring system. It tracks obligations across hundreds of agreements: Example Bank owes a quarterly audit report by the 15th, so the system sends a reminder on the 10th, flags it as overdue on the 16th, and escalates to legal on the 20th. The system doesn't just find text about deadlines. It acts on them.&lt;/p&gt; &lt;p&gt;That requires something different at the data layer. The system needs to understand that Party A owes Party B deliverable X by date Y under condition Z. And it needs to connect those facts across documents. Not just find text about obligations, but actually know what's owed to whom and when.&lt;/p&gt; &lt;p&gt;The preprocessing has to pull out that structure, not just preserve text for later search. You're not chunking paragraphs. You're turning &amp;quot;Example Bank shall submit quarterly compliance reports within 15 days of quarter end&amp;quot; into data you can query: party, obligation type, deadline, conditions. Think rows in a database, not passages in a search index.&lt;/p&gt; &lt;h3&gt;Two parallel paths&lt;/h3&gt; &lt;p&gt;The architecture ends up looking completely different.&lt;/p&gt; &lt;p&gt;RAG has a linear pipeline. Documents go in, chunking happens, embeddings get created, vectors get stored. At query time, search, retrieve, generate.&lt;/p&gt; &lt;p&gt;Agentic systems need two tracks running in parallel. The main one pulls structured data out of documents. An LLM reads each contract, extracts the obligations, parties, dates, and conditions, and writes them to a graph database. Why a graph? Because you're not just storing isolated facts, you're storing how they connect. Example Bank owes a report. That report is due quarterly. The obligation comes from Section 4.2 of Contract #1847. Those connections between entities are what graph databases are built for. This is what powers the actual monitoring.&lt;/p&gt; &lt;p&gt;But you still need embeddings. Just for different reasons.&lt;/p&gt; &lt;p&gt;The second track catches what extraction misses. Sometimes &amp;quot;the Lender&amp;quot; in paragraph 12 needs to connect to &amp;quot;Example Bank&amp;quot; from paragraph 3. Sometimes you don't know what patterns matter until you see them repeated across documents. The vector search helps you find connections that weren't obvious enough to extract upfront.&lt;/p&gt; &lt;p&gt;So you end up with two databases working together. The graph database stores entities and their relationships: who owes what to whom by when. The vector database helps you find things you didn't know to look for.&lt;/p&gt; &lt;p&gt;I wrote the rest on my &lt;a href="https://nickrichu.me/posts/the-preprocessing-gap-between-rag-and-agentic"&gt;blog&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OnyxProyectoUno"&gt; /u/OnyxProyectoUno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qen6w1/the_preprocessing_gap_between_rag_and_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qen6w1/the_preprocessing_gap_between_rag_and_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qen6w1/the_preprocessing_gap_between_rag_and_agentic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T17:59:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qepzot</id>
    <title>Polymcp Integrates Ollama – Local and Cloud Execution Made Simple</title>
    <updated>2026-01-16T19:41:16+00:00</updated>
    <author>
      <name>/u/Just_Vugg_PolyMCP</name>
      <uri>https://old.reddit.com/user/Just_Vugg_PolyMCP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qepzot/polymcp_integrates_ollama_local_and_cloud/"&gt; &lt;img alt="Polymcp Integrates Ollama – Local and Cloud Execution Made Simple" src="https://external-preview.redd.it/uRBPZkkzgOSFpr8EhLBqbooPoKdELZzAq8ZwsFfCuh8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbb437599f34354b12d63c620969d4b8f9783757" title="Polymcp Integrates Ollama – Local and Cloud Execution Made Simple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Polymcp integrates with Ollama for local and cloud execution!&lt;/p&gt; &lt;p&gt;You can seamlessly run models like gpt-oss:120b, Kimi K2, Nemotron, and others with just a few lines of code. Here’s a simple example of how to use gpt-oss:120b via Ollama:&lt;/p&gt; &lt;p&gt;from polymcp.polyagent import PolyAgent, OllamaProvider, OpenAIProvider&lt;/p&gt; &lt;p&gt;def create_llm_provider():&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;Use Ollama with gpt-oss:120b.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;return OllamaProvider(model=&amp;quot;gpt-oss:120b&amp;quot;)&lt;/p&gt; &lt;p&gt;def main():&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;Execute a task using PolyAgent.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;llm_provider = create_llm_provider()&lt;/p&gt; &lt;p&gt;agent = PolyAgent(llm_provider=llm_provider, mcp_servers=[&amp;quot;http://localhost:8000/mcp&amp;quot;])&lt;/p&gt; &lt;p&gt;query = &amp;quot;What is the capital of France?&amp;quot;&lt;/p&gt; &lt;p&gt;print(f&amp;quot;Query: {query}&amp;quot;)&lt;/p&gt; &lt;p&gt;response = agent.run(query)&lt;/p&gt; &lt;p&gt;print(f&amp;quot;Response: {response}\n&amp;quot;)&lt;/p&gt; &lt;p&gt;if __name__ == &amp;quot;__main__&amp;quot;:&lt;/p&gt; &lt;p&gt;main()&lt;/p&gt; &lt;p&gt;This integration makes it easy to run your models locally or in the cloud. No extra setup required—just integrate, run, and go.&lt;/p&gt; &lt;p&gt;Let me know how you’re using it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Vugg_PolyMCP"&gt; /u/Just_Vugg_PolyMCP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/poly-mcp/Polymcp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qepzot/polymcp_integrates_ollama_local_and_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qepzot/polymcp_integrates_ollama_local_and_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T19:41:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qepvki</id>
    <title>Do you actually need prompt engineering to get value from AI?</title>
    <updated>2026-01-16T19:36:53+00:00</updated>
    <author>
      <name>/u/Xthebuilder</name>
      <uri>https://old.reddit.com/user/Xthebuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been using AI daily for about 6 months while building a local AI inferencing app, and one thing that surprised me is how little prompt engineering mattered compared to other factors.&lt;/p&gt; &lt;p&gt;What ended up making the biggest difference for me was:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;giving the model enough &lt;strong&gt;context&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;iterating on ideas &lt;em&gt;with&lt;/em&gt; the model before writing real code&lt;/li&gt; &lt;li&gt;choosing models that are actually good at the specific task&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Because LLMs have some randomness, I found they’re most useful early on, when you’re still figuring things out. Iterating with the model helped surface bad assumptions before I committed to an approach. They’re especially good at starting broad and narrowing down if you keep the conversation going so context builds up.&lt;/p&gt; &lt;p&gt;When I add new features now, I don’t explain my app’s architecture anymore. I just link the relevant GitHub repos so the model can see how things are structured. That alone cut feature dev time from weeks to about a day in one case.&lt;/p&gt; &lt;p&gt;I’m not saying prompt engineering is useless, just that for most practical work, context, iteration, and model choice mattered more for me.&lt;/p&gt; &lt;p&gt;Curious how others here approach this. Has prompt engineering been critical for you, or have you seen similar results?&lt;/p&gt; &lt;p&gt;(I wrote up the full experience here if anyone wants more detail: &lt;a href="https://xthebuilder.github.io"&gt;https://xthebuilder.github.io&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xthebuilder"&gt; /u/Xthebuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T19:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfauq9</id>
    <title>Help a noob figure out how to achieve something in a game engine with Ollama</title>
    <updated>2026-01-17T11:26:10+00:00</updated>
    <author>
      <name>/u/MountainPlantation</name>
      <uri>https://old.reddit.com/user/MountainPlantation</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I want to use Ollama to integrate it with a game engine. It's already in the engine and working, but I have some questions on what model I should use, and any tips in general for the experiments I want to do. I understand most LLMs running locally will take a while to think and generate a response, but for now let's ignore that.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;NPC Chat with commands: I know most people have tried doing NPC chatbots in engines, but I was thinking I could try to spice that up by integrating commands on it. Like the LLM would have a list of commands, given by me, that it could use contextually, like /laugh /cry /givePlayer(item), things like that. And I can make a system that parses the string and extracts/executes the commands. I attempted this one time, not in engine, just by using regular chat GPT and it would eventually come up with its own commands that were not stipulated by me. How to avoid that? Is there a model I should use for that?&lt;/li&gt; &lt;li&gt;NPC consistency in character. I also tried one time to keep chat GPT in character, a peasant from the medieval ages, but I would ask about modern events like COVID and it would eventually break and talk about it as if he knew what it was.&lt;/li&gt; &lt;li&gt;NPC Memory. What if I wanted to have NPCs remember things they have witnessed? I imagine I should make a log system that keeps every action done to that npc (NPC was hit by Player. NPC killed bandit. NPC found 1 gold etc) and then adding it to the beggining of the prompt as a little memory. Is that enough?&lt;/li&gt; &lt;li&gt;Can I reliably limit the response length or is it finicky? Like, setting a limit of how many words per response &lt;/li&gt; &lt;li&gt;Is there a way to guarantee responses are always in character? Because sometimes some of the LLMs will say &amp;quot;I cannot answer to things related to that&amp;quot; and that would be a big immersion breaker &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And another general question, is there a way to train certain models to get them used to a certani context? like i said, using commands I create in my game, or training them to act like a specific type of character etc.&lt;/p&gt; &lt;p&gt;Again, other than my experiments with just the chat GPT window, I am pretty new to this. If you have advice on what models to use or best practices, I'm listening.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MountainPlantation"&gt; /u/MountainPlantation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-17T11:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfzbd4</id>
    <title>Ollama not detecting intel arc graphics</title>
    <updated>2026-01-18T04:52:04+00:00</updated>
    <author>
      <name>/u/Titanlucifer18</name>
      <uri>https://old.reddit.com/user/Titanlucifer18</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Thinkpad E14 G7, with intel core ultra 5 225H processor, running Fedora 43. I tried to install Ollama, but it did not detect any GPU. I tried to search docs but couldn’t find anything, or maybe I weren’t looking at the right place.&lt;/p&gt; &lt;p&gt;If anyone can guide me would be helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Titanlucifer18"&gt; /u/Titanlucifer18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfzbd4/ollama_not_detecting_intel_arc_graphics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfzbd4/ollama_not_detecting_intel_arc_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qfzbd4/ollama_not_detecting_intel_arc_graphics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T04:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfzb7g</id>
    <title>Ollama not detecting intel arc graphics</title>
    <updated>2026-01-18T04:51:50+00:00</updated>
    <author>
      <name>/u/Titanlucifer18</name>
      <uri>https://old.reddit.com/user/Titanlucifer18</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Thinkpad E14 G7, with intel core ultra 5 225H processor, running Fedora 43. I tried to install Ollama, but it did not detect any GPU. I tried to search docs but couldn’t find anything, or maybe I weren’t looking at the right place.&lt;/p&gt; &lt;p&gt;If anyone can guide me would be helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Titanlucifer18"&gt; /u/Titanlucifer18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfzb7g/ollama_not_detecting_intel_arc_graphics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfzb7g/ollama_not_detecting_intel_arc_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qfzb7g/ollama_not_detecting_intel_arc_graphics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T04:51:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg9pek</id>
    <title>[D] Validate Production GenAI Challenges - Seeking Feedback</title>
    <updated>2026-01-18T14:17:25+00:00</updated>
    <author>
      <name>/u/No_Barracuda_415</name>
      <uri>https://old.reddit.com/user/No_Barracuda_415</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Guys,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Quick Backstory:&lt;/strong&gt; While working on LLMOps in past 2 years, I felt chaos with massive LLM workflows where costs exploded without clear attribution(which agent/prompt/retries?), silent sensitive data leakage and compliance had no replayable audit trails. Peers in other teams and externally felt the same: fragmented tools (metrics but not LLM aware), no real-time controls and growing risks with scaling. We felt the major need was &lt;strong&gt;control over costs, security and auditability without overhauling with multiple stacks/tools or adding latency&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problems we're seeing:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Unexplained LLM Spend:&lt;/strong&gt; Total bill known, but no breakdown by model/agent/workflow/team/tenant. Inefficient prompts/retries hide waste.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Silent Security Risks:&lt;/strong&gt; PII/PHI/PCI, API keys, prompt injections/jailbreaks slip through without real-time detection/enforcement.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No Audit Trail:&lt;/strong&gt; Hard to explain AI decisions (prompts, tools, responses, routing, policies) to Security/Finance/Compliance.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Does this resonate with anyone running GenAI workflows/multi-agents?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Few open questions I am having:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is this problem space worth pursuing in production GenAI?&lt;/li&gt; &lt;li&gt;Biggest challenges in cost/security observability to prioritize?&lt;/li&gt; &lt;li&gt;Are there other big pains in observability/governance I'm missing?&lt;/li&gt; &lt;li&gt;How do you currently hack around these (custom scripts, LangSmith, manual reviews)?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Barracuda_415"&gt; /u/No_Barracuda_415 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qg9pek/d_validate_production_genai_challenges_seeking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qg9pek/d_validate_production_genai_challenges_seeking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qg9pek/d_validate_production_genai_challenges_seeking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T14:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfufg3</id>
    <title>Claude Code with Anthropic API compatibility</title>
    <updated>2026-01-18T01:01:12+00:00</updated>
    <author>
      <name>/u/GhettoFob</name>
      <uri>https://old.reddit.com/user/GhettoFob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qfufg3/claude_code_with_anthropic_api_compatibility/"&gt; &lt;img alt="Claude Code with Anthropic API compatibility" src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="Claude Code with Anthropic API compatibility" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhettoFob"&gt; /u/GhettoFob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/blog/claude"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfufg3/claude_code_with_anthropic_api_compatibility/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qfufg3/claude_code_with_anthropic_api_compatibility/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T01:01:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgbl6g</id>
    <title>(linux) i'm interested in historical roleplay (1600s)/early modern period), what would be your setup ?</title>
    <updated>2026-01-18T15:33:57+00:00</updated>
    <author>
      <name>/u/Mid-Pri6170</name>
      <uri>https://old.reddit.com/user/Mid-Pri6170</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;my longer term goal is to use gemini or other ai to make a little isometric world in Godot i can explore.&lt;/p&gt; &lt;p&gt;yesterday gemini had me instal olama and lama3 on my pc. &lt;/p&gt; &lt;p&gt;i only ran it in the terminal, but i am interested in what other things to consider to make it emersive.... considering cgpt etc are nerf'd&lt;/p&gt; &lt;p&gt;Gemini suggest Dolphin, Qwen and Nemo models too. however i was wondering if these models have a lot obscure trivia, knowledge of the period, language etc in them like the big llms do, otherwise they will quickly sound stale.&lt;/p&gt; &lt;p&gt;i was thinking there might be a specially trained model on period language/literature?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mid-Pri6170"&gt; /u/Mid-Pri6170 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgbl6g/linux_im_interested_in_historical_roleplay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgbl6g/linux_im_interested_in_historical_roleplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qgbl6g/linux_im_interested_in_historical_roleplay/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T15:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgtc9i</id>
    <title>Summary and Tagging</title>
    <updated>2026-01-19T03:39:53+00:00</updated>
    <author>
      <name>/u/FlibblesHexEyes</name>
      <uri>https://old.reddit.com/user/FlibblesHexEyes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all; I don't usually use LLM's so thought I'd ask here if I'm doing this correctly - or if there is a better way to do it.&lt;/p&gt; &lt;p&gt;I run the Hasheous project - the idea is that if you supply an MD5/SHA1 hash, Hasheous can respond with mappings to video game metadata suppliers such as IGDB and others.&lt;/p&gt; &lt;p&gt;Just as a &amp;quot;I thought this might be a cool addition&amp;quot; feature, I wanted to add descriptions and tags to each record generated from the mapped metadata sources so that I could provide data to ROM management apps to provide similar games and the basis of a game recommendation engine.&lt;/p&gt; &lt;p&gt;I don't have the budget for offloading this to commercial AI providers (this is a free open source project), so I'm going with a distributed model where anyone could download an agent to use their own installation of Ollama to generate the description and tags.&lt;/p&gt; &lt;p&gt;With the help of Copilot, I came up with the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;pull the description for each mapped data source (IGDB, GiantBomb, Wikipedia, etc) and add them as embedded content. Copilot recommended the nomic-embed-text model to generate the vectors.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;run a CosineSimularity over the response to extract the top &lt;code&gt;x&lt;/code&gt; results (I won't pretend to understand how this function works!)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;run this through a prompt generator which generates a string with the top &lt;code&gt;x&lt;/code&gt; embeddings under the heading &amp;quot;Context:&amp;quot;, and the the prompts below under the heading &amp;quot;Instructions:&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;call the &lt;code&gt;/generate&lt;/code&gt; endpoint with the RAG prompt generator to create the response&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For the description model, I'm using Gemma3:12b, with the prompt: ``` Generate a detailed description/synopsis for the game &amp;lt;DATA_OBJECT_NAME&amp;gt; for &amp;lt;DATA_OBJECT_PLATFORM&amp;gt;.&lt;/p&gt; &lt;p&gt;If present; use the Wikipedia source as context for the other provided sources.&lt;/p&gt; &lt;p&gt;You MUST respond only with the description/synopsis. Do not acknowledge you've received this request.&lt;/p&gt; &lt;p&gt;The description should be engaging and informative, highlighting plot, key features, and gameplay. Keep the description concise, ideally between 150 to 200 words, but no more than 250 words. The output should be in markdown format. ```&lt;/p&gt; &lt;p&gt;For the tags model, I'm using qwen3:8b, with the prompt: ``` You are an expert whose responsibility is to help with automatic tagging for a game recommendation engine.&lt;/p&gt; &lt;p&gt;Generate detailed tags for the game &amp;lt;DATA_OBJECT_NAME&amp;gt; for &amp;lt;DATA_OBJECT_PLATFORM&amp;gt;.&lt;/p&gt; &lt;p&gt;If present; use the Wikipedia source as context for the other provided sources.&lt;/p&gt; &lt;p&gt;The tags should accurately represent the game. Only generate tags in the following categories: Genre, Gameplay, Features, Theme, Perspective, and Art Style.&lt;/p&gt; &lt;p&gt;Each tag should be no more than three words long. Ensure each tag is specific and commonly used within the gaming community, but avoid overly broad or generic terms. If you are unable to generate tags relevant to the category, leave it empty.&lt;/p&gt; &lt;p&gt;Generate a minimum of three tags and a maximum of ten tags per category.&lt;/p&gt; &lt;p&gt;Format the output as a raw JSON object, containing an array of tags for each category.&lt;/p&gt; &lt;p&gt;Make sure the JSON is properly structured and valid.&lt;/p&gt; &lt;p&gt;Example output: { &amp;quot;Genre&amp;quot;: [ &amp;quot;Action&amp;quot;, &amp;quot;Adventure&amp;quot; ], &amp;quot;Gameplay&amp;quot;: [ &amp;quot;Open World&amp;quot;, &amp;quot;Multiplayer&amp;quot; ], &amp;quot;Features&amp;quot;: [ &amp;quot;Crafting&amp;quot;, &amp;quot;Character Customization&amp;quot; ], &amp;quot;Theme&amp;quot;: [ &amp;quot;Sci-Fi&amp;quot;, &amp;quot;Fantasy&amp;quot; ], &amp;quot;Perspective&amp;quot;: [ &amp;quot;First-Person&amp;quot;, &amp;quot;Third-Person&amp;quot; ], &amp;quot;Art Style&amp;quot;: [ &amp;quot;Realistic&amp;quot;, &amp;quot;Pixel Art&amp;quot; ] }&lt;/p&gt; &lt;p&gt;Do not include any additional text or content outside of the JSON object. ```&lt;/p&gt; &lt;p&gt;Example output here: &lt;a href="https://beta.hasheous.org/index.html?page=dataobjectdetail&amp;amp;type=game&amp;amp;id=109"&gt;https://beta.hasheous.org/index.html?page=dataobjectdetail&amp;amp;type=game&amp;amp;id=109&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was wondering if anyone had any advice or suggestions to make this process faster or more accurate - or just better :)&lt;/p&gt; &lt;p&gt;Currently takes about 3 minutes per game on my GTX970 (my best GPU sadly) to generate the description and tags, so performance improvements would also be appreciated.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlibblesHexEyes"&gt; /u/FlibblesHexEyes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgtc9i/summary_and_tagging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgtc9i/summary_and_tagging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qgtc9i/summary_and_tagging/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T03:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgythz</id>
    <title>Handle files with ollama SDK</title>
    <updated>2026-01-19T08:35:24+00:00</updated>
    <author>
      <name>/u/mr_dattebayo</name>
      <uri>https://old.reddit.com/user/mr_dattebayo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I have a question regarding file attachments in the new ollama desktop app.&lt;br /&gt; I have been evaluating different models via the app for an inferrence task on a large JSON file, which gave me good results.&lt;/p&gt; &lt;p&gt;But I actually need to use the ollama sdk to prompt the models and neither SDK nor the REST api offer the option to pass files. Directly appending the file content in the prompt is producing far worse results. So I am looking for a way to get the same results as with the desktop app.&lt;/p&gt; &lt;p&gt;Does anyone know how ollama handles file attachments in the desktop app or can point me into the right direction on how to get the same outcome when using the SDK?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_dattebayo"&gt; /u/mr_dattebayo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgythz/handle_files_with_ollama_sdk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgythz/handle_files_with_ollama_sdk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qgythz/handle_files_with_ollama_sdk/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T08:35:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh1k45</id>
    <title>LocalCopilot</title>
    <updated>2026-01-19T11:18:37+00:00</updated>
    <author>
      <name>/u/Huzaifa_Tech</name>
      <uri>https://old.reddit.com/user/Huzaifa_Tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using Copilot with the Sonnet-4 agent. It works very fast and performs coding tasks well while understanding context, but it is expensive for day-to-day coding and development.&lt;/p&gt; &lt;p&gt;What should I do if I want to run LLMs locally that work similarly to Sonnet-4 and can also understand context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Huzaifa_Tech"&gt; /u/Huzaifa_Tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qh1k45/localcopilot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qh1k45/localcopilot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qh1k45/localcopilot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T11:18:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhirii</id>
    <title>Electricity saving</title>
    <updated>2026-01-19T22:21:59+00:00</updated>
    <author>
      <name>/u/Original-Chapter-112</name>
      <uri>https://old.reddit.com/user/Original-Chapter-112</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Original-Chapter-112"&gt; /u/Original-Chapter-112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/homeassistant/comments/1qhibzi/electricity_saving/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhirii/electricity_saving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qhirii/electricity_saving/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T22:21:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhj3vw</id>
    <title>Has anyone got Ollama to work on an Arc Pro B50 in a proxmox VM?</title>
    <updated>2026-01-19T22:35:11+00:00</updated>
    <author>
      <name>/u/gregusmeus</name>
      <uri>https://old.reddit.com/user/gregusmeus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve tried a dozen ways to try and get ollama to see the GPU but it’s refusing. Any help gratefully received. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gregusmeus"&gt; /u/gregusmeus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhj3vw/has_anyone_got_ollama_to_work_on_an_arc_pro_b50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhj3vw/has_anyone_got_ollama_to_work_on_an_arc_pro_b50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qhj3vw/has_anyone_got_ollama_to_work_on_an_arc_pro_b50/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T22:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhjxf7</id>
    <title>M5 Metal compilation error</title>
    <updated>2026-01-19T23:07:09+00:00</updated>
    <author>
      <name>/u/sidanos</name>
      <uri>https://old.reddit.com/user/sidanos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I’m running into a reproducible crash with Ollama on macOS after updating to macOS 26.2 (Build 25C56) on an Apple M5 machine.&lt;/p&gt; &lt;p&gt;Everything worked fine yesterday. Today, any attempt to run Llama 3.1 with GPU (Metal) fails during Metal library initialization.&lt;/p&gt; &lt;p&gt;Environment&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• macOS: 26.2 (25C56) • Hardware: Apple M5 • Ollama: 0.14.x (Homebrew) • Model: llama3.1:latest, llama3.1:8b, llama3.1:8b- &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;instruct (all fail the same way)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• Xcode Command Line Tools: updated • Rebooted: yes &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Has anyone countered this? And maybe has workaround?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sidanos"&gt; /u/sidanos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhjxf7/m5_metal_compilation_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhjxf7/m5_metal_compilation_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qhjxf7/m5_metal_compilation_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T23:07:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgxulm</id>
    <title>Would Anthropic Block Ollama?</title>
    <updated>2026-01-19T07:36:07+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Few hours ago, Ollama announced following:&lt;/p&gt; &lt;p&gt;Ollama now has Anthropic API compatibility. This enables tools like Claude Code to be used with open-source models.&lt;/p&gt; &lt;p&gt;Ollama Blog: &lt;a href="https://ollama.com/blog/claude"&gt;Claude Code with Anthropic API compatibility · Ollama Blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hands-on Guide: &lt;a href="https://youtu.be/Pbsn-6JEE2s?si=7pdAv5LU9GiBx7aN"&gt;https://youtu.be/Pbsn-6JEE2s?si=7pdAv5LU9GiBx7aN&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For now it's working but for how long?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgxulm/would_anthropic_block_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgxulm/would_anthropic_block_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qgxulm/would_anthropic_block_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T07:36:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh10xr</id>
    <title>Demo: On-device browser agent (Qwen) running locally in Chrome</title>
    <updated>2026-01-19T10:48:47+00:00</updated>
    <author>
      <name>/u/thecoder12322</name>
      <uri>https://old.reddit.com/user/thecoder12322</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qh10xr/demo_ondevice_browser_agent_qwen_running_locally/"&gt; &lt;img alt="Demo: On-device browser agent (Qwen) running locally in Chrome" src="https://external-preview.redd.it/2j5zq51thj98y10lUAVh-0Pz732bAfFjUfnIlXTUBeM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b649fa820188efe6eb47208c536eb1bdfe9933d2" title="Demo: On-device browser agent (Qwen) running locally in Chrome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecoder12322"&gt; /u/thecoder12322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ljp6zwzfcaeg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qh10xr/demo_ondevice_browser_agent_qwen_running_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qh10xr/demo_ondevice_browser_agent_qwen_running_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T10:48:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhvtnp</id>
    <title>timed out waiting for llama runner to start: context canceled</title>
    <updated>2026-01-20T08:41:50+00:00</updated>
    <author>
      <name>/u/DerZwirbel</name>
      <uri>https://old.reddit.com/user/DerZwirbel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; I’m seeing intermittent model load failures with &lt;strong&gt;Ollama 0.13.4&lt;/strong&gt; running in &lt;strong&gt;Docker&lt;/strong&gt; when loading &lt;strong&gt;phi4&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Error excerpt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;time=2026-01-20T08:17:55.413Z level=INFO source=sched.go:470 msg=&amp;quot;Load failed&amp;quot; model=/data/ollama/blobs/sha256-fd7b6731c33c57f61767612f56517460ec2d1e2e5a3f0163e0eb3d8d8cb5df20 error=&amp;quot;timed out waiting for llama runner to start: context canceled&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This typically happens during container startup or under load.&lt;br /&gt; CUDA is available, but the failure is non-deterministic (cold start related?).&lt;/p&gt; &lt;p&gt;Has anyone seen this with phi4 recently, or has guidance on? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DerZwirbel"&gt; /u/DerZwirbel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhvtnp/timed_out_waiting_for_llama_runner_to_start/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhvtnp/timed_out_waiting_for_llama_runner_to_start/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qhvtnp/timed_out_waiting_for_llama_runner_to_start/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-20T08:41:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhr198</id>
    <title>GLM 4.7 is apparently almost ready on Ollama</title>
    <updated>2026-01-20T04:18:14+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/"&gt; &lt;img alt="GLM 4.7 is apparently almost ready on Ollama" src="https://a.thumbs.redditmedia.com/1aab6UqlDDpJ6gz379kpHuTSrReU-J_gAGirTkMBCr8.jpg" title="GLM 4.7 is apparently almost ready on Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's listed, just not downloadable yet. Trying in WebOllama, and in CLI gives weird excuses&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/96ly2bgckfeg1.png?width=1723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d8bfc2386dd789ef4f28ff0516de4893bf5c6772"&gt;https://preview.redd.it/96ly2bgckfeg1.png?width=1723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d8bfc2386dd789ef4f28ff0516de4893bf5c6772&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-20T04:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh5z0j</id>
    <title>I built a voice-first AI mirror that runs fully on Ollama.</title>
    <updated>2026-01-19T14:43:25+00:00</updated>
    <author>
      <name>/u/DirectorChance4012</name>
      <uri>https://old.reddit.com/user/DirectorChance4012</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qh5z0j/i_built_a_voicefirst_ai_mirror_that_runs_fully_on/"&gt; &lt;img alt="I built a voice-first AI mirror that runs fully on Ollama." src="https://external-preview.redd.it/Y3p4ZmcwM3FpYmVnMXwaGz6cYTO_1FXYid56crf85mAMdgg0ECh85UXNOmp7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a490bc51d40c3d188f9b4edee608ffa0b05365bf" title="I built a voice-first AI mirror that runs fully on Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a voice-first AI mirror that runs fully on Ollama.&lt;/p&gt; &lt;p&gt;The idea was to explore what a “voice-native” interface looks like&lt;/p&gt; &lt;p&gt;when it’s ambient and always there — not a chat window.&lt;/p&gt; &lt;p&gt;Everything runs locally (LLM via Ollama), no cloud dependency.&lt;/p&gt; &lt;p&gt;Still very experimental, but surprisingly usable.&lt;/p&gt; &lt;p&gt;Blog (how it works + design decisions):&lt;/p&gt; &lt;p&gt;&lt;a href="https://noted.lol/mirrormate/"&gt;https://noted.lol/mirrormate/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub (WIP, self-hostable):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/orangekame3/mirrormate"&gt;https://github.com/orangekame3/mirrormate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DirectorChance4012"&gt; /u/DirectorChance4012 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bjeyts2qibeg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qh5z0j/i_built_a_voicefirst_ai_mirror_that_runs_fully_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qh5z0j/i_built_a_voicefirst_ai_mirror_that_runs_fully_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T14:43:25+00:00</published>
  </entry>
</feed>
