<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-01T19:50:27+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qr24r9</id>
    <title>How to respond with a tool call</title>
    <updated>2026-01-30T10:44:24+00:00</updated>
    <author>
      <name>/u/Super_Nova02</name>
      <uri>https://old.reddit.com/user/Super_Nova02</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm creating a little chatbot, which main function is to call some tools and create an answer with the info the tool give it. &lt;/p&gt; &lt;p&gt;I'm using Ollama in a javascript environment (so ollama-js). The tool call use Functiongemma:270m as model and works fine (it is a &lt;a href="http://ollama.chat"&gt;ollama.chat&lt;/a&gt; request).&lt;br /&gt; Then I try to rewrite the info so that the aswer is more &amp;quot;human-like&amp;quot;: for example, if the tool returns an array of objects, it would be perfect if the chatbot answers with list with the info well laid out.&lt;br /&gt; This is the code of this second request:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;const toolResult = await executeTool( toolCall.name, toolCall.arguments || {}, { BACKEND_URL }, ); const bullets = formatSensors(toolResult); const chunks = chunkArray(bullets, 5); let summaries = []; for (let i = 0; i &amp;lt; chunks.length; i++) { const chunk = chunks[i]; const result = await ollama.generate({ model: &amp;quot;gemma3:270m&amp;quot;, options: { temperature: 0 }, prompt: ` You are an assistant. Respond to the user as follows: - If the user requested the sensors, start with a natural intro like &amp;quot;Sure, here's the list of all the sensors:&amp;quot; and then immediately list all the items exactly as provided below. - If the user added sensors, start with &amp;quot;I've added the sensors with the information you provided me, here's how it looks:&amp;quot; followed by the list exactly as provided. - Do NOT modify, remove, or reorder any items in the list. - Include the list exactly as it appears below in your output. SENSOR LIST START ${chunk.join(&amp;quot;\n&amp;quot;)} SENSOR LIST END `, }); summaries.push(result.response); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The problem is that the llm just prints &amp;quot;Okay, I understand. I will respond to the user as requested, keeping the list exactly as it appears in the user's message.&amp;quot; or similar messages, without really printing the tool info I've given it.&lt;br /&gt; Please keep in mind that I can't use bigger models: my pc would not be able to run them and also for the specific purpose of my chatbot I don't think I need bigger models.&lt;/p&gt; &lt;p&gt;In the end I would like to have something like &amp;quot;Here's the list of elements you asked for&amp;quot; or &amp;quot;sure, i've added the element with the info you provided, here's how it looks&amp;quot;, and so on for my various functionalities. &lt;/p&gt; &lt;p&gt;I don't really understand what I'm doing wrong. Is it the model? Is it my code?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Super_Nova02"&gt; /u/Super_Nova02 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr24r9/how_to_respond_with_a_tool_call/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr24r9/how_to_respond_with_a_tool_call/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qr24r9/how_to_respond_with_a_tool_call/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T10:44:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr3zvt</id>
    <title>Does Ollama respect parameters and system prompts for cloud models?</title>
    <updated>2026-01-30T12:23:10+00:00</updated>
    <author>
      <name>/u/soteko</name>
      <uri>https://old.reddit.com/user/soteko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using OpenWebUI and for local models I have workspace with system prompt and parameters for different use cases. &lt;/p&gt; &lt;p&gt;How that works with cloud models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/soteko"&gt; /u/soteko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr3zvt/does_ollama_respect_parameters_and_system_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr3zvt/does_ollama_respect_parameters_and_system_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qr3zvt/does_ollama_respect_parameters_and_system_prompts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T12:23:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr40ns</id>
    <title>Recommendation for Best Offline Ollama Models for Tailored CV Generation</title>
    <updated>2026-01-30T12:24:11+00:00</updated>
    <author>
      <name>/u/MavFir</name>
      <uri>https://old.reddit.com/user/MavFir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I am currently developing a script that uses offline Ollama models locally on my laptop to generate a tailored CV based on the following inputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Job description&lt;/li&gt; &lt;li&gt;Required skills&lt;/li&gt; &lt;li&gt;Original CV&lt;/li&gt; &lt;li&gt;Custom prompt&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I tested LLaMA 2, but the model mostly copies the original CV text instead of effectively tailoring it to the job requirements.&lt;/p&gt; &lt;p&gt;Due to memory constraints, I cannot download or experiment with many models. Therefore, I would really appreciate recommendations for one or two offline models that perform well in tasks like CV rewriting, summarization, and content adaptation.&lt;/p&gt; &lt;p&gt;Thank you in advance for your suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MavFir"&gt; /u/MavFir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr40ns/recommendation_for_best_offline_ollama_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr40ns/recommendation_for_best_offline_ollama_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qr40ns/recommendation_for_best_offline_ollama_models_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T12:24:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqiyot</id>
    <title>I wrote a biological memory layer for Ollama in Rust to replace stateless RAG</title>
    <updated>2026-01-29T19:48:35+00:00</updated>
    <author>
      <name>/u/ChikenNugetBBQSauce</name>
      <uri>https://old.reddit.com/user/ChikenNugetBBQSauce</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using Ollama heavily for local development but the lack of state persistence is a major bottleneck. The moment you terminate the session, the context is lost. Standard RAG implementations are often inefficient wrappers that flood the context window with low-relevance tokens.&lt;/p&gt; &lt;p&gt;I decided to solve this by engineering a dedicated memory server called Vestige.&lt;/p&gt; &lt;p&gt;It acts as a biological hippocampus for local agents. Instead of a flat vector search, I implemented the FSRS 6 algorithm directly in Rust to handle memory decay and reinforcement.&lt;/p&gt; &lt;p&gt;Here is the architecture.&lt;/p&gt; &lt;p&gt;The system uses a directed graph where nodes represent memories and edges represent synaptic weights. When Llama 3 queries the system, it calculates a retrievability score based on the spacing effect. Information you access frequently is reinforced, while irrelevant data naturally decays over time. This mimics biological neuroplasticity and keeps the context window efficient.&lt;/p&gt; &lt;p&gt;I initially prototyped this in Python but the serialization overhead during the graph traversal was unacceptable for real time chat. I rewrote the core logic in Rust using tokio and serde. The retrieval latency is now consistently under 8ms.&lt;/p&gt; &lt;p&gt;I designed this to run as a Model Context Protocol server. It sits alongside Ollama and handles the long-term state management so your agent actually remembers project details across sessions.&lt;/p&gt; &lt;p&gt;If you are tired of your local models resetting every time you close the terminal, you can check the code here.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/samvallad33/vestige"&gt;https://github.com/samvallad33/vestige&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChikenNugetBBQSauce"&gt; /u/ChikenNugetBBQSauce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqiyot/i_wrote_a_biological_memory_layer_for_ollama_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqiyot/i_wrote_a_biological_memory_layer_for_ollama_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qqiyot/i_wrote_a_biological_memory_layer_for_ollama_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T19:48:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrit9k</id>
    <title>Ollama AMD apprechiation post</title>
    <updated>2026-01-30T21:35:20+00:00</updated>
    <author>
      <name>/u/SnowTim07</name>
      <uri>https://old.reddit.com/user/SnowTim07</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnowTim07"&gt; /u/SnowTim07 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1qriswe/ollama_amd_apprechiation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrit9k/ollama_amd_apprechiation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qrit9k/ollama_amd_apprechiation_post/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T21:35:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrhttt</id>
    <title>Porting prompts from OpenAI/Claude to local Ollama models - best practices?</title>
    <updated>2026-01-30T20:58:38+00:00</updated>
    <author>
      <name>/u/gogeta1202</name>
      <uri>https://old.reddit.com/user/gogeta1202</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community ðŸ‘‹&lt;/p&gt; &lt;p&gt;Love the local-first approach. But I'm hitting a wall with prompt portability.&lt;/p&gt; &lt;p&gt;My prompts were developed on GPT-4/Claude and don't translate cleanly to local models.&lt;/p&gt; &lt;p&gt;Issues I'm seeing:&lt;br /&gt; â€¢ Instruction following is different&lt;br /&gt; â€¢ System prompt handling varies by model&lt;br /&gt; â€¢ Function calling support is inconsistent&lt;br /&gt; â€¢ Context window differences change behavior&lt;/p&gt; &lt;p&gt;How do you handle this?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Do you rewrite prompts from scratch for Ollama?&lt;/li&gt; &lt;li&gt;Is there a &amp;quot;universal&amp;quot; prompt style that works across models?&lt;/li&gt; &lt;li&gt;Any tools that help with conversion?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What I've built:&lt;/p&gt; &lt;p&gt;A prompt conversion tool focused on OpenAI â†” Anthropic right now. Quality validation using embeddings, checkpoint/rollback support.&lt;/p&gt; &lt;p&gt;Honest note: Local model support (Ollama/vLLM) isn't fully built yet. I'm validating if cloud â†’ local conversion is a real pain point worth solving.&lt;/p&gt; &lt;p&gt;Would love to hear:&lt;br /&gt; â€¢ What local models do you primarily use?&lt;br /&gt; â€¢ Biggest friction moving from cloud â†’ local?&lt;br /&gt; â€¢ Would you test a converter if local models were supported?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gogeta1202"&gt; /u/gogeta1202 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrhttt/porting_prompts_from_openaiclaude_to_local_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrhttt/porting_prompts_from_openaiclaude_to_local_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qrhttt/porting_prompts_from_openaiclaude_to_local_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T20:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrh5a9</id>
    <title>How do you choose a model and estimate hardware specs for a LangChain app (Ollama) ?</title>
    <updated>2026-01-30T20:33:04+00:00</updated>
    <author>
      <name>/u/XxDarkSasuke69xX</name>
      <uri>https://old.reddit.com/user/XxDarkSasuke69xX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. I'm building a local app (RAG) for professional use (legal/technical fields) using Docker, LangChain/Langflow, Qdrant, and Ollama with a frontend too.&lt;/p&gt; &lt;p&gt;The goal is a strict, reliable agent that answers based only on the provided files, cites sources, and states its confidence level. Since this is for professionals, accuracy is more important than speed, but I don't want it to take forever either. Also it would be nice if it could also look for an answer online if no relevant info was found in the files.&lt;/p&gt; &lt;p&gt;I'm struggling to figure out how to find the right model/hardware balance for this and would love some input.&lt;/p&gt; &lt;p&gt;How to choose a model for my need and that is available on Ollama ? I need something that follows system prompts well (like &amp;quot;don't guess if you don't know&amp;quot;) and handles a lot of context well. How to decide on number of parameters for example ? How to find the sweetspot without testing each and every model ?&lt;/p&gt; &lt;p&gt;How do you calculate the requirements for this ? If I'm loading a decent sized vector store and need a decently big context window, how much VRAM/RAM should I be targeting to run the LLM + embedding model + Qdrant smoothly ?&lt;/p&gt; &lt;p&gt;Like are there any benchmarks to estimate this ? I looked online but it's still pretty vague to me. Thx in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XxDarkSasuke69xX"&gt; /u/XxDarkSasuke69xX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrh5a9/how_do_you_choose_a_model_and_estimate_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrh5a9/how_do_you_choose_a_model_and_estimate_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qrh5a9/how_do_you_choose_a_model_and_estimate_hardware/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T20:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr4blw</id>
    <title>My first Local LLM</title>
    <updated>2026-01-30T12:38:17+00:00</updated>
    <author>
      <name>/u/swipegod43</name>
      <uri>https://old.reddit.com/user/swipegod43</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qr4blw/my_first_local_llm/"&gt; &lt;img alt="My first Local LLM" src="https://preview.redd.it/bxspnf0rehgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3e8245ed998f16b0a863ce0e65130cdb230e130" title="My first Local LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek-R1 q4_k_m 14b parameters on 12gb Vram it seems pretty fast ðŸ˜³ never woulda thought my old gaming PC could run an LLM this is pretty fascinating to me ðŸ˜‚ i literally just wanted to try it and got it up and running in a few hours im never using copilot again ðŸ’¯&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swipegod43"&gt; /u/swipegod43 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bxspnf0rehgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr4blw/my_first_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qr4blw/my_first_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T12:38:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qro11g</id>
    <title>AMD AI bundle</title>
    <updated>2026-01-31T01:03:43+00:00</updated>
    <author>
      <name>/u/Daine06</name>
      <uri>https://old.reddit.com/user/Daine06</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! I'm new to Local LLM so please bear with me. &lt;/p&gt; &lt;p&gt;I purchased a new card last week (9070 xt, if it matters). While I was fiddling with AMD software, I saw the AI bundle it offers to install. Intrigued, I tried installing Ollama. &lt;/p&gt; &lt;p&gt;Tried using their UI, prompted, entered, and I noticed that it was not using my GPU. Instead, it is using my CPU. Is it possible to offload from CPU to GPU? Is there any tutorial I can follow so I can set up Ollama properly?&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;What I kinda want to experiment on is Claude code and n8n.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daine06"&gt; /u/Daine06 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qro11g/amd_ai_bundle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qro11g/amd_ai_bundle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qro11g/amd_ai_bundle/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-31T01:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrtl3a</id>
    <title>Best local llm coding &amp; reasoning (Mac M1) ?</title>
    <updated>2026-01-31T05:21:28+00:00</updated>
    <author>
      <name>/u/Sherlock_holmes0007</name>
      <uri>https://old.reddit.com/user/Sherlock_holmes0007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says which is the best llm for coding and reasoning for Mac M1, doesn't have to be fully optimised a little slow is also okay but would prefer suggestions for both.&lt;/p&gt; &lt;p&gt;I'm trying to build a whole pipeline for my Mac that controls every task and even captures what's on the screen and debugs it live.&lt;/p&gt; &lt;p&gt;let's say I gave it a task of coding something and it creates code now ask it to debug and it's able to do that by capturing the content on screen.&lt;/p&gt; &lt;p&gt;Was also thinking about doing a hybrid setup where I have local model for normal tasks and Claude API for high reasoning and coding tasks.&lt;/p&gt; &lt;p&gt;Other suggestions and whole pipeline setup ideas would be very welcomed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sherlock_holmes0007"&gt; /u/Sherlock_holmes0007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AgentsOfAI/comments/1qrtg8y/best_local_llm_coding_reasoning_mac_m1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrtl3a/best_local_llm_coding_reasoning_mac_m1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qrtl3a/best_local_llm_coding_reasoning_mac_m1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-31T05:21:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs4fsv</id>
    <title>Llm for personal health</title>
    <updated>2026-01-31T14:56:14+00:00</updated>
    <author>
      <name>/u/pyare-p13</name>
      <uri>https://old.reddit.com/user/pyare-p13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pyare-p13"&gt; /u/pyare-p13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ArtificialInteligence/comments/1qs4ev5/llm_for_personal_health/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qs4fsv/llm_for_personal_health/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qs4fsv/llm_for_personal_health/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-31T14:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrkbsr</id>
    <title>Run Ollama on your Android!</title>
    <updated>2026-01-30T22:32:38+00:00</updated>
    <author>
      <name>/u/DutchOfBurdock</name>
      <uri>https://old.reddit.com/user/DutchOfBurdock</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Want to put this out here. I have a Samsung S20 and a Pixel 8 Pro. Both of these devices pack 12GB of RAM, one an octacore arrangement and the other a nonacore. Now, this is pure CPU and even Vulkan (despite hardware support), doesn't work.&lt;/p&gt; &lt;p&gt;First, get yourself Termux from F-Droid or GitHub. &lt;strong&gt;Don't use the Play Store version.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Upon launching Termux, update the package manager and install some things needed..&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pkg up pkg i build-essential git cmake golang git clone https://github.com/ollama/ollama.git cd ollama go generate ./... go build . &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If all went well, you'll end up with an &lt;code&gt;ollama&lt;/code&gt; executable in the folder.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./ollama serve &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Open a new terminal in the gitted ollama folder &lt;/p&gt; &lt;pre&gt;&lt;code&gt;./ollama pull smollm2 ./ollama run smollm2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This model should be small enough for even 4GB devices and is pretty fast.&lt;/p&gt; &lt;p&gt;Enjoy and start exploring!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DutchOfBurdock"&gt; /u/DutchOfBurdock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrkbsr/run_ollama_on_your_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrkbsr/run_ollama_on_your_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qrkbsr/run_ollama_on_your_android/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T22:32:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsjp2s</id>
    <title>Free AI Tool Training - 100 Licenses (Claude Code, Claude Desktop, OpenClaw)</title>
    <updated>2026-02-01T00:47:54+00:00</updated>
    <author>
      <name>/u/SeriousDocument7905</name>
      <uri>https://old.reddit.com/user/SeriousDocument7905</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qsjp2s/free_ai_tool_training_100_licenses_claude_code/"&gt; &lt;img alt="Free AI Tool Training - 100 Licenses (Claude Code, Claude Desktop, OpenClaw)" src="https://external-preview.redd.it/BfjyCn6CzN8PQQImI976uTwVuuLCP3_lcIwslOoXhTg.png?width=140&amp;amp;height=73&amp;amp;auto=webp&amp;amp;s=d9576a1d44e11039aeaeaee9e2c35a43613ec8bb" title="Free AI Tool Training - 100 Licenses (Claude Code, Claude Desktop, OpenClaw)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SeriousDocument7905"&gt; /u/SeriousDocument7905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/vibecoding/comments/1qsj42l/free_ai_tool_training_100_licenses_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qsjp2s/free_ai_tool_training_100_licenses_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qsjp2s/free_ai_tool_training_100_licenses_claude_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T00:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qry7r9</id>
    <title>[Ollama Cloud] 29.7% failure rate, 3,500+ errors in one session, support ignoring tickets for 2 weeks - Is this normal?</title>
    <updated>2026-01-31T09:46:10+00:00</updated>
    <author>
      <name>/u/Few-Point-3626</name>
      <uri>https://old.reddit.com/user/Few-Point-3626</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;'ve been using Ollama Cloud API for my production workflow (content moderation)&lt;/p&gt; &lt;p&gt;and I'm experiencing catastrophic reliability issues that are making the service&lt;/p&gt; &lt;p&gt;unusable.&lt;/p&gt; &lt;p&gt;## The Numbers (documented with full logs)&lt;/p&gt; &lt;p&gt;| Metric | Value |&lt;/p&gt; &lt;p&gt;|--------|-------|&lt;/p&gt; &lt;p&gt;| Total requests sent | 4,079 |&lt;/p&gt; &lt;p&gt;| Successful responses | 2,868 |&lt;/p&gt; &lt;p&gt;| **Failed requests** | **1,211** |&lt;/p&gt; &lt;p&gt;| **Failure rate** | **29.7%** |&lt;/p&gt; &lt;p&gt;## Incident Timeline&lt;/p&gt; &lt;p&gt;| Date | Error 429 | Error 500 | Success Rate |&lt;/p&gt; &lt;p&gt;|------|-----------|-----------|--------------|&lt;/p&gt; &lt;p&gt;| Dec 10, 2025 | 235 | 0 | 0% |&lt;/p&gt; &lt;p&gt;| Dec 20, 2025 | 0 | 30 | 0% |&lt;/p&gt; &lt;p&gt;| **Jan 4, 2026** | **3,508** | 0 | **0%** |&lt;/p&gt; &lt;p&gt;| Jan 29, 2026 | 0 | 0 | 86.8% |&lt;/p&gt; &lt;p&gt;| Jan 30, 2026 | 0 | 0 | 74.3% |&lt;/p&gt; &lt;p&gt;| **Jan 31, 2026** | 0 | **194** | **28.8%** |&lt;/p&gt; &lt;p&gt;Yes, you read that right: **3,508 consecutive 429 errors in 40 minutes** on&lt;/p&gt; &lt;p&gt;January 4th.&lt;/p&gt; &lt;p&gt;## The Pattern&lt;/p&gt; &lt;p&gt;Every session follows the same pattern:&lt;/p&gt; &lt;p&gt;- ~30 requests succeed normally&lt;/p&gt; &lt;p&gt;- Then the server crashes with 500 errors&lt;/p&gt; &lt;p&gt;- All subsequent requests fail&lt;/p&gt; &lt;p&gt;- I have to restart and hope for the best&lt;/p&gt; &lt;p&gt;## My Configuration&lt;/p&gt; &lt;p&gt;- Model: deepseek-v3.1:671b&lt;/p&gt; &lt;p&gt;- Concurrent requests: 3 (using 3 separate API keys)&lt;/p&gt; &lt;p&gt;- Workers per key: 1 (minimal load)&lt;/p&gt; &lt;p&gt;- Timeout: 25 seconds&lt;/p&gt; &lt;p&gt;I'm not hammering the API. 3 concurrent requests with 3 different API keys is&lt;/p&gt; &lt;p&gt;extremely conservative.&lt;/p&gt; &lt;p&gt;## Support Response&lt;/p&gt; &lt;p&gt;I opened a support ticket on **January 18th, 2026**.&lt;/p&gt; &lt;p&gt;**Response received: NONE.**&lt;/p&gt; &lt;p&gt;It's been 2 weeks. Radio silence. No acknowledgment, no &amp;quot;we're looking into it&amp;quot;,&lt;/p&gt; &lt;p&gt;nothing.&lt;/p&gt; &lt;p&gt;## Questions for the Community&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Is anyone else experiencing similar issues with deepseek models on Ollama Cloud?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Is this level of unreliability normal?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Has anyone actually gotten a response from Ollama support (&lt;a href="mailto:hello@ollama.com"&gt;hello@ollama.com&lt;/a&gt;)?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Are there alternative providers for deepseek-v3 that are more reliable?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;## What I'm Asking Ollama&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Investigate why your servers are returning 3,500+ 429 errors in a single session&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Investigate the 500 errors that crash the service after ~30 requests&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Respond to support tickets&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Credit for the failed requests that were still billed&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I have complete logs documenting every single error with timestamps. Happy to&lt;/p&gt; &lt;p&gt;share with Ollama support if they ever decide to respond.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**Edit:** I'll update this post if/when I get a response.&lt;/p&gt; &lt;p&gt;**Edit 2:** For those asking, my use case is legitimate content moderation for a&lt;/p&gt; &lt;p&gt;French platform. ~200-300 requests per day, nothing excessive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Point-3626"&gt; /u/Few-Point-3626 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qry7r9/ollama_cloud_297_failure_rate_3500_errors_in_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qry7r9/ollama_cloud_297_failure_rate_3500_errors_in_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qry7r9/ollama_cloud_297_failure_rate_3500_errors_in_one/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-31T09:46:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qslux9</id>
    <title>`Request timed out` when running `ollama launch claude` with `glm-4.7-flash:latest`</title>
    <updated>2026-02-01T02:23:15+00:00</updated>
    <author>
      <name>/u/o-rka</name>
      <uri>https://old.reddit.com/user/o-rka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running &lt;code&gt;claude-code&lt;/code&gt; via &lt;code&gt;ollama&lt;/code&gt; using the &lt;code&gt;glm-4.7-flash:latest&lt;/code&gt; model on a M4 MacMini and I've made sure to adjust my context window to 64k. Here's the specs below: &lt;/p&gt; &lt;p&gt;``` Chip: Apple M4 Pro Total Number of Cores: 14 (10 performance and 4 efficiency) Memory: 64 GB&lt;/p&gt; &lt;pre&gt;&lt;code&gt; Type: GPU Bus: Built-In Total Number of Cores: 20 Vendor: Apple (0x106b) Metal Support: Metal 3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Is there any other settings I can adjust or is my machine not powerful enough to handle the task? &lt;/p&gt; &lt;p&gt;The task being to modify a Nextflow pipeline based on the specifications in my &lt;code&gt;CLAUDE.md&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/o-rka"&gt; /u/o-rka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qslux9/request_timed_out_when_running_ollama_launch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qslux9/request_timed_out_when_running_ollama_launch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qslux9/request_timed_out_when_running_ollama_launch/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T02:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt661t</id>
    <title>Does that even make sense?</title>
    <updated>2026-02-01T18:21:36+00:00</updated>
    <author>
      <name>/u/artwik22</name>
      <uri>https://old.reddit.com/user/artwik22</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a homelab running on Intel n97 and 16gb of ram. Is there any llm model I could run?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/artwik22"&gt; /u/artwik22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt661t/does_that_even_make_sense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt661t/does_that_even_make_sense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qt661t/does_that_even_make_sense/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T18:21:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs233a</id>
    <title>Best open weight llm model to run with 8gb of vram</title>
    <updated>2026-01-31T13:16:18+00:00</updated>
    <author>
      <name>/u/Sweazou</name>
      <uri>https://old.reddit.com/user/Sweazou</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd like to get your thought on the best model you can use with 8gb of vram in 2026, with the best performance possible for general purpose and coding, the least censorship possible, i know this won't be as good as state of the art llm but i'd like to try something good i can run locally&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sweazou"&gt; /u/Sweazou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qs233a/best_open_weight_llm_model_to_run_with_8gb_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qs233a/best_open_weight_llm_model_to_run_with_8gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qs233a/best_open_weight_llm_model_to_run_with_8gb_of_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-31T13:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsqs24</id>
    <title>The two agentic loops - the architectural insight in how we built and scaled agents</title>
    <updated>2026-02-01T06:23:19+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey peeps - been building agents for the Fortune500 and seeing some patterns emerge that cut the &lt;strong&gt;gargantuan gap&lt;/strong&gt; from prototype to production &lt;/p&gt; &lt;p&gt;The post below introduces the concept of &amp;quot;two agentic loops&amp;quot;: the inner loop that handles reasoning and tool use, while the outer loop handles everything that makes agents ready for productionâ€”orchestration, guardrails, observability, and bounded execution. The outer loop is real infrastructure that needs to be built and maintained independently in a framework-friendly and protocol-first way. Hope you enjoy the read&lt;/p&gt; &lt;p&gt;&lt;a href="https://planoai.dev/blog/the-two-agentic-loops-how-to-design-and-scale-agentic-apps"&gt;https://planoai.dev/blog/the-two-agentic-loops-how-to-design-and-scale-agentic-apps&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qsqs24/the_two_agentic_loops_the_architectural_insight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qsqs24/the_two_agentic_loops_the_architectural_insight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qsqs24/the_two_agentic_loops_the_architectural_insight/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T06:23:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt42lf</id>
    <title>I can not have a quick respond when using Ollama run with Claude on my local machine</title>
    <updated>2026-02-01T17:07:21+00:00</updated>
    <author>
      <name>/u/Cultural_Somewhere70</name>
      <uri>https://old.reddit.com/user/Cultural_Somewhere70</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qt42lf/i_can_not_have_a_quick_respond_when_using_ollama/"&gt; &lt;img alt="I can not have a quick respond when using Ollama run with Claude on my local machine" src="https://b.thumbs.redditmedia.com/MPmmxYIQweWKa7Qu1T02Wh1p477Klw0-CIHtjRDGEWQ.jpg" title="I can not have a quick respond when using Ollama run with Claude on my local machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I am student in back end developer. I just found that we can run Ollama by Claude on local machine.&lt;/p&gt; &lt;p&gt;I just made it by the blog guideline and it was installed. But i actually facing some issues:&lt;/p&gt; &lt;p&gt;- I really want to know why it reply so slow, is that because i don't have GPU cause now i run it on CPU.&lt;/p&gt; &lt;p&gt;- How many RAM gb should i upgrade to make it faster? Current 24gb Ram.&lt;/p&gt; &lt;p&gt;- How do you run ollama by claude on your laptop?&lt;/p&gt; &lt;p&gt;- what i actually need to add and upgrade to run a quick respond by using AI local?&lt;/p&gt; &lt;p&gt;I am really appreciate!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p5ze5zd0zwgg1.png?width=872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c412ac53782d5194cd8055afb582551ddab9d1db"&gt;https://preview.redd.it/p5ze5zd0zwgg1.png?width=872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c412ac53782d5194cd8055afb582551ddab9d1db&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cultural_Somewhere70"&gt; /u/Cultural_Somewhere70 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt42lf/i_can_not_have_a_quick_respond_when_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt42lf/i_can_not_have_a_quick_respond_when_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qt42lf/i_can_not_have_a_quick_respond_when_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T17:07:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt5x7i</id>
    <title>Ollama on R9700 AI Pro</title>
    <updated>2026-02-01T18:12:55+00:00</updated>
    <author>
      <name>/u/grimescene2</name>
      <uri>https://old.reddit.com/user/grimescene2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello fellow Radeonans (I just made that up)&lt;/p&gt; &lt;p&gt;I recently procured the Radeon R9700 AI pro GPU with 32gb VRAM. The experience has been solid so far with Comfyui / Flux generation on Windows 11.&lt;/p&gt; &lt;p&gt;But I have not been able to run Ollama properly on the machine. The installation doesnâ€™t detect the card, and then even after doing some hacks in the Environment Variables (thanks for Gemini) only the smaller (3-4B) models work. Anything greater than 8B just crashes it.&lt;/p&gt; &lt;p&gt;Has anyone here had similar experiences? Any fixes?&lt;/p&gt; &lt;p&gt;Would appreciate guidance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grimescene2"&gt; /u/grimescene2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt5x7i/ollama_on_r9700_ai_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt5x7i/ollama_on_r9700_ai_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qt5x7i/ollama_on_r9700_ai_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T18:12:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt04u5</id>
    <title>OpenClaw For data scientist that support Ollama</title>
    <updated>2026-02-01T14:39:27+00:00</updated>
    <author>
      <name>/u/jasonhon2013</name>
      <uri>https://old.reddit.com/user/jasonhon2013</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qt04u5/openclaw_for_data_scientist_that_support_ollama/"&gt; &lt;img alt="OpenClaw For data scientist that support Ollama" src="https://external-preview.redd.it/6n0lheaeZysEyY3dN5Kt5g7XRf3lYCD1kKO5LEgkWkA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f1f98cfb816d1f258b4f3b19195340579de5e4d" title="OpenClaw For data scientist that support Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an open-source tool that works like OpenClaw (i.e., web searches all the necessary content in the background and provides you with data). It supports Ollama. You can give it a tryâ€”hehe, and maybe give me a little star as well!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonhon2013"&gt; /u/jasonhon2013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/JasonHonKL/PardusClawer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt04u5/openclaw_for_data_scientist_that_support_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qt04u5/openclaw_for_data_scientist_that_support_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T14:39:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt8fph</id>
    <title>Sentinel: Monitoring logs with local AI (Ollama) &amp; .NET 8</title>
    <updated>2026-02-01T19:40:45+00:00</updated>
    <author>
      <name>/u/Itsaliensbro453</name>
      <uri>https://old.reddit.com/user/Itsaliensbro453</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Itsaliensbro453"&gt; /u/Itsaliensbro453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/SideProject/comments/1qt8fc1/sentinel_monitoring_logs_with_local_ai_ollama_net/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt8fph/sentinel_monitoring_logs_with_local_ai_ollama_net/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qt8fph/sentinel_monitoring_logs_with_local_ai_ollama_net/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T19:40:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsjn38</id>
    <title>Running Ollama fully air-gapped, anyone else?</title>
    <updated>2026-02-01T00:45:31+00:00</updated>
    <author>
      <name>/u/thefilthybeard</name>
      <uri>https://old.reddit.com/user/thefilthybeard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been building AI tools that run fully air-gapped for classified environments. No internet, no cloud, everything local.&lt;/p&gt; &lt;p&gt;Ollama has been solid for this. Running it on hardware that never touches a network. Biggest challenges were model selection (needed stuff that performs well without massive VRAM) and building workflows that don't assume any external API calls.&lt;/p&gt; &lt;p&gt;Curious what others are doing for fully offline deployments. Anyone else running Ollama in secure or disconnected environments? What models are you using and what are you running it on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thefilthybeard"&gt; /u/thefilthybeard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qsjn38/running_ollama_fully_airgapped_anyone_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qsjn38/running_ollama_fully_airgapped_anyone_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qsjn38/running_ollama_fully_airgapped_anyone_else/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T00:45:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt6kik</id>
    <title>Vlm models on cpu</title>
    <updated>2026-02-01T18:35:21+00:00</updated>
    <author>
      <name>/u/uqurluuqur</name>
      <uri>https://old.reddit.com/user/uqurluuqur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt; &lt;p&gt;I am tasked to convert handwritten notebook texts. I have tried several models including:&lt;/p&gt; &lt;p&gt;Qwen2.5vl- 7b&lt;/p&gt; &lt;p&gt;Qwen2.5vl- 32b&lt;/p&gt; &lt;p&gt;Qwen3vl-32b&lt;/p&gt; &lt;p&gt;Llama3.2-vision11b&lt;/p&gt; &lt;p&gt;However, i am struggling with hallucinations. Instead of writing unable to read (which i ask for it in the prompt), models often start to hallucinate or getting stuck in the header (repeat loop). Improving or trying other prompts did not helped. I have tried preprocessing, which improved the quality but did not prevent hallucinations. Do you have any suggestions?&lt;/p&gt; &lt;p&gt;I have amd threadripper cpu and 64 gb ram. Speed is not an issue since it is a one time thing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uqurluuqur"&gt; /u/uqurluuqur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt6kik/vlm_models_on_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt6kik/vlm_models_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qt6kik/vlm_models_on_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T18:35:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt7wzm</id>
    <title>Reprompt - Simple desktop GUI application to avoid writing the same prompts repeatedly</title>
    <updated>2026-02-01T19:22:11+00:00</updated>
    <author>
      <name>/u/PuzzleheadedHeat9056</name>
      <uri>https://old.reddit.com/user/PuzzleheadedHeat9056</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qt7wzm/reprompt_simple_desktop_gui_application_to_avoid/"&gt; &lt;img alt="Reprompt - Simple desktop GUI application to avoid writing the same prompts repeatedly" src="https://preview.redd.it/u520ax05nxgg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=b849674b757f4c804d3b68e8106d96bd54802b4a" title="Reprompt - Simple desktop GUI application to avoid writing the same prompts repeatedly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'd like to share the app I created last summer, and have been using it since then.&lt;br /&gt; It is called Reprompt - &lt;a href="https://github.com/grouzen/reprompt"&gt;https://github.com/grouzen/reprompt&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;It is a simple desktop GUI app written in Rust and egui that allows users to ask models the same questions without having to type the prompts repeatedly.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;I personally found it useful for language-related tasks, such as translation, correcting typos, and improving grammar. Currently, it supports Ollama only, but other providers can be easily added if needed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PuzzleheadedHeat9056"&gt; /u/PuzzleheadedHeat9056 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u520ax05nxgg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qt7wzm/reprompt_simple_desktop_gui_application_to_avoid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qt7wzm/reprompt_simple_desktop_gui_application_to_avoid/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-01T19:22:11+00:00</published>
  </entry>
</feed>
