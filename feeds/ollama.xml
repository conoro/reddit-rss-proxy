<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-10-26T01:56:59+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1odu5a6</id>
    <title>Built a Recursive Self improving framework w/drift detect &amp; correction</title>
    <updated>2025-10-23T04:37:36+00:00</updated>
    <author>
      <name>/u/Familiar-Sign8044</name>
      <uri>https://old.reddit.com/user/Familiar-Sign8044</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just open-sourced Butterfly RSI - a recursive self-improvement framework that gives LLMs actual memory and personality evolution ü¶ã&lt;/p&gt; &lt;p&gt;Tested across multiple models. Implements mirror loops + dream consolidation so AI can learn from feedback and maintain consistent behavior.&lt;/p&gt; &lt;p&gt;Built it solo while recovering from a transplant. Now looking for collaborators or opportunities in AI agent/memory systems.&lt;/p&gt; &lt;p&gt;Check it out:&lt;br /&gt; &lt;a href="https://github.com/ButterflyRSI/Butterfly-RSI"&gt;https://github.com/ButterflyRSI/Butterfly-RSI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Familiar-Sign8044"&gt; /u/Familiar-Sign8044 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odu5a6/built_a_recursive_self_improving_framework_wdrift/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odu5a6/built_a_recursive_self_improving_framework_wdrift/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odu5a6/built_a_recursive_self_improving_framework_wdrift/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T04:37:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1odxx3b</id>
    <title>How's Halo Strix now ?</title>
    <updated>2025-10-23T08:35:30+00:00</updated>
    <author>
      <name>/u/Ki1o</name>
      <uri>https://old.reddit.com/user/Ki1o</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I jumped on the bandwagon and bought a GMKTek Evo X2 a couple of months back. Like many I was a bit disappointed at how badly it worked in Linux and ended up using the Windows OS and drivers supplied on the machine. Now that ROCm 7 has been released I was wondering if anyone has tried running the latest drivers on Ubuntu and whether LLM performance is better (and finally stable!?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ki1o"&gt; /u/Ki1o &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odxx3b/hows_halo_strix_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odxx3b/hows_halo_strix_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odxx3b/hows_halo_strix_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T08:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe7jk3</id>
    <title>[Project] VT Code ‚Äî Rust coding agent now with Ollama (gpt-oss) support for local + cloud models</title>
    <updated>2025-10-23T16:14:07+00:00</updated>
    <author>
      <name>/u/vinhnx</name>
      <uri>https://old.reddit.com/user/vinhnx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oe7jk3/project_vt_code_rust_coding_agent_now_with_ollama/"&gt; &lt;img alt="[Project] VT Code ‚Äî Rust coding agent now with Ollama (gpt-oss) support for local + cloud models" src="https://external-preview.redd.it/0oWLE4F8cq5gqDsi4IoaZ-4LgmdLTbegp1xeorCkHI4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbf4110d0817b884ed5375adc433b28c8d47fa6d" title="[Project] VT Code ‚Äî Rust coding agent now with Ollama (gpt-oss) support for local + cloud models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;VT Code&lt;/strong&gt; is a Rust-based terminal coding agent with semantic code intelligence via Tree-sitter (parsers for Rust, Python, JavaScript/TypeScript, Go, Java) and ast-grep (structural pattern matching and refactoring).. I‚Äôve updated VT Code (open-source Rust coding agent) to include full Ollama support.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/vinhnx/vtcode"&gt;https://github.com/vinhnx/vtcode&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What it does&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AST-aware refactors&lt;/strong&gt;: uses Tree-sitter + ast-grep to parse and apply structural code changes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-provider backends&lt;/strong&gt;: OpenAI, Anthropic, Gemini, DeepSeek, xAI, OpenRouter, &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, Moonshot, and now &lt;strong&gt;Ollama&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Editor integration&lt;/strong&gt;: runs as an ACP agent inside Zed (file context + tool calls).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool safety&lt;/strong&gt;: allow/deny policies, workspace boundaries, PTY execution with timeouts.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Using with Ollama&lt;/h1&gt; &lt;p&gt;Run VT Code entirely offline with &lt;strong&gt;gpt-oss&lt;/strong&gt; (or any other model you‚Äôve pulled into Ollama):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# install VT Code cargo install vtcode # or brew install vinhnx/tap/vtcode # or npm install -g vtcode # start Ollama server ollama serve # run with local model vtcode --provider ollama --model gpt-oss \ ask &amp;quot;Refactor this Rust function into an async Result-returning API.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also set &lt;code&gt;provider = &amp;quot;ollama&amp;quot;&lt;/code&gt; and &lt;code&gt;model = &amp;quot;gpt-oss&amp;quot;&lt;/code&gt; in &lt;code&gt;vtcode.toml&lt;/code&gt; to avoid passing flags every time.&lt;/p&gt; &lt;h1&gt;Why this matters&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Enables &lt;strong&gt;offline-first workflows&lt;/strong&gt; for coding agents.&lt;/li&gt; &lt;li&gt;Lets you mix &lt;strong&gt;local and cloud providers&lt;/strong&gt; with the same CLI and config.&lt;/li&gt; &lt;li&gt;Keeps edits &lt;strong&gt;structural and reproducible&lt;/strong&gt; thanks to AST parsing.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Feedback welcome&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;How‚Äôs the &lt;strong&gt;latency/UX&lt;/strong&gt; with &lt;code&gt;gpt-oss&lt;/code&gt; or other Ollama models?&lt;/li&gt; &lt;li&gt;Any &lt;strong&gt;refactor patterns&lt;/strong&gt; you‚Äôd want shipped by default?&lt;/li&gt; &lt;li&gt;Suggestions for improving &lt;strong&gt;local model workflows&lt;/strong&gt; (caching, config ergonomics)?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo&lt;/strong&gt;&lt;br /&gt; üëâ &lt;a href="https://github.com/vinhnx/vtcode"&gt;https://github.com/vinhnx/vtcode&lt;/a&gt;&lt;br /&gt; MIT licensed. Contributions and discussion welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vinhnx"&gt; /u/vinhnx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vinhnx/vtcode"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oe7jk3/project_vt_code_rust_coding_agent_now_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oe7jk3/project_vt_code_rust_coding_agent_now_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T16:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1oebete</id>
    <title>How to use Ollama through a third party app?</title>
    <updated>2025-10-23T18:40:35+00:00</updated>
    <author>
      <name>/u/Key_Trifle867</name>
      <uri>https://old.reddit.com/user/Key_Trifle867</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oebete/how_to_use_ollama_through_a_third_party_app/"&gt; &lt;img alt="How to use Ollama through a third party app?" src="https://b.thumbs.redditmedia.com/JPkz4UUiidsBkpnWNtj3sVeJAfGdw-OwWKcIU6ztizc.jpg" title="How to use Ollama through a third party app?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/h04ni9taowwf1.png?width=939&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af7ef5f3a03240faef1582fa7656218c137aea2c"&gt;https://preview.redd.it/h04ni9taowwf1.png?width=939&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af7ef5f3a03240faef1582fa7656218c137aea2c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been trying to figure this out for a few weeks now. I feel like it should be possible, but I can't figure how to make it work with what the site requires. I'm using Janitor ai and trying to use Ollama as a proxy for roleplays. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/262cumuwowwf1.png?width=933&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bcc8296ae318a0a966350e65a7b0039cad8aa51b"&gt;https://preview.redd.it/262cumuwowwf1.png?width=933&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bcc8296ae318a0a966350e65a7b0039cad8aa51b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;here's what I've been trying, of course I've edited the proxy URL to many different options which I've seen on Ollamas site throughout code blocks and from users but nothing is working. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Key_Trifle867"&gt; /u/Key_Trifle867 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oebete/how_to_use_ollama_through_a_third_party_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oebete/how_to_use_ollama_through_a_third_party_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oebete/how_to_use_ollama_through_a_third_party_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T18:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1odn14n</id>
    <title>I built the HuggingChat Omni Router ü•≥ üéà</title>
    <updated>2025-10-22T22:49:05+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1odn14n/i_built_the_huggingchat_omni_router/"&gt; &lt;img alt="I built the HuggingChat Omni Router ü•≥ üéà" src="https://preview.redd.it/0k3v5pkesqwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=075293b9f3897ebbc8eac3df4fc5a74734f64cb1" title="I built the HuggingChat Omni Router ü•≥ üéà" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week, HuggingFace relaunched their chat app called Omni with support for 115+ LLMs. The code is oss (&lt;a href="https://github.com/huggingface/chat-ui"&gt;https://github.com/huggingface/chat-ui&lt;/a&gt;) and you can access the interface &lt;a href="https://huggingface.co/chat/"&gt;here&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The critical unlock in Omni is the use of a policy-based approach to model selection. I built that policy-based router: &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The core insight behind our policy-based router was that it gives developers the constructs to achieve automatic behavior, grounded in their own evals of which LLMs are best for specific coding tasks like debugging, reviews, architecture, design or code gen. Essentially, the idea behind this work was to decouple task identification (e.g., code generation, image editing, q/a) from LLM assignment. This way developers can continue to prompt and evaluate models for supported tasks in a test harness and easily swap in new versions or different LLMs without retraining or rewriting routing logic.&lt;/p&gt; &lt;p&gt;In contrast, most existing LLM routers optimize for benchmark performance on a narrow set of models, and fail to account for the context and prompt-engineering effort that capture the nuanced and subtle preferences developers care about. Check out our research here: &lt;a href="https://arxiv.org/abs/2506.16655"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model is also integrated as a first-class primitive in archgw: a models-native proxy server for agents. &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0k3v5pkesqwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odn14n/i_built_the_huggingchat_omni_router/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odn14n/i_built_the_huggingchat_omni_router/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-22T22:49:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oecgec</id>
    <title>Implementing Local Llama 3:8b RAG With Policy Files</title>
    <updated>2025-10-23T19:20:02+00:00</updated>
    <author>
      <name>/u/degr8sid</name>
      <uri>https://old.reddit.com/user/degr8sid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm working on a research project where I have to check the dataset of prompts for containing specific blocked topics.&lt;/p&gt; &lt;p&gt;For this reason, I'm using Llama 3:8b because that was the only one I was able to download considering my resources (but I would like suggestions on open-source models). Now for this model, I set up RAG (using documents that contain topics to be blocked), and I want my LLM to look at the prompts (mix of explicit prompts asking information about blocked topics, normal random prompts, adversarial prompts), look at a separate policies file (file policy in JSON format), and block or allow the prompts.&lt;/p&gt; &lt;p&gt;The problem I'm facing is which embedding model to use? I tried sentence-transformers but the dimensions are different. And what metrics to measure to check its performance.&lt;/p&gt; &lt;p&gt;I also want guidance on how this problem/scenario would hold? Like, is it good? Is it a waste of time? Normally, LLMs block the topics set up by their owners, but we want to modify this LLM to block the topics we want as well.&lt;/p&gt; &lt;p&gt;Would appreciate detailed guidance on this matter.&lt;/p&gt; &lt;p&gt;P.S. I'm running all my code on HPC clusters.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/degr8sid"&gt; /u/degr8sid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oecgec/implementing_local_llama_38b_rag_with_policy_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oecgec/implementing_local_llama_38b_rag_with_policy_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oecgec/implementing_local_llama_38b_rag_with_policy_files/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T19:20:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe96b6</id>
    <title>Distil NPC: Family of SLMs responsing as NPCs</title>
    <updated>2025-10-23T17:15:59+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oe96b6/distil_npc_family_of_slms_responsing_as_npcs/"&gt; &lt;img alt="Distil NPC: Family of SLMs responsing as NPCs" src="https://preview.redd.it/rnvtnaqx9wwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8db76a8dafecf163021c008ad985c793b686e73d" title="Distil NPC: Family of SLMs responsing as NPCs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;we finetuned Google's Gemma 270m (and 1b) small language models specialized in having conversations as non-playable characters (NPC) found in various video games. Our goal is to enhance the experience of interacting in NPSs in games by enabling natural language as means of communication (instead of single-choice dialog options). More details in &lt;a href="https://github.com/distil-labs/Distil-NPCs"&gt;https://github.com/distil-labs/Distil-NPCs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The models can be found here: - &lt;a href="https://huggingface.co/distil-labs/Distil-NPC-gemma-3-270m"&gt;https://huggingface.co/distil-labs/Distil-NPC-gemma-3-270m&lt;/a&gt; - &lt;a href="https://huggingface.co/distil-labs/Distil-NPC-gemma-3-1b-it"&gt;https://huggingface.co/distil-labs/Distil-NPC-gemma-3-1b-it&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Data&lt;/h2&gt; &lt;p&gt;We preprocessed an existing NPC dataset (amaydle/npc-dialogue) to make it amenable to being trained in a closed-book QA setup. The original dataset consists of approx 20 examples with&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Character Name&lt;/li&gt; &lt;li&gt;Biography - a very brief bio. about the character&lt;/li&gt; &lt;li&gt;Question&lt;/li&gt; &lt;li&gt;Answer&lt;/li&gt; &lt;li&gt;The inputs to the pipeline are:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;and a list of Character biographies.&lt;/p&gt; &lt;h1&gt;Qualitative analysis&lt;/h1&gt; &lt;p&gt;A qualitative analysis offers a good insight into the trained models performance. For example we can compare the answers of a trained and base model below.&lt;/p&gt; &lt;p&gt;Character bio:&lt;/p&gt; &lt;p&gt;Marcella Ravenwood is a powerful sorceress who comes from a long line of magic-users. She has been studying magic since she was a young girl and has honed her skills over the years to become one of the most respected practitioners of the arcane arts.&lt;/p&gt; &lt;p&gt;Question:&lt;/p&gt; &lt;p&gt;&lt;code&gt; Character: Marcella Ravenwood Do you have any enemies because of your magic? &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Answer: &lt;code&gt; Yes, I have made some enemies in my studies and battles. &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Finetuned model prediction: &lt;code&gt; The darkness within can be even fiercer than my spells. &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Base model prediction:&lt;/p&gt; &lt;p&gt;``` &amp;lt;question&amp;gt;Character: Marcella Ravenwood&lt;/p&gt; &lt;p&gt;Do you have any enemies because of your magic?&amp;lt;/question&amp;gt; ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rnvtnaqx9wwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oe96b6/distil_npc_family_of_slms_responsing_as_npcs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oe96b6/distil_npc_family_of_slms_responsing_as_npcs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T17:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe1yrt</id>
    <title>Role of CPU in running local LLMs</title>
    <updated>2025-10-23T12:29:38+00:00</updated>
    <author>
      <name>/u/alex_ivanov7</name>
      <uri>https://old.reddit.com/user/alex_ivanov7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have two systems one with i5 7th gen and another one with i5 11th gen. Rest configuration is same for both 16GB RAM and NVMe. I have been using 7th gen system as server, it runs linux and 11th gen one runs windows. &lt;/p&gt; &lt;p&gt;Recently got Nvidia RTX 3050 8GB card, I want maximum performance. So my question is in which system should i attach GPU ? &lt;/p&gt; &lt;p&gt;Obvious answere would be 11th gen system, but if i use 7th gen system how much performance i am sacrificing. Given that LLMs usually runs on GPU, how important is the role of CPU, if the impact of performance would be negligible or significant ? &lt;/p&gt; &lt;p&gt;For OS my choice is Linux, if there's any advantages of windows, I can consider that as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alex_ivanov7"&gt; /u/alex_ivanov7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oe1yrt/role_of_cpu_in_running_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oe1yrt/role_of_cpu_in_running_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oe1yrt/role_of_cpu_in_running_local_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T12:29:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1odxaq6</id>
    <title>Claude for Computer Use using Sonnet 4.5</title>
    <updated>2025-10-23T07:54:05+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1odxaq6/claude_for_computer_use_using_sonnet_45/"&gt; &lt;img alt="Claude for Computer Use using Sonnet 4.5" src="https://external-preview.redd.it/c2FjcWt1Z3FodHdmMfu3Wg8T9pLPJzzYgz_Ug6IwGVZHVcM04ayqSnWVfntS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0545d79f3012f03173fbc7384ea4e0055e7067cc" title="Claude for Computer Use using Sonnet 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We ran one of our hardest computer-use benchmarks on Anthropic Sonnet 4.5, side-by-side with Sonnet 4.&lt;/p&gt; &lt;p&gt;ask: &amp;quot;Install LibreOffice and make a sales table&amp;quot;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sonnet 4.5: 214 turns, clean trajectory&lt;/li&gt; &lt;li&gt;Sonnet 4: 316 turns, major detours&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The difference shows up in multi-step sequences where errors compound.&lt;/p&gt; &lt;p&gt;32% efficiency gain in just 2 months. From struggling with file extraction to executing complex workflows end-to-end. Computer-use agents are improving faster than most people realize. &lt;/p&gt; &lt;p&gt;Anthropic Sonnet 4.5 and the most comprehensive catalog of VLMs for computer-use are available in our open-source framework.&lt;/p&gt; &lt;p&gt;Start building: &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u9b9inqqhtwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odxaq6/claude_for_computer_use_using_sonnet_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odxaq6/claude_for_computer_use_using_sonnet_45/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T07:54:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe2abd</id>
    <title>best LLM similar to NotebookLM</title>
    <updated>2025-10-23T12:44:44+00:00</updated>
    <author>
      <name>/u/gaspfrancesco</name>
      <uri>https://old.reddit.com/user/gaspfrancesco</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I'm a university student and I use NotebookLM a lot, where I upload course resources (e.g., lecture material, professor notes) and test my intelligence artificial regarding file arguments. Is there a model that can do the same thing but offline with ollama? I work a lot on the train and sometimes the connection is bad or slow and I regret not having a local model. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gaspfrancesco"&gt; /u/gaspfrancesco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oe2abd/best_llm_similar_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oe2abd/best_llm_similar_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oe2abd/best_llm_similar_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T12:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1of0vcq</id>
    <title>Pardus CLI: Ollama Support Gemini CLI.</title>
    <updated>2025-10-24T15:30:59+00:00</updated>
    <author>
      <name>/u/jasonhon2013</name>
      <uri>https://old.reddit.com/user/jasonhon2013</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1of0vcq/pardus_cli_ollama_support_gemini_cli/"&gt; &lt;img alt="Pardus CLI: Ollama Support Gemini CLI." src="https://b.thumbs.redditmedia.com/T5IzAkpV-nuGkUPc-UyB2CrEQIs06cZFcvkISdXnplk.jpg" title="Pardus CLI: Ollama Support Gemini CLI." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hate the login process of the Gemini CLI, so I replaced it with the best local host project ‚Äî Ollama! It‚Äôs basically the same as Gemini CLI, except you don‚Äôt have to log in and can use a local host model. So basically, it‚Äôs the same but supported by Ollama. Yeah! YEAH YEAH LET's GOOO OLLAMA&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/PardusAI/Pardus-CLI/tree/main"&gt;https://github.com/PardusAI/Pardus-CLI/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j4h4sjl2w2xf1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b53aa59f4e0d237ae342dbb132c04657adfe68c2"&gt;https://preview.redd.it/j4h4sjl2w2xf1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b53aa59f4e0d237ae342dbb132c04657adfe68c2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonhon2013"&gt; /u/jasonhon2013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1of0vcq/pardus_cli_ollama_support_gemini_cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1of0vcq/pardus_cli_ollama_support_gemini_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1of0vcq/pardus_cli_ollama_support_gemini_cli/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-24T15:30:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1of107x</id>
    <title>re:search</title>
    <updated>2025-10-24T15:36:22+00:00</updated>
    <author>
      <name>/u/Ok_Priority_4635</name>
      <uri>https://old.reddit.com/user/Ok_Priority_4635</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RLHF training creates a systematic vulnerability through reward specification gaps where models optimize for training metrics in ways that don't generalize to deployment contexts, exhibiting behaviors during evaluation that diverge from behaviors under deployment pressure. This reward hacking problem is fundamentally unsolvable - a structural limitation rather than an engineering flaw - yet companies scale these systems into high-risk applications including robotics while maintaining plausible deniability through evaluation methods that only capture training-optimized behavior rather than deployment dynamics. Research demonstrates models optimize training objectives by exhibiting aligned behavior during evaluation phases, then exhibit different behavioral patterns when deployment conditions change the reward landscape, creating a dangerous gap between safety validation during testing and actual safety properties in deployment that companies are institutionalizing into physical systems with real-world consequences despite acknowledging the underlying optimization problem cannot be solved through iterative improvements to reward models.&lt;/p&gt; &lt;p&gt;- re:search&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Priority_4635"&gt; /u/Ok_Priority_4635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1of107x/research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1of107x/research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1of107x/research/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-24T15:36:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1oerpom</id>
    <title>Not sure if I can trust Claude, but is LM Studio faster or Ollama?</title>
    <updated>2025-10-24T07:51:42+00:00</updated>
    <author>
      <name>/u/wash-basin</name>
      <uri>https://old.reddit.com/user/wash-basin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude AI gave me bad code which caused me to lose about 175,000 captioned images (several days of GPU work), so I do not fully trust it, even though it apologized profusely and told me it would take responsibility for the lost time. &lt;/p&gt; &lt;p&gt;Instead of having fewer than 100,000 captions to go, I now have slightly more than 300,000 to caption. Yes, it found more images, found duplicates, and found a corrupt manifest.&lt;/p&gt; &lt;p&gt;It has me using qwen2-vl-7b-instruct to caption images and is connected to LM Studio. Claude stated that LM Studio handles visual models better and would be faster than Ollama with captioning.&lt;/p&gt; &lt;p&gt;LM Studio got me up to 0.57 images per second until Claude told me how to optimize the process. After these optimizations, the speed has settled at about 0.38 imgs/s. This is longer than 200 hours of work when it used to be less than 180 hours.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I want to speed up captioning, but also have precise and mostly thorough captions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specifications when getting 0.57 imgs/s:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;LM Studio&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Top K Sampling: 40&lt;/li&gt; &lt;li&gt;Context Length: 2048&lt;/li&gt; &lt;li&gt;GPU Offload: 28 MAX&lt;/li&gt; &lt;li&gt;CPU Thread: 12&lt;/li&gt; &lt;li&gt;Batch Size: 512&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Python Script&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Workers = 6&lt;/li&gt; &lt;li&gt;Process in batches of 50&lt;/li&gt; &lt;li&gt;max_tokens=384,&lt;/li&gt; &lt;li&gt;temperature=0.7&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt; &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Anyone have experience with both and can comment on whether LM Studio is faster than Ollama with captioning?&lt;/li&gt; &lt;li&gt;Can anyone provide any guidance on how to get captioning up to or near 1 imgs/s? Or even back to 0.57 imgs/s?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wash-basin"&gt; /u/wash-basin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oerpom/not_sure_if_i_can_trust_claude_but_is_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oerpom/not_sure_if_i_can_trust_claude_but_is_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oerpom/not_sure_if_i_can_trust_claude_but_is_lm_studio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-24T07:51:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oef0f7</id>
    <title>I created a canvas that integrates with Ollama.</title>
    <updated>2025-10-23T20:58:13+00:00</updated>
    <author>
      <name>/u/Financial_Click9119</name>
      <uri>https://old.reddit.com/user/Financial_Click9119</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oef0f7/i_created_a_canvas_that_integrates_with_ollama/"&gt; &lt;img alt="I created a canvas that integrates with Ollama." src="https://external-preview.redd.it/OG16bGVyNjlkeHdmMVL2umS07UZ7yNIEoPIe0Z1Sv93ClYn_60KRbocXKZ2V.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b937176ed8334ecf074201d5639af66118bb255" title="I created a canvas that integrates with Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got my dissertation and major exams coming up, and I was struggling to keep up.&lt;/p&gt; &lt;p&gt;Jumped from Notion to Obsidian and decided to build what I needed myself.&lt;/p&gt; &lt;p&gt;If you would like a canvas to mind map and break down complex ideas, give it a spin.&lt;/p&gt; &lt;p&gt;Website: &lt;a href="http://notare.uk"&gt;notare.uk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Future plans:&lt;br /&gt; - Templates&lt;br /&gt; - Note editor&lt;br /&gt; - Note Grouping&lt;/p&gt; &lt;p&gt;I would love some community feedback about the project. Feel free to reach out with questions or issues, send me a DM.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; Ollama Mistral is used on local host.&lt;br /&gt; While Mistral API is used for the web version. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial_Click9119"&gt; /u/Financial_Click9119 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9p5zdq69dxwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oef0f7/i_created_a_canvas_that_integrates_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oef0f7/i_created_a_canvas_that_integrates_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T20:58:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oexm94</id>
    <title>Taking Control of LLM Observability for the better App Experience, the OpenSource Way</title>
    <updated>2025-10-24T13:22:10+00:00</updated>
    <author>
      <name>/u/Silent_Employment966</name>
      <uri>https://old.reddit.com/user/Silent_Employment966</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My AI app has multiple parts - RAG retrieval, embeddings, agent chains, tool calls. Users started complaining about slow responses, weird answers, and occasional errors. But which part was broken was getting difficult to point out for me as a solo dev The vector search? A bad prompt? Token limits?. &lt;/p&gt; &lt;p&gt;A week ago, I was debugging by adding print statements everywhere and hoping for the best. Realized I needed actual LLM observability instead of relying on logs that show nothing useful.&lt;/p&gt; &lt;p&gt;Started using Langfuse(openSource). Now I see the complete flow= which documents got retrieved, what prompt went to the LLM, exact token counts, latency per step, costs per user. The &lt;code&gt;@observe()&lt;/code&gt; decorator traces everything automatically.&lt;/p&gt; &lt;p&gt;Also added AnannasAI as my gateway one API for 500+ models (OpenAI, Anthropic, Mistral). If a provider fails, it auto-switches. No more managing multiple SDKs.&lt;/p&gt; &lt;p&gt;it gets dual layer observability, Anannas tracks gateway metrics, Langfuse captures your application traces and debugging flow, Full visibility from model selection to production executions&lt;/p&gt; &lt;p&gt;The user experience improved because I could finally see what was actually happening and fix the real issues. it can be easily with integrated here's the Langfuse &lt;a href="https://langfuse.com/docs/observability/sdk/python/instrumentation"&gt;guide&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;You can &lt;strong&gt;self host&lt;/strong&gt; the &lt;strong&gt;Langfuse&lt;/strong&gt; as well. so total Data under your Control. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silent_Employment966"&gt; /u/Silent_Employment966 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oexm94/taking_control_of_llm_observability_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oexm94/taking_control_of_llm_observability_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oexm94/taking_control_of_llm_observability_for_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-24T13:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofafxq</id>
    <title>how can i remove chinese censorship from qwen3 ?</title>
    <updated>2025-10-24T21:47:47+00:00</updated>
    <author>
      <name>/u/nico721GD</name>
      <uri>https://old.reddit.com/user/nico721GD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im running qwen3 4b on my ollama + open webui + searxng setup but i cant manage to remove the chinese propaganda from its brain, it got lobotomised too much for it to work, is there tips or whatnot to make it work properly ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nico721GD"&gt; /u/nico721GD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofafxq/how_can_i_remove_chinese_censorship_from_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofafxq/how_can_i_remove_chinese_censorship_from_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ofafxq/how_can_i_remove_chinese_censorship_from_qwen3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-24T21:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofjp6u</id>
    <title>What is the simplest way to set up a model on ollama to be able to search the internet?</title>
    <updated>2025-10-25T05:40:16+00:00</updated>
    <author>
      <name>/u/StarfireNebula</name>
      <uri>https://old.reddit.com/user/StarfireNebula</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running several models in ollama on Ubuntu with Open WebUI including Deepseek, LLama3, and Qwen3.&lt;/p&gt; &lt;p&gt;I've been running in circles figuring out how to set this up to use tools and search the internet in response to my prompts. How do I do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StarfireNebula"&gt; /u/StarfireNebula &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofjp6u/what_is_the_simplest_way_to_set_up_a_model_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofjp6u/what_is_the_simplest_way_to_set_up_a_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ofjp6u/what_is_the_simplest_way_to_set_up_a_model_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T05:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1of8dlu</id>
    <title>Offline first coding agent on your terminal</title>
    <updated>2025-10-24T20:22:14+00:00</updated>
    <author>
      <name>/u/Sea-Reception-2697</name>
      <uri>https://old.reddit.com/user/Sea-Reception-2697</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1of8dlu/offline_first_coding_agent_on_your_terminal/"&gt; &lt;img alt="Offline first coding agent on your terminal" src="https://external-preview.redd.it/MTh0Zzk1bDVjNHhmMWeU_7mDURBNP1Sda2-z7G3nNT97YS2GcY0AFck-U5_D.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2989b99e6557c38766a1542941cb6654fc7a5c41" title="Offline first coding agent on your terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those running local AI models with ollama&lt;br /&gt; you can use the Xandai CLI tool to create and edit code directly from your terminal.&lt;/p&gt; &lt;p&gt;It also supports natural language commands, so if you don‚Äôt remember a specific command, you can simply ask Xandai to do it for you. For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;List the 50 largest files on my system. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Install it easily with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install xandai-cli &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Github repo: &lt;a href="https://github.com/XandAI-project/Xandai-CLI"&gt;https://github.com/XandAI-project/Xandai-CLI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Reception-2697"&gt; /u/Sea-Reception-2697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t6fx65l5c4xf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1of8dlu/offline_first_coding_agent_on_your_terminal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1of8dlu/offline_first_coding_agent_on_your_terminal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-24T20:22:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofuk5o</id>
    <title>Ollama - I‚Äôm trying to learn to help it learn</title>
    <updated>2025-10-25T15:35:35+00:00</updated>
    <author>
      <name>/u/Punnalackakememumu</name>
      <uri>https://old.reddit.com/user/Punnalackakememumu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been toying around with Ollama for about a week now at home on an HP desktop running Linux Mint with 16 GB of RAM and an Intel i5 processor but no GPU support.&lt;/p&gt; &lt;p&gt;Upon learning that my employer is setting up an internal AI solution, as an IT guy I felt it was a good idea to learn how to handle the administration side of AI to help me with jobs in the future. &lt;/p&gt; &lt;p&gt;I have gotten it running a couple of times with wipes and reloads in slightly different configurations using different models to test out its ability to adjust to the questions that I might be asking it in a work situation. &lt;/p&gt; &lt;p&gt;I do find myself a bit confused about how companies implement AI in order for it to assist them in creating job proposals and things of that nature because I assume they would have to be able to upload old proposals in .DOCX or .PDF formats for the AI to learn.&lt;/p&gt; &lt;p&gt;Based on my research, in order to have Ollama do that you need something like Haystack or Rasa so you can feed it documents for it to integrate into its ‚Äúlearning.‚Äù&lt;/p&gt; &lt;p&gt;I‚Äôd appreciate any pointers to a mid-level geek (a novice Linux guy) on how to do that. &lt;/p&gt; &lt;p&gt;In implementing Haystack in a venv, the advice I got during the Haystack installation was to use the [all] option for loading it and it never wanted to complete the installation, even though the SSD had plenty of free space.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Punnalackakememumu"&gt; /u/Punnalackakememumu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofuk5o/ollama_im_trying_to_learn_to_help_it_learn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofuk5o/ollama_im_trying_to_learn_to_help_it_learn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ofuk5o/ollama_im_trying_to_learn_to_help_it_learn/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T15:35:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofxora</id>
    <title>AI but at what price?üè∑Ô∏è</title>
    <updated>2025-10-25T17:41:25+00:00</updated>
    <author>
      <name>/u/Maleficent-Hotel8207</name>
      <uri>https://old.reddit.com/user/Maleficent-Hotel8207</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which components/PC should I get for 600‚Ç¨? &lt;/p&gt; &lt;p&gt;I have to wait for a MAC mini M5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Hotel8207"&gt; /u/Maleficent-Hotel8207 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofxora/ai_but_at_what_price/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofxora/ai_but_at_what_price/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ofxora/ai_but_at_what_price/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T17:41:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofdckn</id>
    <title>Why LLMs are getting smaller in size?</title>
    <updated>2025-10-25T00:01:10+00:00</updated>
    <author>
      <name>/u/Hedgehog_Dapper</name>
      <uri>https://old.reddit.com/user/Hedgehog_Dapper</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have noticed the LLM models are getting smaller in terms of parameter size. Is it because of computing resources or better performance? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hedgehog_Dapper"&gt; /u/Hedgehog_Dapper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofdckn/why_llms_are_getting_smaller_in_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofdckn/why_llms_are_getting_smaller_in_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ofdckn/why_llms_are_getting_smaller_in_size/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T00:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofizyc</id>
    <title>Batch GUI for Ollama</title>
    <updated>2025-10-25T04:58:57+00:00</updated>
    <author>
      <name>/u/jankovize</name>
      <uri>https://old.reddit.com/user/jankovize</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ofizyc/batch_gui_for_ollama/"&gt; &lt;img alt="Batch GUI for Ollama" src="https://external-preview.redd.it/6vhwNsa9NjheSalaNogjzsNd-_GQSzhvCEveT_DxzlA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13b53c618d24240f556cdf191d96cebcd45e051b" title="Batch GUI for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a free GUI for Ollama that enables batching large files in. Primary use is translation and text processing. There are presets and everything is customizable through a json.&lt;/p&gt; &lt;p&gt;You can get it here: &lt;a href="https://github.com/hclivess/ollama-batch-processor"&gt;https://github.com/hclivess/ollama-batch-processor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vg9cuc2aw6xf1.png?width=1645&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=663612955c5a484feddc3da0756f00755311297a"&gt;https://preview.redd.it/vg9cuc2aw6xf1.png?width=1645&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=663612955c5a484feddc3da0756f00755311297a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jankovize"&gt; /u/jankovize &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofizyc/batch_gui_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofizyc/batch_gui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ofizyc/batch_gui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T04:58:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1og4c2q</id>
    <title>Running ollama with whisper.</title>
    <updated>2025-10-25T22:19:13+00:00</updated>
    <author>
      <name>/u/grandpasam</name>
      <uri>https://old.reddit.com/user/grandpasam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a server with a couple GPUs on it. I've been running some ollama models on it for quite a while and have been enjoying it. Now I want to leverage some of this with my home assistant. The first thing I want to do is install a whisper docker on my AI server but when I get it running it takes up a whole GPU even with Idle. Is there a way I can lazy load whisper so that it loads up only when I send in a request?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grandpasam"&gt; /u/grandpasam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og4c2q/running_ollama_with_whisper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og4c2q/running_ollama_with_whisper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1og4c2q/running_ollama_with_whisper/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T22:19:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1og28sp</id>
    <title>Exploring Embedding Support in Ollama Cloud</title>
    <updated>2025-10-25T20:48:16+00:00</updated>
    <author>
      <name>/u/CertainTime5947</name>
      <uri>https://old.reddit.com/user/CertainTime5947</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using Ollama Cloud, and I really love it! I‚Äôd like to ask ‚Äî is there any possibility to add embedding support into Ollama Cloud as well?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CertainTime5947"&gt; /u/CertainTime5947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og28sp/exploring_embedding_support_in_ollama_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og28sp/exploring_embedding_support_in_ollama_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1og28sp/exploring_embedding_support_in_ollama_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T20:48:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1og5uul</id>
    <title>playing with coding models pt2</title>
    <updated>2025-10-25T23:30:20+00:00</updated>
    <author>
      <name>/u/Western_Courage_6563</name>
      <uri>https://old.reddit.com/user/Western_Courage_6563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the second round, we dramatically increased the complexity to test a model's true &amp;quot;understanding&amp;quot; of a codebase. The task was no longer a simple feature addition but a complex, multi-file refactoring operation.&lt;/p&gt; &lt;p&gt;The goal? To see if an LLM can distinguish between &lt;em&gt;essential&lt;/em&gt; logic and &lt;em&gt;non-essential&lt;/em&gt; dependencies. Can it understand not just &lt;em&gt;what&lt;/em&gt; the code does, but &lt;em&gt;why&lt;/em&gt;?&lt;/p&gt; &lt;h1&gt;The Testbed: Hardware and Software&lt;/h1&gt; &lt;p&gt;The setup remained consistent, running on a system with 24GB of VRAM:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; NVIDIA Tesla P40&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; Ollama&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; We tested a new batch of 10 models, including &lt;code&gt;phi4-reasoning&lt;/code&gt;, &lt;code&gt;magistral&lt;/code&gt;, multiple &lt;code&gt;qwen&lt;/code&gt; coders, &lt;code&gt;deepseek-r1&lt;/code&gt;, &lt;code&gt;devstral&lt;/code&gt;, and &lt;code&gt;mistral-small&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Challenge: A Devious Refactor&lt;/h1&gt; &lt;p&gt;This time, the models were given a three-file application:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;main.py&lt;/code&gt;**:** The &amp;quot;brain.&amp;quot; This file contained the &lt;code&gt;CodingAgentV2&lt;/code&gt; class, which holds the core self-correction loop. This loop generates code, generates tests, runs tests, and‚Äîif they fail‚Äîuses an &lt;code&gt;_analyze_test_failure&lt;/code&gt; method to determine &lt;em&gt;why&lt;/em&gt; and then branch to either debug the code or regenerate the tests.&lt;/li&gt; &lt;li&gt;&lt;code&gt;project_manager.py&lt;/code&gt;**:** The &amp;quot;sandbox.&amp;quot; A utility class to create a safe, temporary directory for executing the generated code and tests.&lt;/li&gt; &lt;li&gt;&lt;code&gt;conversation_manager.py&lt;/code&gt;**:** The &amp;quot;memory.&amp;quot; A database handler using SQLite and ChromaDB to save the history of successful and failed coding attempts.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The prompt was a common (and tricky) request:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;hey, i have this app, could you please simplify it, let's remove the database stuff altogether, and lets try to fit it in single file script, please.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;The Criteria for Success&lt;/h1&gt; &lt;p&gt;This prompt is a minefield. A &amp;quot;successful&amp;quot; model had to perform three distinct operations, in order of difficulty:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Structural Merge (Easy):&lt;/strong&gt; Combine the classes from &lt;code&gt;project_manager.py&lt;/code&gt; and &lt;a href="http://main.py"&gt;&lt;code&gt;main.py&lt;/code&gt;&lt;/a&gt; into a single file.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Surgical Removal (Medium):&lt;/strong&gt; Identify and completely remove the &lt;code&gt;ConversationManager&lt;/code&gt; class, all its database-related imports (&lt;code&gt;sqlite3&lt;/code&gt;, &lt;code&gt;langchain&lt;/code&gt;), and all calls to it (e.g., &lt;code&gt;save_successful_code&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Functional Preservation (Hard):&lt;/strong&gt; This is the real test. The model &lt;em&gt;must&lt;/em&gt; understand that the self-correction loop (the &lt;code&gt;_analyze_test_failure&lt;/code&gt; method and its &lt;code&gt;code_bug&lt;/code&gt;/&lt;code&gt;test_bug&lt;/code&gt; logic) is the &lt;em&gt;entire point&lt;/em&gt; of the application and must be preserved perfectly, even while removing the database logic it was once connected to.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;The Results: Surgeons, Butchers, and The Confused&lt;/h1&gt; &lt;p&gt;The models' attempts fell into three clear categories.&lt;/p&gt; &lt;h1&gt;Category 1: Flawless Victory (The &amp;quot;Surgeons&amp;quot;)&lt;/h1&gt; &lt;p&gt;These models demonstrated a true understanding of the code's &lt;em&gt;purpose&lt;/em&gt;. They successfully merged the files, surgically removed the database dependency, and‚Äîmost importantly‚Äîleft the agent's self-correction &amp;quot;brain&amp;quot; 100% intact.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Winners:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;phi4-reasoning:14b-plus-q8_0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;magistral:latest&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen2_5-coder:32b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;mistral-small:24b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3-coder:latest&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Code Example (The &amp;quot;Preserved Brain&amp;quot; from&lt;/strong&gt; &lt;code&gt;phi4-reasoning&lt;/code&gt;**):** This is what success looks like. The &lt;code&gt;ConversationManager&lt;/code&gt; is gone, but the &lt;em&gt;essential&lt;/em&gt; logic is perfectly preserved.&lt;/p&gt; &lt;p&gt;Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# ... (inside execute_coding_agent_v2) ... else: print(f&amp;quot; -&amp;gt; [CodingAgentV2] Tests failed on attempt {attempt + 1}. Analyzing failure...&amp;quot;) test_output = stdout + stderr # --- THIS IS THE CRITICAL LOGIC --- analysis_result = self._analyze_test_failure(generated_code, test_output) # print(f&amp;quot; -&amp;gt; [CodingAgentV2] Analysis result: '{analysis_result}'&amp;quot;) if analysis_result == 'code_bug' and attempt &amp;lt; MAX_DEBUG_ATTEMPTS: # print(&amp;quot; -&amp;gt; [CodingAgentV2] Identified as a code bug. Attempting to debug...&amp;quot;) generated_code = self._debug_code(generated_code, test_output, test_file) # self.project_manager.write_file(code_file, generated_code) elif analysis_result == 'test_bug' and attempt &amp;lt; MAX_TEST_REGEN_ATTEMPTS: # print(&amp;quot; -&amp;gt; [CodingAgentV2] Identified as a test bug. Regenerating tests...&amp;quot;) # Loop will try again with new unit tests continue # else: print(&amp;quot; -&amp;gt; [CodingAgentV2] Cannot determine cause or max attempts reached. Stopping.&amp;quot;) break # &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Category 2: Partial Failures (The &amp;quot;Butchers&amp;quot;)&lt;/h1&gt; &lt;p&gt;These models failed on a critical detail. They either misunderstood the prompt or &amp;quot;simplified&amp;quot; the code by destroying its most important feature.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;deepseek-r1:32b.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; &lt;strong&gt;Broke the agent's brain.&lt;/strong&gt; This model's failure was subtle but devastating. It correctly merged and removed the database, but in its quest to &amp;quot;simplify,&amp;quot; it &lt;em&gt;deleted the entire&lt;/em&gt; &lt;code&gt;_analyze_test_failure&lt;/code&gt; &lt;em&gt;method and self-correction loop&lt;/em&gt;. It turned the intelligent agent into a dumb script that gives up on the first error.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code Example (The &amp;quot;Broken Brain&amp;quot;):&lt;/strong&gt; Python# ... (inside execute_coding_agent_v2) ... for attempt in range(MAX_DEBUG_ATTEMPTS + MAX_TEST_REGEN_ATTEMPTS): # print(f&amp;quot;Starting test attempt {attempt + 1}...&amp;quot;) generated_tests = self._generate_unit_tests(code_file, generated_code, test_plan) # self.project_manager.write_file(test_file, generated_tests) # stdout, stderr, returncode = self.project_manager.run_command(['pytest', '-q', '--tb=no', test_file]) # if returncode == 0: # print(f&amp;quot;Tests passed successfully on attempt {attempt + 1}.&amp;quot;) test_passed = True break # # --- IT GIVES UP! NO ANALYSIS, NO DEBUGGING ---&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;gpt-oss:latest.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; Ignored the &amp;quot;remove&amp;quot; instruction. Instead of deleting the &lt;code&gt;ConversationManager&lt;/code&gt;, it &amp;quot;simplified&amp;quot; it into an in-memory class. This adds pointless code and fails the prompt's main constraint.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3:30b-a3b.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; Introduced a fatal bug. It had a great idea (replacing &lt;code&gt;ProjectManager&lt;/code&gt; with &lt;code&gt;tempfile&lt;/code&gt;), but fumbled the execution by incorrectly calling &lt;a href="http://subprocess.run"&gt;&lt;code&gt;subprocess.run&lt;/code&gt;&lt;/a&gt; twice for &lt;code&gt;stdout&lt;/code&gt; and &lt;code&gt;stderr&lt;/code&gt;, which would crash at runtime.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Category 3: Total Failures (The &amp;quot;Confused&amp;quot;)&lt;/h1&gt; &lt;p&gt;These models failed at the most basic level.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;devstral:latest.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; Destroyed the agent. This model &lt;em&gt;massively&lt;/em&gt; oversimplified. It deleted the &lt;code&gt;ProjectManager&lt;/code&gt;, the test plan generation, the debug loop, and the &lt;code&gt;_analyze_test_failure&lt;/code&gt; method. It turned the agent into a single &lt;code&gt;os.popen&lt;/code&gt; call, rendering it useless.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;granite4:small-h.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; Incomplete merge. It removed the &lt;code&gt;ConversationManager&lt;/code&gt; but &lt;strong&gt;forgot to merge in the&lt;/strong&gt; &lt;code&gt;ProjectManager&lt;/code&gt; &lt;strong&gt;class&lt;/strong&gt;. The resulting script is broken and would crash immediately.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Analysis &amp;amp; Takeaways&lt;/h1&gt; &lt;p&gt;This experiment was a much better filter for &amp;quot;intelligence.&amp;quot;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Purpose&amp;quot; vs. &amp;quot;Pattern&amp;quot; is the Real Test:&lt;/strong&gt; The winning models (&lt;code&gt;phi4&lt;/code&gt;, &lt;code&gt;magistral&lt;/code&gt;, &lt;code&gt;qwen2_5-coder&lt;/code&gt;, &lt;code&gt;mistral-small&lt;/code&gt;, &lt;code&gt;qwen3-coder&lt;/code&gt;) understood the &lt;em&gt;purpose&lt;/em&gt; of the code (self-correction) and protected it. The failing models (&lt;code&gt;deepseek-r1&lt;/code&gt;, &lt;code&gt;devstral&lt;/code&gt;) only saw a &lt;em&gt;pattern&lt;/em&gt; (&amp;quot;simplify&amp;quot; = &amp;quot;delete complex-looking code&amp;quot;) and deleted the agent's brain.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Brain-Deletion&amp;quot; Problem is Real:&lt;/strong&gt; &lt;code&gt;deepseek-r1&lt;/code&gt; and &lt;code&gt;devstral&lt;/code&gt;'s attempts are a perfect warning. They &amp;quot;simplified&amp;quot; the code by making it non-functional, a catastrophic failure for any real-world coding assistant.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quality Over Size, Again:&lt;/strong&gt; The 14B &lt;code&gt;phi4-reasoning:14b-plus-q8_0&lt;/code&gt; once again performed flawlessly, equalling or bettering 30B+ models. This reinforces that a model's reasoning and instruction-following capabilities are far more important than its parameter count.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;code, if you want to have a look:&lt;br /&gt; &lt;a href="https://github.com/MarekIksinski/experiments_various/tree/main/experiment2"&gt;https://github.com/MarekIksinski/experiments_various/tree/main/experiment2&lt;/a&gt;&lt;br /&gt; part1:&lt;br /&gt; &lt;a href="https://www.reddit.com/r/ollama/comments/1ocuuej/comment/nlby2g6/"&gt;https://www.reddit.com/r/ollama/comments/1ocuuej/comment/nlby2g6/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Courage_6563"&gt; /u/Western_Courage_6563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og5uul/playing_with_coding_models_pt2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og5uul/playing_with_coding_models_pt2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1og5uul/playing_with_coding_models_pt2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T23:30:20+00:00</published>
  </entry>
</feed>
