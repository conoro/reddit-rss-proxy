<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-17T02:13:03+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1negg4w</id>
    <title>Introducing Ally, an open source CLI assistant</title>
    <updated>2025-09-11T18:24:40+00:00</updated>
    <author>
      <name>/u/YassinK97</name>
      <uri>https://old.reddit.com/user/YassinK97</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1negg4w/introducing_ally_an_open_source_cli_assistant/"&gt; &lt;img alt="Introducing Ally, an open source CLI assistant" src="https://external-preview.redd.it/uwkmfNcDLcZDlQ8FYiWmiighX4Q13I5okEpaYg1NwcY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=726d52086d0a6f2f9f545a19b8caab3e4fb43a58" title="Introducing Ally, an open source CLI assistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d9f9kw2uvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a11406fa1b82a2c11d87a83923206dc663f3dcaf"&gt;https://preview.redd.it/d9f9kw2uvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a11406fa1b82a2c11d87a83923206dc663f3dcaf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/YassWorks/Ally"&gt;Ally&lt;/a&gt; is a CLI multi-agent assistant that can assist with coding, searching and running commands.&lt;/p&gt; &lt;p&gt;I made this tool because I wanted to make agents with Ollama models but then added support for OpenAI, Anthropic, Gemini (Google Gen AI) and Cerebras for more flexibility.&lt;/p&gt; &lt;p&gt;What makes Ally special is that It can be 100% local and private. A law firm or a lab could run this on a server and benefit from all the things tools like Claude Code and Gemini Code have to offer. It’s also designed to understand context (by not feeding entire history and irrelevant tool calls to the LLM) and use tokens efficiently, providing a reliable, hallucination-free experience even on smaller models.&lt;/p&gt; &lt;p&gt;While still in its early stages, Ally provides a vibe coding framework that goes through brainstorming and coding phases with all under human supervision.&lt;/p&gt; &lt;p&gt;I intend to more features (one coming soon is RAG) but preferred to post about it at this stage for some feedback and visibility.&lt;/p&gt; &lt;p&gt;Give it a go: &lt;a href="https://github.com/YassWorks/Ally"&gt;https://github.com/YassWorks/Ally&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More screenshots:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zyl96inuvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de9cf20053ff2e5f890ea2e7fc9dc668600a263a"&gt;https://preview.redd.it/zyl96inuvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de9cf20053ff2e5f890ea2e7fc9dc668600a263a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8wp9awvuvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b7104092cd3020b43162082000ce2d8f77dabe5"&gt;https://preview.redd.it/8wp9awvuvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b7104092cd3020b43162082000ce2d8f77dabe5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YassinK97"&gt; /u/YassinK97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1negg4w/introducing_ally_an_open_source_cli_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1negg4w/introducing_ally_an_open_source_cli_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1negg4w/introducing_ally_an_open_source_cli_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-11T18:24:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1neupwp</id>
    <title>Recommended model for lightweight text tagging</title>
    <updated>2025-09-12T05:17:37+00:00</updated>
    <author>
      <name>/u/xegoba7006</name>
      <uri>https://old.reddit.com/user/xegoba7006</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I don't know much about LLMS so I'm looking for somebody with more experience to recommend me a model for a side project of mine.&lt;/p&gt; &lt;p&gt;I need something super lightweight (as it's running on a cheap hetzner VPS). &lt;/p&gt; &lt;p&gt;The use case is also pretty simple: I want to feed it some text (Just a couple sentences, nothing long) and get some recommended categories/labels/tags for the given text.&lt;/p&gt; &lt;p&gt;What would you recommend?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xegoba7006"&gt; /u/xegoba7006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neupwp/recommended_model_for_lightweight_text_tagging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neupwp/recommended_model_for_lightweight_text_tagging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1neupwp/recommended_model_for_lightweight_text_tagging/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T05:17:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf18q3</id>
    <title>Llama Builds is now in beta! PcPartPicker for Local AI Builds</title>
    <updated>2025-09-12T11:58:42+00:00</updated>
    <author>
      <name>/u/Vegetable_Low2907</name>
      <uri>https://old.reddit.com/user/Vegetable_Low2907</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nf18q3/llama_builds_is_now_in_beta_pcpartpicker_for/"&gt; &lt;img alt="Llama Builds is now in beta! PcPartPicker for Local AI Builds" src="https://b.thumbs.redditmedia.com/piGiXthvayvHA3TydCUURjfU2oa57pJxlMBT0HnK9oo.jpg" title="Llama Builds is now in beta! PcPartPicker for Local AI Builds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Low2907"&gt; /u/Vegetable_Low2907 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nersq0/llama_builds_is_now_in_beta_pcpartpicker_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nf18q3/llama_builds_is_now_in_beta_pcpartpicker_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nf18q3/llama_builds_is_now_in_beta_pcpartpicker_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T11:58:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf3pbh</id>
    <title>Recommendations On Model For Journal Style Writing</title>
    <updated>2025-09-12T13:46:57+00:00</updated>
    <author>
      <name>/u/Extra_Upstairs4075</name>
      <uri>https://old.reddit.com/user/Extra_Upstairs4075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All, found some time today to do something I've been wanting to do for a while now. Download and setup MSTY and also Ollama now it has a UI. So far so good. One of the main tasks I was wanting to complete was to take many, many pages of daily notes, written in dot points, and run them through AI to turn them into paragraph style notes / journal entries.&lt;/p&gt; &lt;p&gt;I tested this with with ChatGPT some time ago and was surprised how well it worked, though, I would like to complete this on a local AI. So - I have Qwen3 and DeepSeek R1 models running. I gave both of these a daily section of dot points to write into a paragraph style journal entry, they both seemed relatively average, they both completely added in bits that didn't exist in the summary I provided.&lt;/p&gt; &lt;p&gt;My question, as somebody new to this - there's so many models available, is there any that could be recommended for my use case? Is there any recommendations I could try to improve the answers I receive?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extra_Upstairs4075"&gt; /u/Extra_Upstairs4075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nf3pbh/recommendations_on_model_for_journal_style_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nf3pbh/recommendations_on_model_for_journal_style_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nf3pbh/recommendations_on_model_for_journal_style_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T13:46:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfgehx</id>
    <title>Small model recommendation for evaluating web data</title>
    <updated>2025-09-12T22:06:55+00:00</updated>
    <author>
      <name>/u/Busy_Satisfaction791</name>
      <uri>https://old.reddit.com/user/Busy_Satisfaction791</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Guys, I've been looking for some small models that can run on my MBA M1 16GB Ram with Browser use to play around with AI test automation. &lt;/p&gt; &lt;p&gt;So far, the ones that gives hope are Qwen2.5-Coder-3B it and Qwen2.5-Coder-7B it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Busy_Satisfaction791"&gt; /u/Busy_Satisfaction791 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfgehx/small_model_recommendation_for_evaluating_web_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfgehx/small_model_recommendation_for_evaluating_web_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nfgehx/small_model_recommendation_for_evaluating_web_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T22:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nezvor</id>
    <title>I made script to allow an Ollama server to be ran off of kaggle with a Ngrok domain.</title>
    <updated>2025-09-12T10:46:18+00:00</updated>
    <author>
      <name>/u/jam06452</name>
      <uri>https://old.reddit.com/user/jam06452</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I made a Kaggle script that sets up an Ollama server with GPU acceleration, this is amazing for 30 hours/week of Kaggle GPU time for free.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Installs CUDA + dependencies&lt;/li&gt; &lt;li&gt;Downloads the latest Ollama (since it doesn’t persist on Kaggle)&lt;/li&gt; &lt;li&gt;Serves the API with ngrok&lt;/li&gt; &lt;li&gt;Installs two models: &lt;code&gt;deepseek-r1:14b&lt;/code&gt; and &lt;code&gt;qwen3-coder:30b&lt;/code&gt; &lt;em&gt;(You can swap these out—just keep total size under ~30GB for 2×T4s)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Once it’s running, you can use the API from your terminal or even connect it to an Open-webui instance in the cloud, like myself.&lt;/p&gt; &lt;p&gt;It uses an ngrok tunnel since kaggle provides random IPv4s every time. It's easier to use with a static domain&lt;/p&gt; &lt;p&gt;GitHub link: &lt;a href="https://github.com/jam06452/Ollama-Server-on-Kaggle"&gt;https://github.com/jam06452/Ollama-Server-on-Kaggle&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback or ideas for other models to try!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jam06452"&gt; /u/jam06452 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nezvor/i_made_script_to_allow_an_ollama_server_to_be_ran/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nezvor/i_made_script_to_allow_an_ollama_server_to_be_ran/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nezvor/i_made_script_to_allow_an_ollama_server_to_be_ran/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T10:46:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfdj7j</id>
    <title>Is there a way to enable mfa on OpenWebUI?</title>
    <updated>2025-09-12T20:13:13+00:00</updated>
    <author>
      <name>/u/Dense-Land-5927</name>
      <uri>https://old.reddit.com/user/Dense-Land-5927</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I am in the process of seeing about implementing Ollama where I work. However, after messing around with Ollama and the OpenWebUI, I cannot for the life of me find where you can activate mfa easily. &lt;/p&gt; &lt;p&gt;I saw another post on another website where someone said &amp;quot;It's in the settings&amp;quot; but no matter where I go in OpenWebUI, I don't have a setting where it says &amp;quot;turn on MFA.&amp;quot;&lt;/p&gt; &lt;p&gt;Any help would be nice. Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense-Land-5927"&gt; /u/Dense-Land-5927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfdj7j/is_there_a_way_to_enable_mfa_on_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfdj7j/is_there_a_way_to_enable_mfa_on_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nfdj7j/is_there_a_way_to_enable_mfa_on_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T20:13:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1neyrkj</id>
    <title>Best LLM for my laptop</title>
    <updated>2025-09-12T09:38:53+00:00</updated>
    <author>
      <name>/u/Silly_Bad_7692</name>
      <uri>https://old.reddit.com/user/Silly_Bad_7692</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys! I've a thinkpad x1 carbon G9 (i7 1165G7, 32GB ram) and I was wondering what's the best LLM I can run on my pc. I'm new to local LLM and ollama so please be kind with me!&lt;/p&gt; &lt;p&gt;Also I would like to run it with a GUI. How can I do it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silly_Bad_7692"&gt; /u/Silly_Bad_7692 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neyrkj/best_llm_for_my_laptop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neyrkj/best_llm_for_my_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1neyrkj/best_llm_for_my_laptop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T09:38:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfbba3</id>
    <title>Ollama integration!!</title>
    <updated>2025-09-12T18:45:49+00:00</updated>
    <author>
      <name>/u/Direct_Effort_4892</name>
      <uri>https://old.reddit.com/user/Direct_Effort_4892</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nfbba3/ollama_integration/"&gt; &lt;img alt="Ollama integration!!" src="https://external-preview.redd.it/6YKpG1RrqJFlYrGZQcvKrQrg8zYisa5ZdVMLcKEnEJg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ce98aaadd3050590046aac15357e304aa66ed43" title="Ollama integration!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Direct_Effort_4892"&gt; /u/Direct_Effort_4892 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Animesh-Varma/Mythryl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfbba3/ollama_integration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nfbba3/ollama_integration/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T18:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1neupyc</id>
    <title>Gpt oss 20b ft 3090 in proxmox</title>
    <updated>2025-09-12T05:17:40+00:00</updated>
    <author>
      <name>/u/LeftelfinX</name>
      <uri>https://old.reddit.com/user/LeftelfinX</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1neupyc/gpt_oss_20b_ft_3090_in_proxmox/"&gt; &lt;img alt="Gpt oss 20b ft 3090 in proxmox" src="https://external-preview.redd.it/YW5vNDZsYmM0b29mMSNQ3BfJaOBXHJDEKG-O492PgD5xrAutyBU3r3wb6pnX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b212bd16cc2038bb8d505e8035b4b599832749b0" title="Gpt oss 20b ft 3090 in proxmox" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just installed a 3090 which I got for 450$ into my proxmox server and viola that's another tier of perfomance unlocked. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeftelfinX"&gt; /u/LeftelfinX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dx9re49c4oof1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neupyc/gpt_oss_20b_ft_3090_in_proxmox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1neupyc/gpt_oss_20b_ft_3090_in_proxmox/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T05:17:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng02h7</id>
    <title>So many models...confused how to pick the right one. Need one to help fix English grammar and text.</title>
    <updated>2025-09-13T15:16:25+00:00</updated>
    <author>
      <name>/u/MoChuang</name>
      <uri>https://old.reddit.com/user/MoChuang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am working on a project that needs a step to fix some closed captioning text to make it more coherent. Example input and output text below. I have a laptop with an RTX 3050 4GB so the models I can run are pretty limited but I think it is still sufficient for what I need. I've tried qwen2.5:1.5b-instruct-q4_K_M and qwen2.5:3b-instruct-q4_K_M mostly so far. I am going to start testing some phi, gemma, and llama models as well. But there are so many versions, sizes, and quantizations its kind of overwhelming.&lt;/p&gt; &lt;p&gt;For example, Gemma3 is newer and better than Gemma2, but on my GPU I have to choose between Gemma3:1b and Gemma2:2b, and generally 2b is better than 1b...so in my case which option is actually better? I know ultimately I need to test things myself to see which I am more satisfied with, but is there some logical reasoning I can do to at least narrow down the possible options to a handful that should work better before embarking on all this testing?&lt;/p&gt; &lt;p&gt;Example input text:&lt;/p&gt; &lt;p&gt;|| || |All right, I'm goingAllAll right, I'm going to get started with a question for the three of our panelists who are older and You've all been in the field You've all You've all been in the field for a lifetime. Here's Here's my question, because there's a lot of younger people in this room. What Expected What are the things that you thought? Expect|&lt;/p&gt; &lt;p&gt;Prompt used for qwen2.5:3b-instruct-q4_K_M:&lt;/p&gt; &lt;p&gt;|| || |Remove repeated words and phrases from the following sentences. Make the sentences grammatically correct, but do not add, remove, or change the meaning of the text: {text}|&lt;/p&gt; &lt;p&gt;Corrected output:&lt;/p&gt; &lt;p&gt;|| || |All right, I'm going to get started with a question for the three of our older panelists. You've all been in the field for a lifetime. Here's my question, because there are a lot of younger people in this room. What are the things that you expected and believed?|&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoChuang"&gt; /u/MoChuang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ng02h7/so_many_modelsconfused_how_to_pick_the_right_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ng02h7/so_many_modelsconfused_how_to_pick_the_right_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ng02h7/so_many_modelsconfused_how_to_pick_the_right_one/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-13T15:16:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ngrits</id>
    <title>Comment utiliser le GPU ?</title>
    <updated>2025-09-14T13:37:17+00:00</updated>
    <author>
      <name>/u/Maleficent-Hotel8207</name>
      <uri>https://old.reddit.com/user/Maleficent-Hotel8207</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ngrits/comment_utiliser_le_gpu/"&gt; &lt;img alt="Comment utiliser le GPU ?" src="https://preview.redd.it/xb4qwochv4pf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e16ed4fdbfab7be2cfc1832b6e804dfecad39491" title="Comment utiliser le GPU ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Comment utiliser le GPU sur ollama j’ai une GTX 1050 et je n’arrive pas à l’utiliser pour exécuter des modèles &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Hotel8207"&gt; /u/Maleficent-Hotel8207 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xb4qwochv4pf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ngrits/comment_utiliser_le_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ngrits/comment_utiliser_le_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-14T13:37:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfytlv</id>
    <title>Gemma 3 12B versus GPT 5 Nano</title>
    <updated>2025-09-13T14:25:54+00:00</updated>
    <author>
      <name>/u/fundal_alb</name>
      <uri>https://old.reddit.com/user/fundal_alb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is just me or that Gemma version is better or equal to GPT 5 Nano?&lt;/p&gt; &lt;p&gt;In my case...:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nano is responding with the first token after 6-10 seconds&lt;/li&gt; &lt;li&gt;Gemma has better language understanding than 5 Nano.&lt;/li&gt; &lt;li&gt;Gemma is structuring the output in a more readable way&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fundal_alb"&gt; /u/fundal_alb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfytlv/gemma_3_12b_versus_gpt_5_nano/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfytlv/gemma_3_12b_versus_gpt_5_nano/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nfytlv/gemma_3_12b_versus_gpt_5_nano/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-13T14:25:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng9tdq</id>
    <title>Ollama start all models on CPU instead GPU [Arch/Nvidia]</title>
    <updated>2025-09-13T21:52:04+00:00</updated>
    <author>
      <name>/u/MrDoc79</name>
      <uri>https://old.reddit.com/user/MrDoc79</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ng9tdq/ollama_start_all_models_on_cpu_instead_gpu/"&gt; &lt;img alt="Ollama start all models on CPU instead GPU [Arch/Nvidia]" src="https://b.thumbs.redditmedia.com/t2wX7z3e3ylFI8LXPaCJf9wbkIBJekACEeCKiiZ4G4Y.jpg" title="Ollama start all models on CPU instead GPU [Arch/Nvidia]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Idk why, but all models, what i started, are running on CPU, and, had small speed for generate answer. However, nvidia-smi works, and driver is available. I'm on EndeavourOS (Arch-based), with RTX 2060 on 6gb. All screenshots pinned&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrDoc79"&gt; /u/MrDoc79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ng9tdq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ng9tdq/ollama_start_all_models_on_cpu_instead_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ng9tdq/ollama_start_all_models_on_cpu_instead_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-13T21:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhhdac</id>
    <title>GPT-OSS-120B Performance Benchmarks and Provider Trade-Offs</title>
    <updated>2025-09-15T09:40:37+00:00</updated>
    <author>
      <name>/u/Sumanth_077</name>
      <uri>https://old.reddit.com/user/Sumanth_077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking at the latest Artificial Analysis benchmarks for GPT-OSS-120B and noticed some interesting differences between providers, especially for those using it in production.&lt;/p&gt; &lt;p&gt;Time to first token (TTFT) ranges from under 0.3 seconds to nearly a second depending on the provider. That can be significant for applications where responsiveness matters. Throughput also varies, from under 200 tokens per second to over 400.&lt;/p&gt; &lt;p&gt;Cost per million tokens adds another consideration. Some providers offer high throughput at a higher cost, while others like CompactifAI are cheaper but very slower. Clarifai, for example, delivers low TTFT, solid throughput, and relatively low cost.&lt;/p&gt; &lt;p&gt;The takeaway is that no single metric tells the full story. Latency affects responsiveness, throughput matters for larger tasks, and cost impacts scaling. The best provider depends on which of these factors is most important for your use case.&lt;/p&gt; &lt;p&gt;For those using GPT-OSS-120B in production, which of these do you find the hardest to manage: step latency, throughput, or cost?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/92e9yu2ctapf1.png?width=3408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ff0bd76c8ac202aad4c9e453dc975c7e67cfa6d7"&gt;https://preview.redd.it/92e9yu2ctapf1.png?width=3408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ff0bd76c8ac202aad4c9e453dc975c7e67cfa6d7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sumanth_077"&gt; /u/Sumanth_077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhhdac/gptoss120b_performance_benchmarks_and_provider/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhhdac/gptoss120b_performance_benchmarks_and_provider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nhhdac/gptoss120b_performance_benchmarks_and_provider/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-15T09:40:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhl7h0</id>
    <title>Advice on building an enterprise-scale, privacy-first conversational assistant (local LLMs with Ollama vs fine-tuning)</title>
    <updated>2025-09-15T13:01:05+00:00</updated>
    <author>
      <name>/u/jamalhassouni</name>
      <uri>https://old.reddit.com/user/jamalhassouni</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m working on a project to design a &lt;strong&gt;conversational AI assistant for employee well-being and productivity&lt;/strong&gt; inside a large enterprise (think thousands of staff, high compliance/security requirements). The assistant should provide personalized nudges, lightweight recommendations, and track anonymized engagement data — without sending sensitive data outside the organization.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key constraints:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Must be &lt;strong&gt;privacy-first&lt;/strong&gt; (local deployment or private cloud — no SaaS APIs).&lt;/li&gt; &lt;li&gt;Needs to support &lt;strong&gt;personalized recommendations&lt;/strong&gt; and &lt;strong&gt;ongoing employee state tracking&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Must handle &lt;strong&gt;enterprise scale&lt;/strong&gt; (hundreds–thousands of concurrent users).&lt;/li&gt; &lt;li&gt;Regulatory requirements: &lt;strong&gt;PII protection, anonymization, auditability&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I’d love advice on:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Local LLM deployment&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Is using &lt;strong&gt;Ollama with models like Gemma/MedGemma&lt;/strong&gt; a solid foundation for production at enterprise scale?&lt;/li&gt; &lt;li&gt;What are the pros/cons of Ollama vs more MLOps-oriented solutions (vLLM, TGI, LM Studio, custom Dockerized serving)?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model strategy: RAG vs fine-tuning&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;For delivering contextual, evolving guidance: would you start with &lt;strong&gt;RAG (vector DB + retrieval)&lt;/strong&gt; or jump straight into &lt;strong&gt;fine-tuning a domain model&lt;/strong&gt;?&lt;/li&gt; &lt;li&gt;Any rule of thumb on when fine-tuning becomes necessary in real-world enterprise use cases?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model choice&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Experiences with &lt;strong&gt;Gemma/MedGemma&lt;/strong&gt; or other open-source models for well-being / health-adjacent guidance?&lt;/li&gt; &lt;li&gt;Alternatives you’d recommend (Mistral, LLaMA 3, Phi-3, Qwen, etc.) in terms of reasoning, safety, and multilingual support?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Infrastructure &amp;amp; scaling&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Minimum GPU/CPU/RAM targets to support &lt;strong&gt;hundreds of concurrent chats&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Vector DB choices: FAISS, Milvus, Weaviate, Pinecone — what works best at enterprise scale?&lt;/li&gt; &lt;li&gt;Monitoring, evaluation, and safe deployment patterns (A/B testing, hallucination mitigation, guardrails).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Security &amp;amp; compliance&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Best practices to prevent &lt;strong&gt;PII leakage into embeddings/prompts&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Recommended architectures for &lt;strong&gt;GDPR/HIPAA-like compliance&lt;/strong&gt; when dealing with well-being data.&lt;/li&gt; &lt;li&gt;Any proven strategies to balance personalization with strict privacy requirements?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation &amp;amp; KPIs&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;How to measure assistant effectiveness (safety checks, employee satisfaction, retention impact).&lt;/li&gt; &lt;li&gt;Tooling for anonymized analytics dashboards at the org level.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jamalhassouni"&gt; /u/jamalhassouni &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhl7h0/advice_on_building_an_enterprisescale/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhl7h0/advice_on_building_an_enterprisescale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nhl7h0/advice_on_building_an_enterprisescale/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-15T13:01:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhkjk8</id>
    <title>What are the ways to use Ollama 120B without breaking the bank?</title>
    <updated>2025-09-15T12:32:02+00:00</updated>
    <author>
      <name>/u/Significant_Loss_541</name>
      <uri>https://old.reddit.com/user/Significant_Loss_541</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hello, i have been looking into running the ollama 120b model for a project, but honestly the hardware/hosting side looks kinda tough to setup for me. i really dont want to set up big servers or spend a lot initially just to try it out.&lt;/p&gt; &lt;p&gt;are there any ways people here are running it cheaper? like cloud setups, colab hacks, lighter quantized versions, or anything similar?&lt;/p&gt; &lt;p&gt;also curious if it even makes sense to skip self-hosting and just use a service that already runs it (saw deepinfra has it with an api, and it’s way less than openai prices but still not free). has anyone tried going that route vs rolling your own?&lt;/p&gt; &lt;p&gt;what’s the most practical way for someone who doesn’t want to melt their credit card on gpu rentals?&lt;/p&gt; &lt;p&gt;thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Loss_541"&gt; /u/Significant_Loss_541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhkjk8/what_are_the_ways_to_use_ollama_120b_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhkjk8/what_are_the_ways_to_use_ollama_120b_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nhkjk8/what_are_the_ways_to_use_ollama_120b_without/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-15T12:32:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1niendu</id>
    <title>how to make custom chatbot for my website</title>
    <updated>2025-09-16T11:19:31+00:00</updated>
    <author>
      <name>/u/Comfortable-Fan-8931</name>
      <uri>https://old.reddit.com/user/Comfortable-Fan-8931</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i am student ,&lt;br /&gt; how to make custom chatbot for my website . &lt;/p&gt; &lt;p&gt;when i ask question related to my website then, chatbot gives answer .&lt;br /&gt; And please suggest best approach and steps to create this chatbot&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Fan-8931"&gt; /u/Comfortable-Fan-8931 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1niendu/how_to_make_custom_chatbot_for_my_website/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1niendu/how_to_make_custom_chatbot_for_my_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1niendu/how_to_make_custom_chatbot_for_my_website/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T11:19:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhiixf</id>
    <title>Was working in RAG recently got to know how well Gemma3 4B performs</title>
    <updated>2025-09-15T10:49:31+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nhiixf/was_working_in_rag_recently_got_to_know_how_well/"&gt; &lt;img alt="Was working in RAG recently got to know how well Gemma3 4B performs" src="https://preview.redd.it/91u5300g6bpf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=fc7c57a6e1b897ac550462d08b77359bb65c2b95" title="Was working in RAG recently got to know how well Gemma3 4B performs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got this working and had to share because wtf, this tiny model is way better than expected.&lt;/p&gt; &lt;p&gt;Built a RAG system that renders docs as a knowledge graph you can actually navigate through. Using Gemma3 4B via Ollama and honestly shocked at how well it clusters related content.&lt;/p&gt; &lt;p&gt;The crazy part? Sub-200ms responses and the semantic relationships actually make sense. Running smooth on small GPU&lt;/p&gt; &lt;p&gt;Anyone else trying local models for RAG? Kinda nice not sending everything to OpenAI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/91u5300g6bpf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhiixf/was_working_in_rag_recently_got_to_know_how_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nhiixf/was_working_in_rag_recently_got_to_know_how_well/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-15T10:49:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni5cfx</id>
    <title>Fully local data analysis assistant (plus new Model)</title>
    <updated>2025-09-16T02:17:40+00:00</updated>
    <author>
      <name>/u/mshintaro777</name>
      <uri>https://old.reddit.com/user/mshintaro777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ni5cfx/fully_local_data_analysis_assistant_plus_new_model/"&gt; &lt;img alt="Fully local data analysis assistant (plus new Model)" src="https://preview.redd.it/ifula3tiqfpf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=e334cd9bdf8f0d006bb34767d9f02bebb39206c1" title="Fully local data analysis assistant (plus new Model)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mshintaro777"&gt; /u/mshintaro777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ifula3tiqfpf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ni5cfx/fully_local_data_analysis_assistant_plus_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ni5cfx/fully_local_data_analysis_assistant_plus_new_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T02:17:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nicqtd</id>
    <title>Need a simple UI/UX for chat (similar to OpenAI Chatgpt) using Ollama</title>
    <updated>2025-09-16T09:28:13+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Appreciate any advice. I ask chatgpt to create but not getting the right look. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nicqtd/need_a_simple_uiux_for_chat_similar_to_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nicqtd/need_a_simple_uiux_for_chat_similar_to_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nicqtd/need_a_simple_uiux_for_chat_similar_to_openai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T09:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nisb0j</id>
    <title>A PHP Proxy script to work with Ollama from HTTPS apps</title>
    <updated>2025-09-16T20:09:35+00:00</updated>
    <author>
      <name>/u/New_Cranberry_6451</name>
      <uri>https://old.reddit.com/user/New_Cranberry_6451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Ollama friends!&lt;/p&gt; &lt;p&gt;I have written a small PHP script that allows you to have a Proxy to work with your Ollama API from web apps under HTTPS. I probably reinvented a wheel here, but the thing is that I wasn't able to find a small, dependency free, PHP script that did this job for me. Others I tried couldn't handle streaming for example, or had too many things I don't need for my use case. That's why I ended up with this and as I wished to find something similar when I needed it, I am sharing it with you hoping someone finds it useful.&lt;/p&gt; &lt;p&gt;All feedback is welcome, let me know if there's another proxy option better than this solution (I am sure it will) or if you find any security concerns. This is not intended to work in production, it's just a straight-forward script that does the job. &lt;/p&gt; &lt;p&gt;Repo here: &lt;a href="https://github.com/aritzolaba/ow-proxy-ollama"&gt;OllamaProxy on Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope it helps someone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Cranberry_6451"&gt; /u/New_Cranberry_6451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nisb0j/a_php_proxy_script_to_work_with_ollama_from_https/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nisb0j/a_php_proxy_script_to_work_with_ollama_from_https/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nisb0j/a_php_proxy_script_to_work_with_ollama_from_https/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T20:09:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nisqoc</id>
    <title>[Release] Doc Builder (MD + PDF) v1.7 for Open WebUI Store – clean Markdown + styled PDF exports</title>
    <updated>2025-09-16T20:25:54+00:00</updated>
    <author>
      <name>/u/Nefhis</name>
      <uri>https://old.reddit.com/user/Nefhis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nisqoc/release_doc_builder_md_pdf_v17_for_open_webui/"&gt; &lt;img alt="[Release] Doc Builder (MD + PDF) v1.7 for Open WebUI Store – clean Markdown + styled PDF exports" src="https://b.thumbs.redditmedia.com/LmvS5C2MZB0TEDMGq5XmzAqMtL2As73VlizrAZO8S2Q.jpg" title="[Release] Doc Builder (MD + PDF) v1.7 for Open WebUI Store – clean Markdown + styled PDF exports" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nefhis"&gt; /u/Nefhis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenWebUI/comments/1nismqj/release_doc_builder_md_pdf_v17_for_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nisqoc/release_doc_builder_md_pdf_v17_for_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nisqoc/release_doc_builder_md_pdf_v17_for_open_webui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T20:25:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nizasb</id>
    <title>computron_9000</title>
    <updated>2025-09-17T00:59:45+00:00</updated>
    <author>
      <name>/u/larz01larz</name>
      <uri>https://old.reddit.com/user/larz01larz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nizasb/computron_9000/"&gt; &lt;img alt="computron_9000" src="https://external-preview.redd.it/rx0LgLIbfwDLaMi2xA7oRUXabQDybdqzNQj1X9cd914.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b126cb1c8b3bcdc563ad80a3688099487c5fad9f" title="computron_9000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Still working on &lt;a href="https://github.com/lefoulkrod/computron_9000"&gt;computron&lt;/a&gt;. It's not really just a chat UI on top of ollama, althought it does do that. It is more like my own personal AI assistant. I've been adding a bunch of tools and agents to it so it can do web research, write and run code, execute shell commands. It's kind of big heap of agents and tools but I'm slowly stitching it together into something useful. Take a look and if interested in contributing feel free to submit a PR.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mih4jhyzimpf1.png?width=3744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc36c55b82bae69f66cad127da1441c7bcb8bbd1"&gt;https://preview.redd.it/mih4jhyzimpf1.png?width=3744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc36c55b82bae69f66cad127da1441c7bcb8bbd1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/larz01larz"&gt; /u/larz01larz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nizasb/computron_9000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nizasb/computron_9000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nizasb/computron_9000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T00:59:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1niknpf</id>
    <title>Fix AI pipeline bugs before they hit your local stack: a semantic firewall + grandma clinic (beginner friendly, MIT)</title>
    <updated>2025-09-16T15:28:20+00:00</updated>
    <author>
      <name>/u/onestardao</name>
      <uri>https://old.reddit.com/user/onestardao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1niknpf/fix_ai_pipeline_bugs_before_they_hit_your_local/"&gt; &lt;img alt="Fix AI pipeline bugs before they hit your local stack: a semantic firewall + grandma clinic (beginner friendly, MIT)" src="https://external-preview.redd.it/HmNdz-XJDpdQ-3JLiqDAFOCdjg6rPrU9CAiH44bGSXo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6cd3a31987288764c4bae70846352bac786ad4a5" title="Fix AI pipeline bugs before they hit your local stack: a semantic firewall + grandma clinic (beginner friendly, MIT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;last time i shared the 16-problem checklist for AI failures. many here are pros running ollama with custom RAG, agents, or tool flows. today is the beginner-friendly version. same math and guardrails, but explained like you’re showing a junior teammate. the idea is simple: install a tiny “semantic firewall” that runs before output, so unstable answers never reach your pipeline.&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;why this matters&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;most stacks fix things after generation. model talks, you add a reranker, a regex, a few if-elses. the same bug returns in a new shape.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;a semantic firewall flips the order. it inspects meaning first. if the state is unstable it loops, narrows, or resets. only a stable state is allowed to speak. once a failure mode is mapped, you fix it once and it stays fixed.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;what “before vs after” feels like&lt;/p&gt; &lt;ul&gt; &lt;li&gt;after: firefighting, patch debt, fragile flows.&lt;/li&gt; &lt;li&gt;before: a gate that checks drift against the question, demands a source card, and blocks ungrounded text. fewer retries. fewer wrong triggers. cleaner audits.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;copy-paste “grandma gate” into your ollama prompt or system section put this at the top of your system prompt or prepend to each user question. it’s provider-agnostic and text-only.&lt;/p&gt; &lt;p&gt;``` grandma gate (pre-output):&lt;/p&gt; &lt;p&gt;1) show a source card before any answer: - doc or dataset name (id ok) - exact location (page or lines, or section id) - one sentence why this matches the question&lt;/p&gt; &lt;p&gt;2) mid-chain checkpoint: - if reasoning drifts, reset once and try a narrower route&lt;/p&gt; &lt;p&gt;3) only continue when both hold: - meaning matches clearly (small drift) - coverage is high (most of the answer is supported by the citation)&lt;/p&gt; &lt;p&gt;4) if either fails: - do not answer - ask me to pick a file, a section, or to narrow the question ```&lt;/p&gt; &lt;p&gt;ollama quick-start: 3 ways&lt;/p&gt; &lt;p&gt;way 1: Modelfile system policy&lt;/p&gt; &lt;p&gt;``` FROM llama3 SYSTEM &amp;quot;&amp;quot;&amp;quot; you are behind a semantic firewall. &amp;lt;paste the grandma gate here&amp;gt; when answering, first print:&lt;/p&gt; &lt;p&gt;source: doc: &amp;lt;name or id&amp;gt; location: &amp;lt;page/lines/section&amp;gt; why this matches: &amp;lt;one sentence&amp;gt;&lt;/p&gt; &lt;p&gt;answer: &amp;lt;keep it inside the cited scope.&amp;gt; &amp;quot;&amp;quot;&amp;quot; PARAMETER temperature 0.3 ```&lt;/p&gt; &lt;p&gt;then:&lt;/p&gt; &lt;p&gt;&lt;code&gt; ollama create safe-llama -f Modelfile ollama run safe-llama &lt;/code&gt;&lt;/p&gt; &lt;p&gt;way 2: one-off CLI with a prelude&lt;/p&gt; &lt;p&gt;&lt;code&gt; PRELUDE=&amp;quot;&amp;lt;&amp;lt;grandma gate text here&amp;gt;&amp;gt;&amp;quot; QUESTION=&amp;quot;summarize section 2 of our faq about refunds&amp;quot; echo -e &amp;quot;$PRELUDE\n\n$QUESTION&amp;quot; | ollama run llama3 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;way 3: local HTTP call&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash curl http://localhost:11434/api/generate \ -d '{ &amp;quot;model&amp;quot;:&amp;quot;llama3&amp;quot;, &amp;quot;prompt&amp;quot;:&amp;quot;'&amp;quot;$(printf &amp;quot;%s\n\n%s&amp;quot; &amp;quot;$PRELUDE&amp;quot; &amp;quot;extract the steps from policy v3, section refunds&amp;quot;)&amp;quot;'&amp;quot;, &amp;quot;options&amp;quot;:{&amp;quot;temperature&amp;quot;:0.3} }' &lt;/code&gt;&lt;/p&gt; &lt;p&gt;rag and embeddings: 3 sanity checks for ollama users&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;dimensions and normalization: do not mix 384-dim and 768-dim vectors. if you swap embed models, rebuild the store. normalize vectors consistently.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;chunk→embed contract: keep code, tables, and headers as blocks. do not flatten to prose. store chunk ids and line ranges so your source card can point back.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;citation first: require the card to print before prose. if you only see text, block the automation step and ask the user to pick a section. —&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;fast “before” recipes that work well with ollama&lt;/p&gt; &lt;p&gt;recipe a: card-first filter for shell pipelines&lt;/p&gt; &lt;ul&gt; &lt;li&gt;many people pipe ollama into jq, awk, or a webhook. add a tiny gate.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt; ollama run safe-llama -p &amp;quot;$INPUT&amp;quot; | awk ' BEGIN{card=0} /^source:/ {card=1} END{ if(card==0) { exit 42 } } ' || { echo &amp;quot;blocked: missing source card&amp;quot;; exit 1; } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;recipe b: warm the model to avoid first-call collapse&lt;/p&gt; &lt;ul&gt; &lt;li&gt;first request after load often looks confident but wrong. warm it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;``` ollama run llama3 &amp;quot;ready check. say ok.&amp;quot; &amp;gt;/dev/null&lt;/p&gt; &lt;h1&gt;or keep the model warm for 5 minutes&lt;/h1&gt; &lt;p&gt;ollama run --keep-alive 5m llama3 &amp;quot;ready check&amp;quot; &amp;gt;/dev/null ```&lt;/p&gt; &lt;p&gt;recipe c: small canary before production action&lt;/p&gt; &lt;ul&gt; &lt;li&gt;before the agent writes to disk or calls a tool, force a tiny canary question and verify the card prints a real section. if not, stop the run.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;common pipeline failures this firewall prevents&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;hallucination and chunk drift: pretty cosine neighbor, wrong meaning. the gate demands the card and rejects the output if the card is off.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;interpretation collapse: the chunk is correct, the reading is wrong. mid-chain checkpoint catches drift and resets once.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;debugging black box: answers with no trace. the card glues answer to a real location, so you can redo and audit.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;bootstrap ordering: calling tools or indexes before they are warm. run a warmup, then allow speech.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;pre-deploy collapse: empty vector store or wrong env vars on first call. verify store size and secrets before the agent speaks.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;acceptance targets, so you know it is working&lt;/p&gt; &lt;ul&gt; &lt;li&gt;drift small. the cited text clearly belongs to the question.&lt;/li&gt; &lt;li&gt;coverage high. most of the answer is inside the cited scope.&lt;/li&gt; &lt;li&gt;card first. proof appears before prose.&lt;/li&gt; &lt;li&gt;hold across two paraphrases. if it swings, keep the gate closed and ask the user to pick a file or narrow scope.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;mini before/after demo you can try now&lt;/p&gt; &lt;ol&gt; &lt;li&gt;ask normally: “what are the refund steps” against your policy doc. watch it improvise or hedge.&lt;/li&gt; &lt;li&gt;ask with the gate + “card first.” you should see a doc id, section, and a one-sentence why. if the citation is wrong, the model must refuse and ask for a narrower query or a file pick. result: fewer wrong runs get past your terminal, scripts, or webhooks.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;faq&lt;/p&gt; &lt;p&gt;q: do i need a library or sdk a: no. it is a text policy plus tiny filters. works in ollama, claude, openrouter, and inside automations.&lt;/p&gt; &lt;p&gt;q: will this slow me down a: it usually speeds you up. you skip broken runs early instead of repairing them downstream.&lt;/p&gt; &lt;p&gt;q: can i keep creative formatting a: yes. ground the factual part first with a real card, then allow formatting. for freeform tasks, ask for a small example before the full answer.&lt;/p&gt; &lt;p&gt;q: what if the model keeps saying “unstable” a: your question is too broad or your store lacks the right chunk. pick a file and section, or ingest the missing page. once the card matches, the flow unlocks.&lt;/p&gt; &lt;p&gt;q: where is the plain language guide a: “Grandma Clinic” explains the 16 common failure modes with tiny fixes. beginner friendly.&lt;/p&gt; &lt;p&gt;closing if mods limit links, reply “drop one-file” and i’ll paste a single text you can save as a Modelfile or prelude. if you post a screenshot of a failure, i can map which failure number it is and give the smallest patch that fits an ollama stack.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onestardao"&gt; /u/onestardao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/onestardao/WFGY/blob/main/ProblemMap/GrandmaClinic/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1niknpf/fix_ai_pipeline_bugs_before_they_hit_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1niknpf/fix_ai_pipeline_bugs_before_they_hit_your_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T15:28:20+00:00</published>
  </entry>
</feed>
