<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-16T21:43:12+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1r51oqk</id>
    <title>What is the best model to use?</title>
    <updated>2026-02-15T01:22:26+00:00</updated>
    <author>
      <name>/u/Massive-Farm-3410</name>
      <uri>https://old.reddit.com/user/Massive-Farm-3410</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I want to ask which one of out these models you reccomend to use:&lt;br /&gt; deepseek-v2:latest 7c8c332f2df7 8.9 GB About an hour ago&lt;/p&gt; &lt;p&gt;phi3.5:latest 61819fb370a3 2.2 GB About an hour ago&lt;/p&gt; &lt;p&gt;deepseek-r1:7b 755ced02ce7b 4.7 GB About an hour ago&lt;/p&gt; &lt;p&gt;rnj-1:latest d20e29ab8d0f 5.1 GB 2 hours ago&lt;/p&gt; &lt;p&gt;llama3.1:latest 46e0c10c039e 4.9 GB 2 hours ago&lt;/p&gt; &lt;p&gt;qwen3:14b bdbd181c33f2 9.3 GB 2 hours ago&lt;/p&gt; &lt;p&gt;pdevine/gpt-oss:20b-q8_0 b8b78fe2245f 12 GB 3 hours ago&lt;/p&gt; &lt;p&gt;jgmolinawork/gpt5.2-lite:latest 8cbd283299dd 2.0 GB 3 hours ago&lt;/p&gt; &lt;p&gt;llama3.1:8b 46e0c10c039e 4.9 GB 3 hours ago&lt;/p&gt; &lt;p&gt;tinyllama:1.1b 2644915ede35 637 MB 3 hours ago&lt;/p&gt; &lt;p&gt;qwen3-vl:30b eda0be100877 19 GB 12 hours ago&lt;/p&gt; &lt;p&gt;qwen3:4b 359d7dd4bcda 2.5 GB 13 hours ago&lt;/p&gt; &lt;p&gt;qwen2.5:14b 7cdf5a0187d5 9.0 GB 13 hours ago&lt;/p&gt; &lt;p&gt;llama3.2:latest a80c4f17acd5 2.0 GB 13 hours ago&lt;/p&gt; &lt;p&gt;qwen2.5-coder:7b dae161e27b0e 4.7 GB 13 hours ago&lt;/p&gt; &lt;p&gt;qwen3:8b 500a1f067a9f 5.2 GB 14 hours ago&lt;/p&gt; &lt;p&gt;llama2:13b d475bf4c50bc 7.4 GB 20 hours ago&lt;/p&gt; &lt;p&gt;llama2:7b 78e26419b446 3.8 GB 21 hours ago&lt;/p&gt; &lt;p&gt;smarterai:latest 65086a4a68ab 13 GB 12 days ago&lt;/p&gt; &lt;p&gt;qwen2.5max:latest ec4ed45cc7ba 4.7 GB 12 days ago&lt;/p&gt; &lt;p&gt;qwen2.5:latest 845dbda0ea48 4.7 GB 12 days ago&lt;/p&gt; &lt;p&gt;smartai:latest 2ce63b7f7652 17 GB 12 days ago&lt;/p&gt; &lt;p&gt;gemma3:27b a418f5838eaf 17 GB 12 days ago&lt;/p&gt; &lt;p&gt;qwen2.5:7b 845dbda0ea48 4.7 GB 13 days ago&lt;/p&gt; &lt;p&gt;gemma3:4b a2af6cc3eb7f 3.3 GB 13 days ago&lt;/p&gt; &lt;p&gt;mistral:7b-instruct 6577803aa9a0 4.4 GB 13 days ago&lt;/p&gt; &lt;p&gt;qwen3:1.7b 8f68893c685c 1.4 GB 13 days ago&lt;/p&gt; &lt;p&gt;gpt-oss:20b 17052f91a42e 13 GB 13 days ago&lt;/p&gt; &lt;p&gt;llama3:latest 365c0bd3c000 4.7 GB 2 weeks ago&lt;/p&gt; &lt;p&gt;mistral:latest 6577803aa9a0 4.4 GB 2 weeks ago&lt;/p&gt; &lt;p&gt;Thanks and if you can please reccomend me some other models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Massive-Farm-3410"&gt; /u/Massive-Farm-3410 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r51oqk/what_is_the_best_model_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r51oqk/what_is_the_best_model_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r51oqk/what_is_the_best_model_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T01:22:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3liuv</id>
    <title>llm-checker 3.1.0 scans your hardware and tells you which Ollama models to run</title>
    <updated>2026-02-13T10:14:27+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r3liuv/llmchecker_310_scans_your_hardware_and_tells_you/"&gt; &lt;img alt="llm-checker 3.1.0 scans your hardware and tells you which Ollama models to run" src="https://external-preview.redd.it/dnIweHlkNXZrOGpnMcLi5bhvKZm7PKP0JDDl5p3lMW6riYPZaH99PltHi9FJ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f2955d317bb9c6ba05d20abed6b66bab2382bfb" title="llm-checker 3.1.0 scans your hardware and tells you which Ollama models to run" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt; &lt;em&gt;built a CLI tool that checks your system specs and recommends the best models to run on Ollama based on what your machine&lt;/em&gt; &lt;/p&gt; &lt;p&gt; &lt;em&gt;can actually handle. scores each model on quality, speed, fit, and context window so you're not just guessing or&lt;/em&gt; &lt;/p&gt; &lt;p&gt; &lt;em&gt;downloading&lt;/em&gt; &lt;em&gt;random&lt;/em&gt; &lt;em&gt;stuff&lt;/em&gt; &lt;em&gt;to&lt;/em&gt; &lt;em&gt;see&lt;/em&gt; &lt;em&gt;if&lt;/em&gt; &lt;em&gt;it&lt;/em&gt; &lt;em&gt;runs.&lt;/em&gt;&lt;/p&gt; &lt;p&gt; &lt;em&gt;just&lt;/em&gt; &lt;em&gt;pushed&lt;/em&gt; &lt;em&gt;3.1.0&lt;/em&gt; &lt;em&gt;recalibrated&lt;/em&gt; &lt;em&gt;the&lt;/em&gt; &lt;em&gt;whole&lt;/em&gt; &lt;em&gt;scoring&lt;/em&gt; &lt;em&gt;engine,&lt;/em&gt; &lt;em&gt;35+&lt;/em&gt; &lt;em&gt;curated&lt;/em&gt; &lt;em&gt;models&lt;/em&gt; &lt;em&gt;from&lt;/em&gt; &lt;em&gt;1B&lt;/em&gt; &lt;em&gt;to&lt;/em&gt; &lt;em&gt;32B,&lt;/em&gt; &lt;em&gt;memory&lt;/em&gt; &lt;em&gt;estimates&lt;/em&gt; &lt;em&gt;are&lt;/em&gt; &lt;em&gt;now&lt;/em&gt;&lt;/p&gt; &lt;p&gt; &lt;em&gt;aligned&lt;/em&gt; &lt;em&gt;with&lt;/em&gt; &lt;em&gt;actual&lt;/em&gt; &lt;em&gt;Ollama&lt;/em&gt; &lt;em&gt;sizes.&lt;/em&gt; &lt;em&gt;recommendations&lt;/em&gt; &lt;em&gt;are&lt;/em&gt; &lt;em&gt;way&lt;/em&gt; &lt;em&gt;more&lt;/em&gt; &lt;em&gt;accurate&lt;/em&gt; &lt;em&gt;than&lt;/em&gt; &lt;em&gt;before.&lt;/em&gt;&lt;/p&gt; &lt;p&gt; &lt;em&gt;supports&lt;/em&gt; &lt;em&gt;Apple&lt;/em&gt; &lt;em&gt;Silicon,&lt;/em&gt; &lt;em&gt;NVIDIA,&lt;/em&gt; &lt;em&gt;AMD,&lt;/em&gt; &lt;em&gt;Intel&lt;/em&gt; &lt;em&gt;Arc,&lt;/em&gt; &lt;em&gt;and&lt;/em&gt; &lt;em&gt;CPU-only&lt;/em&gt; &lt;em&gt;setups.&lt;/em&gt;&lt;/p&gt; &lt;p&gt; &lt;em&gt;if&lt;/em&gt; &lt;em&gt;you&lt;/em&gt; &lt;em&gt;tried&lt;/em&gt; &lt;em&gt;an&lt;/em&gt; &lt;em&gt;earlier&lt;/em&gt; &lt;em&gt;version&lt;/em&gt; &lt;em&gt;and&lt;/em&gt; &lt;em&gt;the&lt;/em&gt; &lt;em&gt;suggestions&lt;/em&gt; &lt;em&gt;were&lt;/em&gt; &lt;em&gt;off,&lt;/em&gt; &lt;em&gt;give&lt;/em&gt; &lt;em&gt;it&lt;/em&gt; &lt;em&gt;another&lt;/em&gt; &lt;em&gt;shot.&lt;/em&gt;&lt;/p&gt; &lt;p&gt; &lt;em&gt;npm&lt;/em&gt; &lt;em&gt;install&lt;/em&gt; &lt;em&gt;-g&lt;/em&gt; &lt;em&gt;llm-checker&lt;/em&gt;&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/Pavelevich/llm-checker"&gt;&lt;em&gt;https://github.com/Pavelevich/llm-checker&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h3lqoe5vk8jg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r3liuv/llmchecker_310_scans_your_hardware_and_tells_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r3liuv/llmchecker_310_scans_your_hardware_and_tells_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-13T10:14:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4bj70</id>
    <title>I built a self-hosted AI platform with multi-model orchestration on Ollama ‚Äî models debate each other before answering you</title>
    <updated>2026-02-14T04:50:58+00:00</updated>
    <author>
      <name>/u/Zealousideal-Tap1302</name>
      <uri>https://old.reddit.com/user/Zealousideal-Tap1302</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r4bj70/i_built_a_selfhosted_ai_platform_with_multimodel/"&gt; &lt;img alt="I built a self-hosted AI platform with multi-model orchestration on Ollama ‚Äî models debate each other before answering you" src="https://preview.redd.it/lhoh0th48ejg1.png?width=140&amp;amp;height=132&amp;amp;auto=webp&amp;amp;s=cef3f84fd12a12b7aabbafc447edbcd2a26e5cef" title="I built a self-hosted AI platform with multi-model orchestration on Ollama ‚Äî models debate each other before answering you" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been running Ollama locally for a while and kept hitting the same problem: one model is great at code, another at reasoning, another at creative writing. Switching between them manually got old fast.&lt;/p&gt; &lt;p&gt;So I built a platform where you pick a &amp;quot;mode&amp;quot; and it routes your question to the right model automatically. Code question ‚Üí coding model. Math ‚Üí reasoning model. Or you can let it decide (&lt;code&gt;auto&lt;/code&gt; mode).&lt;/p&gt; &lt;p&gt;The part I'm most excited about: &lt;strong&gt;Discussion Mode&lt;/strong&gt; ‚Äî multiple models actually debate each other on your question, cross-check facts, and synthesize a final answer. It's like having a panel of experts argue before giving you a response.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Screenshot:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lhoh0th48ejg1.png?width=2128&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b695ae796ce9b24d53f2a997e568e2c8250e4a2"&gt;https://preview.redd.it/lhoh0th48ejg1.png?width=2128&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b695ae796ce9b24d53f2a997e568e2c8250e4a2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smart auto-routing across multiple Ollama models&lt;/li&gt; &lt;li&gt;AI-to-AI discussion (models cross-check each other)&lt;/li&gt; &lt;li&gt;Deep research agent (breaks down topics, searches web, writes reports)&lt;/li&gt; &lt;li&gt;MCP tool integration&lt;/li&gt; &lt;li&gt;RAG with document upload (PDF, code)&lt;/li&gt; &lt;li&gt;Long-term memory per user&lt;/li&gt; &lt;li&gt;Multi-user auth (JWT + OAuth)&lt;/li&gt; &lt;li&gt;Cluster support for multiple Ollama nodes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Tech stack: Node.js/Express, vanilla JS frontend (no React ‚Äî intentional), PostgreSQL, Ollama API. MIT licensed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;You can try it right now:&lt;/strong&gt; &lt;a href="http://rasplay.tplinkdns.com:52416"&gt;http://rasplay.tplinkdns.com:52416&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://github.com/openmake/openmake_llm"&gt;https://github.com/openmake/openmake_llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This started as a side project from a small open-source community in Korea that's been building hardware/software projects since 2013. Two of us are actively working on it, but there's way more to do than two people can handle ‚Äî especially frontend polish, RAG pipeline tuning, and i18n.&lt;/p&gt; &lt;p&gt;If anyone's interested in contributing or just wants to poke around, the repo is public and MIT licensed. Happy to answer any questions about the architecture or design choices.&lt;/p&gt; &lt;p&gt;What model combinations are you all running for different tasks? Curious how others handle the multi-model problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Tap1302"&gt; /u/Zealousideal-Tap1302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r4bj70/i_built_a_selfhosted_ai_platform_with_multimodel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r4bj70/i_built_a_selfhosted_ai_platform_with_multimodel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r4bj70/i_built_a_selfhosted_ai_platform_with_multimodel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-14T04:50:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5c07b</id>
    <title>Ultra 165h - model recommendation</title>
    <updated>2026-02-15T11:05:49+00:00</updated>
    <author>
      <name>/u/Peter-the-Hermit</name>
      <uri>https://old.reddit.com/user/Peter-the-Hermit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I have Intel core ultra 7 165h with 96 GB RAM. I am looking for local model that will be able to help me with terminal usage - I wanted to use it with Wave terminal. What model should I try for comfortable use? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peter-the-Hermit"&gt; /u/Peter-the-Hermit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5c07b/ultra_165h_model_recommendation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5c07b/ultra_165h_model_recommendation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5c07b/ultra_165h_model_recommendation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T11:05:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5k1r4</id>
    <title>Is ollama Open source?</title>
    <updated>2026-02-15T17:13:14+00:00</updated>
    <author>
      <name>/u/Daorooo</name>
      <uri>https://old.reddit.com/user/Daorooo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sry I cant find the answer and its important. Would be nice If anyone could provide a source. I can find Ollama on GitHub which assumes it is Open source but on the Ollama Website is a pricing model listed which assumes it is Not Open source.&lt;/p&gt; &lt;p&gt;Could anyone maybe Help me please?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daorooo"&gt; /u/Daorooo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5k1r4/is_ollama_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5k1r4/is_ollama_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5k1r4/is_ollama_open_source/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T17:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r56bwg</id>
    <title>Cloud Pro Usage Limits</title>
    <updated>2026-02-15T05:21:54+00:00</updated>
    <author>
      <name>/u/Spooknik</name>
      <uri>https://old.reddit.com/user/Spooknik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm looking at getting a Pro subscription after being really impressed with Kimi K2.5 in Claude Code. But they don‚Äôt list any usage limits, just that pro is for, ‚ÄúDay-to-day work‚ÄîRAG, document analysis, and coding tasks‚Äù. Well I fall into that category but is it 5 requests an hour or is it 50? &lt;/p&gt; &lt;p&gt;There‚Äôs lots of places offering subscriptions and it seems strange not to be more transparent. I like Ollama and have been using it for years and the cloud works really well but giving away $20 a month just for a ‚Äúvibe based‚Äù usage limit leaves me hesitant.&lt;/p&gt; &lt;p&gt;Anyone with Pro want to chime in? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spooknik"&gt; /u/Spooknik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r56bwg/cloud_pro_usage_limits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r56bwg/cloud_pro_usage_limits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r56bwg/cloud_pro_usage_limits/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T05:21:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5a58u</id>
    <title>Does a distributed Ollama network exist?</title>
    <updated>2026-02-15T09:10:16+00:00</updated>
    <author>
      <name>/u/Toontje</name>
      <uri>https://old.reddit.com/user/Toontje</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most of us have Ollama running locally. My instance, and I suspect many other instances as well, is mostly idle.&lt;/p&gt; &lt;p&gt;Is there a network where we can share Ollama resources? Like a distributed Ollama network? If this doesn‚Äôt exist yet, why not? Technically not viable? Other reasons other than that you won‚Äôt let anybody get their dirty fingers in your local LLM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Toontje"&gt; /u/Toontje &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5a58u/does_a_distributed_ollama_network_exist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5a58u/does_a_distributed_ollama_network_exist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5a58u/does_a_distributed_ollama_network_exist/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T09:10:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5dkjk</id>
    <title>llm-checker update added MCP server so Claude manages your Ollama from the terminal (hw detection, benchmarks, cleanup, the whole thing)</title>
    <updated>2026-02-15T12:35:06+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r5dkjk/llmchecker_update_added_mcp_server_so_claude/"&gt; &lt;img alt="llm-checker update added MCP server so Claude manages your Ollama from the terminal (hw detection, benchmarks, cleanup, the whole thing)" src="https://external-preview.redd.it/MWUxMWhnY29pbmpnMeOR4dWR-pplwDGWPpwXX8YHflDMyfvJsFJscQlA-YBi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80a0b5477cde51a97ebb519e79b0cf71c123bc68" title="llm-checker update added MCP server so Claude manages your Ollama from the terminal (hw detection, benchmarks, cleanup, the whole thing)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;some of you might remember llm-checker from my post&lt;/p&gt; &lt;p&gt;since then i've been grinding on new features and the biggest one: &lt;strong&gt;**llm-checker&lt;/strong&gt; &lt;strong&gt;now&lt;/strong&gt; &lt;strong&gt;works&lt;/strong&gt; &lt;strong&gt;as&lt;/strong&gt; &lt;strong&gt;an&lt;/strong&gt; &lt;strong&gt;MCP&lt;/strong&gt; &lt;strong&gt;server*\&lt;/strong&gt;*. meaning you hook it into Claude Code and claude can directly&lt;/p&gt; &lt;p&gt;talk to your ollama setup. no copy pasting specs, no manual checks you just ask and it does it.&lt;/p&gt; &lt;p&gt; &amp;quot;what's the best coding model for my hardware?&amp;quot; llm-checker scans your GPU, scores 35+ models, gives you the answer.&lt;/p&gt; &lt;p&gt;&amp;quot;benchmark deepseek-r1 vs qwen2.5-coder&amp;quot; runs standardized prompts, gives you real tok/s.&lt;/p&gt; &lt;p&gt; &amp;quot;clean up my ollama&amp;quot; finds redundant models eating your disk.&lt;/p&gt; &lt;p&gt; two lines to set it up:&lt;/p&gt; &lt;p&gt; npm install -g llm-checker&lt;/p&gt; &lt;p&gt; claude mcp add llm-checker -- llm-checker-mcp&lt;/p&gt; &lt;p&gt;claude gets 16 tools hw_detect, recommend, benchmark, compare_models, ollama_pull, ollama_run, ollama_remove, ollama_optimize, project_recommend (this one scans your&lt;/p&gt; &lt;p&gt;codebase and picks the best model for it), ollama_monitor, cleanup_models, and more.&lt;/p&gt; &lt;p&gt; &lt;strong&gt;new&lt;/strong&gt; &lt;strong&gt;stuff&lt;/strong&gt; &lt;strong&gt;since&lt;/strong&gt; &lt;strong&gt;the&lt;/strong&gt; &lt;strong&gt;ollama&lt;/strong&gt; &lt;strong&gt;post:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; - MCP server integration (the big one)&lt;/p&gt; &lt;p&gt; - ollama_optimize generates env vars tuned to your setup&lt;/p&gt; &lt;p&gt; - project_recommend codebase-aware model picking&lt;/p&gt; &lt;p&gt; - compare_models side by side with actual numbers&lt;/p&gt; &lt;p&gt; - ollama_monitor real-time system status + memory&lt;/p&gt; &lt;p&gt; - better memory calibration across quantizations&lt;/p&gt; &lt;p&gt;everything that was already there still works as a standalone CLI too. no lock-in to claude.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;one&lt;/strong&gt; &lt;strong&gt;thing&lt;/strong&gt; the ollama post showed me there's real demand for this kind of tooling. i want to keep pushing llm-checker further but i need to know what's breaking for people&lt;/p&gt; &lt;p&gt;and what features matter most. if you run into bugs or have ideas, &lt;strong&gt;drop&lt;/strong&gt; &lt;strong&gt;an&lt;/strong&gt; &lt;strong&gt;issue&lt;/strong&gt; &lt;strong&gt;on&lt;/strong&gt; &lt;strong&gt;github&lt;/strong&gt;: &lt;a href="https://github.com/Pavelevich/llm-checker/issues"&gt;https://github.com/Pavelevich/llm-checker/issues&lt;/a&gt;&lt;/p&gt; &lt;p&gt;not joking every issue helps me decide what to build next. multi-GPU support, more model families, whatever. tell me what you need.&lt;/p&gt; &lt;p&gt; repo: &lt;a href="https://github.com/Pavelevich/llm-checker"&gt;https://github.com/Pavelevich/llm-checker&lt;/a&gt;&lt;/p&gt; &lt;p&gt; npm: npm install -g llm-checker&lt;/p&gt; &lt;p&gt;PRs welcome ask me anything in the comments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/udkn0wboinjg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5dkjk/llmchecker_update_added_mcp_server_so_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5dkjk/llmchecker_update_added_mcp_server_so_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T12:35:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5eyo9</id>
    <title>Causal Ability Injectors - Deterministic Behavioural Override (During Runtime)</title>
    <updated>2026-02-15T13:43:51+00:00</updated>
    <author>
      <name>/u/frank_brsrk</name>
      <uri>https://old.reddit.com/user/frank_brsrk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r5eyo9/causal_ability_injectors_deterministic/"&gt; &lt;img alt="Causal Ability Injectors - Deterministic Behavioural Override (During Runtime)" src="https://preview.redd.it/ifj3miyywnjg1.png?width=140&amp;amp;height=93&amp;amp;auto=webp&amp;amp;s=0d8d938ad3fdc60831283dee7ba2c26bcace46cd" title="Causal Ability Injectors - Deterministic Behavioural Override (During Runtime)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been spending a lot of time lately trying to fix agent's drift or get lost in long loops. While most everyone just feeds them more text, I wanted to build the rules that actually command how they think. Today, I am open sourcing the Causal Ability Injectors. A way to switch the AI's mindset in real-time based on what's happening while in the flow.&lt;/p&gt; &lt;p&gt;[ Example:&lt;br /&gt; during a critical question the input goes through lightweight rag node that dynamically corresponds to the query style and that picks up the most confident way of thinking to enforce to the model and keeping it on track and prohibit model drifting]&lt;/p&gt; &lt;p&gt;[ integrate as retrieval step before agent, OR upsert in your existing doc db for opportunistical retrieval, OR &lt;strong&gt;best case&lt;/strong&gt; add in an isolated namespace and use as behavioral contstraint retrieval]&lt;/p&gt; &lt;p&gt;[Data is already graph-augmented and ready for upsertion]&lt;/p&gt; &lt;p&gt;You can find the registry here: &lt;a href="https://huggingface.co/datasets/frankbrsrk/causal-ability-injectors"&gt;https://huggingface.co/datasets/frankbrsrk/causal-ability-injectors&lt;/a&gt; And the source is here: &lt;a href="https://github.com/frankbrsrkagentarium/causal-ability-injectors-csv"&gt;https://github.com/frankbrsrkagentarium/causal-ability-injectors-csv&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;How it works:&lt;/h1&gt; &lt;p&gt;The registry contains specific mindsets, like reasoning for root causes or checking for logic errors. When the agent hits a bottleneck, it pulls the exact injector it needs. I added columns for things like graph instructions, so each row is a command the machine can actually execute. It's like programming a nervous system instead of just chatting with a bot.&lt;/p&gt; &lt;p&gt;This is the next link in the &lt;em&gt;Architecture of Why&lt;/em&gt;. Build it and you will feel how the information moves once you start using it. Please check it out; I am sure it‚Äôs going to help if you are building complex RAG systems.&lt;/p&gt; &lt;h1&gt;Agentarium | Causal Ability Injectors Walkthrough&lt;/h1&gt; &lt;h1&gt;1. What this is&lt;/h1&gt; &lt;p&gt;Think of this as a blueprint for instructions. It's structured in rows, so each row is the embedding text you want to match against specific situations. I added columns for logic commands that tell the system exactly how to modify the context.&lt;/p&gt; &lt;h1&gt;2. Logic clusters&lt;/h1&gt; &lt;p&gt;I grouped these into four domains. Some are for checking errors, some are for analyzing big systems, and others are for ethics or safety. For example, CA001 is for challenging causal claims and CA005 is for red-teaming a plan.&lt;/p&gt; &lt;h1&gt;3. How to trigger it&lt;/h1&gt; &lt;p&gt;You use the &lt;/p&gt; &lt;pre&gt;&lt;code&gt;trigger_condition &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If the agent is stuck or evaluating a plan, it knows exactly which ability to inject. This keeps the transformer's attention focused on the right constraint at the right time.&lt;/p&gt; &lt;h1&gt;4. Standalone design&lt;/h1&gt; &lt;p&gt;I encoded each row to have everything it needs. Each one has a full JSON payload, so you don't have to look up other files. It's meant to be portable and easy to drop into a vector DB namespace like &lt;/p&gt; &lt;pre&gt;&lt;code&gt;causal-abilities &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;5. Why it's valuable&lt;/h1&gt; &lt;p&gt;It's not just the knowledge; it's the procedures. Instead of a massive 4k-token prompt, you just pull exactly what the AI needs for that one step. It stops the agent from drifting and keeps the reasoning sharp.&lt;/p&gt; &lt;p&gt;It turns ai vibes, to adaptive thought , through retrieved hard-coded instruction set.&lt;/p&gt; &lt;p&gt;State A always pulls Rule B.&lt;br /&gt; Fixed hierarchy resolves every conflict.&lt;br /&gt; Commands the system instead of just adding text.&lt;/p&gt; &lt;p&gt;Repeatable, traceable reasoning that works every single time.&lt;/p&gt; &lt;p&gt;Take Dataset and Use It, Just Download It and Give It To Ur LLM for Analysis&lt;/p&gt; &lt;p&gt;I designed it for power users, and If u like it, give me some feedback report,&lt;/p&gt; &lt;p&gt;This is my work's broader vision, applying cognition when needed, through my personal attention on data driven ability.&lt;/p&gt; &lt;p&gt;frank_brsrk&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ifj3miyywnjg1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce2e010cd3106d92f8823ac7445df4b2cbf78641"&gt;dataset containing forced rules instructions , machine runnable .&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frank_brsrk"&gt; /u/frank_brsrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5eyo9/causal_ability_injectors_deterministic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5eyo9/causal_ability_injectors_deterministic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5eyo9/causal_ability_injectors_deterministic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T13:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5dae9</id>
    <title>Enrichissement corpus et entra√Ænement IA open source</title>
    <updated>2026-02-15T12:20:06+00:00</updated>
    <author>
      <name>/u/Master_Way4672</name>
      <uri>https://old.reddit.com/user/Master_Way4672</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bonjour √† tous, j‚Äôai le projet d'installer un mod√®le IA en Opensource (oLlama ou autre), et je voudrais l‚Äôenrichir et l‚Äôentra√Æner sur un m√©tier particulier.&lt;/p&gt; &lt;p&gt;quelqu‚Äôun pour me guider ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Master_Way4672"&gt; /u/Master_Way4672 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5dae9/enrichissement_corpus_et_entra√Ænement_ia_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5dae9/enrichissement_corpus_et_entra√Ænement_ia_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5dae9/enrichissement_corpus_et_entra√Ænement_ia_open/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T12:20:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5wysb</id>
    <title>Does anybody know how to get llama.cpp? Thanks!</title>
    <updated>2026-02-16T02:06:12+00:00</updated>
    <author>
      <name>/u/Massive-Farm-3410</name>
      <uri>https://old.reddit.com/user/Massive-Farm-3410</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Massive-Farm-3410"&gt; /u/Massive-Farm-3410 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5wysb/does_anybody_know_how_to_get_llamacpp_thanks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5wysb/does_anybody_know_how_to_get_llamacpp_thanks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5wysb/does_anybody_know_how_to_get_llamacpp_thanks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T02:06:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5u9at</id>
    <title>Openclaw on cloud models</title>
    <updated>2026-02-16T00:01:10+00:00</updated>
    <author>
      <name>/u/cfipilot715</name>
      <uri>https://old.reddit.com/user/cfipilot715</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running ollama on a separate server.&lt;/p&gt; &lt;p&gt;Created a openclaw bot and using the cloud models on the ollama server, but it‚Äôs so so slow. Is there a way to go direct to ollama cloud without having to hit my local ollama? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cfipilot715"&gt; /u/cfipilot715 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5u9at/openclaw_on_cloud_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5u9at/openclaw_on_cloud_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5u9at/openclaw_on_cloud_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T00:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5gkph</id>
    <title>Opencode Agent Swarms!</title>
    <updated>2026-02-15T14:55:17+00:00</updated>
    <author>
      <name>/u/Available-Craft-5795</name>
      <uri>https://old.reddit.com/user/Available-Craft-5795</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r5gkph/opencode_agent_swarms/"&gt; &lt;img alt="Opencode Agent Swarms!" src="https://preview.redd.it/j7ipb4qp9ojg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=e0dbc6737089eabf63dc85e0cf89855d7ac3d6d7" title="Opencode Agent Swarms!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/lanefiedler731-gif/OpencodeSwarms"&gt;https://github.com/lanefiedler731-gif/OpencodeSwarms&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I vibecoded this with opencode btw.&lt;/p&gt; &lt;p&gt;This fork emulates Kimi K2.5 Agent Swarms, any model, up to 100 agents at a time.&lt;br /&gt; You will have to build this yourself.&lt;br /&gt; (Press tab until you see &amp;quot;Swarm_manager&amp;quot; mode enabled)&lt;br /&gt; All of them run in parallel.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j7ipb4qp9ojg1.png?width=447&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0eddc72b57bee16dd9ea6f3e30947e9d77523c70"&gt;https://preview.redd.it/j7ipb4qp9ojg1.png?width=447&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0eddc72b57bee16dd9ea6f3e30947e9d77523c70&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available-Craft-5795"&gt; /u/Available-Craft-5795 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5gkph/opencode_agent_swarms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5gkph/opencode_agent_swarms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5gkph/opencode_agent_swarms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T14:55:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5bkwf</id>
    <title>Can I pull models from Huggingface?</title>
    <updated>2026-02-15T10:39:56+00:00</updated>
    <author>
      <name>/u/Keensworth</name>
      <uri>https://old.reddit.com/user/Keensworth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm new to Ollama and just finished installing it and integrating it with OpenWebUI. I'm wondering if it was possible to pull models from Huggingface via a API call or a plugin?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Keensworth"&gt; /u/Keensworth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5bkwf/can_i_pull_models_from_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5bkwf/can_i_pull_models_from_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5bkwf/can_i_pull_models_from_huggingface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T10:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5xruc</id>
    <title>Has anyone gotten AMD Radeon 780M to work with Ollama?</title>
    <updated>2026-02-16T02:44:36+00:00</updated>
    <author>
      <name>/u/Famous-Weight2271</name>
      <uri>https://old.reddit.com/user/Famous-Weight2271</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alas, I'm giving up at this point, concluding that it is not possible for me to run Ollama on the GPU with my 780M. The main go to reference for this is &lt;a href="https://github.com/likelovewant/ollama-for-amd"&gt;https://github.com/likelovewant/ollama-for-amd&lt;/a&gt;, which may have worked in the past, or may just not like my specific machine. The instructions I followed were for HIP SDK 6.4.2 and gfx1103 / &lt;a href="https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU"&gt;&lt;strong&gt;ROCmLibs-for-gfx1103-AMD780M-APU&lt;/strong&gt;&lt;/a&gt; matching the HIP SDK (&lt;a href="https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU/releases/download/v0.6.4.2/rocm.gfx1103.for.hip.6.4.2.7z"&gt;rocm.gfx1103.for.hip.6.4.2.7z&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I also tried &lt;a href="https://github.com/ByronLeeeee/Ollama-For-AMD-Installer"&gt;https://github.com/ByronLeeeee/Ollama-For-AMD-Installer&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The only time Ollama works on my machine &amp;quot;ollama ps&amp;quot; reports 100% CPU. Very slow with llama 3.1. I have configured my dedicated GPU RAM in the BIOS to be 8 GB (the max for my machine) of the 32GB available on my GMKtex K4.&lt;/p&gt; &lt;p&gt;When playing with ROCM libs above, the worst case (like using the no-longer-maintained BrysonLee installer) give the dreaded &amp;quot;500 Internal Server Error: llama runner process has terminated: ROCm error&amp;quot;.&lt;/p&gt; &lt;p&gt;When running on CPU, I see the usual logs the others running Ollama on AMD GPUs see: that ollama can't find a suitable GPU.&lt;/p&gt; &lt;p&gt;If anyone has gotten this to work, there's many of us out there who'd love to know what the trick is.&lt;/p&gt; &lt;p&gt;(If your suggestion is to buy another PC, please consider being more helpful than that. My dev machine has a RTX 4080, but I'd like to run zeroclaw on this little NUC box, which should be perfect for it. And zeroclaw works fine, of course, with a paid API (Grok, OpenAI, Gemini), but for obvious reasons would like an open source model running locally, too.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Famous-Weight2271"&gt; /u/Famous-Weight2271 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5xruc/has_anyone_gotten_amd_radeon_780m_to_work_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5xruc/has_anyone_gotten_amd_radeon_780m_to_work_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5xruc/has_anyone_gotten_amd_radeon_780m_to_work_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T02:44:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r63th5</id>
    <title>Liquid LFM2-VL 450M (Q4_0) running in-browser via WebGPU (local inference)</title>
    <updated>2026-02-16T08:05:07+00:00</updated>
    <author>
      <name>/u/New_Inflation_6927</name>
      <uri>https://old.reddit.com/user/New_Inflation_6927</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r63th5/liquid_lfm2vl_450m_q4_0_running_inbrowser_via/"&gt; &lt;img alt="Liquid LFM2-VL 450M (Q4_0) running in-browser via WebGPU (local inference)" src="https://external-preview.redd.it/NVlVHryhjyeP5-OGKDEiV_ex2BfxuDpWKO79stNO5RU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=034922f8ce7d38b2b351cd03b76eba3bcaf114fb" title="Liquid LFM2-VL 450M (Q4_0) running in-browser via WebGPU (local inference)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Inflation_6927"&gt; /u/New_Inflation_6927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hds7nl76dtjg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r63th5/liquid_lfm2vl_450m_q4_0_running_inbrowser_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r63th5/liquid_lfm2vl_450m_q4_0_running_inbrowser_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T08:05:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6cgfw</id>
    <title>How to integrate local ollama into vs code?</title>
    <updated>2026-02-16T15:23:34+00:00</updated>
    <author>
      <name>/u/elixon</name>
      <uri>https://old.reddit.com/user/elixon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r6cgfw/how_to_integrate_local_ollama_into_vs_code/"&gt; &lt;img alt="How to integrate local ollama into vs code?" src="https://preview.redd.it/bqtub1ngjvjg1.png?width=140&amp;amp;height=139&amp;amp;auto=webp&amp;amp;s=772ba358c46068f37cb3a852d1f2a330ced77190" title="How to integrate local ollama into vs code?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elixon"&gt; /u/elixon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/vscode/comments/1r6cg6r/how_to_integrate_local_ollama_into_vs_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6cgfw/how_to_integrate_local_ollama_into_vs_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6cgfw/how_to_integrate_local_ollama_into_vs_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T15:23:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6dsun</id>
    <title>New to the space - Hey guys üëã</title>
    <updated>2026-02-16T16:12:52+00:00</updated>
    <author>
      <name>/u/notalentwasted</name>
      <uri>https://old.reddit.com/user/notalentwasted</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So let me start with that I'm broke rn so don't just. I feel I should brag about my set up though since it's necromancy. &lt;/p&gt; &lt;p&gt;I have a Asus Maximus IV gene Z i7 3770 - 24gb ram (1333) , M10 32gb (4 x 8gb GM107 dies) plus a GTX750TI 2gb for embeddings models. OS is Ubuntu on 128gb ssd. Model storage is on pcie to m.2 256gb.1.4gb read - I use 8b or less and get sub 4 second cold swap. The m10 is in the first x16 slot. Pcie to m.2 is in x8 slot (space doesn't allow) then I got the gtx750ti to run on the x4 slot. 1tb hdd auxiliary storage &lt;/p&gt; &lt;p&gt;I run 18 ish services and have a 300 plus line docker compose file. (Changes frequently) &lt;/p&gt; &lt;p&gt;I only started this journey last may after finishing school. Kinda just finding the limits of the architecture and seeing what I can squeeze out of this gpu set up.&lt;/p&gt; &lt;p&gt;I'm new to the community and looking for my tribe! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notalentwasted"&gt; /u/notalentwasted &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6dsun/new_to_the_space_hey_guys/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6dsun/new_to_the_space_hey_guys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6dsun/new_to_the_space_hey_guys/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T16:12:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5wlwk</id>
    <title>What kind of models can I run on my M5 MBP with 24GB RAM?</title>
    <updated>2026-02-16T01:49:32+00:00</updated>
    <author>
      <name>/u/saintforlife1</name>
      <uri>https://old.reddit.com/user/saintforlife1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got a new M5 MacBook Pro with 24GB RAM. What kind of models are best suited to run on such a machine?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/saintforlife1"&gt; /u/saintforlife1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5wlwk/what_kind_of_models_can_i_run_on_my_m5_mbp_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5wlwk/what_kind_of_models_can_i_run_on_my_m5_mbp_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5wlwk/what_kind_of_models_can_i_run_on_my_m5_mbp_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T01:49:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6gyv6</id>
    <title>How do you decide to choose between fine tuning an LLM model or using RAG?</title>
    <updated>2026-02-16T18:05:55+00:00</updated>
    <author>
      <name>/u/degr8sid</name>
      <uri>https://old.reddit.com/user/degr8sid</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/degr8sid"&gt; /u/degr8sid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LLM/comments/1r6gy86/how_do_you_decide_to_choose_between_fine_tuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6gyv6/how_do_you_decide_to_choose_between_fine_tuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6gyv6/how_do_you_decide_to_choose_between_fine_tuning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T18:05:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6h59q</id>
    <title>PolyClaw ‚Äì An Autonomous Docker-First MCP Agent for PolyMCP</title>
    <updated>2026-02-16T18:12:17+00:00</updated>
    <author>
      <name>/u/Just_Vugg_PolyMCP</name>
      <uri>https://old.reddit.com/user/Just_Vugg_PolyMCP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r6h59q/polyclaw_an_autonomous_dockerfirst_mcp_agent_for/"&gt; &lt;img alt="PolyClaw ‚Äì An Autonomous Docker-First MCP Agent for PolyMCP" src="https://external-preview.redd.it/od8MSIgruh5Q6AVcijBeXw-iTs44_Suwgl-5tJxhnnM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=311527a31504769db78a4645ad608cd000b27422" title="PolyClaw ‚Äì An Autonomous Docker-First MCP Agent for PolyMCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Vugg_PolyMCP"&gt; /u/Just_Vugg_PolyMCP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/poly-mcp/PolyMCP"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6h59q/polyclaw_an_autonomous_dockerfirst_mcp_agent_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6h59q/polyclaw_an_autonomous_dockerfirst_mcp_agent_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T18:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5r9tp</id>
    <title>Why Ollama and not ChatGPT?</title>
    <updated>2026-02-15T21:54:40+00:00</updated>
    <author>
      <name>/u/Humble_Ad_7053</name>
      <uri>https://old.reddit.com/user/Humble_Ad_7053</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Asking out of curiosity because I‚Äôm new to Ollama. I do understand the local storage and privacy concerns, and working offline. But I want to know what other reasons people go for it. Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humble_Ad_7053"&gt; /u/Humble_Ad_7053 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5r9tp/why_ollama_and_not_chatgpt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r5r9tp/why_ollama_and_not_chatgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r5r9tp/why_ollama_and_not_chatgpt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-15T21:54:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6l3vr</id>
    <title>Anything usable for everyday nonenglish chat on limited mac HW?</title>
    <updated>2026-02-16T20:35:00+00:00</updated>
    <author>
      <name>/u/librewolf</name>
      <uri>https://old.reddit.com/user/librewolf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there, got recently a Macbook Air M4 with just 16gb ram and 256gb ssd. To end my hopes faster- is there any model to chat general purpose ways with any of the current model usable enough with my specs, for czech langauge?&lt;/p&gt; &lt;p&gt;For coding Im very happy sith Codex, im just looking for chat with private stuff I dont wanr leaving my computer&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/librewolf"&gt; /u/librewolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6l3vr/anything_usable_for_everyday_nonenglish_chat_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6l3vr/anything_usable_for_everyday_nonenglish_chat_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6l3vr/anything_usable_for_everyday_nonenglish_chat_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T20:35:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6lgl6</id>
    <title>openclaw locally?</title>
    <updated>2026-02-16T20:48:08+00:00</updated>
    <author>
      <name>/u/Efficient-Level1944</name>
      <uri>https://old.reddit.com/user/Efficient-Level1944</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Efficient-Level1944"&gt; /u/Efficient-Level1944 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1r6ldr1/openclaw_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6lgl6/openclaw_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6lgl6/openclaw_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T20:48:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6fqfs</id>
    <title>Building a local RAG system for academic</title>
    <updated>2026-02-16T17:22:09+00:00</updated>
    <author>
      <name>/u/kejitoo</name>
      <uri>https://old.reddit.com/user/kejitoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I‚Äôd like to know if it‚Äôs possible to use Ollama to create a search system for dissertations and theses from a local graduate program. What would be the costs and hardware requirements?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kejitoo"&gt; /u/kejitoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6fqfs/building_a_local_rag_system_for_academic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6fqfs/building_a_local_rag_system_for_academic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6fqfs/building_a_local_rag_system_for_academic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-16T17:22:09+00:00</published>
  </entry>
</feed>
