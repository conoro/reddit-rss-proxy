<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-19T23:54:29+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1r735eo</id>
    <title>Self Hosted Alternative to NotebookLM</title>
    <updated>2026-02-17T11:06:53+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, SurfSense is an open-source alternative to NotebookLM, Perplexity, and Glean.&lt;/p&gt; &lt;p&gt;It connects any LLM to your internal knowledge sources, then lets teams chat, comment, and collaborate in real time. Think of it as a team-first research workspace with citations, connectors, and agentic workflows.&lt;/p&gt; &lt;p&gt;I‚Äôm looking for contributors. If you‚Äôre into AI agents, RAG, search, browser extensions, or open-source research tooling, would love your help.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Self-hostable (Docker)&lt;/li&gt; &lt;li&gt;25+ external connectors (search engines, Drive, Slack, Teams, Jira, Notion, GitHub, Discord, and more)&lt;/li&gt; &lt;li&gt;Realtime Group Chats&lt;/li&gt; &lt;li&gt;Hybrid retrieval (semantic + full-text) with cited answers&lt;/li&gt; &lt;li&gt;Deep agent architecture (planning + subagents + filesystem access)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs and 6000+ embedding models (via OpenAI-compatible APIs + LiteLLM)&lt;/li&gt; &lt;li&gt;50+ file formats (including Docling/local parsing options)&lt;/li&gt; &lt;li&gt;Podcast generation (multiple TTS providers)&lt;/li&gt; &lt;li&gt;Cross-browser extension to save dynamic/authenticated web pages&lt;/li&gt; &lt;li&gt;RBAC roles for teams&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Slide creation support&lt;/li&gt; &lt;li&gt;Multilingual podcast support&lt;/li&gt; &lt;li&gt;Video creation agent&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r735eo/self_hosted_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r735eo/self_hosted_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r735eo/self_hosted_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T11:06:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r84oeh</id>
    <title>"Cognitive Steering" Instructions for Agentic RAG</title>
    <updated>2026-02-18T14:37:51+00:00</updated>
    <author>
      <name>/u/frank_brsrk</name>
      <uri>https://old.reddit.com/user/frank_brsrk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r84oeh/cognitive_steering_instructions_for_agentic_rag/"&gt; &lt;img alt="&amp;quot;Cognitive Steering&amp;quot; Instructions for Agentic RAG" src="https://external-preview.redd.it/HXb-D9eU2P5vIXUt6gq3lvWTQEJlvBhE48-2XXbWXhI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c69018b0d010ba9192b19df33508bcb8979f250" title="&amp;quot;Cognitive Steering&amp;quot; Instructions for Agentic RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frank_brsrk"&gt; /u/frank_brsrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/u_frank_brsrk/comments/1r8401f/cognitive_steering_instructions_for_agentic_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r84oeh/cognitive_steering_instructions_for_agentic_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r84oeh/cognitive_steering_instructions_for_agentic_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T14:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7tga7</id>
    <title>Intelligent (local + cloud) routing for OpenClaw via Plano</title>
    <updated>2026-02-18T04:36:05+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r7tga7/intelligent_local_cloud_routing_for_openclaw_via/"&gt; &lt;img alt="Intelligent (local + cloud) routing for OpenClaw via Plano" src="https://preview.redd.it/g9cwxqtwl6kg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5201cb2344d316a29a063feea66b959fcfdbfa03" title="Intelligent (local + cloud) routing for OpenClaw via Plano" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenClaw is notorious about its token usage, and for many the price of Opus 4.6 can be cost prohibitive for personal projects. The usual workaround is ‚Äújust switch to a cheaper model‚Äù (Kimi k2.5, etc.), but then you are accepting a trade off: you either eat a noticeable drop in quality or you end up constantly swapping models back and forth based on usage patterns&lt;/p&gt; &lt;p&gt;I packaged Arch-Router (used b HF: &lt;a href="https://x.com/ClementDelangue/status/1979256873669849195"&gt;https://x.com/ClementDelangue/status/1979256873669849195&lt;/a&gt;) into Plano and now calls from OpenClaw can get automatically routed to the right upstream LLM based on preferences you set. Preference could be anything that you can encapsulate as a task. For e.g. for daily calendar and email work you could redirect calls to Ollama-based models locally and for building apps with OpenClaw you could redirect that traffic to Opus 4.6&lt;/p&gt; &lt;p&gt;This hard choice of choosing one model over another goes away with this release. Links to the project below&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g9cwxqtwl6kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7tga7/intelligent_local_cloud_routing_for_openclaw_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r7tga7/intelligent_local_cloud_routing_for_openclaw_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T04:36:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r88jyj</id>
    <title>I got tired of on-device LLMs crashing my mobile apps, so I built a "Managed" runtime (14k LOC</title>
    <updated>2026-02-18T17:00:32+00:00</updated>
    <author>
      <name>/u/Mundane-Tea-3488</name>
      <uri>https://old.reddit.com/user/Mundane-Tea-3488</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have played around with loads of on-device AI demo for 30 sec they look mesmerising, then the phone turns into a heater and the OS kilss the app dies to memory spikes .&lt;/p&gt; &lt;p&gt;Spent last few months building &lt;a href="https://pub.dev/packages/edge_veda"&gt;Edge-Veda&lt;/a&gt;. I's nt just another wrapper; its a supervised runtime that treats LLMs like prod workloads.&lt;/p&gt; &lt;p&gt;Whats init that makes it cooler:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The Scheduler: Monitors ios/android thermal and battery levels in real time. If the phone gets too hot, it downscales the token/sec&lt;/li&gt; &lt;li&gt;Full StackL Support for GGUF(Text), Whisper(Speech), and VLMs(Vision)&lt;/li&gt; &lt;li&gt;Local RAG: Built in Vector Search(HNSW) thats stays 100% offline&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Its completely opensource &amp;amp; runs via FFI with zero cloud dependencies&lt;/p&gt; &lt;p&gt;Github &lt;a href="https://github.com/ramanujammv1988/edge-veda"&gt;link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane-Tea-3488"&gt; /u/Mundane-Tea-3488 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r88jyj/i_got_tired_of_ondevice_llms_crashing_my_mobile/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r88jyj/i_got_tired_of_ondevice_llms_crashing_my_mobile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r88jyj/i_got_tired_of_ondevice_llms_crashing_my_mobile/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T17:00:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8fjpk</id>
    <title>OllamaFX 0.5.0 Agrupar Chats en carpetas</title>
    <updated>2026-02-18T21:14:39+00:00</updated>
    <author>
      <name>/u/Electronic-Reason582</name>
      <uri>https://old.reddit.com/user/Electronic-Reason582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r8fjpk/ollamafx_050_agrupar_chats_en_carpetas/"&gt; &lt;img alt="OllamaFX 0.5.0 Agrupar Chats en carpetas" src="https://preview.redd.it/ab9qezp3kbkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=516aee86a17e3019fbeab55654ce4fd182df9d3c" title="OllamaFX 0.5.0 Agrupar Chats en carpetas" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sigo en el desarrollo de OllamaFX para su proxima versi√≥n 0.5.0 y una de sus principales caracterisitcas ser√° poder agrupar los chats con tus LLms en carpetas, mover chats entre carpetas y la implementaci√≥n de la papelera de reciclaje, que tal les parece esta funcionalidad, que mas le agregarian, a quienes deseen probar este es el repo de GitHub:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/fredericksalazar/OllamaFX"&gt;https://github.com/fredericksalazar/OllamaFX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="/submit/?source_id=t3_1r8feu9"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Reason582"&gt; /u/Electronic-Reason582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ab9qezp3kbkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8fjpk/ollamafx_050_agrupar_chats_en_carpetas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8fjpk/ollamafx_050_agrupar_chats_en_carpetas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T21:14:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8jhjf</id>
    <title>I built a local AI dev assistant with hybrid RAG (vector + knowledge graph) that works with any Ollama model</title>
    <updated>2026-02-18T23:48:52+00:00</updated>
    <author>
      <name>/u/ikchain</name>
      <uri>https://old.reddit.com/user/ikchain</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r8jhjf/i_built_a_local_ai_dev_assistant_with_hybrid_rag/"&gt; &lt;img alt="I built a local AI dev assistant with hybrid RAG (vector + knowledge graph) that works with any Ollama model" src="https://external-preview.redd.it/U7NHh829KotVUT2qBxP3c7Yx8tVj-dzHdJX1IPE9G-s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=842ea2b5b1eced725c695fc923264016241430b1" title="I built a local AI dev assistant with hybrid RAG (vector + knowledge graph) that works with any Ollama model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikchain"&gt; /u/ikchain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1r8jgwv/i_built_a_local_ai_dev_assistant_with_hybrid_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8jhjf/i_built_a_local_ai_dev_assistant_with_hybrid_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8jhjf/i_built_a_local_ai_dev_assistant_with_hybrid_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T23:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8strr</id>
    <title>First impression of running new Omi app with the Terminator bundle. built in coding agent, browser with your active sessions (no need to log in), access to your files</title>
    <updated>2026-02-19T07:26:22+00:00</updated>
    <author>
      <name>/u/Deep_Ad1959</name>
      <uri>https://old.reddit.com/user/Deep_Ad1959</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r8strr/first_impression_of_running_new_omi_app_with_the/"&gt; &lt;img alt="First impression of running new Omi app with the Terminator bundle. built in coding agent, browser with your active sessions (no need to log in), access to your files" src="https://external-preview.redd.it/MWdhMTZqazVsZWtnMU3U_53tuytiWKXwIQfZXr1-wpNHCxa9_lfgDuhwha5m.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=574d1fd195b16c806eb9f6d660e12f838a61f0f8" title="First impression of running new Omi app with the Terminator bundle. built in coding agent, browser with your active sessions (no need to log in), access to your files" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fully open source, MIT license, works better than openclaw &lt;a href="https://github.com/BasedHardware/omi"&gt;https://github.com/BasedHardware/omi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep_Ad1959"&gt; /u/Deep_Ad1959 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b4d55uj5lekg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8strr/first_impression_of_running_new_omi_app_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8strr/first_impression_of_running_new_omi_app_with_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T07:26:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8i5lo</id>
    <title>Built a lightweight Go daemon that gives Ollama a voice on Slack, Discord and Telegram - fully auditable, no cloud, no nonsense</title>
    <updated>2026-02-18T22:54:15+00:00</updated>
    <author>
      <name>/u/_pdp_</name>
      <uri>https://old.reddit.com/user/_pdp_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;You've already done the hard part. You've got Ollama running locally, your models are tuned, your data stays on your machine. The only thing missing is a way to actually talk to it outside a terminal.&lt;/p&gt; &lt;p&gt;Pantalk solves exactly that. Run &lt;strong&gt;pantalkd&lt;/strong&gt; alongside Ollama CLI and it connects your model to your messaging platforms. Slack, Discord, Telegram, Mattermost and more. Send it a message, get a response, let it notify you when a task is done. All local.&lt;/p&gt; &lt;p&gt;The tool is written in Go, fully auditable, and compiles from source. No cloud dependency. No hidden network calls. No bloated dependencies sneaking in through the back door. If you've spent time building a local setup you're proud of, Pantalk won't be the thing that ruins it.&lt;/p&gt; &lt;p&gt;Ollama does the work. Pantalk just gives it a voice.&lt;/p&gt; &lt;p&gt;Links to the GitHub page in the comments below.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_pdp_"&gt; /u/_pdp_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8i5lo/built_a_lightweight_go_daemon_that_gives_ollama_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8i5lo/built_a_lightweight_go_daemon_that_gives_ollama_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8i5lo/built_a_lightweight_go_daemon_that_gives_ollama_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T22:54:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r87oyn</id>
    <title>Mac mini m4 24gb</title>
    <updated>2026-02-18T16:30:21+00:00</updated>
    <author>
      <name>/u/Aromatic_Radio1650</name>
      <uri>https://old.reddit.com/user/Aromatic_Radio1650</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi guys i was wondering what models i could run on a mac mini m4 24gb ram, would i be able to run gpt-oss:20b? or even a 30b model? or do i need to lower my standards and run a 14b model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aromatic_Radio1650"&gt; /u/Aromatic_Radio1650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r87oyn/mac_mini_m4_24gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r87oyn/mac_mini_m4_24gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r87oyn/mac_mini_m4_24gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T16:30:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8a164</id>
    <title>üñãÔ∏è Just released AI-Writer: A free, offline desktop app for AI-assisted writing powered by Ollama + PyQt5 Body:</title>
    <updated>2026-02-18T17:52:42+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r8a164/just_released_aiwriter_a_free_offline_desktop_app/"&gt; &lt;img alt="üñãÔ∏è Just released AI-Writer: A free, offline desktop app for AI-assisted writing powered by Ollama + PyQt5 Body:" src="https://preview.redd.it/3oi0wof5kakg1.png?width=140&amp;amp;height=95&amp;amp;auto=webp&amp;amp;s=0d09f41acfbd80bf2da6d5e7479c010906c5c7d7" title="üñãÔ∏è Just released AI-Writer: A free, offline desktop app for AI-assisted writing powered by Ollama + PyQt5 Body:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to share a project I've been working on: **AI-Writer** ‚Äî a sleek, privacy-focused desktop app that lets you write with your *local* LLMs via Ollama. No API keys, no cloud, no telemetry. Just you, your words, and your model.&lt;/p&gt; &lt;p&gt;üîó **GitHub:** &lt;a href="https://github.com/Laszlobeer/AI-Writer"&gt;https://github.com/Laszlobeer/AI-Writer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3oi0wof5kakg1.png?width=1076&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1687bc6dd7da2820e48afdf040d6365e689416a1"&gt;https://preview.redd.it/3oi0wof5kakg1.png?width=1076&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1687bc6dd7da2820e48afdf040d6365e689416a1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;### ‚ú® What it does:&lt;/p&gt; &lt;p&gt;- ü§ñ **AI Text Completion**: Highlight text or place your cursor and let your local model continue your story, article, or notes&lt;/p&gt; &lt;p&gt;- üé® **Light/Dark Mode**: Because eyes matter&lt;/p&gt; &lt;p&gt;- üå°Ô∏è **Temperature &amp;amp; Token Controls**: Fine-tune creativity vs. focus on the fly&lt;/p&gt; &lt;p&gt;- üìö **Model Switching**: Instantly swap between any Ollama models you have installed&lt;/p&gt; &lt;p&gt;- üíæ **Export Flexibility**: Save your work as `.txt` or `.docx` (Word-compatible)&lt;/p&gt; &lt;p&gt;- ‚å®Ô∏è **Keyboard Shortcuts**: Write faster with intuitive hotkeys&lt;/p&gt; &lt;p&gt;### üõ†Ô∏è Built with:&lt;/p&gt; &lt;p&gt;- Python 3.8+&lt;/p&gt; &lt;p&gt;- PyQt5 for the GUI&lt;/p&gt; &lt;p&gt;- Requests for Ollama API communication&lt;/p&gt; &lt;p&gt;- python-docx for Word export&lt;/p&gt; &lt;p&gt;### üöÄ Quick Start:&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;ollama pull thewindmom/hermes-3-llama-3.1-8b&lt;/p&gt; &lt;p&gt;# 2. Clone &amp;amp; install&lt;/p&gt; &lt;p&gt;git clone &lt;a href="https://github.com/Laszlobeer/AI-Writer.git"&gt;https://github.com/Laszlobeer/AI-Writer.git&lt;/a&gt;&lt;/p&gt; &lt;p&gt;cd AI-Writer&lt;/p&gt; &lt;p&gt;pip install -r requirements.txt&lt;/p&gt; &lt;p&gt;# 3. Launch&lt;/p&gt; &lt;p&gt;python ai_writer.py&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;### üí° Why I built this:&lt;/p&gt; &lt;p&gt;I wanted a distraction-free writing environment that leverages local AI *without* sending my drafts to the cloud. Whether you're drafting fiction, technical docs, or journaling ‚Äî AI-Writer keeps your workflow private and under your control.&lt;/p&gt; &lt;p&gt;### üôè Feedback welcome!&lt;/p&gt; &lt;p&gt;This is my first major PyQt5 project, so I'd love to hear:&lt;/p&gt; &lt;p&gt;- What features would make your writing workflow better?&lt;/p&gt; &lt;p&gt;- Any bugs or UX quirks you spot?&lt;/p&gt; &lt;p&gt;- Ideas for export formats or integrations?&lt;/p&gt; &lt;p&gt;All contributions and suggestions are welcome on GitHub! üôå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8a164/just_released_aiwriter_a_free_offline_desktop_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8a164/just_released_aiwriter_a_free_offline_desktop_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8a164/just_released_aiwriter_a_free_offline_desktop_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T17:52:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8tbjl</id>
    <title>Showcase: An Autonomous AI Agent Engine built with FastAPI &amp; Asyncio</title>
    <updated>2026-02-19T07:57:14+00:00</updated>
    <author>
      <name>/u/Emqnuele</name>
      <uri>https://old.reddit.com/user/Emqnuele</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r8tbjl/showcase_an_autonomous_ai_agent_engine_built_with/"&gt; &lt;img alt="Showcase: An Autonomous AI Agent Engine built with FastAPI &amp;amp; Asyncio" src="https://external-preview.redd.it/h8KYdDxXrs-2CTlj9ANBL2lZwKnP5gX0E9VgoWQVrLY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=239ed6bf516e31b39c7d021fea4d373f7ac06cbb" title="Showcase: An Autonomous AI Agent Engine built with FastAPI &amp;amp; Asyncio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emqnuele"&gt; /u/Emqnuele &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Python/comments/1r8snoe/showcase_an_autonomous_ai_agent_engine_built_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8tbjl/showcase_an_autonomous_ai_agent_engine_built_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8tbjl/showcase_an_autonomous_ai_agent_engine_built_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T07:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r858n6</id>
    <title>new interactive UI for my CLI OLLAMA (LLM Checker) command palette, hardware-aware model picks, MCP support .....</title>
    <updated>2026-02-18T15:00:17+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r858n6/new_interactive_ui_for_my_cli_ollama_llm_checker/"&gt; &lt;img alt="new interactive UI for my CLI OLLAMA (LLM Checker) command palette, hardware-aware model picks, MCP support ....." src="https://external-preview.redd.it/eWprdDhpOHRtOWtnMQcNrfIPwOqic-_j14Vn0j5QNYVM7Lkrrt4-l7TtShiW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2406c7a9d06eb3534d5020101ade4189ba7d206" title="new interactive UI for my CLI OLLAMA (LLM Checker) command palette, hardware-aware model picks, MCP support ....." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What changed in this release:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;llm-checker now opens an interactive panel when run with no args&lt;/li&gt; &lt;li&gt;/ opens the full command list, arrows + Enter to run&lt;/li&gt; &lt;li&gt;you can add extra flags before executing a command&lt;/li&gt; &lt;li&gt;new animated banner and cleaner help output (llm-checker help)&lt;/li&gt; &lt;li&gt;recommendations now score against &lt;strong&gt;200+ dynamic Ollama models&lt;/strong&gt; (with &lt;strong&gt;35+ fallback&lt;/strong&gt; when needed)&lt;/li&gt; &lt;li&gt;deterministic &lt;strong&gt;4D scoring&lt;/strong&gt;: Quality / Speed / Fit / Context&lt;/li&gt; &lt;li&gt;better hardware detection across Apple Silicon, NVIDIA CUDA, AMD ROCm, Intel, CPU&lt;/li&gt; &lt;li&gt;calibration + routing flow: calibrate ‚Üí --calibrated / --policy in recommend and ai-run&lt;/li&gt; &lt;li&gt;policy audit export support (JSON / CSV / SARIF)&lt;/li&gt; &lt;li&gt;MCP server mode for Claude/Codex workflows (llm-checker-mcp)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Install/update: npm i -g llm-checker&lt;br /&gt; Repo: &lt;a href="https://github.com/Pavelevich/llm-checker"&gt;https://github.com/Pavelevich/llm-checker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback on the UI/flow is welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cymz3w7tm9kg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r858n6/new_interactive_ui_for_my_cli_ollama_llm_checker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r858n6/new_interactive_ui_for_my_cli_ollama_llm_checker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T15:00:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8gfwa</id>
    <title>Schedule mode is coming to Nanocoder... Run project background tasks on a cron schedule üöÄ</title>
    <updated>2026-02-18T21:48:14+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r8gfwa/schedule_mode_is_coming_to_nanocoder_run_project/"&gt; &lt;img alt="Schedule mode is coming to Nanocoder... Run project background tasks on a cron schedule üöÄ" src="https://external-preview.redd.it/s-ytFBPksextywodml2eXUVDHp5dUg_7-gDMuxnUgPU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97c257e47d5c846e99be28e236a586039dc7e347" title="Schedule mode is coming to Nanocoder... Run project background tasks on a cron schedule üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4c72kn5rnbkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8gfwa/schedule_mode_is_coming_to_nanocoder_run_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8gfwa/schedule_mode_is_coming_to_nanocoder_run_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T21:48:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8pcpb</id>
    <title>Chat Completions in Ollama Cloud</title>
    <updated>2026-02-19T04:15:27+00:00</updated>
    <author>
      <name>/u/Cerbrus-spillus</name>
      <uri>https://old.reddit.com/user/Cerbrus-spillus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found that local Ollama supports some of the features of OpenAI's Chat Completions endpoint. However, I am wondering if the Ollama Cloud also supports this, as I could not find an appropriate &lt;code&gt;base_url&lt;/code&gt; to target for Chat Completions with Ollama Cloud. I could not find much information about this from their documentation either. Does anyone have a clue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cerbrus-spillus"&gt; /u/Cerbrus-spillus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8pcpb/chat_completions_in_ollama_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8pcpb/chat_completions_in_ollama_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8pcpb/chat_completions_in_ollama_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T04:15:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8vpb0</id>
    <title>Got $800 of credits on a cloud platform (for GPU usage). Anyone here that's into AI training and inference and could make use of it?</title>
    <updated>2026-02-19T10:24:33+00:00</updated>
    <author>
      <name>/u/DocumentFun9077</name>
      <uri>https://old.reddit.com/user/DocumentFun9077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have around 800 bucks worth of GPU usage credits on one of the major platform, those can be used specifically for GPU and clusters. So if any individual or hobbyist or anyone out here is training models or inference, or anything else, please contact!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocumentFun9077"&gt; /u/DocumentFun9077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8vpb0/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8vpb0/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8vpb0/got_800_of_credits_on_a_cloud_platform_for_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T10:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r87tdt</id>
    <title>packaged claude code + omi + terminator, now it's better than openclaw</title>
    <updated>2026-02-18T16:34:35+00:00</updated>
    <author>
      <name>/u/Deep_Ad1959</name>
      <uri>https://old.reddit.com/user/Deep_Ad1959</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r87tdt/packaged_claude_code_omi_terminator_now_its/"&gt; &lt;img alt="packaged claude code + omi + terminator, now it's better than openclaw" src="https://external-preview.redd.it/YTk1amRlMjg2YWtnMXjfFo5q1zD6K1Ubw5_sFCkd17GjeVIYKU9ByDARBA-a.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83f20fb8d8583165dc35e6384f12e34a07932696" title="packaged claude code + omi + terminator, now it's better than openclaw" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try asking your AI agent to do things for you, it will ask you for 10 confirmations, and still won't do most of the things for you directly, it's frustrating, kills the whole purpose...&lt;/p&gt; &lt;p&gt;What i want is I give a task, and it works on it without bothering me while using my computer, but without taking over my computer. Sounds contradicting, but it's now possible. &lt;/p&gt; &lt;p&gt;Omi + Terminator + Claude Code can work on your computer using your active browser without taking over your keyboard or mouse. See how the agent buys me a ticket with a single prompt. &lt;/p&gt; &lt;p&gt;First, I tried asking Claude Cowork to do that, and it refused, but then Omi did it all. Omi has all the context about me, all the preferences, and access to my credit card. Claude code is spinned up by Omi to work on the task, while Terminator can execute based of the function calls from Claude Code. See it in action &lt;/p&gt; &lt;p&gt;Open source &lt;a href="https://github.com/BasedHardware/omi"&gt;https://github.com/BasedHardware/omi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mediar-ai/terminator"&gt;https://github.com/mediar-ai/terminator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep_Ad1959"&gt; /u/Deep_Ad1959 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/on9h6a286akg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r87tdt/packaged_claude_code_omi_terminator_now_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r87tdt/packaged_claude_code_omi_terminator_now_its/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T16:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8kj39</id>
    <title>Building local for general chatbot, code assistance, and content creation - Need help with model selection</title>
    <updated>2026-02-19T00:33:12+00:00</updated>
    <author>
      <name>/u/RobDoesData</name>
      <uri>https://old.reddit.com/user/RobDoesData</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I work in data engineering and analytics. I am building a a locally hosted chatbot to help me in my work. I have it setup so that I can have 3 standalone chats at one time and jump between them while their context and conversation is isolated from one another. Each chat has its own specified temp, top_p, max tokens, system prompt, and LLM.&lt;/p&gt; &lt;p&gt;I'm after recommendations for models to use, system prompts and parameter values for the following usecases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General chatbot for all sorts of things related to work, tech, reporting, etc.&lt;/li&gt; &lt;li&gt;Code generation (Python) and Azure&lt;/li&gt; &lt;li&gt;Content review, editing, and brainstorming&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'm running a 5070TI with 12 GB VRAM. Looking forward to your responses.&lt;/p&gt; &lt;p&gt;I've tried Llama3, Phi4, and Mistral-Nemo 12B thus far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobDoesData"&gt; /u/RobDoesData &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8kj39/building_local_for_general_chatbot_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8kj39/building_local_for_general_chatbot_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8kj39/building_local_for_general_chatbot_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T00:33:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8iea9</id>
    <title>ClawCache: Free local caching + tracking for Ollama calls ‚Äì cuts waste on repeats in agents/scripts inspired by efficiency threads here</title>
    <updated>2026-02-18T23:03:40+00:00</updated>
    <author>
      <name>/u/SignificantClub4279</name>
      <uri>https://old.reddit.com/user/SignificantClub4279</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In local LLM communities like this one, a common observation is that while local runs give excellent privacy and avoid cloud per-token fees, agents and scripts frequently re-process the same or similar prompts ‚Äî especially in loops, tool calls, or debugging sessions ‚Äî leading to wasted compute and time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Common issues I've seen in recent posts:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Long runs repeating queries and burning unnecessary GPU cycles&lt;/li&gt; &lt;li&gt;Agent frameworks chaining similar calls without reuse&lt;/li&gt; &lt;li&gt;Debugging sessions re-inferring the same content repeatedly&lt;/li&gt; &lt;li&gt;Searches for caching layers, context optimizations, or efficiency tools&lt;/li&gt; &lt;li&gt;Hybrid Ollama + API setups still incurring token costs on repeats&lt;/li&gt; &lt;li&gt;No easy built-in exact (or semantic) caching across most local workflows&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The core problem:&lt;/strong&gt; Ollama makes local inference straightforward and powerful, but when building agents or complex scripts, exact repeats are frequent ‚Äî and without caching, every call re-runs the model from scratch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So I built ClawCache ‚Äì inspired by these local optimization discussions.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It does three simple things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Tracks every call&lt;/strong&gt; ‚Äì Logs Ollama (or any LLM) usage with token counts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Caches locally&lt;/strong&gt; ‚Äì SQLite-based, serves exact repeats from disk (proven ~58% hit rate in agent-like workflows ‚Üí skips re-inference)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gives reports&lt;/strong&gt; ‚Äì Daily CLI summaries: calls, hits, saved compute&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Results from my own Ollama setups (agents + scripts):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tested on repetitive tasks/loops&lt;/li&gt; &lt;li&gt;~58% of calls cached&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real savings&lt;/strong&gt; ‚Äì no re-running the model on repeats&lt;/li&gt; &lt;li&gt;Foundation for semantic caching (coming in Pro ‚Äì matches similar prompts)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why it fits Ollama/local runs:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;100% local ‚Äì everything on your machine, no telemetry. Works as a lightweight wrapper around any Ollama call (or other providers in hybrid).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Free forever. MIT licensed.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;bash&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;pip install clawcache-free&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt;&lt;a href="https://github.com/AbYousef739/clawcache-free"&gt; https://github.com/AbYousef739/clawcache-free&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If it saves you GPU cycles, a on GitHub helps others find it.&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SignificantClub4279"&gt; /u/SignificantClub4279 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8iea9/clawcache_free_local_caching_tracking_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8iea9/clawcache_free_local_caching_tracking_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8iea9/clawcache_free_local_caching_tracking_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T23:03:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r90q4o</id>
    <title>Can't able access ollama through network in cline cli</title>
    <updated>2026-02-19T14:32:16+00:00</updated>
    <author>
      <name>/u/DismalBunch6739</name>
      <uri>https://old.reddit.com/user/DismalBunch6739</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi ,&lt;/p&gt; &lt;p&gt;I have ollama installed in my workstation with RTX 3090 24gb vram .&lt;/p&gt; &lt;p&gt;I tried to access ollama via internet with my domain and its running properly in command prompt.&lt;/p&gt; &lt;p&gt;But if I tried to use that API in cline cli , it can't able to access the workstation ollama.&lt;/p&gt; &lt;p&gt;I tried i LAN network same problem.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Cluade CLI is working&lt;/em&gt;&lt;/p&gt; &lt;p&gt;$env:OLLAMA_HOST=&amp;quot;&lt;a href="http://MYCUSTOMDOMAIN.in:11434"&gt;http://MYCUSTOMDOMAIN.in:11434&lt;/a&gt;&amp;quot;&lt;/p&gt; &lt;p&gt;ollama launch claude&lt;/p&gt; &lt;p&gt;This opens claude code temporarily.&lt;/p&gt; &lt;p&gt;But, I couldn't config cline CLI with ollama in every options domain/public IP/local IP .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DismalBunch6739"&gt; /u/DismalBunch6739 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r90q4o/cant_able_access_ollama_through_network_in_cline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r90q4o/cant_able_access_ollama_through_network_in_cline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r90q4o/cant_able_access_ollama_through_network_in_cline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T14:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r969u3</id>
    <title>Using Ollama to fight executive dysfunction: A local-first app that turns hourly CSV logs and Jira references into daily stand-up summaries.</title>
    <updated>2026-02-19T17:58:33+00:00</updated>
    <author>
      <name>/u/Tech_Devils</name>
      <uri>https://old.reddit.com/user/Tech_Devils</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r969u3/using_ollama_to_fight_executive_dysfunction_a/"&gt; &lt;img alt="Using Ollama to fight executive dysfunction: A local-first app that turns hourly CSV logs and Jira references into daily stand-up summaries." src="https://external-preview.redd.it/fY2XJ1XQCwNWaM0O_h5P96oPa02vBKxm3Tbx8p4T08I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e7fbd50b90b20b2880b17405e054ac42038252e" title="Using Ollama to fight executive dysfunction: A local-first app that turns hourly CSV logs and Jira references into daily stand-up summaries." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I recently stepped out of my usual C#/SQL stack to build a Python app to help manage my ADHD and time blindness. It's called SheepCat-TrackingMyWork.&lt;/p&gt; &lt;p&gt;It runs in the background, prompts you hourly for what you've done (or Jira ticket references), and saves it to a local CSV. The core engine? It hooks directly into your local Ollama setup via Docker to generate hourly summaries and a final end-of-day stand-up report, keeping all confidential enterprise data 100% private.&lt;/p&gt; &lt;p&gt;I just did a full write-up with the architecture details, the open-source GitHub links, and my commercial license plans over in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Since this sub knows the ecosystem best, I‚Äôd love your input on the original thread: Which models (8B and under) in the Ollama library are you finding best for structured CSV summarization right now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tech_Devils"&gt; /u/Tech_Devils &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1r964lb/using_ollama_to_fight_executive_dysfunction_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r969u3/using_ollama_to_fight_executive_dysfunction_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r969u3/using_ollama_to_fight_executive_dysfunction_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T17:58:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r96uu8</id>
    <title>The greatest openclaw fork ever!</title>
    <updated>2026-02-19T18:19:05+00:00</updated>
    <author>
      <name>/u/CryptographerLow6360</name>
      <uri>https://old.reddit.com/user/CryptographerLow6360</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r96uu8/the_greatest_openclaw_fork_ever/"&gt; &lt;img alt="The greatest openclaw fork ever!" src="https://external-preview.redd.it/o4H0hywG3ZDU87_JTEfOMrijrwF8Wvzu1Bq8kfuGALU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d3b3be26adb7242635de1b4fcb6dd5db66d0360a" title="The greatest openclaw fork ever!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CryptographerLow6360"&gt; /u/CryptographerLow6360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Localclaw/comments/1r96qac/the_greatest_openclaw_fork_ever/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r96uu8/the_greatest_openclaw_fork_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r96uu8/the_greatest_openclaw_fork_ever/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T18:19:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r984g4</id>
    <title>Causal Failure Anti-Patterns (RAG)</title>
    <updated>2026-02-19T19:04:04+00:00</updated>
    <author>
      <name>/u/frank_brsrk</name>
      <uri>https://old.reddit.com/user/frank_brsrk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r984g4/causal_failure_antipatterns_rag/"&gt; &lt;img alt="Causal Failure Anti-Patterns (RAG)" src="https://preview.redd.it/3t3nkygrxhkg1.png?width=140&amp;amp;height=124&amp;amp;auto=webp&amp;amp;s=f6a1710c547af4023b8139d2b1c718cdcab68534" title="Causal Failure Anti-Patterns (RAG)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frank_brsrk"&gt; /u/frank_brsrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/u_frank_brsrk/comments/1r97eyi/causal_failure_antipatterns_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r984g4/causal_failure_antipatterns_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r984g4/causal_failure_antipatterns_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T19:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r99cqy</id>
    <title>Building a lightweight Python bridge for Qwen 2.5 Coder (7B) Handling loops and context poisoning in a 3-tier memory setup?</title>
    <updated>2026-02-19T19:49:40+00:00</updated>
    <author>
      <name>/u/This-Magazine4277</name>
      <uri>https://old.reddit.com/user/This-Magazine4277</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/This-Magazine4277"&gt; /u/This-Magazine4277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1r99c0h/building_a_lightweight_python_bridge_for_qwen_25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r99cqy/building_a_lightweight_python_bridge_for_qwen_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r99cqy/building_a_lightweight_python_bridge_for_qwen_25/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T19:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9a0gv</id>
    <title>I built a local-first code search tool with Ollama + CocoIndex to save tokens when chatting about codebases.</title>
    <updated>2026-02-19T20:13:57+00:00</updated>
    <author>
      <name>/u/VioletCranberryy</name>
      <uri>https://old.reddit.com/user/VioletCranberryy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to play with Ollama, CocoIndex, and how semantic vs hybrid search actually works (RRF fusion and all that) - and ended up building something I actually use daily, more and more. The idea is pretty old: index your codebase locally so AI assistants can search it semantically instead of stuffing entire files into context. Fewer tokens, better results.&lt;/p&gt; &lt;p&gt;It comes with a web dashboard, MCP server, CLI, and works as a Claude Code plugin.&lt;/p&gt; &lt;p&gt;As a DevOps engineer, I also wanted to make it expandable with custom &amp;quot;grammars&amp;quot; ‚Äî so things like Helm charts, GitHub Actions, Docker Compose, and Terraform get proper structure-aware chunking instead of being treated as generic YAML/HCL. And of course, a Docker Compose setup to start everything with a single command :) &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/VioletCranberry/coco-search"&gt;https://github.com/VioletCranberry/coco-search&lt;/a&gt; &lt;/p&gt; &lt;p&gt;P.S.&lt;br /&gt; Yeah, I used AI assistants heavily during development which is kind of fitting since the tool is built to make AI-assisted coding better. I still don't know how to feel about it, first project of this type of mine. &lt;/p&gt; &lt;p&gt;P.P.S.&lt;br /&gt; Would love feedback, especially on embedding model choice for code. Something else as small as nomic-embed-text and as powerful? CocoIndex updates are incremental, but on large codebases you still need to wait when building the first index.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VioletCranberryy"&gt; /u/VioletCranberryy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9a0gv/i_built_a_localfirst_code_search_tool_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9a0gv/i_built_a_localfirst_code_search_tool_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r9a0gv/i_built_a_localfirst_code_search_tool_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T20:13:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8rgg8</id>
    <title>I built a small CLI tool to help beginners see if their hardware can actually handle local LLMs</title>
    <updated>2026-02-19T06:06:29+00:00</updated>
    <author>
      <name>/u/Narrow-Detective9885</name>
      <uri>https://old.reddit.com/user/Narrow-Detective9885</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been lurking here for a while and learning a ton from all the superusers and experts here. As a beginner myself, I often found it a bit overwhelming to figure out which models would actually run &amp;quot;well&amp;quot; on my specific machine versus just running &amp;quot;slowly.&amp;quot;&lt;/p&gt; &lt;p&gt;To help myself learn and to give something back to other newcomers, I put together a small CLI tool in Go called &lt;strong&gt;RigRank&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; It‚Äôs basically a simple benchmarking suite for Ollama. It doesn‚Äôt measure how &amp;quot;smart&amp;quot; a model is‚Äîthere are way better tools for that‚Äîbut it measures the &amp;quot;snappiness&amp;quot; of your actual hardware. It runs a few stages (code gen, summarization, reasoning, etc.) and gives you a &amp;quot;Report Card&amp;quot; with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;TTFT (Time To First Token):&lt;/strong&gt; How long you‚Äôre waiting for that first word.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Writing Speed:&lt;/strong&gt; How fast it actually spits out text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reading Speed:&lt;/strong&gt; How quickly it processes your prompts.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Who this is for:&lt;/strong&gt; Honestly, if you already have a complex benchmarking pipeline or a massive GPU cluster, this probably isn't for you. It‚Äôs designed for the person who just downloaded Ollama and wants to know: &lt;em&gt;&amp;quot;Is Llama3-8B too heavy for my laptop, or is it just me?&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I would love your feedback&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/rohanelukurthy/rig-rank"&gt;https://github.com/rohanelukurthy/rig-rank&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Narrow-Detective9885"&gt; /u/Narrow-Detective9885 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8rgg8/i_built_a_small_cli_tool_to_help_beginners_see_if/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8rgg8/i_built_a_small_cli_tool_to_help_beginners_see_if/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8rgg8/i_built_a_small_cli_tool_to_help_beginners_see_if/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T06:06:29+00:00</published>
  </entry>
</feed>
