<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-23T07:40:53+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qhvtnp</id>
    <title>timed out waiting for llama runner to start: context canceled</title>
    <updated>2026-01-20T08:41:50+00:00</updated>
    <author>
      <name>/u/DerZwirbel</name>
      <uri>https://old.reddit.com/user/DerZwirbel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; I‚Äôm seeing intermittent model load failures with &lt;strong&gt;Ollama 0.13.4&lt;/strong&gt; running in &lt;strong&gt;Docker&lt;/strong&gt; when loading &lt;strong&gt;phi4&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Error excerpt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;time=2026-01-20T08:17:55.413Z level=INFO source=sched.go:470 msg=&amp;quot;Load failed&amp;quot; model=/data/ollama/blobs/sha256-fd7b6731c33c57f61767612f56517460ec2d1e2e5a3f0163e0eb3d8d8cb5df20 error=&amp;quot;timed out waiting for llama runner to start: context canceled&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This typically happens during container startup or under load.&lt;br /&gt; CUDA is available, but the failure is non-deterministic (cold start related?).&lt;/p&gt; &lt;p&gt;Has anyone seen this with phi4 recently, or has guidance on? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DerZwirbel"&gt; /u/DerZwirbel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhvtnp/timed_out_waiting_for_llama_runner_to_start/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhvtnp/timed_out_waiting_for_llama_runner_to_start/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qhvtnp/timed_out_waiting_for_llama_runner_to_start/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-20T08:41:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qift89</id>
    <title>This was created by my autonomous enhanced programmer, it is no longer for sale.</title>
    <updated>2026-01-20T22:31:04+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qift89/this_was_created_by_my_autonomous_enhanced/"&gt; &lt;img alt="This was created by my autonomous enhanced programmer, it is no longer for sale." src="https://preview.redd.it/43lapgdkykeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=795d987a6ceb4c7d009870904f10a54410329127" title="This was created by my autonomous enhanced programmer, it is no longer for sale." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NeuralNet ‚Äì Your Intelligent Communication Assistant**&lt;/p&gt; &lt;p&gt;Imagine having your own intelligent assistant that understands your speech, translates languages, and gives you instant access to information. That‚Äôs exactly what NeuralNet offers you! This powerful application, created in LM Studio, acts as a flexible server that allows you to communicate with AI that is constantly learning and adapting.&lt;/p&gt; &lt;p&gt;**Here‚Äôs what NeuralNet can do for you:**&lt;/p&gt; &lt;p&gt;* **Seamless Text Communication:** Just type your questions or instructions ‚Äì NeuralNet responds in natural language.&lt;/p&gt; &lt;p&gt;* **Diverse and Intensive Internet Search:** NeuralNet actively searches the Internet to provide you with up-to-date information and answers without the need for links.&lt;/p&gt; &lt;p&gt;* **Multi-Language Support:** Simply set your preferred language (including English!) for optimal communication and translations.&lt;/p&gt; &lt;p&gt;* **Off-Pc Usage:** Thanks to APIs like Engrok, you can also use NeuralNet on your mobile! When your computer is on, NeuralNet is also available offline. You can use the local model directly installed on your device.&lt;/p&gt; &lt;p&gt;* **Creative Translations and Contextual Understanding:** From slang terms to more complex phrases, NeuralNet can translate accurately and with nuance.&lt;/p&gt; &lt;p&gt;**Key Features:**&lt;/p&gt; &lt;p&gt;* **Local Server Operation (LM Studio):** NeuralNet can run locally for maximum privacy and control.&lt;/p&gt; &lt;p&gt;* **API Integration:** Seamless access to external services, like Engrok, for remote use.&lt;/p&gt; &lt;p&gt;* **Continuous Learning:** NeuralNet is constantly improving its understanding based on your interactions.&lt;/p&gt; &lt;p&gt;**Ready to experience the future of communication? Start chatting with NeuralNet today!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/43lapgdkykeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qift89/this_was_created_by_my_autonomous_enhanced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qift89/this_was_created_by_my_autonomous_enhanced/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-20T22:31:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhr198</id>
    <title>GLM 4.7 is apparently almost ready on Ollama</title>
    <updated>2026-01-20T04:18:14+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/"&gt; &lt;img alt="GLM 4.7 is apparently almost ready on Ollama" src="https://a.thumbs.redditmedia.com/1aab6UqlDDpJ6gz379kpHuTSrReU-J_gAGirTkMBCr8.jpg" title="GLM 4.7 is apparently almost ready on Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's listed, just not downloadable yet. Trying in WebOllama, and in CLI gives weird excuses&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/96ly2bgckfeg1.png?width=1723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d8bfc2386dd789ef4f28ff0516de4893bf5c6772"&gt;https://preview.redd.it/96ly2bgckfeg1.png?width=1723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d8bfc2386dd789ef4f28ff0516de4893bf5c6772&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-20T04:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh5z0j</id>
    <title>I built a voice-first AI mirror that runs fully on Ollama.</title>
    <updated>2026-01-19T14:43:25+00:00</updated>
    <author>
      <name>/u/DirectorChance4012</name>
      <uri>https://old.reddit.com/user/DirectorChance4012</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qh5z0j/i_built_a_voicefirst_ai_mirror_that_runs_fully_on/"&gt; &lt;img alt="I built a voice-first AI mirror that runs fully on Ollama." src="https://external-preview.redd.it/Y3p4ZmcwM3FpYmVnMXwaGz6cYTO_1FXYid56crf85mAMdgg0ECh85UXNOmp7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a490bc51d40c3d188f9b4edee608ffa0b05365bf" title="I built a voice-first AI mirror that runs fully on Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a voice-first AI mirror that runs fully on Ollama.&lt;/p&gt; &lt;p&gt;The idea was to explore what a ‚Äúvoice-native‚Äù interface looks like&lt;/p&gt; &lt;p&gt;when it‚Äôs ambient and always there ‚Äî not a chat window.&lt;/p&gt; &lt;p&gt;Everything runs locally (LLM via Ollama), no cloud dependency.&lt;/p&gt; &lt;p&gt;Still very experimental, but surprisingly usable.&lt;/p&gt; &lt;p&gt;Blog (how it works + design decisions):&lt;/p&gt; &lt;p&gt;&lt;a href="https://noted.lol/mirrormate/"&gt;https://noted.lol/mirrormate/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub (WIP, self-hostable):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/orangekame3/mirrormate"&gt;https://github.com/orangekame3/mirrormate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DirectorChance4012"&gt; /u/DirectorChance4012 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bjeyts2qibeg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qh5z0j/i_built_a_voicefirst_ai_mirror_that_runs_fully_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qh5z0j/i_built_a_voicefirst_ai_mirror_that_runs_fully_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T14:43:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi6ngs</id>
    <title>Model choice for big (huge) text-based data search and analysis</title>
    <updated>2026-01-20T17:01:24+00:00</updated>
    <author>
      <name>/u/qball2kb</name>
      <uri>https://old.reddit.com/user/qball2kb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I‚Äôm looking at setting up an Ollama model to assist non-technical folks with searching through some massive (potentially greater than terabyte-sized), text-based datasets (stored locally in TSV/CSV, SQLite and similar formats). It would ideally be run completely offline. Is there a particular model that does this sort of thing well? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qball2kb"&gt; /u/qball2kb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qi6ngs/model_choice_for_big_huge_textbased_data_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qi6ngs/model_choice_for_big_huge_textbased_data_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qi6ngs/model_choice_for_big_huge_textbased_data_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-20T17:01:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi8ouc</id>
    <title>Plano 0.4.3 ‚≠êÔ∏è Filter Chains via MCP and OpenRouter Integration</title>
    <updated>2026-01-20T18:12:47+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qi8ouc/plano_043_filter_chains_via_mcp_and_openrouter/"&gt; &lt;img alt="Plano 0.4.3 ‚≠êÔ∏è Filter Chains via MCP and OpenRouter Integration" src="https://preview.redd.it/o590ks78pjeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4bc3a071d59ce6ea4b05664dd2470be1e05764a" title="Plano 0.4.3 ‚≠êÔ∏è Filter Chains via MCP and OpenRouter Integration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey peeps - excited to ship &lt;a href="https://github.com/katanemo/plano"&gt;Plano&lt;/a&gt; 0.4.3. Two critical updates that I think could be helpful for developers.&lt;/p&gt; &lt;p&gt;1/Filter Chains&lt;/p&gt; &lt;p&gt;Filter chains are Plano‚Äôs way of capturing &lt;strong&gt;reusable workflow steps&lt;/strong&gt; in the data plane, without duplication and coupling logic into application code. A filter chain is an ordered list of &lt;strong&gt;mutations&lt;/strong&gt; that a request flows through before reaching its final destination ‚Äîsuch as an agent, an LLM, or a tool backend. Each filter is a network-addressable service/path that can:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Inspect the incoming prompt, metadata, and conversation state.&lt;/li&gt; &lt;li&gt;Mutate or enrich the request (for example, rewrite queries or build context).&lt;/li&gt; &lt;li&gt;Short-circuit the flow and return a response early (for example, block a request on a compliance failure).&lt;/li&gt; &lt;li&gt;Emit structured logs and traces so you can debug and continuously improve your agents.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In other words, filter chains provide a lightweight programming model over HTTP for building reusable steps in your agent architectures.&lt;/p&gt; &lt;p&gt;2/ Passthrough Client Bearer Auth&lt;/p&gt; &lt;p&gt;When deploying Plano in front of LLM proxy services that manage their own API key validation (such as LiteLLM, OpenRouter, or custom gateways), users currently have to configure a static access_key. However, in many cases, it's desirable to forward the client's original Authorization header instead. This allows the upstream service to handle per-user authentication, rate limiting, and virtual keys.&lt;/p&gt; &lt;p&gt;0.4.3 introduces a passthrough_auth option iWhen set to true, Plano will forward the client's Authorization header to the upstream instead of using the configured access_key.&lt;/p&gt; &lt;p&gt;Use Cases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;OpenRouter: Forward requests to OpenRouter with per-user API keys.&lt;/li&gt; &lt;li&gt;Multi-tenant Deployments: Allow different clients to use their own credentials via Plano.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Hope you all enjoy these updates&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o590ks78pjeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qi8ouc/plano_043_filter_chains_via_mcp_and_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qi8ouc/plano_043_filter_chains_via_mcp_and_openrouter/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-20T18:12:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qidbdr</id>
    <title>Local LLM (16GBRAM + 8VRAM) for gamedev</title>
    <updated>2026-01-20T20:58:38+00:00</updated>
    <author>
      <name>/u/RagingBass2020</name>
      <uri>https://old.reddit.com/user/RagingBass2020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a developer that has been doing gamedev for 2 years but I used to be a backend developer for almost 10 years and a CS researcher before that.&lt;/p&gt; &lt;p&gt;I use mostly Unity and Jetbrains Rider. &lt;/p&gt; &lt;p&gt;Although I have a computer with more RAM at home, I need something that runs on a 16+8 GB laptop.&lt;/p&gt; &lt;p&gt;I don't want to use it to develop full systems. I want something that is decent enough to create boilerplate code and help with some scripts and maybe some stuff I'm less used to (getting ready for the global game jam).&lt;/p&gt; &lt;p&gt;It needs to run offline with no access to the internet. I'm using ollama but I also have ComfyUI for some uni classes I was taking last semester.&lt;/p&gt; &lt;p&gt;If anyone could give me recommendations, I'd appreciate it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RagingBass2020"&gt; /u/RagingBass2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qidbdr/local_llm_16gbram_8vram_for_gamedev/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qidbdr/local_llm_16gbram_8vram_for_gamedev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qidbdr/local_llm_16gbram_8vram_for_gamedev/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-20T20:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiwpdm</id>
    <title>has anyone her got Monadgpt working? my one seems to spurt odd broken gibberish.</title>
    <updated>2026-01-21T12:33:43+00:00</updated>
    <author>
      <name>/u/Mid-Pri6170</name>
      <uri>https://old.reddit.com/user/Mid-Pri6170</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(im no llm expert here)&lt;/p&gt; &lt;p&gt;it seems Monadgpt lacks logic. it speaks in 17th century style which is cool. but 2 sentences in it will turn to mush.&lt;/p&gt; &lt;p&gt;does it need extra stuff like a lora or whatever to make it work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mid-Pri6170"&gt; /u/Mid-Pri6170 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qiwpdm/has_anyone_her_got_monadgpt_working_my_one_seems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qiwpdm/has_anyone_her_got_monadgpt_working_my_one_seems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qiwpdm/has_anyone_her_got_monadgpt_working_my_one_seems/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-21T12:33:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qixxxm</id>
    <title>Fedora and it's installation.</title>
    <updated>2026-01-21T13:30:53+00:00</updated>
    <author>
      <name>/u/kellz_90</name>
      <uri>https://old.reddit.com/user/kellz_90</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kellz_90"&gt; /u/kellz_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Fedora/comments/1qhppvv/fedora_and_its_installation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qixxxm/fedora_and_its_installation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qixxxm/fedora_and_its_installation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-21T13:30:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qijhth</id>
    <title>Weekend Project: An Open-Source Claude Cowork That Can Handle Skills</title>
    <updated>2026-01-21T00:59:50+00:00</updated>
    <author>
      <name>/u/Frequent_Cash2598</name>
      <uri>https://old.reddit.com/user/Frequent_Cash2598</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent last weekend building something I had been thinking about for a while. Claude Cowork is great, but I wanted an open-source, lightweight version that could run with any model, so I created Open Cowork.&lt;/p&gt; &lt;p&gt;It's written entirely in Rust, which I had never used before. Starting from scratch meant no heavy dependencies, no Python bloat, and no reliance on existing agent SDKs. Just a tiny, fast binary that works anywhere.&lt;/p&gt; &lt;p&gt;Security was a big concern since the agents can execute code. Open Cowork handles this by running tasks inside temporary Docker containers. Everything stays isolated, but you can still experiment freely.&lt;/p&gt; &lt;p&gt;You can plug in any model you want. OpenAI, Anthropic, or even fully offline LLMs through Ollama are all supported. You keep full control over your API keys and your data.&lt;/p&gt; &lt;p&gt;It already comes with built-in skills for handling documents like PDFs and Excel files. I was surprised by how useful it became right away.&lt;/p&gt; &lt;p&gt;The development experience was wild. An AI agent helped me build a secure, open-source version of itself, and I learned Rust along the way. It was one of those projects where everything just clicked together in a weekend.&lt;/p&gt; &lt;p&gt;The code is live on GitHub: &lt;a href="https://github.com/kuse-ai/kuse_cowork"&gt;https://github.com/kuse-ai/kuse_cowork&lt;/a&gt; . It's still early, but I'd love to hear feedback from anyone who wants to try it out or contribute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frequent_Cash2598"&gt; /u/Frequent_Cash2598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-21T00:59:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjhufj</id>
    <title>Claude Code + Ollama 404 errors</title>
    <updated>2026-01-22T02:13:16+00:00</updated>
    <author>
      <name>/u/Kohanin</name>
      <uri>https://old.reddit.com/user/Kohanin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a fully working Ollama setup, OpenWebUI etc. I followed the instructions on allowing Claude Code to use Ollama models, however I keep getting 404 errors. &lt;/p&gt; &lt;p&gt;I have tried running Claude Code locally on the Ollama server and well as another machine on my lan, same results.&lt;/p&gt; &lt;p&gt;Any ideas?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;p&gt;[GIN] 2026/01/21 - 21:06:57 | 404 | 3.79¬µs | &lt;a href="http://192.168.1.245"&gt;192.168.1.245&lt;/a&gt; | POST &amp;quot;/api/event_logging/batch&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2026/01/21 - 21:07:14 | 404 | 13.361¬µs | &lt;a href="http://192.168.1.245"&gt;192.168.1.245&lt;/a&gt; | POST &amp;quot;/v1/messages?beta=true&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2026/01/21 - 21:07:14 | 404 | 2.972¬µs | &lt;a href="http://192.168.1.245"&gt;192.168.1.245&lt;/a&gt; | POST &amp;quot;/v1/messages?beta=true&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2026/01/21 - 21:07:14 | 404 | 3.04¬µs | &lt;a href="http://192.168.1.245"&gt;192.168.1.245&lt;/a&gt; | POST &amp;quot;/v1/messages?beta=true&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2026/01/21 - 21:07:14 | 404 | 4.58¬µs | &lt;a href="http://192.168.1.245"&gt;192.168.1.245&lt;/a&gt; | POST &amp;quot;/v1/messages?beta=true&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2026/01/21 - 21:07:14 | 404 | 3.064¬µs | &lt;a href="http://192.168.1.245"&gt;192.168.1.245&lt;/a&gt; | POST &amp;quot;/v1/messages?beta=true&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2026/01/21 - 21:07:14 | 404 | 3.94¬µs | &lt;a href="http://192.168.1.245"&gt;192.168.1.245&lt;/a&gt; | POST &amp;quot;/v1/messages?beta=true&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2026/01/21 - 21:07:14 | 404 | 3.219¬µs | &lt;a href="http://192.168.1.245"&gt;192.168.1.245&lt;/a&gt; | POST &amp;quot;/v1/messages?beta=true&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2026/01/21 - 21:07:14 | 404 | 3.83¬µs | &lt;a href="http://192.168.1.245"&gt;192.168.1.245&lt;/a&gt; | POST &amp;quot;/v1/messages?beta=true&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2026/01/21 - 21:07:14 | 404 | 4.89¬µs | &lt;a href="http://192.168.1.245"&gt;192.168.1.245&lt;/a&gt; | POST &amp;quot;/v1/messages?beta=true&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kohanin"&gt; /u/Kohanin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjhufj/claude_code_ollama_404_errors/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjhufj/claude_code_ollama_404_errors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qjhufj/claude_code_ollama_404_errors/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-22T02:13:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjccc6</id>
    <title>Nanocoder 1.21.0 ‚Äì Better Config Management and Smarter AI Tool Handling</title>
    <updated>2026-01-21T22:23:52+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qjccc6/nanocoder_1210_better_config_management_and/"&gt; &lt;img alt="Nanocoder 1.21.0 ‚Äì Better Config Management and Smarter AI Tool Handling" src="https://a.thumbs.redditmedia.com/OC4ZEw3T1f7D-RWP09YCz0m3mK9Sr15ITK4XHqdgBe0.jpg" title="Nanocoder 1.21.0 ‚Äì Better Config Management and Smarter AI Tool Handling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/nanocoder/comments/1qjcc6f/nanocoder_1210_better_config_management_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjccc6/nanocoder_1210_better_config_management_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qjccc6/nanocoder_1210_better_config_management_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-21T22:23:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiv7v8</id>
    <title>I built a CLI tool using Ollama (nomic-embed-text) to replace grep with Semantic Code Search</title>
    <updated>2026-01-21T11:16:14+00:00</updated>
    <author>
      <name>/u/Technical_Meeting_81</name>
      <uri>https://old.reddit.com/user/Technical_Meeting_81</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I've been working on an open-source tool called &lt;strong&gt;GrepAI&lt;/strong&gt;, and I wanted to share it here because it relies heavily on &lt;strong&gt;Ollama&lt;/strong&gt; to function.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is it?&lt;/strong&gt; GrepAI is a CLI tool (written in Go) designed to help AI agents (like Claude Code, Cursor, or local agents) understand your codebase better.&lt;/p&gt; &lt;p&gt;Instead of using standard regex &lt;code&gt;grep&lt;/code&gt; to find code‚Äîwhich often misses the context‚ÄîGrepAI uses &lt;strong&gt;Ollama&lt;/strong&gt; to generate local embeddings of your code. This allows you to perform &lt;strong&gt;semantic searches&lt;/strong&gt; directly from the terminal.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Core:&lt;/strong&gt; Written in Go.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Embeddings:&lt;/strong&gt; Connects to your local Ollama instance (defaults to &lt;code&gt;nomic-embed-text&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector Store:&lt;/strong&gt; In-memory / Local (fast and private).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why use Ollama for this?&lt;/strong&gt; I wanted a solution that respects privacy and doesn't cost a fortune in API credits just to index a repo. By using Ollama locally, GrepAI builds an index of your project (respecting &lt;code&gt;.gitignore&lt;/code&gt;) without your code leaving your machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-world Impact (Benchmark)&lt;/strong&gt; I tested this setup by using GrepAI as a filter for Claude Code (instead of the default grep). The idea was to let Ollama decide what files were relevant before sending them to the cloud. The results were huge:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;-97% Input Tokens&lt;/strong&gt; sent to the LLM (because Ollama filtered the noise).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;-27.5% Cost reduction&lt;/strong&gt; on the task.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even if you don't use Claude, this demonstrates how effective local embeddings (via Ollama) are at retrieving the right context for RAG applications.&lt;/p&gt; &lt;p&gt;üëâ &lt;strong&gt;Benchmark details:&lt;/strong&gt;&lt;a href="https://yoanbernabeu.github.io/grepai/blog/benchmark-grepai-vs-grep-claude-code/"&gt;https://yoanbernabeu.github.io/grepai/blog/benchmark-grepai-vs-grep-claude-code/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üì¶ &lt;strong&gt;GitHub:&lt;/strong&gt;&lt;a href="https://github.com/yoanbernabeu/grepai"&gt;https://github.com/yoanbernabeu/grepai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìö &lt;strong&gt;Docs:&lt;/strong&gt;&lt;a href="https://yoanbernabeu.github.io/grepai/"&gt;https://yoanbernabeu.github.io/grepai/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to know what other embedding models you guys are running with Ollama. Currently, &lt;code&gt;nomic-embed-text&lt;/code&gt; gives me the best results for code, but I'm open to suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical_Meeting_81"&gt; /u/Technical_Meeting_81 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qiv7v8/i_built_a_cli_tool_using_ollama_nomicembedtext_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qiv7v8/i_built_a_cli_tool_using_ollama_nomicembedtext_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qiv7v8/i_built_a_cli_tool_using_ollama_nomicembedtext_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-21T11:16:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj02gw</id>
    <title>New Rules for ollama cloud</title>
    <updated>2026-01-21T14:57:44+00:00</updated>
    <author>
      <name>/u/killing_daisy</name>
      <uri>https://old.reddit.com/user/killing_daisy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so i've just seen this:&lt;/p&gt; &lt;p&gt;Pro:&lt;br /&gt; Everything in Free, plus:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run 3 cloud models at a time&lt;/li&gt; &lt;li&gt;Faster responses from cloud hardware&lt;/li&gt; &lt;li&gt;Larger models for challenging tasks&lt;/li&gt; &lt;li&gt;3 private models&lt;/li&gt; &lt;li&gt;3 collaborators per model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;its been a lot slower for usage within zed for me the last hours - does anyone have more information whats happening to the pro subscription? it seems like the changes in the subscription are random and without any notice to users? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/killing_daisy"&gt; /u/killing_daisy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qj02gw/new_rules_for_ollama_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qj02gw/new_rules_for_ollama_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qj02gw/new_rules_for_ollama_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-21T14:57:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj3qkv</id>
    <title>Hi folks, I‚Äôve built an open‚Äësource project that could be useful to some of you</title>
    <updated>2026-01-21T17:10:42+00:00</updated>
    <author>
      <name>/u/panos_s_</name>
      <uri>https://old.reddit.com/user/panos_s_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qj3qkv/hi_folks_ive_built_an_opensource_project_that/"&gt; &lt;img alt="Hi folks, I‚Äôve built an open‚Äësource project that could be useful to some of you" src="https://preview.redd.it/4plhjok3jqeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70730c719d4520817409c69175c8778d9c691100" title="Hi folks, I‚Äôve built an open‚Äësource project that could be useful to some of you" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Web dashboard for NVIDIA GPUs with 30+ real-time metrics (utilisation, memory, temps, clocks, power, processes). Live charts over WebSockets, multi‚ÄëGPU support, and one‚Äëcommand Docker deployment. No agents, minimal setup.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/psalias2006/gpu-hot"&gt;https://github.com/psalias2006/gpu-hot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I built it&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Wanted simple, real‚Äëtime visibility without standing up a full metrics stack.&lt;/li&gt; &lt;li&gt;Needed clear insight into temps, throttling, clocks, and active processes during GPU work.&lt;/li&gt; &lt;li&gt;A lightweight dashboard that‚Äôs easy to run at home or on a workstation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;streams 30+ metrics every ~2s via WebSockets.&lt;/li&gt; &lt;li&gt;Tracks per‚ÄëGPU utilization, memory (used/free/total), temps, power draw/limits, fan, clocks, PCIe, P‚ÄëState, encoder/decoder stats, driver/VBIOS, throttle status.&lt;/li&gt; &lt;li&gt;Shows active GPU processes with PIDs and memory usage.&lt;/li&gt; &lt;li&gt;Clean, responsive UI with live historical charts and basic stats (min/max/avg).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup (Docker)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run -d --name gpu-hot --gpus all -p 1312:1312 ghcr.io/psalias2006/gpu-hot:latest &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panos_s_"&gt; /u/panos_s_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4plhjok3jqeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qj3qkv/hi_folks_ive_built_an_opensource_project_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qj3qkv/hi_folks_ive_built_an_opensource_project_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-21T17:10:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjtqyr</id>
    <title>What do you guys test LLMs in CI/CD?</title>
    <updated>2026-01-22T12:52:52+00:00</updated>
    <author>
      <name>/u/Ok_Constant_9886</name>
      <uri>https://old.reddit.com/user/Ok_Constant_9886</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Constant_9886"&gt; /u/Ok_Constant_9886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AIEval/comments/1qjtq3m/what_do_you_guys_test_llms_in_cicd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjtqyr/what_do_you_guys_test_llms_in_cicd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qjtqyr/what_do_you_guys_test_llms_in_cicd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-22T12:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj3b01</id>
    <title>[Open Sourse] I built a tool that forces 5 AIs to debate and cross-check facts before answering you</title>
    <updated>2026-01-21T16:55:27+00:00</updated>
    <author>
      <name>/u/S_Anv</name>
      <uri>https://old.reddit.com/user/S_Anv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qj3b01/open_sourse_i_built_a_tool_that_forces_5_ais_to/"&gt; &lt;img alt="[Open Sourse] I built a tool that forces 5 AIs to debate and cross-check facts before answering you" src="https://preview.redd.it/ja8et3degqeg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=815f77f8a54e1ef6cf8689c163be58dd45b01704" title="[Open Sourse] I built a tool that forces 5 AIs to debate and cross-check facts before answering you" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I've created a self-hosted platform designed to solve the &amp;quot;blind trust&amp;quot; problem&lt;/p&gt; &lt;p&gt;It works by forcing ChatGPT responses to be verified against other models (such as Gemini, Claude, Mistral, Grok, etc...) in a structured discussion.&lt;/p&gt; &lt;p&gt;I'm looking for users to test this consensus logic and see if it reduces hallucinations&lt;/p&gt; &lt;p&gt;Github + demo animation: &lt;a href="https://github.com/KeaBase/kea-research"&gt;https://github.com/KeaBase/kea-research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. It's provider-agnostic. You can use your own OpenAI keys, connect local models (Ollama), or mix them. Out from the box you can find few system sets of models. More features upcoming&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S_Anv"&gt; /u/S_Anv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ja8et3degqeg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qj3b01/open_sourse_i_built_a_tool_that_forces_5_ais_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qj3b01/open_sourse_i_built_a_tool_that_forces_5_ais_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-21T16:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiug46</id>
    <title>Fine-tuned Qwen3 0.6B for Text2SQL using a claude skill. The result tiny model matches a Deepseek 3.1 and runs locally on CPU.</title>
    <updated>2026-01-21T10:30:24+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qiug46/finetuned_qwen3_06b_for_text2sql_using_a_claude/"&gt; &lt;img alt="Fine-tuned Qwen3 0.6B for Text2SQL using a claude skill. The result tiny model matches a Deepseek 3.1 and runs locally on CPU." src="https://preview.redd.it/fy13421qjoeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5f9926ff9e7f48668957408f4d674a40d2079b2" title="Fine-tuned Qwen3 0.6B for Text2SQL using a claude skill. The result tiny model matches a Deepseek 3.1 and runs locally on CPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing a workflow for training custom models and deploying them to Ollama.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Base small models aren't great at specialized tasks. I needed Text2SQL and Qwen3 0.6B out of the box gave me things like:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sql -- Question: &amp;quot;Which artists have total album sales over 1 million?&amp;quot; SELECT artists.name FROM artists WHERE artists.genre IS NULL OR artists.country IS NULL; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Completely ignores the question. Fine-tuning is the obvious answer, but usually means setting up training infrastructure, formatting datasets, debugging CUDA errors...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The workflow I used:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;distil-cli with a Claude skill that handles the training setup, to get started I installed &lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;Setup&lt;/h1&gt; &lt;p&gt;curl -fsSL &lt;a href="https://cli-assets.distillabs.ai/install.sh"&gt;https://cli-assets.distillabs.ai/install.sh&lt;/a&gt; | sh distil login&lt;/p&gt; &lt;h1&gt;In Claude Code ‚Äî add the skill&lt;/h1&gt; &lt;p&gt;/plugin marketplace add &lt;a href="https://github.com/distil-labs/distil-cli-skill"&gt;https://github.com/distil-labs/distil-cli-skill&lt;/a&gt; /plugin install distil-cli@distil-cli-skill ```&lt;/p&gt; &lt;p&gt;And then, Claude guides me through the training workflow:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash 1. Create a model (`distil model create`) 2. Pick a task type (QA, classification, tool calling, or RAG) 3. Prepare data files (job description, config, train/test sets) 4. Upload data 5. Run teacher evaluation 6. Train the model 7. Download and deploy &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What training produces:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt; downloaded-model/ ‚îú‚îÄ‚îÄ model.gguf (2.2 GB) ‚Äî quantized, Ollama-ready ‚îú‚îÄ‚îÄ Modelfile (system prompt baked in) ‚îú‚îÄ‚îÄ model_client.py (Python wrapper) ‚îú‚îÄ‚îÄ model/ (full HF format) ‚îî‚îÄ‚îÄ model-adapter/ (LoRA weights if you want to merge yourself) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deploying to Ollama:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash ollama create my-text2sql -f Modelfile ollama run my-text2sql &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Custom fine-tuned model, running locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;LLM-as-a-Judge&lt;/th&gt; &lt;th&gt;ROUGE&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Base Qwen3 0.6B&lt;/td&gt; &lt;td&gt;36%&lt;/td&gt; &lt;td&gt;69.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek-V3 (teacher)&lt;/td&gt; &lt;td&gt;80%&lt;/td&gt; &lt;td&gt;88.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Fine-tuned 0.6B&lt;/td&gt; &lt;td&gt;74%&lt;/td&gt; &lt;td&gt;88.5%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Started at 36%, ended at 74% ‚Äî nearly matching the teacher at a fraction of the size.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Before/after:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Question: &amp;quot;How many applicants applied for each position?&amp;quot;&lt;/p&gt; &lt;p&gt;Base: &lt;code&gt;sql SELECT COUNT(DISTINCT position) AS num_applicants FROM applicants; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Fine-tuned: &lt;code&gt;sql SELECT position, COUNT(*) AS applicant_count FROM applicants GROUP BY position; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo app:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Built a quick script that loads CSVs into SQLite and queries via the model:&lt;/p&gt; &lt;p&gt;```bash python app.py --csv employees.csv \ --question &amp;quot;What is the average salary per department?&amp;quot; --show-sql&lt;/p&gt; &lt;h1&gt;Generated SQL: SELECT department, AVG(salary) FROM employees GROUP BY department;&lt;/h1&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;All local.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fy13421qjoeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qiug46/finetuned_qwen3_06b_for_text2sql_using_a_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qiug46/finetuned_qwen3_06b_for_text2sql_using_a_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-21T10:30:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjxwu7</id>
    <title>Trying to analyze personal transactions and use ollama/qwen2.5:7b to provide a report without success</title>
    <updated>2026-01-22T15:44:07+00:00</updated>
    <author>
      <name>/u/hpgm</name>
      <uri>https://old.reddit.com/user/hpgm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This might not be the right place, however I'm hoping that someone might have done this themselves and that I can build on a working model. I'm trying to analyze my last year's spending habits, and have the LLM analyze my back account and credit card transactions. The transactions don't have a category, so that is the LLM part, and then to create an updated csv/xlsx file that I can use to pivot.&lt;/p&gt; &lt;p&gt;The transactions look like the below, date, a descriptor and location, debit, credit, and card number. I tried a number of prompts and haven't been successful, the LLM latches onto one descriptior and then calls everything else that. Because it's my personal finance, I want to keep everything local. I can use a different model, but I only have a 3060TI w/ 16GB VRAM to power it.&lt;/p&gt; &lt;p&gt;Anyone done anything like this?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;2025-12-31,&amp;quot;NATIONAL PARKS, STATE&amp;quot;,,15.75,4********6 2025-12-31,&amp;quot;WENDYS CITY, STATE&amp;quot;,82.11,,4********6 2025-12-30,&amp;quot;WALMART CITY, STATE&amp;quot;,31.60,,4********6 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hpgm"&gt; /u/hpgm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjxwu7/trying_to_analyze_personal_transactions_and_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjxwu7/trying_to_analyze_personal_transactions_and_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qjxwu7/trying_to_analyze_personal_transactions_and_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-22T15:44:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjvhs6</id>
    <title>I can't get qwen2.5-coder:7b working with claude code</title>
    <updated>2026-01-22T14:09:18+00:00</updated>
    <author>
      <name>/u/acidiceyes</name>
      <uri>https://old.reddit.com/user/acidiceyes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qjvhs6/i_cant_get_qwen25coder7b_working_with_claude_code/"&gt; &lt;img alt="I can't get qwen2.5-coder:7b working with claude code" src="https://a.thumbs.redditmedia.com/Rwmz0jhbrtAqKKtq8YVfIQ6QKQIR8WPrTvoJu5FTzU8.jpg" title="I can't get qwen2.5-coder:7b working with claude code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I just read that we can use ollama with claude code now, but I have been trying to get qwen2.5-coder:7b working with claude code, but tool calling just doesn't work.&lt;br /&gt; What am i doing wrong?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mc5u9eoorweg1.png?width=1376&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=403d76d563760d11c890855a3b03e6a62bbc27fd"&gt;https://preview.redd.it/mc5u9eoorweg1.png?width=1376&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=403d76d563760d11c890855a3b03e6a62bbc27fd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/acidiceyes"&gt; /u/acidiceyes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjvhs6/i_cant_get_qwen25coder7b_working_with_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjvhs6/i_cant_get_qwen25coder7b_working_with_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qjvhs6/i_cant_get_qwen25coder7b_working_with_claude_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-22T14:09:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk16na</id>
    <title>Would a p100 be useful?</title>
    <updated>2026-01-22T17:42:35+00:00</updated>
    <author>
      <name>/u/dnielso5</name>
      <uri>https://old.reddit.com/user/dnielso5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, my current setup is a 3060 12g + 1060 6G. I was thinking of getting a p100 (~$90) to replace the 1060. my main focus is running qwen-coder2.5 for some basic coding projects in open webui. &lt;/p&gt; &lt;p&gt;I have some decent success running qwen2.5-coder:14b-instruct-q8_0 but wondering how much it might help with more vram but older card.&lt;/p&gt; &lt;p&gt;and because this is a side project i dont need suggestions to buy a 3090, im looking for around ~$100&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnielso5"&gt; /u/dnielso5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qk16na/would_a_p100_be_useful/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qk16na/would_a_p100_be_useful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qk16na/would_a_p100_be_useful/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-22T17:42:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjqcqv</id>
    <title>Built an open-source, self-hosted AI agent automation platform ‚Äî feedback welcome</title>
    <updated>2026-01-22T09:44:30+00:00</updated>
    <author>
      <name>/u/Feathered-Beast</name>
      <uri>https://old.reddit.com/user/Feathered-Beast</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks üëã&lt;/p&gt; &lt;p&gt;I‚Äôve been building an open-source, self-hosted AI agent automation platform that runs locally and keeps all data under your control. It‚Äôs focused on agent workflows, scheduling, execution logs, and document chat (RAG) without relying on hosted SaaS tools.&lt;/p&gt; &lt;p&gt;I recently put together a small website with docs and a project overview.&lt;/p&gt; &lt;p&gt;Links to the website and GitHub are in the comments.&lt;/p&gt; &lt;p&gt;Would really appreciate feedback from people building or experimenting with open-source AI systems üôå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feathered-Beast"&gt; /u/Feathered-Beast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjqcqv/built_an_opensource_selfhosted_ai_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjqcqv/built_an_opensource_selfhosted_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qjqcqv/built_an_opensource_selfhosted_ai_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-22T09:44:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkg2ro</id>
    <title>Can be run gemma3:1b in My server</title>
    <updated>2026-01-23T03:40:57+00:00</updated>
    <author>
      <name>/u/rachid_nichan</name>
      <uri>https://old.reddit.com/user/rachid_nichan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have server: 2 vCPU cores 8 GB RAM&lt;/p&gt; &lt;p&gt;Can be run gemma3:1b ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rachid_nichan"&gt; /u/rachid_nichan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qkg2ro/can_be_run_gemma31b_in_my_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qkg2ro/can_be_run_gemma31b_in_my_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qkg2ro/can_be_run_gemma31b_in_my_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-23T03:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjtnqm</id>
    <title>How to implement a RAG (Retrieval Augmented Generation) on your laptop</title>
    <updated>2026-01-22T12:48:36+00:00</updated>
    <author>
      <name>/u/Unique_Winner_5927</name>
      <uri>https://old.reddit.com/user/Unique_Winner_5927</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qjtnqm/how_to_implement_a_rag_retrieval_augmented/"&gt; &lt;img alt="How to implement a RAG (Retrieval Augmented Generation) on your laptop" src="https://b.thumbs.redditmedia.com/Ak9Ybl34eS3k6YJHJqbnpAOgTmOIwBEmPTZCS8SWXsw.jpg" title="How to implement a RAG (Retrieval Augmented Generation) on your laptop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This guide explains how to implement a RAG (Retrieval Augmented Generation) on your laptop.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ftsddeqtcweg1.png?width=2184&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=640e3013e9113c3c7780a88b39d6992cd34b8d6f"&gt;https://preview.redd.it/ftsddeqtcweg1.png?width=2184&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=640e3013e9113c3c7780a88b39d6992cd34b8d6f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With n8n, Ollama and Qdrant (with Docker).&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ThomasPlantain/n8n"&gt;https://github.com/ThomasPlantain/n8n&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I put a lot of screenshots to explain how to configure each component.&lt;/p&gt; &lt;p&gt;#Ollama #n8n #Qdrant #dataSovereignty #embeddedAI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unique_Winner_5927"&gt; /u/Unique_Winner_5927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjtnqm/how_to_implement_a_rag_retrieval_augmented/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjtnqm/how_to_implement_a_rag_retrieval_augmented/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qjtnqm/how_to_implement_a_rag_retrieval_augmented/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-22T12:48:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkcg6a</id>
    <title>Boom, the greatest repo yet from Lucas Valbuena ( x1xhlol)</title>
    <updated>2026-01-23T00:57:26+00:00</updated>
    <author>
      <name>/u/OriginalZebraPoo</name>
      <uri>https://old.reddit.com/user/OriginalZebraPoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qkcg6a/boom_the_greatest_repo_yet_from_lucas_valbuena/"&gt; &lt;img alt="Boom, the greatest repo yet from Lucas Valbuena ( x1xhlol)" src="https://preview.redd.it/dspr44juxzeg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42f0ce90b21eba01096601e2cea14bf419aa729d" title="Boom, the greatest repo yet from Lucas Valbuena ( x1xhlol)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OriginalZebraPoo"&gt; /u/OriginalZebraPoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dspr44juxzeg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qkcg6a/boom_the_greatest_repo_yet_from_lucas_valbuena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qkcg6a/boom_the_greatest_repo_yet_from_lucas_valbuena/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-23T00:57:26+00:00</published>
  </entry>
</feed>
