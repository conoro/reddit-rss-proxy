<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-23T16:51:01+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1nn5ur4</id>
    <title>æ¨¡å‹èˆ‡é¡¯ç¤ºå¡ å…¬å¼ åˆ¤æ–· èˆ‡ç”œèœœå€¼</title>
    <updated>2025-09-21T22:37:55+00:00</updated>
    <author>
      <name>/u/kuerys</name>
      <uri>https://old.reddit.com/user/kuerys</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nn5ur4/æ¨¡å‹èˆ‡é¡¯ç¤ºå¡_å…¬å¼_åˆ¤æ–·_èˆ‡ç”œèœœå€¼/"&gt; &lt;img alt="æ¨¡å‹èˆ‡é¡¯ç¤ºå¡ å…¬å¼ åˆ¤æ–· èˆ‡ç”œèœœå€¼" src="https://b.thumbs.redditmedia.com/EXdFBgOu1jvD-3bKlVJ5h5fI220lx4DUTILy8SpwpSk.jpg" title="æ¨¡å‹èˆ‡é¡¯ç¤ºå¡ å…¬å¼ åˆ¤æ–· èˆ‡ç”œèœœå€¼" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;å–®å¡&lt;/p&gt; &lt;p&gt;Bå€¼ Ã— 1.3 = VRAMï¼ˆå¯åŸ·è¡Œå€¼ï¼‰â‰’ (Tâ‰’4096) = ç”œå¯†å€¼&lt;/p&gt; &lt;h1&gt;å–®å¡å…¬å¼ï¼š&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;B Ã— 1.3 â‰’ VRAMï¼ˆå¯åŸ·è¡Œå€¼ï¼‰ â‰’ T â‰’ 4096 = ç”œå¯†å€¼&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;B&lt;/strong&gt;ï¼šæ¨¡å‹åƒæ•¸é‡ï¼ˆBillionï¼‰&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1.3&lt;/strong&gt;ï¼šæ¨ç†æ™‚çš„è¿‘ä¼¼å€å¢ä¿‚æ•¸ï¼ˆä»£è¡¨æ¨¡å‹è¼‰å…¥ + buffer + contextï¼‰&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM&lt;/strong&gt;ï¼šé¡¯ç¤ºè¨˜æ†¶é«”éœ€æ±‚ï¼ˆGBï¼‰&lt;/li&gt; &lt;li&gt;&lt;strong&gt;T&lt;/strong&gt;ï¼šä¸Šä¸‹æ–‡é•·åº¦ï¼ˆtokensï¼‰ï¼Œ4096 ç‚ºå¸¸è¦‹ sweet spot&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ç”œå¯†å€¼&lt;/strong&gt;ï¼šä»£è¡¨ã€Œæ•ˆèƒ½èˆ‡ç©©å®šæ€§å…¼å…·ã€çš„ç†æƒ³æ¨ç†é…ç½®&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;å¤šå¡&lt;/p&gt; &lt;p&gt;Bå€¼ Ã— (VRAMx4096)â‰’ (Tâ‰’1.3) = ç”œå¯†å€¼&lt;/p&gt; &lt;p&gt;å¤šå¡å…¬å¼ï¼š&lt;/p&gt; &lt;p&gt;&lt;strong&gt;B Ã— (VRAM Ã— 4096) â‰’ T â‰’ 1.3 = ç”œå¯†å€¼&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;é€™è£¡ä½ æŠŠ VRAM å’Œä¸Šä¸‹æ–‡é•·åº¦è¦–ç‚ºä¹˜ç©ï¼Œä»£è¡¨å¤šå¡åˆ†æµæ™‚çš„ &lt;strong&gt;ç¸½æ¨ç†å®¹é‡&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;T â‰’ 1.3&lt;/strong&gt;ï¼šé€™è£¡åéä¾†è¡¨ç¤ºã€Œæ¯ GB VRAM å°æ‡‰çš„ token è™•ç†èƒ½åŠ›ã€ç´„ç‚º 1.3B æ¨¡å‹çš„è² è¼‰&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;çˆ†æ©Ÿ&lt;/p&gt; &lt;p&gt;B Ã— (VRAM Ã— 4096) â‰’ T â‰’ 1.3 = G Ã— ç”œå¯†å€¼&lt;br /&gt; çˆ†æ©Ÿå…¬å¼ï¼š&lt;/p&gt; &lt;p&gt;&lt;strong&gt;B Ã— (VRAM Ã— 4096) â‰’ T â‰’ 1.3 = G Ã— ç”œå¯†å€¼&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;G&lt;/strong&gt;ï¼šGPU æ•¸é‡ï¼ˆæˆ– GPU ç­‰ç´šä¿‚æ•¸ï¼‰&lt;/li&gt; &lt;li&gt;é€™æ˜¯ä½ å°ã€Œçˆ†æ©Ÿã€çš„å®šç¾©ï¼šç•¶æ¨¡å‹åƒæ•¸é‡ Ã— æ¨ç†è² è¼‰è¶…é GPU çš„è™•ç†èƒ½åŠ›æ™‚ï¼Œå°±æœƒçˆ†æ©Ÿ&lt;/li&gt; &lt;li&gt;é€™å…¬å¼ä¹Ÿå¯ä»¥åæ¨ï¼š&lt;strong&gt;G Ã— ç”œå¯†å€¼&lt;/strong&gt; æ˜¯ä½ èƒ½æ‰¿å—çš„æœ€å¤§æ¨ç†è² &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k6t9usnzllqf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08a077afed8b3e0ba1b1ce914780d27f08cd5fcd"&gt;https://preview.redd.it/k6t9usnzllqf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08a077afed8b3e0ba1b1ce914780d27f08cd5fcd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;é›™æ©Ÿ å–®å¡ é›™å¡ æ¸¬è©¦ç’°å¢ƒ é©—è­‰ èˆ‡çˆ†æ©Ÿæ¸¬è©¦&lt;/p&gt; &lt;p&gt;ç­†é›»:&lt;/p&gt; &lt;p&gt;Thin GF63 12UCX-289TW&lt;/p&gt; &lt;p&gt;CPU:12th Gen Intel(R) Core(TM) i7-12650H (2.30 GHz)&lt;/p&gt; &lt;p&gt;GPU:Intel(R) UHD Graphics/NVIDIA GeForce RTX 3050 Laptop GPU (4G)&lt;/p&gt; &lt;p&gt;RAM:16G X2 (32G)&lt;/p&gt; &lt;p&gt;SSD:Phison 512G ESO512GHLCA1-21C-2MS(C:)/ADATA SU800(D:)&lt;br /&gt; (å‰å°æ©Ÿ)&lt;/p&gt; &lt;p&gt;Xeon&lt;/p&gt; &lt;p&gt;CPU:Intel(R) Xeon(R) CPU E5-2696 v4 @ 2.20GHz 2.20 GHz (2 å€‹è™•ç†å™¨)&lt;/p&gt; &lt;p&gt;RAM:16G X8 (128G)&lt;/p&gt; &lt;p&gt;SSD:ä¸‰æ˜Ÿ 512G M.2 NVMe (C:\) åéŠ“ VULCAN Z 1TB SSD/2.5å‹ è®€å¯«550/500 (D:\)&lt;/p&gt; &lt;p&gt;GPU:RTX 3060 GD6 12G ç³»çµ±é¡¯ PCI-E 1 (X16)&lt;/p&gt; &lt;p&gt;GPU:RTX 2080TI é­”æ”¹22GB *2 NVLink PCI-E 3/5(X16) (NvLink)&lt;/p&gt; &lt;p&gt;----------------------------------------------&lt;br /&gt; C:\Users\Admin&amp;gt;ollama run gpt-oss:120b&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; æ—©å®‰ä½ å¥½&lt;/p&gt; &lt;p&gt;Thinking...&lt;/p&gt; &lt;p&gt;The user says in Chinese: &amp;quot;æ—©å®‰ä½ å¥½&amp;quot; which means &amp;quot;Good morning, hello&amp;quot;. Probably they want a greeting. We can&lt;/p&gt; &lt;p&gt;respond in Chinese, ask how can help. Maybe ask if they need anything. So reply: &amp;quot;æ—©å®‰ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ&amp;quot;&lt;/p&gt; &lt;p&gt;Also can be friendly.&lt;/p&gt; &lt;p&gt;...done thinking.&lt;/p&gt; &lt;p&gt;æ—©å®‰ï¼ğŸ˜Š&lt;/p&gt; &lt;p&gt;æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿå¦‚æœæœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦èŠå¤©ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼ç¥æ‚¨ä»Šå¤©æœ‰ä¸ªç¾å¥½çš„ä¸€å¤©ã€‚&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; é€™æ˜¯OLLAMA ä¸‹çš„æ¨¡å‹æ¨æ›ç®—å…¬å¼ å–®å¡&lt;/p&gt; &lt;p&gt;... Bå€¼ 1.3 = VRAMï¼ˆå¯åŸ·è¡Œå€¼ï¼‰ (T4096) = ç”œå¯†å€¼&lt;/p&gt; &lt;p&gt;...&lt;/p&gt; &lt;p&gt;... å¤šå¡&lt;/p&gt; &lt;p&gt;... Bå€¼ (VRAMx4096) (T1.3) = ç”œå¯†å€¼&lt;/p&gt; &lt;p&gt;...&lt;/p&gt; &lt;p&gt;... çˆ†æ©Ÿ&lt;/p&gt; &lt;p&gt;... B (VRAM 4096) T 1.3 = G ç”œå¯†å€¼&lt;/p&gt; &lt;p&gt;Thinking...&lt;/p&gt; &lt;p&gt;The user wrote in Chinese: &amp;quot;é€™æ˜¯OLLAMA ä¸‹çš„æ¨¡å‹æ¨æ›ç®—å…¬å¼ å–®å¡ Bå€¼ 1.3 = VRAMï¼ˆå¯åŸ·è¡Œå€¼ï¼‰ (T4096) = ç”œå¯†å€¼ å¤šå¡ Bå€¼&lt;/p&gt; &lt;p&gt;(VRAMx4096) (T1.3) = ç”œå¯†å€¼ çˆ†æ©Ÿ B (VRAM 4096) T 1.3 = G ç”œå¯†å€¼&amp;quot;&lt;/p&gt; &lt;p&gt;They seem to be describing a formula for OLLAMA model inference conversion? The text is confusing. They probably&lt;/p&gt; &lt;p&gt;want explanation or help to interpret the formula. Need to ask clarifying question or provide explanation.&lt;/p&gt; &lt;p&gt;Probably they want to understand the formula, maybe compute values? Provide guidance on how to calculate &amp;quot;ç”œå¯†å€¼&amp;quot;?&lt;/p&gt; &lt;p&gt;Could be translation: &amp;quot;sweetness value&amp;quot;? Actually &amp;quot;ç”œå¯†å€¼&amp;quot; maybe &amp;quot;sweetness density&amp;quot;? Could be a typo for &amp;quot;ç”œå¯†å€¼&lt;/p&gt; &lt;p&gt;&amp;quot;? Not sure.&lt;/p&gt; &lt;p&gt;We should respond in Chinese, ask for clarification, and try to interpret: Single GPU: B = 1.3 = VRAM (executable&lt;/p&gt; &lt;p&gt;value) (T4096) = sweetness value. Multi-GPU: B = (VRAM x 4096) (T1.3) = sweetness value. Overclock? &amp;quot;çˆ†æ©Ÿ&amp;quot; maybe&lt;/p&gt; &lt;p&gt;overclocked: B (VRAM 4096) T 1.3 = G sweetness value.&lt;/p&gt; &lt;p&gt;We can explain that B is some scaling factor, VRAM is memory, T is token length? Provide formula: sweetness = B *&lt;/p&gt; &lt;p&gt;VRAM? Might need to derive.&lt;/p&gt; &lt;p&gt;Let's respond: ask what they want: calculate for specific GPU? Provide example: for 24GB VRAM single card, B =&lt;/p&gt; &lt;p&gt;1.3, T = 4096 =&amp;gt; sweetness = 1.3 * 24? Or something.&lt;/p&gt; &lt;p&gt;We can propose a clearer formula: Sweetness = B * VRAM * T? Or Sweetness = VRAM^(B) * T? Not clear.&lt;/p&gt; &lt;p&gt;We can ask: &amp;quot;æ‚¨æƒ³è¦æŠŠé€™å€‹å…¬å¼å¥—ç”¨åœ¨ä»€éº¼æƒ…æ³ï¼Ÿæƒ³ç®—å‡ºæŸå¼µå¡çš„ç”œå¯†å€¼å—ï¼Ÿè«‹æä¾› VRAM å®¹é‡èˆ‡å¡æ•¸ï¼Œæˆ‘å¯ä»¥å¹«æ‚¨ç®—ã€‚&amp;quot;&lt;/p&gt; &lt;p&gt;Provide explanation.&lt;/p&gt; &lt;p&gt;Will answer in Chinese, friendly, ask for more details.&lt;/p&gt; &lt;p&gt;...done thinking.&lt;/p&gt; &lt;p&gt;ä½ å¥½ï¼çœ‹èµ·ä¾†ä½ åœ¨æ•´ç† OLLAMAï¼ˆæˆ–é¡ä¼¼çš„æœ¬åœ° LLM æ¨ç†ï¼‰æ™‚ï¼Œé—œæ–¼ **ã€Œç”œå¯†å€¼ã€**ï¼ˆæˆ‘æš«æ™‚æŠŠå®ƒç•¶ä½œä¸€å€‹è¡¡é‡æ¨¡å‹å¯åŸ·è¡Œæ•ˆç‡æˆ–&lt;/p&gt; &lt;p&gt;æ•ˆèƒ½çš„æŒ‡æ¨™ï¼‰çš„è¨ˆç®—å…¬å¼ã€‚&lt;/p&gt; &lt;p&gt;ä¸‹é¢å…ˆæŠŠä½ æä¾›çš„ä¸‰ç¨®æƒ…æ³åšå€‹æ•´ç†ï¼Œç„¶å¾Œå†èªªæ˜æ¯å€‹åƒæ•¸å¯èƒ½ä»£è¡¨çš„æ„ç¾©ï¼Œæœ€å¾Œçµ¦å‡ºä¸€å€‹ã€Œå¥—ç”¨ã€çš„ç¯„ä¾‹ï¼Œè®“ä½ å¯ä»¥ç›´æ¥æŠŠè‡ªå·±çš„&lt;/p&gt; &lt;p&gt;é¡¯å¡è¦æ ¼å¸¶é€²å»ç®—ã€‚&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 1ï¸âƒ£ å…¬å¼ç¸½è¦½&lt;/p&gt; &lt;p&gt;| æƒ…æ³ | å…¬å¼ï¼ˆåŸæ–‡ï¼‰ | å¯èƒ½çš„æ•¸å­¸å¯«æ³• | åƒæ•¸èªªæ˜ |&lt;/p&gt; &lt;p&gt;|------|--------------|----------------|----------|&lt;/p&gt; &lt;p&gt;| **å–®å¡** | `Bå€¼ 1.3 = VRAMï¼ˆå¯åŸ·è¡Œå€¼ï¼‰ (T4096) = ç”œå¯†å€¼` | `ç”œå¯†å€¼ = 1.3 Ã— VRAM Ã— 4096` | - **B**ï¼šå›ºå®šä¿‚æ•¸&lt;/p&gt; &lt;p&gt;1.3&amp;lt;br&amp;gt;- **VRAM**ï¼šå¡ä¸Šå¯ç”¨çš„é¡¯å­˜ï¼ˆGBï¼‰&amp;lt;br&amp;gt;- **T**ï¼šæ¨¡å‹ä¸Šä¸‹æ–‡é•·åº¦ï¼ˆtoken æ•¸ï¼‰ï¼Œé€™è£¡å›ºå®š 4096 |&lt;/p&gt; &lt;p&gt;| **å¤šå¡** | `Bå€¼ (VRAMx4096) (T1.3) = ç”œå¯†å€¼` | `ç”œå¯†å€¼ = (VRAM Ã— 4096) Ã— 1.3`ï¼ˆæˆ–ç­‰åƒ¹çš„ `ç”œå¯†å€¼ = VRAM Ã— 4096&lt;/p&gt; &lt;p&gt;Ã— 1.3`ï¼‰ | - **VRAM**ï¼šå–®å¼µå¡çš„é¡¯å­˜ï¼ˆGBï¼‰&amp;lt;br&amp;gt;- **4096**ï¼šä»ç„¶æ˜¯ token é•·åº¦&amp;lt;br&amp;gt;- **1.3**ï¼šæ”¹æˆäº† **T**ï¼ˆä¹Ÿå°±æ˜¯ä¿‚æ•¸ï¼‰&lt;/p&gt; &lt;p&gt;|&lt;/p&gt; &lt;p&gt;| **çˆ†æ©Ÿ**ï¼ˆæ¥µé™/è¶…é »ï¼‰| `B (VRAM 4096) T 1.3 = G ç”œå¯†å€¼` | `ç”œå¯†å€¼ = VRAM Ã— 4096 Ã— 1.3`ï¼ˆå†ä¹˜ä¸Šä¸€å€‹ **G**ï¼Œä»£è¡¨&lt;/p&gt; &lt;p&gt;â€œè¶…é¡â€ æˆ– â€œå¢ç›Šâ€ï¼‰| - **G**ï¼šé€šå¸¸æ˜¯é¡å¤–çš„å¢ç›Šå› å­ï¼ˆä¾‹å¦‚ 1.2ã€1.5ï¼‰ï¼Œç”¨ä¾†è¡¨ç¤ºè¶…é »ã€PCIeâ€‘Gen4 å¸¶å¯¬æå‡ç­‰ |&lt;/p&gt; &lt;p&gt;&amp;gt; **è¨»**ï¼šä¸Šè¿°çš„ã€ŒÃ—ã€ç¬¦è™Ÿæ˜¯æ ¹æ“šæ–‡å­—çš„æ’åˆ—æ¨æ¸¬çš„ä¹˜æ³•é—œä¿‚ã€‚å¦‚æœå¯¦éš›ä¸Šæ˜¯é™¤æ³•æˆ–æŒ‡æ•¸é‹ç®—ï¼Œè«‹å†å‘Šè¨´æˆ‘ï¼Œæˆ‘æœƒç«‹å³èª¿æ•´ã€‚&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 2ï¸âƒ£ åƒæ•¸èƒŒå¾Œçš„æ„ç¾©ï¼ˆæˆ‘çš„ç†è§£ï¼‰&lt;/p&gt; &lt;p&gt;| åƒæ•¸ | å¸¸è¦‹å«ç¾© | ç‚ºä»€éº¼æœƒå‡ºç¾åœ¨å…¬å¼è£¡ |&lt;/p&gt; &lt;p&gt;|------|----------|----------------------|&lt;/p&gt; &lt;p&gt;| **Bï¼ˆæˆ– Tï¼‰** | **ä¿‚æ•¸**ï¼Œç”¨ä¾†æŠŠã€Œé¡¯å­˜ã€çš„å®¹é‡è½‰æ›æˆã€Œå¯åŸ·è¡Œå€¼ã€çš„å°ºåº¦ã€‚&amp;lt;br&amp;gt;åœ¨å–®å¡æƒ…æ³ä¸‹å®ƒæ˜¯ **1.3**ï¼Œå¤šå¡æ™‚è®Šæˆ&lt;/p&gt; &lt;p&gt;**1.3** ä½œç‚º **T**ï¼Œç›¸ç•¶æ–¼æŠŠ 1.3 ç•¶ä½œã€Œæ¯ 1GB VRAM å¯æ”¯æ´çš„ token æ•¸ã€çš„å€ç‡ã€‚ |&lt;/p&gt; &lt;p&gt;| **VRAM** | é¡¯å¡å¯ç”¨é¡¯å­˜ï¼ˆGBï¼‰ï¼Œç›´æ¥æ±ºå®šæ¨¡å‹èƒ½è¼‰å…¥çš„å¤§å°èˆ‡ä¸€æ¬¡æ€§æ¨ç†çš„ token æ•¸é‡ã€‚ |&lt;/p&gt; &lt;p&gt;| **4096** | å¸¸è¦‹çš„ **ä¸Šä¸‹æ–‡é•·åº¦**ï¼ˆtoken ä¸Šé™ï¼‰ï¼Œå¾ˆå¤š LLMï¼ˆå¦‚ LLaMAã€Mistralï¼‰é è¨­çš„æœ€å¤§ä¸Šä¸‹æ–‡é•·åº¦å°±æ˜¯ 4096ã€‚ |&lt;/p&gt; &lt;p&gt;| **G** | **å¢ç›Šå› å­**ï¼Œåœ¨ã€Œçˆ†æ©Ÿã€æ¨¡å¼ä¸‹ï¼Œå¯èƒ½æ˜¯è¶…é »ã€PCIeâ€‘Gen4 å¸¶å¯¬æå‡ã€æˆ–æ˜¯ä½¿ç”¨é«˜æ•ˆèƒ½çš„ NVMe äº¤æ›ç©ºé–“ç­‰å¸¶ä¾†çš„é¡å¤–&lt;/p&gt; &lt;p&gt;æ•ˆèƒ½æå‡ã€‚ |&lt;/p&gt; &lt;p&gt;ç°¡å–®èªªï¼Œ**ç”œå¯†å€¼** â‰ˆ `VRAM Ã— 4096 Ã— ä¿‚æ•¸`ï¼Œä¿‚æ•¸æ ¹æ“šå–®å¡/å¤šå¡/è¶…é »è€Œä¸åŒã€‚&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 3ï¸âƒ£ å¯¦éš›è¨ˆç®—ç¯„ä¾‹&lt;/p&gt; &lt;p&gt;å‡è¨­ä½ æœ‰ä»¥ä¸‹å¹¾ç¨®ç¡¬é«”é…ç½®ï¼š&lt;/p&gt; &lt;p&gt;| å¡å‹è™Ÿ | VRAM (GB) | å¡æ•¸ | æ˜¯å¦è¶…é » | ä¿‚æ•¸ï¼ˆB/Tï¼‰ | å¢ç›Š G |&lt;/p&gt; &lt;p&gt;|--------|-----------|------|----------|-------------|--------|&lt;/p&gt; &lt;p&gt;| RTX 4090 | 24 | 1 | å¦ | 1.3 | 1.0 |&lt;/p&gt; &lt;p&gt;| RTX 4090 | 24 | 2 | å¦ | 1.3 | 1.0 |&lt;/p&gt; &lt;p&gt;| RTX 4090 | 24 | 1 | æ˜¯ï¼ˆçˆ†æ©Ÿï¼‰ | 1.3 | 1.2 |&lt;/p&gt; &lt;p&gt;### 3.1 å–®å¡ï¼ˆä¸è¶…é »ï¼‰&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;ç”œå¯†å€¼ = 1.3 Ã— 24 Ã— 4096&lt;/p&gt; &lt;p&gt;= 1.3 Ã— 98,304&lt;/p&gt; &lt;p&gt;= 127,795.2 (ç´„ 1.28e5)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;### 3.2 å¤šå¡ï¼ˆ2 å¼µï¼‰&lt;/p&gt; &lt;p&gt;æ ¹æ“šã€Œå¤šå¡ã€å…¬å¼ï¼Œæˆ‘ä»ç„¶æŠŠ **VRAM** è¦–ç‚º **å–®å¡** çš„å®¹é‡ï¼ˆå› ç‚ºå…¬å¼è£¡çš„ `VRAMx4096` å·²ç¶“éš±å«äº†ã€Œæ¯å¼µå¡ã€çš„ä¹˜ç®—ï¼‰ï¼š&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;ç”œå¯†å€¼ = (24 Ã— 4096) Ã— 1.3&lt;/p&gt; &lt;p&gt;= 98,304 Ã— 1.3&lt;/p&gt; &lt;p&gt;= 127,795.2 ï¼ˆå’Œå–®å¡ç›¸åŒï¼Œé™¤éä½ æŠŠå¡æ•¸ä¹˜é€²å»ï¼‰&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&amp;gt; **æ³¨æ„**ï¼šå¦‚æœä½ å¸Œæœ›æŠŠ *å¡æ•¸* ç›´æ¥ç®—é€²å»ï¼ˆå³ã€Œç¸½ VRAMã€ï¼‰ï¼Œåªè¦æŠŠ `VRAM` æ›æˆ **ç¸½ VRAM** å³å¯ã€‚&lt;/p&gt; &lt;p&gt;&amp;gt; ä¾‹å¦‚ï¼š`ç¸½VRAM = 24 Ã— 2 = 48` â†’ `ç”œå¯†å€¼ = 1.3 Ã— 48 Ã— 4096 = 255,590.4`ã€‚&lt;/p&gt; &lt;p&gt;### 3.3 çˆ†æ©Ÿï¼ˆå–®å¡ã€è¶…é »ï¼‰&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;ç”œå¯†å€¼ = VRAM Ã— 4096 Ã— 1.3 Ã— G&lt;/p&gt; &lt;p&gt;= 24 Ã— 4096 Ã— 1.3 Ã— 1.2&lt;/p&gt; &lt;p&gt;= 98,304 Ã— 1.3 Ã— 1.2&lt;/p&gt; &lt;p&gt;= 127,795.2 Ã— 1.2&lt;/p&gt; &lt;p&gt;= 153,354.24&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 4ï¸âƒ£ å¦‚ä½•è‡ªè¡Œå¥—ç”¨&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;**ç¢ºå®šä¸‰å€‹åŸºç¤è®Šæ•¸**&lt;/p&gt; &lt;p&gt;- `VRAM`ï¼šå–®å¼µå¡çš„é¡¯å­˜ï¼ˆGBï¼‰&lt;/p&gt; &lt;p&gt;- `T`ï¼ˆæˆ– `4096`ï¼‰ï¼šä½ æ‰“ç®—ä½¿ç”¨çš„ä¸Šä¸‹æ–‡é•·åº¦ï¼ˆå¦‚æœä¸æ˜¯ 4096ï¼Œç›´æ¥æ›¿æ›ï¼‰&lt;/p&gt; &lt;p&gt;- `ä¿‚æ•¸ B/T`ï¼šæ ¹æ“šç¡¬é«”ç‹€æ…‹ï¼ˆå–®å¡ 1.3ã€å¤šå¡ 1.3ã€çˆ†æ©Ÿ 1.3ï¼‰ä»¥åŠæ˜¯å¦æœ‰é¡å¤–å¢ç›Š `G`&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**å¥—ç”¨å…¬å¼**&lt;/p&gt; &lt;p&gt;- **å–®å¡**ï¼š`ç”œå¯†å€¼ = B Ã— VRAM Ã— T`&lt;/p&gt; &lt;p&gt;- **å¤šå¡**ï¼ˆè‹¥æƒ³æŠŠå¡æ•¸ç´å…¥ï¼‰ï¼šã€Œç¸½ VRAMã€= `VRAM Ã— å¡æ•¸` â†’ `ç”œå¯†å€¼ = B Ã— ç¸½ VRAM Ã— T`&lt;/p&gt; &lt;p&gt;- **çˆ†æ©Ÿ**ï¼šåœ¨å–®å¡å…¬å¼æœ€å¾Œå†ä¹˜ä¸Š `G`ï¼ˆ&amp;gt;1ï¼‰&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**ä¾‹å­**ï¼ˆä½ å¯ä»¥ç›´æ¥åœ¨ Excel/Google Sheet æˆ– Python ä¸­ç®—ï¼‰&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;```python&lt;/p&gt; &lt;p&gt;def sweet_score(vram_gb, token_len=4096, coeff=1.3, cards=1, gain=1.0):&lt;/p&gt; &lt;p&gt;total_vram = vram_gb * cards&lt;/p&gt; &lt;p&gt;return coeff * total_vram * token_len * gain&lt;/p&gt; &lt;p&gt;# å–®å¡ RTX4090&lt;/p&gt; &lt;p&gt;print(sweet_score(24)) # 127795.2&lt;/p&gt; &lt;p&gt;# å…©å¡ RTX4090&lt;/p&gt; &lt;p&gt;print(sweet_score(24, cards=2)) # 255590.4&lt;/p&gt; &lt;p&gt;# çˆ†æ©Ÿå–®å¡ (gain=1.2)&lt;/p&gt; &lt;p&gt;print(sweet_score(24, gain=1.2)) # 153354.24&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## 5ï¸âƒ£ éœ€è¦é€²ä¸€æ­¥çš„è³‡è¨Šï¼Ÿ&lt;/p&gt; &lt;p&gt;- **ã€Œç”œå¯†å€¼ã€çš„å¯¦éš›æ„ç¾©**ï¼šå®ƒæ˜¯ç”¨ä¾†é ä¼°ä¸€æ¬¡æ¨ç†èƒ½å®¹ç´å¤šå°‘ tokenã€æˆ–æ˜¯è¡¡é‡æ¨¡å‹è¼‰å…¥çš„ã€Œå®‰å…¨ä¸Šé™ã€å—ï¼Ÿå¦‚æœæœ‰å…·é«”çš„é˜ˆå€¼&lt;/p&gt; &lt;p&gt;ï¼ˆä¾‹å¦‚ç”œå¯†å€¼ &amp;gt; 200k è¡¨ç¤ºå¯ä»¥è·‘ 70B åƒæ•¸æ¨¡å‹ï¼‰ï¼Œè«‹å‘Šè¨´æˆ‘ï¼Œæˆ‘å¯ä»¥å¹«ä½ æŠŠçµæœæ˜ å°„æˆæ¨¡å‹å°ºå¯¸å»ºè­°ã€‚&lt;/p&gt; &lt;p&gt;- **æ˜¯å¦æœ‰å…¶ä»–è®Šæ•¸**ï¼šåƒæ˜¯ **PCIe å¸¶å¯¬**ã€**CPU è¨˜æ†¶é«”**ã€**ç£ç¢Ÿäº¤æ›ç©ºé–“** ç­‰ï¼Œé€™äº›åœ¨ã€Œçˆ†æ©Ÿã€æƒ…æ³ä¸‹ä¹Ÿæœƒå½±éŸ¿æœ€çµ‚æ•ˆèƒ½&lt;/p&gt; &lt;p&gt;ã€‚&lt;/p&gt; &lt;p&gt;- **ä½ æƒ³è¦çš„è¼¸å‡ºæ ¼å¼**ï¼šä¾‹å¦‚ä¸€å€‹ç°¡æ½”çš„ã€Œå¡ç‰‡ã€å¼å ±å‘Šã€æˆ–æ˜¯ CSV/JSON è¡¨æ ¼ï¼Œæ–¹ä¾¿ä½ ç›´æ¥åŒ¯å…¥åˆ°è‡ªå·±çš„æ¸¬è©¦è…³æœ¬è£¡ã€‚&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;### å°çµ&lt;/p&gt; &lt;p&gt;- å…¬å¼æœ¬è³ªä¸Šæ˜¯ `ç”œå¯†å€¼ â‰ˆ ä¿‚æ•¸ Ã— VRAM Ã— token_len Ã— (å¡æ•¸æˆ–å¢ç›Š)`ã€‚&lt;/p&gt; &lt;p&gt;- åªè¦æŠŠä½ çš„ç¡¬é«”è¦æ ¼ä»£å…¥ï¼Œå°±èƒ½å¿«é€Ÿå¾—åˆ°ä¸€å€‹ç›¸å°ã€Œå¯åŸ·è¡Œã€çš„æŒ‡æ¨™ã€‚&lt;/p&gt; &lt;p&gt;- è‹¥ä½ æœ‰æ›´è©³ç´°çš„éœ€æ±‚ï¼ˆä¾‹å¦‚ä¸åŒæ¨¡å‹çš„åƒæ•¸é‡ã€ä¸åŒ token é•·åº¦ã€æˆ–æ˜¯æƒ³æŠŠ CPU/ç£ç¢Ÿå› ç´ ä¸€èµ·ç´å…¥ï¼‰ï¼Œéš¨æ™‚å‘Šè¨´æˆ‘ï¼Œæˆ‘å¯ä»¥å¹«ä½ &lt;/p&gt; &lt;p&gt;æ“´å±•å…¬å¼æˆ–å¯«ä¸€å€‹å°å·¥å…·ã€‚&lt;/p&gt; &lt;p&gt;å¸Œæœ›ä»¥ä¸Šèªªæ˜å°ä½ æœ‰å¹«åŠ©ï¼å¦‚æœé‚„æœ‰å…¶ä»–ç–‘å•æˆ–æƒ³è¦é€²ä¸€æ­¥ç®—å‡ºå…·é«”çš„æ•¸å€¼ï¼Œè«‹ç›´æ¥æä¾›ä½ çš„é¡¯å¡è¦æ ¼ï¼ˆVRAMã€å¡æ•¸ã€æ˜¯å¦è¶…é »ï¼‰ä»¥&lt;/p&gt; &lt;p&gt;åŠæƒ³è¦çš„ `token_len`ï¼Œæˆ‘æœƒç«‹åˆ»çµ¦ä½ çµæœã€‚ç¥ä½ ç©å¾—é–‹å¿ƒ ğŸš€ï¼&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; Send a message (/? for help)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kuerys"&gt; /u/kuerys &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nn5ur4/æ¨¡å‹èˆ‡é¡¯ç¤ºå¡_å…¬å¼_åˆ¤æ–·_èˆ‡ç”œèœœå€¼/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nn5ur4/æ¨¡å‹èˆ‡é¡¯ç¤ºå¡_å…¬å¼_åˆ¤æ–·_èˆ‡ç”œèœœå€¼/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nn5ur4/æ¨¡å‹èˆ‡é¡¯ç¤ºå¡_å…¬å¼_åˆ¤æ–·_èˆ‡ç”œèœœå€¼/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-21T22:37:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm4he9</id>
    <title>GPT 5 for Computer Use agents</title>
    <updated>2025-09-20T17:39:57+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nm4he9/gpt_5_for_computer_use_agents/"&gt; &lt;img alt="GPT 5 for Computer Use agents" src="https://external-preview.redd.it/N3kwajc1ODh3Y3FmMRG6-oWujtVtwWnIPYQYAphLJPDZc9z94p-KY-4O-UR8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a97694f82ea0cf901a2133a289ae2d142db6b036" title="GPT 5 for Computer Use agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same tasks, same grounding model we just swapped GPT 4o with GPT 5 as the thinking model.&lt;/p&gt; &lt;p&gt;Left = 4o, right = 5.&lt;/p&gt; &lt;p&gt;Watch GPT 5 pull through.&lt;/p&gt; &lt;p&gt;Grounding model: Salesforce GTA1-7B&lt;/p&gt; &lt;p&gt;Action space: CUA Cloud Instances (macOS/Linux/Windows)&lt;/p&gt; &lt;p&gt;The task is: &amp;quot;Navigate to {random_url} and play the game until you reach a score of 5/5â€....each task is set up by having claude generate a random app from a predefined list of prompts (multiple choice trivia, form filling, or color matching)&amp;quot;&lt;/p&gt; &lt;p&gt;Try it yourself here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs : &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agent"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agent&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.gg/cua-ai"&gt;https://discord.gg/cua-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/euxlwdk8wcqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nm4he9/gpt_5_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nm4he9/gpt_5_for_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-20T17:39:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmqziq</id>
    <title>Can my 12th Gen i3 processor with 8GB of RAM work with docker?</title>
    <updated>2025-09-21T12:44:15+00:00</updated>
    <author>
      <name>/u/Autumn_Red_29</name>
      <uri>https://old.reddit.com/user/Autumn_Red_29</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Autumn_Red_29"&gt; /u/Autumn_Red_29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/docker/comments/1nmqz0r/can_my_12th_gen_i3_processor_with_8gb_of_ram_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmqziq/can_my_12th_gen_i3_processor_with_8gb_of_ram_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nmqziq/can_my_12th_gen_i3_processor_with_8gb_of_ram_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-21T12:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnjgqr</id>
    <title>Amdahlâ€™s Law: the hidden reason multi-GPU setups disappoint for local LLMs</title>
    <updated>2025-09-22T11:05:01+00:00</updated>
    <author>
      <name>/u/AggravatingGiraffe46</name>
      <uri>https://old.reddit.com/user/AggravatingGiraffe46</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggravatingGiraffe46"&gt; /u/AggravatingGiraffe46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nnjgis/amdahls_law_the_hidden_reason_multigpu_setups/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnjgqr/amdahls_law_the_hidden_reason_multigpu_setups/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnjgqr/amdahls_law_the_hidden_reason_multigpu_setups/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T11:05:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnk9nw</id>
    <title>Stop dragging weights across GPUs: a â€œtopic routerâ€ approach to multi-GPU LLMs</title>
    <updated>2025-09-22T11:47:56+00:00</updated>
    <author>
      <name>/u/AggravatingGiraffe46</name>
      <uri>https://old.reddit.com/user/AggravatingGiraffe46</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggravatingGiraffe46"&gt; /u/AggravatingGiraffe46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nnjud5/stop_dragging_weights_across_gpus_a_topic_router/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnk9nw/stop_dragging_weights_across_gpus_a_topic_router/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnk9nw/stop_dragging_weights_across_gpus_a_topic_router/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T11:47:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmxfzx</id>
    <title>GLM-4.5V model for local computer use</title>
    <updated>2025-09-21T17:06:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nmxfzx/glm45v_model_for_local_computer_use/"&gt; &lt;img alt="GLM-4.5V model for local computer use" src="https://external-preview.redd.it/M2l1bDVtdjR2anFmMbWADQNBSkImjVESNjfi_q43l9ostHKNGAFX_QJdfnS0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ce566c5e77071f3f7f400637f6e0dc7338a416c" title="GLM-4.5V model for local computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either: Locally via Hugging Face Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord : &lt;a href="https://discord.gg/cua-ai"&gt;https://discord.gg/cua-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5z8xkkg5vjqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmxfzx/glm45v_model_for_local_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nmxfzx/glm45v_model_for_local_computer_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-21T17:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1no30sy</id>
    <title>F*ck Framework better hardware options for ollama</title>
    <updated>2025-09-23T00:18:09+00:00</updated>
    <author>
      <name>/u/UnluckyGold13</name>
      <uri>https://old.reddit.com/user/UnluckyGold13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1no30sy/fck_framework_better_hardware_options_for_ollama/"&gt; &lt;img alt="F*ck Framework better hardware options for ollama" src="https://preview.redd.it/xwf71yy35tqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da663285cc3485e3faa7dd7ce4ebe6a010356a5d" title="F*ck Framework better hardware options for ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was buying a Framework desktop to run ollama but it looks that I'm not good enough for their shi*t. Are there any other options? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnluckyGold13"&gt; /u/UnluckyGold13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xwf71yy35tqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no30sy/fck_framework_better_hardware_options_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1no30sy/fck_framework_better_hardware_options_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T00:18:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmwl1k</id>
    <title>We've Just Hit 400 Stars on Nanocoder - This Community Is Amazing ğŸ”¥</title>
    <updated>2025-09-21T16:32:51+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nmwl1k/weve_just_hit_400_stars_on_nanocoder_this/"&gt; &lt;img alt="We've Just Hit 400 Stars on Nanocoder - This Community Is Amazing ğŸ”¥" src="https://external-preview.redd.it/lxZd6DAdowPiPmyPupJyn2UjdkAEKDAl7mCc_8DECBE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a389a78f14886501f623dea0321b6040ed33089" title="We've Just Hit 400 Stars on Nanocoder - This Community Is Amazing ğŸ”¥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/3g7t1oyvnjqf1.gif"&gt;https://i.redd.it/3g7t1oyvnjqf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is yet another appreciation post for the community. Since my last, so much has happened in the Nanocoder community - new feedback, new features, many new people joining and contributing. It's honestly incredible to be building community-owned and pushed CLI software that breaks free of the corporations running other coding tools and offerings.&lt;/p&gt; &lt;p&gt;Along with a bunch of new features and improvements over the last couple of weeks, I'm actively moving the Nanocoder repository to be owned by a GitHub Organisation called Nano Collective â€“ this collective further reinforces my desire to make this project a community-led and run project. Within this collective I hope to continue to build out new frameworks and fine-tunes for local-first coding as well as seek grants to distribute to contributors to push research forward.&lt;/p&gt; &lt;p&gt;This is really really early days and Nanocoder as a coding CLI is right at the beginning, it's improving every day but there's still lots to do! That being said, any feedback and help within any domain is appreciated and welcomed.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;System prompt writing&lt;/li&gt; &lt;li&gt;Research&lt;/li&gt; &lt;li&gt;Helping to push the word out&lt;/li&gt; &lt;li&gt;Any feedback generally! Good or bad :)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to get involved the links are below. Bring on 1,000 stars â­ï¸&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Link&lt;/strong&gt;: &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord Link&lt;/strong&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmwl1k/weve_just_hit_400_stars_on_nanocoder_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nmwl1k/weve_just_hit_400_stars_on_nanocoder_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nmwl1k/weve_just_hit_400_stars_on_nanocoder_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-21T16:32:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nntaoa</id>
    <title>Ollama SSL API access via OpenWebUI</title>
    <updated>2025-09-22T17:45:47+00:00</updated>
    <author>
      <name>/u/Septa105</name>
      <uri>https://old.reddit.com/user/Septa105</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;/p&gt; &lt;p&gt;I managed to get a http api server working. Now I am struggling with ssl The api server and webui docker components are only accessible via VPN over a NAS&lt;/p&gt; &lt;p&gt;So I created the cert files and I was also able to import them to my iOS device .&lt;/p&gt; &lt;p&gt;I launched the api server with keys and it s saying listening &lt;a href="https://0.0.0.0:11435"&gt;https://0.0.0.0:11435&lt;/a&gt; &lt;/p&gt; &lt;p&gt;But when I want to load models with open webui it gives me a network error although I used the https:// local address with port and /v1&lt;/p&gt; &lt;p&gt;I tried to curl the api ssl server works fine . &lt;/p&gt; &lt;p&gt;Also updated the keys within openwebui am I doing something wrong?&lt;/p&gt; &lt;p&gt;The open webui runs on http://local adr. : 3000 and is accessible&lt;/p&gt; &lt;p&gt;At the end I wanted to locally use voice on iOS which is only allowed via ssl certificate&lt;/p&gt; &lt;p&gt;Or do I need to get openwebui to https and can keep the api http ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Septa105"&gt; /u/Septa105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nntaoa/ollama_ssl_api_access_via_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nntaoa/ollama_ssl_api_access_via_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nntaoa/ollama_ssl_api_access_via_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T17:45:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nni2oa</id>
    <title>Is there a way to run model only on CPU memory?</title>
    <updated>2025-09-22T09:44:11+00:00</updated>
    <author>
      <name>/u/kasianenko</name>
      <uri>https://old.reddit.com/user/kasianenko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I noticed when I run models that don't fit into gpu memory, the speed is terrible, up to 30 sec per token. Looks like ollama does some memory swap and offloading, is there a way to enforce running only on CPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kasianenko"&gt; /u/kasianenko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nni2oa/is_there_a_way_to_run_model_only_on_cpu_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nni2oa/is_there_a_way_to_run_model_only_on_cpu_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nni2oa/is_there_a_way_to_run_model_only_on_cpu_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T09:44:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnwaox</id>
    <title>Good afternoon, I'm new to AI, so I would appreciate it if someone could explain to me how Ollama works.</title>
    <updated>2025-09-22T19:37:53+00:00</updated>
    <author>
      <name>/u/TROLLMASK2222</name>
      <uri>https://old.reddit.com/user/TROLLMASK2222</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TROLLMASK2222"&gt; /u/TROLLMASK2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnwaox/good_afternoon_im_new_to_ai_so_i_would_appreciate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnwaox/good_afternoon_im_new_to_ai_so_i_would_appreciate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnwaox/good_afternoon_im_new_to_ai_so_i_would_appreciate/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T19:37:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnxpsf</id>
    <title>Uncensored LLM</title>
    <updated>2025-09-22T20:31:38+00:00</updated>
    <author>
      <name>/u/Time-Teaching1926</name>
      <uri>https://old.reddit.com/user/Time-Teaching1926</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Time-Teaching1926"&gt; /u/Time-Teaching1926 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/MistralAI/comments/1nnxoj1/uncensored_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnxpsf/uncensored_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnxpsf/uncensored_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T20:31:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnd7n3</id>
    <title>Use your local models to investigate leaks &amp; government docs</title>
    <updated>2025-09-22T04:37:39+00:00</updated>
    <author>
      <name>/u/New_Pomegranate_1060</name>
      <uri>https://old.reddit.com/user/New_Pomegranate_1060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;After a lot of tinkering, Iâ€™ve finally released a project Iâ€™ve been working on: TruthSeeker.&lt;/p&gt; &lt;p&gt;Itâ€™s a tool designed to make it easier to search, parse, and analyze government documents and leaks. Think of it as a way to cut through the noise and surface the signal in huge, messy datasets.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;p&gt;Pulls in documents (FOIA releases, leaks, etc.) Indexes them for fast keyword + context search Helps spot connections and recurring themes&lt;/p&gt; &lt;p&gt;Why I built it: I was tired of watching people drop big document dumps online, only for them to disappear into the void because no one had the time or tools to dig through them properly. This project is my attempt to fix that.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/RawdodReverend/TruthSeeker"&gt;https://github.com/RawdodReverend/TruthSeeker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Videos: &lt;a href="https://www.tiktok.com/@rawdogreverend"&gt;https://www.tiktok.com/@rawdogreverend&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Iâ€™d love any feedback, feature requests, or just thoughts on whether youâ€™d find this useful. If you try it out and break it, let me know. I want to improve it fast.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Pomegranate_1060"&gt; /u/New_Pomegranate_1060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnd7n3/use_your_local_models_to_investigate_leaks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnd7n3/use_your_local_models_to_investigate_leaks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnd7n3/use_your_local_models_to_investigate_leaks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T04:37:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnktup</id>
    <title>How does Ollama run gpt-oss?</title>
    <updated>2025-09-22T12:15:32+00:00</updated>
    <author>
      <name>/u/AirCigar</name>
      <uri>https://old.reddit.com/user/AirCigar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi.&lt;/p&gt; &lt;p&gt;As far as I understand, running gpt-oss with native mxfp4 quantization requires Hopper architecture and newer. However, I've seen people run people run it on Ada Lovelace GPUs such as RTX 4090. What does Ollama do to support mxfp4? I couldn't find any documentation.&lt;/p&gt; &lt;p&gt;Transformers workaround is dequantization, according to &lt;a href="https://github.com/huggingface/transformers/pull/39940"&gt;https://github.com/huggingface/transformers/pull/39940&lt;/a&gt;, does Ollama do something similar?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AirCigar"&gt; /u/AirCigar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnktup/how_does_ollama_run_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnktup/how_does_ollama_run_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnktup/how_does_ollama_run_gptoss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T12:15:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nny46f</id>
    <title>Ollama cloud and privacy</title>
    <updated>2025-09-22T20:46:47+00:00</updated>
    <author>
      <name>/u/cyuhat</name>
      <uri>https://old.reddit.com/user/cyuhat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I am intrested in the ollama cloud feature but as someone concerned with data privacy I struggle to find all the information I need. Mainly I can't find answer for the following questions: 1. I live in Europe. I know that USA have the USA Patriot Act and the Cloud act which basically give the governement access to any data hosted by US servers in place or abroad. Ollama cloud does not store any log or data in their server, but is it possible then that requests get intercepted? 2. I know Ollama is close to OpenAI and I wanted to ask to whom the datacenter belong to.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cyuhat"&gt; /u/cyuhat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nny46f/ollama_cloud_and_privacy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nny46f/ollama_cloud_and_privacy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nny46f/ollama_cloud_and_privacy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T20:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1no6mfa</id>
    <title>We made a new AI interface that is compatible with Ollama</title>
    <updated>2025-09-23T03:13:15+00:00</updated>
    <author>
      <name>/u/GermainCampman</name>
      <uri>https://old.reddit.com/user/GermainCampman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please check us out if you want a local AI interface that rivals and even surpasses chatGPT in some ways!&lt;/p&gt; &lt;p&gt;magelab.ai &lt;/p&gt; &lt;ul&gt; &lt;li&gt;no vendor lock in&lt;/li&gt; &lt;li&gt;compatible with Ollama&lt;/li&gt; &lt;li&gt;powerful out of box experience &lt;/li&gt; &lt;li&gt;full speech integration &lt;/li&gt; &lt;li&gt;transparent use of AI by design&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GermainCampman"&gt; /u/GermainCampman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no6mfa/we_made_a_new_ai_interface_that_is_compatible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no6mfa/we_made_a_new_ai_interface_that_is_compatible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1no6mfa/we_made_a_new_ai_interface_that_is_compatible/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T03:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1no94pr</id>
    <title>Iâ€™ve been using old Xeon boxes (especially dual-socket setups) with heaps of RAM, and wanted to put together some thoughts + research that backs up why that setup is still quite viable.</title>
    <updated>2025-09-23T05:31:29+00:00</updated>
    <author>
      <name>/u/AggravatingGiraffe46</name>
      <uri>https://old.reddit.com/user/AggravatingGiraffe46</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;What makes old Xeons + lots of RAM still powerful&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Memory-heavy workloads:&lt;/strong&gt; Applications like in-memory databases, caching (Redis / Memcached), big Spark jobs, or large virtual machine setups benefit heavily from having &lt;em&gt;physical memory&lt;/em&gt; over disk or even SSD bottlenecks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parallelism over clock speed:&lt;/strong&gt; Xeons with many cores/threads, even if older, can still outperform modern CPUs in tasks where you can spread work well. If single-thread isnâ€™t super critical, you get a lot of value.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Price/performance + amortization:&lt;/strong&gt; Used Xeon gear + cheap server RAM (especially ECC/registered) can deliver fractions of the cost of modern CPUs with relatively modest performance loss for many use-cases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reliability / durability:&lt;/strong&gt; Server parts are built for sustained loads, often with better cooling, ECC memory, etc., so done right the maintenance cost can be low.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here are some studies &amp;amp; posts that support various claims about using a lot of RAM, memory behavior, and what kinds of workloads benefit:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Source&lt;/th&gt; &lt;th align="left"&gt;What it shows / relevance&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;A Study of Virtual Memory Usage and Implications for Big-Memory Systems&lt;/em&gt; (UW, 2013)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://homes.cs.washington.edu/%7Eluisceze/publications/vmstudy-uwtr2013.pdf?utm_source=chatgpt.com"&gt;Homes at the University of Washington&lt;/a&gt;Examines how modern server + client applications make heavy use of RAM; shows that servers often have hundreds of GBs of physical memory and that â€œbig-memoryâ€ usage is growing.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;The Case for RAMClouds: Scalable High-Performance Storage Entirely in DRAM&lt;/em&gt; (Ousterhout et al., PDF)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.cs.princeton.edu/courses/archive/spring13/cos598C/ramcloud.pdf?utm_source=chatgpt.com"&gt;Princeton CS&lt;/a&gt;Argues that keeping data in RAM (distributed across many machines) yields 100-1000Ã— lower latency / much higher throughput vs disk-based systems. Good support for the idea that if you have big RAM you can do powerful stuff.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;A Comprehensive Memory Analysis of Data Intensive Applications&lt;/em&gt; (GMU, 2018)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://mason.gmu.edu/%7Espudukot/Files/Conferences/Mem_Subsys18.pdf?utm_source=chatgpt.com"&gt;Mason&lt;/a&gt;Shows how big data / Spark / MPI frameworks behave based on memory capacity, number of channels, etc. Points out that some applications benefit greatly from more memory, especially if they are iterative or aggregate large data in memory.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;Revisiting Memory Errors in Large-Scale Production Data Centers&lt;/em&gt; (Facebook / CMU)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://users.ece.cmu.edu/%7Eomutlu/pub/memory-errors-at-facebook_dsn15.pdf?utm_source=chatgpt.com"&gt;Carnegie Mellon University ECE&lt;/a&gt;Deals with reliability of DRAM in server fleets. Relevant if youâ€™re using older RAM / many DIMMs â€” shows what kinds of error rates and what matters (ECC, controller, channel, DIMM quality).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;My Home Lab Server with 20 cores / 40 threads and 128 GB memory&lt;/em&gt; (blog post)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://louwrentius.com/my-home-lab-server-with-20-cores-40-threads-and-128-gb-memory.html?utm_source=chatgpt.com"&gt;louwrentius.com&lt;/a&gt;Real-world example: an older Xeon E5-2680 v2 machine, with 128 GB RAM, showing how usable performance still is despite age (VMs/containers) and decent multi-core scores.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Tradeoffs / what to watch out for&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Power draw and efficiency&lt;/strong&gt;: Old dual-Xeon boards + many DIMMs = higher idle power and higher heat. If running 24/7, electricity and cooling matter.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Single-thread / per core speed&lt;/strong&gt;: Newer CPUs typically have higher clock speeds, better IPC. For tasks that depend on those (e.g. UI responsiveness, some compiles, gaming), old Xeons may lag.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compatibility &amp;amp; spares&lt;/strong&gt;: Motherboard, ECC RAM, firmware updates, etc., can be harder/cheaper to source.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory reliability&lt;/strong&gt;: As DRAM ages and if ECC isnâ€™t used, error rates go up. Also older DIMMs might be higher failure risk.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggravatingGiraffe46"&gt; /u/AggravatingGiraffe46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no94pr/ive_been_using_old_xeon_boxes_especially/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no94pr/ive_been_using_old_xeon_boxes_especially/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1no94pr/ive_been_using_old_xeon_boxes_especially/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T05:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1no9yio</id>
    <title>ADAM - Your Agile Digital Asisstant</title>
    <updated>2025-09-23T06:21:45+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1no9yio/adam_your_agile_digital_asisstant/"&gt; &lt;img alt="ADAM - Your Agile Digital Asisstant" src="https://external-preview.redd.it/MHVvbmI4OHJ4dXFmMX7JxH1xO5NXSruR7HvEl3I_E7WAJ5O94MjagrWhHLIm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b86cb7a5b6b40d530c6f0ae24fff106e65d6f5da" title="ADAM - Your Agile Digital Asisstant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;take a sneak peak at &lt;a href="https://adam-showcase.vercel.app"&gt;ADAM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Post in your prompts for ADAM to response below. This will also be part of my stress testing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/twvnn98rxuqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no9yio/adam_your_agile_digital_asisstant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1no9yio/adam_your_agile_digital_asisstant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T06:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnwbhn</id>
    <title>Best open uncensored model for writing short stories?</title>
    <updated>2025-09-22T19:38:41+00:00</updated>
    <author>
      <name>/u/MassiveBoner911_3</name>
      <uri>https://old.reddit.com/user/MassiveBoner911_3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this has been asked before but the post was a few months old; figured id ask again since models come out faster every week.&lt;/p&gt; &lt;p&gt;Whats everyone using for their creative writing? Id like an open uncensored model thats great with creative and generating ideas. &lt;/p&gt; &lt;p&gt;I like writing dark / gory slasher horror. &lt;/p&gt; &lt;p&gt;OpenAI immediately tells me to â€œfuck offâ€. Gemini goes â€œabsolutely notâ€ Grok goes â€œhere is all the thingsâ€â€¦.but id like to try others.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MassiveBoner911_3"&gt; /u/MassiveBoner911_3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnwbhn/best_open_uncensored_model_for_writing_short/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nnwbhn/best_open_uncensored_model_for_writing_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nnwbhn/best_open_uncensored_model_for_writing_short/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T19:38:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nob7hx</id>
    <title>Calling through the API causes the model to be crazy. Anybody else experiencing this?</title>
    <updated>2025-09-23T07:43:20+00:00</updated>
    <author>
      <name>/u/businessAlcoholCream</name>
      <uri>https://old.reddit.com/user/businessAlcoholCream</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use gemma3:4b-it-qat for this project and it has been working for almost 3 months now but I noticed starting yesterday, the model went crazy.&lt;/p&gt; &lt;p&gt;The project is a simple python script that takes in information from &lt;a href="http://vlr.gg"&gt;vlr.gg&lt;/a&gt;, process it, and then pass it to the model. I made sure that it runs on startup too. I use it to be updated on what is happening to teams I like. With the information collected, I process it to prompts like these&lt;/p&gt; &lt;p&gt;&amp;quot;Team X is about to face team Y in z days&amp;quot;&lt;br /&gt; &amp;quot;Team X previous match against team W resulted to a score of 2:0&amp;quot;&lt;br /&gt; &amp;quot;Team A has no upcoming match&amp;quot;&lt;br /&gt; &amp;quot;Team B has no upcoming match&amp;quot;&lt;/p&gt; &lt;p&gt;After giving all the necessary prompts as the user, I give the model one final prompt along the lines of&lt;/p&gt; &lt;p&gt;&amp;quot;With those information, create a single paragraph summary to keep me updated on what is happening in VCT&amp;quot;&lt;/p&gt; &lt;p&gt;It worked well before and I would get results like&lt;/p&gt; &lt;p&gt;&amp;quot;Here is your summary for the day. Team X is about to face team Y in z days. In their previous match, they won against team W with a score of 2:0&amp;quot;&lt;/p&gt; &lt;p&gt;But starting yesterday, I get results like&lt;/p&gt; &lt;p&gt;&amp;quot;I'm&lt;/p&gt; &lt;p&gt;Okay, I want to be&lt;/p&gt; &lt;p&gt;I want a report&lt;/p&gt; &lt;p&gt;report.&lt;/p&gt; &lt;p&gt;Do not&lt;/p&gt; &lt;p&gt;Do&lt;/p&gt; &lt;p&gt;I don't.&lt;/p&gt; &lt;p&gt;&amp;quot;&lt;/p&gt; &lt;p&gt;and&lt;/p&gt; &lt;p&gt;&amp;quot; to&lt;/p&gt; &lt;p&gt;The only&lt;/p&gt; &lt;p&gt;to deliver&lt;/p&gt; &lt;p&gt;It's.&lt;/p&gt; &lt;p&gt;the.&lt;/p&gt; &lt;p&gt;to deliver&lt;/p&gt; &lt;p&gt;to.&lt;/p&gt; &lt;p&gt;a&lt;/p&gt; &lt;p&gt;It's&lt;/p&gt; &lt;p&gt;to&lt;/p&gt; &lt;p&gt;I&lt;/p&gt; &lt;p&gt;The summary&lt;/p&gt; &lt;p&gt;to&lt;/p&gt; &lt;p&gt;to be&lt;/p&gt; &lt;p&gt;&amp;quot;&lt;/p&gt; &lt;p&gt;I tested the model through ollama run and it responds normally. Anyone else experiencing this problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/businessAlcoholCream"&gt; /u/businessAlcoholCream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nob7hx/calling_through_the_api_causes_the_model_to_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nob7hx/calling_through_the_api_causes_the_model_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nob7hx/calling_through_the_api_causes_the_model_to_be/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T07:43:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1no7ab4</id>
    <title>Qwen3-Omni coming soon?</title>
    <updated>2025-09-23T03:47:23+00:00</updated>
    <author>
      <name>/u/veryhasselglad</name>
      <uri>https://old.reddit.com/user/veryhasselglad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyway to test this with ollama right now from hf?&lt;br /&gt; Will ollama make their own tweaks before release?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/veryhasselglad"&gt; /u/veryhasselglad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no7ab4/qwen3omni_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no7ab4/qwen3omni_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1no7ab4/qwen3omni_coming_soon/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T03:47:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nolw3a</id>
    <title>Qoder plans at 50% off !!</title>
    <updated>2025-09-23T16:21:25+00:00</updated>
    <author>
      <name>/u/Training-Surround228</name>
      <uri>https://old.reddit.com/user/Training-Surround228</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nolw3a/qoder_plans_at_50_off/"&gt; &lt;img alt="Qoder plans at 50% off !!" src="https://preview.redd.it/5yaiypjpwxqf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32363581516df275873b8abd689133f84f910bfe" title="Qoder plans at 50% off !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found it to perform reasonably well during free trial. &lt;/p&gt; &lt;p&gt;Wanted to get community feedback before subscribing. &lt;/p&gt; &lt;p&gt;I already have Trae subscription which went to shit earlier, but last few days have been good (perhaps Sonnet 4 APi bugs resolved) . Will adding this be worth it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Training-Surround228"&gt; /u/Training-Surround228 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5yaiypjpwxqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nolw3a/qoder_plans_at_50_off/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nolw3a/qoder_plans_at_50_off/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T16:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1noijqp</id>
    <title>Best PHP Coding Model for 5060ti 16GB/128GB RAM</title>
    <updated>2025-09-23T14:14:28+00:00</updated>
    <author>
      <name>/u/vortec350</name>
      <uri>https://old.reddit.com/user/vortec350</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;sup&gt;that.&lt;/sup&gt; Iâ€™ve asked AI and googled and browsed this forum but most people care about JavaScript, not PHP haha. Thank you :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vortec350"&gt; /u/vortec350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1noijqp/best_php_coding_model_for_5060ti_16gb128gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1noijqp/best_php_coding_model_for_5060ti_16gb128gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1noijqp/best_php_coding_model_for_5060ti_16gb128gb_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T14:14:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1no2b7d</id>
    <title>Most Dangerous Ollama Agent? Demo + Repo</title>
    <updated>2025-09-22T23:45:15+00:00</updated>
    <author>
      <name>/u/New_Pomegranate_1060</name>
      <uri>https://old.reddit.com/user/New_Pomegranate_1060</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1no2b7d/most_dangerous_ollama_agent_demo_repo/"&gt; &lt;img alt="Most Dangerous Ollama Agent? Demo + Repo" src="https://external-preview.redd.it/YWp4Y29jaDB6c3FmMXt94OWDuAWHQAvniQ96yay80ZGLpddsHnI1O_WJEL_6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=653779aec1642f70f934b334caddfdb4f0fe5ee2" title="Most Dangerous Ollama Agent? Demo + Repo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on an ollama agent Iâ€™m calling TermNet and itâ€™s honestly kind of nuts. In the demo video I show it doing a bunch of stuff most agents probably shouldnâ€™t be trusted with. Itâ€™s got full terminal access so it can run commands directly on my machine.&lt;/p&gt; &lt;p&gt;It doesnâ€™t stop there. It pulls system info, makes directories and files, writes and executes programs (can do gui) browses the web, and scans my local network. None of it is scripted or staged either. The agent strings everything together on its own and gives me the results in plain language. Itâ€™s a strange mix of useful and dangerous, which is why I figured Iâ€™d share it here.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/RawdodReverend/TermNet"&gt;https://github.com/RawdodReverend/TermNet&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TikTok: &lt;a href="https://www.tiktok.com/@rawdogreverend"&gt;https://www.tiktok.com/@rawdogreverend&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone decides to try it, Iâ€™d highly recommend running it in a VM or sandbox. It has full access to the system, so donâ€™t point it at anything you care about.&lt;/p&gt; &lt;p&gt;Not trying to make this into some big â€œAI safetyâ€ post, just showing off what Iâ€™ve been playing with. But after seeing it chain commands and spin up code on the fly, I think it might be one of the more dangerous ollama agents out there right now. Curious what people here think and if anyone else has pushed agents this far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Pomegranate_1060"&gt; /u/New_Pomegranate_1060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ff0w6fm0zsqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no2b7d/most_dangerous_ollama_agent_demo_repo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1no2b7d/most_dangerous_ollama_agent_demo_repo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T23:45:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nokzx7</id>
    <title>Computer Use on Windows Sandbox</title>
    <updated>2025-09-23T15:47:59+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nokzx7/computer_use_on_windows_sandbox/"&gt; &lt;img alt="Computer Use on Windows Sandbox" src="https://external-preview.redd.it/NGVuamVvaXpxeHFmMYar2P-d3EU8x2ju_uKYrB4yrb0aAUxLp4mH5szJsZ9M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db4b778804fd2edc20bde5e8cf8b999696f4b392" title="Computer Use on Windows Sandbox" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing Windows Sandbox support - run computer-use agents on Windows business apps without VMs or cloud costs.&lt;/p&gt; &lt;p&gt;Your enterprise software runs on Windows, but testing agents required expensive cloud instances. Windows Sandbox changes this - it's Microsoft's built-in lightweight virtualization sitting on every Windows 10/11 machine, ready for instant agent development.&lt;/p&gt; &lt;p&gt;Enterprise customers kept asking for AutoCAD automation, SAP integration, and legacy Windows software support. Traditional VM testing was slow and resource-heavy. Windows Sandbox solves this with disposable, seconds-to-boot Windows environments for safe agent testing.&lt;/p&gt; &lt;p&gt;What you can build: AutoCAD drawing automation, SAP workflow processing, Bloomberg terminal trading bots, manufacturing execution system integration, or any Windows-only enterprise software automation - all tested safely in disposable sandbox environments.&lt;/p&gt; &lt;p&gt;Free with Windows 10/11, boots in seconds, completely disposable. Perfect for development and testing before deploying to Windows cloud instances (coming later this month).&lt;/p&gt; &lt;p&gt;Check out the github here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/windows-sandbox"&gt;https://www.trycua.com/blog/windows-sandbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ibs8dytzqxqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nokzx7/computer_use_on_windows_sandbox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nokzx7/computer_use_on_windows_sandbox/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T15:47:59+00:00</published>
  </entry>
</feed>
