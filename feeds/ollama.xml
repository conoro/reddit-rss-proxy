<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-10-08T15:49:14+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1nxms96</id>
    <title>Pardus AI: Open source AI Assistant thanks for the help with Ollama</title>
    <updated>2025-10-04T06:38:42+00:00</updated>
    <author>
      <name>/u/jasonhon2013</name>
      <uri>https://old.reddit.com/user/jasonhon2013</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nxms96/pardus_ai_open_source_ai_assistant_thanks_for_the/"&gt; &lt;img alt="Pardus AI: Open source AI Assistant thanks for the help with Ollama" src="https://external-preview.redd.it/Ake-kZcpHuhp7eDMuefYBJn8h_Eek4tI7hJD6Fg5Ly0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cababb145e25a96f1e5dfbe2653ec717bfc7a70" title="Pardus AI: Open source AI Assistant thanks for the help with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys. I always love open source. Our team decided to open source the Pardus AI Assistant &lt;a href="https://github.com/PardusAI/PardusAI"&gt;https://github.com/PardusAI/PardusAI&lt;/a&gt;, which basically is an AI assistant that memorizes what you have done, and you can ask it about your personal information, like what you have to do later, or ask it about the information you have just visited. The underlying relies on Ollama to do the embedding about you; actually, you can change everything locally, not even needing to use OpenRouter! Please give us a little star :) (begging star right now, loll) Thanks to the Ollama community, and I always love this community so much!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1nxms96/video/whyexd2ai1tf1/player"&gt;https://reddit.com/link/1nxms96/video/whyexd2ai1tf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonhon2013"&gt; /u/jasonhon2013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxms96/pardus_ai_open_source_ai_assistant_thanks_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxms96/pardus_ai_open_source_ai_assistant_thanks_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nxms96/pardus_ai_open_source_ai_assistant_thanks_for_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-04T06:38:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ny1rtf</id>
    <title>Help configuring an Intel Arc B50</title>
    <updated>2025-10-04T18:36:15+00:00</updated>
    <author>
      <name>/u/RetroZelda</name>
      <uri>https://old.reddit.com/user/RetroZelda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, im still fairly new to self hosting LLMs but I was able to successfully get ollama running on my local debian machine utilizing my rtx a2000 by simply running the install script from &lt;a href="https://ollama.com/download"&gt;https://ollama.com/download&lt;/a&gt;, However, im now failing to get the new intel arc B50 to work as well.&lt;/p&gt; &lt;p&gt;To give some context, this is the machine:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OS: Debian Testing(Forky)&lt;/li&gt; &lt;li&gt;Kernel: 6.16.3+deb13-amd64&lt;/li&gt; &lt;li&gt;CPU: AMD Ryzen 7 5700X&lt;/li&gt; &lt;li&gt;RAM: 128GB&lt;/li&gt; &lt;li&gt;NVIDIA: (via nvidia-smi) Driver Version: 550.163.01 | CUDA Version: 12.4&lt;/li&gt; &lt;li&gt;Intel: (via vainfo) VA-API version: 1.22 (libva 2.22.0) | Intel iHD driver for Intel(R) Gen Graphics - 25.3.4&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ lspci -k | grep -iA3 vga 25:00.0 VGA compatible controller: Intel Corporation Battlemage G21 [Intel Graphics] Subsystem: Intel Corporation Device 1114 Kernel driver in use: xe Kernel modules: xe -- 2d:00.0 VGA compatible controller: NVIDIA Corporation GA106 [RTX A2000 12GB] (rev a1) Subsystem: NVIDIA Corporation Device 1611 Kernel driver in use: nvidia Kernel modules: nvidia &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I started by install One Api by following &lt;a href="https://www.intel.com/content/www/us/en/docs/oneapi/installation-guide-linux/2025-2/base-online-offline.html#BASE-ONLINE-OFFLINE"&gt;this guide&lt;/a&gt; for the offline installation.&lt;/p&gt; &lt;p&gt;And then I followed step 3.3(page 21) from &lt;a href="https://www.intel.com/content/www/us/en/content-details/826081/running-ollama-with-open-webui-on-intel-hardware-platform.html"&gt;this guide&lt;/a&gt; from intel to build and run IPEX-LLM with ollama. Since it seems to only work with python 3.11, I manually pulled the source and built python 3.11.9 to get that to function.&lt;/p&gt; &lt;p&gt;I then modified the ollama systemd service to look like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Unit] Description=Ollama Service After=network-online.target [Service] ExecStart=/bin/bash -c 'source /home/gpt/intel/oneapi/setvars.sh &amp;amp;&amp;amp; exec /home/gpt/ollama/llama-cpp/ollama serve' User=gpt Group=gpt Restart=always RestartSec=3 Environment=&amp;quot;PATH=/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/gpt/.cache/lm-studio/bin:/home/gpt/.cache/lm-studio/bin:/home/gpt/intel/oneapi/2025.2/bin&amp;quot; Environment=&amp;quot;OLLAMA_HOST=0.0.0.0:11434&amp;quot; Environment=&amp;quot;SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1&amp;quot; Environment=&amp;quot;OLLAMA_NUM_GPU=999&amp;quot; Environment=&amp;quot;no_proxy=localhost,127.0.0.1&amp;quot; Environment=&amp;quot;ZES_ENABLE_SYSMAN=1&amp;quot; Environment=&amp;quot;SYCL_CACHE_PERSISTENT=1&amp;quot; Environment=&amp;quot;OLLAMA_INTEL_GPU=1&amp;quot; Environment=&amp;quot;OLLAMA_NUM_PARALLEL=1&amp;quot; # Limit concurrency to avoid overload Environment=&amp;quot;OLLAMA_NUM_GPU=999&amp;quot; WorkingDirectory=/home/gpt [Install] WantedBy=default.target &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However, when I run &lt;code&gt;$ ollama run phi3:latest&lt;/code&gt; i get this error:&lt;br /&gt; &lt;code&gt;Error: 500 Internal Server Error: llama runner process has terminated: exit status 2&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Checking the Ollama serve logs I have this output:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;:: initializing oneAPI environment ... start-ollama.sh: BASH_VERSION = 5.3.3(1)-release args: Using &amp;quot;$@&amp;quot; for setvars.sh arguments: :: advisor -- latest :: ccl -- latest :: compiler -- latest :: dal -- latest :: debugger -- latest :: dev-utilities -- latest :: dnnl -- latest :: dpcpp-ct -- latest :: dpl -- latest :: ipp -- latest :: ippcp -- latest :: mkl -- latest :: mpi -- latest :: pti -- latest :: tbb -- latest :: umf -- latest :: vtune -- latest :: oneAPI environment initialized :: time=2025-10-04T14:26:27.398-04:00 level=INFO source=routes.go:1235 msg=&amp;quot;server config&amp;quot; env=&amp;quot;map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:true OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/gpt/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:localhost,127.0.0.1]&amp;quot; time=2025-10-04T14:26:27.399-04:00 level=INFO source=images.go:476 msg=&amp;quot;total blobs: 20&amp;quot; time=2025-10-04T14:26:27.400-04:00 level=INFO source=images.go:483 msg=&amp;quot;total unused blobs removed: 0&amp;quot; [GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached. [GIN-debug] [WARNING] Running in &amp;quot;debug&amp;quot; mode. Switch to &amp;quot;release&amp;quot; mode in production. - using env: export GIN_MODE=release - using code: gin.SetMode(gin.ReleaseMode) [GIN-debug] HEAD / --&amp;gt; github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers) [GIN-debug] GET / --&amp;gt; github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers) [GIN-debug] HEAD /api/version --&amp;gt; github.com/ollama/ollama/server.(*Server).GenerateRoutes.func3 (5 handlers) [GIN-debug] GET /api/version --&amp;gt; github.com/ollama/ollama/server.(*Server).GenerateRoutes.func4 (5 handlers) [GIN-debug] POST /api/pull --&amp;gt; github.com/ollama/ollama/server.(*Server).PullHandler-fm (5 handlers) [GIN-debug] POST /api/push --&amp;gt; github.com/ollama/ollama/server.(*Server).PushHandler-fm (5 handlers) [GIN-debug] HEAD /api/tags --&amp;gt; github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers) [GIN-debug] GET /api/tags --&amp;gt; github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers) [GIN-debug] POST /api/show --&amp;gt; github.com/ollama/ollama/server.(*Server).ShowHandler-fm (5 handlers) [GIN-debug] DELETE /api/delete --&amp;gt; github.com/ollama/ollama/server.(*Server).DeleteHandler-fm (5 handlers) [GIN-debug] POST /api/create --&amp;gt; github.com/ollama/ollama/server.(*Server).CreateHandler-fm (5 handlers) [GIN-debug] POST /api/blobs/:digest --&amp;gt; github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers) [GIN-debug] HEAD /api/blobs/:digest --&amp;gt; github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers) [GIN-debug] POST /api/copy --&amp;gt; github.com/ollama/ollama/server.(*Server).CopyHandler-fm (5 handlers) [GIN-debug] GET /api/ps --&amp;gt; github.com/ollama/ollama/server.(*Server).PsHandler-fm (5 handlers) [GIN-debug] POST /api/generate --&amp;gt; github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers) [GIN-debug] POST /api/chat --&amp;gt; github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers) [GIN-debug] POST /api/embed --&amp;gt; github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers) [GIN-debug] POST /api/embeddings --&amp;gt; github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers) [GIN-debug] POST /v1/chat/completions --&amp;gt; github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers) [GIN-debug] POST /v1/completions --&amp;gt; github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers) [GIN-debug] POST /v1/embeddings --&amp;gt; github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers) [GIN-debug] GET /v1/models --&amp;gt; github.com/ollama/ollama/server.(*Server).ListHandler-fm (6 handlers) [GIN-debug] GET /v1/models/:model --&amp;gt; github.com/ollama/ollama/server.(*Server).ShowHandler-fm (6 handlers) time=2025-10-04T14:26:27.400-04:00 level=INFO source=routes.go:1288 msg=&amp;quot;Listening on [::]:11434 (version 0.9.3)&amp;quot; time=2025-10-04T14:26:27.400-04:00 level=INFO source=gpu.go:217 msg=&amp;quot;looking for compatible GPUs&amp;quot; time=2025-10-04T14:26:27.400-04:00 level=INFO source=gpu.go:218 msg=&amp;quot;using Intel GPU&amp;quot; time=2025-10-04T14:26:27.519-04:00 level=INFO source=types.go:130 msg=&amp;quot;inference compute&amp;quot; id=GPU-40eaab82-b153-1201-6487-49c7446c9327 library=cuda variant=v12 compute=8.6 driver=12.4 name=&amp;quot;NVIDIA RTX A2000 12GB&amp;quot; total=&amp;quot;11.8 GiB&amp;quot; available=&amp;quot;11.7 GiB&amp;quot; time=2025-10-04T14:26:27.519-04:00 level=INFO source=types.go:130 msg=&amp;quot;inference compute&amp;quot; id=0 library=oneapi variant=&amp;quot;&amp;quot; compute=&amp;quot;&amp;quot; driver=0.0 name=&amp;quot;Intel(R) Graphics [0xe212]&amp;quot; total=&amp;quot;15.9 GiB&amp;quot; available=&amp;quot;15.1 GiB&amp;quot; [GIN] 2025/10/04 - 14:26:48 | 200 | 35.88µs | 127.0.0.1 | HEAD &amp;quot;/&amp;quot; [GIN] 2025/10/04 - 14:26:48 | 200 | 7.380578ms | 127.0.0.1 | POST &amp;quot;/api/show&amp;quot; time=2025-10-04T14:26:48.773-04:00 level=INFO source=sched.go:788 msg=&amp;quot;new model will fit in available VRAM in single GPU, loading&amp;quot; model=/home/gpt/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf gpu=GPU-40eaab82-b153-1201-6487-49c7446c9327 parallel=1 available=12509773824 required=&amp;quot;3.4 GiB&amp;quot; time=2025-10-04T14:26:48.866-04:00 level=INFO source=server.go:135 msg=&amp;quot;system memory&amp;quot; total=&amp;quot;125.7 GiB&amp;quot; free=&amp;quot;114.3 GiB&amp;quot; free_swap=&amp;quot;936.5 MiB&amp;quot; time=2025-10-04T14:26:48.866-04:00 level=INFO source=server.go:187 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split=&amp;quot;&amp;quot; memory.available=&amp;quot;[11.7 GiB]&amp;quot; memory.gpu_overhead=&amp;quot;0 B&amp;quot; memory.required.full=&amp;quot;3.4 GiB&amp;quot; memory.required.partial=&amp;quot;3.4 GiB&amp;quot; memory.required.kv=&amp;quot;768.0 MiB&amp;quot; memory.required.allocations=&amp;quot;[3.4 GiB]&amp;quot; memory.weights.total=&amp;quot;2.0 GiB&amp;quot; memory.weights.repeating=&amp;quot;1.9 GiB&amp;quot; memory.weights.nonrepeating=&amp;quot;77.1 MiB&amp;quot; memory.graph.full=&amp;quot;128.0 MiB&amp;quot; memory.graph.partial=&amp;quot;128.0 MiB&amp;quot; llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /home/gpt/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output. llama_model_loader: - kv 0: general.architecture str = phi3 llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = Phi 3 Mini 128k Instruct llama_model_loader: - kv 3: general.finetune str = 128k-instruct llama_model_loader: - kv 4: general.basename str = Phi-3 llama_model_loader: - kv 5: general.size_label str = mini llama_model_loader: - kv 6: general.license str = mit llama_model_loader: - kv 7: general.license.link str = https://huggingface.co/microsoft/Phi-... llama_model_loader: - kv 8: general.tags arr[str,3] = [&amp;quot;nlp&amp;quot;, &amp;quot;code&amp;quot;, &amp;quot;text-generation&amp;quot;] llama_model_loader: - kv 9: general.languages arr[str,1] = [&amp;quot;en&amp;quot;] llama_model_loader: - kv 10: phi3.context_length u32 = 131072 llama_model_loader: - kv 11: phi3.rope.scaling.original_context_length u32 = 4096 llama_model_loader: - kv 12: phi3.embedding_length u32 = 3072 llama_model_loader: - kv 13: phi3.feed_forward_length u32 = 8192 llama_model_loader: - kv 14: phi3.block_count u32 = 32 llama_model_loader: - kv 15: phi3.attention.head_count u32 = 32 llama_model_loader: - kv 16: phi3.attention.head_count_kv u32 = 32 llama_model_loader: - kv 17: phi3.attention.layer_norm_rms_epsilon f32 = 0.000010 llama_model_loader: - kv 18: phi3.rope.dimension_count u32 = 96 llama_model_loader: - kv 19: phi3.rope.freq_base f32 = 10000.000000 llama_model_loader: - kv 20: general.file_type u32 = 2 llama_model_loader: - kv 21: phi3.attention.sliding_window u32 = 262144 llama_model_loader: - kv 22: phi3.rope.scaling.attn_factor f32 = 1.190238 llama_model_loader: - kv 23: tokenizer.ggml.model str = llama llama_model_loader: - kv 24: tokenizer.ggml.pre str = default llama_model_loader: - kv 25: tokenizer.ggml.tokens arr[str,32064] = [&amp;quot;&amp;lt;unk&amp;gt;&amp;quot;, &amp;quot;&amp;lt;s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;/s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;0x00&amp;gt;&amp;quot;, &amp;quot;&amp;lt;... llama_model_loader: - kv 26: tokenizer.ggml.scores arr[f32,32064] = [-1000.000000, -1000.000000, -1000.00... llama_model_loader: - kv 27: tokenizer.ggml.token_type arr[i32,32064] = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ... llama_model_loader: - kv 28: tokenizer.ggml.bos_token_id u32 = 1 llama_model_loader: - kv 29: tokenizer.ggml.eos_token_id u32 = 32000 llama_model_loader: - kv 30: tokenizer.ggml.unknown_token_id u32 = 0 llama_model_loader: - kv 31: tokenizer.ggml.padding_token_id u32 = 32000 llama_model_loader: - kv 32: tokenizer.ggml.add_bos_token bool = false llama_model_loader: - kv 33: tokenizer.ggml.add_eos_token bool = false llama_model_loader: - kv 34: tokenizer.chat_template str = {% for message in messages %}{% if me... llama_model_loader: - kv 35: general.quantization_version u32 = 2 llama_model_loader: - type f32: 67 tensors llama_model_loader: - type q4_0: 129 tensors llama_model_loader: - type q6_K: 1 tensors print_info: file format = GGUF V3 (latest) print_info: file type = Q4_0 print_info: file size = 2.03 GiB (4.55 BPW) load: special tokens cache size = 14 load: token to piece cache size = 0.1685 MB print_info: arch = phi3 print_info: vocab_only = 1 print_info: model type = ?B print_info: model params = 3.82 B print_info: general.name= Phi 3 Mini 128k Instruct print_info: vocab type = SPM print_info: n_vocab = 32064 print_info: n_merges = 0 print_info: BOS token = 1 '&amp;lt;s&amp;gt;' print_info: EOS token = 32000 '&amp;lt;|endoftext|&amp;gt;' print_info: EOT token = 32007 '&amp;lt;|end|&amp;gt;' print_info: UNK token = 0 '&amp;lt;unk&amp;gt;' print_info: PAD token = 32000 '&amp;lt;|endoftext|&amp;gt;' print_info: LF token = 13 '&amp;lt;0x0A&amp;gt;' print_info: EOG token = 32000 '&amp;lt;|endoftext|&amp;gt;' print_info: EOG token = 32007 '&amp;lt;|end|&amp;gt;' print_info: max token length = 48 llama_model_load: vocab only - skipping tensors time=2025-10-04T14:26:48.890-04:00 level=INFO source=server.go:458 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;/home/gpt/ollama/llm_env/lib/python3.11/site-packages/bigdl/cpp/libs/ollama/ollama-lib runner --model /home/gpt/.ollama/models/blobs/sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf --ctx-size 2048 --batch-size 512 --n-gpu-layers 999 --threads 8 --parallel 1 --port 34853&amp;quot; time=2025-10-04T14:26:48.891-04:00 level=INFO source=sched.go:483 msg=&amp;quot;loaded runners&amp;quot; count=1 time=2025-10-04T14:26:48.891-04:00 level=INFO source=server.go:618 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; time=2025-10-04T14:26:48.891-04:00 level=INFO source=server.go:652 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server not responding&amp;quot; using override patterns: [] time=2025-10-04T14:26:48.936-04:00 level=INFO source=runner.go:851 msg=&amp;quot;starting go runner&amp;quot; Abort was called at 15 line in file: ./shared/source/gmm_helper/resource_info.cpp SIGABRT: abort PC=0x7f1a3da9e95c m=0 sigcode=18446744073709551610 signal arrived during cgo execution &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And following that in the logs there are these blocks. The first 3 seem unique, but from 4 to 22 they appear to generally be the same as 3:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;goroutine 1 gp=0xc000002380 m=0 mp=0x20e5760 [syscall]: runtime.cgocall(0x1168610, 0xc00012d538) /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/cgocall.go:167 +0x4b fp=0xc00012d510 sp=0xc00012d4d8 pc=0x49780b github.com/ollama/ollama/ml/backend/ggml/ggml/src._Cfunc_ggml_backend_load_all_from_path(0x9e38ed0) _cgo_gotypes.go:195 +0x3a fp=0xc00012d538 sp=0xc00012d510 pc=0x84307a github.com/ollama/ollama/ml/backend/ggml/ggml/src.init.func1.1({0xc000056014, 0x4b}) /home/runner/_work/llm.cpp/llm.cpp/ollama-internal/ml/backend/ggml/ggml/src/ggml.go:97 +0xf5 fp=0xc00012d5d0 sp=0xc00012d538 pc=0x842b15 github.com/ollama/ollama/ml/backend/ggml/ggml/src.init.func1() /home/runner/_work/llm.cpp/llm.cpp/ollama-internal/ml/backend/ggml/ggml/src/ggml.go:98 +0x526 fp=0xc00012d860 sp=0xc00012d5d0 pc=0x842966 github.com/ollama/ollama/ml/backend/ggml/ggml/src.init.OnceFunc.func2() /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/sync/oncefunc.go:27 +0x62 fp=0xc00012d8a8 sp=0xc00012d860 pc=0x842362 sync.(*Once).doSlow(0x0?, 0x0?) /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/sync/once.go:78 +0xab fp=0xc00012d900 sp=0xc00012d8a8 pc=0x4ac7eb sync.(*Once).Do(0x0?, 0x0?) /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/sync/once.go:69 +0x19 fp=0xc00012d920 sp=0xc00012d900 pc=0x4ac719 github.com/ollama/ollama/ml/backend/ggml/ggml/src.init.OnceFunc.func3() /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/sync/oncefunc.go:32 +0x2d fp=0xc00012d950 sp=0xc00012d920 pc=0x8422cd github.com/ollama/ollama/llama.BackendInit() /home/runner/_work/llm.cpp/llm.cpp/ollama-internal/llama/llama.go:57 +0x16 fp=0xc00012d960 sp=0xc00012d950 pc=0x846c76 github.com/ollama/ollama/runner/llamarunner.Execute({0xc000034120, 0xe, 0xe}) /home/runner/_work/llm.cpp/llm.cpp/ollama-internal/runner/llamarunner/runner.go:853 +0x7d4 fp=0xc00012dd08 sp=0xc00012d960 pc=0x905cf4 github.com/ollama/ollama/runner.Execute({0xc000034110?, 0x0?, 0x0?}) /home/runner/_work/llm.cpp/llm.cpp/ollama-internal/runner/runner.go:22 +0xd4 fp=0xc00012dd30 sp=0xc00012dd08 pc=0x98b474 github.com/ollama/ollama/cmd.NewCLI.func2(0xc000506f00?, {0x141a6a2?, 0x4?, 0x141a6a6?}) /home/runner/_work/llm.cpp/llm.cpp/ollama-internal/cmd/cmd.go:1529 +0x45 fp=0xc00012dd58 sp=0xc00012dd30 pc=0x10e7c05 github.com/spf13/cobra.(*Command).execute(0xc00053fb08, {0xc00016b420, 0xe, 0xe}) /root/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:940 +0x85c fp=0xc00012de78 sp=0xc00012dd58 pc=0x6120bc github.com/spf13/cobra.(*Command).ExecuteC(0xc000148f08) /root/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5 fp=0xc00012df30 sp=0xc00012de78 pc=0x612905 github.com/spf13/cobra.(*Command).Execute(...) /root/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:992 github.com/spf13/cobra.(*Command).ExecuteContext(...) /root/go/pkg/mod/github.com/spf13/cobra@v1.7.0/command.go:985 main.main() /home/runner/_work/llm.cpp/llm.cpp/ollama-internal/main.go:12 +0x4d fp=0xc00012df50 sp=0xc00012df30 pc=0x10e868d runtime.main() /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:283 +0x28b fp=0xc00012dfe0 sp=0xc00012df50 pc=0x466f6b runtime.goexit({}) /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc00012dfe8 sp=0xc00012dfe0 pc=0x4a22e1 goroutine 2 gp=0xc000002e00 m=nil [force gc (idle)]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?) /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc000094fa8 sp=0xc000094f88 pc=0x49ac8e runtime.goparkunlock(...) /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:441 runtime.forcegchelper() /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:348 +0xb3 fp=0xc000094fe0 sp=0xc000094fa8 pc=0x4672b3 runtime.goexit({}) /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc000094fe8 sp=0xc000094fe0 pc=0x4a22e1 created by runtime.init.7 in goroutine 1 /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:336 +0x1a goroutine 3 gp=0xc000003340 m=nil [GC sweep wait]: runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?) /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:435 +0xce fp=0xc000095780 sp=0xc000095760 pc=0x49ac8e runtime.goparkunlock(...) /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/proc.go:441 runtime.bgsweep(0xc0000c0000) /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgcsweep.go:316 +0xdf fp=0xc0000957c8 sp=0xc000095780 pc=0x451adf runtime.gcenable.gowrap1() /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:204 +0x25 fp=0xc0000957e0 sp=0xc0000957c8 pc=0x445f45 runtime.goexit({}) /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/asm_amd64.s:1700 +0x1 fp=0xc0000957e8 sp=0xc0000957e0 pc=0x4a22e1 created by runtime.gcenable in goroutine 1 /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.linux-amd64/src/runtime/mgc.go:204 +0x66 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have also tried using &lt;a href="https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md#linux-quickstart"&gt;intel's portable ipex-llm&lt;/a&gt; but it gives the same result. So im wondering if anyone has ran into a similar issue with Battlemage cards and was able to get it working. Thanks in advanced.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RetroZelda"&gt; /u/RetroZelda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ny1rtf/help_configuring_an_intel_arc_b50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ny1rtf/help_configuring_an_intel_arc_b50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ny1rtf/help_configuring_an_intel_arc_b50/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-04T18:36:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxqaiv</id>
    <title>[Tool Release] ollama_server_manager: A Simple Web UI to Manage Models Across Multiple Local Ollama Servers</title>
    <updated>2025-10-04T10:19:49+00:00</updated>
    <author>
      <name>/u/GennadiiM</name>
      <uri>https://old.reddit.com/user/GennadiiM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was struggling to keep track of models across my &lt;strong&gt;three local Ollama servers&lt;/strong&gt; using only the command line. It got tedious!&lt;/p&gt; &lt;p&gt;To solve this, I created &lt;code&gt;ollama_server_manager&lt;/code&gt;- a simple tool that provides a &lt;strong&gt;web-based dashboard&lt;/strong&gt; to overview which models are present on which server.&lt;/p&gt; &lt;p&gt;Since I only use this on my &lt;strong&gt;private, trusted network&lt;/strong&gt;, I kept it intentionally &lt;strong&gt;simple with no authentication&lt;/strong&gt; required.&lt;/p&gt; &lt;p&gt;Hope others find this useful for managing their local setups!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Project Link:&lt;/strong&gt; &lt;a href="https://github.com/GhennadiiMir/ollama_server_manager"&gt;&lt;code&gt;https://github.com/GhennadiiMir/ollama_server_manager&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GennadiiM"&gt; /u/GennadiiM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxqaiv/tool_release_ollama_server_manager_a_simple_web/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxqaiv/tool_release_ollama_server_manager_a_simple_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nxqaiv/tool_release_ollama_server_manager_a_simple_web/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-04T10:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxto2o</id>
    <title>So much fun... I broke DeepSeek</title>
    <updated>2025-10-04T13:13:25+00:00</updated>
    <author>
      <name>/u/SparhawkBlather</name>
      <uri>https://old.reddit.com/user/SparhawkBlather</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi-&lt;/p&gt; &lt;p&gt;Brand new to this, just spun up a VM on my EPYC 7713 / A2000 6GB proxmox and installed ollama and openwebui and was spinning up models pretty darn fast. Never thought I would be running 7b models at home, nor that they would be so good. This is crazy. I know you all know this, but this is insane that I can do this kind of inference on a 70W card.&lt;/p&gt; &lt;p&gt;That said, the kinds of things you can get as weird edge / failure cases is incredible. I love deepseek for presenting its thinking stage, and sometimes it gets absurd (this is deepseek-r1:7b):&lt;/p&gt; &lt;p&gt;At the end, it just ground to a halt. Couldn't even get the last period out. It's like HAL9000.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: What's the way in which I can think intuitively about how one might calculate the digits of pi? If I was to do it computationally to an arbitrary number of digits of precision, how can I think about it in basic analogistic terms? Assistant: &amp;lt;think&amp;gt; First, the user's query is asking about calculating pi intuitively how to arbitrary precision, computationally, but in basic terms. They might be thinking computistic. Probably means computational or something like computation. Let me rephrase: &amp;quot;how to compute pi to arbitrary digits computationally&amp;quot; and think in basic terms. User said &amp;quot;how to computationally number of digits&amp;quot; probably typo, so computational. &amp;quot;computational. &amp;quot;think basic&amp;quot; might be &amp;quot;think&amp;quot; computational. So they mean intuitive computational terms. User wants to compute pi digits computationally, but intuitively, in basic way, like analog terms. Pi is irrational, so infinite, can't compute exactly, but approximate. Need approximate. User might think about methods to compute pi digits, like geometrically or series or something. Perhaps Monte Carlo method or something. But analog, like physical analogy. User said &amp;quot;analog terms. Maybe physical analogy. Pi can think about circle circumference, diameter, area. But that's basic. Perhaps think about how compute pi using polygons inscribe circle, like Archimedes did. Or use. But that's not computational. Another way. Perhaps think random points, like throwing darts or something. Monte Carlo. But analog. But analog. User said &amp;quot;analog. Perhaps think physical way. So physical analogy. Pi can think about circle, like ratio circumference diameter. But to diameter. But to compute digits, need precise. Perhaps think about inscribe polygons with many sides. But that. Or think about probability. Another way. Think about circle, pi as limit of series. But. But. Perhaps user said basic. Think intuitively. So, simple way. Perhaps think about pi as circle, say, diameter 3.14 or something. But no. Or think about circumference. Another. Think about circle, pi as average or something. Perhaps think about circle. Think about waves. I think. Perhaps think about pi as angle. I think. I think. Perhaps think about circle. User might think about pi as, say, if I have circle, diameter1, circumference, but. But. Perhaps think. Think about. Another way: think about pi as probability. Think about random points. But. I think. Perhaps. Think about circle inscribe circle square. I think. I think. I think. Perhaps think about. Let's say, think about pi as ratio. I think. I think. I think. Perhaps think about. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I think. I. I think. I think. I think. I think. I. I. I think. I think. I. I. I. I think. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I. I &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SparhawkBlather"&gt; /u/SparhawkBlather &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxto2o/so_much_fun_i_broke_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxto2o/so_much_fun_i_broke_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nxto2o/so_much_fun_i_broke_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-04T13:13:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ny4f78</id>
    <title>How to connect MCP Server using Google ADK (Completely Free using Ollama)</title>
    <updated>2025-10-04T20:21:27+00:00</updated>
    <author>
      <name>/u/Tough_Wrangler_6075</name>
      <uri>https://old.reddit.com/user/Tough_Wrangler_6075</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ny4f78/how_to_connect_mcp_server_using_google_adk/"&gt; &lt;img alt="How to connect MCP Server using Google ADK (Completely Free using Ollama)" src="https://external-preview.redd.it/HI_WWjElJyHbOviadLvlsRERD0CD2CuiF2GB7aNY6jY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf9a9b697454460137fc28a6dc881ed6b27e84cb" title="How to connect MCP Server using Google ADK (Completely Free using Ollama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I just wrote an article about my journey connect MCP server using google ADK. As we know that ADK is google's product, but fortunately we still use open model (in my case Llama3.2) instead of paid Gemini. I hope this article helpful for you guys.&lt;br /&gt; I also put the github link at end of article. Please give it star if you find my github project helpful as well&lt;br /&gt; Happy Coding guys &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tough_Wrangler_6075"&gt; /u/Tough_Wrangler_6075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://zackydzacky.medium.com/how-to-connect-mcp-server-using-google-adk-completely-free-using-open-model-808b420ca896"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ny4f78/how_to_connect_mcp_server_using_google_adk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ny4f78/how_to_connect_mcp_server_using_google_adk/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-04T20:21:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyktng</id>
    <title>ollama codex tool call failure</title>
    <updated>2025-10-05T10:35:26+00:00</updated>
    <author>
      <name>/u/goobeyond</name>
      <uri>https://old.reddit.com/user/goobeyond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i'm running ollama v 0.12.3 with gpt-oss:20b, codex 0.44.0 on ubuntu 24.04.&lt;/p&gt; &lt;p&gt;i ask codex &amp;quot;whats in this codebase?&amp;quot; and this is the transcript:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;thinking&lt;/p&gt; &lt;p&gt;The user &amp;quot;whats in this codebase ?&amp;quot; They likely want an overview of the repo. We can list folder structure and key files. We should inspect the&lt;/p&gt; &lt;p&gt;repository: use ls -R maybe.The user &amp;quot;whats in this codebase ?&amp;quot; They likely want an overview of the repo. We can list folder structure and key&lt;/p&gt; &lt;p&gt;files. We should inspect the repository: use ls -R maybe.&lt;/p&gt; &lt;p&gt;thinking&lt;/p&gt; &lt;p&gt;We need to inspect repo. Use repo_browser.print_tree is not provided, but we can use repo_browser.print_tree. Let's try.We need to inspect repo.&lt;/p&gt; &lt;p&gt;Use repo_browser.print_tree is not provided, but we can use repo_browser.print_tree. Let's try.&lt;/p&gt; &lt;p&gt;thinking&lt;/p&gt; &lt;p&gt;We must view repo. Possibly repo_browser.print_tree not supported? Let's check print_tree? The repo_browser tool has &amp;quot;repo_browser.print_tree&amp;quot;&lt;/p&gt; &lt;p&gt;available. But above error says unsupported. Maybe need to pass args. Let's try again.We must view repo. Possibly repo_browser.print_tree not&lt;/p&gt; &lt;p&gt;supported? Let's check print_tree? The repo_browser tool has &amp;quot;repo_browser.print_tree&amp;quot; available. But above error says unsupported. Maybe need&lt;/p&gt; &lt;p&gt;to pass args. Let's try again.&lt;/p&gt; &lt;p&gt;thinking&lt;/p&gt; &lt;p&gt;We need to inspect repository tree. Use repo_browser.print_tree? The function is not supported, maybe &amp;quot;repo_browser.print_tree&amp;quot; is not a&lt;/p&gt; &lt;p&gt;function. The repo_browser functions are defined earlier: print_tree, list_files, print_file. We can call repo_browser.print_tree? They said&lt;/p&gt; &lt;p&gt;unsupported call. Actually there is print_tree defined. Let's call again.We need to inspect repository tree. Use repo_browser.print_tree? The&lt;/p&gt; &lt;p&gt;function is not supported, maybe &amp;quot;repo_browser.print_tree&amp;quot; is not a function. The repo_browser functions are defined earlier: print_tree,&lt;/p&gt; &lt;p&gt;list_files, print_file. We can call repo_browser.print_tree? They said unsupported call. Actually there is print_tree defined. Let's call again.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;i found a similar issue here: &lt;a href="https://github.com/zed-industries/zed/issues/37384"&gt;https://github.com/zed-industries/zed/issues/37384&lt;/a&gt; which says its something to do with ollama , so i reinstalled ollama but still getting the same error.&lt;/p&gt; &lt;p&gt;any help would be appreciated&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goobeyond"&gt; /u/goobeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nyktng/ollama_codex_tool_call_failure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nyktng/ollama_codex_tool_call_failure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nyktng/ollama_codex_tool_call_failure/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-05T10:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nye32v</id>
    <title>COMPUTRON_9000 is getting the ability to use a browser</title>
    <updated>2025-10-05T03:51:14+00:00</updated>
    <author>
      <name>/u/larz01larz</name>
      <uri>https://old.reddit.com/user/larz01larz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nye32v/computron_9000_is_getting_the_ability_to_use_a/"&gt; &lt;img alt="COMPUTRON_9000 is getting the ability to use a browser" src="https://external-preview.redd.it/9Wqn3C-wnN2n8okOg0sOAKK06n8Lso9p9WLmLKdTz3I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=224b46ee12f7cb333306ce7d754981eed4656295" title="COMPUTRON_9000 is getting the ability to use a browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1nye32v/video/rncfxov0u7tf1/player"&gt;https://reddit.com/link/1nye32v/video/rncfxov0u7tf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;COMPUTRON runs on 3x3090s plus 1 3060. Using gpt-oss:120b and qwen2.5vl:30b. &lt;/p&gt; &lt;p&gt;Its open source and you can run it too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lefoulkrod/computron_9000"&gt;https://github.com/lefoulkrod/computron_9000&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/larz01larz"&gt; /u/larz01larz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nye32v/computron_9000_is_getting_the_ability_to_use_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nye32v/computron_9000_is_getting_the_ability_to_use_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nye32v/computron_9000_is_getting_the_ability_to_use_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-05T03:51:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nytxfy</id>
    <title>CoexistAI Now Supports Docker Setup, Also now you can turn any text into Podcasts and Speech Easily</title>
    <updated>2025-10-05T17:12:21+00:00</updated>
    <author>
      <name>/u/Optimalutopic</name>
      <uri>https://old.reddit.com/user/Optimalutopic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nytxfy/coexistai_now_supports_docker_setup_also_now_you/"&gt; &lt;img alt="CoexistAI Now Supports Docker Setup, Also now you can turn any text into Podcasts and Speech Easily" src="https://external-preview.redd.it/1lbtuTIGlXroGvtLVbMfebEYVWfSR6Nw8inE4I1SvqU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63bc74c711389d587961e85da7aa68d748e4f6b6" title="CoexistAI Now Supports Docker Setup, Also now you can turn any text into Podcasts and Speech Easily" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thanks for all the continued love and feedback! Based on what we’ve heard, we’ve added a simple &lt;strong&gt;Docker setup&lt;/strong&gt; (&lt;a href="https://github.com/SPThole/CoexistAI/blob/main/README.docker.md"&gt;https://github.com/SPThole/CoexistAI/blob/main/README.docker.md&lt;/a&gt;) to make it much easier to get started across different platforms.&lt;/p&gt; &lt;h1&gt;Key Updates&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Text to Podcast:&lt;/strong&gt; Convert written content into full podcast episodes — perfect for repurposing articles or blogs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text to Speech:&lt;/strong&gt; Generate high-quality audio from text, with flexible integration for agents or standalone use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docker Installation:&lt;/strong&gt; A new containerized setup with a quick script (&lt;code&gt;./quick_setup_docker.sh&lt;/code&gt;) that handles everything — including an admin UI to tweak settings effortlessly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Just run one script, and you’ll have a &lt;strong&gt;powerful set of tools&lt;/strong&gt; at your fingertips —&lt;br /&gt; get answers from sources across the web, YouTube, Reddit, Maps, Git, or local files;&lt;br /&gt; turn your research into a podcast;&lt;/p&gt; &lt;p&gt;summarise any web page, any youtube video, get newsletters from reddit based on days hot topics,&lt;br /&gt; convert text into speech when you don’t want to read; I can think off so many use cases&lt;br /&gt; and connect your local LLMs/Embedders to powerful tools to build your own &lt;strong&gt;Perplexity/Exa-like research engine/you can achieve much more than that.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Some head to head Comparison in comments&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optimalutopic"&gt; /u/Optimalutopic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/SPThole/CoexistAI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nytxfy/coexistai_now_supports_docker_setup_also_now_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nytxfy/coexistai_now_supports_docker_setup_also_now_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-05T17:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyuamb</id>
    <title>What I Would Like</title>
    <updated>2025-10-05T17:26:04+00:00</updated>
    <author>
      <name>/u/booknerdcarp</name>
      <uri>https://old.reddit.com/user/booknerdcarp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What I would really like is to be able to vibe code with an Ollama model and have it be able to read and write files in my project folder. I am new to this, learning, and having fun but I am just not sure how to accomplish this. Does any one have detailed info or can help. I would appreciate it. The recent changes to Anthropic have made it unbearable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/booknerdcarp"&gt; /u/booknerdcarp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nyuamb/what_i_would_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nyuamb/what_i_would_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nyuamb/what_i_would_like/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-05T17:26:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nykt3u</id>
    <title>Sneak Preview: Ollama Bench</title>
    <updated>2025-10-05T10:34:31+00:00</updated>
    <author>
      <name>/u/phantagom</name>
      <uri>https://old.reddit.com/user/phantagom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nykt3u/sneak_preview_ollama_bench/"&gt; &lt;img alt="Sneak Preview: Ollama Bench" src="https://preview.redd.it/0nec59t9p9tf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1da048e9acbe9b8959f540c8d86b301f7837805c" title="Sneak Preview: Ollama Bench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantagom"&gt; /u/phantagom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0nec59t9p9tf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nykt3u/sneak_preview_ollama_bench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nykt3u/sneak_preview_ollama_bench/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-05T10:34:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nytsku</id>
    <title>Holo1.5 3B as UI Grounding model + Claude as thinking model for Computer Use</title>
    <updated>2025-10-05T17:07:18+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nytsku/holo15_3b_as_ui_grounding_model_claude_as/"&gt; &lt;img alt="Holo1.5 3B as UI Grounding model + Claude as thinking model for Computer Use" src="https://external-preview.redd.it/ZXU5cjkxcjJzYnRmMW31g_v519zh4lqVlORxH-jtbfthhY6srytiSohAXo_q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=627c44e3b263045f5843d0148fdbf93efc7cc36b" title="Holo1.5 3B as UI Grounding model + Claude as thinking model for Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Runner H making some sense of GIMP.&lt;/p&gt; &lt;p&gt;Try yourself : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lbh6if03sbtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nytsku/holo15_3b_as_ui_grounding_model_claude_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nytsku/holo15_3b_as_ui_grounding_model_claude_as/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-05T17:07:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o005ie</id>
    <title>Type of maps</title>
    <updated>2025-10-07T00:23:33+00:00</updated>
    <author>
      <name>/u/Several-Mongoose3571</name>
      <uri>https://old.reddit.com/user/Several-Mongoose3571</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o005ie/type_of_maps/"&gt; &lt;img alt="Type of maps" src="https://preview.redd.it/mfahivws2ltf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ed4b9cff5302609bbe03d73b8f8c8c86bfe63f2" title="Type of maps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on this setting for a few weeks, it’s a world where oceanic trade empires dominate, and most inland territories are scattered and underdeveloped due to ancient magical floods.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Several-Mongoose3571"&gt; /u/Several-Mongoose3571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mfahivws2ltf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o005ie/type_of_maps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o005ie/type_of_maps/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T00:23:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzc25a</id>
    <title>We made an Open Source Llama Agent Kit, memory-enabled Node.js agent framework for Ollama</title>
    <updated>2025-10-06T06:54:40+00:00</updated>
    <author>
      <name>/u/AideTop8744</name>
      <uri>https://old.reddit.com/user/AideTop8744</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At our company I was investigating the possibility of switching to a self hosted Agentic AI using docker for a product we have, it didnt turn out as i hoped to be honest. The response times are not as fast as the API solutions. And i dont think we can really use it in a commercial setting without building our own AI Server with GPUs. I was hoping the CPU based models would perform better. &lt;/p&gt; &lt;p&gt;However the result of this work we decided to open-source, I hope it helps with someone elses work. It does integrate tool calling, threading and memory so it is usable if you are okay with waiting a zibbilion hours. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/EmreMutlu99/Ollama-Agent-Kit"&gt;https://github.com/EmreMutlu99/Ollama-Agent-Kit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AideTop8744"&gt; /u/AideTop8744 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nzc25a/we_made_an_open_source_llama_agent_kit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nzc25a/we_made_an_open_source_llama_agent_kit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nzc25a/we_made_an_open_source_llama_agent_kit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-06T06:54:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyvinr</id>
    <title>Ally finally got RAG – everything runs local now</title>
    <updated>2025-10-05T18:11:18+00:00</updated>
    <author>
      <name>/u/YassinK97</name>
      <uri>https://old.reddit.com/user/YassinK97</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nyvinr/ally_finally_got_rag_everything_runs_local_now/"&gt; &lt;img alt="Ally finally got RAG – everything runs local now" src="https://b.thumbs.redditmedia.com/tl_M5Cc-ul8MmqrX8T3MH0xhoym2th1IDoyyts6xSYU.jpg" title="Ally finally got RAG – everything runs local now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thanks everyone for the support (and stars) from my first posts featuring &lt;a href="https://github.com/YassWorks/Ally"&gt;Ally&lt;/a&gt;, the fully local agentic CLI.&lt;/p&gt; &lt;p&gt;As promised, I've been working on the RAG feature and it's finally here (v0.4.0 as of writing this post). There are currently only local embedding options (HuggingFace or Ollama). You can choose the embedding settings during setup which is really easy and you'll be ready to dive in.&lt;/p&gt; &lt;p&gt;Ally is instructed to only ever answer based on the data provided during RAG sessions. But you can give it permission to use external data as well like the web.&lt;/p&gt; &lt;p&gt;Because the workflow runs entirely locally, you can turn off your internet connection and still have a fully private chat with all your documents (of any types!).&lt;/p&gt; &lt;p&gt;Continuing old conversations is now an option as well with the &lt;code&gt;-i &amp;lt;conversation_id&amp;gt;&lt;/code&gt; flag.&lt;/p&gt; &lt;p&gt;Give it a try and let me know what to improve!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/YassWorks/Ally"&gt;https://github.com/YassWorks/Ally&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YassinK97"&gt; /u/YassinK97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nyvinr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nyvinr/ally_finally_got_rag_everything_runs_local_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nyvinr/ally_finally_got_rag_everything_runs_local_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-05T18:11:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0a0gp</id>
    <title>What the point of Ollama? Need agent builder inside it , it's sole purpose to run when system on and work in background.</title>
    <updated>2025-10-07T09:28:19+00:00</updated>
    <author>
      <name>/u/WestMurky1658</name>
      <uri>https://old.reddit.com/user/WestMurky1658</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WestMurky1658"&gt; /u/WestMurky1658 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0a0gp/what_the_point_of_ollama_need_agent_builder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0a0gp/what_the_point_of_ollama_need_agent_builder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0a0gp/what_the_point_of_ollama_need_agent_builder/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T09:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzip8q</id>
    <title>What coding tools do you use with ollama?</title>
    <updated>2025-10-06T13:13:03+00:00</updated>
    <author>
      <name>/u/SeaBit7159</name>
      <uri>https://old.reddit.com/user/SeaBit7159</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I seek a tool to rapidly implement simple features. Since my computer can run LLMs locally, I want to test its limits.&lt;/p&gt; &lt;p&gt;Claude Code with Claude Code Router fails to modify files and codex seems to have a weak MCP service.&lt;/p&gt; &lt;p&gt;Previously, I used Trae and Kiro, but they can’t run with Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SeaBit7159"&gt; /u/SeaBit7159 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nzip8q/what_coding_tools_do_you_use_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nzip8q/what_coding_tools_do_you_use_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nzip8q/what_coding_tools_do_you_use_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-06T13:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzdnio</id>
    <title>I created an open-source Invisible AI Assistant called Pluely - now at 890+ GitHub stars. You can add and use Ollama for free. Better interface for all your works.</title>
    <updated>2025-10-06T08:39:21+00:00</updated>
    <author>
      <name>/u/iam-neighbour</name>
      <uri>https://old.reddit.com/user/iam-neighbour</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nzdnio/i_created_an_opensource_invisible_ai_assistant/"&gt; &lt;img alt="I created an open-source Invisible AI Assistant called Pluely - now at 890+ GitHub stars. You can add and use Ollama for free. Better interface for all your works." src="https://external-preview.redd.it/aGNzcTFhZTRkZ3RmMYn6P6th22FDriw78c6Xj5wjFaOcKQQ0FizRfMPuGFUP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=019a15ba0d0a6ab04bc4afffa7341ad159d4441b" title="I created an open-source Invisible AI Assistant called Pluely - now at 890+ GitHub stars. You can add and use Ollama for free. Better interface for all your works." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pluely is Your Invisible AI Assistant: Lightning-fast, privacy-first AI assistant that works seamlessly during meetings, interviews, and conversations without anyone knowing. Completely undetectable in video calls, screen shares. All your data is stored locally on your system. Pluely is designed with privacy as a priority, so no external calls are made to our servers. This applies to both free and Pro users.&lt;/p&gt; &lt;p&gt;By far pluely is the best invisible open-source ai assistant, compared to big firms like Cluely, interviewCoder or any.&lt;/p&gt; &lt;p&gt;all with: solo contribution, $0 funding, and endless nights.&lt;/p&gt; &lt;p&gt;Menu you need on your desktop:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;System audio capture&lt;/li&gt; &lt;li&gt;Microphone audio capture&lt;/li&gt; &lt;li&gt;Input for all your queries&lt;/li&gt; &lt;li&gt;Screenshots (auto/manual)&lt;/li&gt; &lt;li&gt;Attach images&lt;/li&gt; &lt;li&gt;History&lt;/li&gt; &lt;li&gt;Settings&lt;/li&gt; &lt;li&gt;Drag handle&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;On free plan: Pluely supports all major LLM providers just bring your own api key, you can also add your own custom providers with cURL commands, same for speech to text providers as well.&lt;/p&gt; &lt;p&gt;On Pro plan: Pluely now has 80+ premium AI models with instant access including with GPT-5 and many other openai models, One-click model switching, Advanced speech-to-text with highest accuracy, and generating system prompts with AI.&lt;/p&gt; &lt;p&gt;Downloads: &lt;a href="https://pluely.com/downloads"&gt;https://pluely.com/downloads&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://pluely.com"&gt;https://pluely.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/iamsrikanthnani/pluely"&gt;https://github.com/iamsrikanthnani/pluely&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know your experience, and how i can improve more. Features to add are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iam-neighbour"&gt; /u/iam-neighbour &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vpmva3e4dgtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nzdnio/i_created_an_opensource_invisible_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nzdnio/i_created_an_opensource_invisible_ai_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-06T08:39:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0fh7w</id>
    <title>Managing Prompts in Python</title>
    <updated>2025-10-07T13:56:36+00:00</updated>
    <author>
      <name>/u/Ultralytics_Burhan</name>
      <uri>https://old.reddit.com/user/Ultralytics_Burhan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of working with strings for prompts in Python code, so I put together a small package I'm calling &lt;code&gt;proompt&lt;/code&gt; to use more object oriented prompts. It's definitely opinionated, and I 'm sure not everyone will like it, but I'd appreciate anyone's feedback on it that tries it out (even if you don't like it).&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Burhan-Q/proompt"&gt;https://github.com/Burhan-Q/proompt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The package is mostly scaffolding to use with custom defined prompt classes, so it's not something fully featured, it's more of a boiler plate structure to use as a foundational interface. I've included some &lt;a href="https://github.com/Burhan-Q/proompt/tree/main/examples"&gt;examples&lt;/a&gt; in the repo to see how to use it and what the advantages are. The principle is kind of like Lego, building blocks to make something that suits your specific needs, using basic Python code.&lt;/p&gt; &lt;p&gt;As a quick overview, here are the interfaces provided:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Context&lt;/code&gt;: Container for runtime information, inherit into a custom concrete class to use the standard interface&lt;/p&gt; &lt;p&gt;&lt;code&gt;ToolContext&lt;/code&gt;: Container for tool calls (callable), inherits from &lt;code&gt;Context&lt;/code&gt; interface, helps with modularity.&lt;/p&gt; &lt;p&gt;&lt;code&gt;BaseProvider&lt;/code&gt;: Core interface for objects that inject information into prompts (I think of it as a data-provider), use to define a custom concrete class for adding information/data to prompts at runtime.&lt;/p&gt; &lt;p&gt;&lt;code&gt;PromptSection&lt;/code&gt;: Defines a subsection of the overall prompt and inherited into custom concrete class as needed, makes prompts a bit more modular since sections can be removed/added quickly; accepts &lt;code&gt;Context&lt;/code&gt;, &lt;code&gt;ToolContext&lt;/code&gt;, and/or &lt;code&gt;BaseProvider&lt;/code&gt; objects.&lt;/p&gt; &lt;p&gt;&lt;code&gt;BasePrompt&lt;/code&gt;: This is the full prompt interface, create a concrete class that inherits &lt;code&gt;BasePrompt&lt;/code&gt; to define your custom prompt, accepts any number of &lt;code&gt;PromptSection&lt;/code&gt; objects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ultralytics_Burhan"&gt; /u/Ultralytics_Burhan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0fh7w/managing_prompts_in_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0fh7w/managing_prompts_in_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0fh7w/managing_prompts_in_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T13:56:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0osx0</id>
    <title>Ollama Throttling</title>
    <updated>2025-10-07T19:34:41+00:00</updated>
    <author>
      <name>/u/geesuth</name>
      <uri>https://old.reddit.com/user/geesuth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o0osx0/ollama_throttling/"&gt; &lt;img alt="Ollama Throttling" src="https://b.thumbs.redditmedia.com/ILv_i23BOc1p51HT5LYnCSM6MEEmx_IWgggvX_FIrkE.jpg" title="Ollama Throttling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I Just rent a VPS I want to trying to host LLM my self, after I download ollama and start it, it's started when I trying to connect the server ollama from the n8n I saw the request coming to ollama server and this what I saw in console. it's take a long and not reply then I stop the request, &lt;/p&gt; &lt;p&gt;My VPS: AMD 16-Core Processor, 32 GB &lt;/p&gt; &lt;p&gt;What this exactly means? &lt;/p&gt; &lt;p&gt;Thank you. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6eb3o332rqtf1.png?width=457&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9996e39f5dd1948bcd09d3f2d5c17e7bddd505ba"&gt;https://preview.redd.it/6eb3o332rqtf1.png?width=457&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9996e39f5dd1948bcd09d3f2d5c17e7bddd505ba&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geesuth"&gt; /u/geesuth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0osx0/ollama_throttling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0osx0/ollama_throttling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0osx0/ollama_throttling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T19:34:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0jo95</id>
    <title>new "decentralised" ai art model, sounds like bs but does it actually works pretty well?</title>
    <updated>2025-10-07T16:31:54+00:00</updated>
    <author>
      <name>/u/Westlake029</name>
      <uri>https://old.reddit.com/user/Westlake029</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(not relevant to this sub but anyways) found this model called &lt;a href="https://github.com/bageldotcom/paris"&gt;paris&lt;/a&gt; today and i wont lie i was super skeptical at first. the whole &amp;quot;decentralised training&amp;quot; thing sounded more like some crypto marketing nonsense but after trying it i am kinda impressed by it. basically instead of training one huge model they trained 8 separate ones and use some router thing to pick which one to use (pretty smart). might sound weird but the results are legit better than i expected for something thats completely free not gonna lie, still prefer my midjourney subscription for serious stuff but for just messing around this is pretty solid. no rate limits, no watermarks, you just name it. just download and go.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Westlake029"&gt; /u/Westlake029 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0jo95/new_decentralised_ai_art_model_sounds_like_bs_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0jo95/new_decentralised_ai_art_model_sounds_like_bs_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0jo95/new_decentralised_ai_art_model_sounds_like_bs_but/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T16:31:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0h2av</id>
    <title>Proxmox or BareMetal to run Ollama?</title>
    <updated>2025-10-07T14:56:04+00:00</updated>
    <author>
      <name>/u/Wonderfullyboredme</name>
      <uri>https://old.reddit.com/user/Wonderfullyboredme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What’s the best option to run Ollama and LLMs long term?&lt;/p&gt; &lt;p&gt;Currently running llama on a LXC and then WebUI in another LXC but it some issues with the pass through getting snags when I update. So I wanted to know what’s the best practice? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wonderfullyboredme"&gt; /u/Wonderfullyboredme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0h2av/proxmox_or_baremetal_to_run_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0h2av/proxmox_or_baremetal_to_run_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0h2av/proxmox_or_baremetal_to_run_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T14:56:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0q2i9</id>
    <title>Ollama error</title>
    <updated>2025-10-07T20:21:25+00:00</updated>
    <author>
      <name>/u/lora_in_lichen_</name>
      <uri>https://old.reddit.com/user/lora_in_lichen_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o0q2i9/ollama_error/"&gt; &lt;img alt="Ollama error" src="https://preview.redd.it/bolbaeqj0rtf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4de6d184091067b21e1c783e32707c6d2c7340de" title="Ollama error" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I need to set up a model to use it with python script with ollama. After pulling cyberlis/saiga-mistral:7b-lora-q8_0 from ollama repository, i tried to run it? but got nothing as a respond. I checked ollama server output and saw the following: (picture)&lt;/p&gt; &lt;p&gt;I would be glad if anyone knows how to fix it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lora_in_lichen_"&gt; /u/lora_in_lichen_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bolbaeqj0rtf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0q2i9/ollama_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0q2i9/ollama_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T20:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0s6we</id>
    <title>MemoryLLM - A conversational AI system using Ollama with persistent memory capabilities.</title>
    <updated>2025-10-07T21:41:16+00:00</updated>
    <author>
      <name>/u/maranone5</name>
      <uri>https://old.reddit.com/user/maranone5</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o0s6we/memoryllm_a_conversational_ai_system_using_ollama/"&gt; &lt;img alt="MemoryLLM - A conversational AI system using Ollama with persistent memory capabilities." src="https://external-preview.redd.it/2KXnTfjRF7fYN7gSWUGWKW5ZuPplML-5c1Y2NAD-nBA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f38a9d6da1ab27070cb74725e72327b05b0f22f9" title="MemoryLLM - A conversational AI system using Ollama with persistent memory capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1o0s6we/video/t56td2xqertf1/player"&gt;https://reddit.com/link/1o0s6we/video/t56td2xqertf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi this is an experiment I've wanted to do for quite a while, It's been done again and again here but this is just a simple implementation.&lt;/p&gt; &lt;p&gt;The idea behind it is to have memory extracted during &amp;quot;idle&amp;quot; time between responses so there's both the memories for rag and the conversation history.&lt;/p&gt; &lt;p&gt;Then just a simple tweak of the system prompt to try and have the model adapt to your tone/mood nothing fancy just prompt.&lt;/p&gt; &lt;p&gt;Anyways it is an attempt of having a conversational local llm that with time get's to know you, with gemma3:4b not so much though :)&lt;/p&gt; &lt;p&gt;Then there is a very bad result of trying to have the llm have a perception of time elapsed between sessions (model gets timestamps and also current date-time)&lt;/p&gt; &lt;p&gt;Anyways here is the repo if someone wants to use or have a blueprint &lt;a href="https://github.com/maranone/MemoryLLM"&gt;MemoryLLM - Github&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maranone5"&gt; /u/maranone5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0s6we/memoryllm_a_conversational_ai_system_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0s6we/memoryllm_a_conversational_ai_system_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0s6we/memoryllm_a_conversational_ai_system_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T21:41:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o11o3u</id>
    <title>Meer CLI — an open-source Claude Code Alternative</title>
    <updated>2025-10-08T05:06:02+00:00</updated>
    <author>
      <name>/u/msaifeldeen</name>
      <uri>https://old.reddit.com/user/msaifeldeen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 I built Meer CLI — an open-source AI command-line tool that talks to any model (Ollama, OpenAI, Claude, etc.)&lt;/p&gt; &lt;p&gt;Hey folks 👋 I’ve been working on a developer-first CLI called Meer AI, now live at meerai.dev.&lt;/p&gt; &lt;p&gt;It’s designed for builders who love the terminal and want to use AI locally or remotely without switching between dashboards or UIs.&lt;/p&gt; &lt;p&gt;🧠 What it does • 🔗 Model-agnostic — works with Ollama, OpenAI, Claude, Gemini, etc. • 🧰 Plug-and-play CLI — run prompts, analyze code, or run agents directly from your terminal • 💾 Local memory — remembers your context across sessions • ⚙️ Configurable providers — choose or self-host your backend (e.g., Ollama on your own server) • 🌊 “Meer” = Sea — themed around ocean intelligence 🌊&lt;/p&gt; &lt;p&gt;💡 Why I built it&lt;/p&gt; &lt;p&gt;I wanted a simple way to unify my self-hosted models and APIs without constant context loss or UI juggling. The goal is to make AI interaction feel native to the command line.&lt;/p&gt; &lt;p&gt;🐳 Try it&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://meerai.dev"&gt;https://meerai.dev&lt;/a&gt; It’s early but functional — you can chat with models, run commands, and customize providers.&lt;/p&gt; &lt;p&gt;Would love feedback, ideas, or contributors who want to shape the future of CLI-based AI tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/msaifeldeen"&gt; /u/msaifeldeen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o11o3u/meer_cli_an_opensource_claude_code_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o11o3u/meer_cli_an_opensource_claude_code_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o11o3u/meer_cli_an_opensource_claude_code_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-08T05:06:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0ivqz</id>
    <title>Hi folks, sorry for the self‑promo. I’ve built an open‑source project that could be useful to some of you</title>
    <updated>2025-10-07T16:02:24+00:00</updated>
    <author>
      <name>/u/panos_s_</name>
      <uri>https://old.reddit.com/user/panos_s_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o0ivqz/hi_folks_sorry_for_the_selfpromo_ive_built_an/"&gt; &lt;img alt="Hi folks, sorry for the self‑promo. I’ve built an open‑source project that could be useful to some of you" src="https://preview.redd.it/61oh2s9aqptf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a22f4bd151e1659d6c522d89b6519dd0237f6d1" title="Hi folks, sorry for the self‑promo. I’ve built an open‑source project that could be useful to some of you" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Web dashboard for NVIDIA GPUs with 30+ real-time metrics (utilisation, memory, temps, clocks, power, processes). Live charts over WebSockets, multi‑GPU support, and one‑command Docker deployment. No agents, minimal setup.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/psalias2006/gpu-hot"&gt;https://github.com/psalias2006/gpu-hot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I built it&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Wanted simple, real‑time visibility without standing up a full metrics stack.&lt;/li&gt; &lt;li&gt;Needed clear insight into temps, throttling, clocks, and active processes during GPU work.&lt;/li&gt; &lt;li&gt;A lightweight dashboard that’s easy to run at home or on a workstation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Polls nvidia-smi and streams 30+ metrics every ~2s via WebSockets.&lt;/li&gt; &lt;li&gt;Tracks per‑GPU utilization, memory (used/free/total), temps, power draw/limits, fan, clocks, PCIe, P‑State, encoder/decoder stats, driver/VBIOS, throttle status.&lt;/li&gt; &lt;li&gt;Shows active GPU processes with PIDs and memory usage.&lt;/li&gt; &lt;li&gt;Clean, responsive UI with live historical charts and basic stats (min/max/avg).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup (Docker)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/psalias2006/gpu-hot cd gpu-hot docker-compose up --build # open http://localhost:1312 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Looking for feedback&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panos_s_"&gt; /u/panos_s_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/61oh2s9aqptf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0ivqz/hi_folks_sorry_for_the_selfpromo_ive_built_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0ivqz/hi_folks_sorry_for_the_selfpromo_ive_built_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T16:02:24+00:00</published>
  </entry>
</feed>
