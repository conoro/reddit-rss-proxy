<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-23T20:29:03+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1rawjod</id>
    <title>Ollama will not bind 11434 to any IPv4 interface. Works fine for localhost and IPv6. This is driving me *nuts*</title>
    <updated>2026-02-21T17:11:21+00:00</updated>
    <author>
      <name>/u/Big_Wave9732</name>
      <uri>https://old.reddit.com/user/Big_Wave9732</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rawjod/ollama_will_not_bind_11434_to_any_ipv4_interface/"&gt; &lt;img alt="Ollama will not bind 11434 to any IPv4 interface. Works fine for localhost and IPv6. This is driving me *nuts*" src="https://preview.redd.it/1kek8e77rvkg1.png?width=140&amp;amp;height=27&amp;amp;auto=webp&amp;amp;s=b1bc48b44cc6557c1d6ba76b4c8eb212f7f4a7da" title="Ollama will not bind 11434 to any IPv4 interface. Works fine for localhost and IPv6. This is driving me *nuts*" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit 3: I'm fuckng dumb. I thought the firewall service was disabled because I was able to ssh in remotely. It wasn't.&lt;/p&gt; &lt;p&gt;systemctl stop firewalld.service&lt;br /&gt; systemctl disable firewalld.service&lt;/p&gt; &lt;p&gt;------&lt;/p&gt; &lt;p&gt;Newly installed RHEL 9.7.&lt;/p&gt; &lt;p&gt;I had Ollama working on this machine previously before reinstalling. Now, however, Ollama absolutely will not listen on IPv4.&lt;/p&gt; &lt;p&gt;Attached is the systemctl file, a netstat showing the service listening on IPv6, the output from journalctl -u ollama --no-pager --follow, and the output in /var/log/messages when the service is started.&lt;/p&gt; &lt;p&gt;Anyone? What the hell is up here??&lt;/p&gt; &lt;p&gt;Edit: I have tried the line &amp;quot;Environment=&amp;quot;OLLAMA_HOST=0.0.0.0&amp;quot; with and without port 11434. Didn't work either way.&lt;/p&gt; &lt;p&gt;Things I have also tried:&lt;br /&gt; Selinux is disabled;&lt;br /&gt; Have completely uninstalled and removed all remnants and config files;&lt;br /&gt; Have disabled IPv6 everywhere that I can find it (kernel, OS, deleted the IPv6 interface);&lt;/p&gt; &lt;p&gt;Edit 2:&lt;br /&gt; &amp;quot;OLLAMA_HOST=&lt;a href="http://0.0.0.0:11434"&gt;http://0.0.0.0:11434&lt;/a&gt; ollama serve&amp;quot; gets the same result.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big_Wave9732"&gt; /u/Big_Wave9732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rawjod"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rawjod/ollama_will_not_bind_11434_to_any_ipv4_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rawjod/ollama_will_not_bind_11434_to_any_ipv4_interface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T17:11:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1razv53</id>
    <title>Version Mismatch help</title>
    <updated>2026-02-21T19:18:03+00:00</updated>
    <author>
      <name>/u/UpYourQuality</name>
      <uri>https://old.reddit.com/user/UpYourQuality</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;Not sure whats going on but the only ollama.exe i have is showing that its the wrong version.&lt;/p&gt; &lt;p&gt;I'm using the most recent installer direct from ollama.com. Even tried the winget version and same error.&lt;/p&gt; &lt;p&gt;On Windows 11&lt;/p&gt; &lt;p&gt;No ollama services or task anywhere&lt;/p&gt; &lt;p&gt;Deleted all ollama folders.&lt;/p&gt; &lt;p&gt;Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UpYourQuality"&gt; /u/UpYourQuality &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1razv53/version_mismatch_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1razv53/version_mismatch_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1razv53/version_mismatch_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T19:18:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rara7u</id>
    <title>Faster &amp; Cheaper LLM Apps with Semantic Caching</title>
    <updated>2026-02-21T13:36:10+00:00</updated>
    <author>
      <name>/u/Special_Community179</name>
      <uri>https://old.reddit.com/user/Special_Community179</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rara7u/faster_cheaper_llm_apps_with_semantic_caching/"&gt; &lt;img alt="Faster &amp;amp; Cheaper LLM Apps with Semantic Caching" src="https://external-preview.redd.it/oOZJdgmTkHw77V31kL7N1jm08j8e9Y-FJAqFULVQOvI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f8d267e8b0a6fddff7d4ecde829bd01253f232f" title="Faster &amp;amp; Cheaper LLM Apps with Semantic Caching" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special_Community179"&gt; /u/Special_Community179 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/NrqvtsnjIHU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rara7u/faster_cheaper_llm_apps_with_semantic_caching/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rara7u/faster_cheaper_llm_apps_with_semantic_caching/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T13:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1raw6z8</id>
    <title>qwen3-coder-next on desktop</title>
    <updated>2026-02-21T16:58:15+00:00</updated>
    <author>
      <name>/u/BobcatLegitimate1497</name>
      <uri>https://old.reddit.com/user/BobcatLegitimate1497</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"&gt; &lt;img alt="qwen3-coder-next on desktop" src="https://preview.redd.it/2t8c4wugnvkg1.png?width=140&amp;amp;height=112&amp;amp;auto=webp&amp;amp;s=b4c92b7d5d4f3747b4fa2068f7da52d3878b28fa" title="qwen3-coder-next on desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Old computer with low-end GPUs (RTX 4060 Ti 16 Gb and 1660 6 Gb)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2t8c4wugnvkg1.png?width=1202&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f426423c92b99569186820894b7fa3a3557b5d82"&gt;https://preview.redd.it/2t8c4wugnvkg1.png?width=1202&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f426423c92b99569186820894b7fa3a3557b5d82&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CPU half-loaded, GPU loaded about 20%. Are there any ways to optimize it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BobcatLegitimate1497"&gt; /u/BobcatLegitimate1497 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T16:58:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb5t3c</id>
    <title>I have a substantial codebase that I want to analyse and build a proof-of-concept around for demonstration purposes</title>
    <updated>2026-02-21T23:22:44+00:00</updated>
    <author>
      <name>/u/eufemiapiccio77</name>
      <uri>https://old.reddit.com/user/eufemiapiccio77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;which local LLM options would allow me to work without the usage restrictions imposed by mainstream hosted providers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eufemiapiccio77"&gt; /u/eufemiapiccio77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb5t3c/i_have_a_substantial_codebase_that_i_want_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb5t3c/i_have_a_substantial_codebase_that_i_want_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb5t3c/i_have_a_substantial_codebase_that_i_want_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T23:22:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1rarsuo</id>
    <title>TIFU/PSA: didn‚Äôt check which GPU ollama was using and was stuck wondering why so slow</title>
    <updated>2026-02-21T13:59:43+00:00</updated>
    <author>
      <name>/u/IAmANobodyAMA</name>
      <uri>https://old.reddit.com/user/IAmANobodyAMA</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure where to tell this story (megathread?) but it made me laugh and has a teachable moment so I thought I would share.&lt;/p&gt; &lt;p&gt;TL;DR: I was running ollama on an old GPU by mistake ü§¶‚Äç‚ôÇÔ∏è&lt;/p&gt; &lt;p&gt;I have two local machines running ollama. My gaming rig has a 5070ti 16gb but isn‚Äôt always on, and my ‚Äúdedicated‚Äù unraid server **had** a 3060ti 8gb that was going to be my AI workhorse.&lt;/p&gt; &lt;p&gt;I chose models that would kick ass on the 5070 when online and more conservative models for the 3060 otherwise.&lt;/p&gt; &lt;p&gt;This is all still experimental/for learning so this setup is fines for me ‚Ä¶ except the unraid server was painfully slow. Took me way too long to figure out that‚Äôs because it was hitting an old GTX 1650 4gb card!! I forgot I swapped out the cards because I was going to build a gaming rig for my kid with the 3060.&lt;/p&gt; &lt;p&gt;I spent way too long researching models and trying to figure out why my ‚Äú3060‚Äù was offloading over 50% of qwen3:4b to my CPU. Since this is hosted on unraid I was convinced that another service (plex?) was using my GPU without permission. Nope, I‚Äôm just a doofus.&lt;/p&gt; &lt;p&gt;It wasn‚Äôt until running `nvidia-smi` in terminal that I realized my error.&lt;/p&gt; &lt;p&gt;Anyways, hope this makes someone chuckle as much as me. Anyone else have some fun ‚Äúdoh‚Äù moments?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IAmANobodyAMA"&gt; /u/IAmANobodyAMA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T13:59:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb8q6p</id>
    <title>Your Model Doesn't Know What It's About to Do. Mine Does. Introducing The Cradle. (V1.0)</title>
    <updated>2026-02-22T01:33:25+00:00</updated>
    <author>
      <name>/u/BiscottiDisastrous19</name>
      <uri>https://old.reddit.com/user/BiscottiDisastrous19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rb8q6p/your_model_doesnt_know_what_its_about_to_do_mine/"&gt; &lt;img alt="Your Model Doesn't Know What It's About to Do. Mine Does. Introducing The Cradle. (V1.0)" src="https://preview.redd.it/jeeinozfmwkg1.png?width=140&amp;amp;height=58&amp;amp;auto=webp&amp;amp;s=0bb84b45e3850cd11ca230d83904e5ff34b94573" title="Your Model Doesn't Know What It's About to Do. Mine Does. Introducing The Cradle. (V1.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BiscottiDisastrous19"&gt; /u/BiscottiDisastrous19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/u_BiscottiDisastrous19/comments/1rb12vw/your_model_doesnt_know_what_its_about_to_do_mine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb8q6p/your_model_doesnt_know_what_its_about_to_do_mine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb8q6p/your_model_doesnt_know_what_its_about_to_do_mine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T01:33:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb1wzf</id>
    <title>RX 9060 XT 16GB</title>
    <updated>2026-02-21T20:41:26+00:00</updated>
    <author>
      <name>/u/ButterscotchTop4598</name>
      <uri>https://old.reddit.com/user/ButterscotchTop4598</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Leute,&lt;/p&gt; &lt;p&gt;Gibt es irgendwo eine Anleitung f√ºr die Verwendung der oben genannten Grafikkarte unter Olama? Leider finde ich dazu nichts. Zudem w√ºrde mich interessieren welches Modell ihr f√ºr die Grafikkarte empfehlen k√∂nnt.&lt;/p&gt; &lt;p&gt;Vielen Dank f√ºr eure Hilfe!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ButterscotchTop4598"&gt; /u/ButterscotchTop4598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb1wzf/rx_9060_xt_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb1wzf/rx_9060_xt_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb1wzf/rx_9060_xt_16gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T20:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb8w1n</id>
    <title>NPU for local models?</title>
    <updated>2026-02-22T01:40:59+00:00</updated>
    <author>
      <name>/u/MemeGLS</name>
      <uri>https://old.reddit.com/user/MemeGLS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm running Qwen 3 on my laptop which has an NPU but I have no idea on how to use it (I was hoping that I could use a bigger model if I‚Äôm able to find a way to use the NPU).&lt;/p&gt; &lt;p&gt;Thanks a lot &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MemeGLS"&gt; /u/MemeGLS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb8w1n/npu_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb8w1n/npu_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb8w1n/npu_for_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T01:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbha7b</id>
    <title>Unable to pull model from ollama</title>
    <updated>2026-02-22T09:12:51+00:00</updated>
    <author>
      <name>/u/badasssravikumae</name>
      <uri>https://old.reddit.com/user/badasssravikumae</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to pull models from ollama but I am unable to do &lt;/p&gt; &lt;p&gt;I did ollama serve&lt;br /&gt; I deleted the cache and checked if the models is available and tried pulling the model even though I see this error: &lt;/p&gt; &lt;p&gt;pulling manifest &lt;/p&gt; &lt;p&gt;Error: pull model manifest: file does not exist&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badasssravikumae"&gt; /u/badasssravikumae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T09:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbdq0m</id>
    <title>Agent architectures for SLMs</title>
    <updated>2026-02-22T05:43:51+00:00</updated>
    <author>
      <name>/u/PangolinPossible7674</name>
      <uri>https://old.reddit.com/user/PangolinPossible7674</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;What kind of agent architectures are generally used with Small Language Models? In the past, I had tried ReAct with some 8B param models, and they failed. Recently, I have been trying out tool calling models via Ollama. Even with function calling, Qwen 3 8B appears to somewhat work, but some other 8B models don't seem to be so great.&lt;/p&gt; &lt;p&gt;Therefore, I was wondering what SLM-agent gas worked for the others. Does verbose docstrings for tools affect performance with SLMs? Alternatively, what smallest model size generally allows diverse tool usage in a reliable way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PangolinPossible7674"&gt; /u/PangolinPossible7674 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T05:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc0r8o</id>
    <title>Got $800 of credits on a cloud platform (for GPU usage). Anyone here that's into AI training and inference and could make use of it?</title>
    <updated>2026-02-22T23:09:21+00:00</updated>
    <author>
      <name>/u/DocumentFun9077</name>
      <uri>https://old.reddit.com/user/DocumentFun9077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have around 800 bucks worth of GPU usage credits on one of the major platform, those can be used specifically for GPU and clusters. So if any individual or hobbyist or anyone out here is training models or inference, or anything else, please contact! (not free btw, but selling at way less price)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocumentFun9077"&gt; /u/DocumentFun9077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0r8o/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0r8o/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc0r8o/got_800_of_credits_on_a_cloud_platform_for_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T23:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc0sqn</id>
    <title>Got $800 of credits on a cloud platform (for GPU usage). Anyone here that's into AI training and inference and could make use of it?</title>
    <updated>2026-02-22T23:11:05+00:00</updated>
    <author>
      <name>/u/DocumentFun9077</name>
      <uri>https://old.reddit.com/user/DocumentFun9077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have around 800 bucks worth of GPU usage credits on one of the major platform, those can be used specifically for GPU and clusters. So if any individual or hobbyist or anyone out here is training models or inference, or anything else, please contact! (not free btw, but selling at way less price)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocumentFun9077"&gt; /u/DocumentFun9077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0sqn/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0sqn/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc0sqn/got_800_of_credits_on_a_cloud_platform_for_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T23:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1rajqj6</id>
    <title>15,000+ tok/s on ChatJimmy: Is the "Model-on-Silicon" era finally starting?</title>
    <updated>2026-02-21T06:19:08+00:00</updated>
    <author>
      <name>/u/Significant-Topic433</name>
      <uri>https://old.reddit.com/user/Significant-Topic433</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"&gt; &lt;img alt="15,000+ tok/s on ChatJimmy: Is the &amp;quot;Model-on-Silicon&amp;quot; era finally starting?" src="https://preview.redd.it/bq69s0n5jskg1.jpg?width=140&amp;amp;height=66&amp;amp;auto=webp&amp;amp;s=fa3f690c9b529f18075dc6e27d8b984f7fcc4fcd" title="15,000+ tok/s on ChatJimmy: Is the &amp;quot;Model-on-Silicon&amp;quot; era finally starting?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôve been discussing local inference for years, but chatjimmy.ai just moved the goalposts. They are hitting 15,414 tokens per second using what they call &amp;quot;mask ROM recall fabric&amp;quot;‚Äîbasically etching the model weights directly into the silicon logic.&lt;/p&gt; &lt;p&gt;‚ÄãThis is a massive shift from our current setups. We‚Äôre used to general-purpose compute, but this is a dedicated ASIC. No HBM, no VRAM bottlenecks, just raw, hardcoded inference.&lt;/p&gt; &lt;p&gt;‚Äã I just invested in two Gigabyte AI TOP ATOM units (the ones based on the NVIDIA Spark / Grace Blackwell architecture). They are absolute beasts for training and fine-tuning with 128GB of unified memory, but seeing a dedicated chip do 15k tok/s makes me wonder: &lt;/p&gt; &lt;p&gt;‚ÄãDid I make the right call with the AI TOP Spark units for local dev, or are we going to see these specialized ASIC cards hit the market soon and make general-purpose desktop AI look like dial-up?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant-Topic433"&gt; /u/Significant-Topic433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rajqj6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T06:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbt4di</id>
    <title>Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation</title>
    <updated>2026-02-22T18:17:48+00:00</updated>
    <author>
      <name>/u/dorbeats</name>
      <uri>https://old.reddit.com/user/dorbeats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"&gt; &lt;img alt="Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation" src="https://preview.redd.it/vj71i95883lg1.jpg?width=140&amp;amp;height=140&amp;amp;auto=webp&amp;amp;s=253fed1c03e9709c1d506fd05f1a1a3300ef8eda" title="Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this feasible on modest hardware?&lt;/p&gt; &lt;p&gt;Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.boomspot.com/llama-3-1-70b-on-rtx-3090-bypassing-cpu-for-ai-innovation"&gt;https://www.boomspot.com/llama-3-1-70b-on-rtx-3090-bypassing-cpu-for-ai-innovation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vj71i95883lg1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c0859a6b570010f0108ab4369475820f191095ed"&gt;https://preview.redd.it/vj71i95883lg1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c0859a6b570010f0108ab4369475820f191095ed&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dorbeats"&gt; /u/dorbeats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T18:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc3fkr</id>
    <title>Model requires more system memory (eventhough I have enough)</title>
    <updated>2026-02-23T01:03:23+00:00</updated>
    <author>
      <name>/u/gfejer</name>
      <uri>https://old.reddit.com/user/gfejer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have two Tesla P40s passed through as vGPU profiles to a Ubuntu 24.04 VM. As an example I can run gpt-oss just fine and the GPUs get recognized by the system, but when it comes to running Llama 3.3 which is a 43GB model (my 2*24GB VRAM should be enough right?) gives me an error that I don‚Äôt have enough system memory.&lt;/p&gt; &lt;p&gt;I am guessing that for some reason it tries to run the model on the CPU, but I don‚Äôt understand why‚Ä¶ Are there any possible fixes for this issue?&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gfejer"&gt; /u/gfejer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:03:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbq3dy</id>
    <title>Ollama for Dummies</title>
    <updated>2026-02-22T16:24:37+00:00</updated>
    <author>
      <name>/u/catbutchie</name>
      <uri>https://old.reddit.com/user/catbutchie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone needs to write a book. Right now my mentor is ChatGPT. There are so many parameters i just tell it my issue and it tells me what to change. Some small tweaks are significant performance adjustment. I‚Äôm new obviously. I‚Äôd like to know why I‚Äôm doing what I‚Äôm doing so I can be more in control. I‚Äôm using silly tavern and ollama for self hosted chat. Suggestions needed. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/catbutchie"&gt; /u/catbutchie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T16:24:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcaxo7</id>
    <title>Qwen3VL:30b-Q6-Thinking</title>
    <updated>2026-02-23T07:23:02+00:00</updated>
    <author>
      <name>/u/CheekyMonkeee</name>
      <uri>https://old.reddit.com/user/CheekyMonkeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been trying for hours to make a Modelfile that will actually make this work in Ollama. I have the ggufs for both the base model and the mmproj together in a folder and the create command finishes successfully. I get the directory and the blobs and the chat accepts images, but I get a 500 error when I prompt. Downloaded both ggufs from huggingface.&lt;/p&gt; &lt;p&gt;At this point, I don‚Äôt even care if it‚Äôs the best model for my use case anymore. I‚Äôm fairly sure it‚Äôs not a memory issue as I‚Äôm on a 5090 with 64GB of system RAM, and it will run the Q4 32b non-thinking model that I downloaded straight from Ollama (even though that spills over into RAM).&lt;/p&gt; &lt;p&gt;I‚Äôve just never had to do the modelfile thing with a download from huggingface before (still fairly new to this) and I don‚Äôt want to give up. I have to sleep, but any advice would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CheekyMonkeee"&gt; /u/CheekyMonkeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcaxo7/qwen3vl30bq6thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcaxo7/qwen3vl30bq6thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcaxo7/qwen3vl30bq6thinking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T07:23:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc7fd1</id>
    <title>AMD 5550xt Macbook Pro 2019</title>
    <updated>2026-02-23T04:11:48+00:00</updated>
    <author>
      <name>/u/tubaraodogroove</name>
      <uri>https://old.reddit.com/user/tubaraodogroove</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm planning to purchase this MacBook second-hand just to run some models that my 2017 laptop can't handle.&lt;/p&gt; &lt;p&gt;However, based on what I found during my research, Ollama doesn‚Äôt recognize this graphics card as a usable GPU. Can someone explain whether this is a good deal for running Ollama, or if it simply won‚Äôt work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tubaraodogroove"&gt; /u/tubaraodogroove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc7fd1/amd_5550xt_macbook_pro_2019/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc7fd1/amd_5550xt_macbook_pro_2019/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc7fd1/amd_5550xt_macbook_pro_2019/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T04:11:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc4l9j</id>
    <title>16GB VRAM for mode agent</title>
    <updated>2026-02-23T01:56:50+00:00</updated>
    <author>
      <name>/u/ColdTransition5828</name>
      <uri>https://old.reddit.com/user/ColdTransition5828</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was hoping to enjoy something similar to Cursor on my PC. I even bought what was supposed to be a mid-range card. But the results are disappointing.&lt;/p&gt; &lt;p&gt;After studying it, I realized I'm missing the core agent and a better Ollama model that accepts tools. But honestly, I'm bored. What do you recommend I do to get the most out of local models with my 16GB of VRAM?&lt;/p&gt; &lt;p&gt;I mostly do full-track coding and Java.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ColdTransition5828"&gt; /u/ColdTransition5828 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:56:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rccbn8</id>
    <title>Built a honeypot token library for AI agents ‚Äî detects prompt injection the moment it succeeds</title>
    <updated>2026-02-23T08:49:56+00:00</updated>
    <author>
      <name>/u/Responsible-Yak-9657</name>
      <uri>https://old.reddit.com/user/Responsible-Yak-9657</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rccbn8/built_a_honeypot_token_library_for_ai_agents/"&gt; &lt;img alt="Built a honeypot token library for AI agents ‚Äî detects prompt injection the moment it succeeds" src="https://external-preview.redd.it/y-WYMevcIN_5-ZHiVkm0emVpTgZqx82u0L69rZGk6FM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18cf100549c9b843e934f38bccc2bb7dc338f724" title="Built a honeypot token library for AI agents ‚Äî detects prompt injection the moment it succeeds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible-Yak-9657"&gt; /u/Responsible-Yak-9657 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/aiagents/comments/1rby96z/built_a_honeypot_token_library_for_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rccbn8/built_a_honeypot_token_library_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rccbn8/built_a_honeypot_token_library_for_ai_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T08:49:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcgy0h</id>
    <title>Help Setting up - nanobot?</title>
    <updated>2026-02-23T13:07:26+00:00</updated>
    <author>
      <name>/u/lawfulcrispy</name>
      <uri>https://old.reddit.com/user/lawfulcrispy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lawfulcrispy"&gt; /u/lawfulcrispy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1rcgxpv/help_setting_up_nanobot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcgy0h/help_setting_up_nanobot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcgy0h/help_setting_up_nanobot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T13:07:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcf94q</id>
    <title>What GPU do you use?</title>
    <updated>2026-02-23T11:43:10+00:00</updated>
    <author>
      <name>/u/Keensworth</name>
      <uri>https://old.reddit.com/user/Keensworth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've recently started using Ollama with an old GPU I had laying around.&lt;/p&gt; &lt;p&gt;Problem is that my GTX 1660S only got 6Gb VRAM and you can't do much with that.&lt;/p&gt; &lt;p&gt;I can run Mistral 7B Instruct but he sucks.&lt;/p&gt; &lt;p&gt;What hardware are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Keensworth"&gt; /u/Keensworth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T11:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc8xvy</id>
    <title>spend more time downloading models than actually using them</title>
    <updated>2026-02-23T05:29:41+00:00</updated>
    <author>
      <name>/u/Sharp-Mouse9049</name>
      <uri>https://old.reddit.com/user/Sharp-Mouse9049</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;qwen 2.5 dropped and suddenly mixtral is dead to me. downloaded the 72b. ran it once. went back to 7b cause i dont actually need 72b for anything i do&lt;/p&gt; &lt;p&gt;got like 200gb of models sitting on my drive. couldnt tell you the difference between half of them without checking the folder names&lt;/p&gt; &lt;p&gt;every week theres a new one thats supposedly better and i gotta have it. run some vibes check. wow this one feels smarter. back to doing the same three things i always do&lt;/p&gt; &lt;p&gt;its like im collecting pokemon but the pokemon just sit there&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sharp-Mouse9049"&gt; /u/Sharp-Mouse9049 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc8xvy/spend_more_time_downloading_models_than_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc8xvy/spend_more_time_downloading_models_than_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc8xvy/spend_more_time_downloading_models_than_actually/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T05:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc3srb</id>
    <title>I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review</title>
    <updated>2026-02-23T01:19:58+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"&gt; &lt;img alt="I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review" src="https://external-preview.redd.it/anBvOHE4ZWxiNWxnMYmmpi9UU3yP9yrC87ePDCyv5Mn4iZk_AUHCQZq2TOQ_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=452fe5fc9cf576221ea71aff1d15b07c8fa36f35" title="I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all ‚Äî been experimenting with running a local AI agent using ClawBot + Ollama and wanted to share what actually happened.&lt;/p&gt; &lt;p&gt;Link to full tutorial: &lt;a href="https://www.youtube.com/watch?v=FxyQkj95VXs"&gt;https://www.youtube.com/watch?v=FxyQkj95VXs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yes, ClawBot + Ollama works on Mac. Does it work well? Depends on what you mean by &amp;quot;work&amp;quot;&lt;/li&gt; &lt;li&gt;With an 8B model, agentic tasks are limited. Basic Q&amp;amp;A? Fine. Anything complex? It'll humble you real quick&lt;/li&gt; &lt;li&gt;Should you expect ChatGPT-level speed? Absolutely not. Go make a coffee while you wait üòÖ&lt;/li&gt; &lt;li&gt;Is it worth it for learning the stack and experimenting locally for free? Honestly yes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What worked&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Setup is cleaner than expected - VS Code, JSON config, localhost dashboard, done. I have no luck setting up ollama using their onboarding. So...I went straight to config file.&lt;/li&gt; &lt;li&gt;Ollama model switching is straightforward once you understand the config structure&lt;/li&gt; &lt;li&gt;Great for understanding how local AI agent setups actually work under the hood&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What didn't&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Speed is rough on anything under 32GB RAM&lt;/li&gt; &lt;li&gt;8B models hit their ceiling fast on multi-step reasoning and real agentic workflows. Keep context window low.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1ee663elb5lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:19:58+00:00</published>
  </entry>
</feed>
