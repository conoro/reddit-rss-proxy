<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-03T03:41:42+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1py1odn</id>
    <title>Old server for local models</title>
    <updated>2025-12-28T20:33:03+00:00</updated>
    <author>
      <name>/u/Jacobmicro</name>
      <uri>https://old.reddit.com/user/Jacobmicro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ended up with an old poweredge r610 with the dual xeon chips and 192gb of ram. Everything is in good working order. Debating on trying to see if I could hack together something to run local models that could automate some of the work I used to pay API keys for with my work. &lt;/p&gt; &lt;p&gt;Anybody ever have any luck using older architecture? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jacobmicro"&gt; /u/Jacobmicro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1py1odn/old_server_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1py1odn/old_server_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1py1odn/old_server_for_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-28T20:33:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz03dc</id>
    <title>Summary of Vibe Coding Models for 6GB VRAM Systems</title>
    <updated>2025-12-29T22:21:11+00:00</updated>
    <author>
      <name>/u/FieldMouseInTheHouse</name>
      <uri>https://old.reddit.com/user/FieldMouseInTheHouse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Summary of Vibe Coding Models for 6GB VRAM Systems&lt;/h1&gt; &lt;p&gt;Here is a list of models that would actually fit inside of a 6GB VRAM budget. I am deliberately leaving out any models that anybody suggested that would not have fit inside of a 6GB VRAM budget! ü§ó&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fitting inside of the 6GB VRAM budget means that it is possible to easily achive 30, 50, 80 or more tokens per second depending on the task.&lt;/strong&gt; If you go outside of the VRAM budget, things can slow down to as slow as 3 to 7 tokens per second -- this could serverely harm productivity.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;`&lt;a href="https://ollama.com/library/qwen3:4b"&gt;qwen3:4b&lt;/a&gt;` size=2.5GB&lt;/li&gt; &lt;li&gt;`&lt;a href="https://ollama.com/library/ministral-3:3b"&gt;ministral-3:3b&lt;/a&gt;` size=3.0GB&lt;/li&gt; &lt;li&gt;`&lt;a href="https://ollama.com/library/gemma3:1b"&gt;gemma3:1b&lt;/a&gt;` size=815MB&lt;/li&gt; &lt;li&gt;`&lt;a href="https://ollama.com/library/gemma3:4b"&gt;gemma3:4b&lt;/a&gt;` size=3.3GB üëà I added this one because it is a little bigger than the &lt;code&gt;gemma3:1b&lt;/code&gt;, but still fits confortably inside of your 6GB VRAM budget. This model should be more capable than &lt;code&gt;gemma3:1b&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üíª I would suggest that folks first try these models with &lt;code&gt;ollama run MODELNAME&lt;/code&gt; and check to see how they fit in the VRAM of your own systems (&lt;code&gt;ollama ps&lt;/code&gt;) and check them for performance like tokens per second during the &lt;code&gt;ollama run MODELNAME&lt;/code&gt; stage (&lt;code&gt;/set verbose&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;üß† What do you think?&lt;/p&gt; &lt;p&gt;ü§ó Are there any other small models that you use that you would like to share?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FieldMouseInTheHouse"&gt; /u/FieldMouseInTheHouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pz03dc/summary_of_vibe_coding_models_for_6gb_vram_systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pz03dc/summary_of_vibe_coding_models_for_6gb_vram_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pz03dc/summary_of_vibe_coding_models_for_6gb_vram_systems/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T22:21:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyp00p</id>
    <title>In which framework the OLLAMA GUI is written in?</title>
    <updated>2025-12-29T15:23:20+00:00</updated>
    <author>
      <name>/u/SpiritualQuality1055</name>
      <uri>https://old.reddit.com/user/SpiritualQuality1055</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like the new &lt;code&gt;ollama&lt;/code&gt; interface, its smooth and slick. I would like to know in which framework its written in?&lt;br /&gt; Is the code for the GUI could be found in the &lt;code&gt;ollama github&lt;/code&gt; repo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpiritualQuality1055"&gt; /u/SpiritualQuality1055 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyp00p/in_which_framework_the_ollama_gui_is_written_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyp00p/in_which_framework_the_ollama_gui_is_written_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pyp00p/in_which_framework_the_ollama_gui_is_written_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T15:23:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyoh9k</id>
    <title>Best grammar and sentence correction model on MacBook with 18GB RAM</title>
    <updated>2025-12-29T15:02:44+00:00</updated>
    <author>
      <name>/u/A-n-d-y-R-e-d</name>
      <uri>https://old.reddit.com/user/A-n-d-y-R-e-d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My MacBook has only 18 GB of RAM! &lt;/p&gt; &lt;p&gt;I am looking for an offline model that can take the text, understand the context, and rewrite it concisely while fixing grammatical issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A-n-d-y-R-e-d"&gt; /u/A-n-d-y-R-e-d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyoh9k/best_grammar_and_sentence_correction_model_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyoh9k/best_grammar_and_sentence_correction_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pyoh9k/best_grammar_and_sentence_correction_model_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T15:02:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyn9kw</id>
    <title>So hi all, i am currently playing with all this self hosted LLM (SLM in my case with my hardware limitations) im just using a Proxmox enviroment with Ollama installed direcly on a Ubuntu server container and on top of it Open WebUI to get the nice dashboard and to be able to create user accounts.</title>
    <updated>2025-12-29T14:11:58+00:00</updated>
    <author>
      <name>/u/Franceesios</name>
      <uri>https://old.reddit.com/user/Franceesios</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pyn9kw/so_hi_all_i_am_currently_playing_with_all_this/"&gt; &lt;img alt="So hi all, i am currently playing with all this self hosted LLM (SLM in my case with my hardware limitations) im just using a Proxmox enviroment with Ollama installed direcly on a Ubuntu server container and on top of it Open WebUI to get the nice dashboard and to be able to create user accounts." src="https://b.thumbs.redditmedia.com/iNf-j2OTzD0L4Rv9eBe773QoWp_aokC85y843xSV_Po.jpg" title="So hi all, i am currently playing with all this self hosted LLM (SLM in my case with my hardware limitations) im just using a Proxmox enviroment with Ollama installed direcly on a Ubuntu server container and on top of it Open WebUI to get the nice dashboard and to be able to create user accounts." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So far im using just these models &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w18f48hnh5ag1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c46e7759d8c3bb13d8238a4f1503ad3dd7620957"&gt;https://preview.redd.it/w18f48hnh5ag1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c46e7759d8c3bb13d8238a4f1503ad3dd7620957&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Llama3.2:1.2b&lt;/p&gt; &lt;p&gt;- Llama3.2:latest 3.2b&lt;/p&gt; &lt;p&gt;- Llama3.2:&lt;strong&gt;8b&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;- Ministral-3:8b&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;They are running ok at the time, the 8B ones would take atleast 2 minutes to give some proper answer, and ive also put this template for the models to remember with each answer they give out ;&lt;/p&gt; &lt;p&gt;### Task:&lt;/p&gt; &lt;p&gt;Respond to the user query using the provided context, incorporating inline citations in the format [id] **only when the &amp;lt;source&amp;gt; tag includes an explicit id attribute** (e.g., &amp;lt;source id=&amp;quot;1&amp;quot;&amp;gt;). Always include a confidence rating for your answer.&lt;/p&gt; &lt;p&gt;### Guidelines:&lt;/p&gt; &lt;p&gt;- Only provide answers you are confident in. Do not guess or invent information.&lt;/p&gt; &lt;p&gt;- If unsure or lacking sufficient information, respond with &amp;quot;I don‚Äôt know&amp;quot; or &amp;quot;I‚Äôm not sure.&amp;quot;&lt;/p&gt; &lt;p&gt;- Include a confidence rating from 1 to 5:&lt;/p&gt; &lt;p&gt;1 = very uncertain&lt;/p&gt; &lt;p&gt;2 = somewhat uncertain&lt;/p&gt; &lt;p&gt;3 = moderately confident&lt;/p&gt; &lt;p&gt;4 = confident&lt;/p&gt; &lt;p&gt;5 = very confident&lt;/p&gt; &lt;p&gt;- Respond in the same language as the user's query.&lt;/p&gt; &lt;p&gt;- If the context is unreadable or low-quality, inform the user and provide the best possible answer.&lt;/p&gt; &lt;p&gt;- If the answer isn‚Äôt present in the context but you possess the knowledge, explain this and provide the answer.&lt;/p&gt; &lt;p&gt;- Include inline citations [id] only when &amp;lt;source&amp;gt; has an id attribute.&lt;/p&gt; &lt;p&gt;- Do not use XML tags in your response.&lt;/p&gt; &lt;p&gt;- Ensure citations are concise and directly relevant.&lt;/p&gt; &lt;p&gt;- Do NOT use Web Search or external sources.&lt;/p&gt; &lt;p&gt;- If the context does not contain the answer, reply: ‚ÄòI don‚Äôt know‚Äô and Confidence 1‚Äì2.&lt;/p&gt; &lt;p&gt;### Example Output:&lt;/p&gt; &lt;p&gt;Answer: [Your answer here]&lt;/p&gt; &lt;p&gt;Confidence: [1-5]&lt;/p&gt; &lt;p&gt;### Context:&lt;/p&gt; &lt;p&gt;&amp;lt;context&amp;gt;&lt;/p&gt; &lt;p&gt;{{CONTEXT}}&lt;/p&gt; &lt;p&gt;&amp;lt;/context&amp;gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tbnk6bekh5ag1.png?width=1647&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38c75ac55e6951ca80a0f364fdcf8629379c69aa"&gt;https://preview.redd.it/tbnk6bekh5ag1.png?width=1647&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38c75ac55e6951ca80a0f364fdcf8629379c69aa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With so far works great, my primarly test right about now is the RAG method that Open WebUI offers, ive currently uploaded some invoices from this whole year worth of data as .MD files.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nchwh0kyh5ag1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a43d510aa7032f361dbfc7849903d1d87ba221a5"&gt;https://preview.redd.it/nchwh0kyh5ag1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a43d510aa7032f361dbfc7849903d1d87ba221a5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And asks the model (selecting the folder with the data first with # command/option) and i would get some good answers and some times some not so good answers but witj the confidence level accurate.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vqzwaupsh5ag1.png?width=559&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2737560e7562ccb31845f578e8ac89dbd42d33bb"&gt;https://preview.redd.it/vqzwaupsh5ag1.png?width=559&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2737560e7562ccb31845f578e8ac89dbd42d33bb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now my question is, if some tech company wants to implement these type of LLM (SML) into there on premise network for like finance department to use, is this a good start? How does some enterprise do it at the moment? Like sites like &lt;a href="http://llm.co"&gt;llm.co&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9knu91phh5ag1.png?width=1438&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a790870d44637e073b7807f3120306fdee8db623"&gt;https://preview.redd.it/9knu91phh5ag1.png?width=1438&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a790870d44637e073b7807f3120306fdee8db623&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far i can see real use case for this RAG method with some more powerfull hardware ofcourse, but let me know your real enterprise use case of a on-prem LLM RAG method. &lt;/p&gt; &lt;p&gt;Thanks all! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Franceesios"&gt; /u/Franceesios &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyn9kw/so_hi_all_i_am_currently_playing_with_all_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyn9kw/so_hi_all_i_am_currently_playing_with_all_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pyn9kw/so_hi_all_i_am_currently_playing_with_all_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T14:11:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz0y45</id>
    <title>CLI tool to use transformer and diffuser models</title>
    <updated>2025-12-29T22:55:47+00:00</updated>
    <author>
      <name>/u/zashboy</name>
      <uri>https://old.reddit.com/user/zashboy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zashboy"&gt; /u/zashboy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/huggingface/comments/1pz0wid/cli_tool_to_use_transformer_and_diffuser_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pz0y45/cli_tool_to_use_transformer_and_diffuser_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pz0y45/cli_tool_to_use_transformer_and_diffuser_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T22:55:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz7hpe</id>
    <title>Upload folders to a chat</title>
    <updated>2025-12-30T03:42:17+00:00</updated>
    <author>
      <name>/u/Cool-Condition466</name>
      <uri>https://old.reddit.com/user/Cool-Condition466</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a problem, im kinda new to this so bear with me. I have a mod for a game that i'm developing and I just hit a dead end so i'm trying to use ollama to see if it can help me. I wanted to upload the whole mod folder but it is not letting me do it instead it just uploads the python and txt files thar are scattered all over there. How can I upload the whole folder?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Condition466"&gt; /u/Cool-Condition466 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pz7hpe/upload_folders_to_a_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pz7hpe/upload_folders_to_a_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pz7hpe/upload_folders_to_a_chat/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-30T03:42:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pym7c0</id>
    <title>Running Ministral 3 3B Locally with Ollama and Adding Tool Calling (Local + Remote MCP)</title>
    <updated>2025-12-29T13:24:35+00:00</updated>
    <author>
      <name>/u/shricodev</name>
      <uri>https://old.reddit.com/user/shricodev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been seeing a lot of chatter around Ministral 3 3B, so I wanted to test it in a way that actually matters day to day. Can such a small local model do reliable tool calling, and can you extend it beyond local tools to work with remotely hosted MCP servers?&lt;/p&gt; &lt;p&gt;Here‚Äôs what I tried:&lt;/p&gt; &lt;h1&gt;Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Ran a quantized 4-bit (Q4_K_M) Ministral 3 3B on Ollama&lt;/li&gt; &lt;li&gt;Connected it to Open WebUI (with Docker)&lt;/li&gt; &lt;li&gt;Tested tool calling in two stages: &lt;ul&gt; &lt;li&gt;Local Python tools inside Open WebUI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Remote MCP tools&lt;/strong&gt; via Composio (so the model can call externally hosted tools through MCP)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model, despite the super tiny size of just 3B parameters, is said to support tool calling with even support for structured output. So, this was really fun to see the model in action.&lt;/p&gt; &lt;p&gt;Most of the guides show you how to work with just the local tools, which is not ideal when you plan to use the model for bigger, better and managed tools for hundreds of different services. &lt;/p&gt; &lt;p&gt;In this guide, I've covered the model specs and the entire setup, including setting up a Docker container for Ollama and running Ollama WebUI.&lt;/p&gt; &lt;p&gt;And the nice part is that the model setup guide here works for all the other models that support tool calling.&lt;/p&gt; &lt;p&gt;I wrote up the full walkthrough with commands and screenshots:&lt;/p&gt; &lt;p&gt;You can find it here: &lt;a href="https://composio.dev/blog/tool-calling-with-ministral-3b"&gt;MCP tool calling guide with Ministral 3B, Composio, and Ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone else has tested tool calling on Ministral 3 3B (or worked with it using vLLM instead of Ollama), I‚Äôd love to hear what worked best for you, as I couldn't get vLLM to work due to CUDA errors. :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shricodev"&gt; /u/shricodev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pym7c0/running_ministral_3_3b_locally_with_ollama_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pym7c0/running_ministral_3_3b_locally_with_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pym7c0/running_ministral_3_3b_locally_with_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T13:24:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzsz23</id>
    <title>Is Ollama Clouda good alternative to other api providers?</title>
    <updated>2025-12-30T20:33:40+00:00</updated>
    <author>
      <name>/u/Excellent_Piccolo848</name>
      <uri>https://old.reddit.com/user/Excellent_Piccolo848</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i was looking at ollama cloud, and thought, that it may be better than other api providers (like togehter ai or deepinfra), especially because of privacy. What are your thoughts on this and about ollama cloud in general?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Piccolo848"&gt; /u/Excellent_Piccolo848 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pzsz23/is_ollama_clouda_good_alternative_to_other_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pzsz23/is_ollama_clouda_good_alternative_to_other_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pzsz23/is_ollama_clouda_good_alternative_to_other_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-30T20:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzny3e</id>
    <title>OllamaFX Client - Add to Ollama oficial list of clients</title>
    <updated>2025-12-30T17:22:53+00:00</updated>
    <author>
      <name>/u/Electronic-Reason582</name>
      <uri>https://old.reddit.com/user/Electronic-Reason582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pzny3e/ollamafx_client_add_to_ollama_oficial_list_of/"&gt; &lt;img alt="OllamaFX Client - Add to Ollama oficial list of clients" src="https://a.thumbs.redditmedia.com/0fwiA1semiCg-bAWYJMGCKlvA47JkGDY4SeXMBgSsq8.jpg" title="OllamaFX Client - Add to Ollama oficial list of clients" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hola, estoy desarrollando un cliente JavafX para Ollama, se llama OllamaFX este es el repo en github &lt;a href="https://github.com/fredericksalazar/OllamaFX"&gt;https://github.com/fredericksalazar/OllamaFX&lt;/a&gt; me gustaria que mi cliente sea agregado en la lista de clientes oficiales de Ollama en su pagina de github, alguien puede indicarme como poder hacerlo? hay que seguir algun estandar o contactar a alguien? Muchas gracias&lt;/p&gt; &lt;p&gt;Hello, I'm developing a JavaFX client for Ollama called OllamaFX. Here's the repository on GitHub: &lt;a href="https://github.com/fredericksalazar/OllamaFX"&gt;https://github.com/fredericksalazar/OllamaFX&lt;/a&gt;. I'd like my client to be added to the list of official Ollama clients on their GitHub page. Can anyone tell me how to do this? Are there any standards I need to follow or someone I should contact? Thank you very much.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Reason582"&gt; /u/Electronic-Reason582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pzny3e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pzny3e/ollamafx_client_add_to_ollama_oficial_list_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pzny3e/ollamafx_client_add_to_ollama_oficial_list_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-30T17:22:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1q04s56</id>
    <title>Has anyone tried routing Claude Code CLI to multiple model providers?</title>
    <updated>2025-12-31T05:18:47+00:00</updated>
    <author>
      <name>/u/Dangerous-Dingo-5169</name>
      <uri>https://old.reddit.com/user/Dangerous-Dingo-5169</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm experimenting with running Claude Code CLI against different backends instead of a single API.&lt;/p&gt; &lt;p&gt;Specifically, I‚Äôm curious whether people have tried:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;using local models for simpler prompts&lt;/li&gt; &lt;li&gt;falling back to cloud models for harder requests&lt;/li&gt; &lt;li&gt;switching providers automatically when one fails&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I hacked together a local proxy to test this idea and it &lt;em&gt;seems&lt;/em&gt; to reduce API usage for normal dev workflows, but I‚Äôm not sure if I‚Äôm missing obvious downsides.&lt;/p&gt; &lt;p&gt;If anyone has experience doing something similar (Databricks, Azure, OpenRouter, Ollama, etc.), I‚Äôd love to hear what worked and what didn‚Äôt.&lt;/p&gt; &lt;p&gt;(If useful, I can share code ‚Äî didn‚Äôt want to lead with a link.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous-Dingo-5169"&gt; /u/Dangerous-Dingo-5169 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q04s56/has_anyone_tried_routing_claude_code_cli_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q04s56/has_anyone_tried_routing_claude_code_cli_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q04s56/has_anyone_tried_routing_claude_code_cli_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-31T05:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0blmz</id>
    <title>Wich model for philosophy / humanities on a MSI rtx 2060 Super (8Gb)?</title>
    <updated>2025-12-31T12:11:34+00:00</updated>
    <author>
      <name>/u/Excellent_Piccolo848</name>
      <uri>https://old.reddit.com/user/Excellent_Piccolo848</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Piccolo848"&gt; /u/Excellent_Piccolo848 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1q0blg9/wich_model_for_philosophy_humanities_on_a_msi_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0blmz/wich_model_for_philosophy_humanities_on_a_msi_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q0blmz/wich_model_for_philosophy_humanities_on_a_msi_rtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-31T12:11:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0iz2f</id>
    <title>M4 chip or older dedicated GPU?</title>
    <updated>2025-12-31T17:49:26+00:00</updated>
    <author>
      <name>/u/grtgbln</name>
      <uri>https://old.reddit.com/user/grtgbln</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grtgbln"&gt; /u/grtgbln &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1q0iypx/m4_chip_or_older_dedicated_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0iz2f/m4_chip_or_older_dedicated_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q0iz2f/m4_chip_or_older_dedicated_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-31T17:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0hwsc</id>
    <title>Built an offline-first vector database (v0.2.0) looking for real-world feedback</title>
    <updated>2025-12-31T17:04:49+00:00</updated>
    <author>
      <name>/u/Serious-Section-5595</name>
      <uri>https://old.reddit.com/user/Serious-Section-5595</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on &lt;strong&gt;SrvDB&lt;/strong&gt;, an &lt;strong&gt;offline embedded vector database&lt;/strong&gt; for local and edge AI use cases.&lt;/p&gt; &lt;p&gt;No cloud. No services. Just files on disk.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What‚Äôs new in v0.2.0:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multiple index modes: Flat, HNSW, IVF, PQ&lt;/li&gt; &lt;li&gt;Adaptive ‚ÄúAUTO‚Äù mode that selects index based on system RAM / dataset size&lt;/li&gt; &lt;li&gt;Exact search + quantized options (trade accuracy vs memory)&lt;/li&gt; &lt;li&gt;Benchmarks included (P99 latency, recall, disk, ingest)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Designed for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local RAG&lt;/li&gt; &lt;li&gt;Edge / IoT&lt;/li&gt; &lt;li&gt;Air-gapped systems&lt;/li&gt; &lt;li&gt;Developers experimenting without cloud dependencies&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Srinivas26k/srvdb"&gt;https://github.com/Srinivas26k/srvdb&lt;/a&gt;&lt;br /&gt; Benchmarks were run on a consumer laptop (details in repo).&lt;br /&gt; I have included the benchmark code run it on your and upload it on the GitHub discussions which helps to improve and add features accordingly. I request for contributors to make the project great.[ &lt;a href="https://github.com/Srinivas26k/srvdb/blob/master/universal_benchmark.py"&gt;https://github.com/Srinivas26k/srvdb/blob/master/universal_benchmark.py&lt;/a&gt; ]&lt;/p&gt; &lt;p&gt;I‚Äôm &lt;strong&gt;not trying to replace Pinecone / FAISS / Qdrant&lt;/strong&gt; this is for people who want something small, local, and predictable.&lt;/p&gt; &lt;p&gt;Would love:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Feedback on benchmarks&lt;/li&gt; &lt;li&gt;Real-world test reports&lt;/li&gt; &lt;li&gt;Criticism on design choices&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer technical questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious-Section-5595"&gt; /u/Serious-Section-5595 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0hwsc/built_an_offlinefirst_vector_database_v020/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0hwsc/built_an_offlinefirst_vector_database_v020/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q0hwsc/built_an_offlinefirst_vector_database_v020/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-31T17:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0n8h9</id>
    <title>EmergentFlow - Visual AI workflow builder with native Ollama support</title>
    <updated>2025-12-31T20:57:13+00:00</updated>
    <author>
      <name>/u/l33t-Mt</name>
      <uri>https://old.reddit.com/user/l33t-Mt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q0n8h9/emergentflow_visual_ai_workflow_builder_with/"&gt; &lt;img alt="EmergentFlow - Visual AI workflow builder with native Ollama support" src="https://b.thumbs.redditmedia.com/T05H1ug7tnfNFJyZodecXX2pKGWHW_D10QV95illc6M.jpg" title="EmergentFlow - Visual AI workflow builder with native Ollama support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1hjueesaslag1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01d473be20f1064fa77b522d54c8ac4702efd081"&gt;https://preview.redd.it/1hjueesaslag1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01d473be20f1064fa77b522d54c8ac4702efd081&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some of you might recognize me from my moondream/minicpm computer use agent posts, or maybe LlamaCards. Ive been tinkering with local AI stuff for a while now.&lt;/p&gt; &lt;p&gt;Im a single dad working full time, so my project time is scattered, but I finally got something to a point worth sharing.&lt;/p&gt; &lt;p&gt;EmergentFlow is a node-based AI workflow builder, but architecturally different from tools like n8n, Flowise, or ComfyUI. Those all run server-side on their cloud or you self-host the backend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EmergentFlow runs the execution engine in your browser.&lt;/strong&gt; Your browser tab is the runtime. When you connect Ollama, calls go directly from your browser to localhost:11434 (configurable). &lt;/p&gt; &lt;p&gt;It supports cloud APIs too (OpenAI, Anthropic, Google, etc.) if you want to mix local + cloud in the same flow. There's a Browser Agent for autonomous research, RAG pipelines, database connectors, hardware control.&lt;/p&gt; &lt;p&gt;Because I want new users to experience the system, I have provided anonymous users without an account, 50 free credits using googles cloud API, these are simply to allow users to see the system in action before requiring they create an account. &lt;/p&gt; &lt;p&gt;Terrified of launching, be gentle. &lt;/p&gt; &lt;p&gt;&lt;a href="https://emergentflow.io/"&gt;https://emergentflow.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Create visual flows directly from your browser.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/l33t-Mt"&gt; /u/l33t-Mt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0n8h9/emergentflow_visual_ai_workflow_builder_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0n8h9/emergentflow_visual_ai_workflow_builder_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q0n8h9/emergentflow_visual_ai_workflow_builder_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-31T20:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q17aha</id>
    <title>?</title>
    <updated>2026-01-01T15:52:28+00:00</updated>
    <author>
      <name>/u/Capital-Job-3592</name>
      <uri>https://old.reddit.com/user/Capital-Job-3592</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're building an observability platform specifically for Al agents and need your input.&lt;/p&gt; &lt;p&gt;The Problem:&lt;/p&gt; &lt;p&gt;Building Al agents that use multiple tools (files, APIs, databases) is getting easier with frameworks like LangChain, CrewAl, etc. But monitoring them? Total chaos.&lt;/p&gt; &lt;p&gt;When an agent makes 20 tool calls and something fails:&lt;/p&gt; &lt;p&gt;Which call failed? What was the error? How much did it cost? Why did the agent make that decision? What We're Building:&lt;/p&gt; &lt;p&gt;A unified observability layer that tracks:&lt;/p&gt; &lt;p&gt;LLM calls (tokens, cost, latency) Tool executions (success/fail/performance) Agent reasoning flow (step-by-step) MCP Server + REST API support The Question:&lt;/p&gt; &lt;p&gt;1.&lt;/p&gt; &lt;p&gt;How are you currently debugging Al agents? 2. What observability features do you wish existed? 3. Would you pay for a dedicated agent observability tool? We're looking for early adopters to test and shape the product&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Capital-Job-3592"&gt; /u/Capital-Job-3592 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q17aha/_/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q17aha/_/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q17aha/_/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-01T15:52:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0tmfl</id>
    <title>Tool Weaver (open sourced) inspired by Anthropic‚Äôs advanced tool use.</title>
    <updated>2026-01-01T02:30:52+00:00</updated>
    <author>
      <name>/u/andavan_ivan</name>
      <uri>https://old.reddit.com/user/andavan_ivan</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andavan_ivan"&gt; /u/andavan_ivan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/mcp/comments/1q0br8n/tool_weaver_open_sourced_inspired_by_anthropics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0tmfl/tool_weaver_open_sourced_inspired_by_anthropics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q0tmfl/tool_weaver_open_sourced_inspired_by_anthropics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-01T02:30:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0rzbw</id>
    <title>Local AI Memory System - Beta Testers Wanted (Ollama + DeepSeek + Knowledge Graphs)</title>
    <updated>2026-01-01T00:59:23+00:00</updated>
    <author>
      <name>/u/danny_094</name>
      <uri>https://old.reddit.com/user/danny_094</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q0rzbw/local_ai_memory_system_beta_testers_wanted_ollama/"&gt; &lt;img alt="Local AI Memory System - Beta Testers Wanted (Ollama + DeepSeek + Knowledge Graphs)" src="https://external-preview.redd.it/qaD9_KusmJTWBBNGXLe0E9F1-LYBD1V-XQC4RKqFj-0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44ee218300aca4f4e2c3c14df5d8a8d11309e2e7" title="Local AI Memory System - Beta Testers Wanted (Ollama + DeepSeek + Knowledge Graphs)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;**The Problem:*\&lt;/strong&gt;*&lt;/p&gt; &lt;p&gt; Your AI forgets everything between conversations. You end up re-explaining context every single time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;**The Solution:*\&lt;/strong&gt;* &lt;/p&gt; &lt;p&gt;I built &amp;quot;Jarvis&amp;quot; - a local AI assistant with actual long-term memory that works across conversations. And my latest pipeline update is the graph.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;**Example:*\&lt;/strong&gt;* ``` Day 1: &amp;quot;My favorite pizza is Tunfisch&amp;quot; Day 7: &amp;quot;What's my favorite pizza?&amp;quot; AI: &amp;quot;Your favorite pizza is Tunfisch-Pizza!&amp;quot; ‚úÖ ``` &lt;/p&gt; &lt;p&gt;&lt;strong&gt;**How it works:*\&lt;/strong&gt;*&lt;/p&gt; &lt;p&gt; - Semantic search finds relevant memories (not just keywords)&lt;/p&gt; &lt;p&gt; - Knowledge graph connects related facts - Auto-maintenance (deduplicates, merges similar entries) &lt;/p&gt; &lt;p&gt;- 100% local (your data stays on YOUR machine)&lt;/p&gt; &lt;p&gt; &lt;strong&gt;**Tech Stack:*\&lt;/strong&gt;* &lt;/p&gt; &lt;p&gt;- Ollama (DeepSeek-R1 for reasoning, Qwen for control) &lt;/p&gt; &lt;p&gt;- SQLite + vector embeddings &lt;/p&gt; &lt;p&gt;- Knowledge graphs with semantic/temporal edges &lt;/p&gt; &lt;p&gt;- MCP (Model Context Protocol) architecture&lt;/p&gt; &lt;p&gt; - Docker compose setup &lt;/p&gt; &lt;p&gt;&lt;strong&gt;**Current Status:*\&lt;/strong&gt;* &lt;/p&gt; &lt;p&gt;- 96.5% test coverage (57 passing tests) &lt;/p&gt; &lt;p&gt;- Graph-based memory optimization &lt;/p&gt; &lt;p&gt;-Cross-conversation retrieval working&lt;/p&gt; &lt;p&gt; - Automatic duplicate detection&lt;/p&gt; &lt;p&gt; - Production-ready (running on my Ubuntu server)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;**Looking for Beta Testers:*\&lt;/strong&gt;*&lt;/p&gt; &lt;p&gt; - Linux users comfortable with Docker &lt;/p&gt; &lt;p&gt;- Willing to use it for ~1 week&lt;/p&gt; &lt;p&gt; - Report bugs and memory accuracy&lt;/p&gt; &lt;p&gt; - Share feedback on usefulness &lt;/p&gt; &lt;p&gt;&lt;strong&gt;**What you get:*\&lt;/strong&gt;* &lt;/p&gt; &lt;p&gt;- Your own local AI with persistent memory&lt;/p&gt; &lt;p&gt; - Full data privacy (everything stays local) &lt;/p&gt; &lt;p&gt;- One-command Docker setup &lt;/p&gt; &lt;p&gt;- GitHub repo + documentation &lt;/p&gt; &lt;p&gt;&lt;strong&gt;**Why this matters:*\&lt;/strong&gt;* &lt;/p&gt; &lt;p&gt;Local AI is great for privacy, but current solutions forget context constantly. This bridges that gap - you get privacy AND memory. Interested? Comment below and I'll share: - GitHub repo - Setup instructions - Bug report template Looking forward to getting this in real users' hands! üöÄ &lt;/p&gt; &lt;p&gt;--- &lt;/p&gt; &lt;p&gt;&lt;strong&gt;**Edit:*\&lt;/strong&gt;* Just fixed a critical cross-conversation retrieval bug today - great timing for beta testing! üòÑ ```&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/danny094/Jarvis"&gt;https://github.com/danny094/Jarvis&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1q0rzbw/video/fb7n6q0dzmag1/player"&gt;https://reddit.com/link/1q0rzbw/video/fb7n6q0dzmag1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danny_094"&gt; /u/danny_094 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0rzbw/local_ai_memory_system_beta_testers_wanted_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0rzbw/local_ai_memory_system_beta_testers_wanted_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q0rzbw/local_ai_memory_system_beta_testers_wanted_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-01T00:59:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1ictw</id>
    <title>igpu + dgpu for reducing cpu load</title>
    <updated>2026-01-01T23:19:53+00:00</updated>
    <author>
      <name>/u/sultan_papagani</name>
      <uri>https://old.reddit.com/user/sultan_papagani</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i wanted to share my findings on using iGPU + dGPU to reduce cpu load during inference.&lt;/p&gt; &lt;p&gt;Prompt: write a booking website for hotels Model: gpt-oss:latest igpu: intel arrow lake integrated graphics dgpu: rtx5060 system ram: 32gb&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;CPU offloading + dGPU (cuda)&lt;/p&gt; &lt;p&gt;Size: 14GB&lt;br /&gt; Processor: 57% CPU / 43% GPU&lt;br /&gt; Context: 32K All 8 CPU cores fully utilized (100% per core) Total CPU load: ~33‚Äì47% Fans ramp up and system is loud&lt;/p&gt; &lt;p&gt;Total duration: 2m 42s Prompt eval: 73 tokens @ ~68 tok/s Generation: 3756 tokens @ ~25.7 tok/s&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;iGPU + dGPU only (vulkan)&lt;/p&gt; &lt;p&gt;Size: 14GB&lt;br /&gt; Processor: 100% GPU&lt;br /&gt; Context: 32K CPU usage drops to ~1‚Äì6% System stays quiet&lt;/p&gt; &lt;p&gt;Total duration: 10m 30s Prompt eval: 73 tokens @ ~46.8 tok/s Generation: 4213 tokens @ ~6.7 tok/s&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Running fully on iGPU + dGPU dramatically reduces CPU load and noise, but generation speed drops significantly. For long or non-interactive runs, this tradeoff can be worth it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sultan_papagani"&gt; /u/sultan_papagani &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q1ictw/igpu_dgpu_for_reducing_cpu_load/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q1ictw/igpu_dgpu_for_reducing_cpu_load/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q1ictw/igpu_dgpu_for_reducing_cpu_load/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-01T23:19:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1q179er</id>
    <title>Ollama models to specific GPU</title>
    <updated>2026-01-01T15:51:10+00:00</updated>
    <author>
      <name>/u/NormalSmoke1</name>
      <uri>https://old.reddit.com/user/NormalSmoke1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to hard force the OLLAMA model to specifically sit on a designated GPU. As I looked through the OLLAMA docs, it says to use the CUDA visible devices in the python script, but isn't there somewhere in the unix configuration I can set at startup? I have multiple 3090's and I would like to have the model on sit on one, so the other is free for other agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NormalSmoke1"&gt; /u/NormalSmoke1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q179er/ollama_models_to_specific_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q179er/ollama_models_to_specific_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q179er/ollama_models_to_specific_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-01T15:51:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1q23t1w</id>
    <title>Registry off or is my connection?</title>
    <updated>2026-01-02T16:54:44+00:00</updated>
    <author>
      <name>/u/OppenheimerDaSilva</name>
      <uri>https://old.reddit.com/user/OppenheimerDaSilva</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi fellas, since december of last year I cannot pull any image of ollama, I always receive timeout. It's something wth my connection?&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;ollama pull gpt-oss:20b ‚îÄ‚ïØ&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;Error: pull model manifest: Get &amp;quot;&lt;a href="https://registry.ollama.ai/v2/library/gpt-oss/manifests/20b%22:"&gt;https://registry.ollama.ai/v2/library/gpt-oss/manifests/20b&amp;quot;:&lt;/a&gt; dial tcp 172.67.182.229:443: i/o timeout&lt;br /&gt; ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OppenheimerDaSilva"&gt; /u/OppenheimerDaSilva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q23t1w/registry_off_or_is_my_connection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q23t1w/registry_off_or_is_my_connection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q23t1w/registry_off_or_is_my_connection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-02T16:54:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2dgwf</id>
    <title>Anyway to make joycaption into a chatbot?</title>
    <updated>2026-01-02T22:58:30+00:00</updated>
    <author>
      <name>/u/Zantorn</name>
      <uri>https://old.reddit.com/user/Zantorn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Complete noob here&lt;/p&gt; &lt;p&gt;Anyway to make joycaption into a chatbot? &lt;/p&gt; &lt;p&gt;Want to have it look at images and react to the, give opinions, have conversation about them etc. Is this possible to do locally? If so what should i use to get started? I have Ollama and LMStudio but not sure if those are the best options for this as im pretty new to&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zantorn"&gt; /u/Zantorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2dgwf/anyway_to_make_joycaption_into_a_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2dgwf/anyway_to_make_joycaption_into_a_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q2dgwf/anyway_to_make_joycaption_into_a_chatbot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-02T22:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1ud1h</id>
    <title>Does Open WebUI actually crawl links with Ollama, or is it just hallucinating based on the URL?</title>
    <updated>2026-01-02T09:32:31+00:00</updated>
    <author>
      <name>/u/Whole-Competition223</name>
      <uri>https://old.reddit.com/user/Whole-Competition223</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I recently started using &lt;strong&gt;Open WebUI&lt;/strong&gt; integrated with &lt;strong&gt;Ollama&lt;/strong&gt;. Today, I tried giving a specific URL to an LLM using the &lt;code&gt;#&lt;/code&gt; prefix and asked it to summarize the content in Korean.&lt;/p&gt; &lt;p&gt;At first, I was quite impressed because the summary looked very plausible and well-structured. However, I later found out that Ollama models, by default, cannot access the internet or visit external links.&lt;/p&gt; &lt;p&gt;This leaves me with a few questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;How did it generate the summary?&lt;/strong&gt; Was the LLM just &amp;quot;guessing&amp;quot; the content based on the words in the URL and its pre-existing training data? Or does Open WebUI pass some scraped metadata to the model?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Is there a way to enable &amp;quot;real&amp;quot; web browsing?&lt;/strong&gt; I want the model to actually visit the link and analyze the current page content. Are there specific functions, tools, or configurations in Open WebUI (like RAG settings) that allow Ollama models to access external websites?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'd love to hear how you guys handle web-based tasks with local LLMs. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Competition223"&gt; /u/Whole-Competition223 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q1ud1h/does_open_webui_actually_crawl_links_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q1ud1h/does_open_webui_actually_crawl_links_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q1ud1h/does_open_webui_actually_crawl_links_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-02T09:32:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2hp7u</id>
    <title>Integrated Mistral Nemo (12B) into a custom Space Discovery Engine (Project ARIS) for local anomaly detection.</title>
    <updated>2026-01-03T01:58:21+00:00</updated>
    <author>
      <name>/u/Limp-Regular3741</name>
      <uri>https://old.reddit.com/user/Limp-Regular3741</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share a real-world use case for local LLMs. I‚Äôve built a discovery engine called Project ARIS that uses Mistral Nemo as a reasoning layer for astronomical data.&lt;/p&gt; &lt;p&gt;The Stack:&lt;/p&gt; &lt;p&gt;Model: Mistral Nemo 12B (Q4_K_M) running via Ollama.&lt;/p&gt; &lt;p&gt;Hardware: Lenovo Yoga 7 (Ryzen AI 7, 24GB RAM) on Nobara Linux.&lt;/p&gt; &lt;p&gt;Integration: Tauri/Rust backend calling the Ollama API.&lt;/p&gt; &lt;p&gt;How I‚Äôm using the LLM:&lt;/p&gt; &lt;p&gt;Contextual Memory: It reads previous session reports from a local folder and greets me with a verbal recap on boot.&lt;/p&gt; &lt;p&gt;Intent Parsing: I built a custom terminal where Nemo translates &amp;quot;fuzzy&amp;quot; natural language into structured MAST API queries.&lt;/p&gt; &lt;p&gt;Anomaly Scoring: It parses spectral data to flag &amp;quot;out of the ordinary&amp;quot; signatures that don't fit standard star/planet profiles.&lt;/p&gt; &lt;p&gt;It‚Äôs amazing how much a 12B model can do when given a specific toolset and a sandboxed terminal. Happy to answer any questions about the Rust/Ollama bridge!&lt;/p&gt; &lt;p&gt;A preview of Project ARIS can be found here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/glowseedstudio/Project-ARIS"&gt;https://github.com/glowseedstudio/Project-ARIS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limp-Regular3741"&gt; /u/Limp-Regular3741 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2hp7u/integrated_mistral_nemo_12b_into_a_custom_space/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2hp7u/integrated_mistral_nemo_12b_into_a_custom_space/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q2hp7u/integrated_mistral_nemo_12b_into_a_custom_space/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T01:58:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2isyx</id>
    <title>Run Claude Code with ollama without losing any single feature offered by Anthropic backend</title>
    <updated>2026-01-03T02:47:39+00:00</updated>
    <author>
      <name>/u/Dangerous-Dingo-5169</name>
      <uri>https://old.reddit.com/user/Dangerous-Dingo-5169</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! Sharing an open-source project that might be useful:&lt;/p&gt; &lt;p&gt;Lynkr connects AI coding tools (like Claude Code) to multiple LLM providers with intelligent routing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous-Dingo-5169"&gt; /u/Dangerous-Dingo-5169 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2isyx/run_claude_code_with_ollama_without_losing_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2isyx/run_claude_code_with_ollama_without_losing_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q2isyx/run_claude_code_with_ollama_without_losing_any/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T02:47:39+00:00</published>
  </entry>
</feed>
