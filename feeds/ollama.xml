<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-26T06:47:15+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pt6u00</id>
    <title>Exclusive Holiday Offer! Perplexity AI PRO 1-Year Subscription ‚Äì Save 90%!</title>
    <updated>2025-12-22T18:22:15+00:00</updated>
    <author>
      <name>/u/A2uniquenickname</name>
      <uri>https://old.reddit.com/user/A2uniquenickname</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pt6u00/exclusive_holiday_offer_perplexity_ai_pro_1year/"&gt; &lt;img alt="Exclusive Holiday Offer! Perplexity AI PRO 1-Year Subscription ‚Äì Save 90%!" src="https://preview.redd.it/l7i791akss8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5880d65caad3ce59e1f5508d0383eef7130d534f" title="Exclusive Holiday Offer! Perplexity AI PRO 1-Year Subscription ‚Äì Save 90%!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get Perplexity AI PRO (1-Year) ‚Äì at 90% OFF!&lt;/p&gt; &lt;p&gt;Order here: &lt;a href="https://cheapgpts.store/Perplexity"&gt;CHEAPGPT.STORE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Plan: 12 Months&lt;/p&gt; &lt;p&gt;üí≥ Pay with: PayPal or Revolut or your favorite payment method&lt;/p&gt; &lt;p&gt;Reddit reviews: &lt;a href="https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu"&gt;FEEDBACK POST&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TrustPilot: &lt;a href="https://www.trustpilot.com/review/cheapgpt.store"&gt;TrustPilot FEEDBACK&lt;/a&gt; &lt;/p&gt; &lt;p&gt;NEW YEAR BONUS: Apply code PROMO5 for extra discount OFF your order! &lt;/p&gt; &lt;p&gt;BONUS!: Enjoy the AI Powered automated web browser. (Presented by Perplexity) included WITH YOUR PURCHASE!&lt;/p&gt; &lt;p&gt;Trusted and the cheapest! Check all feedbacks before you purchase&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A2uniquenickname"&gt; /u/A2uniquenickname &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l7i791akss8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pt6u00/exclusive_holiday_offer_perplexity_ai_pro_1year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pt6u00/exclusive_holiday_offer_perplexity_ai_pro_1year/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-22T18:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pske2v</id>
    <title>virtual pet / life simulation using Ollama and Unity 6</title>
    <updated>2025-12-21T23:32:22+00:00</updated>
    <author>
      <name>/u/rzarekta</name>
      <uri>https://old.reddit.com/user/rzarekta</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pske2v/virtual_pet_life_simulation_using_ollama_and/"&gt; &lt;img alt="virtual pet / life simulation using Ollama and Unity 6" src="https://external-preview.redd.it/eTdyYWZodTc2bjhnMXUjJLkZMiuDknoqj2U9nKzIooYTPd9cVtalvt_A-w2b.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc908aaaf303ce0483f79c3f1fb4417f43feaa87" title="virtual pet / life simulation using Ollama and Unity 6" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a virtual pet / life simulation in Unity 6, and it‚Äôs slowly turning into a living little ecosystem. This is a prototype, no fancy graphics or eye candy has been added. &lt;/p&gt; &lt;p&gt;Each creature is fully AI-driven, the AI controls all movement and decisions. They choose where to go, when to wander, when to eat, when to sleep, and when to interact. The green squares are food, and the purple rectangles are beds, which they seek out naturally based on their needs.&lt;/p&gt; &lt;p&gt;You can talk to the creatures individually, and they also talk amongst themselves. What you say to one creature can influence how it behaves and how it talks to others. Conversations aren‚Äôt isolated, they actually affect memory, mood, and social relationships.&lt;/p&gt; &lt;p&gt;You can also give direct commands like &lt;em&gt;stop&lt;/em&gt;, &lt;em&gt;go left&lt;/em&gt;, &lt;em&gt;go right&lt;/em&gt;, &lt;em&gt;follow&lt;/em&gt;, or &lt;em&gt;find another creature&lt;/em&gt;. The creatures don‚Äôt blindly obey, they evaluate each command based on personality, trust, current needs, and survival priorities, then respond honestly.&lt;/p&gt; &lt;p&gt;All AI logic and dialogue run fully locally using Ollama, on an RTX 2070 (8GB) AI server.&lt;/p&gt; &lt;p&gt;Watching emergent behavior form instead of scripting it has been wild.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzarekta"&gt; /u/rzarekta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9519oat76n8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pske2v/virtual_pet_life_simulation_using_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pske2v/virtual_pet_life_simulation_using_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T23:32:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt771o</id>
    <title>Prompt Injection demo in Ollama - help, please?</title>
    <updated>2025-12-22T18:36:00+00:00</updated>
    <author>
      <name>/u/West-Candy-5732</name>
      <uri>https://old.reddit.com/user/West-Candy-5732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, everyone. &lt;/p&gt; &lt;p&gt;I am working on my project for a Cybersecurity class and I would like to showcase the risks of Prompt Injection. I had this idea in my mind with many different things, but I wanted to actually start with something simple. However, even using small models like Phi3 or GPT2, I fail to actually override the system prompt (classic example of a translator agent, in my case English -&amp;gt; German), and get it to say &amp;quot;Haha, I got hacked!&amp;quot;. &lt;/p&gt; &lt;p&gt;Is there some prompt injection security in Ollama that I am not aware of? Can it be turned off?&lt;/p&gt; &lt;p&gt;Alternatively: do you guys have any better ideas how to demonstrate this? I tried using an API (Claude), but the results I got were not what I expected, quite quirky.&lt;/p&gt; &lt;p&gt;Thanks in advance for the help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/West-Candy-5732"&gt; /u/West-Candy-5732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pt771o/prompt_injection_demo_in_ollama_help_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pt771o/prompt_injection_demo_in_ollama_help_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pt771o/prompt_injection_demo_in_ollama_help_please/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-22T18:36:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt9tv0</id>
    <title>Title: Update: Yesterday it was 2D. Today, my Local Agent (Qwen 30B) figured out 3D Raycasting. Built from scratch in Python with no 3D engines.</title>
    <updated>2025-12-22T20:19:31+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pt9tv0/title_update_yesterday_it_was_2d_today_my_local/"&gt; &lt;img alt="Title: Update: Yesterday it was 2D. Today, my Local Agent (Qwen 30B) figured out 3D Raycasting. Built from scratch in Python with no 3D engines." src="https://external-preview.redd.it/ZDJjcXY5czlkdDhnMVpKx18zu2Al60haJHCIipoecy1_uH38KnrawZp01IuI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bbeb7e4bb3c56f42b0266cde29ca822530ff055" title="Title: Update: Yesterday it was 2D. Today, my Local Agent (Qwen 30B) figured out 3D Raycasting. Built from scratch in Python with no 3D engines." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following my previous post where the agent built a 2D tile engine, I pushed it to the next level: &lt;strong&gt;3D Raycasting.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Challenge:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create a Wolfenstein 3D style engine in pure Python (&lt;code&gt;pygame&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;No 3D libraries allowed, just raw math (Trigonometry).&lt;/li&gt; &lt;li&gt;Must handle wall collisions and perspective correction.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Result:&lt;/strong&gt; The agent (running on Qwen 30B via Ollama/LM Studio) successfully implemented the &lt;strong&gt;DDA Algorithm&lt;/strong&gt;. It initially struggled with a &amp;quot;barcode effect&amp;quot; and low FPS, but after a few autonomous feedback loops, it optimized the rendering to draw 4-pixel strips instead of single lines.&lt;/p&gt; &lt;p&gt;It also autonomously implemented &lt;strong&gt;Directional Shading&lt;/strong&gt; (lighter color for X-walls, darker for Y-walls) to give it that &amp;quot;Cyberpunk/Tron&amp;quot; depth.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/th2iyeo9dt8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pt9tv0/title_update_yesterday_it_was_2d_today_my_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pt9tv0/title_update_yesterday_it_was_2d_today_my_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-22T20:19:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptqyzh</id>
    <title>Local vs VPS...</title>
    <updated>2025-12-23T10:47:53+00:00</updated>
    <author>
      <name>/u/pagurix</name>
      <uri>https://old.reddit.com/user/pagurix</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pagurix"&gt; /u/pagurix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1ptp9dq/local_vs_vps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ptqyzh/local_vs_vps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ptqyzh/local_vs_vps/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-23T10:47:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptqvvh</id>
    <title>Ollama for 3D models</title>
    <updated>2025-12-23T10:42:12+00:00</updated>
    <author>
      <name>/u/Digital_Calendar_695</name>
      <uri>https://old.reddit.com/user/Digital_Calendar_695</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ptqvvh/ollama_for_3d_models/"&gt; &lt;img alt="Ollama for 3D models" src="https://external-preview.redd.it/zj7DSc3w-zwxzS2_x9K_PvO2eD7C1IjPQMGbnhyQXVU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e0fb3cbb460579fb92b2abb3e828cb0973f7f48" title="Ollama for 3D models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have check this video using local LLMs to create 3D models in Blender?&lt;/p&gt; &lt;p&gt;It seems small models cannot handle many tasks Has anyone tried bigger local models with MCP like this one?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Digital_Calendar_695"&gt; /u/Digital_Calendar_695 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/0PSOCFHBAfw?si=eDYokRcNPD5iYDBL"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ptqvvh/ollama_for_3d_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ptqvvh/ollama_for_3d_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-23T10:42:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1puhxd4</id>
    <title>Which is the smallest, fastest text generation model on ollama that can be used as a ai friend?</title>
    <updated>2025-12-24T07:52:32+00:00</updated>
    <author>
      <name>/u/Status_Yam_9212</name>
      <uri>https://old.reddit.com/user/Status_Yam_9212</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to have my own friend, somewhat similar to &lt;a href="http://c.ai"&gt;c.ai&lt;/a&gt;, but smaller, faster, and can run locally and fully offline. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Status_Yam_9212"&gt; /u/Status_Yam_9212 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puhxd4/which_is_the_smallest_fastest_text_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puhxd4/which_is_the_smallest_fastest_text_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1puhxd4/which_is_the_smallest_fastest_text_generation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T07:52:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu409q</id>
    <title>Ollama not outputing for Qwen3 80B Next Instruct, but works for Thinking model. Nothing in log.</title>
    <updated>2025-12-23T20:19:39+00:00</updated>
    <author>
      <name>/u/vulcan4d</name>
      <uri>https://old.reddit.com/user/vulcan4d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a weird issue where Ollama does not give me any output for Gwen3 Next 80B Instruct though it gives me token results. I see the same thing running in terminal. When I pull up the log I don't see anything useful. Anyone come accross something like this? Everything is on the latest version. I tried Q4 down to Q2 Quants, but the thinking version of this model works without any issues.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/27ooi0og209g1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55579ada7461fa7258cc1c6a908111b1fb957005"&gt;https://preview.redd.it/27ooi0og209g1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55579ada7461fa7258cc1c6a908111b1fb957005&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The log shows absolutely nothing useful&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ts6lb8t7309g1.png?width=1341&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84785ddb224466e38803a10a37f8d05bab3c08d7"&gt;Running from Open WebUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j9ujcugk309g1.png?width=1351&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b31d610451aa2550cba448960ec82e2c6b09c22"&gt;Running locally via terminal&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vulcan4d"&gt; /u/vulcan4d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pu409q/ollama_not_outputing_for_qwen3_80b_next_instruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pu409q/ollama_not_outputing_for_qwen3_80b_next_instruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pu409q/ollama_not_outputing_for_qwen3_80b_next_instruct/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-23T20:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pufqor</id>
    <title>DOOM JS: Master Protocol - The Power of 392 AI Patterns</title>
    <updated>2025-12-24T05:41:26+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pufqor/doom_js_master_protocol_the_power_of_392_ai/"&gt; &lt;img alt="DOOM JS: Master Protocol - The Power of 392 AI Patterns" src="https://external-preview.redd.it/Njk4ZnFsaDFhMzlnMQpHkqJe4EhnCoJ9VzNKO0zpC9YcnnCThFB-jTIXDZe8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=150cef2e178fc764e57e5aa784849834c45602bc" title="DOOM JS: Master Protocol - The Power of 392 AI Patterns" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This Christmas release represents a breakthrough in AI-driven development. By merging the collective intelligence of DeepSeek, Claude, and Perplexity into a library of 400 &lt;strong&gt;learned patterns&lt;/strong&gt;, I have eliminated random guessing and hallucinations.&lt;/p&gt; &lt;p&gt;What you see is a strictly governed horror engine:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Atmosphere:&lt;/strong&gt; Deep black background (0x000000) with calibrated fog layers for maximum tension.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Physics:&lt;/strong&gt; Hard-locked 1.6m eye-level gravity and relative FPS movement protocols.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI:&lt;/strong&gt; Aggressive yellow entities using unified chasing logic.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No more blind attempts. Just pure, structured execution. The AI is finally learning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vcabkr81a39g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pufqor/doom_js_master_protocol_the_power_of_392_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pufqor/doom_js_master_protocol_the_power_of_392_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T05:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1puguld</id>
    <title>ollama cannot run the model on Mac.</title>
    <updated>2025-12-24T06:45:59+00:00</updated>
    <author>
      <name>/u/Ok-Money-9173</name>
      <uri>https://old.reddit.com/user/Ok-Money-9173</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Metal library compilation error after macOS 26.2 / Xcode CLT update: bfloat/half type mismatch&lt;/p&gt; &lt;p&gt;Has anyone encountered the same error?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Money-9173"&gt; /u/Ok-Money-9173 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puguld/ollama_cannot_run_the_model_on_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puguld/ollama_cannot_run_the_model_on_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1puguld/ollama_cannot_run_the_model_on_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T06:45:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1puhr3d</id>
    <title>Now you can run local LLM inference with formal privacy guarantees</title>
    <updated>2025-12-24T07:41:27+00:00</updated>
    <author>
      <name>/u/IIITDkaLaunda</name>
      <uri>https://old.reddit.com/user/IIITDkaLaunda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1puhr3d/now_you_can_run_local_llm_inference_with_formal/"&gt; &lt;img alt="Now you can run local LLM inference with formal privacy guarantees" src="https://preview.redd.it/fb8lnvwns39g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6c3eb438356f4e3dd467972106afdcdbacf09b5" title="Now you can run local LLM inference with formal privacy guarantees" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IIITDkaLaunda"&gt; /u/IIITDkaLaunda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fb8lnvwns39g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puhr3d/now_you_can_run_local_llm_inference_with_formal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1puhr3d/now_you_can_run_local_llm_inference_with_formal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T07:41:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pun56y</id>
    <title>Which GPU should I use to caption ~50k images/day</title>
    <updated>2025-12-24T13:15:55+00:00</updated>
    <author>
      <name>/u/koteklidkapi</name>
      <uri>https://old.reddit.com/user/koteklidkapi</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koteklidkapi"&gt; /u/koteklidkapi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pun4kk/which_gpu_should_i_use_to_caption_50k_imagesday/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pun56y/which_gpu_should_i_use_to_caption_50k_imagesday/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pun56y/which_gpu_should_i_use_to_caption_50k_imagesday/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T13:15:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1puskwn</id>
    <title>Writing custom code to connect to llm api via Ollama and mTLS?</title>
    <updated>2025-12-24T17:27:18+00:00</updated>
    <author>
      <name>/u/Patladjan1738</name>
      <uri>https://old.reddit.com/user/Patladjan1738</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I am pretty new to Ollama and wanted to test it out, but I'm not sure if it can support my use case.&lt;/p&gt; &lt;p&gt;I have my own setup of an LLM API, running on a private server and secured via mTLS, so not just an api key but an api Id, a secret password, and I have to send a certificate and private key file in the payload. &lt;/p&gt; &lt;p&gt;I want to set up tools like langflow and dyad, but they dont seem to easily support all my custom auth code with cert and private key files. &lt;/p&gt; &lt;p&gt;But langflow and dyad do easily connect to Ollama.&lt;/p&gt; &lt;p&gt;Now I am thinking of setting up Ollama as a proxy server, where I can easily connect tools to Ollama, then Ollama can basically run my custom Python code to connect to my private llm server.&lt;/p&gt; &lt;p&gt;Has anyone ever done this with Ollama? Does anyone know if it's possible? What part of the documentation should I look into to kick start my implementation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Patladjan1738"&gt; /u/Patladjan1738 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puskwn/writing_custom_code_to_connect_to_llm_api_via/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puskwn/writing_custom_code_to_connect_to_llm_api_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1puskwn/writing_custom_code_to_connect_to_llm_api_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T17:27:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu6sgl</id>
    <title>I built a native Go runtime to give local Llama 3 "Real Hands" (File System + Browser)</title>
    <updated>2025-12-23T22:17:40+00:00</updated>
    <author>
      <name>/u/AgencySpecific</name>
      <uri>https://old.reddit.com/user/AgencySpecific</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Frustration: Running DeepSeek V3 or Llama 3 locally via Ollama is amazing, but let's be honest: they are &amp;quot;Brains in Jars.&amp;quot;&lt;/p&gt; &lt;p&gt;They can write incredible code, but they can't save it. They can plan research, but they can't browse the docs. I got sick of the &amp;quot;Chat -&amp;gt; Copy Code -&amp;gt; Alt-Tab -&amp;gt; Paste -&amp;gt; Error&amp;quot; loop.&lt;/p&gt; &lt;p&gt;The Project (Runiq): I didn't want another fragile Python wrapper that breaks my venv every week. So I built a standalone MCP Server in Go.&lt;/p&gt; &lt;p&gt;What it actually does:&lt;/p&gt; &lt;p&gt;File System Access: You prompt: &amp;quot;Refactor the ./src folder.&amp;quot; Runiq actually reads the files, sends the context to Ollama, and applies the edits locally.&lt;/p&gt; &lt;p&gt;Stealth Browser: You prompt: &amp;quot;Check the docs at stripe.com.&amp;quot; It spins up a headless browser (bypassing Cloudflare) to give the model real-time context.&lt;/p&gt; &lt;p&gt;The &amp;quot;Air Gap&amp;quot; Firewall: Giving a local model root is scary. Runiq intercepts every write or delete syscall. You get a native OS popup to approve the action. It can't wipe your drive unless you say yes.&lt;/p&gt; &lt;p&gt;Why Go?&lt;/p&gt; &lt;p&gt;Speed: It's instant.&lt;/p&gt; &lt;p&gt;Portability: Single 12MB binary. No pip install, no Docker.&lt;/p&gt; &lt;p&gt;Safety: Memory safe and strictly typed.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/qaysSE/runiq"&gt;https://github.com/qaysSE/runiq&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I built this to turn my local Ollama setup into a fully autonomous agent. Let me know what you think of the architecture.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AgencySpecific"&gt; /u/AgencySpecific &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pu6sgl/i_built_a_native_go_runtime_to_give_local_llama_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pu6sgl/i_built_a_native_go_runtime_to_give_local_llama_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pu6sgl/i_built_a_native_go_runtime_to_give_local_llama_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-23T22:17:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1puzyql</id>
    <title>Meetaugust Scored 100% in USMLE : outperforming OpenAI‚Äôs GPT - 5 and Google MedPaLM 2.</title>
    <updated>2025-12-24T23:19:33+00:00</updated>
    <author>
      <name>/u/vaibhavs8</name>
      <uri>https://old.reddit.com/user/vaibhavs8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1puzyql/meetaugust_scored_100_in_usmle_outperforming/"&gt; &lt;img alt="Meetaugust Scored 100% in USMLE : outperforming OpenAI‚Äôs GPT - 5 and Google MedPaLM 2." src="https://b.thumbs.redditmedia.com/tNks7nCpb3Q-gpPoKucKZ6jijHDMBzY_lgT9GPbdZcY.jpg" title="Meetaugust Scored 100% in USMLE : outperforming OpenAI‚Äôs GPT - 5 and Google MedPaLM 2." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i spent 3 years building Meetaugust and published research on benchmarking health AI accuracy. The goal was simple: make reliable health guidance accessible to anyone.&lt;/p&gt; &lt;p&gt;I know there are a lots of symptom checkers and health apps out there but most are not safe. I wanted something safe and conversational just explain your symptoms naturally and get clear answers.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;p&gt;* Analyzes symptoms through natural conversation (no checkboxes)&lt;/p&gt; &lt;p&gt;* Explains lab reports and prescriptions in simple terms&lt;/p&gt; &lt;p&gt;* Works in multiple languages via WhatsApp also (photos, voice, text)&lt;/p&gt; &lt;p&gt;* Helps determine if something needs urgent attention&lt;/p&gt; &lt;p&gt;* Stores your medical history as a &amp;quot;second brain&amp;quot;&lt;/p&gt; &lt;p&gt;* Available 24/7 for health questions&lt;/p&gt; &lt;p&gt;It won't prescribe medicines it's meant to help you understand your health and know when to see a doctor. We achieved 81.8% diagnostic accuracy in our research testing across 400 clinical cases.&lt;/p&gt; &lt;p&gt;free if anyone wants to try it : &lt;a href="https://www.meetaugust.ai/"&gt;https://www.meetaugust.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs8"&gt; /u/vaibhavs8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1puzyql"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puzyql/meetaugust_scored_100_in_usmle_outperforming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1puzyql/meetaugust_scored_100_in_usmle_outperforming/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T23:19:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pugkbg</id>
    <title>Self Hosted Alternative to NotebookLM</title>
    <updated>2025-12-24T06:28:53+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pugkbg/self_hosted_alternative_to_notebooklm/"&gt; &lt;img alt="Self Hosted Alternative to NotebookLM" src="https://external-preview.redd.it/VQoBiFueOCMY1op6qhV-TxY7TpiBx_VDJmILmMOmfX0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ea68aad29b25cc93508c57524884674c64e162b" title="Self Hosted Alternative to NotebookLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1pugkbg/video/939ag7c3j39g1/player"&gt;https://reddit.com/link/1pugkbg/video/939ag7c3j39g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be one of the open-source alternative to NotebookLM but connected to extra data sources.&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here's a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep Agent with Built-in Tools (knowledge base search, podcast generation, web scraping, link previews, image display)&lt;/li&gt; &lt;li&gt;Note Management (Notion like)&lt;/li&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi Collaborative Chats&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Installation (Self-Host)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Linux/macOS:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 \ -v surfsense-data:/data \ --name surfsense \ --restart unless-stopped \ ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Windows (PowerShell):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 ` -v surfsense-data:/data ` --name surfsense ` --restart unless-stopped ` ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pugkbg/self_hosted_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pugkbg/self_hosted_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pugkbg/self_hosted_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T06:28:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvgxr4</id>
    <title>Holiday Promo: Perplexity AI PRO Offer | 95% Cheaper!</title>
    <updated>2025-12-25T16:30:26+00:00</updated>
    <author>
      <name>/u/A2uniquenickname</name>
      <uri>https://old.reddit.com/user/A2uniquenickname</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pvgxr4/holiday_promo_perplexity_ai_pro_offer_95_cheaper/"&gt; &lt;img alt="Holiday Promo: Perplexity AI PRO Offer | 95% Cheaper!" src="https://preview.redd.it/e81patkcnd9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a149e002d4d1aba0004c26bf419b2b4c70699453" title="Holiday Promo: Perplexity AI PRO Offer | 95% Cheaper!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get Perplexity AI PRO (1-Year) ‚Äì at 90% OFF!&lt;/p&gt; &lt;p&gt;Order here: &lt;a href="https://cheapgpts.store/Perplexity"&gt;CHEAPGPT.STORE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Plan: 12 Months&lt;/p&gt; &lt;p&gt;üí≥ Pay with: PayPal or Revolut or your favorite payment method&lt;/p&gt; &lt;p&gt;Reddit reviews: &lt;a href="https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu"&gt;FEEDBACK POST&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TrustPilot: &lt;a href="https://www.trustpilot.com/review/cheapgpt.store"&gt;TrustPilot FEEDBACK&lt;/a&gt; &lt;/p&gt; &lt;p&gt;NEW YEAR BONUS: Apply code PROMO5 for extra discount OFF your order! &lt;/p&gt; &lt;p&gt;BONUS!: Enjoy the AI Powered automated web browser. (Presented by Perplexity) included WITH YOUR PURCHASE!&lt;/p&gt; &lt;p&gt;Trusted and the cheapest! Check all feedbacks before you purchase&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A2uniquenickname"&gt; /u/A2uniquenickname &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e81patkcnd9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pvgxr4/holiday_promo_perplexity_ai_pro_offer_95_cheaper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pvgxr4/holiday_promo_perplexity_ai_pro_offer_95_cheaper/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T16:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv9vra</id>
    <title>What models are compatible with the Goose agent?</title>
    <updated>2025-12-25T09:38:39+00:00</updated>
    <author>
      <name>/u/Ok_Imagination_1571</name>
      <uri>https://old.reddit.com/user/Ok_Imagination_1571</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I installed goose-cli 1.18.0 and ollama 0.12.6.&lt;/p&gt; &lt;p&gt;Goose configuration has an option for local ollama provider.&lt;/p&gt; &lt;p&gt;Goose definitely connects to ollama server and shows list of models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;NAME ID SIZE MODIFIED qwen2.5:0.5b a8b0c5157701 397 MB 12 minutes ago deepseek-r1:8b 6995872bfe4c 5.2 GB 11 hours ago phi3:mini 4f2222927938 2.2 GB 14 hours ago There are 3 models on the list but Goose is only able to complete the configuration successfully with Qwen. Is there a criteria to check compatibility of an ollama model upfront with Goose without wasting traffic and time? &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Imagination_1571"&gt; /u/Ok_Imagination_1571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv9vra/what_models_are_compatible_with_the_goose_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv9vra/what_models_are_compatible_with_the_goose_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pv9vra/what_models_are_compatible_with_the_goose_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T09:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pulykd</id>
    <title>Qwen3:4b Too Many Model thoughts to respond to a simple "hi"</title>
    <updated>2025-12-24T12:10:03+00:00</updated>
    <author>
      <name>/u/slow-fast-person</name>
      <uri>https://old.reddit.com/user/slow-fast-person</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pulykd/qwen34b_too_many_model_thoughts_to_respond_to_a/"&gt; &lt;img alt="Qwen3:4b Too Many Model thoughts to respond to a simple &amp;quot;hi&amp;quot;" src="https://preview.redd.it/nfzkw0ex759g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8cdeebcba38e88484dad342114e070ce2c9b6c93" title="Qwen3:4b Too Many Model thoughts to respond to a simple &amp;quot;hi&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is quite hilarious on how the model does not have adaptive chain of thought and puts so much work in something as simple as a &amp;quot;hi&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slow-fast-person"&gt; /u/slow-fast-person &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nfzkw0ex759g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pulykd/qwen34b_too_many_model_thoughts_to_respond_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pulykd/qwen34b_too_many_model_thoughts_to_respond_to_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T12:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvkm0h</id>
    <title>Local LLMs unstable</title>
    <updated>2025-12-25T19:19:25+00:00</updated>
    <author>
      <name>/u/OcelotOk5761</name>
      <uri>https://old.reddit.com/user/OcelotOk5761</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pvkm0h/local_llms_unstable/"&gt; &lt;img alt="Local LLMs unstable" src="https://b.thumbs.redditmedia.com/2yKwQ8y3RFiqDiG8AS4WIn9Y6uYD_O398t8q4oQvGck.jpg" title="Local LLMs unstable" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've been having problems with local LLms recently. I cannot tell if its an ollama issue or specifically an open-webui issue.&lt;/p&gt; &lt;p&gt;Firstly: Some of the models are very buggy, take almost a minute to process and are having problems returning outputs specifically with Qwen3-14B or any 'thinking' model in-fact. they take ages to load (even on GPU) and begin processing. when they do, the model sometimes keeps getting stuck in thinking loops or outright refuses to unload when asked to.&lt;/p&gt; &lt;p&gt;Second: When trying out Qwen3-vl from Ollama even with all the updates and when used in open-webui, the model is outright unusable for me, it either keeps thinking forever or refuses to load, or even refuses to unload making me have to open the terminal to kill with sudo. Rinse and repeat.&lt;/p&gt; &lt;p&gt;Has anyone been having problems recently or is it just me? I am running open-webui through pip (I don't like docker) and it's been very frustrating to use. I really don't know if it's an ollama issue or an open-webui issue.&lt;/p&gt; &lt;p&gt;P.S: I am using Linux (not sure if it's a Linux issue or not)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/30mxo17gff9g1.png?width=494&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0bb43d89fb8b99e213534f9ae1d9e6e42e2fc9e"&gt;https://preview.redd.it/30mxo17gff9g1.png?width=494&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0bb43d89fb8b99e213534f9ae1d9e6e42e2fc9e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nice one man. Idk what to even say. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OcelotOk5761"&gt; /u/OcelotOk5761 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pvkm0h/local_llms_unstable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pvkm0h/local_llms_unstable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pvkm0h/local_llms_unstable/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T19:19:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pveom2</id>
    <title>Interesting...</title>
    <updated>2025-12-25T14:39:22+00:00</updated>
    <author>
      <name>/u/EggDroppedSoup</name>
      <uri>https://old.reddit.com/user/EggDroppedSoup</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pveom2/interesting/"&gt; &lt;img alt="Interesting..." src="https://preview.redd.it/v6zdqwxh3d9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0e4fc9a05bc15c73b2018a2cf6767be32fe277b" title="Interesting..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EggDroppedSoup"&gt; /u/EggDroppedSoup &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v6zdqwxh3d9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pveom2/interesting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pveom2/interesting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T14:39:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvfv1h</id>
    <title>Distributed Cognition and Context Control: gait and gaithub</title>
    <updated>2025-12-25T15:39:14+00:00</updated>
    <author>
      <name>/u/automateyournetwork</name>
      <uri>https://old.reddit.com/user/automateyournetwork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pvfv1h/distributed_cognition_and_context_control_gait/"&gt; &lt;img alt="Distributed Cognition and Context Control: gait and gaithub" src="https://external-preview.redd.it/QboS9f2QUI2oU5tnJHS_qXaoU7qrXkIcqyb5tZ65XrE.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=181344eb7ee38bf3d107b3dca75beba6ca77d8c1" title="Distributed Cognition and Context Control: gait and gaithub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the last few weeks, I‚Äôve been building - and just finished demoing - something I think we‚Äôre going to look back on as obvious in hindsight. &lt;/p&gt; &lt;p&gt;Distributed Cognition. Decentralized context control. &lt;/p&gt; &lt;p&gt;GAIT + GaitHub &lt;/p&gt; &lt;p&gt;A Git-like system ‚Äî but not for code. &lt;/p&gt; &lt;p&gt;For AI reasoning, memory, and context. &lt;/p&gt; &lt;p&gt;We‚Äôve spent decades perfecting how we:&lt;br /&gt; ‚Ä¢ version code&lt;br /&gt; ‚Ä¢ review changes&lt;br /&gt; ‚Ä¢ collaborate safely&lt;br /&gt; ‚Ä¢ reproduce results &lt;/p&gt; &lt;p&gt;And yet today, we let LLMs:&lt;br /&gt; ‚Ä¢ make architectural decisions&lt;br /&gt; ‚Ä¢ generate production content&lt;br /&gt; ‚Ä¢ influence real systems&lt;br /&gt; ‚Ä¶with almost no version control at all. &lt;/p&gt; &lt;p&gt;Chat logs aren‚Äôt enough. &lt;/p&gt; &lt;p&gt;Prompt files aren‚Äôt enough. &lt;/p&gt; &lt;p&gt;Screenshots definitely aren‚Äôt enough. &lt;/p&gt; &lt;p&gt;So I built something different. &lt;/p&gt; &lt;p&gt;What GAIT actually versions &lt;/p&gt; &lt;p&gt;GAIT treats AI interactions as first-class, content-addressed objects. &lt;/p&gt; &lt;p&gt;That includes:&lt;br /&gt; ‚Ä¢ user intent&lt;br /&gt; ‚Ä¢ model responses&lt;br /&gt; ‚Ä¢ memory state&lt;br /&gt; ‚Ä¢ branches of reasoning&lt;br /&gt; ‚Ä¢ resumable conversations &lt;/p&gt; &lt;p&gt;Every turn is hashed. Every decision is traceable. Every outcome is reproducible. &lt;/p&gt; &lt;p&gt;If Git solved ‚Äúit worked on my machine,‚Äù &lt;/p&gt; &lt;p&gt;GAIT solves ‚Äúwhy did the AI decide that?‚Äù &lt;/p&gt; &lt;p&gt;The demo (high-level walkthrough) &lt;/p&gt; &lt;p&gt;I recorded a full end-to-end demo showing how this works in practice: &lt;/p&gt; &lt;p&gt;Start in a clean folder ‚Äî no server, no UI &lt;/p&gt; &lt;p&gt;* Initialize GAIT locally&lt;br /&gt; * Run an AI chat session that‚Äôs automatically tracked&lt;br /&gt; * Ask a real, non-trivial technical question&lt;br /&gt; * Inspect the reasoning log&lt;br /&gt; * Resume the conversation later ‚Äî exactly where it left off&lt;br /&gt; * Branch the reasoning into alternate paths&lt;br /&gt; * Verify object integrity and state&lt;br /&gt; * Add a remote (GaitHub)&lt;br /&gt; * Create a remote repo from the CLI&lt;br /&gt; * Authenticate with a simple token&lt;br /&gt; * Push AI reasoning to the cloud&lt;br /&gt; * Fork another repo‚Äôs reasoning&lt;br /&gt; * Open a pull request on ideas, not code&lt;br /&gt; * Merge reasoning deterministically &lt;/p&gt; &lt;p&gt;No magic. No hidden state. No ‚Äútrust me, the model said so.‚Äù &lt;/p&gt; &lt;p&gt;Why this matters (especially for enterprises). AI is no longer a toy. &lt;/p&gt; &lt;p&gt;It‚Äôs:&lt;br /&gt; ‚Ä¢ part of decision pipelines&lt;br /&gt; ‚Ä¢ embedded in workflows&lt;br /&gt; ‚Ä¢ influencing customers, networks, and systems &lt;/p&gt; &lt;p&gt;But we can‚Äôt:&lt;br /&gt; ‚Ä¢ audit it&lt;br /&gt; ‚Ä¢ diff it&lt;br /&gt; ‚Ä¢ reproduce it&lt;br /&gt; ‚Ä¢ roll it back &lt;/p&gt; &lt;p&gt;That‚Äôs not sustainable. &lt;/p&gt; &lt;p&gt;GAIT introduces:&lt;br /&gt; ‚Ä¢ reproducible AI workflows&lt;br /&gt; ‚Ä¢ auditable reasoning history&lt;br /&gt; ‚Ä¢ collaborative cognition&lt;br /&gt; ‚Ä¢ local-first, cloud-optional design &lt;/p&gt; &lt;p&gt;This is infrastructure ‚Äî not a chatbot wrapper. This is not ‚ÄúGitHub for prompts‚Äù. That framing misses the point. &lt;/p&gt; &lt;p&gt;This is Git for cognition. &lt;/p&gt; &lt;p&gt;From:&lt;br /&gt; ‚Ä¢ commits ‚Üí conversations&lt;br /&gt; ‚Ä¢ diffs ‚Üí decisions&lt;br /&gt; ‚Ä¢ branches ‚Üí alternate reasoning&lt;br /&gt; ‚Ä¢ merges ‚Üí shared understanding &lt;/p&gt; &lt;p&gt;I genuinely believe version control for AI reasoning will become as fundamental as version control for source code. &lt;/p&gt; &lt;p&gt;The question isn‚Äôt if. &lt;/p&gt; &lt;p&gt;It‚Äôs who builds it correctly. &lt;/p&gt; &lt;p&gt;I‚Äôm excited to keep pushing this forward ‚Äî openly, transparently, and with the community. &lt;/p&gt; &lt;p&gt;More demos, docs, and real-world use cases coming soon. &lt;/p&gt; &lt;p&gt;If this resonates with you, I‚Äôd love to hear your thoughts üëá&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/automateyournetwork"&gt; /u/automateyournetwork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=0PyFHsYxjbk&amp;amp;si=ugLwYfnV_ETZ_VSR"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pvfv1h/distributed_cognition_and_context_control_gait/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pvfv1h/distributed_cognition_and_context_control_gait/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T15:39:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv33vq</id>
    <title>Llama 3.2 refuses to analyze dark web threat intel. Need uncensored 7B recommendations</title>
    <updated>2025-12-25T02:19:49+00:00</updated>
    <author>
      <name>/u/Loud-Goal190</name>
      <uri>https://old.reddit.com/user/Loud-Goal190</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm crawling onion sites for a defensive threat intel tool, but my local LLM (Llama 3.2) refuses to analyze the raw text due to safety filters. It sees &amp;quot;leak&amp;quot; or &amp;quot;.onion&amp;quot; and shuts down, even with jailbreak prompts. Regex captures emails but misses the context (like company names or data volume). Any recommendations for an uncensored 7B model that handles this well, or should I switch to a BERT model for extraction?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud-Goal190"&gt; /u/Loud-Goal190 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv33vq/llama_32_refuses_to_analyze_dark_web_threat_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv33vq/llama_32_refuses_to_analyze_dark_web_threat_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pv33vq/llama_32_refuses_to_analyze_dark_web_threat_intel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T02:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv71pg</id>
    <title>Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16.</title>
    <updated>2025-12-25T06:23:45+00:00</updated>
    <author>
      <name>/u/Double-Primary-2871</name>
      <uri>https://old.reddit.com/user/Double-Primary-2871</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pv71pg/finetuning_gptoss20b_on_a_ryzen_5950x_because/"&gt; &lt;img alt="Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16." src="https://preview.redd.it/6w9h1554na9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2621fb072eeb16ce8a0f4599cdbbfeb44b9f1c90" title="Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;at 1am.&lt;/p&gt; &lt;p&gt;I am fine-tuning my personal AI, into a gpt-oss-20b model, via LoRA, on a Ryzen 5950x CPU.&lt;/p&gt; &lt;p&gt;I had to pain stakingly deal with massive axolotl errors, venv and python version hell, yaml misconfigs, even fought with my other ai assistant, whom literally told me this couldn't be done on my system.... for hours and hours, for over a week.&lt;/p&gt; &lt;p&gt;Can't fine-tune with my radeon 7900XT because of bf16 kernel issues with ROCm on axolotl. I literally even tried to rent an h100 to help, and ran into serious roadblocks.&lt;/p&gt; &lt;p&gt;So the soultion was for me to convert the mxfp4 (bf16 format) weights back to fp32 and tell axolotl to stop downcasting back fp16.&lt;/p&gt; &lt;p&gt;Sure this will take days to compute all three of the shards, but after days of banging my head against the nearest convenient wall and keyboard, I finally got this s-o-b to work.&lt;/p&gt; &lt;p&gt;üòÅ also hi, new here. just wanted to share my story.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Double-Primary-2871"&gt; /u/Double-Primary-2871 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6w9h1554na9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv71pg/finetuning_gptoss20b_on_a_ryzen_5950x_because/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pv71pg/finetuning_gptoss20b_on_a_ryzen_5950x_because/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T06:23:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv88yv</id>
    <title>I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier models</title>
    <updated>2025-12-25T07:44:21+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pv88yv/i_built_planoa3b_most_efficient_llms_for_agent/"&gt; &lt;img alt="I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier models" src="https://preview.redd.it/059dttgf1b9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=898f1ffead5f76ea7c739cebaf5d8c1a413e3efc" title="I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone ‚Äî I‚Äôm on the Katanemo research team. Today we‚Äôre thrilled to launch &lt;strong&gt;Plano-Orchestrator&lt;/strong&gt;, a new family of LLMs built for fast multi-agent orchestration.&lt;/p&gt; &lt;p&gt;What do these new LLMs do? given a user request and the conversation context, Plano-Orchestrator decides which agent(s) should handle the request and in what sequence. In other words, it acts as the supervisor agent in a multi-agent system. Designed for multi-domain scenarios, it works well across general chat, coding tasks, and long, multi-turn conversations, while staying efficient enough for low-latency production deployments.&lt;/p&gt; &lt;p&gt;Why did we built this? Our applied research is focused on helping teams deliver agents safely and efficiently, with better real-world performance and latency ‚Äî the kind of ‚Äúglue work‚Äù that usually sits outside any single agent‚Äôs core product logic.&lt;/p&gt; &lt;p&gt;Plano-Orchestrator is integrated into Plano, our models-native proxy and dataplane for agents. Hope you enjoy it ‚Äî and we‚Äôd love feedback from anyone building multi-agent systems&lt;/p&gt; &lt;p&gt;Learn more about the LLMs &lt;a href="https://huggingface.co/collections/katanemo/plano-orchestrator"&gt;here&lt;/a&gt;&lt;br /&gt; About our open source project: &lt;a href="https://github.com/katanemo/plano"&gt;https://github.com/katanemo/plano&lt;/a&gt;&lt;br /&gt; And about our research: &lt;a href="https://planoai.dev/research"&gt;https://planoai.dev/research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/059dttgf1b9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv88yv/i_built_planoa3b_most_efficient_llms_for_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pv88yv/i_built_planoa3b_most_efficient_llms_for_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T07:44:21+00:00</published>
  </entry>
</feed>
