<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-06T21:34:48+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pdw272</id>
    <title>Ollama powered chat component for any website</title>
    <updated>2025-12-04T09:51:52+00:00</updated>
    <author>
      <name>/u/ovi_nation</name>
      <uri>https://old.reddit.com/user/ovi_nation</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pdw272/ollama_powered_chat_component_for_any_website/"&gt; &lt;img alt="Ollama powered chat component for any website" src="https://preview.redd.it/d7xbh8i2t55g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=514ff5f50122295dad7147a13b43b905a32c3830" title="Ollama powered chat component for any website" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! I have open sourced a project called Deep Chat. It is a feature-rich chat web component that can be used to connect and converse with Ollama models.&lt;/p&gt; &lt;p&gt;Check it out at:&lt;br /&gt; &lt;a href="https://github.com/OvidijusParsiunas/deep-chat?fbclid=IwAR0uSvTiVXL5rICg3YfKqV2er0E355LGrg5ha6JVkUEaem8PKU98sU6ysbE"&gt;https://github.com/OvidijusParsiunas/deep-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A GitHub star is ALWAYS appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ovi_nation"&gt; /u/ovi_nation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d7xbh8i2t55g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdw272/ollama_powered_chat_component_for_any_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdw272/ollama_powered_chat_component_for_any_website/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T09:51:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdv7ya</id>
    <title>Quick real coding test GPT-OS-20b-Code vs. Ministral-3b-fp16</title>
    <updated>2025-12-04T08:56:20+00:00</updated>
    <author>
      <name>/u/New_Cranberry_6451</name>
      <uri>https://old.reddit.com/user/New_Cranberry_6451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few minutes ago I downloaded Ministral-3b-fp16 and plugged it to my 8Gb VRAM laptop running Windows and Ollama. Just a plain 0.13.1 version on a plain standard laptop.&lt;/p&gt; &lt;p&gt;I needed a simple PHP script to export rows from a mysql db to CSV, but I had a function that failed escaping when using &amp;quot;,&amp;quot; as delim.&lt;/p&gt; &lt;p&gt;I asked Ministral and obtained a classic working function that I will resume as (skipped to focus on point):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$header = str_getcsv(implode(',', array_keys($export_data[0])), ','); $csv_data = implode($csv_delim, $header) . &amp;quot;\r\n&amp;quot;; [...] // Escape commas and quotes in values $escaped_value = preg_replace('/\s+/', ' ', $value); $escaped_value = str_replace(',', ' ', $escaped_value); $escaped_value = str_replace('&amp;quot;', '&amp;quot;&amp;quot;', $escaped_value); $row_values[] = '&amp;quot;' . $escaped_value . '&amp;quot;'; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Cool. It works. Now I paste the exact same prompt into GPT-OS-20b-Code: (once again, skipped to focus on point)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// Open a temporary memory stream $fp = fopen( 'php://temp', 'r+' ); [...] fputcsv( $fp, $row, $csv_delim ); rewind( $fp ); $csv_data = stream_get_contents( $fp ); fclose( $fp ); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;WTF! This is so elegant. It delegates escaping to a core function so that the regExp-s, that are prone to fail in weird cases, are not even needed, and on the other hand, the way of treating the array as a temp stream to read and rewind blew my mind.&lt;/p&gt; &lt;p&gt;One shot prompts like these make me say THANKS for this open source gifts I can run locally and privately :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Cranberry_6451"&gt; /u/New_Cranberry_6451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdv7ya/quick_real_coding_test_gptos20bcode_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdv7ya/quick_real_coding_test_gptos20bcode_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdv7ya/quick_real_coding_test_gptos20bcode_vs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T08:56:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe1dba</id>
    <title>Hito 1.7 GGUF release</title>
    <updated>2025-12-04T14:27:29+00:00</updated>
    <author>
      <name>/u/TastyWriting8360</name>
      <uri>https://old.reddit.com/user/TastyWriting8360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I will leave this here open wieghts &lt;a href="https://huggingface.co/hitonet/hito-1.7b"&gt;https://huggingface.co/hitonet/hito-1.7b&lt;/a&gt; and gguf release &lt;a href="https://huggingface.co/hitonet/hito-1.7b-GGUFI"&gt;https://huggingface.co/hitonet/hito-1.7b-GGUFI&lt;/a&gt; will make a proper post in the next few hours, gotta catch some sleep but cant wait to share this with you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TastyWriting8360"&gt; /u/TastyWriting8360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe1dba/hito_17_gguf_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe1dba/hito_17_gguf_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe1dba/hito_17_gguf_release/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T14:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe2tk3</id>
    <title>ministral-3 is not using gpu in ollama</title>
    <updated>2025-12-04T15:26:18+00:00</updated>
    <author>
      <name>/u/AdhesivenessLatter57</name>
      <uri>https://old.reddit.com/user/AdhesivenessLatter57</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;why ministral-3 is running on cpu only with ollama version is 0.13.1? &lt;/p&gt; &lt;p&gt;this model starts loading in gpu and later offloads to cpu.&lt;/p&gt; &lt;p&gt;I tried 3b,8b and 14b, while qwen3-coder is running fine in same gpu. &lt;/p&gt; &lt;p&gt;Some issue in ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdhesivenessLatter57"&gt; /u/AdhesivenessLatter57 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2tk3/ministral3_is_not_using_gpu_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2tk3/ministral3_is_not_using_gpu_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe2tk3/ministral3_is_not_using_gpu_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T15:26:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe5b1t</id>
    <title>Computer Use with Claude Opus 4.5</title>
    <updated>2025-12-04T17:01:11+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pe5b1t/computer_use_with_claude_opus_45/"&gt; &lt;img alt="Computer Use with Claude Opus 4.5" src="https://external-preview.redd.it/eWc1Z2J3ZG94NzVnMY644GvbBlUugVuvecrsXUzyNOCzfLoiwHL0f-lwE9G6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64ec98da6d0fbfaba8c88013ccf90ed592f5e91c" title="Computer Use with Claude Opus 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude Opus 4.5 support to the Cua VLM Router and Playground - and you can already see it running inside Windows sandboxes. Early results are seriously impressive, even on tricky desktop workflows.&lt;/p&gt; &lt;p&gt;Benchmark results:&lt;/p&gt; &lt;p&gt;-new SOTA 66.3% on OSWorld (beats Sonnet 4.5’s 61.4% in the general model category)&lt;/p&gt; &lt;p&gt;-88.9% on tool-use&lt;/p&gt; &lt;p&gt;Better reasoning. More reliable multi-step execution.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try the playground here : &lt;a href="https://cua.ai"&gt;https://cua.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9q65hzoox75g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe5b1t/computer_use_with_claude_opus_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe5b1t/computer_use_with_claude_opus_45/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T17:01:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe0s1q</id>
    <title>New Feature in RAGLight: Multimodal PDF Ingestion</title>
    <updated>2025-12-04T14:02:16+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"&gt; &lt;img alt="New Feature in RAGLight: Multimodal PDF Ingestion" src="https://b.thumbs.redditmedia.com/UkmGLN7LsRnojkAulEBihmGFp1HrsIPIMO8GS0KQ9ko.jpg" title="New Feature in RAGLight: Multimodal PDF Ingestion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I just added a small but powerful feature to &lt;strong&gt;RAGLight&lt;/strong&gt;: you can now override any document processor, and this unlocks &lt;strong&gt;a new built-in example : a VLM-powered PDF parser.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Find repo here :&lt;/strong&gt; &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;https://github.com/Bessouat40/RAGLight&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try this new feature with many LLM provider such as &lt;strong&gt;Ollama&lt;/strong&gt; !&lt;/p&gt; &lt;h1&gt;What it does&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Extracts &lt;strong&gt;text AND images&lt;/strong&gt; from PDFs&lt;/li&gt; &lt;li&gt;Sends images to a &lt;strong&gt;Vision-Language Model&lt;/strong&gt; (Mistral, OpenAI, etc.)&lt;/li&gt; &lt;li&gt;Captions them and injects the result into your vector store&lt;/li&gt; &lt;li&gt;Makes RAG truly understand diagrams, block schemas, charts, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Super helpful for technical documentation, research papers, engineering PDFs…&lt;/p&gt; &lt;h1&gt;Minimal Example&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/di41m8op175g1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d01198e5a8bf6c0b0c64f46e9b8fe176debc2707"&gt;https://preview.redd.it/di41m8op175g1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d01198e5a8bf6c0b0c64f46e9b8fe176debc2707&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;p&gt;Most RAG tools ignore images entirely. Now RAGLight can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;interpret diagrams&lt;/li&gt; &lt;li&gt;index visual content&lt;/li&gt; &lt;li&gt;retrieve multimodal meaning&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T14:02:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdui4a</id>
    <title>Pipeshub just hit 2k GitHub stars.</title>
    <updated>2025-12-04T08:08:14+00:00</updated>
    <author>
      <name>/u/Inevitable-Letter385</name>
      <uri>https://old.reddit.com/user/Inevitable-Letter385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re super excited to share a milestone that wouldn’t have been possible without this community. &lt;strong&gt;PipesHub just crossed 2,000 GitHub stars!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thank you to everyone who tried it out, shared feedback, opened issues, or even just followed the project.&lt;/p&gt; &lt;p&gt;For those who haven’t heard of it yet, &lt;strong&gt;PipesHub&lt;/strong&gt; is a fully open-source enterprise search platform we’ve been building over the past few months. Our goal is simple: bring powerful &lt;strong&gt;Enterprise Search&lt;/strong&gt; and &lt;strong&gt;Agent Builders&lt;/strong&gt; to every team, without vendor lock-in. PipesHub brings all your business data together and makes it instantly searchable.&lt;/p&gt; &lt;p&gt;It integrates with tools like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, and even local files. You can deploy it with a single Docker Compose command.&lt;/p&gt; &lt;p&gt;Under the hood, PipesHub runs on a &lt;strong&gt;Kafka powered event streaming architecture&lt;/strong&gt;, giving it real time, scalable, fault tolerant indexing. It combines a vector database with a knowledge graph and uses &lt;strong&gt;Agentic RAG&lt;/strong&gt; to keep responses grounded in source of truth. You get visual citations, reasoning, and confidence scores, and if information isn’t found, it simply says so instead of hallucinating.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enterprise knowledge graph for deep understanding of users, orgs, and teams&lt;/li&gt; &lt;li&gt;Connect to any AI model: OpenAI, Gemini, Claude, Ollama, or any OpenAI compatible endpoint&lt;/li&gt; &lt;li&gt;Vision Language Models and OCR for images and scanned documents&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, and SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs&lt;/li&gt; &lt;li&gt;Support for all major file types, including PDFs with images and diagrams&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Builder&lt;/strong&gt; for actions like sending emails, scheduling meetings, deep research, internet search, and more&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning Agent&lt;/strong&gt; with planning capabilities&lt;/li&gt; &lt;li&gt;&lt;strong&gt;40+ connectors&lt;/strong&gt; for integrating with your business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’d love for you to check it out and share your thoughts or feedback. It truly helps guide the roadmap:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1pdudws"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Letter385"&gt; /u/Inevitable-Letter385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T08:08:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe18c7</id>
    <title>How does Ollama truncate the context when it's too long?</title>
    <updated>2025-12-04T14:21:49+00:00</updated>
    <author>
      <name>/u/AmphibianFrog</name>
      <uri>https://old.reddit.com/user/AmphibianFrog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to understand how prompts are truncated then they exceed the context limit. Let's say I have the num_ctx parameter set to 1000 tokens. I then send in a system prompt, and a whole bunch of user and assistant messages, but in total there are 2000 tokens in the context. What happens?&lt;/p&gt; &lt;p&gt;Specific questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does truncation happen per message or do partial messages get fed into the LLM?&lt;/li&gt; &lt;li&gt;Does the system prompt get truncated? Or is it always passed in even if other messages are removed from the input?&lt;/li&gt; &lt;li&gt;Do the messages get removed in pairs, i.e will it always make sure a user message is first in the prompt?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;My conclusion is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The system prompt is never truncated&lt;/li&gt; &lt;li&gt;Entire messages are dropped from the context, not individual tokens&lt;/li&gt; &lt;li&gt;Ollama will include as many messages as possible, getting rid of the oldest messages first (apart from the system prompt which is always included)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hopefully this helps someone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AmphibianFrog"&gt; /u/AmphibianFrog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T14:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe2xbe</id>
    <title>how close to codex or claude code can you get with ollama and a 4090?</title>
    <updated>2025-12-04T15:30:22+00:00</updated>
    <author>
      <name>/u/reelznfeelz</name>
      <uri>https://old.reddit.com/user/reelznfeelz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to run an agentic code assistant that can use tools on top of ollama without having to spend a whole ton of time on building it yourself? &lt;/p&gt; &lt;p&gt;Somebody here has probably followed or played in that space. &lt;/p&gt; &lt;p&gt;Just thinking/planning for the day when openAI/Claude has to charge what's actually needed to be profitable, which will be like $900/mo for the kind of usage I have. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reelznfeelz"&gt; /u/reelznfeelz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T15:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexhan</id>
    <title>Es Qwen3 el mejor modelo de IA del mundo en general en este momento? Mienten los benchmarks?</title>
    <updated>2025-12-05T15:08:59+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Algunos no nos fiamos de los Benchmarks..nos fiamos mas de lo que vemos por nosotros mismos…&lt;/p&gt; &lt;p&gt;Vosotros que pensais?&lt;/p&gt; &lt;p&gt;Es Qwen el mejor optimizado y el mas preciso mas inteligente y que mejor resultados da en matematicas fisica y programacion ? Siendo un modelo general razonablemente pequeño en comparacion con otros…aunque no he probado el nuevo modelo 3.2 exp de deepseek pero yo con el qwen3 estoy muy muy contento&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexhan/es_qwen3_el_mejor_modelo_de_ia_del_mundo_en/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexhan/es_qwen3_el_mejor_modelo_de_ia_del_mundo_en/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pexhan/es_qwen3_el_mejor_modelo_de_ia_del_mundo_en/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T15:08:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf07j7</id>
    <title>Looking for something to manage API usage</title>
    <updated>2025-12-05T16:54:07+00:00</updated>
    <author>
      <name>/u/tecneeq</name>
      <uri>https://old.reddit.com/user/tecneeq</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for something that allows me to give different amounts of tokens per day or week to different users for different models. Basically some kind of rate limiting.&lt;/p&gt; &lt;p&gt;Does anyone know something like that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tecneeq"&gt; /u/tecneeq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pf07j7/looking_for_something_to_manage_api_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pf07j7/looking_for_something_to_manage_api_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pf07j7/looking_for_something_to_manage_api_usage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T16:54:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1petnr2</id>
    <title>Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices</title>
    <updated>2025-12-05T12:20:16+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1petnr2/translating_webpages_using_local_model_on_ipad/"&gt; &lt;img alt="Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices" src="https://external-preview.redd.it/dmJremhrcWZvZDVnMVvVURK3C0St0olAiXhHNsqHlBUDFrodjuu0gc-gowgU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=471e58ef80ba2c5d0ce953debe5fc9808e05745f" title="Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Browsers comes with translation tool - but few of them provide legible translations. We are not used to the high quality translation provided by LLMs, and we expect the same experience with webpages translation when browsing.&lt;/p&gt; &lt;p&gt;I am pleased to announce that Vector Space now integrates Webpage translation. Featuring:&lt;/p&gt; &lt;p&gt;- Use a LLM instead of translation APIs&lt;/p&gt; &lt;p&gt;- Works on mobile&lt;/p&gt; &lt;p&gt;- Call local models for unlimited and private, translation&lt;/p&gt; &lt;p&gt;- Perserve HTML structures and visuals&lt;/p&gt; &lt;p&gt;- Connect to OpenAI API for faster transaction (enter your API in the settings)&lt;/p&gt; &lt;p&gt;Result is some very nice translations! Please see the video. It is filmed on a M1 iPad.&lt;/p&gt; &lt;p&gt;Try it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://short.yomer.jp/vector-space"&gt;https://short.yomer.jp/vector-space&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Limitations and next directions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Right now a relative large model (~4B) is needed for preserving HTML tags and improving translation quality. I believe a fine tuned model of a much smaller size can do the trick. With enough people supporting me I can work on it to increase translation speed at least 10x.&lt;/li&gt; &lt;li&gt;Due to Apple restriction on running GPU work in the background, currently only iPad multi tasking is supported on iOS. I believe this is solvable by either looking at Background Tasks framework or move to neural engine.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i3d55jqfod5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1petnr2/translating_webpages_using_local_model_on_ipad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1petnr2/translating_webpages_using_local_model_on_ipad/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T12:20:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1peu215</id>
    <title>A better way to share and talk to long PDFs</title>
    <updated>2025-12-05T12:40:44+00:00</updated>
    <author>
      <name>/u/simplext</name>
      <uri>https://old.reddit.com/user/simplext</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1peu215/a_better_way_to_share_and_talk_to_long_pdfs/"&gt; &lt;img alt="A better way to share and talk to long PDFs" src="https://external-preview.redd.it/OGpvMjQ4ZmNyZDVnMbGTwPFoXGKKHKurfth5fGD5PZl_uCHNFDWTG1GTjAFD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0adbc635fe108608aa908b6ae8ac3dae15871e9" title="A better way to share and talk to long PDFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Imagine going through the long and boring jobs data PDF shared by the BLS. What if they had shared a presentation that you could talk to ? &lt;/p&gt; &lt;p&gt;Visual Book allows you to break down an PDF into slides and then share it with people so they can talk to it.&lt;/p&gt; &lt;p&gt;Visual Book: &lt;a href="https://www.visualbook.app"&gt;https://www.visualbook.app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simplext"&gt; /u/simplext &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3vqnevecrd5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1peu215/a_better_way_to_share_and_talk_to_long_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1peu215/a_better_way_to_share_and_talk_to_long_pdfs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T12:40:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pekbzp</id>
    <title>No Uncensored Models?</title>
    <updated>2025-12-05T03:16:26+00:00</updated>
    <author>
      <name>/u/One_Spaceman</name>
      <uri>https://old.reddit.com/user/One_Spaceman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just downloaded Ollama, pulled Dolphin-Ollama3 from &lt;a href="https://www.youtube.com/watch?v=cTxENLLX1ho"&gt;THIS &lt;/a&gt;vid but its completely censored, is there anything that is totally uncensored? idk what to do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One_Spaceman"&gt; /u/One_Spaceman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T03:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfkede</id>
    <title>Y si Elon musk nos regalo MOE y nadie le puede dar las gracias?</title>
    <updated>2025-12-06T08:32:48+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all know who financed with 100 billion dollars the artificial intelligence from the beginning.&lt;br /&gt; We all know that Elon Musk complained that they were hiding from him the results that the AI was giving.&lt;br /&gt; Some of us think that out of envy of being the richest in the world, they want you only to put the money, but the other shareholders and companies did not want &amp;quot;the richest&amp;quot; to know about the advances.&lt;br /&gt; Elon Musk went out... he left voluntarily, right after leaving Musk, the AI explodes suddenly and becomes the most advanced on the planet.. What a coincidence?? While he contributed money the AI does not give results and as soon as I leave, the results appear.&lt;br /&gt; Then he would consider it a scam (They used him to finance it and keep it for themselves)&lt;br /&gt; What Musk financed according to my point of view was the MOE technology (The core of everything)&lt;br /&gt; If that were so, the only way to be able to compete with them, would be taking them to the starting square, and for that he should take away the MOE technology from them, and the best way would be talking with some Chinese engineers, so that they passed it to the Chinese for free, (or to the French of MIXTRAL) he would support them to do it in exchange for committing to release the technology to the world, that way he would put OPENAI to start from zero, and they would start to compete with GROK from zero.&lt;br /&gt; We DO NOT know for sure what has happened, but I know that no one gives away something that is worth billions, neither to the French nor to the Chinese, and I do not believe that they had the capacity to have developed something so valuable.... I believe that here something has happened that no one knows, and I believe that maybe, for having used and scammed Elon Musk, he decided that the technology that he paid out of his pocket was not going to stay with the competition if he wanted to compete with them, and that he looked for a way to give it away to the French of Mixtral or to the Chinese, in order to thus send them all to the starting square of the race and start competing with them with GROK from zero.&lt;br /&gt; The only way to put a company that has a secret worth billions to start from zero, is to release that secret, and that we all have it, that way it stops having value.&lt;br /&gt; I have the feeling that here all this conspiracy that everyone thought of technology thefts and sabotages, no one ever thought that it was the MOE technology, the one that made the race balance and everyone started to compete from the same starting square.&lt;br /&gt; But I have in my head stuck that something has to do with Elon Musk with us having this MOE technology (because think about it well) this is worth millions and they give it to us for free!!!!&lt;br /&gt; Who has paid for it? it was Elon Musk!! HE WAS THE ONE WHO MAINLY FINANCED OPEN AI AT THE BEGINNING TO DEVELOP THIS TECHNOLOGY!!!&lt;br /&gt; In the end the technology fell into our hands, not because he wanted it, but because they tried to scam deceive and use him out of envy of being the richest of all.&lt;br /&gt; How if this were true it cannot be known??? The entire OPENSOURCE community should thank ELON MUSK for what he has done, but since in theory it is something hypothetical that no one knows, we cannot give them...&lt;br /&gt; But I give them anyway, in case he was the one who gave us the MOE technology, because if you think about it well, it is what makes the difference between the AI, The MOE technology is what allows the AI to be used on a large scale.&lt;br /&gt; So nothing.. if hypothetical theory were true... Thanks Elon for being so brave!!!!&lt;/p&gt; &lt;p&gt;Todos sabemos quien financio con 100 mil millones de dolares la inteligencia artificial desde el principio.&lt;/p&gt; &lt;p&gt;Todos sabemos que Elon musk se quejo que le ocultaban los resultados que estaba dando la IA&lt;/p&gt; &lt;p&gt;Algunos pensamos que por envidia de ser el mas rico del mundo , te quieren solo para poner el dinero , pero los otros accionistas y empresas no querian que &amp;quot;el mas rico&amp;quot; supiese de los avances.&lt;/p&gt; &lt;p&gt;Elon musk se fue fuera...abandono voluntariamente , justo despues de abandonar Musk , la IA explota de repente y se vuelve lo mas avanzado en el planeta..Que coincidencia?? MIentras aporto dinero la IA no da resultados y en cuento me voy , aparecen los resultados.&lt;/p&gt; &lt;p&gt;Entonces el lo consideraria una estafa (Lo utilizaron para financiarla y quedarse ellos con ella)&lt;/p&gt; &lt;p&gt;Lo que musk financio segun mi punto de vista fue la tecnologia MOE (El nucleo de todo)&lt;/p&gt; &lt;p&gt;SI ello asi fuese , la unica forma de poder competir con ellos , seria llevandolos a ellos a la casilla de salida , y para ello deberia sacarles la tecnologia MOE , y la mejor forma seria hablando con algunos ingenieros chinos , para que se la pasaron a los chinos de forma gratuita ,(o a los franceses de MIXTRAL) el los apoyaria a hacerlo a cambio de comprometerse a librar la tecnologia al mundo , asi pondria a OPENAI a empezar desde cero , y empezarian a competir con GROK desde cero.&lt;/p&gt; &lt;p&gt;NO sabemos a ciencia cierta que ha pasado , pero yo se que nadie regala algo que vale miles de millones , ni a los franceses ni a los chinos , y no creo que ellos tuviesen la capacidad de haber desarrollado algo tan valioso....yo creo que aqui algo ha pasado que nadie sabe , y creo que a lo mejor , por haber utilizado y estafado a Elon musk , el decidio que la tecnologia que el pago de su bolsillo no se iba a quedar en la competencia si el queria competir con ellos , y que busco la forma de regalarsela a los Franceses de Mixtral o a los chinos , para poder asi mandarlos a todos a la casilla de salida de la carrera y empezar a competir con ellos con GROK desde cero.&lt;/p&gt; &lt;p&gt;La unica forma de poner auna empresa que tiene un secreto que vale miles de millones a empezar de cero , es liberar ese secreto , y que todos lo tengamos , asi deja de tener valor.&lt;/p&gt; &lt;p&gt;Yo tengo la sensacion de que aqui toda esta conspiracion que todo el mundo penso de robos de tecnologias y sabotajes , nunca nadie penso que fue la tecnologia MOE , la que hizo que la carrera se equilibrara y empezaran todos a competir desde la misma casilla de salida.&lt;/p&gt; &lt;p&gt;Pero yo tengo en la cabeza metido que algo tiene que ver Elon musk con nostros tener esta tecnologia MOE (porque pensadlo bien) esto vale millones y nos lo dan dado gratis!!!!&lt;/p&gt; &lt;p&gt;Quien lo ha pagado? fue elon musk!! EL FUE QUIEN PRINCIPALMENTE FINANCIA A OPEN AI AL PRINCIPIO PARA DESARROLLAR ESTA TECNOLOGIA!!!&lt;/p&gt; &lt;p&gt;Al final la tecnologia cayo en nuestras manos , no porque el quisiera , sino porque lo intentaron estafar engañar e utilizar por la envidia de ser el mas rico de todos.&lt;/p&gt; &lt;p&gt;Como esto si fuese cierto no se puede saber??? Toda la comunidad OPENSOURCE le deberia dar las gracias a ELON MUSK por lo que ha echo , pero como en teoria es algo hipotetico que nadie sabe , no se las podemos dar...&lt;/p&gt; &lt;p&gt;Pero yo se las doy igual , por si el fue quien nos regalo la tecnologia MOE , porque si lo pensais bien , es lo que marca la diferencia entre la IA , La tecnologia MOE es lo que permite que la IA pueda ser usada a gran escala.&lt;/p&gt; &lt;p&gt;Asi que nada..si hipotetica teoria fuese cierta...Gracias Elon por ser tan valiente!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfkede/y_si_elon_musk_nos_regalo_moe_y_nadie_le_puede/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfkede/y_si_elon_musk_nos_regalo_moe_y_nadie_le_puede/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfkede/y_si_elon_musk_nos_regalo_moe_y_nadie_le_puede/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T08:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexi8g</id>
    <title>Confused and unsure</title>
    <updated>2025-12-05T15:10:01+00:00</updated>
    <author>
      <name>/u/SirEblingMis</name>
      <uri>https://old.reddit.com/user/SirEblingMis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there.&lt;/p&gt; &lt;p&gt;I've seen lots of different rankings, but I haven't found a good concise resource that explains how I judge a model fitting onto 16gb vram or on 20-24gb M4 pro [mtx on LMstudio? or similar]&lt;/p&gt; &lt;p&gt;I'm genuinely just interested in a solid model to help administrative tasks as I do my Master's degree. I use Overleaf for it's great help with LaTex, and Perplexity for finding papers, teaching myself code or LaTex, etc.&lt;/p&gt; &lt;p&gt;But I want to run this stuff locally, especially since I may sometimes end up working with datasets that are confidential or secure.&lt;/p&gt; &lt;p&gt;Apologies if this post is a repeat or faux pas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SirEblingMis"&gt; /u/SirEblingMis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexi8g/confused_and_unsure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexi8g/confused_and_unsure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pexi8g/confused_and_unsure/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T15:10:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pev1eh</id>
    <title>I can't make Ministral 3 14B to work.</title>
    <updated>2025-12-05T13:27:40+00:00</updated>
    <author>
      <name>/u/No-Acanthisitta9773</name>
      <uri>https://old.reddit.com/user/No-Acanthisitta9773</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pev1eh/i_cant_make_ministral_3_14b_to_work/"&gt; &lt;img alt="I can't make Ministral 3 14B to work." src="https://preview.redd.it/usb606890e5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48964b9a072420216bf48a6c3636ad0d9b29b044" title="I can't make Ministral 3 14B to work." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Ministral-3-14B-Instruct-2512-Q5_K_M &lt;/p&gt; &lt;p&gt;I've tried different kind of modelfile, but it always responds the same nonsense. Ollama is up-to-date. Did you manage to make it work? What was your modelfile like? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Acanthisitta9773"&gt; /u/No-Acanthisitta9773 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/usb606890e5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pev1eh/i_cant_make_ministral_3_14b_to_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pev1eh/i_cant_make_ministral_3_14b_to_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T13:27:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf9grf</id>
    <title>Best model in the 8B range for RAG in 2025</title>
    <updated>2025-12-05T23:00:35+00:00</updated>
    <author>
      <name>/u/Hour-Entertainer-478</name>
      <uri>https://old.reddit.com/user/Hour-Entertainer-478</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hour-Entertainer-478"&gt; /u/Hour-Entertainer-478 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pf92li/best_model_in_the_8b_range_for_rag_in_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pf9grf/best_model_in_the_8b_range_for_rag_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pf9grf/best_model_in_the_8b_range_for_rag_in_2025/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T23:00:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pffgp1</id>
    <title>Noob here, looking for the perfect local LLM for my M3 Macbook Air 24GB RAM</title>
    <updated>2025-12-06T03:45:27+00:00</updated>
    <author>
      <name>/u/sylntnyte</name>
      <uri>https://old.reddit.com/user/sylntnyte</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sylntnyte"&gt; /u/sylntnyte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pff8b5/noob_here_looking_for_the_perfect_local_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pffgp1/noob_here_looking_for_the_perfect_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pffgp1/noob_here_looking_for_the_perfect_local_llm_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T03:45:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfj633</id>
    <title>Llm for log analysis</title>
    <updated>2025-12-06T07:14:35+00:00</updated>
    <author>
      <name>/u/gargento83</name>
      <uri>https://old.reddit.com/user/gargento83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is a good LLM model for security log analysis for cybersecurity?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gargento83"&gt; /u/gargento83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfj633/llm_for_log_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfj633/llm_for_log_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfj633/llm_for_log_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T07:14:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfibo6</id>
    <title>CocoIndex 0.3.1 - Open-Source Data Engine for Dynamic Context Engineering</title>
    <updated>2025-12-06T06:23:30+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I'm back with a new version of &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;CocoIndex&lt;/a&gt; (v0.3.1 - Ollama natively supported), with significant updates since last one. CocoIndex is ultra performant data transformation for AI &amp;amp; Dynamic Context Engineering - Simple to connect to source, and keep the target always fresh for all the heavy AI transformations (and any transformations) with incremental processing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Adaptive Batching&lt;/strong&gt;&lt;br /&gt; Supports automatic, knob-free batching across all functions. In our benchmarks with MiniLM, batching delivered ~5× higher throughput and ~80% lower runtime by amortizing GPU overhead with no manual tuning. I think particular if you have large AI workloads, this can help and is relevant to this sub-reddit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Custom Sources&lt;/strong&gt;&lt;br /&gt; With custom source connector, you can now use it to any external system — APIs, DBs, cloud storage, file systems, and more. CocoIndex handles incremental ingestion, change tracking, and schema alignment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Runtime &amp;amp; Reliability&lt;/strong&gt;&lt;br /&gt; Safer async execution and correct cancellation, Centralized HTTP utility with retries + clear errors, and many others.&lt;/p&gt; &lt;p&gt;You can find the full release notes here: &lt;a href="https://cocoindex.io/blogs/changelog-0310"&gt;https://cocoindex.io/blogs/changelog-0310&lt;/a&gt;&lt;br /&gt; Open source project here : &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;https://github.com/cocoindex-io/cocoindex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Btw, we are also on Github trending in Rust today :) it has Python SDK.&lt;/p&gt; &lt;p&gt;We have been growing so much with feedbacks from this community, thank you so much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfibo6/cocoindex_031_opensource_data_engine_for_dynamic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfibo6/cocoindex_031_opensource_data_engine_for_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfibo6/cocoindex_031_opensource_data_engine_for_dynamic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T06:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfr31a</id>
    <title>NornicDB - V1 MemoryOS for LLMs - MIT</title>
    <updated>2025-12-06T14:50:57+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1pfr2tj/nornicdb_v1_memoryos_for_llms_mit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfr31a/nornicdb_v1_memoryos_for_llms_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfr31a/nornicdb_v1_memoryos_for_llms_mit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T14:50:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfo6zh</id>
    <title>Vllama: CLI based framework to run vision models in local or remote gpus(inspired from Ollama)</title>
    <updated>2025-12-06T12:30:17+00:00</updated>
    <author>
      <name>/u/Weekly_Layer_9315</name>
      <uri>https://old.reddit.com/user/Weekly_Layer_9315</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, this is my first post. I have built a simple CLI tool, which can help all to run the llms, vision models like image and video gen, models in the local system and if the system doesn't have the gpu or sufficient ram, they can also run it using kaggle's gpu(which is 30 hrs free for a week).&lt;/p&gt; &lt;p&gt;This is inspired from Ollama, which made downloading llms easy and interacting with it much easy, so I thought of why can't this be made for vision models, so I tried this first on my system, basic image generation is working but not that good, then I thought, why can't we use the Kaggle's GPU to generate videos and images and that can happen directly from the terminal with a single step, so that everyone can use this, so I built this VLLAMA.&lt;/p&gt; &lt;p&gt;In this, currently there are many features, like image, video generation in local and kaggles gpu session; download llms and make it run and also interact with it from anywhere (inspired by ollama) also improved it further by creating a vs code extension VLLAMA, using which you can chat directly from the vs code's chat section, users can chat with the local running llm with just adding &amp;quot;@vllama&amp;quot; at the start of the message and this doesn't use any usage cost and can be used as much as anyone wants, you can check this out at in the vscode extensions.&lt;/p&gt; &lt;p&gt;I want to implement this further so that the companies or anyone with gpu access can download the best llms for their usage and initialize it in their gpu servers, and can directly interact with it from the vscode's chat section and also in further versions, I am planning to implement agentic features so that users can use the local llm to use for code editing, in line suggestions, so that they don't have to pay for premiums and many more.&lt;/p&gt; &lt;p&gt;Currently it also has simple Text-to-Speech, and Speech-to-Text, which I am planning to include in the further versions, using open source audio models and also in further, implement 3D generation models, so that everyone can leverage the use of the open models directly from their terminal, and making the complex process of the using open models easy with just a single command in the terminal.&lt;/p&gt; &lt;p&gt;I have also implemented simple functionalities which can help, like listing the downloaded models and their sizes. Other things available are, basic dataset preprocessing, and training ML models directly with just two commands by just providing it the dataset. This is a basic implementation and want to further improve this so that users with just a dataset can clean and pre-process the data, train the models in their local or using the kaggle's or any free gpu providing services or their own gpus or cloud provided gpus, and can directly deploy the models and can use it any cases.&lt;/p&gt; &lt;p&gt;Currently this are the things it is doing and I want to improve such that everyone can use this for any case of the AI and leveraging the use of open models.&lt;/p&gt; &lt;p&gt;Please checkout the work at: &lt;a href="https://github.com/ManvithGopu13/Vllama"&gt;https://github.com/ManvithGopu13/Vllama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Published version at: &lt;a href="https://pypi.org/project/vllama/"&gt;https://pypi.org/project/vllama/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also the extension: &lt;a href="https://marketplace.visualstudio.com/items?itemName=ManvithGopu.vllama"&gt;https://marketplace.visualstudio.com/items?itemName=ManvithGopu.vllama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would appreciate your time for reading and thankful for everyone who want to contribute and spread a word of it.&lt;/p&gt; &lt;p&gt;Please leave your requests for improvements and any suggestions, ideas, and even roasts or anything in the comments or in the issues, this is well taken and appreciated. Thanks in advance. If you find the project useful, kindly contribute and can star it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weekly_Layer_9315"&gt; /u/Weekly_Layer_9315 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T12:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg03g5</id>
    <title>Fara-7b on ollama?</title>
    <updated>2025-12-06T21:09:52+00:00</updated>
    <author>
      <name>/u/tracagnotto</name>
      <uri>https://old.reddit.com/user/tracagnotto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; i was trying running Fara-7B from microsoft, they require vllm and it's a nightmare.&lt;/p&gt; &lt;p&gt;I found that it's usable on ollama but I don't really understand how. Ollama is chat, and that model needs to do continuous agentic work. In fact my test was:&lt;br /&gt; ME:&lt;br /&gt; [URL OF SOME BLOG]&lt;/p&gt; &lt;p&gt;what's this blog about&lt;/p&gt; &lt;p&gt;OLLAMA/FARA:&lt;/p&gt; &lt;p&gt;This is a screenshot of the latest news page on Matthew Leer-Bobinson's blog. The URL (url of the blog) indicates it's the second page of the latest news posts. &amp;lt;tool\_call&amp;gt; {&amp;quot;name&amp;quot;: &amp;quot;olution&amp;quot;, &amp;quot;arguments&amp;quot;: {&amp;quot;t&amp;quot;: &amp;quot;info&amp;quot;}} &amp;lt;/tool\_call&amp;gt;&lt;/p&gt; &lt;p&gt;ME:&lt;br /&gt; Examine content pls&lt;/p&gt; &lt;p&gt;OLLAMA/FARA:&lt;br /&gt; I’ll click on the link to open the latest-news page and examine its content. &amp;lt;tool\_call&amp;gt; {&amp;quot;name&amp;quot;: &amp;quot;olution&amp;quot;, &amp;quot;arguments&amp;quot;: {&amp;quot;t&amp;quot;: &amp;quot;left_click&amp;quot;, &amp;quot;coordinate&amp;quot;: [100, 10]}} &amp;lt;/tool\_call&amp;gt;&lt;/p&gt; &lt;p&gt;what am I missing?&lt;br /&gt; Is ollama only a way to run it and then you have to build on it a whole MCP/Agentic stack that takes those result and make actions? lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tracagnotto"&gt; /u/tracagnotto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg03g5/fara7b_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg03g5/fara7b_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pg03g5/fara7b_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T21:09:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfiedc</id>
    <title>If it weren't for the Chinese we wouldn't have local AI</title>
    <updated>2025-12-06T06:28:03+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GRACIAS A ELON MUSK!! ESTOY SEGURO!!! Elon el mas trabajador e inteligente del planeta , lo quisieron fastidiar y ahora OpenAi corre riesgo de ir a la quiebra por traicionarlo...todos lo que lo traicionaron por envidia y le ocultaron los resultados de la IA (google Y microsoft) Ahora veremos lo que agutanta OpenAI sin el dinero de musk...&lt;/p&gt; &lt;p&gt;Mientras GROK Y LOS CHINOS avanzan con las cuentas saneadas, con las empresas sin riesgo..bien apalancadas financiadas con seguridad , OPENAI esta temblando de miedo de ir a la quiebra aajjajajajajajaja&lt;/p&gt; &lt;p&gt;Todos sabemos el desastre que pasó cuando un ingeniero chino robó tecnología de Open AI (O eso nos han contado) con todo el lío que causó cuando Deepseek sacó su modelo de IA al mundo Opensource,&lt;/p&gt; &lt;p&gt;Todos sabemos que Open AI se negó a sacar lo mejor de su tecnología...luego cuando MOE technology salió con Qwen e hizo que la bolsa americana cayera por trillones de dólares dándose cuenta que los chips Nvidia quizás no fueran necesarios para la IA.&lt;/p&gt; &lt;p&gt;Sacudió los cimientos de los mercados de valores americanos. No sabemos exactamente qué pasó... si Elon Musk tuvo algo que ver con todo esto al irse de la empresa y se aprovecharon de su financiación al principio... Quizás le ocultaron a Elon Musk los verdaderos resultados que estaba dando la IA para que siguiera financiándolos y él se vengó cuando vio que no había resultados cuando se fue de Open AI y justo después de que Elon Musk se fuera, Open AI dio el gran salto.&lt;/p&gt; &lt;p&gt;Fue demasiada coincidencia...Quizás quien realmente controlaba Open AI (Microsoft), ya que Elon Musk estaba muy ocupado con sus problemas en sus empresas, aprovecharon la oportunidad para apostar y beneficiarse ya que ahora son el accionista mayoritario...ya es cosa tuya saber si Elon Musk estaba sobornando a ingenieros para que le dieran la tecnología MOE y como se negaron hizo un pacto con ellos para dársela a los chinos...y por ahí openai mientras sacaban Grok al mercado...nunca sabremos que pasó en esa telenovela...toda una historia de conspiraciones e hipótesis extrañas...&lt;/p&gt; &lt;p&gt;Lo extraño es que Qwen parece haber adquirido la tecnología que todos creen que está en los modelos de Open AI...y nunca lo sabremos porque los modelos son cerrados y no sabemos qué ingeniería hay realmente detrás. Todo esto pensando... y si no hubiera pasado toda esta historia de robo de tecnología... conspiraciones... etc... caída de la bolsa por el miedo de los inversores a que la burbuja de la IA estallara... fin... si todo eso no hubiera pasado...&lt;/p&gt; &lt;p&gt;Yo creo que la comunidad Opensource no estaría disfrutando de los modelos MOE que tenemos hoy en día... ya que eso conllevó a una feroz competencia y a la salida de modelos para perjudicarse unos a otros... lo que tenemos que ver es que si no hubiera pasado lo de Open AI... creo que no hubieran sacado el GPT-OSS y también creo que nunca volverán a sacar un modelo similar...&lt;/p&gt; &lt;p&gt;Estas empresas son muy avariciosas y quieren todo para ellas y si no fuera por la presión de los chinos, la comunidad Opensource tendría muchos menos modelos capaces y tenemos que estar agradecidos de que QWEN Y DEEPSEEK y otros metieran presión sacando sus modelos...porque los occidentales...si por ellos fuera...se quedarían todo para ellos como hace Google con su buscador...facebook o Amazon con sus algoritmos o Apple Con sus tecnologías...&lt;/p&gt; &lt;p&gt;Normalmente los chinos siempre han sido muy cerrados...pero la competencia por ver quién es ahora la superpotencia que domina el mundo ha traído esos golpes que han sido maravillosos para la comunidad Opensource...&lt;/p&gt; &lt;p&gt;El miedo que tengo es que no se repita algo así y los modelos que saquen de ahora en adelante sean calderilla... las migajas... y lo realmente bueno y las grandes cosas se las queden ellos y lo protejan todo con patentes... y mucha seguridad... ya viste la caída que OpenAi tuvo que ser fundada sin ánimo de lucro para el beneficio de la Humanidad...&lt;/p&gt; &lt;p&gt;Se ha convertido en un negocio que mueve billones con una deuda muy apalancada...y juegan funambulistas colgados de un hilo...Cómo termina todo esto...si todo explota...o siguen sacando modelos...porque los inversores no invierten en el pasado...invierten en el futuro...y si no hay novedades...los occidentales van a estar en problemas...algo que los chinos se financian ellos mismos y son cautos y buscaron la forma de hacer el negocio más rentable requiriendo menos potencia de cálculo...&lt;/p&gt; &lt;p&gt;En resumen, los chinos y asiáticos siempre han destacado en muchas áreas como la electrónica... pero ahora China se está convirtiendo en una Superpotencia mayor que EEUU en muchas áreas y creo que no debemos subestimarlos ni menospreciarlos... OPEN AI va a quebrar completamente creo... y los chinos van a ganar esta batalla... creo que será el principio de la hegemonía de China sobre el mundo...&lt;/p&gt; &lt;p&gt;Son gente trabajadora, eficiente y pacífica...así que realmente apoyo sus modelos y creo que gracias a ellos esta competencia por ver quién gana está beneficiando a la comunidad Opensource de una manera inimaginable que nunca pensé que podríamos tener modelos tan capaces y útiles en nuestras manos para trabajar completamente offline...y gran parte es gracias a los chinos!!!!! Quizás no es gracias a los chinos... sino... a ELON MUSK. Si me equivoco en mis suposiciones OPENAI estaría evolucionando por detrás de todos los demás competidores...ya que serían los mejores...aunque sin la financiación de Elon Musk...todo cambia.&lt;/p&gt; &lt;p&gt;Pronto veremos que ha pasado aquí y que está pasando...porque Elon Musk es una persona brillante y valiente...y una buena persona...y las buenas personas se enfrían...creo que Microsoft pensó que su jugada contra Musk iba a salir bien...pero pronto veremos si OPEN AI quiebra...&lt;/p&gt; &lt;p&gt;Si mis suposiciones son ciertas, Elon Musk nos habría dado a todos la tecnología MOE completamente gratis para que la IA china diera un golpe fuerte a OPENAI para frenarlos y entrar en el top 3 con GROK. Si eso fuera cierto deberíamos agradecerle... pero no se pudo hacer porque nadie sabría que todos los modelos de alta calidad que disfrutamos offline son gracias a ÉL. Al hombre más rico del planeta que nos dio esta tecnología porque algunas personas querían engañarlo!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T06:28:03+00:00</published>
  </entry>
</feed>
