<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-10T05:24:56+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ph2sem</id>
    <title>Ollama is the next big thing to slice bread.</title>
    <updated>2025-12-08T04:07:38+00:00</updated>
    <author>
      <name>/u/newbietofx</name>
      <uri>https://old.reddit.com/user/newbietofx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing and setting up my own ui for pulling response from the ollama api server and fronting it. The docker approach is amazing. How do I put claude code prompt into tinyllama? It still spouts nonsense. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newbietofx"&gt; /u/newbietofx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ph2sem/ollama_is_the_next_big_thing_to_slice_bread/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ph2sem/ollama_is_the_next_big_thing_to_slice_bread/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ph2sem/ollama_is_the_next_big_thing_to_slice_bread/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T04:07:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgmbna</id>
    <title>Need opinion/help on my Memory System for LLM</title>
    <updated>2025-12-07T16:19:38+00:00</updated>
    <author>
      <name>/u/I_DiMooo</name>
      <uri>https://old.reddit.com/user/I_DiMooo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! I've been slowly learning and developing a LLM based on the character &lt;strong&gt;Cyn&lt;/strong&gt; from the series &amp;quot;Murder Drones&amp;quot;. My goal is to bring that silly robot to life someday but right now I'm developing her software controlled by an LLM.&lt;/p&gt; &lt;p&gt;I'm currently trying to figure out the &lt;em&gt;(hopefully)&lt;/em&gt; ideal &lt;strong&gt;memory system&lt;/strong&gt; for her. I've been developing this whole project with the help from ChatGPT, we've been brainstorming and we landed on an idea but I want to get some experienced peoples opinions before implementing it.&lt;/p&gt; &lt;p&gt;Cyn currently receives something I call &lt;strong&gt;&amp;quot;State Calls&amp;quot;&lt;/strong&gt; containing various world data and she responds with an array of &lt;strong&gt;&amp;quot;Executable Functions&amp;quot;&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example: {&amp;quot;finalized_speech&amp;quot;: &amp;quot;hi cyn&amp;quot;, &amp;quot;battery&amp;quot;: 80} ---&amp;gt; [&amp;quot;name&amp;quot;: &amp;quot;speak&amp;quot;, &amp;quot;params&amp;quot;: {&amp;quot;text&amp;quot;: &amp;quot;Hello&amp;quot;}]&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So the idea for the &lt;strong&gt;Memory System&lt;/strong&gt; is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;State Calls and Executable Functions are converted into easily readable information (finalized_speech would be: &amp;quot;User said smth&amp;quot;), this gets embedded and stored in recent_memories.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Every State Call will be analyzed and with embedding we will return some memories in &amp;quot;memory&amp;quot; variable within state call.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Every Minute/Hour/etc. a seperate summarizer model will make a minute/hour/etc. summary of the memories. These summary memories will simulate memory decays. We could store them as long-term memories after some point.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That is the base for the system. I am also thinking about making memory types and some memory storing system like cataloging the people she meets and other stuff like that, but right now I just want to land on a base that will make conversations with her have actual continuity, context and meaning.&lt;/p&gt; &lt;p&gt;I'd really appreciate the opinions and possible help with enhancing the idea for the system to make it as stable and lively as possible. If someone wants to help and needs some clarifications I'm happy to answer them!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_DiMooo"&gt; /u/I_DiMooo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgmbna/need_opinionhelp_on_my_memory_system_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgmbna/need_opinionhelp_on_my_memory_system_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgmbna/need_opinionhelp_on_my_memory_system_for_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T16:19:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgio6a</id>
    <title>GPU acceleration on Ryzen AI 9 HX 370</title>
    <updated>2025-12-07T13:43:34+00:00</updated>
    <author>
      <name>/u/GarauGarau</name>
      <uri>https://old.reddit.com/user/GarauGarau</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm running a machine with the new Ryzen AI 9 HX 370 and 96GB of RAM on Windows 11.&lt;/p&gt; &lt;p&gt;I have a large dataset to process (~320k rows) and I'm trying to determine if it's possible for Ollama to utilize the Radeon 890M iGPU.&lt;/p&gt; &lt;p&gt;Current Status:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: qwen2.5:7b&lt;/li&gt; &lt;li&gt;Setup: Python script sending requests to Ollama with 2 parallel workers.&lt;/li&gt; &lt;li&gt;Performance: I am averaging 8.68s/it (seconds per iteration/row).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been trying to get it to work, but I'm curious if anyone else with this specific chip has actually succeeded in enabling hardware acceleration on Windows, or if we are currently limited to CPU-only inference due to driver/software maturity issues.&lt;/p&gt; &lt;p&gt;I attempted to force GPU usage via PowerShell environment variables using Gemini's advice, but the logs always show &lt;code&gt;inference compute id=cpu&lt;/code&gt; and &lt;code&gt;entering low vram mode&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I have tried the following configurations (restarting the server each time):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Standard Parallelism: $env:OLLAMA_NUM_PARALLEL = &amp;quot;4&amp;quot;&lt;/li&gt; &lt;li&gt;Forcing Vulkan: $env:OLLAMA_VULKAN = &amp;quot;1&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you got it working, determine if it's possible to get Ollama to utilize the Radeon 890M iGPU at this time with stable or preview drivers?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GarauGarau"&gt; /u/GarauGarau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgio6a/gpu_acceleration_on_ryzen_ai_9_hx_370/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgio6a/gpu_acceleration_on_ryzen_ai_9_hx_370/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgio6a/gpu_acceleration_on_ryzen_ai_9_hx_370/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T13:43:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgl21p</id>
    <title>DevCrew agent swarm for accelerating your software development</title>
    <updated>2025-12-07T15:28:24+00:00</updated>
    <author>
      <name>/u/The_Research_Ninja</name>
      <uri>https://old.reddit.com/user/The_Research_Ninja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Fam. A new version of DevCrew_s1 with 20 agents is available at &lt;a href="https://github.com/GSA-TTS/devCrew_s1"&gt;https://github.com/GSA-TTS/devCrew_s1&lt;/a&gt; . Key agents are: BluePrint writer, ADR/ASR writers, Backend Engineer, Frontend Engineer, Code Reviewer, QA tester, Security Auditor, System Architect, UX/UI designer. DevCrew allows your organization to at least bootstrap any new full-stack software project with design documents, codes, tests, and more. &lt;/p&gt; &lt;p&gt;Imagine giving DevCrew one good software program design document in plain language and the crew gives you back a well documented executable full-stack software program. üöÄ DevCrew_s1 means &amp;quot;Specification 1 of DevCrew&amp;quot; where workflows are rigidly structured while &amp;quot;Specification 2&amp;quot; workflows are more flexible. You may implement and deploy Specification 1 with Claude Code, Amazon Strands, or Crew AI. It would be lovely to see some of us implement DevCrew_s1 locally with Ollama. &lt;/p&gt; &lt;p&gt;My upcoming book about production-grade AI Agent Systems will give you more practical guidance on how to build your own production-grade AI agent teams. I'm also seeking reviewers for the beta version of the book. Any experts from Ollama, nVidia, AWS, etc are welcomed - please DM me for more details. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_Research_Ninja"&gt; /u/The_Research_Ninja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgl21p/devcrew_agent_swarm_for_accelerating_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgl21p/devcrew_agent_swarm_for_accelerating_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgl21p/devcrew_agent_swarm_for_accelerating_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T15:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgxojv</id>
    <title>How to create a local developer only agent</title>
    <updated>2025-12-08T00:00:45+00:00</updated>
    <author>
      <name>/u/xtremeLinux</name>
      <uri>https://old.reddit.com/user/xtremeLinux</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently I am working on 4 projects from scratch. Up to now I have avoided AI like the plague for these projects but would want to use it now since several great ideas that I have had over the years are now possible with the technological changes in php, python and web servers.&lt;/p&gt; &lt;p&gt;My current scenario is that I want to (and I do not know if this is possible), show the ful code to the ai and explain to it what it does. Then explain the things I want to add to it, but the flow should be in such a way that the updates the AI gives me, it should apply them itself to the code, save it, test ot and do a live feeding of the output of it to learn about any mistakes it made, then circle back to code the new interation of it.&lt;/p&gt; &lt;p&gt;This cycle would repeat until certain tasks are satisfied.&lt;/p&gt; &lt;p&gt;My current setup hardware for this is Ubuntu, 128gb ddr5, rtx 5090,a 14900k cpu and 8tb of nvme. Don't know if I need anything else.&lt;/p&gt; &lt;p&gt;1 am just throwing this idea out there to see, mainly, if AI would speed up the process. I do not trust it completely since even with gemini 3, grok 4.1 or latest openai they really suck at coding when things get complex.&lt;/p&gt; &lt;p&gt;So maybe limiting the area of expertise (again I am guessing here) to only coding and only to a specific amount of languages and areas of interest like web, sockets, webworkers, http protocol, etc could help in speeding up and producing better results.&lt;/p&gt; &lt;p&gt;Can someone guide me if this iterating process is possible where the AI has read/write access to a folder and sub folder, it has access to a local web page and can thereby process in a loop certain criteria to iterate over it until i t accomplish certain points.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xtremeLinux"&gt; /u/xtremeLinux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgxojv/how_to_create_a_local_developer_only_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgxojv/how_to_create_a_local_developer_only_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgxojv/how_to_create_a_local_developer_only_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T00:00:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1phmg5z</id>
    <title>Either fix the pull, or add the jinja.... GGUF Struggles</title>
    <updated>2025-12-08T19:55:32+00:00</updated>
    <author>
      <name>/u/DriftTony</name>
      <uri>https://old.reddit.com/user/DriftTony</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok, currently I have a crappy internet connection, but when I use wget, everything works quite well, I do have the occasional restart, but it starts right from where I left off....&lt;/p&gt; &lt;p&gt;Now I thought to just download this GGUF file and create a ModelFile and use it like that, but because it is a VL model I thought I take the safe/certain route and use the Ollama Pull Method, because I could not for the life of me figure out how to convert the *.jinja file into the modelfile, nor could I find any documentation (nor could the yelled-at AI) for what Ollama was actually expecting and what from or even if any of the jinja-file is in the GGUF I can download from HF. And does the Model-file also need the link to the MMPROJ?&lt;/p&gt; &lt;p&gt;Then this horror just scoups up the little bandwidth I have (I do remember like a year ago, the Ollama Pull was horrible... but still??)&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 6% ‚ñï‚ñà ‚ñè 1.3 GB/ 21 GB&lt;/p&gt; &lt;p&gt;Error: max retries exceeded: EOF&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 15% ‚ñï‚ñà‚ñà ‚ñè 3.4 GB/ 21 GB&lt;/p&gt; &lt;p&gt;Error: max retries exceeded: EOF&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 36% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 7.9 GB/ 21 GB 649 KB/s 5h55m&lt;/p&gt; &lt;p&gt;Error: max retries exceeded: EOF&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 24% ‚ñï‚ñà‚ñà‚ñà‚ñà ‚ñè 5.2 GB/ 21 GB&lt;/p&gt; &lt;p&gt;Error: max retries exceeded: EOF&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 26% ‚ñï‚ñà‚ñà‚ñà‚ñà ‚ñè 5.8 GB/ 21 GB&lt;/p&gt; &lt;p&gt;Error: max retries exceeded: EOF&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 28% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 6.2 GB/ 21 GB&lt;/p&gt; &lt;p&gt;Error: max retries exceeded: EOF&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 33% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 7.1 GB/ 21 GB 971 KB/s 4h10m^Ccontext canceled&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 36% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 7.9 GB/ 21 GB 199 KB/s 19h19m^Ccontext canceled&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 39% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 8.4 GB/ 21 GB 988 KB/s 3h44m^Ccontext canceled&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 40% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 8.8 GB/ 21 GB 814 KB/s 4h24m^Ccontext canceled&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 41% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 8.9 GB/ 21 GB 781 KB/s 4h34m&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 48% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 10 GB/ 21 GB 934 KB/s 3h20m^Ccontext canceled&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 50% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 10 GB/ 21 GB 608 KB/s 4h56m^Ccontext canceled&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 53% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 11 GB/ 21 GB 504 KB/s 5h39m&lt;/p&gt; &lt;p&gt;Error: max retries exceeded: EOF&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 30% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 6.5 GB/ 21 GB 539 KB/s 7h50m&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;As you can see the only resolution is to use regularly CTRL-C because then it at least resumes where it ends.... but really, is there anyone that could either implement the jinja-files or solve the pull process? This is becoming rather.... 'frustrated' as the AI would say....&lt;/p&gt; &lt;p&gt;And to be honest, I've had many GGUF models in the past (with using pull) that acted wrong or corrupt, so how 'solid' is the automatic pull and Model-file implementation from Ollama? Is there anyone that can shed a light? Did any of you encounter something similar, or perhaps I'm approaching this totally wrong?&lt;/p&gt; &lt;p&gt;Edit: OMG it just jumped from 53% back to 30%.... Sigh&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DriftTony"&gt; /u/DriftTony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phmg5z/either_fix_the_pull_or_add_the_jinja_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phmg5z/either_fix_the_pull_or_add_the_jinja_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phmg5z/either_fix_the_pull_or_add_the_jinja_gguf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T19:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1phi3ln</id>
    <title>Confused about ROCm hardware support</title>
    <updated>2025-12-08T17:13:55+00:00</updated>
    <author>
      <name>/u/NE556</name>
      <uri>https://old.reddit.com/user/NE556</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm looking at what hardware might be compatible to get some reasonably decent performance (looking to put it in my NAS to integrate with Home-Assistant, something like what this guy did &lt;a href="https://www.youtube.com/watch?v=j7yumDPWAEA"&gt;https://www.youtube.com/watch?v=j7yumDPWAEA&lt;/a&gt;), but I'm confused by the AMD documentation on what is compatible. I'm aiming for a 16GB GPU to allow me to run one of the better models, but given it's going to just be sitting there, I'm hoping one of the less powerful (and thus less power hungry) GPUs will work from a previous generation to save some money. &lt;a href="https://rocm.docs.amd.com/en/latest/compatibility/compatibility-matrix.html"&gt;https://rocm.docs.amd.com/en/latest/compatibility/compatibility-matrix.html&lt;/a&gt; seems to imply anything CDNA onwards or RDNA2 onwards is supported. But &lt;a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html"&gt;https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html&lt;/a&gt; starts with RX 7700 XT/RDNA3 as the oldest/lowest hardware that is supported. Anyone able to help me figure this out? I'm aiming to stick with AMD, since the Linux GPU driver support is much better these days in my experience. I've also looked into NPU hardware, but seen that Ollama NPU support is pretty early days, it seems, at the moment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NE556"&gt; /u/NE556 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phi3ln/confused_about_rocm_hardware_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phi3ln/confused_about_rocm_hardware_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phi3ln/confused_about_rocm_hardware_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T17:13:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1phf6qv</id>
    <title>DataKit: your all in browser data studio is open source now</title>
    <updated>2025-12-08T15:24:39+00:00</updated>
    <author>
      <name>/u/Sea-Assignment6371</name>
      <uri>https://old.reddit.com/user/Sea-Assignment6371</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1phf6qv/datakit_your_all_in_browser_data_studio_is_open/"&gt; &lt;img alt="DataKit: your all in browser data studio is open source now" src="https://external-preview.redd.it/0HAU6Wdhz0KvBSjuRcJDgbxcR0Y5tImb_WCXq9uJsF8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=529a415eb7ee57d542327e222db133faed945255" title="DataKit: your all in browser data studio is open source now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Assignment6371"&gt; /u/Sea-Assignment6371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0sm263x4iz5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phf6qv/datakit_your_all_in_browser_data_studio_is_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phf6qv/datakit_your_all_in_browser_data_studio_is_open/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T15:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1phbdte</id>
    <title>data privacy policy of ollama cloud models?</title>
    <updated>2025-12-08T12:40:21+00:00</updated>
    <author>
      <name>/u/EatTFM</name>
      <uri>https://old.reddit.com/user/EatTFM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;we are processing scans containing highly sensitive personal information. Where can I find the details about the data privacy policy when using ollama cloud models?&lt;/p&gt; &lt;p&gt;I have heard that there is something on the roadmap to encrypt data when using cloud models - where to find more info about that?&lt;/p&gt; &lt;p&gt;thank you&lt;/p&gt; &lt;p&gt;Update: Until now, we are processing these data just using the local ollama models. This is not clear from the OP&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EatTFM"&gt; /u/EatTFM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phbdte/data_privacy_policy_of_ollama_cloud_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phbdte/data_privacy_policy_of_ollama_cloud_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phbdte/data_privacy_policy_of_ollama_cloud_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T12:40:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1phbooy</id>
    <title>Ollama vision models can't see images via console</title>
    <updated>2025-12-08T12:55:22+00:00</updated>
    <author>
      <name>/u/jozzo402</name>
      <uri>https://old.reddit.com/user/jozzo402</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SOLVED: My ollama server is remote and I was using a path to a local image lol. FTPing the image to the ollama server, then providing it the path from there works fine!&lt;/p&gt; &lt;p&gt;Original post:&lt;/p&gt; &lt;p&gt;According to Ollama's post here, &lt;a href="https://ollama.com/blog/qwen3-vl"&gt;https://ollama.com/blog/qwen3-vl&lt;/a&gt;, you can provide image paths to vision models when prompting via commandline. The /help command also confirms this.&lt;/p&gt; &lt;p&gt;But when I try to do it, the LLM responses indicate that it doesn't actually see the image, and it just responds as though the image path is literal text, eg. qwen3-vl:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;What do you see in this image? /home/user/Pictures/Screenshot_20250628_223552.png Thinking... Okay, the user is asking what I see in an image located at /home/user/Pictures/Screenshot_20250628_223552.png. Let me break this down.&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;First, I need to check the path. The path starts with /home/user, which is a standard user directory on Linux or Unix systems. The directory name &amp;quot;Pictures&amp;quot; is where images are typically stored. The filename is &amp;quot;Screenshot_20250628_223552.png&amp;quot;. &lt;/p&gt; &lt;p&gt;Wait, the date in the filename is 2025-06-28, which is in the future. That's a bit odd because the current year is 2023. Maybe it's a typo, or maybe it's a future date. But regardless, it's a screenshot filename with the date and time.&lt;/p&gt; &lt;p&gt;The user is asking me to describe what's in this image. But I don't have the actual image. The path provided is just a location, not the content. So I can't see the image. I should inform them that I can't view the image and maybe ask if they want to describe it or use another method&lt;sup&gt;C&lt;/sup&gt; ```&lt;/p&gt; &lt;p&gt;the same for llama3.2-vision:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;what is this image /home/user/Pictures/Screenshot_20250628_223552.png Unfortunately, I'm a large language model, I don't have the ability to directly access or view external files, including images. However, I can try to help you identify what the image might be.&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;If you're asking about the image file &lt;code&gt;/home/user/Pictures/Screenshot_20250628_223552.png&lt;/code&gt;, here are a few possibilities: ... ```&lt;/p&gt; &lt;p&gt;I have tried restarting ollama, updating ollama, restarting the PC it runs on, reinstalling the models. None have helped.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jozzo402"&gt; /u/jozzo402 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phbooy/ollama_vision_models_cant_see_images_via_console/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phbooy/ollama_vision_models_cant_see_images_via_console/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phbooy/ollama_vision_models_cant_see_images_via_console/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T12:55:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pho3g7</id>
    <title>Ollama + OpenVINO</title>
    <updated>2025-12-08T20:57:07+00:00</updated>
    <author>
      <name>/u/G4rp</name>
      <uri>https://old.reddit.com/user/G4rp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a notebook with an Intel NPU, I would like to use Ollama with it. Seems there is an initial support of Ollama with OpenVINO as a backend not much information about.&lt;/p&gt; &lt;p&gt;Anyone has experience with them and docker?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/G4rp"&gt; /u/G4rp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pho3g7/ollama_openvino/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pho3g7/ollama_openvino/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pho3g7/ollama_openvino/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T20:57:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1phzbsa</id>
    <title>Is there any hope one day we could buy Radxa AICore AX-M1?</title>
    <updated>2025-12-09T05:26:08+00:00</updated>
    <author>
      <name>/u/theodiousolivetree</name>
      <uri>https://old.reddit.com/user/theodiousolivetree</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theodiousolivetree"&gt; /u/theodiousolivetree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/SBCs/comments/1phzbfz/is_there_any_hope_one_day_we_could_buy_radxa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phzbsa/is_there_any_hope_one_day_we_could_buy_radxa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phzbsa/is_there_any_hope_one_day_we_could_buy_radxa/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T05:26:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1phprx0</id>
    <title>Nanocoder 1.18.0 - Multi-step tool calls, debugging mode, and searchable model database</title>
    <updated>2025-12-08T22:01:50+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1phprx0/nanocoder_1180_multistep_tool_calls_debugging/"&gt; &lt;img alt="Nanocoder 1.18.0 - Multi-step tool calls, debugging mode, and searchable model database" src="https://external-preview.redd.it/6NB3vTkt_CTvZIVvpoaPWkTNJd0mSRDP_VXtKMbUZkI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=226a675fb4732b1d83b0bfb2fd7f29a4d888dec1" title="Nanocoder 1.18.0 - Multi-step tool calls, debugging mode, and searchable model database" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/nanocoder/comments/1phpris/nanocoder_1180_multistep_tool_calls_debugging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phprx0/nanocoder_1180_multistep_tool_calls_debugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phprx0/nanocoder_1180_multistep_tool_calls_debugging/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T22:01:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1phcz5j</id>
    <title>Newbie: How to "teach" ollama with 150MB PDF</title>
    <updated>2025-12-08T13:54:53+00:00</updated>
    <author>
      <name>/u/GabesVirtualWorld</name>
      <uri>https://old.reddit.com/user/GabesVirtualWorld</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want my local Ollama have the knowledge that is in a 150MB PDF and then ask it questions about that pdf. Am I right in trying to upload this? But I'm hitting the 20MB upload limit, is there a way to change that limit?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GabesVirtualWorld"&gt; /u/GabesVirtualWorld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phcz5j/newbie_how_to_teach_ollama_with_150mb_pdf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phcz5j/newbie_how_to_teach_ollama_with_150mb_pdf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phcz5j/newbie_how_to_teach_ollama_with_150mb_pdf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T13:54:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi6n2c</id>
    <title>Models that has the least collapse when ctx length grows. Especially using it with tools.</title>
    <updated>2025-12-09T12:52:55+00:00</updated>
    <author>
      <name>/u/Express_Quail_1493</name>
      <uri>https://old.reddit.com/user/Express_Quail_1493</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Quail_1493"&gt; /u/Express_Quail_1493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pi6hbx/models_that_has_the_least_collapse_when_ctx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi6n2c/models_that_has_the_least_collapse_when_ctx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pi6n2c/models_that_has_the_least_collapse_when_ctx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T12:52:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi6ww5</id>
    <title>META AI LLM llama3.2 TERMUX</title>
    <updated>2025-12-09T13:05:50+00:00</updated>
    <author>
      <name>/u/PlayOnAndroid</name>
      <uri>https://old.reddit.com/user/PlayOnAndroid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pi6ww5/meta_ai_llm_llama32_termux/"&gt; &lt;img alt="META AI LLM llama3.2 TERMUX" src="https://preview.redd.it/zmeur9p5zw5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c69f4efc2eddb61badb6a791be033627bb1d4f0" title="META AI LLM llama3.2 TERMUX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PlayOnAndroid"&gt; /u/PlayOnAndroid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zmeur9p5zw5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi6ww5/meta_ai_llm_llama32_termux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pi6ww5/meta_ai_llm_llama32_termux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T13:05:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pii5xb</id>
    <title>Local alternative to Adobe Podcast</title>
    <updated>2025-12-09T20:23:23+00:00</updated>
    <author>
      <name>/u/LiveTradingChannel</name>
      <uri>https://old.reddit.com/user/LiveTradingChannel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a local alternative to Adobe Podcast for enhancing audio recordings quality?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiveTradingChannel"&gt; /u/LiveTradingChannel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pii5xb/local_alternative_to_adobe_podcast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pii5xb/local_alternative_to_adobe_podcast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pii5xb/local_alternative_to_adobe_podcast/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T20:23:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pigd6j</id>
    <title>Voiden: API specs, tests, and docs in one Markdown file</title>
    <updated>2025-12-09T19:17:08+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pigd6j/voiden_api_specs_tests_and_docs_in_one_markdown/"&gt; &lt;img alt="Voiden: API specs, tests, and docs in one Markdown file" src="https://external-preview.redd.it/emRkeGFpa2hhODZnMQ_KfphbB1yDooD3F3zuDYpLi0LXAD2_e9FaAoKZ51aj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ebdcf54aa18c2396060e65e8bda4c93c414fb97" title="Voiden: API specs, tests, and docs in one Markdown file" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Switching between API Client, browser, and API documentation tools to test and document APIs can harm your flow and leave your docs outdated.&lt;/p&gt; &lt;p&gt;This is what usually happens: While debugging an API in the middle of a sprint, the API Client says that everything's fine, but the docs still show an old version. &lt;/p&gt; &lt;p&gt;So you jump back to the code, find the updated response schema, then go back to the API Client, which gets stuck, forcing you to rerun the tests. &lt;/p&gt; &lt;p&gt;Voiden takes a different approach: Puts specs, tests &amp;amp; docs all in one Markdown file, stored right in the repo. &lt;/p&gt; &lt;p&gt;Everything stays in sync, versioned with Git, and updated in one place, inside your editor. &lt;/p&gt; &lt;p&gt;Download Voiden here: &lt;a href="https://voiden.md/download"&gt;https://voiden.md/download&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Join the discussion here : &lt;a href="https://discord.com/invite/XSYCf7JF4F"&gt;https://discord.com/invite/XSYCf7JF4F&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ps : I know it‚Äôs not related to ollama but found the community to appreciate other posts not so related.So posting here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y9qod4tha86g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pigd6j/voiden_api_specs_tests_and_docs_in_one_markdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pigd6j/voiden_api_specs_tests_and_docs_in_one_markdown/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T19:17:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1piodsg</id>
    <title>A batteries-included library for local AI ‚Äî spin up sandboxes from OCI images in any app.</title>
    <updated>2025-12-10T00:36:20+00:00</updated>
    <author>
      <name>/u/DorianZheng</name>
      <uri>https://old.reddit.com/user/DorianZheng</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been hacking on a small project that lets you equip (almost) any app with the ability to spawn sandboxes based on OCI-compatible images.&lt;/p&gt; &lt;p&gt;The idea is:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Your app doesn‚Äôt need to know container internals ‚Ä¢ It just asks the library to start a sandbox from an OCI image ‚Ä¢ The sandbox handles isolation, environment, etc. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use cases I had in mind:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Running untrusted code / plugins ‚Ä¢ Providing temporary dev environments ‚Ä¢ Safely executing user workloads from a web app &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Showcase power by this library &lt;a href="https://github.com/boxlite-labs/boxlite-mcp"&gt;https://github.com/boxlite-labs/boxlite-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm not sure if people would find this useful, so I‚Äôd really appreciate:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Feedback on the idea / design ‚Ä¢ Criticism on security assumptions ‚Ä¢ Suggestions for better DX or APIs ‚Ä¢ ‚ÄúThis already exists, go look at X‚Äù comments üôÇ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If there‚Äôs interest I can write a deeper dive on how it works internally (sandbox model, image handling, etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DorianZheng"&gt; /u/DorianZheng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1piodsg/a_batteriesincluded_library_for_local_ai_spin_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1piodsg/a_batteriesincluded_library_for_local_ai_spin_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1piodsg/a_batteriesincluded_library_for_local_ai_spin_up/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-10T00:36:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1piis2t</id>
    <title>Best LLM for writing text/summaries/tables under 30B</title>
    <updated>2025-12-09T20:46:40+00:00</updated>
    <author>
      <name>/u/tombino104</name>
      <uri>https://old.reddit.com/user/tombino104</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Could you recommend me an LLM that is able to make good quality texts, even summaries but also well-structured tables?&lt;/p&gt; &lt;p&gt;If it exists, but I doubt it, one of them allows you to create a sort of concept map even if not from a complete graphic point of view.&lt;/p&gt; &lt;p&gt;I‚Äôm looking for a preferably maximum 30B, I use LLMStudio, but I‚Äôm looking for Ollama models to use.&lt;/p&gt; &lt;p&gt;Thank you! üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tombino104"&gt; /u/tombino104 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1piis2t/best_llm_for_writing_textsummariestables_under_30b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1piis2t/best_llm_for_writing_textsummariestables_under_30b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1piis2t/best_llm_for_writing_textsummariestables_under_30b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T20:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi99uh</id>
    <title>Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found</title>
    <updated>2025-12-09T14:47:41+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pi99uh/which_small_model_is_best_for_finetuning_we/"&gt; &lt;img alt="Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found" src="https://preview.redd.it/kwjv946ey66g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f6c270a4751620f162cbf8017b5321a7a5b5017" title="Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; We fine-tuned 12 small models to find which ones are most tunable and perform best after fine-tuning. Surprise finding: Llama-3.2-1B showed the biggest improvement (most tunable), while Qwen3-4B delivered the best final performance - matching a 120B teacher on 7/8 tasks and outperforming by 19 points on the SQuAD 2.0 dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;12 models total - Qwen3 (8B, 4B, 1.7B, 0.6B), Llama (3.1-8B, 3.2-3B, 3.2-1B), SmolLM2 (1.7B, 135M), Gemma (1B, 270M), and Granite 8B.&lt;/p&gt; &lt;p&gt;Used GPT-OSS 120B as teacher to generate 10k synthetic training examples per task. Fine-tuned everything with identical settings: LoRA rank 64, 4 epochs, 5e-5 learning rate.&lt;/p&gt; &lt;p&gt;Tested on 8 benchmarks: classification tasks (TREC, Banking77, Ecommerce, Mental Health), document extraction, and QA (HotpotQA, Roman Empire, SQuAD 2.0).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding #1: Tunability (which models improve most)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The smallest models showed the biggest gains from fine-tuning. Llama-3.2-1B ranked #1 for tunability, followed by Llama-3.2-3B and Qwen3-0.6B.&lt;/p&gt; &lt;p&gt;This pattern makes sense - smaller models start weaker but have more room to grow. Fine-tuning closed the gap hard. The 8B models ranked lowest for tunability not because they're bad, but because they started strong and had less room to improve.&lt;/p&gt; &lt;p&gt;If you're stuck with small models due to hardware constraints, this is good news. Fine-tuning can make a 1B model competitive with much larger models on specific tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding #2: Best fine-tuned performance (can student match teacher?)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3-4B-Instruct-2507 came out on top for final performance. After fine-tuning, it matched or exceeded the 120B teacher on 7 out of 8 benchmarks.&lt;/p&gt; &lt;p&gt;Breakdown: TREC (+3 points), Docs (+2), Ecommerce (+3), HotpotQA (tied), Mental Health (+1), Roman Empire (+5). Only fell short on Banking77 by 3 points.&lt;/p&gt; &lt;p&gt;SQuAD 2.0 was wild - the 4B student scored 0.71 vs teacher's 0.52. That's a 19 point gap favoring the smaller model. A model 30x smaller outperforming the one that trained it.&lt;/p&gt; &lt;p&gt;Before fine-tuning, the 8B models dominated everything. After fine-tuning, model size mattered way less.&lt;/p&gt; &lt;p&gt;If you're running stuff on your own hardware, you can get frontier-level performance from a 4B model on a single consumer GPU. No expensive cloud instances. No API rate limits.&lt;/p&gt; &lt;p&gt;Let us know if there's a specific model you want benchmarked.&lt;/p&gt; &lt;p&gt;Full write-up: &lt;a href="https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning"&gt;https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kwjv946ey66g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi99uh/which_small_model_is_best_for_finetuning_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pi99uh/which_small_model_is_best_for_finetuning_we/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T14:47:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pif88t</id>
    <title>OSS 120 GPT vs ChatGPT 5.1</title>
    <updated>2025-12-09T18:34:31+00:00</updated>
    <author>
      <name>/u/FX2021</name>
      <uri>https://old.reddit.com/user/FX2021</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In real world performance &amp;quot;intelligence&amp;quot; how close or how far apart is OSS 120 compared to GPT 5.1? in the field of STEM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FX2021"&gt; /u/FX2021 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pif88t/oss_120_gpt_vs_chatgpt_51/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pif88t/oss_120_gpt_vs_chatgpt_51/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pif88t/oss_120_gpt_vs_chatgpt_51/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T18:34:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi5fr6</id>
    <title>Qwen3-Next here!</title>
    <updated>2025-12-09T11:48:29+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ollama.com/library/qwen3-next"&gt;https://ollama.com/library/qwen3-next&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen3-Next-80B-A3B is the first installment in the Qwen3-Next series and features the following key enhancements:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Hybrid Attention: Replaces standard attention with the combination of Gated DeltaNet and Gated Attention, enabling efficient context modeling for ultra-long context length. High-Sparsity Mixture-of-Experts (MoE): Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity. Stability Optimizations: Includes techniques such as zero-centered and weight-decayed layernorm, and other stabilizing enhancements for robust pre-training and post-training. Multi-Token Prediction (MTP): Boosts pretraining model performance and accelerates inference. &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;p&gt;requires ollama 0.13.2 &lt;a href="https://github.com/ollama/ollama/releases/tag/v0.13.2"&gt;https://github.com/ollama/ollama/releases/tag/v0.13.2&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Surprizy good for local model on my benchmark 50k tokens, read whole book &amp;quot;Alice in wonders&amp;quot; and ask all heroes Alice met&lt;/p&gt; &lt;ul&gt; &lt;li&gt;almost consistent inference speed regardless of context size&lt;/li&gt; &lt;li&gt;~40 t/s inference on w7900 48gb&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Upd: llama.cpp gives 40 t/s, ollama only 10 t/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi5fr6/qwen3next_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi5fr6/qwen3next_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pi5fr6/qwen3next_here/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T11:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi492e</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-12-09T10:37:50+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Notion Like Document Editing experience&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agentic chat&lt;/li&gt; &lt;li&gt;Note Management (Like Notion)&lt;/li&gt; &lt;li&gt;Multi Collaborative Chats.&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Installation (Self-Host)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Linux/macOS:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 \ -v surfsense-data:/data \ --name surfsense \ --restart unless-stopped \ ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Windows (PowerShell):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 ` -v surfsense-data:/data ` --name surfsense ` --restart unless-stopped ` ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi492e/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi492e/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pi492e/open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T10:37:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pisiq2</id>
    <title>Ollama now supports the rnj-1 model</title>
    <updated>2025-12-10T03:47:03+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pisiq2/ollama_now_supports_the_rnj1_model/"&gt; &lt;img alt="Ollama now supports the rnj-1 model" src="https://preview.redd.it/kqymea6bta6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4211dbfa7f03d832d8370b4d36bca83dec365ed7" title="Ollama now supports the rnj-1 model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;rnj-1 is the best Open-Source 8B-Parameter LLM Built in the USA and it is optimized for code and STEM with capabilities on par with SOTA open-weight models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; These models require the pre-release version of Ollama &lt;strong&gt;v0.13.3&lt;/strong&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kqymea6bta6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pisiq2/ollama_now_supports_the_rnj1_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pisiq2/ollama_now_supports_the_rnj1_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-10T03:47:03+00:00</published>
  </entry>
</feed>
