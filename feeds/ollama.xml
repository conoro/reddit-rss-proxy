<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-13T17:24:50+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1otsq4j</id>
    <title>RAG. Embedding model. What do u prefer ?</title>
    <updated>2025-11-10T22:52:33+00:00</updated>
    <author>
      <name>/u/apolorotov</name>
      <uri>https://old.reddit.com/user/apolorotov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm doing some research on real-world RAG setups and I‚Äôm curious which embedding models people actually use in production (or serious side projects).&lt;/p&gt; &lt;p&gt;There are dozens of options now ‚Äî OpenAI text-embedding-3, BGE-M3, Voyage, Cohere, Qwen3, local MiniLM, etc. But despite all the talk about ‚Äúdomain-specific embeddings‚Äù, I almost never see anyone training or fine-tuning their own.&lt;/p&gt; &lt;p&gt;So I‚Äôd love to hear from you: 1. Which embedding model(s) are you using, and for what kind of data/tasks? 2. Have you ever tried to fine-tune your own? Why or why not?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apolorotov"&gt; /u/apolorotov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otsq4j/rag_embedding_model_what_do_u_prefer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otsq4j/rag_embedding_model_what_do_u_prefer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otsq4j/rag_embedding_model_what_do_u_prefer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T22:52:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou1x4c</id>
    <title>Built a CLI tool to reuse AI instructions for specific tasks</title>
    <updated>2025-11-11T06:07:04+00:00</updated>
    <author>
      <name>/u/Revolutionary-Judge9</name>
      <uri>https://old.reddit.com/user/Revolutionary-Judge9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ou1x4c/built_a_cli_tool_to_reuse_ai_instructions_for/"&gt; &lt;img alt="Built a CLI tool to reuse AI instructions for specific tasks" src="https://external-preview.redd.it/bGQxbG05aDRpazBnMUXN_YYfWjInCTvbB8XSnnOq8UmtvCL2MyTJG86DAUjb.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bcf65b87e5e6aff456c7e4867e6af1c06677a893" title="Built a CLI tool to reuse AI instructions for specific tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Before creating Askimo, I used to keep my prompt templates in notes - things like &amp;quot;summarize this document concisely&amp;quot; or &amp;quot;write documentation that lists only a class‚Äôs responsibilities, not its implementation details.&amp;quot;&lt;br /&gt; Every time, I‚Äôd copy and paste those instructions into chat to get the same kind of result.&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://github.com/haiphucnguyen/askimo"&gt;Askimo&lt;/a&gt;, a CLI tool that lets you turn those instructions into reusable ‚Äúrecipes.‚Äù&lt;br /&gt; Now I can just run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;askimo -r summarize &amp;lt;file_path&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;User can write their own recipe to instruct the AI response&lt;/p&gt; &lt;p&gt;The CLI tool can also run in interactive mode, just like a regular chat program. It works locally with Ollama, and can also connect to other LLM providers.&lt;/p&gt; &lt;p&gt;If you have the problems like mine, this tool is worth for checkout and you know why I made it :D. &lt;/p&gt; &lt;p&gt;Github Repo: &lt;a href="https://github.com/haiphucnguyen/askimo"&gt;https://github.com/haiphucnguyen/askimo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Revolutionary-Judge9"&gt; /u/Revolutionary-Judge9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q315f9h4ik0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou1x4c/built_a_cli_tool_to_reuse_ai_instructions_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ou1x4c/built_a_cli_tool_to_reuse_ai_instructions_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T06:07:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou6ami</id>
    <title>Enterpise prices license</title>
    <updated>2025-11-11T10:43:43+00:00</updated>
    <author>
      <name>/u/LowTerrible9453</name>
      <uri>https://old.reddit.com/user/LowTerrible9453</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, we're working on an on-premise AI solution for an enterprise client and planning to integrate Ollama with a Web UI. Could you please share the pricing details or licensing requirements for enterprise use? Or if theres any consultant or business professionals who can support us let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LowTerrible9453"&gt; /u/LowTerrible9453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou6ami/enterpise_prices_license/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ou6ami/enterpise_prices_license/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ou6ami/enterpise_prices_license/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T10:43:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1otixju</id>
    <title>Ollama working well on the vs code</title>
    <updated>2025-11-10T16:49:23+00:00</updated>
    <author>
      <name>/u/Dry_Shower287</name>
      <uri>https://old.reddit.com/user/Dry_Shower287</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1otixju/ollama_working_well_on_the_vs_code/"&gt; &lt;img alt="Ollama working well on the vs code" src="https://preview.redd.it/fotbt1hplg0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aff4a58a24ebef77a98f660883ac714cad18fdef" title="Ollama working well on the vs code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Shower287"&gt; /u/Dry_Shower287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fotbt1hplg0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otixju/ollama_working_well_on_the_vs_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otixju/ollama_working_well_on_the_vs_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T16:49:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oubxxp</id>
    <title>model using 100% GPU in idle, is this normal?</title>
    <updated>2025-11-11T15:12:28+00:00</updated>
    <author>
      <name>/u/iMaexx_Backup</name>
      <uri>https://old.reddit.com/user/iMaexx_Backup</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oubxxp/model_using_100_gpu_in_idle_is_this_normal/"&gt; &lt;img alt="model using 100% GPU in idle, is this normal?" src="https://a.thumbs.redditmedia.com/LXyPa4BX87i_btgRKtZWhbozYyuaVqZLJEe8a0REts4.jpg" title="model using 100% GPU in idle, is this normal?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I'm new to this, so I apologize if the question is stupid. &lt;/p&gt; &lt;p&gt;I installed Ollama and Rocm on my Fedora distro, downloaded the latest gpt-oss:20b model and started it. &lt;/p&gt; &lt;p&gt;I noted that, as soon as I start the model, my GPU is and stays 100% utilized until I stop the model, even though I'm taking no inputs at all and the model is just idling. &lt;/p&gt; &lt;p&gt;Is this behavior normal? Shouldn't the utilization only go up after I did an input and the LLM is generating the response?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/olo09xm38n0g1.png?width=1168&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5a2966dad16985173674566b5bfe363312610cd"&gt;llm Idle&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/33oej42z8n0g1.png?width=1172&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d83596b58f27d4df12c1d8ea28d5d1df189b8ca9"&gt;llm off&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iMaexx_Backup"&gt; /u/iMaexx_Backup &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oubxxp/model_using_100_gpu_in_idle_is_this_normal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oubxxp/model_using_100_gpu_in_idle_is_this_normal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oubxxp/model_using_100_gpu_in_idle_is_this_normal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T15:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1our2h2</id>
    <title>Claudette Chatmode + Mimir memory bank integration</title>
    <updated>2025-11-12T00:51:29+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/GithubCopilot/comments/1our2aq/claudette_chatmode_mimir_memory_bank_integration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1our2h2/claudette_chatmode_mimir_memory_bank_integration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1our2h2/claudette_chatmode_mimir_memory_bank_integration/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T00:51:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1otwqev</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-11-11T01:44:51+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Note Management&lt;/li&gt; &lt;li&gt;Multi Collaborative Notebooks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otwqev/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1otwqev/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1otwqev/open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-11T01:44:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouxypo</id>
    <title>I cant get any of the uncensored abliterated gpt oss models to work</title>
    <updated>2025-11-12T06:34:49+00:00</updated>
    <author>
      <name>/u/Frogy_mcfrogyface</name>
      <uri>https://old.reddit.com/user/Frogy_mcfrogyface</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, Im having an issue with pretty much every uncensored gpt oss model. When I run it, it loads up and then nothing. When I load it up from CMD, it gets to the &amp;quot;Send a message (/? for help)&amp;quot;. I type in a message, it does its little spinny thing, then goes back to &amp;quot;Send a message (/? for help)&amp;quot;&lt;/p&gt; &lt;p&gt;current one im trying is Huihui-gpt-oss-20b-BF16-abliterated:Q4_K_M and no dice. The standard versions work just fine, as do other uncensored LLM's Any ideas? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frogy_mcfrogyface"&gt; /u/Frogy_mcfrogyface &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ouxypo/i_cant_get_any_of_the_uncensored_abliterated_gpt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ouxypo/i_cant_get_any_of_the_uncensored_abliterated_gpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ouxypo/i_cant_get_any_of_the_uncensored_abliterated_gpt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T06:34:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovcqwx</id>
    <title>Ollama just making shit up?</title>
    <updated>2025-11-12T18:13:14+00:00</updated>
    <author>
      <name>/u/groundserver</name>
      <uri>https://old.reddit.com/user/groundserver</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ovcqwx/ollama_just_making_shit_up/"&gt; &lt;img alt="Ollama just making shit up?" src="https://preview.redd.it/7c8nyux3av0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee2c701811da0212a4d75aa0dcea292f4df94441" title="Ollama just making shit up?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I asked Ollama 2 uncensored to provide a synopsis of a pdf and provided the link. The synopsis provided was not the subject of the linked PDF. Then I asked it again but didn't provide the link, and it just made shit up. Everytime I asked for a synopsis of a link and did not provide a link, it acted like I did, and returned different results every time. Wierd.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/groundserver"&gt; /u/groundserver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7c8nyux3av0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovcqwx/ollama_just_making_shit_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovcqwx/ollama_just_making_shit_up/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T18:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1our0hc</id>
    <title>What's Stopping you from using local AI models more?</title>
    <updated>2025-11-12T00:48:56+00:00</updated>
    <author>
      <name>/u/ButterscotchNo102</name>
      <uri>https://old.reddit.com/user/ButterscotchNo102</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ButterscotchNo102"&gt; /u/ButterscotchNo102 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1oukfvf/whats_stopping_you_from_using_local_ai_models_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1our0hc/whats_stopping_you_from_using_local_ai_models_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1our0hc/whats_stopping_you_from_using_local_ai_models_more/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T00:48:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov7kqc</id>
    <title>Masking the connection error in Ollama</title>
    <updated>2025-11-12T15:05:45+00:00</updated>
    <author>
      <name>/u/chirchan91</name>
      <uri>https://old.reddit.com/user/chirchan91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a simple way of masking the connection errors in Ollama when it failed to connect to the Model Server.&lt;/p&gt; &lt;p&gt;for example: Head &amp;quot;http://&amp;lt;internal‚Äëip&amp;gt;:11434/&amp;quot;: dial tcp &amp;lt;internal‚Äëip&amp;gt;:11434: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.&lt;/p&gt; &lt;p&gt;instead i should get &amp;quot;connection failed due host not responding. Please contact support&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chirchan91"&gt; /u/chirchan91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7kqc/masking_the_connection_error_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7kqc/masking_the_connection_error_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov7kqc/masking_the_connection_error_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T15:05:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov7t8l</id>
    <title>MCP Server for Blender - Built for PolyMCP Agent Orchestration</title>
    <updated>2025-11-12T15:14:21+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ov7t8l/mcp_server_for_blender_built_for_polymcp_agent/"&gt; &lt;img alt="MCP Server for Blender - Built for PolyMCP Agent Orchestration" src="https://external-preview.redd.it/JIrsfAZEJFJcSC_-Ows_UR3v_W3m64JAE2m0e3rAoNc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1896a8509f0f381f92c8934d66e06ad80e365455" title="MCP Server for Blender - Built for PolyMCP Agent Orchestration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/Blender-MCP-Server"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7t8l/mcp_server_for_blender_built_for_polymcp_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov7t8l/mcp_server_for_blender_built_for_polymcp_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T15:14:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov16ju</id>
    <title>Someone wrote an article about my library PolyMCP</title>
    <updated>2025-11-12T09:58:43+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ov16ju/someone_wrote_an_article_about_my_library_polymcp/"&gt; &lt;img alt="Someone wrote an article about my library PolyMCP" src="https://external-preview.redd.it/y0dgtZ0gVjnRje7BRhie_7gJWNn7zzoil-XFJAfPX90.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80e455756943072492fbf6c6135f659142eced5f" title="Someone wrote an article about my library PolyMCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://levelup.gitconnected.com/why-your-python-functions-arent-ai-tools-yet-and-how-polymcp-fixes-it-in-one-line-d8e62550ac53"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov16ju/someone_wrote_an_article_about_my_library_polymcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov16ju/someone_wrote_an_article_about_my_library_polymcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T09:58:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovdpch</id>
    <title>Is anyone from London?</title>
    <updated>2025-11-12T18:47:26+00:00</updated>
    <author>
      <name>/u/Dry_Music_7160</name>
      <uri>https://old.reddit.com/user/Dry_Music_7160</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Music_7160"&gt; /u/Dry_Music_7160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1ovdp64/is_anyone_from_london/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovdpch/is_anyone_from_london/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovdpch/is_anyone_from_london/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T18:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov62o7</id>
    <title>I taught my AI vtuber how to play osu! Here's how it went...</title>
    <updated>2025-11-12T14:07:41+00:00</updated>
    <author>
      <name>/u/imfstr</name>
      <uri>https://old.reddit.com/user/imfstr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ov62o7/i_taught_my_ai_vtuber_how_to_play_osu_heres_how/"&gt; &lt;img alt="I taught my AI vtuber how to play osu! Here's how it went..." src="https://external-preview.redd.it/kfbIE7cVIUZ7rju-fJnsEvS2hqkafGslwwkxU3XNQtc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb693bfc94522d852a0f35b056c144ca0965d28f" title="I taught my AI vtuber how to play osu! Here's how it went..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;some of you might remember my last post where i showed my AI, &lt;em&gt;Eris&lt;/em&gt;, picking out her dream PC setup on amazon. since then, i‚Äôve been working on something a bit crazier, i decided to teach her how to &lt;strong&gt;play osu!&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;in the video, she chats with me for a bit and then actually plays through &lt;strong&gt;two&lt;/strong&gt; osu! maps using a neural network I integrated into her system. It was a big leap from where she was before, and i learned a ton about AI decision-making, timing, and visual input in the process. &lt;/p&gt; &lt;p&gt;i‚Äôm always open to feedback, whether it‚Äôs about how she looks/it's animated, all the way to how she should respond, possible improvements to her interactivity, or just general advice for the project. &lt;/p&gt; &lt;p&gt;thanks again to everyone who gave feedback last time, it really helped a lot! :D&lt;/p&gt; &lt;p&gt;(she plays in an offline environment, and none of her scores get uploaded publicly incase it seems like this is cheating)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/imfstr"&gt; /u/imfstr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=R778HLEGeWg&amp;amp;t=2s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov62o7/i_taught_my_ai_vtuber_how_to_play_osu_heres_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov62o7/i_taught_my_ai_vtuber_how_to_play_osu_heres_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T14:07:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov7wch</id>
    <title>How high of a spec do you have to have in order to install ollama in local environment?</title>
    <updated>2025-11-12T15:17:29+00:00</updated>
    <author>
      <name>/u/SnooRegrets3378</name>
      <uri>https://old.reddit.com/user/SnooRegrets3378</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently work in a virtual machine environment where any website is unavailable. Is it possible to bring ollama into this type of setting? Exasperated by having to do everything with excel when you can use ai models and i work with sensitive datas so i would have to do the work locally.&lt;/p&gt; &lt;p&gt;Sorry in advance for the possible inaccurate word choice im not a computer guy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooRegrets3378"&gt; /u/SnooRegrets3378 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7wch/how_high_of_a_spec_do_you_have_to_have_in_order/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7wch/how_high_of_a_spec_do_you_have_to_have_in_order/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov7wch/how_high_of_a_spec_do_you_have_to_have_in_order/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T15:17:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovmy2a</id>
    <title>Nothink in the gui</title>
    <updated>2025-11-13T00:43:19+00:00</updated>
    <author>
      <name>/u/sceadwian</name>
      <uri>https://old.reddit.com/user/sceadwian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to /set nothink in the ollama GUI? I'm not seeing any places to pass command line paramaters and it doesn't take the command in the chat dialog box.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sceadwian"&gt; /u/sceadwian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovmy2a/nothink_in_the_gui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovmy2a/nothink_in_the_gui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovmy2a/nothink_in_the_gui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T00:43:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovt199</id>
    <title>Local Llama API</title>
    <updated>2025-11-13T05:36:53+00:00</updated>
    <author>
      <name>/u/TuLiSTua</name>
      <uri>https://old.reddit.com/user/TuLiSTua</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ovt199/local_llama_api/"&gt; &lt;img alt="Local Llama API" src="https://preview.redd.it/o87hysrgoy0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75d5a1be150cde933fbeac9cec14dee93e4c2a45" title="Local Llama API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following Situation: -self hosted Tandoor Recipes 2.3.3 instance -self hosted Ollama instance with llama3.2:latest&lt;/p&gt; &lt;p&gt;I want Tandoor to use my local AI to work as AI provider, but i need an API. The question is, where do I get it from?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TuLiSTua"&gt; /u/TuLiSTua &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o87hysrgoy0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovt199/local_llama_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovt199/local_llama_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T05:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow0x3g</id>
    <title>Thanks to Gowtham Boyina for featuring my library in his latest article üôè</title>
    <updated>2025-11-13T13:21:57+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ow0x3g/thanks_to_gowtham_boyina_for_featuring_my_library/"&gt; &lt;img alt="Thanks to Gowtham Boyina for featuring my library in his latest article üôè" src="https://external-preview.redd.it/y0dgtZ0gVjnRje7BRhie_7gJWNn7zzoil-XFJAfPX90.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80e455756943072492fbf6c6135f659142eced5f" title="Thanks to Gowtham Boyina for featuring my library in his latest article üôè" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://levelup.gitconnected.com/why-your-python-functions-arent-ai-tools-yet-and-how-polymcp-fixes-it-in-one-line-d8e62550ac53"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow0x3g/thanks_to_gowtham_boyina_for_featuring_my_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow0x3g/thanks_to_gowtham_boyina_for_featuring_my_library/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T13:21:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow31je</id>
    <title>Anyone running code model in cpu only VPS?</title>
    <updated>2025-11-13T14:50:13+00:00</updated>
    <author>
      <name>/u/Gcloud-AI</name>
      <uri>https://old.reddit.com/user/Gcloud-AI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to use codeseeker model in my vps but it's not good, not able to get any output from my model üòî. My vps spec is: 8 cpu core 32gb ram 1tb nvme storage Any guidance for me???&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gcloud-AI"&gt; /u/Gcloud-AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow31je/anyone_running_code_model_in_cpu_only_vps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow31je/anyone_running_code_model_in_cpu_only_vps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow31je/anyone_running_code_model_in_cpu_only_vps/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T14:50:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovyfb6</id>
    <title>Everything at Your Fingertips: How PolyMCP Makes Multi-Tool AI Simple</title>
    <updated>2025-11-13T11:14:36+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ovyfb6/everything_at_your_fingertips_how_polymcp_makes/"&gt; &lt;img alt="Everything at Your Fingertips: How PolyMCP Makes Multi-Tool AI Simple" src="https://external-preview.redd.it/OsRUlAPOIfUajUERak6rJLdgfe46_oa2w3fJR_8dpWg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0bc545ee48ac58062b8aa9b228116950bbd4c676" title="Everything at Your Fingertips: How PolyMCP Makes Multi-Tool AI Simple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/Polymcp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovyfb6/everything_at_your_fingertips_how_polymcp_makes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovyfb6/everything_at_your_fingertips_how_polymcp_makes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T11:14:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow3ne9</id>
    <title>Qual a melhor GPU para o llama 3(.1 ou .3)</title>
    <updated>2025-11-13T15:13:39+00:00</updated>
    <author>
      <name>/u/No_Progress432</name>
      <uri>https://old.reddit.com/user/No_Progress432</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Progress432"&gt; /u/No_Progress432 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ow3my9/qual_a_melhor_gpu_para_o_llama_31_ou_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow3ne9/qual_a_melhor_gpu_para_o_llama_31_ou_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow3ne9/qual_a_melhor_gpu_para_o_llama_31_ou_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T15:13:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow5e4z</id>
    <title>MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration</title>
    <updated>2025-11-13T16:20:51+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ow5e4z/mcp_server_for_industrial_iot_built_for_polymcp/"&gt; &lt;img alt="MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration" src="https://external-preview.redd.it/7DcHkpBMRVBJAoq05xem0Cu6v1pmCb6s2RmtluiBv_4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84327b5adc51d58c3e3d8bd34d3475931cf4f24a" title="MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/IoT-Edge-MCP-Server"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow5e4z/mcp_server_for_industrial_iot_built_for_polymcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow5e4z/mcp_server_for_industrial_iot_built_for_polymcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T16:20:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovzgqx</id>
    <title>Most powerful LLM for 10GB RTX 3080?</title>
    <updated>2025-11-13T12:12:34+00:00</updated>
    <author>
      <name>/u/HUG0gamingHD</name>
      <uri>https://old.reddit.com/user/HUG0gamingHD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for a llm that can fully take advantage of this gpu.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HUG0gamingHD"&gt; /u/HUG0gamingHD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovzgqx/most_powerful_llm_for_10gb_rtx_3080/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovzgqx/most_powerful_llm_for_10gb_rtx_3080/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovzgqx/most_powerful_llm_for_10gb_rtx_3080/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T12:12:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow61or</id>
    <title>Help me Kill or Confirm this Idea</title>
    <updated>2025-11-13T16:45:40+00:00</updated>
    <author>
      <name>/u/Navaneeth26</name>
      <uri>https://old.reddit.com/user/Navaneeth26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôre &lt;strong&gt;building ModelMatch&lt;/strong&gt;, a beta open source project that &lt;strong&gt;recommends open source models&lt;/strong&gt; for specific jobs, not generic benchmarks.&lt;/p&gt; &lt;p&gt;So far we cover 5 domains: summarization, therapy advising, health advising, email writing, and finance assistance.&lt;/p&gt; &lt;p&gt;The point is simple: most teams still pick models based on vibes, vendor blogs, or random Twitter threads. In short we help people recommend the best model for a certain use case via our leadboards and open source eval frameworks using gpt 4o and Claude 3.5 Sonnet.&lt;/p&gt; &lt;p&gt;How we do it: we run models through our open source evaluator with task-specific rubrics and strict rules. Each &lt;strong&gt;run produces a 0-10 score&lt;/strong&gt; plus notes. We‚Äôve &lt;strong&gt;finished initial testing&lt;/strong&gt; and have a provisional top three for each domain. We are showing results through short YouTube breakdowns and on our site.&lt;/p&gt; &lt;p&gt;We know it is not perfect yet but what i am looking for is a reality check on the idea itself.&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;looking for feedback&lt;/strong&gt; on this so as to improve. Do u think:&lt;/p&gt; &lt;p&gt;A recommender like this is actually needed for real work, or is model choice not a real pain?&lt;/p&gt; &lt;p&gt;Be blunt. If this is noise, say so and why. If it is useful, tell me the one change that would get you to use it&lt;/p&gt; &lt;p&gt;P.S: we are also &lt;strong&gt;looking for contributors&lt;/strong&gt; to our project&lt;/p&gt; &lt;p&gt;Links in the first comment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Navaneeth26"&gt; /u/Navaneeth26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow61or/help_me_kill_or_confirm_this_idea/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow61or/help_me_kill_or_confirm_this_idea/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow61or/help_me_kill_or_confirm_this_idea/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T16:45:40+00:00</published>
  </entry>
</feed>
