<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-24T12:21:29+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1rbha7b</id>
    <title>Unable to pull model from ollama</title>
    <updated>2026-02-22T09:12:51+00:00</updated>
    <author>
      <name>/u/badasssravikumae</name>
      <uri>https://old.reddit.com/user/badasssravikumae</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to pull models from ollama but I am unable to do &lt;/p&gt; &lt;p&gt;I did ollama serve&lt;br /&gt; I deleted the cache and checked if the models is available and tried pulling the model even though I see this error: &lt;/p&gt; &lt;p&gt;pulling manifest &lt;/p&gt; &lt;p&gt;Error: pull model manifest: file does not exist&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badasssravikumae"&gt; /u/badasssravikumae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T09:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbdq0m</id>
    <title>Agent architectures for SLMs</title>
    <updated>2026-02-22T05:43:51+00:00</updated>
    <author>
      <name>/u/PangolinPossible7674</name>
      <uri>https://old.reddit.com/user/PangolinPossible7674</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;What kind of agent architectures are generally used with Small Language Models? In the past, I had tried ReAct with some 8B param models, and they failed. Recently, I have been trying out tool calling models via Ollama. Even with function calling, Qwen 3 8B appears to somewhat work, but some other 8B models don't seem to be so great.&lt;/p&gt; &lt;p&gt;Therefore, I was wondering what SLM-agent gas worked for the others. Does verbose docstrings for tools affect performance with SLMs? Alternatively, what smallest model size generally allows diverse tool usage in a reliable way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PangolinPossible7674"&gt; /u/PangolinPossible7674 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T05:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc0r8o</id>
    <title>Got $800 of credits on a cloud platform (for GPU usage). Anyone here that's into AI training and inference and could make use of it?</title>
    <updated>2026-02-22T23:09:21+00:00</updated>
    <author>
      <name>/u/DocumentFun9077</name>
      <uri>https://old.reddit.com/user/DocumentFun9077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have around 800 bucks worth of GPU usage credits on one of the major platform, those can be used specifically for GPU and clusters. So if any individual or hobbyist or anyone out here is training models or inference, or anything else, please contact! (not free btw, but selling at way less price)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocumentFun9077"&gt; /u/DocumentFun9077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0r8o/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0r8o/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc0r8o/got_800_of_credits_on_a_cloud_platform_for_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T23:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc0sqn</id>
    <title>Got $800 of credits on a cloud platform (for GPU usage). Anyone here that's into AI training and inference and could make use of it?</title>
    <updated>2026-02-22T23:11:05+00:00</updated>
    <author>
      <name>/u/DocumentFun9077</name>
      <uri>https://old.reddit.com/user/DocumentFun9077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have around 800 bucks worth of GPU usage credits on one of the major platform, those can be used specifically for GPU and clusters. So if any individual or hobbyist or anyone out here is training models or inference, or anything else, please contact! (not free btw, but selling at way less price)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocumentFun9077"&gt; /u/DocumentFun9077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0sqn/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc0sqn/got_800_of_credits_on_a_cloud_platform_for_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc0sqn/got_800_of_credits_on_a_cloud_platform_for_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T23:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1rajqj6</id>
    <title>15,000+ tok/s on ChatJimmy: Is the "Model-on-Silicon" era finally starting?</title>
    <updated>2026-02-21T06:19:08+00:00</updated>
    <author>
      <name>/u/Significant-Topic433</name>
      <uri>https://old.reddit.com/user/Significant-Topic433</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"&gt; &lt;img alt="15,000+ tok/s on ChatJimmy: Is the &amp;quot;Model-on-Silicon&amp;quot; era finally starting?" src="https://preview.redd.it/bq69s0n5jskg1.jpg?width=140&amp;amp;height=66&amp;amp;auto=webp&amp;amp;s=fa3f690c9b529f18075dc6e27d8b984f7fcc4fcd" title="15,000+ tok/s on ChatJimmy: Is the &amp;quot;Model-on-Silicon&amp;quot; era finally starting?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weâ€™ve been discussing local inference for years, but chatjimmy.ai just moved the goalposts. They are hitting 15,414 tokens per second using what they call &amp;quot;mask ROM recall fabric&amp;quot;â€”basically etching the model weights directly into the silicon logic.&lt;/p&gt; &lt;p&gt;â€‹This is a massive shift from our current setups. Weâ€™re used to general-purpose compute, but this is a dedicated ASIC. No HBM, no VRAM bottlenecks, just raw, hardcoded inference.&lt;/p&gt; &lt;p&gt;â€‹ I just invested in two Gigabyte AI TOP ATOM units (the ones based on the NVIDIA Spark / Grace Blackwell architecture). They are absolute beasts for training and fine-tuning with 128GB of unified memory, but seeing a dedicated chip do 15k tok/s makes me wonder: &lt;/p&gt; &lt;p&gt;â€‹Did I make the right call with the AI TOP Spark units for local dev, or are we going to see these specialized ASIC cards hit the market soon and make general-purpose desktop AI look like dial-up?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant-Topic433"&gt; /u/Significant-Topic433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rajqj6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T06:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbt4di</id>
    <title>Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation</title>
    <updated>2026-02-22T18:17:48+00:00</updated>
    <author>
      <name>/u/dorbeats</name>
      <uri>https://old.reddit.com/user/dorbeats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"&gt; &lt;img alt="Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation" src="https://preview.redd.it/vj71i95883lg1.jpg?width=140&amp;amp;height=140&amp;amp;auto=webp&amp;amp;s=253fed1c03e9709c1d506fd05f1a1a3300ef8eda" title="Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this feasible on modest hardware?&lt;/p&gt; &lt;p&gt;Llama 3.1 70B on RTX 3090: Bypassing CPU for AI Innovation&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.boomspot.com/llama-3-1-70b-on-rtx-3090-bypassing-cpu-for-ai-innovation"&gt;https://www.boomspot.com/llama-3-1-70b-on-rtx-3090-bypassing-cpu-for-ai-innovation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vj71i95883lg1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c0859a6b570010f0108ab4369475820f191095ed"&gt;https://preview.redd.it/vj71i95883lg1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c0859a6b570010f0108ab4369475820f191095ed&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dorbeats"&gt; /u/dorbeats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbt4di/llama_31_70b_on_rtx_3090_bypassing_cpu_for_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T18:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc3fkr</id>
    <title>Model requires more system memory (eventhough I have enough)</title>
    <updated>2026-02-23T01:03:23+00:00</updated>
    <author>
      <name>/u/gfejer</name>
      <uri>https://old.reddit.com/user/gfejer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have two Tesla P40s passed through as vGPU profiles to a Ubuntu 24.04 VM. As an example I can run gpt-oss just fine and the GPUs get recognized by the system, but when it comes to running Llama 3.3 which is a 43GB model (my 2*24GB VRAM should be enough right?) gives me an error that I donâ€™t have enough system memory.&lt;/p&gt; &lt;p&gt;I am guessing that for some reason it tries to run the model on the CPU, but I donâ€™t understand whyâ€¦ Are there any possible fixes for this issue?&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gfejer"&gt; /u/gfejer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:03:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbq3dy</id>
    <title>Ollama for Dummies</title>
    <updated>2026-02-22T16:24:37+00:00</updated>
    <author>
      <name>/u/catbutchie</name>
      <uri>https://old.reddit.com/user/catbutchie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone needs to write a book. Right now my mentor is ChatGPT. There are so many parameters i just tell it my issue and it tells me what to change. Some small tweaks are significant performance adjustment. Iâ€™m new obviously. Iâ€™d like to know why Iâ€™m doing what Iâ€™m doing so I can be more in control. Iâ€™m using silly tavern and ollama for self hosted chat. Suggestions needed. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/catbutchie"&gt; /u/catbutchie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T16:24:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc7fd1</id>
    <title>AMD 5550xt Macbook Pro 2019</title>
    <updated>2026-02-23T04:11:48+00:00</updated>
    <author>
      <name>/u/tubaraodogroove</name>
      <uri>https://old.reddit.com/user/tubaraodogroove</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm planning to purchase this MacBook second-hand just to run some models that my 2017 laptop can't handle.&lt;/p&gt; &lt;p&gt;However, based on what I found during my research, Ollama doesnâ€™t recognize this graphics card as a usable GPU. Can someone explain whether this is a good deal for running Ollama, or if it simply wonâ€™t work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tubaraodogroove"&gt; /u/tubaraodogroove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc7fd1/amd_5550xt_macbook_pro_2019/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc7fd1/amd_5550xt_macbook_pro_2019/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc7fd1/amd_5550xt_macbook_pro_2019/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T04:11:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rccbn8</id>
    <title>Built a honeypot token library for AI agents â€” detects prompt injection the moment it succeeds</title>
    <updated>2026-02-23T08:49:56+00:00</updated>
    <author>
      <name>/u/Responsible-Yak-9657</name>
      <uri>https://old.reddit.com/user/Responsible-Yak-9657</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rccbn8/built_a_honeypot_token_library_for_ai_agents/"&gt; &lt;img alt="Built a honeypot token library for AI agents â€” detects prompt injection the moment it succeeds" src="https://external-preview.redd.it/y-WYMevcIN_5-ZHiVkm0emVpTgZqx82u0L69rZGk6FM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18cf100549c9b843e934f38bccc2bb7dc338f724" title="Built a honeypot token library for AI agents â€” detects prompt injection the moment it succeeds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible-Yak-9657"&gt; /u/Responsible-Yak-9657 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/aiagents/comments/1rby96z/built_a_honeypot_token_library_for_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rccbn8/built_a_honeypot_token_library_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rccbn8/built_a_honeypot_token_library_for_ai_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T08:49:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcgy0h</id>
    <title>Help Setting up - nanobot?</title>
    <updated>2026-02-23T13:07:26+00:00</updated>
    <author>
      <name>/u/lawfulcrispy</name>
      <uri>https://old.reddit.com/user/lawfulcrispy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lawfulcrispy"&gt; /u/lawfulcrispy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1rcgxpv/help_setting_up_nanobot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcgy0h/help_setting_up_nanobot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcgy0h/help_setting_up_nanobot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T13:07:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc4l9j</id>
    <title>16GB VRAM for mode agent</title>
    <updated>2026-02-23T01:56:50+00:00</updated>
    <author>
      <name>/u/ColdTransition5828</name>
      <uri>https://old.reddit.com/user/ColdTransition5828</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was hoping to enjoy something similar to Cursor on my PC. I even bought what was supposed to be a mid-range card. But the results are disappointing.&lt;/p&gt; &lt;p&gt;After studying it, I realized I'm missing the core agent and a better Ollama model that accepts tools. But honestly, I'm bored. What do you recommend I do to get the most out of local models with my 16GB of VRAM?&lt;/p&gt; &lt;p&gt;I mostly do full-track coding and Java.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ColdTransition5828"&gt; /u/ColdTransition5828 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:56:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcaxo7</id>
    <title>Qwen3VL:30b-Q6-Thinking</title>
    <updated>2026-02-23T07:23:02+00:00</updated>
    <author>
      <name>/u/CheekyMonkeee</name>
      <uri>https://old.reddit.com/user/CheekyMonkeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been trying for hours to make a Modelfile that will actually make this work in Ollama. I have the ggufs for both the base model and the mmproj together in a folder and the create command finishes successfully. I get the directory and the blobs and the chat accepts images, but I get a 500 error when I prompt. Downloaded both ggufs from huggingface.&lt;/p&gt; &lt;p&gt;At this point, I donâ€™t even care if itâ€™s the best model for my use case anymore. Iâ€™m fairly sure itâ€™s not a memory issue as Iâ€™m on a 5090 with 64GB of system RAM, and it will run the Q4 32b non-thinking model that I downloaded straight from Ollama (even though that spills over into RAM).&lt;/p&gt; &lt;p&gt;Iâ€™ve just never had to do the modelfile thing with a download from huggingface before (still fairly new to this) and I donâ€™t want to give up. I have to sleep, but any advice would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CheekyMonkeee"&gt; /u/CheekyMonkeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcaxo7/qwen3vl30bq6thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcaxo7/qwen3vl30bq6thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcaxo7/qwen3vl30bq6thinking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T07:23:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcurz4</id>
    <title>Building a service and PWA for Ollama (and other models) with SQLite RAG and artifacts. Is this project interesting to the community?</title>
    <updated>2026-02-23T21:37:21+00:00</updated>
    <author>
      <name>/u/pokemondodo</name>
      <uri>https://old.reddit.com/user/pokemondodo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! For almost a year, Iâ€™ve been working on a project that serves as a smart, functional, and secure UI for LLM models. There are many ready-made solutions, but most often they require complex Docker setups or writing configurations. Projects with a simpler launch but similar functionality are usually paid.&lt;/p&gt; &lt;p&gt;My solution works in the browser or via a PWA application. Absolutely all computations happen on the user's device. The project has no server at all; hosting with SSL is only needed to organize the PWA application.&lt;/p&gt; &lt;p&gt;In general, the project will work even without internet if the models are deployed locally and the PWA application is already downloaded.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical points I focused on:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Client-side data storage&lt;/strong&gt; â€” a SQLite database is used. Once created, you can place it anywhere; the browser will only ask for permission to write to the file. Everything is stored in the database: chats, messages, embeddings, system settings, artifacts. You can change databases whenever you want.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic module&lt;/strong&gt; â€” through triggers, the application extracts any important facts about the user. Name, other people, allergies, city of residence, favorite games â€” anything. Everything is stored in the selected database as a text fact and an embedding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Heuristic module&lt;/strong&gt; â€” the project has a mascot that displays its emotions in the form of stickers under each message. This can be turned off in the settings. Besides this, the assistant has its own mood. Through a mathematical expression, its final behavior is calculated based on variables: general mood, level of sarcasm, level of humor. This doesn't affect the quality of the answer or tasks, but it affects human perception â€” answers can be dry, restrained, or sarcastic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Artifacts&lt;/strong&gt; â€” the project has a library of documents and an application. There are 4 types of artifacts in total: games, applications, documents, analytical documents. You can ask to generate a document by prompt or by feeding information through a file attachment.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Working with files&lt;/strong&gt; â€” PDF, DOCX, XLSX, TXT, and any files that can be interpreted as text or code are accepted. Nothing goes anywhere beyond your device; text is extracted at the moment of the request by the application or the browser tab.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt; â€” the database itself you work in is not encrypted, only the password. This is done so that you don't lose access to your documents and chats even without using the project. But in the project, connecting any database or entering settings happens only through a password.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Operating modes&lt;/strong&gt; â€” there are three modes: Kids, Teens, and Adult. I should probably write a whole separate post for this. Briefly: the kids' mode is protected from adult and dangerous topics, and the assistant will not give answers to homework, only tell and help with what and how to solve.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compatibility&lt;/strong&gt; â€” it will work not only with local models but also through cloud APIs. It's not as secure, of course, but you can buy tokens in Gemini or a plan in OpenRouter â€” everything will work perfectly. There are three connection providers in total: Gemini, OpenRouter, Custom (Ollama, etc.). You can change providers on the fly in any chat; the assistant's behavior will only change due to the power of the model itself.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why Iâ€™m writing this:&lt;/strong&gt; This is a completely free project. It started as a tool for personal needs to check semantics in another project.&lt;/p&gt; &lt;p&gt;The project has no ads, subscriptions, price plans, or anywhere you need to enter your card details.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would the community be interested in a full technical review with a demonstration of functionality?&lt;/strong&gt; Or is this too niche a topic for such a sub?&lt;/p&gt; &lt;p&gt;If a demonstration is needed, won't I get banned for advertising or promotion? And what aspects do you want me to reveal in more detail â€” general demonstration or a deep dive into code and architecture?&lt;/p&gt; &lt;p&gt;I'll be glad to hear your opinion and am ready to answer any questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pokemondodo"&gt; /u/pokemondodo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcurz4/building_a_service_and_pwa_for_ollama_and_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcurz4/building_a_service_and_pwa_for_ollama_and_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcurz4/building_a_service_and_pwa_for_ollama_and_other/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T21:37:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcv5by</id>
    <title>I turned MCP tools into standard CLI commands to solve context pollution</title>
    <updated>2026-02-23T21:51:03+00:00</updated>
    <author>
      <name>/u/_pdp_</name>
      <uri>https://old.reddit.com/user/_pdp_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;One of the biggest issues with MCP is context pollution. Loading a single service might be fine, but when you have 10 or 100 of them, you're spending most of your valuable context on tool definitions.&lt;/p&gt; &lt;p&gt;The usual solution is to use an MCP gateway that exposes a single generic function. Unfortunately, this doesn't work well because with a single function, the context of how and when to use each tool is completely lost.&lt;/p&gt; &lt;p&gt;What I've found is that the best approach is usually the Unix way. So what if, instead of loading MCP tools into the context, you make them available as standard CLIs? Now you can write your own SKILL.md and be happy.&lt;/p&gt; &lt;p&gt;This is what MCPShim does. It starts a background daemon that keeps all your MCPs nicely organized. It supports all authentication types (including OAuth, even when you don't have a publicly exposed HTTP server). Most importantly, you can now call into any of these MCPs and tools like standard commands. MCPShim even automatically generates bashrc aliases and bash completion to make things super easy.&lt;/p&gt; &lt;p&gt;As an added benefit, if you develop your own agents you no longer need to bolt on an MCP library and handle everything manually. You can focus on building a lean, high-quality AI agent and leave the MCP work to system processes.&lt;/p&gt; &lt;p&gt;The link to the open-source project is in the comments below.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_pdp_"&gt; /u/_pdp_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcv5by/i_turned_mcp_tools_into_standard_cli_commands_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcv5by/i_turned_mcp_tools_into_standard_cli_commands_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcv5by/i_turned_mcp_tools_into_standard_cli_commands_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T21:51:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc8xvy</id>
    <title>spend more time downloading models than actually using them</title>
    <updated>2026-02-23T05:29:41+00:00</updated>
    <author>
      <name>/u/Sharp-Mouse9049</name>
      <uri>https://old.reddit.com/user/Sharp-Mouse9049</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;qwen 2.5 dropped and suddenly mixtral is dead to me. downloaded the 72b. ran it once. went back to 7b cause i dont actually need 72b for anything i do&lt;/p&gt; &lt;p&gt;got like 200gb of models sitting on my drive. couldnt tell you the difference between half of them without checking the folder names&lt;/p&gt; &lt;p&gt;every week theres a new one thats supposedly better and i gotta have it. run some vibes check. wow this one feels smarter. back to doing the same three things i always do&lt;/p&gt; &lt;p&gt;its like im collecting pokemon but the pokemon just sit there&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sharp-Mouse9049"&gt; /u/Sharp-Mouse9049 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc8xvy/spend_more_time_downloading_models_than_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc8xvy/spend_more_time_downloading_models_than_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc8xvy/spend_more_time_downloading_models_than_actually/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T05:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc3srb</id>
    <title>I ran ClawBot with Ollama locally on my Mac â€” setup, gotchas, and honest review</title>
    <updated>2026-02-23T01:19:58+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"&gt; &lt;img alt="I ran ClawBot with Ollama locally on my Mac â€” setup, gotchas, and honest review" src="https://external-preview.redd.it/anBvOHE4ZWxiNWxnMYmmpi9UU3yP9yrC87ePDCyv5Mn4iZk_AUHCQZq2TOQ_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=452fe5fc9cf576221ea71aff1d15b07c8fa36f35" title="I ran ClawBot with Ollama locally on my Mac â€” setup, gotchas, and honest review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all â€” been experimenting with running a local AI agent using ClawBot + Ollama and wanted to share what actually happened.&lt;/p&gt; &lt;p&gt;Link to full tutorial: &lt;a href="https://www.youtube.com/watch?v=FxyQkj95VXs"&gt;https://www.youtube.com/watch?v=FxyQkj95VXs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yes, ClawBot + Ollama works on Mac. Does it work well? Depends on what you mean by &amp;quot;work&amp;quot;&lt;/li&gt; &lt;li&gt;With an 8B model, agentic tasks are limited. Basic Q&amp;amp;A? Fine. Anything complex? It'll humble you real quick&lt;/li&gt; &lt;li&gt;Should you expect ChatGPT-level speed? Absolutely not. Go make a coffee while you wait ðŸ˜…&lt;/li&gt; &lt;li&gt;Is it worth it for learning the stack and experimenting locally for free? Honestly yes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What worked&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Setup is cleaner than expected - VS Code, JSON config, localhost dashboard, done. I have no luck setting up ollama using their onboarding. So...I went straight to config file.&lt;/li&gt; &lt;li&gt;Ollama model switching is straightforward once you understand the config structure&lt;/li&gt; &lt;li&gt;Great for understanding how local AI agent setups actually work under the hood&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What didn't&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Speed is rough on anything under 32GB RAM&lt;/li&gt; &lt;li&gt;8B models hit their ceiling fast on multi-step reasoning and real agentic workflows. Keep context window low.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1ee663elb5lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcf94q</id>
    <title>What GPU do you use?</title>
    <updated>2026-02-23T11:43:10+00:00</updated>
    <author>
      <name>/u/Keensworth</name>
      <uri>https://old.reddit.com/user/Keensworth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've recently started using Ollama with an old GPU I had laying around.&lt;/p&gt; &lt;p&gt;Problem is that my GTX 1660S only got 6Gb VRAM and you can't do much with that.&lt;/p&gt; &lt;p&gt;I can run Mistral 7B Instruct but he sucks.&lt;/p&gt; &lt;p&gt;What hardware are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Keensworth"&gt; /u/Keensworth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T11:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd9l5i</id>
    <title>Best practices for running local LLMs for ~70â€“150 developers (agentic coding use case)</title>
    <updated>2026-02-24T07:16:14+00:00</updated>
    <author>
      <name>/u/Resident_Potential97</name>
      <uri>https://old.reddit.com/user/Resident_Potential97</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Resident_Potential97"&gt; /u/Resident_Potential97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1rd9kpk/best_practices_for_running_local_llms_for_70150/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd9l5i/best_practices_for_running_local_llms_for_70150/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rd9l5i/best_practices_for_running_local_llms_for_70150/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T07:16:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rddkjr</id>
    <title>Just Dropped My First Ollama + AI Video â€” What Should I Build Next?</title>
    <updated>2026-02-24T11:21:02+00:00</updated>
    <author>
      <name>/u/SnooApples5040</name>
      <uri>https://old.reddit.com/user/SnooApples5040</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just dropped my first AI video, and it's helping my friends and others around me. Ollama is so easy to use, and I've seen it as the base in #Ethical #Hacking #AI videos. &lt;/p&gt; &lt;p&gt;Here is the video: &lt;a href="https://youtu.be/snDTuCuxfzE"&gt;https://youtu.be/snDTuCuxfzE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want to keep helping my friends. So I'd love your input, &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;What is something about Ollama or AI you wish someone told you?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What would be the next logical step?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Do you have specific resources you find better than others?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;p&gt;Christy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooApples5040"&gt; /u/SnooApples5040 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rddkjr/just_dropped_my_first_ollama_ai_video_what_should/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rddkjr/just_dropped_my_first_ollama_ai_video_what_should/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rddkjr/just_dropped_my_first_ollama_ai_video_what_should/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T11:21:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcxt3m</id>
    <title>How are you monitoring your Ollama calls/usage?</title>
    <updated>2026-02-23T23:32:55+00:00</updated>
    <author>
      <name>/u/gkarthi280</name>
      <uri>https://old.reddit.com/user/gkarthi280</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"&gt; &lt;img alt="How are you monitoring your Ollama calls/usage?" src="https://preview.redd.it/b8gxcch9xblg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=ced5ee353224c286bd1ab54c4819b57ff238fad4" title="How are you monitoring your Ollama calls/usage?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Ollama in my LLM applications and wanted some feedback on what type of metrics people here would find useful to track in an app that eventually would go into prod. I used OpenTelemetry to instrument my app by following this&lt;a href="https://signoz.io/docs/ollama-monitoring/"&gt; Ollama observability guide&lt;/a&gt; and was able to create this dashboard.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b8gxcch9xblg1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc90458a61e2e80c8ed5e283edc3e914ccbddcd6"&gt;Ollama dashboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It tracks things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;token usage&lt;/li&gt; &lt;li&gt;error rate&lt;/li&gt; &lt;li&gt;number of requests&lt;/li&gt; &lt;li&gt;latency&lt;/li&gt; &lt;li&gt;LLM provider and model &amp;amp; token distribution&lt;/li&gt; &lt;li&gt;logs and errors&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Are there any important metrics that you would want to keep track of in prod for monitoring your Ollama usage that aren't included here? And have you guys found any other ways to monitor these llm calls made through ollama?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1r8j5ob"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gkarthi280"&gt; /u/gkarthi280 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T23:32:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdeex0</id>
    <title>GPU issue with running Models locally</title>
    <updated>2026-02-24T12:02:07+00:00</updated>
    <author>
      <name>/u/Badincomputer</name>
      <uri>https://old.reddit.com/user/Badincomputer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have setup my rig with 5 Geforce GTX titan X maxwell gpus. &lt;/p&gt; &lt;p&gt;Nvidia driver is 532. 7b models are running fine but when i try to run modes 30b or higher. I just loads on only one gpu amd then just crashed. I have installed 64 gb ram as well but i have having issues running bigger models. I think this is the software or driver issue. Can anyone help me please&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Badincomputer"&gt; /u/Badincomputer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdeex0/gpu_issue_with_running_models_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdeex0/gpu_issue_with_running_models_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdeex0/gpu_issue_with_running_models_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T12:02:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd5ely</id>
    <title>Full GPU Acceleration for Ollama on Mac Pro 2013 (Dual FirePro D700) - Linux</title>
    <updated>2026-02-24T04:23:21+00:00</updated>
    <author>
      <name>/u/manu7irl</name>
      <uri>https://old.reddit.com/user/manu7irl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;# [Guide] Full GPU Acceleration for Ollama on Mac Pro 2013 (Dual FirePro D700) - Linux&lt;/p&gt; &lt;p&gt;Hey everyone! I finally managed to get full GPU acceleration working for **Ollama** on the legendary **Mac Pro 6.1 (2013 &amp;quot;Trashcan&amp;quot;)** running Nobara Linux (and it should work on other distros too).&lt;/p&gt; &lt;p&gt;The problem with these machines is that they have dual **AMD FirePro D700s (Tahiti XT)**. By default, Linux uses the legacy `radeon` driver for these cards. While `radeon` works for display, it **does not support Vulkan or ROCm**, meaning Ollama defaults to the CPU, which is slow as molasses.&lt;/p&gt; &lt;p&gt;### My Setup:&lt;/p&gt; &lt;p&gt;- **Model:** Mac Pro 6,1 (Late 2013)&lt;/p&gt; &lt;p&gt;- **CPU:** Xeon E5-1680 v2 (8C/16T @ 3.0 GHz)&lt;/p&gt; &lt;p&gt;- **RAM:** 32GB&lt;/p&gt; &lt;p&gt;- **GPU:** Dual AMD FirePro D700 (6GB each, 12GB total VRAM)&lt;/p&gt; &lt;p&gt;- **OS:** Nobara Linux (Fedora 40/41 base)&lt;/p&gt; &lt;p&gt;### The Solution:&lt;/p&gt; &lt;p&gt;We need to force the `amdgpu` driver for the Southern Islands (SI) architecture. Once `amdgpu` is active, Vulkan is enabled, and Ollama picks up both GPUs automatically!&lt;/p&gt; &lt;p&gt;### Performance (The Proof):&lt;/p&gt; &lt;p&gt;I'm currently testing **`qwen2.5-coder:14b`** (9GB model). &lt;/p&gt; &lt;p&gt;- **GPU Offload:** 100% (49/49 layers)&lt;/p&gt; &lt;p&gt;- **VRAM Split:** Perfectly balanced across both D700s (~4GB each)&lt;/p&gt; &lt;p&gt;- **Speed:** **~11.5 tokens/second** ðŸš€&lt;/p&gt; &lt;p&gt;- **Total Response Time:** ~13.8 seconds for a standard coding prompt.&lt;/p&gt; &lt;p&gt;On CPU alone, this model was barely usable at &amp;lt;2 tokens/sec. This fix makes the Trashcan a viable local LLM workstation in 2026!&lt;/p&gt; &lt;p&gt;### How to do it:&lt;/p&gt; &lt;p&gt;**1. Update Kernel Parameters**&lt;/p&gt; &lt;p&gt;Add these to your GRUB configuration:&lt;/p&gt; &lt;p&gt;`radeon.si_support=0 amdgpu.si_support=1`&lt;/p&gt; &lt;p&gt;On Fedora/Nobara:&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;sudo sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;/GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;radeon.si_support=0 amdgpu.si_support=1 /' /etc/default/grub&lt;/p&gt; &lt;p&gt;sudo grub2-mkconfig -o /boot/grub2/grub.cfg&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;**2. Reboot**&lt;/p&gt; &lt;p&gt;`sudo reboot`&lt;/p&gt; &lt;p&gt;**3. Container Config (Crucial!)**&lt;/p&gt; &lt;p&gt;If you're running Ollama in a container (Podman or Docker), you MUST:&lt;/p&gt; &lt;p&gt;- Pass `/dev/dri` to the container.&lt;/p&gt; &lt;p&gt;- Set `OLLAMA_VULKAN=1`.&lt;/p&gt; &lt;p&gt;- Disable security labels (SecurityLabel=disable in Quadlet).&lt;/p&gt; &lt;p&gt;**Result:**&lt;/p&gt; &lt;p&gt;My D700s are now identified as **Vulkan0** and **Vulkan1** in Ollama logs, and they split the model VRAM perfectly! ðŸš€&lt;/p&gt; &lt;p&gt;I've put together a GitHub-ready folder with scripts and configs here: [Link to your repo]&lt;/p&gt; &lt;p&gt;Hope this helps any fellow Trashcan owners out there trying to run local LLMs!&lt;/p&gt; &lt;p&gt;#MacPro #Linux #Ollama #SelfHosted #AMD #FireProD700&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/manu7irl"&gt; /u/manu7irl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd5ely/full_gpu_acceleration_for_ollama_on_mac_pro_2013/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd5ely/full_gpu_acceleration_for_ollama_on_mac_pro_2013/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rd5ely/full_gpu_acceleration_for_ollama_on_mac_pro_2013/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T04:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd8cu5</id>
    <title>Built an open-source Ollama/MLX/OpenAI benchmark and leaderboard site with in-app submissions. Trying to test and collect more data.</title>
    <updated>2026-02-24T06:07:36+00:00</updated>
    <author>
      <name>/u/peppaz</name>
      <uri>https://old.reddit.com/user/peppaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rd8cu5/built_an_opensource_ollamamlxopenai_benchmark_and/"&gt; &lt;img alt="Built an open-source Ollama/MLX/OpenAI benchmark and leaderboard site with in-app submissions. Trying to test and collect more data." src="https://preview.redd.it/tcn61r39rdlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=386c5ea0626fa055a8b52140758687ce7f84b8c9" title="Built an open-source Ollama/MLX/OpenAI benchmark and leaderboard site with in-app submissions. Trying to test and collect more data." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peppaz"&gt; /u/peppaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tcn61r39rdlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd8cu5/built_an_opensource_ollamamlxopenai_benchmark_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rd8cu5/built_an_opensource_ollamamlxopenai_benchmark_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T06:07:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd920r</id>
    <title>Built an app that connects Ollama to your clipboard with âŒ¥C (macOS, open source)</title>
    <updated>2026-02-24T06:44:32+00:00</updated>
    <author>
      <name>/u/morning-cereals</name>
      <uri>https://old.reddit.com/user/morning-cereals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rd920r/built_an_app_that_connects_ollama_to_your/"&gt; &lt;img alt="Built an app that connects Ollama to your clipboard with âŒ¥C (macOS, open source)" src="https://external-preview.redd.it/b3I0ajYxMzQxZWxnMRYe2Ofy5EHN92E2iHf3x_Xw6DIJymXP2cpGHUmRCdgH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eee47683fdc493bda6a62a1bc30963579bd0d86" title="Built an app that connects Ollama to your clipboard with âŒ¥C (macOS, open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to skip the &lt;em&gt;copy â†’ switch window â†’ prompt + paste â†’ copy result â†’ paste&lt;/em&gt; loop and just have Ollama be &lt;em&gt;there&lt;/em&gt; whenever I copy something.&lt;/p&gt; &lt;p&gt;So I built Cai. Press Option+C on any text, it detects what you copied and shows actions powered by your Ollama models. It talks to your local server, so whatever model you're running just works.&lt;/p&gt; &lt;p&gt;Instead of sending everything to the LLM, it detects content type first:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ðŸ“ Addresses (Open in Maps)&lt;/li&gt; &lt;li&gt;ðŸ—“ï¸ Meetings (Create Calendar Event)&lt;/li&gt; &lt;li&gt;ðŸ“ Short Text (Define, Reply, Explain)&lt;/li&gt; &lt;li&gt;ðŸŒ Long Text (Summarize, Translate)&lt;/li&gt; &lt;li&gt;ðŸ’» Code/JSON (Beautify, Explain)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can also trigger custom prompts on-the-fly, and save ones you reuse as shortcuts :)&lt;/p&gt; &lt;p&gt;It should automatically detect the Ollama endpoint after installing, otherwise you can manually set it via settings.&lt;br /&gt; This is free and open source project: &lt;a href="https://getcai.app/"&gt;https://getcai.app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been using Ministral 3B on my Macbook Air M2 16GB RAM, curious what works best for you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/morning-cereals"&gt; /u/morning-cereals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hhklml241elg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd920r/built_an_app_that_connects_ollama_to_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rd920r/built_an_app_that_connects_ollama_to_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T06:44:32+00:00</published>
  </entry>
</feed>
