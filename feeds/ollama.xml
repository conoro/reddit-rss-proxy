<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-23T20:07:12+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1p264l9</id>
    <title>Host open-source LLM on a local server and access it Publicly</title>
    <updated>2025-11-20T15:31:30+00:00</updated>
    <author>
      <name>/u/ibjects</name>
      <uri>https://old.reddit.com/user/ibjects</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p264l9/host_opensource_llm_on_a_local_server_and_access/"&gt; &lt;img alt="Host open-source LLM on a local server and access it Publicly" src="https://external-preview.redd.it/aKJxI02wF_Cz77qgjK9fKjpcdmyKIzBmMYNw1dRgElo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3de5bdd2442b26d9f880ac7a730349c5ee8e0369" title="Host open-source LLM on a local server and access it Publicly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ibjects"&gt; /u/ibjects &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ibjects.medium.com/950f48c6858e?source=friends_link&amp;amp;sk=9caf3c3738c9538cd7b0a54def6c6b18"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p264l9/host_opensource_llm_on_a_local_server_and_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p264l9/host_opensource_llm_on_a_local_server_and_access/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T15:31:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1mnra</id>
    <title>DeepSeek-OCR</title>
    <updated>2025-11-19T22:55:37+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek-OCR is a vision-language model that can perform token-efficient optical character recognition (OCR).&lt;/p&gt; &lt;p&gt;DeepSeek-OCR requires Ollama v0.13.0 or later.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/deepseek-ocr"&gt;https://ollama.com/library/deepseek-ocr&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1mnra/deepseekocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1mnra/deepseekocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1mnra/deepseekocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T22:55:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2evio</id>
    <title>Delta Dialogue for local model conversations with report drafting</title>
    <updated>2025-11-20T20:57:28+00:00</updated>
    <author>
      <name>/u/spreader123</name>
      <uri>https://old.reddit.com/user/spreader123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I rediscovered a project I built a few months ago to help me flesh out ideas, draft designs, and rapidly explore concepts. It‚Äôs called Delta Dialogue, and it‚Äôs a local conversation framework for Ollama models that lets them collaborate on any topic you throw at them.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;What it does&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Core idea: Treat each exchange as a ‚Äúdelta‚Äù (a change-state) so conversations build coherently over rounds.&lt;/li&gt; &lt;li&gt;Local-first: Runs with Ollama models only; no web search. Results depend on model training, but are generally solid.&lt;/li&gt; &lt;li&gt;Flexible use: Works for brainstorming, instructions, design ideas, foreign concepts, you name it, they will give it the old college try. &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;How it works&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model setup: You can run multiple distinct models (one of each), or a single model that chains off its own messages.&lt;/li&gt; &lt;li&gt;Rounds: Set 1‚Äì10 rounds. Example: 2 models √ó 10 rounds = 20 replies in a single chain.&lt;/li&gt; &lt;li&gt;Parallel reporting: Each model updates an executive report before finishing its main turn. It starts as a copy of the live feed, then gets refined as the discussion deepens. The report evolves in parallel to the raw dialogue.&lt;/li&gt; &lt;li&gt;Open-ended: There‚Äôs no automatic conclusion step; you can stop any time.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Setup and usage instructions&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Install requirements: &lt;ul&gt; &lt;li&gt;Ollama, Python, and project dependencies.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Launch: &lt;ul&gt; &lt;li&gt;Run Launchfluidoracle.bat.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Select models: &lt;ul&gt; &lt;li&gt;Click the first model to populate the ‚Äúmodel theater‚Äù staging area.&lt;/li&gt; &lt;li&gt;Add more models: Ctrl+click additional models to include them in the discourse.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Prime the session: &lt;ul&gt; &lt;li&gt;Click Think Mode, then click Activate All beneath the staging area.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Set rounds: &lt;ul&gt; &lt;li&gt;Choose a count from 1‚Äì10 (recommend &amp;gt;1).&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Example: 5 models √ó 10 rounds = 50 messages total.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Enter your prompt: &lt;ul&gt; &lt;li&gt;Type into the Fluid Prompt input.&lt;/li&gt; &lt;li&gt;Important: Do not highlight text at this stage; highlighting clears the staging area.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Start the report: &lt;ul&gt; &lt;li&gt;Click Start Report to activate the executive report drafting process.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Initiate dialogue: &lt;ul&gt; &lt;li&gt;Click Initiate Fluid Dialogue.&lt;/li&gt; &lt;li&gt;You can resize the layout; after initiating, highlighting won‚Äôt affect the queued run (models may visually disappear from staging, but the queue is already loaded).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Wait for responses: &lt;ul&gt; &lt;li&gt;Depending on your machine, expect the first reply after a short delay.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Known limits and tips&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt length: There‚Äôs a character limit. I typically keep inputs under ~20 paragraphs. &lt;ul&gt; &lt;li&gt;Symptom: Empty, instant responses mean the prompt was too long‚Äîshorten it.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;UI quirk: Highlighting text before initiating can clear selected models from staging.&lt;/li&gt; &lt;li&gt;Performance: On lower-spec machines, choose lighter Ollama models to avoid slowdowns.&lt;/li&gt; &lt;li&gt;Portability: Built pre‚Äìturbo/cloud; should be straightforward to port to a cloud setup if you want bigger conversations.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Looking for feedback!!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spreader123"&gt; /u/spreader123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Yufok1/Delta_Dialogue"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2evio/delta_dialogue_for_local_model_conversations_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2evio/delta_dialogue_for_local_model_conversations_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T20:57:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1p25jl7</id>
    <title>We trained an SLM assistants for assistance with commit messages on TypeScript codebases - Qwen 3 model (0.6B parameters) that you can run locally!</title>
    <updated>2025-11-20T15:08:26+00:00</updated>
    <author>
      <name>/u/kruszczynski</name>
      <uri>https://old.reddit.com/user/kruszczynski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p25jl7/we_trained_an_slm_assistants_for_assistance_with/"&gt; &lt;img alt="We trained an SLM assistants for assistance with commit messages on TypeScript codebases - Qwen 3 model (0.6B parameters) that you can run locally!" src="https://preview.redd.it/etw8u82jgf2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=404ecde2c4e912c91cbf66ac37ebd425fa6bbb15" title="We trained an SLM assistants for assistance with commit messages on TypeScript codebases - Qwen 3 model (0.6B parameters) that you can run locally!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kruszczynski"&gt; /u/kruszczynski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/etw8u82jgf2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p25jl7/we_trained_an_slm_assistants_for_assistance_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p25jl7/we_trained_an_slm_assistants_for_assistance_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T15:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2954y</id>
    <title>Evaluating 5090 Desktops for running LLMs locally/ollama</title>
    <updated>2025-11-20T17:24:29+00:00</updated>
    <author>
      <name>/u/Excellent_Composer42</name>
      <uri>https://old.reddit.com/user/Excellent_Composer42</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Composer42"&gt; /u/Excellent_Composer42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1p294oo/evaluating_5090_desktops_for_running_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2954y/evaluating_5090_desktops_for_running_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2954y/evaluating_5090_desktops_for_running_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T17:24:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1p22trj</id>
    <title>Computer Use with Gemini 3 pro</title>
    <updated>2025-11-20T13:13:17+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p22trj/computer_use_with_gemini_3_pro/"&gt; &lt;img alt="Computer Use with Gemini 3 pro" src="https://external-preview.redd.it/Ynh4bWs0aTl3ZTJnMUjWqlAeWHcDEffTfb8EVeAbSnSLMt2a5iQQQ8LfY2lH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b2a472e07bbd2da794d06bc33de06e735e9298d" title="Computer Use with Gemini 3 pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemini 3 pro for Computer Use.&lt;/p&gt; &lt;p&gt;Built with the new windows sandboxes.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs : &lt;a href="https://cua.ai/docs/example-usecases/gemini-complex-ui-navigation"&gt;https://cua.ai/docs/example-usecases/gemini-complex-ui-navigation&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4g3t80r9we2g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p22trj/computer_use_with_gemini_3_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p22trj/computer_use_with_gemini_3_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T13:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2gzxk</id>
    <title>Cortex got a massive update! (ollama UI desktop ap)</title>
    <updated>2025-11-20T22:18:14+00:00</updated>
    <author>
      <name>/u/Ok-Function-7101</name>
      <uri>https://old.reddit.com/user/Ok-Function-7101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p2gzxk/cortex_got_a_massive_update_ollama_ui_desktop_ap/"&gt; &lt;img alt="Cortex got a massive update! (ollama UI desktop ap)" src="https://a.thumbs.redditmedia.com/twAZvPomgU64duwvRIJ4q3p_WaI91rdw3r3eSX7-LB0.jpg" title="Cortex got a massive update! (ollama UI desktop ap)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its entirely open-source and you're invited to come try it out! &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://dovvnloading.github.io/Cortex/"&gt;https://dovvnloading.github.io/Cortex/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8fkh4lhclh2g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9dd2214c5ff9b533cdc596a7217359ed0d07689"&gt;https://preview.redd.it/8fkh4lhclh2g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9dd2214c5ff9b533cdc596a7217359ed0d07689&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ht1e8mhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b86ab8106804a525f3690e85f42c8c2b8612d0cf"&gt;https://preview.redd.it/ht1e8mhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b86ab8106804a525f3690e85f42c8c2b8612d0cf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6jvmrmhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eee7558190f4d4564fdc4284b7e207fb08d4ae7c"&gt;https://preview.redd.it/6jvmrmhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eee7558190f4d4564fdc4284b7e207fb08d4ae7c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j3kzilhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a374639836dc6abdf736458797d3d328c32df70"&gt;https://preview.redd.it/j3kzilhclh2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a374639836dc6abdf736458797d3d328c32df70&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7mpmalhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb9ac99722711d9d4a0981247c3bf883fc88bd0"&gt;https://preview.redd.it/7mpmalhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb9ac99722711d9d4a0981247c3bf883fc88bd0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xm3kklhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b72d44f1aa1495b585e2cc5a08148bf91f53e19"&gt;https://preview.redd.it/xm3kklhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b72d44f1aa1495b585e2cc5a08148bf91f53e19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hza6cnhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85ab8a70697092d5fb0c13e1446f2033fefb707e"&gt;https://preview.redd.it/hza6cnhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85ab8a70697092d5fb0c13e1446f2033fefb707e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kvbxrlhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e538e902ef4db617145140d9ffa90f6df440302"&gt;https://preview.redd.it/kvbxrlhclh2g1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e538e902ef4db617145140d9ffa90f6df440302&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Function-7101"&gt; /u/Ok-Function-7101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2gzxk/cortex_got_a_massive_update_ollama_ui_desktop_ap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2gzxk/cortex_got_a_massive_update_ollama_ui_desktop_ap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2gzxk/cortex_got_a_massive_update_ollama_ui_desktop_ap/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-20T22:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2mro7</id>
    <title>Best &lt; $20k Configuration</title>
    <updated>2025-11-21T02:30:03+00:00</updated>
    <author>
      <name>/u/JMWTech</name>
      <uri>https://old.reddit.com/user/JMWTech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What would you build with $20k to train a model(s) and operate a on-prem chat bot for document and policy retrieval?&lt;/p&gt; &lt;p&gt;I've received quotes from &amp;quot;workstations&amp;quot; with 5090s to rack mounted servers running either four L4s (ewww) to a dual proc single RTX Pro 6000. Just want to make sure we're not wasting money and getting the most bang for the buck.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JMWTech"&gt; /u/JMWTech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2mro7/best_20k_configuration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p2mro7/best_20k_configuration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p2mro7/best_20k_configuration/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T02:30:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3lfua</id>
    <title>üöÄ Just Finished an INSANE MCP + LangChain + Claude Course ‚Äî Mind = Blown ü§Ø</title>
    <updated>2025-11-22T05:40:59+00:00</updated>
    <author>
      <name>/u/Distinct-Truth7165</name>
      <uri>https://old.reddit.com/user/Distinct-Truth7165</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Distinct-Truth7165"&gt; /u/Distinct-Truth7165 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/learnmachinelearning/comments/1p3lfgj/just_finished_an_insane_mcp_langchain_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3lfua/just_finished_an_insane_mcp_langchain_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p3lfua/just_finished_an_insane_mcp_langchain_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-22T05:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p32ism</id>
    <title>4096 token limit</title>
    <updated>2025-11-21T16:04:50+00:00</updated>
    <author>
      <name>/u/aleglr20</name>
      <uri>https://old.reddit.com/user/aleglr20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I‚Äôm not sure if this is the right subreddit for this question ‚Äî if not, please let me know where I should post it.&lt;br /&gt; Anyway, I‚Äôm working on a Java project using the &lt;code&gt;spring-ai-starter-model-openai&lt;/code&gt; dependency, and I‚Äôm currently using &lt;code&gt;gemma3:4b&lt;/code&gt; through Ollama, which exposes OpenAI-compatible endpoints.&lt;/p&gt; &lt;p&gt;I have a chat method where I pass a text as context and then ask a question about it. The text and the question are combined into a single prompt that I send to the model.&lt;br /&gt; From the JSON response, I noticed the token usage data, and I discovered that if I go above roughly 4,070 tokens, the model gives a wrong or incoherent answer ‚Äî it no longer follows the question or the provided context.&lt;/p&gt; &lt;p&gt;Can someone explain to me how the 4,096-token limit works? Even if the model has a 128k context window?&lt;br /&gt; Is the 4,096-token limit related to the output, the prompt, or both? Because I‚Äôm experiencing issues specifically when the &lt;em&gt;prompt&lt;/em&gt; gets too large, even before the output is generated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aleglr20"&gt; /u/aleglr20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p32ism/4096_token_limit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p32ism/4096_token_limit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p32ism/4096_token_limit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T16:04:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3czc0</id>
    <title>What am I missing?</title>
    <updated>2025-11-21T22:51:53+00:00</updated>
    <author>
      <name>/u/SaltbushBillJP</name>
      <uri>https://old.reddit.com/user/SaltbushBillJP</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Page Assist, testing several LLMs and trying to have OLLAMA extract specific values from multiple files (each being an exported email - I've tested PDF and txt formats).&lt;/p&gt; &lt;p&gt;The emails are responses from local government acknowledging permit applications and I want to extract registration number and submission date from each email (file). &lt;/p&gt; &lt;p&gt;This works for one or two, then the response is completed with blank values or N/A or some other rubbish.&lt;/p&gt; &lt;p&gt;I've loaded about 6 of these files into a Knowledge Base and selected it for the evaluation&lt;/p&gt; &lt;p&gt;Should I edit RAG settings?&lt;/p&gt; &lt;p&gt;What else do I need to do so the query correctly evaluates more than 2 documents?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SaltbushBillJP"&gt; /u/SaltbushBillJP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3czc0/what_am_i_missing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3czc0/what_am_i_missing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p3czc0/what_am_i_missing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T22:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p33czu</id>
    <title>Ollama Grid Search v0.9.2: Enhanced LLM Evaluation and Comparison</title>
    <updated>2025-11-21T16:37:01+00:00</updated>
    <author>
      <name>/u/grudev</name>
      <uri>https://old.reddit.com/user/grudev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Happy to announce the release of &lt;a href="https://github.com/dezoito/ollama-grid-search/"&gt;&lt;strong&gt;Ollama Grid Search v0.9.2&lt;/strong&gt;&lt;/a&gt;, a tool created to improve the experience of those of use evaluating and experimenting with multiple LLMs&lt;/p&gt; &lt;p&gt;This addresses issues with damaged &lt;code&gt;.dmg&lt;/code&gt; files that some users experienced during installation (a result of GitHub actions script + Apple's signing requirements). The build process has been updated to improve the setup for all macOS users, particularly those on Apple Silicon (M1/M2/M3/M4) devices.&lt;/p&gt; &lt;h1&gt;About Ollama Grid Search&lt;/h1&gt; &lt;p&gt;For those new to the project, Ollama Grid Search is a desktop application that automates the process of evaluating and comparing multiple Large Language Models (LLMs). Whether you're fine-tuning prompts, selecting the best model for your use case, or conducting A/B tests, this tool will make your life easier.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-Model Testing&lt;/strong&gt;: Automatically fetch and test multiple models from your Ollama servers&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grid Search&lt;/strong&gt;: Iterate over combinations of models, prompts, and parameters&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A/B Testing&lt;/strong&gt;: Compare responses from different prompts and models side-by-side&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt Management&lt;/strong&gt;: Built-in prompt database with autocomplete functionality&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Experiment Logs&lt;/strong&gt;: Track, review, and re-run past experiments&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Concurrent Inference&lt;/strong&gt;: Support for parallel inference calls to speed up evaluations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Results&lt;/strong&gt;: Easy-to-read interface for comparing model outputs&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Getting Started&lt;/h1&gt; &lt;p&gt;Download the latest release from our &lt;a href="https://github.com/dezoito/ollama-grid-search/releases"&gt;releases page&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/dezoito/ollama-grid-search"&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/dezoito/ollama-grid-search/blob/main/CHANGELOG.md"&gt;Full Changelog&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://dezoito.github.io/2023/12/27/rust-ollama-grid-search.html"&gt;In-depth Grid Search Tutorial&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grudev"&gt; /u/grudev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p33czu/ollama_grid_search_v092_enhanced_llm_evaluation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p33czu/ollama_grid_search_v092_enhanced_llm_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p33czu/ollama_grid_search_v092_enhanced_llm_evaluation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T16:37:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p361yc</id>
    <title>MCP Script - an Agent Oriented Programming Language</title>
    <updated>2025-11-21T18:17:58+00:00</updated>
    <author>
      <name>/u/atinylittleshell</name>
      <uri>https://old.reddit.com/user/atinylittleshell</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building a scripting language for composing agentic workflows using MCP as the fundamental building block. It's in super early stage but I'm curious to see if you would find something like this useful. &lt;/p&gt; &lt;p&gt;Repo is here: &lt;a href="https://github.com/mcpscript/mcpscript"&gt;https://github.com/mcpscript/mcpscript&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this language, models, tools, agents, conversations are first-class native constructs. Every function is a tool, every tool is a function - they can be called deterministically or given to an agent. &lt;/p&gt; &lt;p&gt;``` // Configure a local model using Ollama model gpt { provider: &amp;quot;openai&amp;quot;, apiKey: &amp;quot;ollama&amp;quot;, baseURL: &amp;quot;http://localhost:11434/v1&amp;quot;, model: &amp;quot;gpt-oss:20b&amp;quot;, temperature: 0.1 }&lt;/p&gt; &lt;p&gt;// Set up the filesystem MCP server mcp filesystem { command: &amp;quot;npx&amp;quot;, args: [&amp;quot;-y&amp;quot;, &amp;quot;@modelcontextprotocol/server-filesystem@latest&amp;quot;], stderr: &amp;quot;ignore&amp;quot; }&lt;/p&gt; &lt;p&gt;// Read the memory file (AGENTS.md) from the current directory memoryContent = filesystem.read_file({ path: &amp;quot;AGENTS.md&amp;quot; }) print(&amp;quot;Memory file loaded successfully (&amp;quot; + memoryContent.length + &amp;quot; characters)&amp;quot;)&lt;/p&gt; &lt;p&gt;// Define a coding agent with access to filesystem tools agent CodingAgent { model: gpt, systemPrompt: &amp;quot;You are an expert software developer assistant. You have access to filesystem tools and can help with code analysis, debugging, and development tasks. Be concise and helpful.&amp;quot;, tools: [filesystem] } ```&lt;/p&gt; &lt;p&gt;Messages can be piped to an agent or a conversation, so you can -&lt;/p&gt; &lt;p&gt;``` // send a message to an agent convo = &amp;quot;help me fix this bug&amp;quot; | CodingAgent&lt;/p&gt; &lt;p&gt;// append a message to a conversation convo = convo | &amp;quot;review the fix&amp;quot;&lt;/p&gt; &lt;p&gt;// pass that to another agent convo = convo | ReviewAgent&lt;/p&gt; &lt;p&gt;// or just chain them together &amp;quot;help me fix this bug&amp;quot; | CodingAgent | &amp;quot;review the fix&amp;quot; | ReviewAgent ```&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atinylittleshell"&gt; /u/atinylittleshell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p361yc/mcp_script_an_agent_oriented_programming_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p361yc/mcp_script_an_agent_oriented_programming_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p361yc/mcp_script_an_agent_oriented_programming_language/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-21T18:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3zoo5</id>
    <title>Where to run OpenWeb UI?</title>
    <updated>2025-11-22T17:58:22+00:00</updated>
    <author>
      <name>/u/Mundane_Local_2992</name>
      <uri>https://old.reddit.com/user/Mundane_Local_2992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I have a very simple question. I am running a very elaborate agent on an ollama qwen3 installation on a remote linux server (with a rtx3090, 5950x and 64gb of ram). the setup runs great and fast when I directly query ollama on the server via terminal, but it seems that running OpenWebUI on the same server and then accessing it via &amp;lt;server ip&amp;gt;:5050 causes a lot of added latency (responses that takes 14 seconds via terminal takes over 25 seconds when asking OpenWebUI via web browser. so my question is, is the issue that I am running OpenWebUI on the same server as my ollama installation, and I should actually run OpenWebUI on my local machine, which point to the server? or is there a more performant ui that I can use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane_Local_2992"&gt; /u/Mundane_Local_2992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3zoo5/where_to_run_openweb_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3zoo5/where_to_run_openweb_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p3zoo5/where_to_run_openweb_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-22T17:58:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4eoje</id>
    <title>Will an RTX 6000 run the gpt-oss:120b model?</title>
    <updated>2025-11-23T05:31:08+00:00</updated>
    <author>
      <name>/u/pawnstew</name>
      <uri>https://old.reddit.com/user/pawnstew</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sizing a GPU for work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pawnstew"&gt; /u/pawnstew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4eoje/will_an_rtx_6000_run_the_gptoss120b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4eoje/will_an_rtx_6000_run_the_gptoss120b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4eoje/will_an_rtx_6000_run_the_gptoss120b_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T05:31:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4f9j0</id>
    <title>GPT-OSS 20B: Capacit√©s et Tests</title>
    <updated>2025-11-23T06:04:36+00:00</updated>
    <author>
      <name>/u/Desperate_Yellow_541</name>
      <uri>https://old.reddit.com/user/Desperate_Yellow_541</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Yellow_541"&gt; /u/Desperate_Yellow_541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ollama/comments/1p4f8iv/gptoss_20b_capacit√©s_et_tests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4f9j0/gptoss_20b_capacit√©s_et_tests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4f9j0/gptoss_20b_capacit√©s_et_tests/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T06:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ie4a</id>
    <title>Help with a prompt?</title>
    <updated>2025-11-23T09:19:57+00:00</updated>
    <author>
      <name>/u/SystemAromatic</name>
      <uri>https://old.reddit.com/user/SystemAromatic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, i'm having a problem. There is this program that gives me a question and 4 Answers but i cant copypaste and i'm pretty lazy to rewrite it all. I've Tried a couple of prompt but i'm pretty new to this and i don't really know if there even is a proper way to do what i want to do, any suggestion? &lt;/p&gt; &lt;p&gt;Tl;dr: Prompt for ollama to read my screen (?) And answer without copypaste &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SystemAromatic"&gt; /u/SystemAromatic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4ie4a/help_with_a_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4ie4a/help_with_a_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4ie4a/help_with_a_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T09:19:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4jen4</id>
    <title>ToolNeuron ‚Äî Privacy-first AI hub on Android (offline + online models, plugin support)</title>
    <updated>2025-11-23T10:23:14+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p4jen4/toolneuron_privacyfirst_ai_hub_on_android_offline/"&gt; &lt;img alt="ToolNeuron ‚Äî Privacy-first AI hub on Android (offline + online models, plugin support)" src="https://external-preview.redd.it/AuuQ6Jw6VH9e6ZU098Q0G_x_fI41xouhvcXLl_FeCi8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a50a03c0422760aefd8861c44779d24f9c6b02ab" title="ToolNeuron ‚Äî Privacy-first AI hub on Android (offline + online models, plugin support)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://tool-neuron.vercel.app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4jen4/toolneuron_privacyfirst_ai_hub_on_android_offline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4jen4/toolneuron_privacyfirst_ai_hub_on_android_offline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T10:23:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4f8iv</id>
    <title>GPT-OSS 20B: Capacit√©s et Tests</title>
    <updated>2025-11-23T06:02:53+00:00</updated>
    <author>
      <name>/u/Desperate_Yellow_541</name>
      <uri>https://old.reddit.com/user/Desperate_Yellow_541</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Le mythe de la taille : comment un 20B bien r√©gl√© peut rivaliser avec les ‚Äúg√©ants‚Äù sur des t√¢ches d‚Äôextraction r√©elles&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Mod√®le : &lt;strong&gt;GPT-OSS 20B&lt;/strong&gt; (local)&lt;/li&gt; &lt;li&gt;Contexte : &lt;strong&gt;65k tokens&lt;/strong&gt; (menu PDF OCR bien sale)&lt;/li&gt; &lt;li&gt;T√¢che : extraire des &lt;strong&gt;prix corrects&lt;/strong&gt; d‚Äôun document bruit√©&lt;/li&gt; &lt;li&gt;Cl√© du succ√®s : &lt;strong&gt;Mirostat v2 + Repeat Penalty √† 1.2&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Id√©e centrale : le probl√®me n‚Äôest pas ‚Äúle mod√®le est trop petit‚Äù, mais &lt;strong&gt;comment on le contraint&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Contexte : ce qu‚Äôon essaie vraiment de faire&lt;/h1&gt; &lt;p&gt;L‚Äôindustrie de l‚ÄôIA est obs√©d√©e par la taille : 70B, 405B, 1T de param√®tres‚Ä¶&lt;br /&gt; Mais sur du &lt;strong&gt;local&lt;/strong&gt;, avec un 20B, je me suis rendu compte que :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Cas d‚Äôusage concret :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Un &lt;strong&gt;menu de restaurant&lt;/strong&gt; en PDF,&lt;/li&gt; &lt;li&gt;Pass√© par un &lt;strong&gt;OCR imparfait&lt;/strong&gt; (chiffres coll√©s, espaces bizarres, accents foutus),&lt;/li&gt; &lt;li&gt;Inject√© dans une fen√™tre de contexte de &lt;strong&gt;65 536 tokens&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Objectif : extraire un tableau propre &lt;em&gt;plat ‚Üí prix&lt;/em&gt;, &lt;strong&gt;sans halluciner&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Spoiler : au d√©part, le mod√®le a fait n‚Äôimporte quoi‚Ä¶ mais pas pour les raisons qu‚Äôon croit.&lt;/p&gt; &lt;h1&gt;I. L‚Äôillusion de la ‚Äúparesse cognitive‚Äù des LLM locaux&lt;/h1&gt; &lt;p&gt;Quand on joue avec des LLM locaux, on voit souvent :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄúD√©sol√©, ce texte est trop long‚Äù ‚Üí refus implicite&lt;/li&gt; &lt;li&gt;Boucles de texte ‚Üí le mod√®le r√©p√®te la m√™me phrase&lt;/li&gt; &lt;li&gt;Hallucinations de chiffres ou de lignes enti√®res&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;R√©flexe classique :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Sauf que dans mon cas, sur ce menu OCR :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Le mod√®le &lt;strong&gt;comprenait&lt;/strong&gt; globalement la structure,&lt;/li&gt; &lt;li&gt;Mais il avait un comportement ‚Äúparesseux‚Äù : &lt;ul&gt; &lt;li&gt;Il attribuait le &lt;strong&gt;m√™me prix&lt;/strong&gt; √† plusieurs plats,&lt;/li&gt; &lt;li&gt;Ou il ‚Äúinventait‚Äù des prix plausibles visuellement, mais &lt;strong&gt;faux&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ce n‚Äô√©tait pas un probl√®me de compr√©hension linguistique.&lt;br /&gt; C‚Äô√©tait une &lt;strong&gt;optimisation probabiliste&lt;/strong&gt; :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Le mod√®le se cale dans une orni√®re : ‚Äúj‚Äôai d√©j√† √©crit 150, √ßa marche bien, je vais remettre 150‚Äù.&lt;br /&gt; Notre job n‚Äôa pas √©t√© de ‚Äúlui expliquer mieux‚Äù le menu,&lt;br /&gt; mais de &lt;strong&gt;rendre cette orni√®re tr√®s co√ªteuse&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;II. Le ‚ÄúSettings Engineering‚Äù : trianguler les hyperparam√®tres&lt;/h1&gt; &lt;p&gt;Pour transformer ce g√©n√©rateur de texte probabiliste en quelque chose qui ressemble √† un &lt;strong&gt;agent de raisonnement fiable&lt;/strong&gt;, j‚Äôai fini par traiter les r√©glages comme un vrai espace de recherche.&lt;/p&gt; &lt;p&gt;J‚Äôai identifi√© trois leviers qui, ensemble, changent radicalement le comportement :&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Levier de stabilit√©&lt;/strong&gt; : Mirostat vs Top_K / Top_P&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Levier de v√©rit√©&lt;/strong&gt; : la p√©nalit√© de r√©p√©tition (Repeat Penalty)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Levier de charge cognitive&lt;/strong&gt; : ‚Äúforcer‚Äù le mod√®le √† r√©fl√©chir (System 2-like)&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;1. Mirostat v2 : stabiliser le chaos sur 65k tokens&lt;/h1&gt; &lt;p&gt;Les r√©glages classiques (Top_K, Top_P) sont des &lt;strong&gt;filtres statiques&lt;/strong&gt; :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ils coupent la queue de distribution,&lt;/li&gt; &lt;li&gt;Mais ils ne s‚Äôadaptent pas √† la &lt;strong&gt;difficult√© locale&lt;/strong&gt; du passage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sur un contexte de &lt;strong&gt;65k tokens&lt;/strong&gt;, tu as des zones :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Faciles&lt;/strong&gt; : traduction simple, texte tr√®s clair,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ambigu√´s&lt;/strong&gt; : chiffres flous, OCR parasit√©, colonnes bancales.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Avec Top_K / Top_P seul, le mod√®le n‚Äôa aucune conscience que ‚Äúl√†, c‚Äôest chaud‚Äù, ou ‚Äúl√†, c‚Äôest trivial‚Äù.&lt;/p&gt; &lt;p&gt;Avec &lt;strong&gt;Mirostat v2&lt;/strong&gt;, le comportement change :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;L‚Äôalgorithme surveille une forme de &lt;strong&gt;perplexit√©&lt;/strong&gt; (surprise du mod√®le) &lt;em&gt;en temps r√©el&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Intuition (simplifi√©e) : &lt;ul&gt; &lt;li&gt;Si le mod√®le &lt;strong&gt;boucle&lt;/strong&gt; ‚Üí la perplexit√© chute (trop pr√©visible) ‚Üí ‚Üí Mirostat &lt;strong&gt;injecte du chaos&lt;/strong&gt;, oblige le mod√®le √† sortir de la boucle.&lt;/li&gt; &lt;li&gt;Si le mod√®le &lt;strong&gt;part en d√©lire&lt;/strong&gt; ‚Üí perplexit√© explose ‚Üí ‚Üí Mirostat &lt;strong&gt;resserre&lt;/strong&gt; la g√©n√©ration vers quelque chose de plus coh√©rent.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;R√©sultat empirique :&lt;br /&gt; Sur ce menu OCR, Mirostat a rendu les r√©ponses :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Moins sujettes aux &lt;strong&gt;boucles&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Et plus robustes sur des zones ‚Äúcass√©es‚Äù du texte source.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. La p√©nalit√© de r√©p√©tition 1.2 : casser la voie facile&lt;/h1&gt; &lt;p&gt;La vraie bascule, pour l‚Äôextraction de &lt;strong&gt;prix&lt;/strong&gt;, est venue de la &lt;strong&gt;Repeat Penalty&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Ce qu‚Äôon lit partout :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄúMettez 1.05 ou 1.1, au-del√† vous tuez la qualit√© du texte.‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ce que j‚Äôai observ√© :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pour de la &lt;strong&gt;donn√©e chiffr√©e&lt;/strong&gt; dans un contexte bruit√©, le &lt;strong&gt;seuil critique&lt;/strong&gt; n‚Äôest pas 1.1, mais plut√¥t &lt;strong&gt;1.2&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Intuition :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;√Ä 1.1 : &lt;ul&gt; &lt;li&gt;Le ‚Äúco√ªt‚Äù de r√©√©crire &lt;code&gt;150&lt;/code&gt; est encore &lt;strong&gt;faible&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Le ‚Äúco√ªt‚Äù d‚Äôaller chercher &lt;code&gt;190&lt;/code&gt; dans le contexte OCR est &lt;strong&gt;plus √©lev√©&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Le mod√®le choisit : &lt;em&gt;‚ÄúJe recycle 150, c‚Äôest plus confortable.‚Äù&lt;/em&gt; ‚Üí &lt;strong&gt;Hallucination par r√©p√©tition.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;√Ä 1.2 : &lt;ul&gt; &lt;li&gt;La r√©p√©tition de &lt;code&gt;150&lt;/code&gt; devient &lt;strong&gt;prohibitive&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Le mod√®le se retrouve &lt;strong&gt;bloqu√©&lt;/strong&gt; :‚ÄúJe veux √©crire un prix, mais je ne peux pas r√©√©crire 150.‚Äù&lt;/li&gt; &lt;li&gt;Son seul chemin ‚Äúpeu co√ªteux‚Äù devient alors : ‚Üí &lt;strong&gt;replonger dans le contexte&lt;/strong&gt;, chercher un autre motif valide (&lt;code&gt;190&lt;/code&gt;, &lt;code&gt;135&lt;/code&gt;, etc.).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ce n‚Äôest √©videmment pas une ‚Äúv√©rification logique‚Äù au sens humain, mais :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;En pratique, la mont√©e √† &lt;strong&gt;1.2&lt;/strong&gt; a fait dispara√Ætre un grand nombre de &lt;strong&gt;lignes dupliqu√©es&lt;/strong&gt; ou ‚Äúprix coll√©s partout‚Äù.&lt;/p&gt; &lt;h1&gt;III. Quand le mod√®le commence √† ‚Äúcompter‚Äù : √©mergence d‚Äôun pseudo System 2&lt;/h1&gt; &lt;p&gt;Une fois Mirostat v2 activ√© + Repeat Penalty √† 1.2, un truc int√©ressant est arriv√© dans les logs de r√©flexion (Chain-of-Thought).&lt;/p&gt; &lt;p&gt;Le mod√®le a g√©n√©r√© un raisonnement du type :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;C‚Äôest exactement le genre de comportement que j‚Äôattends d‚Äôun mod√®le ‚Äúplus gros‚Äù sur ce type de t√¢che :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Aligner&lt;/strong&gt; une liste de plats avec une liste de prix,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compter&lt;/strong&gt; les √©l√©ments des deux c√¥t√©s,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Recomposer&lt;/strong&gt; un prix cass√© par l‚ÄôOCR (&lt;code&gt;1 9 0&lt;/code&gt; ‚Üí &lt;code&gt;190&lt;/code&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ce qui est int√©ressant ici :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ce n‚Äôest pas ‚Äúmagiquement‚Äù apparu gr√¢ce au nombre de param√®tres,&lt;/li&gt; &lt;li&gt;C‚Äôest apparu &lt;strong&gt;apr√®s&lt;/strong&gt; qu‚Äôon ait &lt;strong&gt;ferm√© les voies faciles&lt;/strong&gt; : &lt;ul&gt; &lt;li&gt;refus,&lt;/li&gt; &lt;li&gt;r√©p√©tition,&lt;/li&gt; &lt;li&gt;hallucination d√©corative.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;On a, en quelque sorte, forc√© le mod√®le √† adopter un comportement plus proche d‚Äôun &lt;strong&gt;System 2&lt;/strong&gt; (au sens Kahneman) :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Et √ßa, avec &lt;strong&gt;un 20B local&lt;/strong&gt;, sur une machine perso, pas un datacenter.&lt;/p&gt; &lt;h1&gt;Conclusion : on n‚Äôa pas (toujours) besoin de mod√®les plus gros, on a besoin de meilleures contraintes&lt;/h1&gt; &lt;p&gt;Ce que cette exp√©rience sugg√®re pour les &lt;strong&gt;LLM locaux&lt;/strong&gt; :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pour beaucoup de t√¢ches &lt;strong&gt;administratives / d‚Äôextraction documentaire&lt;/strong&gt; : &lt;ul&gt; &lt;li&gt;factures, menus, tableaux OCR, listes, etc.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Le vrai levier n‚Äôest &lt;strong&gt;pas forc√©ment&lt;/strong&gt; : &lt;ul&gt; &lt;li&gt;‚Äúmonter en taille‚Äù,&lt;/li&gt; &lt;li&gt;‚Äúpasser au closed-source cloud‚Äù.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Mais plut√¥t :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;En pratique, √ßa veut dire :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jouer s√©rieusement avec : &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Temp√©rature&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mirostat / Top_P&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Repeat Penalty&lt;/strong&gt;,&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Et accepter que pour des t√¢ches &lt;strong&gt;critiques sur les chiffres&lt;/strong&gt;, un mod√®le ‚Äúmoins fluide‚Äù mais &lt;strong&gt;plus contraint&lt;/strong&gt; est &lt;strong&gt;pr√©f√©rable&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Annexe : le ‚ÄúPreset‚Äù qui a d√©clench√© le changement&lt;/h1&gt; &lt;p&gt;Pour ceux qui veulent le preset brut, voici la config qui a march√© &lt;strong&gt;dans mon cas&lt;/strong&gt;&lt;br /&gt; (GPT-OSS 20B via Ollama / OpenWebUI, extraction de prix sur menu OCR, contexte 65k) :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Mod√®le : GPT-OSS 20B Contexte : 65 536 tokens T√¢che : extraction de prix depuis un PDF OCR bruit√© [Architecture cognitive] Mirostat : ON (Mode 2) Mirostat Tau : 5.0 Mirostat Eta : 0.1 Reasoning Effort : High [Param√®tres de contrainte] Temp√©rature : 0.60 Top_p : 0.90 Repeat Penalty : 1.20 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Je ne pr√©tends pas que ce preset est universel, mais :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sur ce cas d‚Äôusage, il a fait la diff√©rence entre : &lt;ul&gt; &lt;li&gt;un mod√®le qui ‚Äúfait joli mais faux‚Äù,&lt;/li&gt; &lt;li&gt;et un mod√®le qui &lt;strong&gt;compte&lt;/strong&gt; et &lt;strong&gt;recroise&lt;/strong&gt; les donn√©es du PDF.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Si √ßa int√©resse du monde, je peux partager dans un autre post :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Un exemple de prompt complet (avec consignes d‚Äôextraction),&lt;/li&gt; &lt;li&gt;La structure de validation que j‚Äôutilise derri√®re (checks simples type ‚Äúnombre de lignes / coh√©rence des prix‚Äù),&lt;/li&gt; &lt;li&gt;Et pourquoi &lt;strong&gt;je teste toujours d‚Äôabord des changements de settings&lt;/strong&gt; avant de conclure qu‚Äôun mod√®le est ‚Äútrop petit‚Äù.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Yellow_541"&gt; /u/Desperate_Yellow_541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4f8iv/gptoss_20b_capacit√©s_et_tests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4f8iv/gptoss_20b_capacit√©s_et_tests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4f8iv/gptoss_20b_capacit√©s_et_tests/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T06:02:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4qhpr</id>
    <title>Manus</title>
    <updated>2025-11-23T16:09:16+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://manus.im/invitation/BHKBHIT0WJFORVO"&gt;https://manus.im/invitation/BHKBHIT0WJFORVO&lt;/a&gt; login with gmail or apple, or microsoft&lt;/p&gt; &lt;p&gt;Find redeem section in invite friends and use this code to get 1000 credits: njexode&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4qhpr/manus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4qhpr/manus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4qhpr/manus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T16:09:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4lxnr</id>
    <title>Best Local Coding Agent Model for 64GB RAM and 12GB VRAM?</title>
    <updated>2025-11-23T12:49:55+00:00</updated>
    <author>
      <name>/u/fallen0523</name>
      <uri>https://old.reddit.com/user/fallen0523</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallen0523"&gt; /u/fallen0523 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1p4lwyc/best_local_coding_agent_model_for_64gb_ram_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4lxnr/best_local_coding_agent_model_for_64gb_ram_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4lxnr/best_local_coding_agent_model_for_64gb_ram_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T12:49:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4od24</id>
    <title>Summarize and manage local files?</title>
    <updated>2025-11-23T14:42:32+00:00</updated>
    <author>
      <name>/u/cl326</name>
      <uri>https://old.reddit.com/user/cl326</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a huge number of files on my laptop. They are not well organized or named. I‚Äôve removed all duplicates that I can by comparing hashes and names. Now I‚Äôd like to use Ollama to summarize each file in a folder so ai can get an idea of what the file is about if I can‚Äôt tell by the name. The files are mostly MS Office documents, PDFs, and images. Is there a model that you‚Äôd suggest? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cl326"&gt; /u/cl326 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4od24/summarize_and_manage_local_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4od24/summarize_and_manage_local_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4od24/summarize_and_manage_local_files/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T14:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3vpni</id>
    <title>Your local Ollama agents can be just as good as closed-source models - I open-sourced Stanford's ACE framework that makes agents learn from mistakes</title>
    <updated>2025-11-22T15:17:51+00:00</updated>
    <author>
      <name>/u/cheetguy</name>
      <uri>https://old.reddit.com/user/cheetguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I implemented Stanford's &lt;a href="https://arxiv.org/abs/2510.04618"&gt;Agentic Context Engineering paper&lt;/a&gt; for Ollama. The framework makes agents learn from their own execution feedback through in-context learning instead of fine-tuning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; Agent runs task ‚Üí reflects on what worked/failed ‚Üí curates strategies into playbook ‚Üí uses playbook on next run&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Improvement:&lt;/strong&gt; Paper shows +17.1pp accuracy improvement vs base LLM (‚âà+40% relative improvement) on agent benchmarks (DeepSeek-V3.1 non-thinking mode), helping close the gap with closed-source models. All through in-context learning (no fine-tuning needed).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Open-Source Implementation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop into existing agents in ~10 lines of code&lt;/li&gt; &lt;li&gt;Works with any Ollama model (Llama, Qwen, Mistral, DeepSeek, etc.)&lt;/li&gt; &lt;li&gt;Real-world test on browser automation agent: &lt;ul&gt; &lt;li&gt;30% ‚Üí 100% success rate&lt;/li&gt; &lt;li&gt;82% fewer steps&lt;/li&gt; &lt;li&gt;65% decrease in token cost&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;https://github.com/kayba-ai/agentic-context-engine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ollama Starter Template: &lt;a href="https://github.com/kayba-ai/agentic-context-engine/blob/main/examples/ollama/ollama_starter_template.py"&gt;https://github.com/kayba-ai/agentic-context-engine/blob/main/examples/ollama/ollama_starter_template.py&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear if anyone tries this with Ollama! Especially curious how it performs with different Ollama models.&lt;/p&gt; &lt;p&gt;I'm currently actively improving this based on feedback - &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;‚≠ê the repo&lt;/a&gt; so you can stay updated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cheetguy"&gt; /u/cheetguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-22T15:17:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4j3gr</id>
    <title>Vanaras ‚Äî Local-First Agentic AI Framework for Developers (FAISS, DAG, Tools, Sandbox, UI)</title>
    <updated>2025-11-23T10:04:06+00:00</updated>
    <author>
      <name>/u/VanarasAgenticAI</name>
      <uri>https://old.reddit.com/user/VanarasAgenticAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[Release] Vanaras ‚Äî Local-First Agentic AI Framework for Developers (FAISS, DAG, Tools, Sandbox, UI)&lt;/p&gt; &lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I‚Äôve been building something for the last few weeks that I think the self-hosted / local-AI community may find useful.&lt;/p&gt; &lt;h1&gt; What is Vanaras?&lt;/h1&gt; &lt;p&gt;Vanaras is an open-source, local-first agentic AI framework designed specifically for developers ‚Äî not chatbots.&lt;/p&gt; &lt;p&gt;It lets you run AI agents that can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Call real tools (read/write files, run code, search project, grep, parse)&lt;/li&gt; &lt;li&gt; Use FAISS vector search for memory &amp;amp; project understanding&lt;/li&gt; &lt;li&gt; Perform RAG over your own code/project&lt;/li&gt; &lt;li&gt; Run a proper Planner + Critic + Decomposer loop&lt;/li&gt; &lt;li&gt; Execute tasks in a secure sandbox (no accidental system access)&lt;/li&gt; &lt;li&gt; Execute DAG-based workflows (similar to Airflow but for AI agents)&lt;/li&gt; &lt;li&gt; Use a lightweight UI to inspect runs and control the agent&lt;/li&gt; &lt;li&gt; Work fully offline with Ollama&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically:&lt;/p&gt; &lt;p&gt;A developer-oriented alternative to Flowise / LangFlow / AutoGen / Crew AI‚Äî but runs locally and edits code safely.&lt;/p&gt; &lt;p&gt;Repo:&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/Vanaras-AI/agent-framework"&gt;https://github.com/Vanaras-AI/agent-framework&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + Website:&lt;/p&gt; &lt;p&gt; &lt;a href="https://vanaras.ai/"&gt;https://vanaras.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PyPI:&lt;/p&gt; &lt;p&gt;pip install vanaras-agent-framework&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VanarasAgenticAI"&gt; /u/VanarasAgenticAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4j3gr/vanaras_localfirst_agentic_ai_framework_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4j3gr/vanaras_localfirst_agentic_ai_framework_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4j3gr/vanaras_localfirst_agentic_ai_framework_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T10:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4o31k</id>
    <title>Nanocoder VS Code Plugin is Coming Along!</title>
    <updated>2025-11-23T14:30:09+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p4o31k/nanocoder_vs_code_plugin_is_coming_along/"&gt; &lt;img alt="Nanocoder VS Code Plugin is Coming Along!" src="https://preview.redd.it/0bdpxyrvm03g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a452110a43bda75376e5ffac7e29e49df77ae63" title="Nanocoder VS Code Plugin is Coming Along!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0bdpxyrvm03g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4o31k/nanocoder_vs_code_plugin_is_coming_along/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4o31k/nanocoder_vs_code_plugin_is_coming_along/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T14:30:09+00:00</published>
  </entry>
</feed>
