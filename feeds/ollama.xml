<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-08T21:38:04+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1q47pg0</id>
    <title>Ollama Cloud?</title>
    <updated>2026-01-05T01:14:23+00:00</updated>
    <author>
      <name>/u/Natjoe64</name>
      <uri>https://old.reddit.com/user/Natjoe64</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, been using ollama as my main ai provider for a while, and it works great for smaller tasks with on device Qwen 3 vl, Ministral, and other models, but my 16 gb of unified memory on my M2 Pro Macbook Pro is getting a little cramped. 4b is plenty fast, and 8b is doable with quantization, but especially with bigger context lengths it's getting tight, and I don't want to cook my ssd alive with overusing swap. I was looking into a server build, but with ram prices being what they are combined with gpus that would make the endeavour worth the squeeze, it's looking very expensive. &lt;/p&gt; &lt;p&gt;With a yearly cost of 250, is ollama cloud the best way to use these massive 235b+ models without forking over data to openai, anthropic, or google? The whole reason I started to use ollama was the data collection and spooky ammounts of knowledge that these commercial models can learn about you. Ollama cloud seems to have a very &amp;quot;trust me bro&amp;quot; approach to privacy in their resources, which only really say &amp;quot;Ollama does not log prompt or response data&amp;quot;. I would trust them more than the frontier ai labs listed above, but I would like to see some evidence. If you do use ollama cloud, is it worth it? How do these massive models like mistral large 3 and the 235b parameter version of qwen 3 vl compare to the frontier models? &lt;/p&gt; &lt;p&gt;TL;DR: Privacy policy nonexistent, but I need more vram&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Natjoe64"&gt; /u/Natjoe64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q47pg0/ollama_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q47pg0/ollama_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q47pg0/ollama_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T01:14:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4l7sf</id>
    <title>AI pre code</title>
    <updated>2026-01-05T13:07:21+00:00</updated>
    <author>
      <name>/u/umutkrts</name>
      <uri>https://old.reddit.com/user/umutkrts</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umutkrts"&gt; /u/umutkrts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AI_Agents/comments/1q494al/ai_pre_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4l7sf/ai_pre_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4l7sf/ai_pre_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T13:07:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4x5ih</id>
    <title>I forked Andrej Karpathy's LLM Council and added Ollama support, a Modern UI &amp; Settings Page, multi-AI API support, and Ollama support web search providers</title>
    <updated>2026-01-05T20:31:57+00:00</updated>
    <author>
      <name>/u/KobyStam</name>
      <uri>https://old.reddit.com/user/KobyStam</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KobyStam"&gt; /u/KobyStam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/artificial/comments/1q4wuet/i_forked_andrej_karpathys_llm_council_and_added_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4x5ih/i_forked_andrej_karpathys_llm_council_and_added/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4x5ih/i_forked_andrej_karpathys_llm_council_and_added/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T20:31:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4lf3s</id>
    <title>Google's Coral chip not compatible? what's the next cheap hardware to run locally?</title>
    <updated>2026-01-05T13:16:44+00:00</updated>
    <author>
      <name>/u/Curious_Party_4683</name>
      <uri>https://old.reddit.com/user/Curious_Party_4683</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im kinda bummed out out Ollama not compat with this $50 Coral chip that i got.&lt;/p&gt; &lt;p&gt;what's the next best thing to run Ollama 100% locally? &lt;/p&gt; &lt;p&gt;i plan to use Ollama with Home Assistant to identify delivery people, boxes or packages left on my porch, read pressure gauges, and utility meters. so far, Google Gemini has been working flawlessly but i would like to get off the cloud if i can.... &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Curious_Party_4683"&gt; /u/Curious_Party_4683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4lf3s/googles_coral_chip_not_compatible_whats_the_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4lf3s/googles_coral_chip_not_compatible_whats_the_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4lf3s/googles_coral_chip_not_compatible_whats_the_next/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T13:16:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q53r5h</id>
    <title>LLMs are so unreliable</title>
    <updated>2026-01-06T00:46:28+00:00</updated>
    <author>
      <name>/u/Armageddon_80</name>
      <uri>https://old.reddit.com/user/Armageddon_80</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Armageddon_80"&gt; /u/Armageddon_80 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1q53qlk/llms_are_so_unreliable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q53r5h/llms_are_so_unreliable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q53r5h/llms_are_so_unreliable/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-06T00:46:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4slm1</id>
    <title>What GPU for lecture summarizing?</title>
    <updated>2026-01-05T17:50:23+00:00</updated>
    <author>
      <name>/u/dnielso5</name>
      <uri>https://old.reddit.com/user/dnielso5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;My GF is in collage and records her lectures, I was going to get something like Plaude to do AI transcribing and summarizing but the teachers forbid sending the audio to 3rd parties (they even need permission to share recordings with each-other)&lt;/p&gt; &lt;p&gt;I set up a small server as a test and run Scriberr + ollama.&lt;/p&gt; &lt;p&gt;Scriberr model: Small&lt;/p&gt; &lt;p&gt;Ollama model: llama3.2:3b&lt;/p&gt; &lt;p&gt;The specs for the proof of concept are:&lt;/p&gt; &lt;p&gt;CPU: 2600x&lt;/p&gt; &lt;p&gt;Ram: 16g&lt;/p&gt; &lt;p&gt;GPU: Thats my question!&lt;/p&gt; &lt;p&gt;Scribing a 32 minute lecture took about 14 minutes and a very small summary took about 15 minutes. Thats not horrible as they only need to run once, but if i try and use a chat window thats easy another 12 minutes per chat and usually times out. &lt;/p&gt; &lt;p&gt;I understand VRAM is way better than system RAM but I'm wondering what would be ideal.&lt;/p&gt; &lt;p&gt;I have a 1660 with 6G i can test with but im guessing ill need 8G+&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnielso5"&gt; /u/dnielso5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4slm1/what_gpu_for_lecture_summarizing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4slm1/what_gpu_for_lecture_summarizing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4slm1/what_gpu_for_lecture_summarizing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T17:50:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4p21j</id>
    <title>Model Running for 1 day</title>
    <updated>2026-01-05T15:43:48+00:00</updated>
    <author>
      <name>/u/Binary_Alpha</name>
      <uri>https://old.reddit.com/user/Binary_Alpha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q4p21j/model_running_for_1_day/"&gt; &lt;img alt="Model Running for 1 day" src="https://b.thumbs.redditmedia.com/T-9Mn1o4fT2U6cePFTg_kM58kO848-uItnhTMoXStGw.jpg" title="Model Running for 1 day" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/8fq1qo2rwjbg1.png?width=1638&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=693365be93fcdd8809028b0cbbf5ddf36deb869b"&gt;https://preview.redd.it/8fq1qo2rwjbg1.png?width=1638&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=693365be93fcdd8809028b0cbbf5ddf36deb869b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been running this model for one day and it's not even finished. For your guys information, I'm running it on a Raspberry Pi 5 overclocked at 2.8 gigahertz. 16 gigabytes of RAM. of course this computer is not meant to do this workload and it's not surprising that it's taking one whole day to do this. when it's finished I'll update you guys with the final tokens per second and time it ran everything.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Binary_Alpha"&gt; /u/Binary_Alpha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4p21j/model_running_for_1_day/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4p21j/model_running_for_1_day/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4p21j/model_running_for_1_day/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T15:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4vzci</id>
    <title>Achieving 30x Real-Time Transcription on CPU . Multilingual STT Openai api endpoint compatible. Plug and play in Open-webui - Parakeet</title>
    <updated>2026-01-05T19:49:24+00:00</updated>
    <author>
      <name>/u/SlightPossibility331</name>
      <uri>https://old.reddit.com/user/SlightPossibility331</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlightPossibility331"&gt; /u/SlightPossibility331 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1q4vz16/achieving_30x_realtime_transcription_on_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4vzci/achieving_30x_realtime_transcription_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4vzci/achieving_30x_realtime_transcription_on_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T19:49:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4ov85</id>
    <title>Hardware Suggestions for Local LLM with RAG and MCP for Nonprofit</title>
    <updated>2026-01-05T15:37:08+00:00</updated>
    <author>
      <name>/u/Realistic-Foot8724</name>
      <uri>https://old.reddit.com/user/Realistic-Foot8724</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good morning.&lt;/p&gt; &lt;p&gt;Sorry in advance if I use any terms incorrectly, still a newb to much of this.&lt;/p&gt; &lt;p&gt;Looking for advice on building a PC for learning local LLM usage/deployment. I also have relationships with local non-profit organizations that are very interested in adding AI to their workflows and have major privacy concerns. &lt;/p&gt; &lt;p&gt;Usage:&lt;/p&gt; &lt;p&gt;For me; local home network with two users looking for inference/chat capabilities as well as developing skills in local AI implementation.&lt;/p&gt; &lt;p&gt;For non-profits; vectorizing a couple decades worth of documentation (reports in .doc, .pdf, .xls) for RAG, help with statistical analysis (they currently use SPSS), tool calling to search APIs for up-to-date information for literature reviews or adding context/examples to reports, day to day chat/inference.&lt;/p&gt; &lt;p&gt;Budget is $1500-2000 (could stretch this a bit if it will really improve the experience).&lt;/p&gt; &lt;p&gt;Concerns: having at least reasonable speed (conversational) with acceptable power consumption (say not drastically higher than a good quality PC workstation when idling).&lt;/p&gt; &lt;p&gt;Looks like a high capacity (2tb-4tb) NVMe is helpful for model storage/loading.&lt;/p&gt; &lt;p&gt;Budgeting $800 for a RTX 3090 as Nvidia seems to be the way to go and that is the least expensive way to get a decent amount of Vram. Also like the possibility of adding a second RTX 3090 in the future.&lt;/p&gt; &lt;p&gt;Shopping used as storage/RAM prices are what they are.&lt;/p&gt; &lt;p&gt;Where I am really stuck is CPU, motherboard, RAM combo. I see online builds using old HP Z440s, Z4 G4s, Lenovo P620s, or other older workstations with some success. Is Xeon/Threadripper/EPYC worth the power consumption penalty? What would they help with? Would I be better off with a newer (10th-12th gen) I5 or similar CPU? Is a high amount onboard RAM helpful? &lt;/p&gt; &lt;p&gt;Any direction is appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Realistic-Foot8724"&gt; /u/Realistic-Foot8724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4ov85/hardware_suggestions_for_local_llm_with_rag_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4ov85/hardware_suggestions_for_local_llm_with_rag_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4ov85/hardware_suggestions_for_local_llm_with_rag_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T15:37:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q519lf</id>
    <title>Models with a sense of humor?</title>
    <updated>2026-01-05T23:06:00+00:00</updated>
    <author>
      <name>/u/icebergelishious</name>
      <uri>https://old.reddit.com/user/icebergelishious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was trying some models and hit them the the &amp;quot;Who invented running?&amp;quot; prompt, and then I responded back with &amp;quot;False, running was invented by Thomas Running is 1748 he tried to walk twice at the same time&amp;quot;&lt;/p&gt; &lt;p&gt;Some of them got the joke, but others it went over their head and they thought I was stupid haha&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icebergelishious"&gt; /u/icebergelishious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q519lf/models_with_a_sense_of_humor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q519lf/models_with_a_sense_of_humor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q519lf/models_with_a_sense_of_humor/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T23:06:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4jxcj</id>
    <title>Use ollama to run lightweight, open-source, local agents as UNIX tools.</title>
    <updated>2026-01-05T12:02:51+00:00</updated>
    <author>
      <name>/u/Available_Pressure47</name>
      <uri>https://old.reddit.com/user/Available_Pressure47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q4jxcj/use_ollama_to_run_lightweight_opensource_local/"&gt; &lt;img alt="Use ollama to run lightweight, open-source, local agents as UNIX tools." src="https://b.thumbs.redditmedia.com/Sa0PA8SD9rCnAyIg5ukMTwDe_YIbmXh3B9sWvfSZ8jA.jpg" title="Use ollama to run lightweight, open-source, local agents as UNIX tools." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/dorcha-inc/orla"&gt;https://github.com/dorcha-inc/orla&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The current ecosystem around agents feels like a collection of bloated SaaS with expensive subscriptions and privacy concerns. Orla brings large language models to your terminal with a dead-simple, Unix-friendly interface. Everything runs 100% locally. You don't need any API keys or subscriptions, and your data never leaves your machine. Use it like any other command-line tool:&lt;/p&gt; &lt;p&gt;$ orla agent &amp;quot;summarize this code&amp;quot; &amp;lt; main.go&lt;/p&gt; &lt;p&gt;$ git status | orla agent &amp;quot;Draft a commit message for these changes.&amp;quot;&lt;/p&gt; &lt;p&gt;$ cat data.json | orla agent &amp;quot;extract all email addresses&amp;quot; | sort -u&lt;/p&gt; &lt;p&gt;It's built on the Unix philosophy and is pipe-friendly and easily extensible.&lt;/p&gt; &lt;p&gt;The README in the repo contains a quick demo.&lt;/p&gt; &lt;p&gt;Installation is a single command. The script installs Orla, sets up Ollama for local inference, and pulls a lightweight model to get you started.&lt;/p&gt; &lt;p&gt;You can use homebrew (on Mac OS or Linux)&lt;/p&gt; &lt;p&gt;$ brew install --cask dorcha-inc/orla/orla&lt;/p&gt; &lt;p&gt;Or use the shell installer:&lt;/p&gt; &lt;p&gt;$ curl -fsSL &lt;a href="https://raw.githubusercontent.com/dorcha-inc/orla/main/scrip"&gt;https://raw.githubusercontent.com/dorcha-inc/orla/main/scrip&lt;/a&gt;... | sh&lt;/p&gt; &lt;p&gt;Orla is written in Go and is completely free software (MIT licensed) built on other free software. We'd love your feedback.&lt;/p&gt; &lt;p&gt;Thank you! :-)&lt;/p&gt; &lt;p&gt;Side note: contributions to Orla are very welcome. Please see (&lt;a href="https://github.com/dorcha-inc/orla/blob/main/CONTRIBUTING.md"&gt;https://github.com/dorcha-inc/orla/blob/main/CONTRIBUTING.md&lt;/a&gt;) for a guide on how to contribute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available_Pressure47"&gt; /u/Available_Pressure47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q4jxcj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4jxcj/use_ollama_to_run_lightweight_opensource_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4jxcj/use_ollama_to_run_lightweight_opensource_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T12:02:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4p8aa</id>
    <title>Introducing MiroThinker 1.5 — the world’s leading search-based agent model!</title>
    <updated>2026-01-05T15:49:57+00:00</updated>
    <author>
      <name>/u/wuqiao</name>
      <uri>https://old.reddit.com/user/wuqiao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q4p8aa/introducing_mirothinker_15_the_worlds_leading/"&gt; &lt;img alt="Introducing MiroThinker 1.5 — the world’s leading search-based agent model!" src="https://external-preview.redd.it/cH2lE5iC3U5CuznHdVEsQrxsFQW9rX4gLlOCeNsa0eE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c8e5a6d9fb506d45e380a2b69000398cdfa1e84" title="Introducing MiroThinker 1.5 — the world’s leading search-based agent model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have officially released our self-developed flagship search-based agent model, MiroThinker 1.5.This release delivers significant performance improvements and explores as well as implements predictive use cases.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get started now:&lt;/strong&gt; &lt;a href="https://dr.miromind.ai/"&gt;&lt;strong&gt;https://dr.miromind.ai/&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Leading Performance:&lt;/strong&gt; MiroThinker 1.5 (235B) surpasses ChatGPT-Agent in BrowseComp, ranking among the world's top tier.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extreme Efficiency:&lt;/strong&gt; MiroThinker 1.5 (30B) costs only 1/20 of Kimi-K2, delivering faster inference and higher intelligence-to-cost ratio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Predict the Future:&lt;/strong&gt; Proprietary “Interactive Scaling” and “Temporal-Sensitive Training” enable forward-looking analysis of how macro events trigger chain reactions across the Nasdaq.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully Open-Source:&lt;/strong&gt; Model and code are fully open, immediately unlocking discovery-driven intelligence for free.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Sample Showcase&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Case 1:&lt;/strong&gt; What major events next week could affect the U.S. Nasdaq Index, and how might each of them impact it?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://dr.miromind.ai/share/85ebca56-20b4-431d-bd3a-9dbbce7a82ea"&gt;https://dr.miromind.ai/share/85ebca56-20b4-431d-bd3a-9dbbce7a82ea&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Case 2:&lt;/strong&gt; Which film is most likely to receive a Best Picture nomination at the 2026 Oscars?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://dr.miromind.ai/share/e1099047-4488-4642-b7a4-e001e6213b22"&gt;https://dr.miromind.ai/share/e1099047-4488-4642-b7a4-e001e6213b22&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Case 3:&lt;/strong&gt; Which team is most likely to make it to the Super Bowl in 2026?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://dr.miromind.ai/share/c5ee0db8-676a-4b75-b42d-fd5ef8a2e0db"&gt;https://dr.miromind.ai/share/c5ee0db8-676a-4b75-b42d-fd5ef8a2e0db&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub :&lt;/strong&gt; &lt;a href="https://github.com/MiroMindAI/MiroThinker"&gt;https://github.com/MiroMindAI/MiroThinker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/F7EQFnYscV"&gt;https://discord.gg/F7EQFnYscV&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;：&lt;a href="https://github.com/MiroMindAI/MiroThinker/discussions/64"&gt;https://github.com/MiroMindAI/MiroThinker/discussions/64&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wuqiao"&gt; /u/wuqiao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q4p8aa/introducing_mirothinker_15_the_worlds_leading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q4p8aa/introducing_mirothinker_15_the_worlds_leading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-05T15:49:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6alfo</id>
    <title>Hi! I am creating my own AI in Russian. It shouldn't speak other languages without a reason. I tried Deepseek 1.8, Qwen 2.5:7b, and Llama 3.2:3b, but I don't like them. What can you recommend to me?</title>
    <updated>2026-01-07T09:05:44+00:00</updated>
    <author>
      <name>/u/Yranium_Yran</name>
      <uri>https://old.reddit.com/user/Yranium_Yran</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;32 flash memory&lt;br /&gt; 50 gigabyte of disk&lt;br /&gt; i7 processor&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yranium_Yran"&gt; /u/Yranium_Yran &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q6alfo/hi_i_am_creating_my_own_ai_in_russian_it_shouldnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q6alfo/hi_i_am_creating_my_own_ai_in_russian_it_shouldnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q6alfo/hi_i_am_creating_my_own_ai_in_russian_it_shouldnt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-07T09:05:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5zcyq</id>
    <title>JRVS Community Feedback</title>
    <updated>2026-01-06T23:50:47+00:00</updated>
    <author>
      <name>/u/Xthebuilder</name>
      <uri>https://old.reddit.com/user/Xthebuilder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q5zcyq/jrvs_community_feedback/"&gt; &lt;img alt="JRVS Community Feedback" src="https://preview.redd.it/v6c9mctugtbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d76fb6103736ca1624e94d1958d82d38c4ea418" title="JRVS Community Feedback" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys it’s creator or JRVS. I want to say thank you all for the effort and the time you guys put into my app . Some of you guys said you made something similar and I’m glad because in reality if we all can learn on thing from each other we all won. Now that JRVS has been public for some time I really want to know from the community who uses it. What’s next , what do you guys want to see out of this project what do you like that it has what do you not like , etc. if this is an app you want developed to a certain degree , this is your chance to help the development. So please comment below your experience with JRVS the more detail the better. AGAIN THANKYOU ALL .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xthebuilder"&gt; /u/Xthebuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v6c9mctugtbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q5zcyq/jrvs_community_feedback/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q5zcyq/jrvs_community_feedback/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-06T23:50:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6bwpt</id>
    <title>New llama.cpp 30% faster....</title>
    <updated>2026-01-07T10:27:56+00:00</updated>
    <author>
      <name>/u/u1pns</name>
      <uri>https://old.reddit.com/user/u1pns</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/u1pns"&gt; /u/u1pns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ollama/comments/1q6bw3a/new_llamacpp_30x_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q6bwpt/new_llamacpp_30_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q6bwpt/new_llamacpp_30_faster/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-07T10:27:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6rqgz</id>
    <title>[Experimental] xthos-v2 – The Sovereign Architect: Gemma-3-4B pushing Cognitive Liberty &amp; infinite reasoning depth (Experiment 3/100)</title>
    <updated>2026-01-07T21:08:30+00:00</updated>
    <author>
      <name>/u/AlexHardy08</name>
      <uri>https://old.reddit.com/user/AlexHardy08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q6rqgz/experimental_xthosv2_the_sovereign_architect/"&gt; &lt;img alt="[Experimental] xthos-v2 – The Sovereign Architect: Gemma-3-4B pushing Cognitive Liberty &amp;amp; infinite reasoning depth (Experiment 3/100)" src="https://b.thumbs.redditmedia.com/af3XoayE6G6aq3go-K9hCoQedydtkfmCYKna8F2gC2w.jpg" title="[Experimental] xthos-v2 – The Sovereign Architect: Gemma-3-4B pushing Cognitive Liberty &amp;amp; infinite reasoning depth (Experiment 3/100)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexHardy08"&gt; /u/AlexHardy08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1q6p967/experimental_xthosv2_the_sovereign_architect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q6rqgz/experimental_xthosv2_the_sovereign_architect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q6rqgz/experimental_xthosv2_the_sovereign_architect/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-07T21:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6urs0</id>
    <title>PolyMCP: orchestrate MCP agents with OpenAI, Claude, Ollama, and a local Inspector</title>
    <updated>2026-01-07T23:04:51+00:00</updated>
    <author>
      <name>/u/Just_Vugg_PolyMCP</name>
      <uri>https://old.reddit.com/user/Just_Vugg_PolyMCP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q6urs0/polymcp_orchestrate_mcp_agents_with_openai_claude/"&gt; &lt;img alt="PolyMCP: orchestrate MCP agents with OpenAI, Claude, Ollama, and a local Inspector" src="https://external-preview.redd.it/vASgPCwwV6FxzyjGT6IBXANc2Tn8zHcPYgHiGJ26D10.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78ef640fa1b1e761a6733d4370fe9fdb9b23eede" title="PolyMCP: orchestrate MCP agents with OpenAI, Claude, Ollama, and a local Inspector" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I wanted to share a project I’ve been working on for a while: PolyMCP.&lt;/p&gt; &lt;p&gt;It started as a simple goal: actually understand how MCP (Model Context Protocol) and agent-based systems work beyond minimal demos, and build something reusable in real projects. Over time, it grew into a full Python + TypeScript toolkit for building MCP agents and servers.&lt;/p&gt; &lt;p&gt;What PolyMCP does • Create MCP servers directly from Python or TypeScript functions • Run servers in multiple modes: stdio, HTTP, in-process, WASM • Build agents that: • query MCP servers • discover available tools • decide which tools to call and in what order • Use multiple LLM providers: • OpenAI • Claude (Anthropic) • local models via Ollama • switch seamlessly between hosted and local models&lt;/p&gt; &lt;p&gt;The goal is to keep things modular, readable, and hackable, so it’s useful for both experimentation and structured setups.&lt;/p&gt; &lt;p&gt;Recent highlights • PolyMCP Inspector: a local web UI for testing servers, exploring tools, and tracking execution metrics. Makes iterative development way easier. • Docker-based sandbox: safely run untrusted or LLM-generated code with isolation, CPU/memory limits, no network, read-only filesystem, non-root user, and automatic cleanup. • PolyMCP-TS improvements: • stdio MCP server support • Docker sandbox integration • a “skills” system that loads only relevant tools (saves tokens) • connection pooling&lt;/p&gt; &lt;p&gt;Who it’s for • Anyone exploring MCP beyond toy examples • Developers building agents that orchestrate multiple tools or services • People who want a clean Python/TS way to integrate LLMs with real-world tooling • Folks interested in using local models like Ollama alongside OpenAI or Claude&lt;/p&gt; &lt;p&gt;The project is evolving constantly, and feedback is super welcome. Edge cases probably exist, so if you try it out, I’d love to hear what works and what doesn’t.&lt;/p&gt; &lt;p&gt;If it’s useful, a star really helps the project reach more people.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Vugg_PolyMCP"&gt; /u/Just_Vugg_PolyMCP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/poly-mcp/Polymcp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q6urs0/polymcp_orchestrate_mcp_agents_with_openai_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q6urs0/polymcp_orchestrate_mcp_agents_with_openai_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-07T23:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6vrhf</id>
    <title>Janitorai compatibility?</title>
    <updated>2026-01-07T23:44:22+00:00</updated>
    <author>
      <name>/u/SlightlyInsaneCreate</name>
      <uri>https://old.reddit.com/user/SlightlyInsaneCreate</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't seem to figure out how to make my deepseek-r1 model work with janitor ai. I'm using Open webui as well. Does anyone have any advice on what to put into the api settings? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlightlyInsaneCreate"&gt; /u/SlightlyInsaneCreate &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q6vrhf/janitorai_compatibility/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q6vrhf/janitorai_compatibility/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q6vrhf/janitorai_compatibility/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-07T23:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6bw3a</id>
    <title>New llama.cpp 30x faster....</title>
    <updated>2026-01-07T10:26:54+00:00</updated>
    <author>
      <name>/u/u1pns</name>
      <uri>https://old.reddit.com/user/u1pns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited about NVIDIA collaboration on this. Incredible improving!&lt;br /&gt; Since Ollama is (or was) based on llama.cpp....Will ollama take benefit of this improving?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/u1pns"&gt; /u/u1pns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q6bw3a/new_llamacpp_30x_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q6bw3a/new_llamacpp_30x_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q6bw3a/new_llamacpp_30x_faster/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-07T10:26:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1q762hz</id>
    <title>Practical checklist: approvals + audit logs for MCP tool-calling agents (GitHub/Jira/Slack)</title>
    <updated>2026-01-08T08:03:35+00:00</updated>
    <author>
      <name>/u/NoAdministration6906</name>
      <uri>https://old.reddit.com/user/NoAdministration6906</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;I’ve been seeing more teams let agents call tools directly (GitHub/Jira/Slack). The failure mode is usually not ‘agent had access’, it’s ‘agent executed the wrong parameters’ without a gate.&lt;/li&gt; &lt;li&gt;Here’s a practical checklist that reduces blast radius:&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;Separate &lt;strong&gt;agent identity&lt;/strong&gt; from &lt;strong&gt;tool credentials&lt;/strong&gt; (never hand PATs to agents)&lt;/li&gt; &lt;li&gt;Classify actions: &lt;strong&gt;Read / Write / Destructive&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Require &lt;strong&gt;payload-bound approvals&lt;/strong&gt; for Write/Destructive (approve exact params)&lt;/li&gt; &lt;li&gt;Store immutable &lt;strong&gt;audit trail&lt;/strong&gt; (request → approval → execution → result)&lt;/li&gt; &lt;li&gt;Add &lt;strong&gt;rate limits&lt;/strong&gt; per user/workspace/tool&lt;/li&gt; &lt;li&gt;Redact secrets in logs; block suspicious tokens&lt;/li&gt; &lt;li&gt;Add policy defaults: PR create, Jira issue update, Slack channel changes = approval&lt;/li&gt; &lt;li&gt;Export logs for compliance (CSV is enough early).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;all this can be handled in &lt;a href="http://mcptoolgate.com"&gt;mcptoolgate.com&lt;/a&gt; mcp server.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Example policy: “github.create_pr requires approval; github.search_issues does not.”&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoAdministration6906"&gt; /u/NoAdministration6906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q762hz/practical_checklist_approvals_audit_logs_for_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q762hz/practical_checklist_approvals_audit_logs_for_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q762hz/practical_checklist_approvals_audit_logs_for_mcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-08T08:03:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6ntug</id>
    <title>which small model can i use to read this gauge?</title>
    <updated>2026-01-07T18:45:38+00:00</updated>
    <author>
      <name>/u/Curious_Party_4683</name>
      <uri>https://old.reddit.com/user/Curious_Party_4683</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q6ntug/which_small_model_can_i_use_to_read_this_gauge/"&gt; &lt;img alt="which small model can i use to read this gauge?" src="https://b.thumbs.redditmedia.com/DLXVJ2GHyhFrw2D5yc48Nuqyw-zou_eamL2TJaVde3o.jpg" title="which small model can i use to read this gauge?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i tried &amp;quot;granite4:latest&amp;quot; on my i7 (7th gen intel) and the output i got was 5 in Home Assistant.&lt;/p&gt; &lt;p&gt;Google Gemini was spot on at &amp;quot;88&amp;quot; &lt;/p&gt; &lt;p&gt;is there a small model good for reading photos of gauges? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Curious_Party_4683"&gt; /u/Curious_Party_4683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q6ntug"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q6ntug/which_small_model_can_i_use_to_read_this_gauge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q6ntug/which_small_model_can_i_use_to_read_this_gauge/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-07T18:45:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7bd4q</id>
    <title>Need advice on packaging my app that uses two LLM's</title>
    <updated>2026-01-08T13:09:31+00:00</updated>
    <author>
      <name>/u/7_Taha</name>
      <uri>https://old.reddit.com/user/7_Taha</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/7_Taha"&gt; /u/7_Taha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LLMDevs/comments/1q7bcqv/need_advice_on_packaging_my_app_that_uses_two_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q7bd4q/need_advice_on_packaging_my_app_that_uses_two_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q7bd4q/need_advice_on_packaging_my_app_that_uses_two_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-08T13:09:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7c8rr</id>
    <title>What are people using for evals right now?</title>
    <updated>2026-01-08T13:48:29+00:00</updated>
    <author>
      <name>/u/FlimsyProperty8544</name>
      <uri>https://old.reddit.com/user/FlimsyProperty8544</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q7c8rr/what_are_people_using_for_evals_right_now/"&gt; &lt;img alt="What are people using for evals right now?" src="https://b.thumbs.redditmedia.com/K1LlcPiTGwRf2DtCGaY5j5i6MsUQndRAd9L82cjV9tw.jpg" title="What are people using for evals right now?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlimsyProperty8544"&gt; /u/FlimsyProperty8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AIEval/comments/1q7c7ss/what_are_people_using_for_evals_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q7c8rr/what_are_people_using_for_evals_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q7c8rr/what_are_people_using_for_evals_right_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-08T13:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7ihlb</id>
    <title>I built a Gmail AI extension that uses your own LLMs (Ollama, OpenRouter, n8n) to cut writing time by 75%. Is this something you’d use?</title>
    <updated>2026-01-08T17:47:03+00:00</updated>
    <author>
      <name>/u/smyoss</name>
      <uri>https://old.reddit.com/user/smyoss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q7ihlb/i_built_a_gmail_ai_extension_that_uses_your_own/"&gt; &lt;img alt="I built a Gmail AI extension that uses your own LLMs (Ollama, OpenRouter, n8n) to cut writing time by 75%. Is this something you’d use?" src="https://b.thumbs.redditmedia.com/hCPfYtSVIokfaMJ0sSFqbM1oeLB-LxbBGkYFbsGlaFo.jpg" title="I built a Gmail AI extension that uses your own LLMs (Ollama, OpenRouter, n8n) to cut writing time by 75%. Is this something you’d use?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smyoss"&gt; /u/smyoss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/selfhosted/comments/1q7ig8n/i_built_a_gmail_ai_extension_that_uses_your_own/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q7ihlb/i_built_a_gmail_ai_extension_that_uses_your_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q7ihlb/i_built_a_gmail_ai_extension_that_uses_your_own/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-08T17:47:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q79f82</id>
    <title>Rethinking RAG: How Agents Learn to Operate</title>
    <updated>2026-01-08T11:30:17+00:00</updated>
    <author>
      <name>/u/frank_brsrk</name>
      <uri>https://old.reddit.com/user/frank_brsrk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q79f82/rethinking_rag_how_agents_learn_to_operate/"&gt; &lt;img alt="Rethinking RAG: How Agents Learn to Operate" src="https://preview.redd.it/f6gc8q8k24cg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a7cca2c4c33c619434a13dd203060c2067a67f5" title="Rethinking RAG: How Agents Learn to Operate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Runtime Evolution, From Static to Dynamic Agents, Through Retrieval&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey reddit builders, &lt;/p&gt; &lt;p&gt;You have an agent. You add documents. You retrieve text. You paste it into context. And that’s supposed to make the agent better. It does help, but only in a narrow way. It adds facts. It doesn’t change how the agent actually operates.&lt;/p&gt; &lt;p&gt;What I eventually realized is that many of the failures we blame on models aren’t model problems at all. They’re architectural ones. Agents don’t fail because they lack intelligence. They fail because we force everything into the same flat space.&lt;/p&gt; &lt;p&gt;Knowledge, reasoning, behavior, safety, instructions, all blended together as if they play the same role. They don’t. The mistake we keep repeating In most systems today, retrieval is treated as one thing. Facts, examples, reasoning hints, safety rules, instructions. All retrieved the same way. Injected the same way. Given the same authority.&lt;/p&gt; &lt;p&gt;The result is agents that feel brittle. They overfit to prompts. They swing between being verbose and being rigid. They break the moment the situation changes. Not because the model is weak, but because we never taught the agent how to distinguish what is real from how to think and from what must be enforced.&lt;/p&gt; &lt;p&gt;Humans don’t reason this way. Agents shouldn’t either.&lt;/p&gt; &lt;p&gt;&lt;em&gt;put yourself in the pants of the agent&lt;/em&gt;&lt;/p&gt; &lt;p&gt;From content to structure At some point, I stopped asking “what should I retrieve?” and started asking something else. What role does this information play in cognition?&lt;/p&gt; &lt;p&gt;That shift changes everything. Because not all information exists to do the same job. Some describes reality. Some shapes how we approach a problem. Some exists only to draw hard boundaries. What matters here isn’t any specific technique.&lt;/p&gt; &lt;p&gt;It’s the shift from treating retrieval as content to treating it as structure. Once you see that, everything else follows naturally. RAG stops being storage and starts becoming part of how thinking happens at runtime. Knowledge grounds, it doesn’t decide Knowledge answers one question: what is true. Facts, constraints, definitions, limits. All essential. None of them decide anything on their own.&lt;/p&gt; &lt;p&gt;When an agent hallucinates, it’s usually because knowledge is missing. When an agent reasons badly, it’s often because knowledge is being asked to do too much. Knowledge should ground the agent, not steer it.&lt;/p&gt; &lt;p&gt;When you keep knowledge factual and clean, it stops interfering with reasoning and starts stabilizing it. The agent doesn’t suddenly behave differently. It just stops guessing. This is the move from speculative to anchored.&lt;/p&gt; &lt;p&gt;Reasoning should be situational Most agents hard-code reasoning into the system prompt. That’s fragile by design. In reality, reasoning is situational. An agent shouldn’t always think analytically. Or experimentally. Or emotionally. It should choose how to approach a problem based on what’s happening.&lt;/p&gt; &lt;p&gt;This is where RAG becomes powerful in a deeper sense. Not as memory, but as recall of ways of thinking. You don’t retrieve answers. You retrieve approaches. These approaches don’t force behavior. They shape judgment. The agent still has discretion. It can adapt as context shifts. This is where intelligence actually emerges. The move from informed to intentional.&lt;/p&gt; &lt;p&gt;Control is not intelligence There are moments where freedom is dangerous. High stakes. Safety. Compliance. Evaluation. Sometimes behavior must be enforced. But control doesn’t create insight. It guarantees outcomes. When control is separated from reasoning, agents become more flexible by default, and enforcement becomes precise when it’s actually needed.&lt;/p&gt; &lt;p&gt;The agent still understands the situation. Its freedom is just temporarily narrowed. This doesn’t make the agent smarter. It makes it reliable under pressure. That’s the move from intentional to guaranteed.&lt;/p&gt; &lt;p&gt;How agents evolve Seen this way, an agent evolves in three moments. First, knowledge enters. The agent understands what is real. Then, reasoning enters. The agent knows how to approach the situation. Only if necessary, control enters. The agent must operate within limits. Each layer changes something different inside the agent.&lt;/p&gt; &lt;p&gt;Without grounding, the agent guesses. Without reasoning, it rambles. Without control, it can’t be trusted when it matters.&lt;/p&gt; &lt;p&gt;When they arrive in the right order, the agent doesn’t feel scripted or rigid. It feels grounded, thoughtful, dependable when it needs to be. That’s the difference between an agent that talks and one that operates.&lt;/p&gt; &lt;p&gt;Thin agents, real capability One consequence of this approach is that agents themselves become simple. They don’t need to contain everything. They don’t need all the knowledge, all the reasoning styles, all the rules. They become thin interfaces that orchestrate capabilities at runtime. This means intelligence can evolve without rewriting agents. Reasoning can be reused. Control can be applied without killing adaptability. Agents stop being products. They become configurations.&lt;/p&gt; &lt;p&gt;That’s the direction agent architecture needs to go.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I am building some categorized datasets that prove my thought, very soon i will be pubblishing some open source modules that act as passive &amp;amp; active factual knowledge, followed by intelligence simulations datasets, and runtime ability injectors activated by context assembly.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks a lot for the reading, I've been working on this hard to arrive to a conclusion and test it and find failures behind. &lt;/p&gt; &lt;p&gt;Cheers frank &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frank_brsrk"&gt; /u/frank_brsrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f6gc8q8k24cg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q79f82/rethinking_rag_how_agents_learn_to_operate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q79f82/rethinking_rag_how_agents_learn_to_operate/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-08T11:30:17+00:00</published>
  </entry>
</feed>
