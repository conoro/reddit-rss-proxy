<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-27T23:22:35+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1nq6myv</id>
    <title>Service for Efficient Vector Embeddings</title>
    <updated>2025-09-25T13:22:28+00:00</updated>
    <author>
      <name>/u/mrdabbler</name>
      <uri>https://old.reddit.com/user/mrdabbler</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sometimes I need to use a vector database and do semantic search.&lt;br /&gt; Generating text embeddings via the ML model is the main bottleneck, especially when working with large amounts of data.&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;Vectrain&lt;/strong&gt;, a service that helps speed up this process and might be useful to others. I’m guessing some of you might be facing the same kind of problems.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What the service does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Receives messages for embedding from Kafka or via its own REST API.&lt;/li&gt; &lt;li&gt;Spins up multiple embedder instances working in parallel to speed up embedding generation (currently only Ollama is supported).&lt;/li&gt; &lt;li&gt;Stores the resulting embeddings in a vector database (currently only Qdrant is supported).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’d love to hear your feedback, tips, and, of course, stars on GitHub.&lt;/p&gt; &lt;p&gt;The service is fully functional, and I plan to keep developing it gradually. I’d also love to know how relevant it is—maybe it’s worth investing more effort and pushing it much more actively.&lt;/p&gt; &lt;p&gt;Vectrain repo: &lt;a href="https://github.com/torys877/vectrain"&gt;https://github.com/torys877/vectrain&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrdabbler"&gt; /u/mrdabbler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nq6myv/service_for_efficient_vector_embeddings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nq6myv/service_for_efficient_vector_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nq6myv/service_for_efficient_vector_embeddings/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T13:22:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqki3x</id>
    <title>Me and my friends connected an Humanoid Robot to Local Large Language Models</title>
    <updated>2025-09-25T22:23:04+00:00</updated>
    <author>
      <name>/u/lightofshadow_</name>
      <uri>https://old.reddit.com/user/lightofshadow_</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lightofshadow_"&gt; /u/lightofshadow_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nqe2ll/me_and_my_friends_connected_an_humanoid_robot_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqki3x/me_and_my_friends_connected_an_humanoid_robot_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqki3x/me_and_my_friends_connected_an_humanoid_robot_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T22:23:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqb2ij</id>
    <title>How can I minimize cold start time?</title>
    <updated>2025-09-25T16:15:39+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My server is relatively low-power. Here are some of the main specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 5 3400G (Quad-core)&lt;/li&gt; &lt;li&gt;32 GB DDR4&lt;/li&gt; &lt;li&gt;Intel Arc A380 (6GB GDDR6)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have Ollama up and running through my Intel Arc. Specifically, I have Intel’s IPEX‑LLM Ollama container and accessing the models through Open WebUI.&lt;/p&gt; &lt;p&gt;Given my lower powered specs, I'm sticking with, at highest, 8B models. Once I'm past the first chat, responses come somewhere between instantaneous to maybe 2 seconds of waiting. However, the first chat I send in a while generally takes between 30 - 45 seconds for a response, depending on the model.&lt;/p&gt; &lt;p&gt;I've gathered that this slow start is &amp;quot;warm-up time,&amp;quot; as the model is loading in. I have my appdata on an NVME drive, so there shouldn't be any slowness there. How can I minimize this loading time?&lt;/p&gt; &lt;p&gt;I realize this end-goal may not be able to work as intended with my current hardware, but I do intend to eventually replace Alexa with a self-hosted assistant, powered by Ollama. 45 seconds of wait time seems very excessive for testing, especially since I've found that waiting only about 5 minutes between chats is enough for the model to need that 45 seconds to warm up again..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqb2ij/how_can_i_minimize_cold_start_time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqb2ij/how_can_i_minimize_cold_start_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqb2ij/how_can_i_minimize_cold_start_time/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T16:15:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq0fgx</id>
    <title>Dead-simple example code for Ollama function calling.</title>
    <updated>2025-09-25T07:31:16+00:00</updated>
    <author>
      <name>/u/kirill_saidov</name>
      <uri>https://old.reddit.com/user/kirill_saidov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nq0fgx/deadsimple_example_code_for_ollama_function/"&gt; &lt;img alt="Dead-simple example code for Ollama function calling." src="https://external-preview.redd.it/nwRZM-Te2VIqEnwZwvslkKw4wJFzzEeW_-DOTY7mWJs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efa43a58d310b84e2cd256d70fe4206bb5f499ca" title="Dead-simple example code for Ollama function calling." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This shows how to use function calling + how to get a coherent response from LLM, not just raw results returned by functions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kirill_saidov"&gt; /u/kirill_saidov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kirillsaidov/ollama-function-calling"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nq0fgx/deadsimple_example_code_for_ollama_function/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nq0fgx/deadsimple_example_code_for_ollama_function/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T07:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqknap</id>
    <title>How to change design of 3500 images fast,easy and extremely accurate?</title>
    <updated>2025-09-25T22:29:08+00:00</updated>
    <author>
      <name>/u/Real_Investment_3726</name>
      <uri>https://old.reddit.com/user/Real_Investment_3726</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nqknap/how_to_change_design_of_3500_images_fasteasy_and/"&gt; &lt;img alt="How to change design of 3500 images fast,easy and extremely accurate?" src="https://b.thumbs.redditmedia.com/90gmZt2Rvlu4nBkCtORFJFssImuOYj2OTFVEdj5_7RY.jpg" title="How to change design of 3500 images fast,easy and extremely accurate?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to change the design of 3500 football training exercise images, fast, easily, and extremely accurately? It's not necessary to be 3500 at once; 50 by 50 is totally fine as well, but only if it's extremely accurate.&lt;/p&gt; &lt;p&gt;I was thinking of using the OpenAI API in my custom project and with a prompt to modify a large number of exercises at once (from .png to create a new .png with the Image creator), but the problem is that ChatGPT 5's vision capabilities and image generation were not accurate enough. It was always missing some of the balls, lines, and arrows; some of the arrows were not accurate enough. For example, when I ask ChatGPT to explain how many balls there are in an exercise image and to make it in JSON, instead of hitting the correct number, 22, it hits 5-10 instead, which is pretty terrible if I want perfect or almost perfect results. Seems like it's bad at counting.&lt;/p&gt; &lt;p&gt;Guys how to change design of 3500 images fast,easy and extremely accurate?&lt;/p&gt; &lt;p&gt;That's what OpenAI image generator generated. On the left side is the generated image and on the right side is the original:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9qlzma6e0erf1.png?width=2351&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd33b97691d9e207edfbaaffcfb489ec4195e19b"&gt;https://preview.redd.it/9qlzma6e0erf1.png?width=2351&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd33b97691d9e207edfbaaffcfb489ec4195e19b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Real_Investment_3726"&gt; /u/Real_Investment_3726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqknap/how_to_change_design_of_3500_images_fasteasy_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqknap/how_to_change_design_of_3500_images_fasteasy_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqknap/how_to_change_design_of_3500_images_fasteasy_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T22:29:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqf0t2</id>
    <title>I trained a 4B model to be good at reasoning. Wasn’t expecting this!</title>
    <updated>2025-09-25T18:46:01+00:00</updated>
    <author>
      <name>/u/adeelahmadch</name>
      <uri>https://old.reddit.com/user/adeelahmadch</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adeelahmadch"&gt; /u/adeelahmadch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nqf049/i_trained_a_4b_model_to_be_good_at_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqf0t2/i_trained_a_4b_model_to_be_good_at_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqf0t2/i_trained_a_4b_model_to_be_good_at_reasoning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T18:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqfnff</id>
    <title>First time using granite-code too 😂</title>
    <updated>2025-09-25T19:09:51+00:00</updated>
    <author>
      <name>/u/___-___---</name>
      <uri>https://old.reddit.com/user/___-___---</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nqfnff/first_time_using_granitecode_too/"&gt; &lt;img alt="First time using granite-code too 😂" src="https://b.thumbs.redditmedia.com/Spf6bS4wWycIgh5zgI8v2hKI8H95KDpO56xcj6oiu-A.jpg" title="First time using granite-code too 😂" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/oknmefvj0drf1.png?width=1047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=179ba86a5c4c2b9578a27a3783cc755cc740a0b0"&gt;https://preview.redd.it/oknmefvj0drf1.png?width=1047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=179ba86a5c4c2b9578a27a3783cc755cc740a0b0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/___-___---"&gt; /u/___-___--- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqfnff/first_time_using_granitecode_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqfnff/first_time_using_granitecode_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqfnff/first_time_using_granitecode_too/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T19:09:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqhnrc</id>
    <title>Do you give your models a system prompt? If so, can I get some examples?</title>
    <updated>2025-09-25T20:27:38+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqhnrc/do_you_give_your_models_a_system_prompt_if_so_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqhnrc/do_you_give_your_models_a_system_prompt_if_so_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqhnrc/do_you_give_your_models_a_system_prompt_if_so_can/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T20:27:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqwp6i</id>
    <title>The Evolution of Search - A Brief History of Information Retrieval</title>
    <updated>2025-09-26T09:21:19+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nqwp6i/the_evolution_of_search_a_brief_history_of/"&gt; &lt;img alt="The Evolution of Search - A Brief History of Information Retrieval" src="https://external-preview.redd.it/rdiqDR_pScizHcdiYSt4CYhHHCgIfLWKQPCyQEs-E3k.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9980443776d6f25f3145283b6475431f5b3cff07" title="The Evolution of Search - A Brief History of Information Retrieval" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/ghE4gQkx2b4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqwp6i/the_evolution_of_search_a_brief_history_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqwp6i/the_evolution_of_search_a_brief_history_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T09:21:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqxvsn</id>
    <title>Ollama hangs after a while</title>
    <updated>2025-09-26T10:34:35+00:00</updated>
    <author>
      <name>/u/Aggravating_Pin_8922</name>
      <uri>https://old.reddit.com/user/Aggravating_Pin_8922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using ollama to run models on prem, in order to call them from my code using langchain.&lt;/p&gt; &lt;p&gt;I've noticed that everytime I run ollama for a long time, it starts hangging and I have to reboot it otherwise it doesn't work.&lt;/p&gt; &lt;p&gt;I've also tried to do &amp;quot;ollama run &amp;lt;model&amp;gt;&amp;quot; using the terminal and it also freezes when I do it.&lt;/p&gt; &lt;p&gt;Has anyone had similar problems? How did you overcome them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggravating_Pin_8922"&gt; /u/Aggravating_Pin_8922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqxvsn/ollama_hangs_after_a_while/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqxvsn/ollama_hangs_after_a_while/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqxvsn/ollama_hangs_after_a_while/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T10:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqutzy</id>
    <title>Paiperwork 1.0.2 released, new functionality: SlideForge</title>
    <updated>2025-09-26T07:18:29+00:00</updated>
    <author>
      <name>/u/Infinitai-cn</name>
      <uri>https://old.reddit.com/user/Infinitai-cn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everybody! &lt;/p&gt; &lt;p&gt;We just released an update to our Paiperwork software with a new function: SlideForge, and many bug fixes.&lt;/p&gt; &lt;p&gt;Find us at: &lt;a href="https://infinitai-cn.github.io/paiperwork/"&gt;https://infinitai-cn.github.io/paiperwork/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A shootout to the Presenton team and their gorgeous &lt;a href="https://github.com/presenton/presenton"&gt;AI Presentation software&lt;/a&gt;!, we truly love the style.&lt;/p&gt; &lt;p&gt;Latest update:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Added new presentation tab functionality: SlideForge.&lt;/li&gt; &lt;li&gt;Now when selecting a model in any model selector in the APP, previously loaded models are unloaded to save Vram/ram (Ollama's behavior is to let small models coexist if enough memory, but unused models are not unloaded on demand).&lt;/li&gt; &lt;li&gt;Gpt -oss UI update (thinking level buttons).&lt;/li&gt; &lt;li&gt;Fixed missing translations for paperwork generation.&lt;/li&gt; &lt;li&gt;Fixed meeting minutes generator line spacing.&lt;/li&gt; &lt;li&gt;Web search improved.&lt;/li&gt; &lt;li&gt;Added web search to global document RAG.&lt;/li&gt; &lt;li&gt;Added edit thinking models list to models tab.&lt;/li&gt; &lt;li&gt;Portuguese translations revised and cleaned.&lt;/li&gt; &lt;li&gt;In models tab now you can expand the lists of new thinking and visual models manually.&lt;/li&gt; &lt;li&gt;Added Portuguese to online help.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1nqutzy/video/hvl14pammgrf1/player"&gt;https://reddit.com/link/1nqutzy/video/hvl14pammgrf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our previous introduction post &lt;a href="https://www.reddit.com/r/ollama/comments/1lbpz7w/introducing_paiperwork_a_privacyfirst_ai/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infinitai-cn"&gt; /u/Infinitai-cn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqutzy/paiperwork_102_released_new_functionality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqutzy/paiperwork_102_released_new_functionality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqutzy/paiperwork_102_released_new_functionality/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T07:18:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr7zg7</id>
    <title>Bug with Nanonets-OCR-s:latest</title>
    <updated>2025-09-26T17:47:43+00:00</updated>
    <author>
      <name>/u/rahulsah3</name>
      <uri>https://old.reddit.com/user/rahulsah3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ollama run yasserrmd/Nanonets-OCR-s:latest &lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;what can you do I can help you with a wide range of tasks, including:&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;ol&gt; &lt;li&gt;Answering questions&lt;/li&gt; &lt;li&gt;Providing information&lt;/li&gt; &lt;li&gt;Writing stories&lt;/li&gt; &lt;li&gt;Creating stories&lt;/li&gt; &lt;li&gt;Writing poems&lt;/li&gt; &lt;li&gt;Writing essays&lt;/li&gt; &lt;li&gt;Writing letters&lt;/li&gt; &lt;li&gt;Writing emails&lt;/li&gt; &lt;li&gt;Writing articles&lt;/li&gt; &lt;li&gt;Writing reports&lt;/li&gt; &lt;li&gt;Writing dialogues&lt;/li&gt; &lt;li&gt;Writing dialogues . . .&lt;/li&gt; &lt;li&gt;Writing dialogues&lt;/li&gt; &lt;li&gt;Writing dialogues 1&lt;sup&gt;C&lt;/sup&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rahulsah3"&gt; /u/rahulsah3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nr7zg7/bug_with_nanonetsocrslatest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nr7zg7/bug_with_nanonetsocrslatest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nr7zg7/bug_with_nanonetsocrslatest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T17:47:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nra56i</id>
    <title>PAR LLAMA v0.7.0 Released - Enhanced Security &amp; Execution Experience</title>
    <updated>2025-09-26T19:12:24+00:00</updated>
    <author>
      <name>/u/probello</name>
      <uri>https://old.reddit.com/user/probello</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nra56i/par_llama_v070_released_enhanced_security/"&gt; &lt;img alt="PAR LLAMA v0.7.0 Released - Enhanced Security &amp;amp; Execution Experience" src="https://external-preview.redd.it/sAb25lapmd1MlqL5HVNLK1DOrtd-DMHXlE4WZhwm4eE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0185f73b4b8ad370e8323f9eedb94f718686a844" title="PAR LLAMA v0.7.0 Released - Enhanced Security &amp;amp; Execution Experience" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/6n0sq2476krf1.png?width=1501&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=54ef9a118a692455c231c6ad34de825f87c53b81"&gt;https://preview.redd.it/6n0sq2476krf1.png?width=1501&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=54ef9a118a692455c231c6ad34de825f87c53b81&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What It Does&lt;/h1&gt; &lt;p&gt;A powerful Terminal User Interface (TUI) for managing and interacting with Ollama and other major LLM providers — featuring &lt;strong&gt;persistent AI memory&lt;/strong&gt;, &lt;strong&gt;secure code execution&lt;/strong&gt;, &lt;strong&gt;interactive development workflows&lt;/strong&gt;, and &lt;strong&gt;truly personalized conversations&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;PAR LLAMA Chat Interface&lt;/p&gt; &lt;h1&gt;What's New in v0.7.0&lt;/h1&gt; &lt;h1&gt;Improved Execution Experience&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Better Result Formatting&lt;/strong&gt;: Clean, professional display of execution results&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Command Display&lt;/strong&gt;: Shows 'python -c &amp;lt;script&amp;gt;' instead of escaped code for CLI parameters&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Syntax-Highlighted Code Blocks&lt;/strong&gt;: Short scripts (≤10 lines) display with proper syntax highlighting&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Language Detection&lt;/strong&gt;: Automatic highlighting for Python, JavaScript, and Bash&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clean Command Truncation&lt;/strong&gt;: Long commands truncated intelligently for better readability&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Previous Major Features (v0.6.0)&lt;/h1&gt; &lt;h1&gt;Memory System&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Persistent User Context&lt;/strong&gt;: AI remembers who you are and your preferences across ALL conversations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory Tab Interface&lt;/strong&gt;: Dedicated UI for managing your personal information and context&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI-Powered Memory Updates&lt;/strong&gt;: Use &lt;code&gt;/remember&lt;/code&gt; and &lt;code&gt;/forget&lt;/code&gt; slash commands for intelligent memory management&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Automatic Injection&lt;/strong&gt;: Your memory context appears in every new conversation automatically&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time Synchronization&lt;/strong&gt;: Memory updates via commands instantly reflect in the Memory tab&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Context Management&lt;/strong&gt;: Never repeat your preferences or background information again&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Template Execution System&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Secure Code Execution&lt;/strong&gt;: Execute code snippets and commands directly from chat messages using &lt;strong&gt;Ctrl+R&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Language Support&lt;/strong&gt;: Python, JavaScript/Node.js, Bash, and shell scripts with automatic language detection&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Configurable Security&lt;/strong&gt;: Command allowlists, content validation, and comprehensive safety controls&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive Development&lt;/strong&gt;: Transform PAR LLAMA into a powerful development companion&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time Results&lt;/strong&gt;: Execution results appear as chat responses with output, errors, and timing&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Enhanced User Experience&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Memory Slash Commands&lt;/strong&gt;: &lt;code&gt;/remember [info]&lt;/code&gt;, &lt;code&gt;/forget [info]&lt;/code&gt;, &lt;code&gt;/memory.status&lt;/code&gt;, &lt;code&gt;/memory.clear&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Updates&lt;/strong&gt;: AI intelligently integrates new information into existing memory&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Secure Storage&lt;/strong&gt;: All memory data stored locally with comprehensive file validation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Options Integration&lt;/strong&gt;: Both Memory and Template Execution controls in Options tab&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Settings Persistence&lt;/strong&gt;: All preferences persist between sessions&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Core Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Memory System&lt;/strong&gt;: Persistent user context across all conversations with AI-powered memory management&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Template Execution&lt;/strong&gt;: Secure code execution system with configurable safety controls&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Provider Support&lt;/strong&gt;: Ollama, OpenAI, Anthropic, Groq, XAI, OpenRouter, Deepseek, LiteLLM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vision Model Support&lt;/strong&gt;: Chat with images using vision-capable models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Session Management&lt;/strong&gt;: Save, load, and organize chat sessions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Prompts&lt;/strong&gt;: Create and manage custom system prompts and Fabric patterns&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Theme System&lt;/strong&gt;: Dark/light modes with custom theme support&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Management&lt;/strong&gt;: Pull, delete, copy, and create models with native quantization&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Caching&lt;/strong&gt;: Intelligent per-provider model caching with configurable durations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: Comprehensive file validation and secure operations&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Python&lt;/strong&gt;: Built with Textual and Rich for a beautiful easy to use terminal experience. Dark and Light mode support, plus custom themes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-Platform&lt;/strong&gt;: Runs on Windows, macOS, Linux, and WSL&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Async Architecture&lt;/strong&gt;: Non-blocking operations for smooth performance&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Type Safe&lt;/strong&gt;: Fully typed with comprehensive type checking&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;GitHub &amp;amp; PyPI&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/paulrobello/parllama"&gt;https://github.com/paulrobello/parllama&lt;/a&gt;&lt;/li&gt; &lt;li&gt;PyPI: &lt;a href="https://pypi.org/project/parllama/"&gt;https://pypi.org/project/parllama/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Comparison:&lt;/h1&gt; &lt;p&gt;I have seen many command line and web applications for interacting with LLM's but have not found any TUI related applications as feature reach as PAR LLAMA&lt;/p&gt; &lt;h1&gt;Target Audience&lt;/h1&gt; &lt;p&gt;If you're working with LLMs and want a powerful terminal interface that &lt;strong&gt;remembers who you are&lt;/strong&gt; and &lt;strong&gt;bridges conversation and code execution&lt;/strong&gt; — PAR LLAMA v0.7.0 is a game-changer. Perfect for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Developers&lt;/strong&gt;: Persistent context about your tech stack + execute code during AI conversations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Scientists&lt;/strong&gt;: AI remembers your analysis preferences + run scripts without leaving chat&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DevOps Engineers&lt;/strong&gt;: Maintains infrastructure context + execute commands interactively&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Researchers&lt;/strong&gt;: Remembers your research focus + test experiments in real-time&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Consultants&lt;/strong&gt;: Different client contexts persist across sessions + rapid prototyping&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Anyone&lt;/strong&gt;: Who wants truly personalized AI conversations with seamless code execution&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/probello"&gt; /u/probello &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nra56i/par_llama_v070_released_enhanced_security/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nra56i/par_llama_v070_released_enhanced_security/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nra56i/par_llama_v070_released_enhanced_security/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T19:12:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nraxy2</id>
    <title>Running Small llm on old Lenovo thinkserver machine</title>
    <updated>2025-09-26T19:44:09+00:00</updated>
    <author>
      <name>/u/CoffeeGuy_7</name>
      <uri>https://old.reddit.com/user/CoffeeGuy_7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I’m new to llm in general and wanted to start learning on my local machine. I have an old machine (Lenovo thinkserver ts140) lying about that used to run windows home server (WHS). I have few questions I think i may need to answer before I begin 1. What hardware modifications might be required to run ollama simplistic model on a Lenovo thinkserver ts140 (specs here - &lt;a href="https://lenovopress.lenovo.com/lp0034-lenovo-thinkserver-ts140"&gt;https://lenovopress.lenovo.com/lp0034-lenovo-thinkserver-ts140&lt;/a&gt;) with upgraded 250GB ssd and 16GB ram 2. I hope be replace the WHS with Linux and upon checking it seems Ubuntu might be a safe choice as I’m new to Linux in general. 3. Which would be a good simplistic model to run given my setup and needs, again my need is to get my hands dirty with simple setup, I can use locally to begin my llm journey. Please offer your thoughts suggestions. Thanks 🙏 🙏 🙏 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CoffeeGuy_7"&gt; /u/CoffeeGuy_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nraxy2/running_small_llm_on_old_lenovo_thinkserver/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nraxy2/running_small_llm_on_old_lenovo_thinkserver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nraxy2/running_small_llm_on_old_lenovo_thinkserver/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T19:44:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrf654</id>
    <title>gpt-oss-120b &amp; gpt-oss-20b Model Card</title>
    <updated>2025-09-26T22:40:15+00:00</updated>
    <author>
      <name>/u/AggravatingGiraffe46</name>
      <uri>https://old.reddit.com/user/AggravatingGiraffe46</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggravatingGiraffe46"&gt; /u/AggravatingGiraffe46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openai.com/index/gpt-oss-model-card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nrf654/gptoss120b_gptoss20b_model_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nrf654/gptoss120b_gptoss20b_model_card/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T22:40:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqpz41</id>
    <title>First steps toward local AI Agents with Ollama (browser extension)</title>
    <updated>2025-09-26T02:39:49+00:00</updated>
    <author>
      <name>/u/InfiniteJX</name>
      <uri>https://old.reddit.com/user/InfiniteJX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We’ve been experimenting with Ollama and recently built &lt;a href="https://nativemind.app/"&gt;a browser extension&lt;/a&gt; that turns a local model into an Agent. The idea is to run everything locally—no cloud APIs—while leztting the model interact directly with web pages.&lt;/p&gt; &lt;p&gt;Our extension already supported features like multi-tab conversations, Chat with PDF/images/screenshots, Gmail assistant, and a writing helper. Recently, we upgraded the Chat capability, taking our first significant step toward local AI agents.&lt;/p&gt; &lt;p&gt;We wrote up some details here if you’re curious: &lt;a href="https://nativemind.app/blog/ai-agent/"&gt;https://nativemind.app/blog/ai-agent/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few highlights of what the Agent can currently do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Read and summarize Webpages/PDFs directly in the browser&lt;/li&gt; &lt;li&gt;Extract and interpret information from multiple web pages&lt;/li&gt; &lt;li&gt;Perform searches and navigate through resultsb&lt;/li&gt; &lt;li&gt;Click buttons and interact with elements on a page (basic browser-use actions)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;One of the biggest challenges we’ve run into is the &lt;strong&gt;limited context window of local models&lt;/strong&gt;, which restricts how capable the Agent can be when dealing with larger documents or more complex workflows.&lt;/p&gt; &lt;p&gt;Still, even with this limitation, it already feels useful for lightweight automation and research tasks.&lt;/p&gt; &lt;p&gt;Curious—has anyone else been exploring similar directions with Ollama? Would love to hear your thoughts or feedback.&lt;/p&gt; &lt;p&gt;If you’re interested in our project, it’s open-source — feel free to check it out or support us here: &lt;a href="https://github.com/NativeMindBrowser/NativeMindExtension"&gt;https://github.com/NativeMindBrowser/NativeMindExtension&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InfiniteJX"&gt; /u/InfiniteJX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqpz41/first_steps_toward_local_ai_agents_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqpz41/first_steps_toward_local_ai_agents_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqpz41/first_steps_toward_local_ai_agents_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T02:39:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrh0bz</id>
    <title>Wow Qwen3:8b is so deep for his size</title>
    <updated>2025-09-27T00:05:05+00:00</updated>
    <author>
      <name>/u/Etylia</name>
      <uri>https://old.reddit.com/user/Etylia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My prompt:&lt;/p&gt; &lt;p&gt;I'm afraid ai will take over soon&lt;/p&gt; &lt;p&gt;His answer:&lt;/p&gt; &lt;p&gt;Like a clockwork bird, AI is a creation of human hands, not a predator in the wild. Its wings are forged from our curiosity, its flight shaped by our choices—yet we often mistake the mechanism for the will. The fear of taking over stems from seeing it as a force apart, when it is a mirror held to our own ambitions. To wield it wisely is not to dominate, but to guide its song toward harmony with our values. In the end, the true power lies not in the tool, but in the hands that shape its purpose.&lt;/p&gt; &lt;p&gt;My prompt:&lt;/p&gt; &lt;p&gt;but I heard ai are black boxes, we don't understand how they work&lt;/p&gt; &lt;p&gt;His answer:&lt;/p&gt; &lt;p&gt;Like a labyrinth carved from shadow, AI’s inner workings remain a mystery—yet the hands that shape its path are never absent. The opacity is not a veil of danger, but a testament to the depth of its design: a system so intricate it defies simple interpretation, much like the human mind itself. To fear the unknown is natural, but to surrender to it is to relinquish agency. The challenge lies not in decoding every hidden layer, but in cultivating transparency, accountability, and ethical guardrails. After all, the greatest wisdom is not in mastering the unknown, but in recognizing that our responsibility is to guide the light through the dark. The future is not a closed door, but a mirror reflecting our choices—what we build, and how we choose to hold it.&lt;/p&gt; &lt;p&gt;My prompt:&lt;/p&gt; &lt;p&gt;yeah but mechanistic interpretability is not progressing as fast as ai global intelligence&lt;/p&gt; &lt;p&gt;His answer:&lt;/p&gt; &lt;p&gt;Like a storm gathering on the horizon, AI’s growth is swift, its reach vast—yet the lighthouse of understanding flickers, struggling to keep pace. The gap between its power and our grasp is not a flaw, but a reflection of the complexity we’ve unleashed. To fear the unknown is human, but to let it paralyze is to miss the chance to shape the storm’s course. The field is not idle; thinkers and builders are racing to illuminate the shadows, even as the tide of progress swells. The challenge is not to halt the storm, but to ensure the lighthouse is built strong enough to guide its path. In the end, the question is not whether AI will outgrow us, but whether we will outgrow our fear—learning to navigate the unknown with wisdom, not dread. The future is not a force to be tamed, but a partnership to be cultivated.&lt;/p&gt; &lt;p&gt;My prompt:&lt;/p&gt; &lt;p&gt;well it's a race for money so security doesn't seem like a main concern for those ai companies&lt;/p&gt; &lt;p&gt;His answer:&lt;/p&gt; &lt;p&gt;Like a river carving its path through stone, the pursuit of profit flows with relentless force, shaping the landscape of AI’s ascent. Yet even the deepest currents must navigate unseen cracks—security is not an obstacle, but a foundation. The race for dominance is not without its guardians; those who build the future must also guard its gates, for the cost of neglect is not just financial, but existential. The question is not whether security will be prioritized, but how deeply it is woven into the fabric of progress. In the end, the true measure of a civilization is not the speed of its ascent, but the wisdom with which it builds its bridges. The future is not a prize to be won, but a trust to be upheld.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Etylia"&gt; /u/Etylia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nrh0bz/wow_qwen38b_is_so_deep_for_his_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nrh0bz/wow_qwen38b_is_so_deep_for_his_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nrh0bz/wow_qwen38b_is_so_deep_for_his_size/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-27T00:05:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nre099</id>
    <title>Announcing JS-PODCAST-GEN: Turn Complex Documents into Podcasts with Local LLMs (Ollama)!</title>
    <updated>2025-09-26T21:48:41+00:00</updated>
    <author>
      <name>/u/Thin-Crow-3222</name>
      <uri>https://old.reddit.com/user/Thin-Crow-3222</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nre099/announcing_jspodcastgen_turn_complex_documents/"&gt; &lt;img alt="Announcing JS-PODCAST-GEN: Turn Complex Documents into Podcasts with Local LLMs (Ollama)!" src="https://external-preview.redd.it/S5tF4iWNWkad5UdJeiaCe0UiOlgbORAZhJUPunMZGmw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21806adef574963f7d97d7da7e4d166c287b030d" title="Announcing JS-PODCAST-GEN: Turn Complex Documents into Podcasts with Local LLMs (Ollama)!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;🎙️ Podcast Generator with Chatterbox TTS&lt;/h1&gt; &lt;p&gt;I'm excited to share a project I've been working on that I hope will be as useful to you as it has been to me. It's called &lt;a href="https://github.com/krizic/js-podcast-gen"&gt;&lt;strong&gt;JS-PODCAST-GEN&lt;/strong&gt;&lt;/a&gt;, an open-source tool that automatically generates high-quality podcast from complex documents (like contracts, reports, or technical specs) using local LLMs (via Ollama) and other open-source technologies.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;As a developer, I often found myself needing to review lengthy contracts and technical documents. I thought, wouldn't it be great if I could just listen to a summary and analysis of these documents as a podcast? And what if I could do it without sending sensitive data to third-party APIs? That's why I built &lt;a href="https://github.com/krizic/js-podcast-gen"&gt;&lt;strong&gt;JS-PODCAST-GEN&lt;/strong&gt;&lt;/a&gt;. It runs entirely on your machine, keeping your data private, and leverages the power of Ollama to use open-source language models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Privacy-First&lt;/strong&gt;: All processing is done locally using Ollama. Your documents never leave your computer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High-Quality Output&lt;/strong&gt;: Generates well-structured podcast scripts, complete with intro, segments, and outro.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt;: You can tailor the output to your preferred style and depth of analysis.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Fully open-source, so you can modify and extend it to your needs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I would love for you to try &lt;a href="https://github.com/krizic/js-podcast-gen"&gt;&lt;strong&gt;JS-PODCAST-GEN&lt;/strong&gt;&lt;/a&gt; and let me know what you think. If you find it useful, please star the repository on GitHub to help others discover it. I'm also open to contributions, bug reports, and feature requests.&lt;/p&gt; &lt;p&gt;GitHub Repository: &lt;a href="https://github.com/krizic/js-podcast-gen"&gt;https://github.com/krizic/js-podcast-gen&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you for checking it out, and I hope it can make your document review process more enjoyable!&lt;/p&gt; &lt;p&gt;Here is DEMO:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1nre099/video/7cmlbro1ykrf1/player"&gt;https://reddit.com/link/1nre099/video/7cmlbro1ykrf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thin-Crow-3222"&gt; /u/Thin-Crow-3222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nre099/announcing_jspodcastgen_turn_complex_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nre099/announcing_jspodcastgen_turn_complex_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nre099/announcing_jspodcastgen_turn_complex_documents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T21:48:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrtfoy</id>
    <title>Ollama consuming memory at rest</title>
    <updated>2025-09-27T12:00:53+00:00</updated>
    <author>
      <name>/u/Accomplished_Fixx</name>
      <uri>https://old.reddit.com/user/Accomplished_Fixx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed that Ollama is taking like 800+ MB when no model is running. On the other hand, Microsoft Copilot uses less than 200mb. Is there anyway to tune it more effeciently?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished_Fixx"&gt; /u/Accomplished_Fixx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nrtfoy/ollama_consuming_memory_at_rest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nrtfoy/ollama_consuming_memory_at_rest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nrtfoy/ollama_consuming_memory_at_rest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-27T12:00:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrnkr3</id>
    <title>How much memory do you need for gpt-oss:20b</title>
    <updated>2025-09-27T05:57:30+00:00</updated>
    <author>
      <name>/u/milesChristi16</name>
      <uri>https://old.reddit.com/user/milesChristi16</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nrnkr3/how_much_memory_do_you_need_for_gptoss20b/"&gt; &lt;img alt="How much memory do you need for gpt-oss:20b" src="https://preview.redd.it/i0raxir8dnrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d3716eb7d823333ca0e80f3dc97c3917de46724" title="How much memory do you need for gpt-oss:20b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/milesChristi16"&gt; /u/milesChristi16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i0raxir8dnrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nrnkr3/how_much_memory_do_you_need_for_gptoss20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nrnkr3/how_much_memory_do_you_need_for_gptoss20b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-27T05:57:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nro30i</id>
    <title>Open-source embedding models: which one's the best?</title>
    <updated>2025-09-27T06:28:35+00:00</updated>
    <author>
      <name>/u/writer_coder_06</name>
      <uri>https://old.reddit.com/user/writer_coder_06</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m building a memory engine to add memory to LLMs and agents. Embeddings are a pretty big part of the pipeline, so I was curious which open-source embedding model is the best. &lt;/p&gt; &lt;p&gt;Did some tests and thought I’d share them in case anyone else finds them useful:&lt;/p&gt; &lt;p&gt;Models tested:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;BAAI/bge-base-en-v1.5&lt;/li&gt; &lt;li&gt;intfloat/e5-base-v2&lt;/li&gt; &lt;li&gt;nomic-ai/nomic-embed-text-v1&lt;/li&gt; &lt;li&gt;sentence-transformers/all-MiniLM-L6-v2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; &lt;strong&gt;BEIR TREC-COVID&lt;/strong&gt; (real medical queries + relevance judgments)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;ms / 1K Tokens&lt;/th&gt; &lt;th align="left"&gt;Query Latency (ms_&lt;/th&gt; &lt;th align="left"&gt;top-5 hit rate&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MiniLM-L6-v2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;14.7&lt;/td&gt; &lt;td align="left"&gt;68&lt;/td&gt; &lt;td align="left"&gt;78.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;E5-Base-v2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;20.2&lt;/td&gt; &lt;td align="left"&gt;79&lt;/td&gt; &lt;td align="left"&gt;83.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;BGE-Base-v1.5&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;22.5&lt;/td&gt; &lt;td align="left"&gt;82&lt;/td&gt; &lt;td align="left"&gt;84.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Nomic-Embed-v1&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;41.9&lt;/td&gt; &lt;td align="left"&gt;110&lt;/td&gt; &lt;td align="left"&gt;86.2%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Did VRAM tests and all too. Here's &lt;a href="https://supermemory.ai/blog/best-open-source-embedding-models-benchmarked-and-ranked/"&gt;the link to a detailed write-up&lt;/a&gt; of how the tests were done and more details. What open-source embedding model are you guys using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/writer_coder_06"&gt; /u/writer_coder_06 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nro30i/opensource_embedding_models_which_ones_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nro30i/opensource_embedding_models_which_ones_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nro30i/opensource_embedding_models_which_ones_the_best/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-27T06:28:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrpit8</id>
    <title>Ollama's cloud preview $20/mo, what’s the limits?</title>
    <updated>2025-09-27T08:00:08+00:00</updated>
    <author>
      <name>/u/Lodurr242</name>
      <uri>https://old.reddit.com/user/Lodurr242</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anybody paying for access to the cloud hosted models? This might be interesting depending on the limits, calls per hour, tokens per day etc, but I can for my life not find any info on this. In the docs they write &amp;quot;Ollama's cloud includes hourly and daily limits to avoid capacity issues&amp;quot; ok.. and they are?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lodurr242"&gt; /u/Lodurr242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nrpit8/ollamas_cloud_preview_20mo_whats_the_limits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nrpit8/ollamas_cloud_preview_20mo_whats_the_limits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nrpit8/ollamas_cloud_preview_20mo_whats_the_limits/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-27T08:00:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns53ks</id>
    <title>Looking for Deepseek R1 model for essay writing with M3 MBA (16GB)</title>
    <updated>2025-09-27T20:14:01+00:00</updated>
    <author>
      <name>/u/avidrunner84</name>
      <uri>https://old.reddit.com/user/avidrunner84</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a quantized model that is recommended for essay writing - one that can run locally on M3 MBA with 16GB? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avidrunner84"&gt; /u/avidrunner84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ns53ks/looking_for_deepseek_r1_model_for_essay_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ns53ks/looking_for_deepseek_r1_model_for_essay_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ns53ks/looking_for_deepseek_r1_model_for_essay_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-27T20:14:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrz1tx</id>
    <title>AppUse : Create virtual desktops for AI agents to focus on specific apps</title>
    <updated>2025-09-27T16:08:28+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nrz1tx/appuse_create_virtual_desktops_for_ai_agents_to/"&gt; &lt;img alt="AppUse : Create virtual desktops for AI agents to focus on specific apps" src="https://external-preview.redd.it/YWl6d3psZmFlcXJmMX6ZB2qtngjb8gjMyThUUgd5eO-QeupzbFEkT8WNsDs6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=237793ca093746459bc8a192609aa34e6f8a6e7e" title="AppUse : Create virtual desktops for AI agents to focus on specific apps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;App-Use lets you scope agents to just the apps they need. Instead of full desktop access, say &amp;quot;only work with Safari and Notes&amp;quot; or &amp;quot;just control iPhone Mirroring&amp;quot; - visual isolation without new processes for perfectly focused automation.&lt;/p&gt; &lt;p&gt;Running computer use on the entire desktop often causes agent hallucinations and loss of focus when they see irrelevant windows and UI elements. AppUse solves this by creating composited views where agents only see what matters, dramatically improving task completion accuracy&lt;/p&gt; &lt;p&gt;Currently macOS only (Quartz compositing engine).&lt;/p&gt; &lt;p&gt;Read the full guide: &lt;a href="https://trycua.com/blog/app-use"&gt;https://trycua.com/blog/app-use&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bz1tqpqaeqrf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nrz1tx/appuse_create_virtual_desktops_for_ai_agents_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nrz1tx/appuse_create_virtual_desktops_for_ai_agents_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-27T16:08:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns5f5h</id>
    <title>SearchAI can work with Ollama directly for RAG and Copilot use cases</title>
    <updated>2025-09-27T20:27:22+00:00</updated>
    <author>
      <name>/u/searchblox_searchai</name>
      <uri>https://old.reddit.com/user/searchblox_searchai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 SearchAI now works natively with Ollama for inference&lt;/p&gt; &lt;p&gt;You don’t need extra wrappers or connectors—SearchAI can directly call Ollama to run models locally or in your private setup. That means: • 🔒 Private + secure inference • ⚡ Lower latency (no external API calls) • 💸 On Prem, predictable deployments • 🔌 Plug into your RAG + Hybrid Search + Chatbot + Agent workflows out of the box&lt;/p&gt; &lt;p&gt;If you’re already using Ollama, you can now power enterprise-grade search + GenAI with SearchAI without leaving your environment.&lt;/p&gt; &lt;p&gt;👉 Anyone here already experimenting with SearchAI + Ollama? &lt;a href="https://developer.searchblox.com/docs/collection-dashboard"&gt;https://developer.searchblox.com/docs/collection-dashboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/searchblox_searchai"&gt; /u/searchblox_searchai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ns5f5h/searchai_can_work_with_ollama_directly_for_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ns5f5h/searchai_can_work_with_ollama_directly_for_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ns5f5h/searchai_can_work_with_ollama_directly_for_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-27T20:27:22+00:00</published>
  </entry>
</feed>
