<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-09T11:58:49+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qyk6p2</id>
    <title>Suggestions for agentic framework?</title>
    <updated>2026-02-07T17:44:12+00:00</updated>
    <author>
      <name>/u/redditor100101011101</name>
      <uri>https://old.reddit.com/user/redditor100101011101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm a sysadmin with a decent home lab, and I‚Äôm dabbling in local agentic stuff. Trying to decide which agentic framework would fit my use the best. &lt;/p&gt; &lt;p&gt;I‚Äôm using ollama as a llm runner. Most of my home infra is Infra as Code, using terraform and ansible. &lt;/p&gt; &lt;p&gt;I‚Äôd like to make agents to act as technicians. Maybe one that can use terraform. Another that can be my ansible agent, etc. &lt;/p&gt; &lt;p&gt;Leaning toward CrewAI but there‚Äôs so many options. Kinda lost haha. &lt;/p&gt; &lt;p&gt;I currently have all my lab configs for tf, ansible, docker, scripts in a git repo. Would be nice if the agents could also be defined in my repo so it‚Äôs all together. &lt;/p&gt; &lt;p&gt;Thoughts? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redditor100101011101"&gt; /u/redditor100101011101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyk6p2/suggestions_for_agentic_framework/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyk6p2/suggestions_for_agentic_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyk6p2/suggestions_for_agentic_framework/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T17:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyn1tc</id>
    <title>Help: Qwen 2.5 Coder 7B stuck on JSON responses (Function Calling) in OpenClaw</title>
    <updated>2026-02-07T19:32:16+00:00</updated>
    <author>
      <name>/u/Odin_261121</name>
      <uri>https://old.reddit.com/user/Odin_261121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Report Content:&lt;/p&gt; &lt;p&gt;System Environment:&lt;/p&gt; &lt;p&gt;‚Ä¢ Operating System: Ubuntu 24.04 running on a Dell G15 5520 laptop.&lt;/p&gt; &lt;p&gt;‚Ä¢ Hardware: NVIDIA RTX 3050 Ti GPU with 4GB of VRAM.&lt;/p&gt; &lt;p&gt;‚Ä¢ AI: Ollama (Local).&lt;/p&gt; &lt;p&gt;‚Ä¢ Model: qwen2.5-coder:7b.&lt;/p&gt; &lt;p&gt;‚Ä¢ Platform: OpenClaw (version 2026.2.6-3).&lt;/p&gt; &lt;p&gt;Problem Description:&lt;/p&gt; &lt;p&gt;I am configuring a custom virtual assistant in Spanish, but the model is unable to maintain a fluid conversation in plain text. Instead, it constantly responds with JSON code structures that invoke internal functions (such as .send, tts, query, or sessions_send).&lt;/p&gt; &lt;p&gt;The model seems to interpret my messages (even simple greetings) as input data to be processed or as function arguments, ignoring the instruction to speak in a human-like and fluent manner.&lt;/p&gt; &lt;p&gt;Tests performed:&lt;/p&gt; &lt;p&gt;‚Ä¢ Configuration Adjustment: I tried adding a systemPrompt to the openclaw.json file to force conversational mode, but the system rejects the key as unrecognized.&lt;/p&gt; &lt;p&gt;‚Ä¢ System Diagnostics: I ran openclaw doctor --fix to ensure the integrity of the configuration file, but the JSON response loop persists.&lt;/p&gt; &lt;p&gt;‚Ä¢ Workspace Instructions: I created an instructions.md file in the working folder defining the agent as a human virtual assistant, but the model continues to prioritize the execution of technical tools.&lt;/p&gt; &lt;p&gt;‚Ä¢ Plugin Disabling: I disabled external channels like Telegram in the JSON file to limit the available functions, but the model continues to try to &amp;quot;call&amp;quot; non-existent functions.&lt;/p&gt; &lt;p&gt;Question for the community:&lt;/p&gt; &lt;p&gt;Is there any way to completely disable &amp;quot;Function Calling&amp;quot; or Native Skills in OpenClaw? I need this model (especially since it's from the Coder family) to ignore the tool schema and simply respond with conversational text.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odin_261121"&gt; /u/Odin_261121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyn1tc/help_qwen_25_coder_7b_stuck_on_json_responses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyn1tc/help_qwen_25_coder_7b_stuck_on_json_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyn1tc/help_qwen_25_coder_7b_stuck_on_json_responses/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T19:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyqvng</id>
    <title>Track Pro Usage</title>
    <updated>2026-02-07T22:04:22+00:00</updated>
    <author>
      <name>/u/booknerdcarp</name>
      <uri>https://old.reddit.com/user/booknerdcarp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there an app (apart from the web page) that you can use that will help track Pro cloud usage?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/booknerdcarp"&gt; /u/booknerdcarp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyqvng/track_pro_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyqvng/track_pro_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyqvng/track_pro_usage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T22:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qypkuo</id>
    <title>Help me chose Hardware and Setup</title>
    <updated>2026-02-07T21:12:18+00:00</updated>
    <author>
      <name>/u/Badincomputer</name>
      <uri>https://old.reddit.com/user/Badincomputer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wan to start running ai models for text generation and image generation. I have motherboard Asrock x99 ws, lenovo thinkstation p710 xeon e5 v4 cpu and lenovo thinkstation p920 with xeon silver cpu. I have 5-6 titan x gpus too. Ram is not an issue for me i have whole stash of 32 and 64 gb ddr4 rams. &lt;/p&gt; &lt;p&gt;I do not want to buy any other hardware at the moment. &lt;/p&gt; &lt;p&gt;What kind of setup with what config should i setup and how. Any guide or suggested will help. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Badincomputer"&gt; /u/Badincomputer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qypkuo/help_me_chose_hardware_and_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qypkuo/help_me_chose_hardware_and_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qypkuo/help_me_chose_hardware_and_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T21:12:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz6gjo</id>
    <title>Calling engineers &amp; experienced developers to build a privacy-first open-source desktop assistant (posting here cuz this open source is using Ollama local model)</title>
    <updated>2026-02-08T11:19:44+00:00</updated>
    <author>
      <name>/u/No-Mess-8224</name>
      <uri>https://old.reddit.com/user/No-Mess-8224</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qz6gjo/calling_engineers_experienced_developers_to_build/"&gt; &lt;img alt="Calling engineers &amp;amp; experienced developers to build a privacy-first open-source desktop assistant (posting here cuz this open source is using Ollama local model)" src="https://preview.redd.it/vn9ov4vv89ig1.gif?width=320&amp;amp;crop=smart&amp;amp;s=40cdbef551f67c6b682ece5969077cb175f60571" title="Calling engineers &amp;amp; experienced developers to build a privacy-first open-source desktop assistant (posting here cuz this open source is using Ollama local model)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building ZYRON started with a fundamental realization: our computers have become black boxes that constantly leak data to the cloud under the guise of &amp;quot;convenience.&amp;quot; We wanted to return to a model where the user has absolute control. The project is a desktop assistant that allows you to interact with your system using natural language, but without the privacy trade-offs of modern AI. Instead of acting as a generic chatbot, ZYRON uses a local LLM strictly for intent parsing. When you ask about your files, system status, or recent activity, the logic is executed through deterministic, whitelisted system calls on your own hardware. Currently, the assistant can find files by context, monitor system vitals like CPU and RAM, track local activity for productivity insights, and integrate with browsers via local extensions‚Äîall while remaining entirely offline. There is no telemetry, no external logging, and no vendor lock-in. It is designed to be a quiet, background utility that acts as a personal butler for your OS. With parallel Linux support now active alongside the core implementation, the foundation is ready. However, to make this a truly robust tool, we need engineers who enjoy deep systems work. We are looking for contributors who want to solve the challenges of local-first automation: optimizing file indexing without draining battery, refining intent parsing to be 100% reliable, and building secure, clean abstractions for OS-level control. This is an effort to build a technically honest, open-source tool for people who value privacy as a first principle. If you prefer building solid architecture over chasing AI hype, we invite you to explore the repo, audit the security model, and help us define the future of local desktop automation.&lt;/p&gt; &lt;p&gt;Link : &lt;a href="https://github.com/Surajkumar5050/zyron-assistant"&gt;https://github.com/Surajkumar5050/zyron-assistant&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mess-8224"&gt; /u/No-Mess-8224 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vn9ov4vv89ig1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qz6gjo/calling_engineers_experienced_developers_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qz6gjo/calling_engineers_experienced_developers_to_build/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T11:19:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy77nm</id>
    <title>Lorph: A Local AI Chat App with Advanced Web Search via Ollama</title>
    <updated>2026-02-07T07:09:52+00:00</updated>
    <author>
      <name>/u/Fantastic-Market-790</name>
      <uri>https://old.reddit.com/user/Fantastic-Market-790</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qy77nm/lorph_a_local_ai_chat_app_with_advanced_web/"&gt; &lt;img alt="Lorph: A Local AI Chat App with Advanced Web Search via Ollama" src="https://b.thumbs.redditmedia.com/DZDn21aY_3gLhXoXi_LQJoasU074c9lAELkVzules-A.jpg" title="Lorph: A Local AI Chat App with Advanced Web Search via Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Today, I'm sharing the Lorph project with you, an AI chat application designed to run locally on your device, offering a seamless interactive experience with powerful large language models (LLMs) via Ollama.&lt;/p&gt; &lt;p&gt;What truly sets Lorph apart is the advanced and excellent search system I've developed. It's not just about conversation; it extends to highly dynamic and effective web search capabilities, enriching AI responses with up-to-date and relevant information.&lt;/p&gt; &lt;p&gt;If you're looking for a powerful AI tool that operates locally with exceptional search capabilities, Lorph is worth trying.&lt;/p&gt; &lt;p&gt;We welcome any technical feedback, criticism, or collaboration.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/AL-MARID/Lorph.git"&gt;GitHub Project Link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic-Market-790"&gt; /u/Fantastic-Market-790 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qy77nm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qy77nm/lorph_a_local_ai_chat_app_with_advanced_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qy77nm/lorph_a_local_ai_chat_app_with_advanced_web/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T07:09:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyswlv</id>
    <title>TRION update. Create skills, create containers? Yes, he can do that.</title>
    <updated>2026-02-07T23:29:46+00:00</updated>
    <author>
      <name>/u/danny_094</name>
      <uri>https://old.reddit.com/user/danny_094</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qyswlv/trion_update_create_skills_create_containers_yes/"&gt; &lt;img alt="TRION update. Create skills, create containers? Yes, he can do that." src="https://external-preview.redd.it/P35qNqGN13qh30JMtTXkR_QVb_ywCPavn-EbYJOJM-8.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=fc49c4d898fa3d3f0b45a9b60838108336081b8a" title="TRION update. Create skills, create containers? Yes, he can do that." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danny_094"&gt; /u/danny_094 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1qyshzy/trion_update_create_skills_create_containers_yes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyswlv/trion_update_create_skills_create_containers_yes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyswlv/trion_update_create_skills_create_containers_yes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T23:29:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyqbpx</id>
    <title>Advice for LLM choosing and configuration my setup</title>
    <updated>2026-02-07T21:41:47+00:00</updated>
    <author>
      <name>/u/Particular-Idea805</name>
      <uri>https://old.reddit.com/user/Particular-Idea805</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys,&lt;/p&gt; &lt;p&gt;I am pretty new to the AI stuff. My wife uses gemini pro and thinking a lot, I sometimes use it for tutorials like setting up a proxmox host with some services like homeassistant, scrypted, jellyfin and so on...&lt;/p&gt; &lt;p&gt;I have a HP Z2 G9 with an Intel i9 and 96gb ram, rtx 4060 which I have installed proxmox and ollama on. Do you have some advice for a LLM model that fits for my setup? Is it possible to have a voice assistant like gemini?&lt;/p&gt; &lt;p&gt;Thanks a lot for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Particular-Idea805"&gt; /u/Particular-Idea805 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyqbpx/advice_for_llm_choosing_and_configuration_my_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyqbpx/advice_for_llm_choosing_and_configuration_my_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyqbpx/advice_for_llm_choosing_and_configuration_my_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T21:41:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz3mta</id>
    <title>questions about experience with ollama pro</title>
    <updated>2026-02-08T08:28:34+00:00</updated>
    <author>
      <name>/u/Slow_Consequence5401</name>
      <uri>https://old.reddit.com/user/Slow_Consequence5401</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm interested in the Ollama Pro subscription; what are the limits? Do you have any experience with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slow_Consequence5401"&gt; /u/Slow_Consequence5401 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qz3mta/questions_about_experience_with_ollama_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qz3mta/questions_about_experience_with_ollama_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qz3mta/questions_about_experience_with_ollama_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T08:28:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz4pth</id>
    <title>Deterministic Thinking for Probabilistic Minds</title>
    <updated>2026-02-08T09:34:43+00:00</updated>
    <author>
      <name>/u/frank_brsrk</name>
      <uri>https://old.reddit.com/user/frank_brsrk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qz4pth/deterministic_thinking_for_probabilistic_minds/"&gt; &lt;img alt="Deterministic Thinking for Probabilistic Minds" src="https://preview.redd.it/vfqzxws5q8ig1.jpg?width=140&amp;amp;height=51&amp;amp;auto=webp&amp;amp;s=31ff1abe6d6423de2dd79b0bd5cf50bde010f9e1" title="Deterministic Thinking for Probabilistic Minds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;**Working on a passion, which i call &amp;quot;intelligence module&amp;quot; composed of decoupled retrievals, and graph build on the fly, composed only of vectors and code. I am building the Reasoning-as-a-Service.**&lt;/p&gt; &lt;p&gt;*CIM - Causal Intelligence Module&lt;/p&gt; &lt;p&gt;The causal workflow handles a user input , analyzes the query, and recognizes which is the most likely steering pattern for the type of causal reasoning style, the aggregator snipes down the highest in confidence pattern of query. That done passes the query to 5 specific designed of causal origin namespaces filled with high signal datasets synthetized through and cross frontier AI models.&lt;br /&gt; The retrieval consists into bringing into surface the common sense and biases of causal perception, the causal cognitive procedures, the ability at the prompt level injection for the AI model receiving final output ( causal thinking styles ), causal math methods, and how the causality propagates ( all datasets graph augmented with necessary nodes and adges).&lt;br /&gt; All of this goes through a graph merger and multiple Context Graph Builders, which maps temporal topology, causal DAGs, entities and possibly connecting cross domain data from previous rags, and concluding to novel hypotheses.&lt;br /&gt; The final row, reasons on all connections, validates against anti patterns, it executes the math to prove information are stable, it conducts propagation math, does complete 50 simulations through monte carlo and zooms in the graph in order to dont lose any important sub graph , needed for reasoning incentives. to be continued with complete Audit Trail ( AI compliance) , Reasoning trace mermaid visualization, Execution Logger, and Final LLM Prompt.&lt;/p&gt; &lt;p&gt;sincerely i am really excited about this development of mine, almost at 97%, i am looking to deploy it as an API service, and i will be looking for testers soon, so please come along.&lt;/p&gt; &lt;p&gt;frank :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vfqzxws5q8ig1.jpg?width=1544&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4bad6f66a2d1bea2efb65ad28aea7d276fd87b81"&gt;https://preview.redd.it/vfqzxws5q8ig1.jpg?width=1544&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4bad6f66a2d1bea2efb65ad28aea7d276fd87b81&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frank_brsrk"&gt; /u/frank_brsrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qz4pth/deterministic_thinking_for_probabilistic_minds/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qz4pth/deterministic_thinking_for_probabilistic_minds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qz4pth/deterministic_thinking_for_probabilistic_minds/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T09:34:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyrotc</id>
    <title>Releasing 1.22. 0 of Nanocoder - an update breakdown üî•</title>
    <updated>2026-02-07T22:37:49+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qyrotc/releasing_122_0_of_nanocoder_an_update_breakdown/"&gt; &lt;img alt="Releasing 1.22. 0 of Nanocoder - an update breakdown üî•" src="https://external-preview.redd.it/IFFJsgrmmVsD9q-i5nEflRo3mXp-4vSYGfldoMnuGDA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4266ff125f5b909fec66b3497e1b578b53adf7f" title="Releasing 1.22. 0 of Nanocoder - an update breakdown üî•" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t790s2gjg5ig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyrotc/releasing_122_0_of_nanocoder_an_update_breakdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyrotc/releasing_122_0_of_nanocoder_an_update_breakdown/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T22:37:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz6w8f</id>
    <title>What graphics card models will you be using with an RTX 3060 12GB in 2026?</title>
    <updated>2026-02-08T11:45:24+00:00</updated>
    <author>
      <name>/u/DespeShaha</name>
      <uri>https://old.reddit.com/user/DespeShaha</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DespeShaha"&gt; /u/DespeShaha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1qz6w36/what_models_are_you_running_on_rtx_3060_12gb_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qz6w8f/what_graphics_card_models_will_you_be_using_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qz6w8f/what_graphics_card_models_will_you_be_using_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T11:45:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyghp3</id>
    <title>Ollie | A Friendly, Local-First AI Companion for Ollama</title>
    <updated>2026-02-07T15:21:30+00:00</updated>
    <author>
      <name>/u/MoonXPlayer</name>
      <uri>https://old.reddit.com/user/MoonXPlayer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/"&gt; &lt;img alt="Ollie | A Friendly, Local-First AI Companion for Ollama" src="https://preview.redd.it/fh544zreb3ig1.png?width=140&amp;amp;height=80&amp;amp;auto=webp&amp;amp;s=16a686c3d99dee138869217dabb9a9cd246fe905" title="Ollie | A Friendly, Local-First AI Companion for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm sharing &lt;strong&gt;Ollie&lt;/strong&gt;, a Linux-native, local-first personal AI assistant built on top of &lt;strong&gt;Ollama&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fh544zreb3ig1.png?width=1682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23c108dff77d288035dbc0d1dff64503bcd370dd"&gt;https://preview.redd.it/fh544zreb3ig1.png?width=1682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23c108dff77d288035dbc0d1dff64503bcd370dd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ollie runs entirely on your machine ‚Äî no cloud (I'm considering optional cloud APIs like Anthropic), no tracking, no CLI. It offers a polished desktop experience for chatting with local LLMs, managing models, analyzing files and images, and monitoring system usage in real time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clean chat UI with full Markdown, code, tables, and math&lt;/li&gt; &lt;li&gt;Built-in model management (pull / delete / switch)&lt;/li&gt; &lt;li&gt;Vision + PDF / text file analysis (drag &amp;amp; drop)&lt;/li&gt; &lt;li&gt;AppImage distribution (download &amp;amp; run)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Built with &lt;strong&gt;Tauri v2 (Rust) + React + TypeScript&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Feedback and technical criticism are very welcome.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MedGm/Ollie"&gt;https://github.com/MedGm/Ollie&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoonXPlayer"&gt; /u/MoonXPlayer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T15:21:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz3ykj</id>
    <title>llm-use ‚Äì An Open-Source Framework for Routing and Orchestrating Multi-LLM Agent Workflows</title>
    <updated>2026-02-08T08:48:57+00:00</updated>
    <author>
      <name>/u/Just_Vugg_PolyMCP</name>
      <uri>https://old.reddit.com/user/Just_Vugg_PolyMCP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qz3ykj/llmuse_an_opensource_framework_for_routing_and/"&gt; &lt;img alt="llm-use ‚Äì An Open-Source Framework for Routing and Orchestrating Multi-LLM Agent Workflows" src="https://external-preview.redd.it/wdg8khR5ZOlxHpHG07yHIDTSwn_qqpgJFwZQsc7I-t8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0ee09820ada5531c9ca9f6d61db150ecd49759" title="llm-use ‚Äì An Open-Source Framework for Routing and Orchestrating Multi-LLM Agent Workflows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Vugg_PolyMCP"&gt; /u/Just_Vugg_PolyMCP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/llm-use"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qz3ykj/llmuse_an_opensource_framework_for_routing_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qz3ykj/llmuse_an_opensource_framework_for_routing_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T08:48:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzdhyy</id>
    <title>What hardware is needed to run qwen3-code locally?</title>
    <updated>2026-02-08T16:33:33+00:00</updated>
    <author>
      <name>/u/Either_Address8955</name>
      <uri>https://old.reddit.com/user/Either_Address8955</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Either_Address8955"&gt; /u/Either_Address8955 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzdhyy/what_hardware_is_needed_to_run_qwen3code_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzdhyy/what_hardware_is_needed_to_run_qwen3code_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzdhyy/what_hardware_is_needed_to_run_qwen3code_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T16:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzfjev</id>
    <title>Ollama does not appear to be running on my gpu(s)</title>
    <updated>2026-02-08T17:49:20+00:00</updated>
    <author>
      <name>/u/WitchesSphincter</name>
      <uri>https://old.reddit.com/user/WitchesSphincter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I have recently installed Ollama / OpenWebUI on a proxmox LXC using the script linked below. Everything appears to be functioning as expected, I can connect to Ollama and interact with it but I am fairly certain it is not using my GPU.&lt;/p&gt; &lt;p&gt;On the lxc container I can run nvidia-smi and see the card is loaded just as I do on other containers but no processes are shown. My understanding is that I should see some process in there.&lt;/p&gt; &lt;p&gt;Are there any troubleshooting or configuration guides? I included the output of systemctl status ollama if that helps&lt;/p&gt; &lt;p&gt;&lt;a href="https://community-scripts.github.io/ProxmoxVE/scripts?id=openwebui"&gt;https://community-scripts.github.io/ProxmoxVE/scripts?id=openwebui&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚óè ollama.service - Ollama Service Loaded: loaded (/etc/systemd/system/ollama.service; enabled; preset: enabled) Active: active (running) since Fri 2026-02-06 15:29:40 EST; 1 day 21h ago Invocation: 385660c640f24a069bfd156bb621250a Main PID: 150 (ollama) Tasks: 36 (limit: 173018) Memory: 5.6G (peak: 5.6G) CPU: 43min 14.073s CGroup: /system.slice/ollama.service ‚îú‚îÄ 150 /usr/bin/ollama serve ‚îî‚îÄ6298 /usr/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --port 45509&lt;/p&gt; &lt;p&gt;Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=device.go:245 msg=&amp;quot;model weights&amp;quot; device=CPU size=&amp;quot;4.9 GiB&amp;quot; Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=ggml.go:494 msg=&amp;quot;offloaded 0/37 layers to GPU&amp;quot; Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=device.go:256 msg=&amp;quot;kv cache&amp;quot; device=CPU size=&amp;quot;576.0 MiB&amp;quot; Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=device.go:267 msg=&amp;quot;compute graph&amp;quot; device=CPU size=&amp;quot;100.0 MiB&amp;quot; Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=device.go:272 msg=&amp;quot;total memory&amp;quot; size=&amp;quot;5.5 GiB&amp;quot; Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=sched.go:537 msg=&amp;quot;loaded runners&amp;quot; count=1 Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=server.go:1349 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=server.go:1383 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server loading model&amp;quot; Feb 08 12:39:56 openwebui ollama[150]: time=2026-02-08T12:39:56.843-05:00 level=INFO source=server.go:1387 msg=&amp;quot;llama runner started in 7.02 seconds&amp;quot; Feb 08 12:41:12 openwebui ollama[150]: [GIN] 2026/02/08 - 12:41:12 | 200 | 1m23s | ::1 | POST &amp;quot;/api/chat&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WitchesSphincter"&gt; /u/WitchesSphincter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzfjev/ollama_does_not_appear_to_be_running_on_my_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzfjev/ollama_does_not_appear_to_be_running_on_my_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzfjev/ollama_does_not_appear_to_be_running_on_my_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T17:49:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzg2zw</id>
    <title>What do we have for Ollama that works like Perplexity‚Äôs Comet Browser?</title>
    <updated>2026-02-08T18:09:16+00:00</updated>
    <author>
      <name>/u/Comfortable_Ad_8117</name>
      <uri>https://old.reddit.com/user/Comfortable_Ad_8117</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does something exist that will do a similar job as Perplexity Comet browser? Full interaction with web pages and tabs and run on my local Ollama server? I tried Ask Steve but could never get it to talk to my remote Ollama server (only an Ollama running on the same box as the plugin) I have a pair of 5060‚Äôs that are part of my dedicated Ai sever running Ollama and I want my workstation browser to be able to utilize the remote server. &lt;/p&gt; &lt;p&gt;Anyone have a chrome plugin for this (other than Steve) or even an entire chromium browser like Comet that can plug into my local Ai? &lt;/p&gt; &lt;p&gt;My use case - I buy antiques at auction and I like to ask the Ai if it sees any lots on the page that may be worth my time to look at, then I want it to find comps that may have sold and give me an idea of what my ROI is going to be. I built an app to do this and it works, but having it right in the web browser side bar using perplexity is nice, but would like to give my hardware a chance at it too. &lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Ad_8117"&gt; /u/Comfortable_Ad_8117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzg2zw/what_do_we_have_for_ollama_that_works_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzg2zw/what_do_we_have_for_ollama_that_works_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzg2zw/what_do_we_have_for_ollama_that_works_like/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T18:09:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzip1x</id>
    <title>Give agent read only zcces to my nucls files</title>
    <updated>2026-02-08T19:44:54+00:00</updated>
    <author>
      <name>/u/T-LAD_the_band</name>
      <uri>https://old.reddit.com/user/T-LAD_the_band</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running ollama locally with qwen2.5-coder:7b&lt;/p&gt; &lt;p&gt;Instead of uploading all my configuration.yaml/automations.yaml,... files (docker, home assistant) I would want it to have read only acces to those files so it knows Mt current configs and I can build fro m there &lt;/p&gt; &lt;p&gt;What is the easiest way to do so? I'm running open web on my nuc and ollama is running on my gaming rig.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/T-LAD_the_band"&gt; /u/T-LAD_the_band &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzip1x/give_agent_read_only_zcces_to_my_nucls_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzip1x/give_agent_read_only_zcces_to_my_nucls_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzip1x/give_agent_read_only_zcces_to_my_nucls_files/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T19:44:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzqasm</id>
    <title>What happened to ollama . org? Tried using it after sometim. The URL is redirecting to FreeGPT</title>
    <updated>2026-02-09T01:00:30+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzqasm/what_happened_to_ollama_org_tried_using_it_after/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzqasm/what_happened_to_ollama_org_tried_using_it_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzqasm/what_happened_to_ollama_org_tried_using_it_after/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T01:00:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzttub</id>
    <title>Loading model into RAM before VRM, PROBLEM</title>
    <updated>2026-02-09T03:52:20+00:00</updated>
    <author>
      <name>/u/No_Mango7658</name>
      <uri>https://old.reddit.com/user/No_Mango7658</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Strix Halo 128gb, 96gb dedicated to GPU. When trying to load models larger than 32gb (CPU RAM allocation) I get an error that I don't have enough ram. How do I bypass this RAM check and just load the model into GPU memory?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Mango7658"&gt; /u/No_Mango7658 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzttub/loading_model_into_ram_before_vrm_problem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzttub/loading_model_into_ram_before_vrm_problem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzttub/loading_model_into_ram_before_vrm_problem/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T03:52:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzrwlb</id>
    <title>Recommend model for openclaw clawdbot running locally on old laptop 4gb vram 16g ram asus</title>
    <updated>2026-02-09T02:21:02+00:00</updated>
    <author>
      <name>/u/Flux-of-Time</name>
      <uri>https://old.reddit.com/user/Flux-of-Time</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flux-of-Time"&gt; /u/Flux-of-Time &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzrwlb/recommend_model_for_openclaw_clawdbot_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzrwlb/recommend_model_for_openclaw_clawdbot_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzrwlb/recommend_model_for_openclaw_clawdbot_running/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T02:21:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzh0op</id>
    <title>I created a small AI Agent</title>
    <updated>2026-02-08T18:43:20+00:00</updated>
    <author>
      <name>/u/Rough_Philosopher877</name>
      <uri>https://old.reddit.com/user/Rough_Philosopher877</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys.. I know it's not so big thing.. just for fun I created a Small AI Agent:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/tysonchamp/Small-AI-Agent"&gt;https://github.com/tysonchamp/Small-AI-Agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love the feedback of the community.. and any suggestions of new ideas.&lt;/p&gt; &lt;p&gt;I created this for my day to day activities.. such as setup reminders, take notes, monitor all my client's website (if they are all ok or not).. monitor all my servers, connecting it to my custom erp for due invoice fetching, project management etc &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rough_Philosopher877"&gt; /u/Rough_Philosopher877 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T18:43:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r01i5f</id>
    <title>I need help so I got openclaw</title>
    <updated>2026-02-09T11:12:19+00:00</updated>
    <author>
      <name>/u/MoreLavishness1061</name>
      <uri>https://old.reddit.com/user/MoreLavishness1061</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which is good to run I have a i900k rtx 2080 looking for a good one to run local&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoreLavishness1061"&gt; /u/MoreLavishness1061 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r01i5f/i_need_help_so_i_got_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r01i5f/i_need_help_so_i_got_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r01i5f/i_need_help_so_i_got_openclaw/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T11:12:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzrd8g</id>
    <title>DaveLovable is an open-source, AI-powered web UI/UX development platform</title>
    <updated>2026-02-09T01:54:32+00:00</updated>
    <author>
      <name>/u/LeadingFun1849</name>
      <uri>https://old.reddit.com/user/LeadingFun1849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on this project for a while.&lt;/p&gt; &lt;p&gt;DaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch. It combines cutting-edge AI orchestration with browser-based execution to offer the most advanced open-source alternative for rapid frontend prototyping.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/davidmonterocrespo24/DaveLovable"&gt;https://github.com/davidmonterocrespo24/DaveLovable&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeadingFun1849"&gt; /u/LeadingFun1849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T01:54:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzzd5x</id>
    <title>Qwen 3 coder next for R coding (academic)</title>
    <updated>2026-02-09T09:01:29+00:00</updated>
    <author>
      <name>/u/Bahaal_1981</name>
      <uri>https://old.reddit.com/user/Bahaal_1981</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am an academic. I have claude via work and it excels at R coding and building Shiny apps with little prompting (Opus 4.5 but Sonnet does fairly well also). This is both for teaching / research. But I also want local models (for various reasons, privacy, reproducibility, etc). I have ollama with cohere / Mistral Large / phi reasoning, running on an M4 Max with 128 gb ram. Reading up I think qwen coder next might do better:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/qwen3-coder-next"&gt;https://ollama.com/library/qwen3-coder-next&lt;/a&gt; --&amp;gt; 85GB model -- additional settings needed?&lt;/p&gt; &lt;p&gt;I also looked for Kimi but could only find the cloud version. Any advice? Many thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bahaal_1981"&gt; /u/Bahaal_1981 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T09:01:29+00:00</published>
  </entry>
</feed>
