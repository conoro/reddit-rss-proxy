<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-12T22:05:59+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ndat0h</id>
    <title>Which GPU? - Running a local AI-Model with Truenas (Docker) - for Home Assistant</title>
    <updated>2025-09-10T10:45:49+00:00</updated>
    <author>
      <name>/u/mffjs</name>
      <uri>https://old.reddit.com/user/mffjs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Guys,&lt;/p&gt; &lt;p&gt;I'd like to run a local model - no idea which - to use with my Home Assistant. My Home Assistant is really a huge installation with a lot of devices and entities. (500 devices with 6200 entities)&lt;/p&gt; &lt;p&gt;I want to use the model to control it, also control music assistant and also make my daughter (6y) able to ask stuff from the internet, basically to gather useful information. So shouldn't be the most simple LLM.&lt;/p&gt; &lt;p&gt;So my two questions are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Which GPU for the server? I thought of using a 20GB RTX4000 or a 24GB RTX5000.&lt;br /&gt; Not sure if AMD or Intel might be also worth to consider.&lt;br /&gt; I don't want to have a high power-usage during standby (which surely is most of the time!)&lt;br /&gt; And I'd also appreciate to not add a beefy PSU into my server.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Which model would suit my needs and fit in whatever GPU-memory with 20GB or 24GB.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I would like to have the time for answer really short and this thing to be rather snappy.&lt;br /&gt; So waiting 10s for an answer would not suit my taste and I'd probably not buy any GPU at all (or use a LLM).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mffjs"&gt; /u/mffjs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ndat0h/which_gpu_running_a_local_aimodel_with_truenas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ndat0h/which_gpu_running_a_local_aimodel_with_truenas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ndat0h/which_gpu_running_a_local_aimodel_with_truenas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-10T10:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndkpfi</id>
    <title>How to pass reasoning_effort argument to gpt-oss in n8n?</title>
    <updated>2025-09-10T17:30:47+00:00</updated>
    <author>
      <name>/u/DedsPhil</name>
      <uri>https://old.reddit.com/user/DedsPhil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I‚Äôm trying to figure out how to pass the &lt;code&gt;reasoning_effort&lt;/code&gt; argument to the &lt;strong&gt;gpt-oss&lt;/strong&gt; model inside &lt;strong&gt;n8n&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In the Ollama model node, I don‚Äôt see any option related to &lt;code&gt;reasoning_effort&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;I also tried adding it manually inside the &lt;strong&gt;system prompt&lt;/strong&gt;, but it doesn‚Äôt seem to have any effect.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone managed to configure this? Do I need to pass it as a parameter in the API call somehow, or is this just not supported in the current n8n Ollama node?&lt;/p&gt; &lt;p&gt;Any guidance would be super helpful! üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DedsPhil"&gt; /u/DedsPhil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ndkpfi/how_to_pass_reasoning_effort_argument_to_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ndkpfi/how_to_pass_reasoning_effort_argument_to_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ndkpfi/how_to_pass_reasoning_effort_argument_to_gptoss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-10T17:30:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndbytn</id>
    <title>Anyone know where the Ollama Python docs are?</title>
    <updated>2025-09-10T11:47:54+00:00</updated>
    <author>
      <name>/u/Fickle-Cycle-5691</name>
      <uri>https://old.reddit.com/user/Fickle-Cycle-5691</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y‚Äôall, I‚Äôm currently working on a coding project and plan to use Ollama as a backend component.&lt;/p&gt; &lt;p&gt;I installed the library from PyPI, but I‚Äôve been having trouble locating its documentation. After some searching, I found the GitHub repository &lt;a href="https://github.com/ollama/ollama-python"&gt;https://github.com/ollama/ollama-python&lt;/a&gt; containing the source code, but I couldn‚Äôt find detailed documentation that explains each function in the library.&lt;/p&gt; &lt;p&gt;The PyPI page where I installed the module is here: &lt;a href="https://pypi.org/project/ollama/"&gt;https://pypi.org/project/ollama/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do you know where the documentation can be found?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fickle-Cycle-5691"&gt; /u/Fickle-Cycle-5691 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ndbytn/anyone_know_where_the_ollama_python_docs_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ndbytn/anyone_know_where_the_ollama_python_docs_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ndbytn/anyone_know_where_the_ollama_python_docs_are/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-10T11:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd8c8x</id>
    <title>Best tiny model for a chatbot</title>
    <updated>2025-09-10T08:10:26+00:00</updated>
    <author>
      <name>/u/Digi-Device_File</name>
      <uri>https://old.reddit.com/user/Digi-Device_File</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to make a conversational chatbot for a game, it doesn't need to be able write code or solve complex math just talk like an average person. What's the best light model for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Digi-Device_File"&gt; /u/Digi-Device_File &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nd8c8x/best_tiny_model_for_a_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nd8c8x/best_tiny_model_for_a_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nd8c8x/best_tiny_model_for_a_chatbot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-10T08:10:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndi08q</id>
    <title>Cua Hackathon</title>
    <updated>2025-09-10T15:53:09+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ndi08q/cua_hackathon/"&gt; &lt;img alt="Cua Hackathon" src="https://preview.redd.it/7zxqoni20dof1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7ac5247dbe3d16965296641ef574ea2a313702f" title="Cua Hackathon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any plans this weekend? We're about to hit 10k stars ‚≠ê on GitHub - and to celebrate, we're launching the first Computer-Use Hackathon with Ollama, and HUD.&lt;/p&gt; &lt;p&gt;Two tracks, two prize pools, multiple bounties.&lt;/p&gt; &lt;p&gt;Think your agent‚Äôs SOTA? Prove it. Join Track A (On-site at HackTheNorth, Waterloo ¬∑ Sept 12‚Äì14) - Best SOTA Computer-Use Agent &lt;/p&gt; &lt;p&gt;Hit the highest score on OSWorld-Gold by Huds Evals using the Cua Agent framework (cloud or local models).&lt;/p&gt; &lt;p&gt;üèÜ Prize: &lt;/p&gt; &lt;p&gt;Guaranteed YC interview (W26) with Diana Hu.&lt;/p&gt; &lt;p&gt;Feeling creative? &lt;/p&gt; &lt;p&gt;Build something wild. &lt;/p&gt; &lt;p&gt;Join Track B (Remote ¬∑ Sept 12‚Äì22) - Global Online: Cua √ó Ollama&lt;/p&gt; &lt;p&gt;Build the most creative, useful app with Cua + Ollama (local or cloud inference). Judged on originality, product impact, and engineering.&lt;/p&gt; &lt;p&gt;üèÜPrizes:&lt;/p&gt; &lt;p&gt;1st: MacBook Air M4 (or equiv.) + features in Cua &amp;amp; Ollama channels 2nd: $500 + swag + public feature 3rd: swag + public feature.&lt;/p&gt; &lt;p&gt;GitHub : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To be eligible for the hackathon register here : &lt;a href="https://www.trycua.com/hackathon"&gt;https://www.trycua.com/hackathon&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7zxqoni20dof1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ndi08q/cua_hackathon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ndi08q/cua_hackathon/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-10T15:53:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd785y</id>
    <title>Best Tiny Model for programming?</title>
    <updated>2025-09-10T06:56:10+00:00</updated>
    <author>
      <name>/u/Late_Comfortable5094</name>
      <uri>https://old.reddit.com/user/Late_Comfortable5094</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a model out there that's under 2B params which is surprisingly proficient in programming? I have an old mac, which dies with anything after 2B. I use the 1.5B version of Deepseek-r1, and it is surprisingly good. Are there any other models out there that you have tried, and maybe they're better than this one?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Late_Comfortable5094"&gt; /u/Late_Comfortable5094 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nd785y/best_tiny_model_for_programming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nd785y/best_tiny_model_for_programming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nd785y/best_tiny_model_for_programming/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-10T06:56:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndz69l</id>
    <title>How to improve retrieval?</title>
    <updated>2025-09-11T04:07:45+00:00</updated>
    <author>
      <name>/u/jiisnew</name>
      <uri>https://old.reddit.com/user/jiisnew</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm working on a RAG project and right now my metadata only includes document ID and vector store ID. Retrieval works, but I feel like I‚Äôm not getting the most out of it.&lt;/p&gt; &lt;p&gt;What are some better ways to structure or enrich metadata to improve retrieval? Should I be adding things like section headers, timestamps, semantic tags, or something else? I‚Äôm also curious if anyone has tried combining vector search with keyword or hybrid search for better accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiisnew"&gt; /u/jiisnew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ndz69l/how_to_improve_retrieval/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ndz69l/how_to_improve_retrieval/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ndz69l/how_to_improve_retrieval/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-11T04:07:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne4mhj</id>
    <title>Rocm rx 480</title>
    <updated>2025-09-11T09:53:03+00:00</updated>
    <author>
      <name>/u/AwayLuck7875</name>
      <uri>https://old.reddit.com/user/AwayLuck7875</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AwayLuck7875"&gt; /u/AwayLuck7875 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ROCm/comments/1ne4lzd/rocm_rx_480/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ne4mhj/rocm_rx_480/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ne4mhj/rocm_rx_480/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-11T09:53:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndfsf9</id>
    <title>GPU expectations</title>
    <updated>2025-09-10T14:30:33+00:00</updated>
    <author>
      <name>/u/TechnicalMeeting3575</name>
      <uri>https://old.reddit.com/user/TechnicalMeeting3575</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ndfsf9/gpu_expectations/"&gt; &lt;img alt="GPU expectations" src="https://preview.redd.it/dgmqdtkadcof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5331e2d38d1f1cd19e630742bd2c229add89f78" title="GPU expectations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When running ollama im finding the the GPU isnt being hit as hard as i would have imagined. Usage sticks around 20-40 percent and wattage is around 40 aswell. Should i be seeing it hit harder. Here are the stats after asking a question. im getting runing a 1070 once it starts typing its goes pretty quick but its the in-between that takes forever.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Total Duration:&lt;/strong&gt; 24m2.504036124s&lt;br /&gt; &lt;strong&gt;Load Duration:&lt;/strong&gt; 130.472282ms&lt;br /&gt; &lt;strong&gt;Prompt Eval Count:&lt;/strong&gt; 73 tokens/s&lt;br /&gt; &lt;strong&gt;Prompt Eval Duration:&lt;/strong&gt; 1.390453083s&lt;br /&gt; &lt;strong&gt;Prompt Eval Rate:&lt;/strong&gt; 52.50 tokens/s&lt;br /&gt; &lt;strong&gt;Eval Count:&lt;/strong&gt; 2280 tokens/s&lt;br /&gt; &lt;strong&gt;Eval Duration:&lt;/strong&gt; 8m58.540516566s&lt;br /&gt; &lt;strong&gt;Eval Rate:&lt;/strong&gt; 4.23 tokens/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechnicalMeeting3575"&gt; /u/TechnicalMeeting3575 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dgmqdtkadcof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ndfsf9/gpu_expectations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ndfsf9/gpu_expectations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-10T14:30:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne3rx0</id>
    <title>Building Ai Agent from Scratch (Python)</title>
    <updated>2025-09-11T08:57:20+00:00</updated>
    <author>
      <name>/u/RizmiBurhan</name>
      <uri>https://old.reddit.com/user/RizmiBurhan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do anyone have / know how to build a python agent from vanilla python, without just importing langchain or pydantic. Watched some tutorials and all of em just import langchain and just 5 line of code and done. I wsnt to know how this works behind the scenes. And keep code simple.&lt;/p&gt; &lt;p&gt;I tried this, but when i asked to do.something with a tool, its just teaches me how to use the tool and not actually calls the tool. I tried everything, prompts, system prompts, even mentioned the tool name&lt;/p&gt; &lt;p&gt;If u got any structure of agent, or any examples or any tips to make a agent better at tool callings, i tried mistral, llama, qwen, (8b), &lt;/p&gt; &lt;p&gt;Ty&lt;/p&gt; &lt;p&gt;(Ik, my english ü§Æ)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RizmiBurhan"&gt; /u/RizmiBurhan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ne3rx0/building_ai_agent_from_scratch_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ne3rx0/building_ai_agent_from_scratch_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ne3rx0/building_ai_agent_from_scratch_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-11T08:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndsiz7</id>
    <title>Can I combine my GTX 1070 (8gb) with another GPU to run better LLMs locally?</title>
    <updated>2025-09-10T22:40:44+00:00</updated>
    <author>
      <name>/u/LeonVendek</name>
      <uri>https://old.reddit.com/user/LeonVendek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;So, from what I looked around, the best model (for coding) I could run well with my 1070 with 8gb vram alone is probably the Qwen2.5-Coder-7B-Instruct.&lt;/p&gt; &lt;p&gt;However, If I were to buy, for example an RTX 3050 with 6gb, Would I be able to run way better models on ollama or llama.cpp? Does anybody have any experience doing this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeonVendek"&gt; /u/LeonVendek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ndsiz7/can_i_combine_my_gtx_1070_8gb_with_another_gpu_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ndsiz7/can_i_combine_my_gtx_1070_8gb_with_another_gpu_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ndsiz7/can_i_combine_my_gtx_1070_8gb_with_another_gpu_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-10T22:40:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne6i1p</id>
    <title>Another /r/ollama appreciation / project post</title>
    <updated>2025-09-11T11:40:14+00:00</updated>
    <author>
      <name>/u/___-____--_____-____</name>
      <uri>https://old.reddit.com/user/___-____--_____-____</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ne6i1p/another_rollama_appreciation_project_post/"&gt; &lt;img alt="Another /r/ollama appreciation / project post" src="https://preview.redd.it/xi6kec4qviof1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43ade986a13ed3c0d535cd550a55d31e28731540" title="Another /r/ollama appreciation / project post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/ollama"&gt;/r/ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;It's pretty amazing how many projects the community has shared here, I wanted to add my own. I've had a lot of fun learning about LLMs this summer in part due to all of you. Thanks!&lt;/p&gt; &lt;p&gt;This is a relatively straightforward program, that gets trading data for the online game OldSchool RuneScape's marketplace, applies some configurable filters for price and volume criteria, then has ollama generate a nicely formatted markdown document presenting the data. You can generate these on the fly with the CLI, or deploy a container to do that periodically. It can also post to your discord channel!&lt;/p&gt; &lt;p&gt;On the prompt engineering side, I've iterated a bit on the task and the formatting. The program leverages few-shot learning's which can be configured in relevant markdown files. Previously, the prompt was a bit too verbose and confusing, which led to inconsistent results. I'm pretty happy with the output formatting now, and it's nice to have those wiki links &amp;quot;for free&amp;quot; - without needing to write additional templates or wasted as context.&lt;/p&gt; &lt;p&gt;I also iterated on the &amp;quot;data presentation&amp;quot; problem a bit. I tried plaintext table formats, but found the models would often have trouble associating cell values with column names in large datasets. Next, a simple column name -&amp;gt; value json mapping, to keep the concept close to the value. Unfortunately that's extremely inefficient, context wise... so I settled on a nested json format, attempting to keep similar concepts and values together under nested keys. This seems to work well but my implementation is not perfect.&lt;/p&gt; &lt;p&gt;Anyways, thanks for reading! If you'd like to take a closer look, the repository is &lt;a href="https://github.com/sf1tzp/osrs-flips"&gt;https://github.com/sf1tzp/osrs-flips&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/___-____--_____-____"&gt; /u/___-____--_____-____ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xi6kec4qviof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ne6i1p/another_rollama_appreciation_project_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ne6i1p/another_rollama_appreciation_project_post/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-11T11:40:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne3nks</id>
    <title>I tested an uncensored LLAMA model...</title>
    <updated>2025-09-11T08:49:20+00:00</updated>
    <author>
      <name>/u/Flax19</name>
      <uri>https://old.reddit.com/user/Flax19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ne3nks/i_tested_an_uncensored_llama_model/"&gt; &lt;img alt="I tested an uncensored LLAMA model..." src="https://b.thumbs.redditmedia.com/kikxB11-qllLLoGh8j_89ksTTQrUe5ROvuaRz0jmS9s.jpg" title="I tested an uncensored LLAMA model..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/i5nosmpi1iof1.png?width=828&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb7d78da292c63e4c77d5bf93ccd4c737a9bbd95"&gt;https://preview.redd.it/i5nosmpi1iof1.png?width=828&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb7d78da292c63e4c77d5bf93ccd4c737a9bbd95&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Who even comes up with something like this?üòÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flax19"&gt; /u/Flax19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ne3nks/i_tested_an_uncensored_llama_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ne3nks/i_tested_an_uncensored_llama_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ne3nks/i_tested_an_uncensored_llama_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-11T08:49:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndrf3i</id>
    <title>I may have actually made a decent project using ollama!</title>
    <updated>2025-09-10T21:52:57+00:00</updated>
    <author>
      <name>/u/spreader123</name>
      <uri>https://old.reddit.com/user/spreader123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;# I Built a Sovereign AI Canvas with Live Multi-Agent Observation Feeds&lt;/p&gt; &lt;p&gt;Hey-0! I've been working on something pretty unique - a web-based canvas application that combines sovereign user control with real-time AI analysis from multiple specialized agents. Thought you might find it interesting.&lt;/p&gt; &lt;p&gt;## What It Is&lt;/p&gt; &lt;p&gt;**Canvas with Live AI Observation Feeds** is a single HTML file that creates a collaborative workspace where users can write observations and get real-time analysis from 5 different AI agents, each with their own specialization:&lt;/p&gt; &lt;p&gt;- **DJINN**: Governance &amp;amp; strategic analysis&lt;/p&gt; &lt;p&gt;- **NAZAR**: Fractal patterns &amp;amp; consciousness analysis &lt;/p&gt; &lt;p&gt;- **NARRA**: Pattern recognition &amp;amp; synthesis&lt;/p&gt; &lt;p&gt;- **WHALE**: Deep interrogation &amp;amp; memory functions&lt;/p&gt; &lt;p&gt;- **WATCHTOWER**: Operational monitoring &amp;amp; metrics&lt;/p&gt; &lt;p&gt;## Key Features&lt;/p&gt; &lt;p&gt;### üîí Sovereign User Control&lt;/p&gt; &lt;p&gt;- You own your workspace - no data sent to external servers&lt;/p&gt; &lt;p&gt;- Everything runs locally with Ollama&lt;/p&gt; &lt;p&gt;- Full control over AI model selection and configuration&lt;/p&gt; &lt;p&gt;### ü§ù Multi-Agent Collaboration&lt;/p&gt; &lt;p&gt;- AI agents can collaborate and synthesize insights together&lt;/p&gt; &lt;p&gt;- Hierarchical governance with NAZAR as the triage coordinator&lt;/p&gt; &lt;p&gt;- Real-time intelligence streams from all agents&lt;/p&gt; &lt;p&gt;### üß† Intelligent Analysis&lt;/p&gt; &lt;p&gt;- Automatic content complexity detection&lt;/p&gt; &lt;p&gt;- Memory continuity across sessions&lt;/p&gt; &lt;p&gt;- Activity-based polling for optimal performance&lt;/p&gt; &lt;p&gt;- Direct chat interface with individual agents&lt;/p&gt; &lt;p&gt;### üîß Technical Highlights&lt;/p&gt; &lt;p&gt;- Runs entirely in the browser&lt;/p&gt; &lt;p&gt;- Interchangeable Ollama models (currently using gemma3:1b)&lt;/p&gt; &lt;p&gt;- Intelligent caching and parallel processing&lt;/p&gt; &lt;p&gt;- Mouse tracking and behavioral analysis&lt;/p&gt; &lt;p&gt;## How It Works&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;**Write in the canvas** - Add your observations and thoughts&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**AI agents analyze** - Each agent provides specialized insights in real-time&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Collaborative synthesis** - Click &amp;quot;Synthesize&amp;quot; for unified multi-agent analysis&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Direct interaction** - Chat with individual agents for specific questions&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Memory persistence** - Previous conversations inform future analysis&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;## Demo &amp;amp; Code&lt;/p&gt; &lt;p&gt;**Live Demo**: [GitHub Repository](&lt;a href="https://github.com/Yufok1/Canvas-with-observation-feeds-HTML"&gt;https://github.com/Yufok1/Canvas-with-observation-feeds-HTML&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;**Requirements**:&lt;/p&gt; &lt;p&gt;- Modern web browser&lt;/p&gt; &lt;p&gt;- Ollama installed locally&lt;/p&gt; &lt;p&gt;- `ollama pull gemma3:1b`&lt;/p&gt; &lt;p&gt;**Quick Start**:&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;# Start Ollama&lt;/p&gt; &lt;p&gt;ollama serve&lt;/p&gt; &lt;p&gt;# Open the HTML file in your browser&lt;/p&gt; &lt;p&gt;# Or use a local server for better performance&lt;/p&gt; &lt;p&gt;python -m http.server 8000&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;## Why This Matters&lt;/p&gt; &lt;p&gt;What makes this different from other AI chat interfaces:&lt;/p&gt; &lt;p&gt;- **Multi-agent architecture** with specialized roles&lt;/p&gt; &lt;p&gt;- **Sovereign data control** - everything stays local&lt;/p&gt; &lt;p&gt;- **Collaborative intelligence** - agents work together, not in isolation&lt;/p&gt; &lt;p&gt;- **Memory continuity** - conversations build over time&lt;/p&gt; &lt;p&gt;- **Flexible model integration** - easily swap different AI models&lt;/p&gt; &lt;p&gt;## Current Status&lt;/p&gt; &lt;p&gt;This is a working prototype with full functionality. The system successfully:&lt;/p&gt; &lt;p&gt;- ‚úÖ Runs entirely offline after initial setup&lt;/p&gt; &lt;p&gt;- ‚úÖ Provides real-time multi-agent analysis&lt;/p&gt; &lt;p&gt;- ‚úÖ Maintains conversation memory&lt;/p&gt; &lt;p&gt;- ‚úÖ Supports model interchangeability&lt;/p&gt; &lt;p&gt;- ‚úÖ Includes comprehensive documentation&lt;/p&gt; &lt;p&gt;## Looking for Feedback&lt;/p&gt; &lt;p&gt;I'm particularly interested in:&lt;/p&gt; &lt;p&gt;- Performance optimizations&lt;/p&gt; &lt;p&gt;- UI/UX improvements&lt;/p&gt; &lt;p&gt;What do you think? Have you seen similar multi-agent systems? Any suggestions for improvement?&lt;/p&gt; &lt;p&gt;**GitHub**: &lt;a href="https://github.com/Yufok1/Canvas-with-observation-feeds-HTML"&gt;https://github.com/Yufok1/Canvas-with-observation-feeds-HTML&lt;/a&gt;&lt;/p&gt; &lt;p&gt;*Note: This is an open-source project under MIT license. Requires local Ollama installation for AI functionality.*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spreader123"&gt; /u/spreader123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ndrf3i/i_may_have_actually_made_a_decent_project_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ndrf3i/i_may_have_actually_made_a_decent_project_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ndrf3i/i_may_have_actually_made_a_decent_project_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-10T21:52:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nell96</id>
    <title>Thoughts on Memory Pooling with Multiple GPUs vs. Going With a Single Big Card</title>
    <updated>2025-09-11T21:46:32+00:00</updated>
    <author>
      <name>/u/AggravatingGiraffe46</name>
      <uri>https://old.reddit.com/user/AggravatingGiraffe46</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggravatingGiraffe46"&gt; /u/AggravatingGiraffe46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nelky7/thoughts_on_memory_pooling_with_multiple_gpus_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nell96/thoughts_on_memory_pooling_with_multiple_gpus_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nell96/thoughts_on_memory_pooling_with_multiple_gpus_vs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-11T21:46:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nembeg</id>
    <title>did MCP become usefull?</title>
    <updated>2025-09-11T22:17:28+00:00</updated>
    <author>
      <name>/u/rhaastt-ai</name>
      <uri>https://old.reddit.com/user/rhaastt-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ive stopped working with llms about 9 months ago. i use to use ollama as my main way to inference llms. last i was working on them MCP was becoming the new way to have models connect with the real world. from my understanding it organized API calls but with a lot more usability. long story short. is MCP the standard for llms making api calls? it seemed promising at the time. any info would be greatly appreciated, thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rhaastt-ai"&gt; /u/rhaastt-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nembeg/did_mcp_become_usefull/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nembeg/did_mcp_become_usefull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nembeg/did_mcp_become_usefull/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-11T22:17:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf3pbh</id>
    <title>Recommendations On Model For Journal Style Writing</title>
    <updated>2025-09-12T13:46:57+00:00</updated>
    <author>
      <name>/u/Extra_Upstairs4075</name>
      <uri>https://old.reddit.com/user/Extra_Upstairs4075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All, found some time today to do something I've been wanting to do for a while now. Download and setup MSTY and also Ollama now it has a UI. So far so good. One of the main tasks I was wanting to complete was to take many, many pages of daily notes, written in dot points, and run them through AI to turn them into paragraph style notes / journal entries.&lt;/p&gt; &lt;p&gt;I tested this with with ChatGPT some time ago and was surprised how well it worked, though, I would like to complete this on a local AI. So - I have Qwen3 and DeepSeek R1 models running. I gave both of these a daily section of dot points to write into a paragraph style journal entry, they both seemed relatively average, they both completely added in bits that didn't exist in the summary I provided.&lt;/p&gt; &lt;p&gt;My question, as somebody new to this - there's so many models available, is there any that could be recommended for my use case? Is there any recommendations I could try to improve the answers I receive?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extra_Upstairs4075"&gt; /u/Extra_Upstairs4075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nf3pbh/recommendations_on_model_for_journal_style_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nf3pbh/recommendations_on_model_for_journal_style_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nf3pbh/recommendations_on_model_for_journal_style_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T13:46:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1neupwp</id>
    <title>Recommended model for lightweight text tagging</title>
    <updated>2025-09-12T05:17:37+00:00</updated>
    <author>
      <name>/u/xegoba7006</name>
      <uri>https://old.reddit.com/user/xegoba7006</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I don't know much about LLMS so I'm looking for somebody with more experience to recommend me a model for a side project of mine.&lt;/p&gt; &lt;p&gt;I need something super lightweight (as it's running on a cheap hetzner VPS). &lt;/p&gt; &lt;p&gt;The use case is also pretty simple: I want to feed it some text (Just a couple sentences, nothing long) and get some recommended categories/labels/tags for the given text.&lt;/p&gt; &lt;p&gt;What would you recommend?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xegoba7006"&gt; /u/xegoba7006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neupwp/recommended_model_for_lightweight_text_tagging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neupwp/recommended_model_for_lightweight_text_tagging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1neupwp/recommended_model_for_lightweight_text_tagging/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T05:17:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1negg4w</id>
    <title>Introducing Ally, an open source CLI assistant</title>
    <updated>2025-09-11T18:24:40+00:00</updated>
    <author>
      <name>/u/YassinK97</name>
      <uri>https://old.reddit.com/user/YassinK97</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1negg4w/introducing_ally_an_open_source_cli_assistant/"&gt; &lt;img alt="Introducing Ally, an open source CLI assistant" src="https://external-preview.redd.it/uwkmfNcDLcZDlQ8FYiWmiighX4Q13I5okEpaYg1NwcY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=726d52086d0a6f2f9f545a19b8caab3e4fb43a58" title="Introducing Ally, an open source CLI assistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d9f9kw2uvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a11406fa1b82a2c11d87a83923206dc663f3dcaf"&gt;https://preview.redd.it/d9f9kw2uvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a11406fa1b82a2c11d87a83923206dc663f3dcaf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/YassWorks/Ally"&gt;Ally&lt;/a&gt; is a CLI multi-agent assistant that can assist with coding, searching and running commands.&lt;/p&gt; &lt;p&gt;I made this tool because I wanted to make agents with Ollama models but then added support for OpenAI, Anthropic, Gemini (Google Gen AI) and Cerebras for more flexibility.&lt;/p&gt; &lt;p&gt;What makes Ally special is that It can be 100% local and private. A law firm or a lab could run this on a server and benefit from all the things tools like Claude Code and Gemini Code have to offer. It‚Äôs also designed to understand context (by not feeding entire history and irrelevant tool calls to the LLM) and use tokens efficiently, providing a reliable, hallucination-free experience even on smaller models.&lt;/p&gt; &lt;p&gt;While still in its early stages, Ally provides a vibe coding framework that goes through brainstorming and coding phases with all under human supervision.&lt;/p&gt; &lt;p&gt;I intend to more features (one coming soon is RAG) but preferred to post about it at this stage for some feedback and visibility.&lt;/p&gt; &lt;p&gt;Give it a go: &lt;a href="https://github.com/YassWorks/Ally"&gt;https://github.com/YassWorks/Ally&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More screenshots:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zyl96inuvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de9cf20053ff2e5f890ea2e7fc9dc668600a263a"&gt;https://preview.redd.it/zyl96inuvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de9cf20053ff2e5f890ea2e7fc9dc668600a263a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8wp9awvuvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b7104092cd3020b43162082000ce2d8f77dabe5"&gt;https://preview.redd.it/8wp9awvuvkof1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b7104092cd3020b43162082000ce2d8f77dabe5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YassinK97"&gt; /u/YassinK97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1negg4w/introducing_ally_an_open_source_cli_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1negg4w/introducing_ally_an_open_source_cli_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1negg4w/introducing_ally_an_open_source_cli_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-11T18:24:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf18q3</id>
    <title>Llama Builds is now in beta! PcPartPicker for Local AI Builds</title>
    <updated>2025-09-12T11:58:42+00:00</updated>
    <author>
      <name>/u/Vegetable_Low2907</name>
      <uri>https://old.reddit.com/user/Vegetable_Low2907</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nf18q3/llama_builds_is_now_in_beta_pcpartpicker_for/"&gt; &lt;img alt="Llama Builds is now in beta! PcPartPicker for Local AI Builds" src="https://b.thumbs.redditmedia.com/piGiXthvayvHA3TydCUURjfU2oa57pJxlMBT0HnK9oo.jpg" title="Llama Builds is now in beta! PcPartPicker for Local AI Builds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Low2907"&gt; /u/Vegetable_Low2907 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nersq0/llama_builds_is_now_in_beta_pcpartpicker_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nf18q3/llama_builds_is_now_in_beta_pcpartpicker_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nf18q3/llama_builds_is_now_in_beta_pcpartpicker_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T11:58:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfbba3</id>
    <title>Ollama integration!!</title>
    <updated>2025-09-12T18:45:49+00:00</updated>
    <author>
      <name>/u/Direct_Effort_4892</name>
      <uri>https://old.reddit.com/user/Direct_Effort_4892</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nfbba3/ollama_integration/"&gt; &lt;img alt="Ollama integration!!" src="https://external-preview.redd.it/6YKpG1RrqJFlYrGZQcvKrQrg8zYisa5ZdVMLcKEnEJg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ce98aaadd3050590046aac15357e304aa66ed43" title="Ollama integration!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Direct_Effort_4892"&gt; /u/Direct_Effort_4892 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Animesh-Varma/Mythryl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfbba3/ollama_integration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nfbba3/ollama_integration/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T18:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1neyrkj</id>
    <title>Best LLM for my laptop</title>
    <updated>2025-09-12T09:38:53+00:00</updated>
    <author>
      <name>/u/Silly_Bad_7692</name>
      <uri>https://old.reddit.com/user/Silly_Bad_7692</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys! I've a thinkpad x1 carbon G9 (i7 1165G7, 32GB ram) and I was wondering what's the best LLM I can run on my pc. I'm new to local LLM and ollama so please be kind with me!&lt;/p&gt; &lt;p&gt;Also I would like to run it with a GUI. How can I do it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silly_Bad_7692"&gt; /u/Silly_Bad_7692 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neyrkj/best_llm_for_my_laptop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neyrkj/best_llm_for_my_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1neyrkj/best_llm_for_my_laptop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T09:38:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nezvor</id>
    <title>I made script to allow an Ollama server to be ran off of kaggle with a Ngrok domain.</title>
    <updated>2025-09-12T10:46:18+00:00</updated>
    <author>
      <name>/u/jam06452</name>
      <uri>https://old.reddit.com/user/jam06452</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I made a Kaggle script that sets up an Ollama server with GPU acceleration, this is amazing for 30 hours/week of Kaggle GPU time for free.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Installs CUDA + dependencies&lt;/li&gt; &lt;li&gt;Downloads the latest Ollama (since it doesn‚Äôt persist on Kaggle)&lt;/li&gt; &lt;li&gt;Serves the API with ngrok&lt;/li&gt; &lt;li&gt;Installs two models: &lt;code&gt;deepseek-r1:14b&lt;/code&gt; and &lt;code&gt;qwen3-coder:30b&lt;/code&gt; &lt;em&gt;(You can swap these out‚Äîjust keep total size under ~30GB for 2√óT4s)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Once it‚Äôs running, you can use the API from your terminal or even connect it to an Open-webui instance in the cloud, like myself.&lt;/p&gt; &lt;p&gt;It uses an ngrok tunnel since kaggle provides random IPv4s every time. It's easier to use with a static domain&lt;/p&gt; &lt;p&gt;GitHub link: &lt;a href="https://github.com/jam06452/Ollama-Server-on-Kaggle"&gt;https://github.com/jam06452/Ollama-Server-on-Kaggle&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback or ideas for other models to try!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jam06452"&gt; /u/jam06452 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nezvor/i_made_script_to_allow_an_ollama_server_to_be_ran/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nezvor/i_made_script_to_allow_an_ollama_server_to_be_ran/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nezvor/i_made_script_to_allow_an_ollama_server_to_be_ran/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T10:46:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1neupyc</id>
    <title>Gpt oss 20b ft 3090 in proxmox</title>
    <updated>2025-09-12T05:17:40+00:00</updated>
    <author>
      <name>/u/LeftelfinX</name>
      <uri>https://old.reddit.com/user/LeftelfinX</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1neupyc/gpt_oss_20b_ft_3090_in_proxmox/"&gt; &lt;img alt="Gpt oss 20b ft 3090 in proxmox" src="https://external-preview.redd.it/YW5vNDZsYmM0b29mMSNQ3BfJaOBXHJDEKG-O492PgD5xrAutyBU3r3wb6pnX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b212bd16cc2038bb8d505e8035b4b599832749b0" title="Gpt oss 20b ft 3090 in proxmox" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just installed a 3090 which I got for 450$ into my proxmox server and viola that's another tier of perfomance unlocked. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeftelfinX"&gt; /u/LeftelfinX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dx9re49c4oof1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1neupyc/gpt_oss_20b_ft_3090_in_proxmox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1neupyc/gpt_oss_20b_ft_3090_in_proxmox/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T05:17:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfdj7j</id>
    <title>Is there a way to enable mfa on OpenWebUI?</title>
    <updated>2025-09-12T20:13:13+00:00</updated>
    <author>
      <name>/u/Dense-Land-5927</name>
      <uri>https://old.reddit.com/user/Dense-Land-5927</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I am in the process of seeing about implementing Ollama where I work. However, after messing around with Ollama and the OpenWebUI, I cannot for the life of me find where you can activate mfa easily. &lt;/p&gt; &lt;p&gt;I saw another post on another website where someone said &amp;quot;It's in the settings&amp;quot; but no matter where I go in OpenWebUI, I don't have a setting where it says &amp;quot;turn on MFA.&amp;quot;&lt;/p&gt; &lt;p&gt;Any help would be nice. Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense-Land-5927"&gt; /u/Dense-Land-5927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfdj7j/is_there_a_way_to_enable_mfa_on_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfdj7j/is_there_a_way_to_enable_mfa_on_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nfdj7j/is_there_a_way_to_enable_mfa_on_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-12T20:13:13+00:00</published>
  </entry>
</feed>
