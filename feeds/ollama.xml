<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-11T14:50:48+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1luwnt5</id>
    <title>Ollama models for debugging code</title>
    <updated>2025-07-08T18:42:44+00:00</updated>
    <author>
      <name>/u/uncager</name>
      <uri>https://old.reddit.com/user/uncager</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a fairly small TSQL stored procedure but I noticed I had a bug in it. Before I fixed it, I thought I'd run it by some local ollama models, asking them to find any bugs. I tried:&lt;br /&gt; qwen2.5-coder:14b&lt;br /&gt; deepseek-coder-v2:16b&lt;br /&gt; codellama:13b&lt;br /&gt; sqlcoder:15b&lt;br /&gt; NONE of them caught the bug, although they all babbled about better parameter value checking and error catching and logging and a lot more useless garbage that I didn't ask for. I asked Claude and it pointed out the bug right away. I was really hoping to be able to run AI locally for debugging source code I'd rather not upload to some service for some employee there to get to see. Too soon? Or is there some way now to get Claude-level smarts locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uncager"&gt; /u/uncager &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luwnt5/ollama_models_for_debugging_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luwnt5/ollama_models_for_debugging_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1luwnt5/ollama_models_for_debugging_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T18:42:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lucq0b</id>
    <title>codex-&gt;ollama (airgapped)</title>
    <updated>2025-07-08T02:03:39+00:00</updated>
    <author>
      <name>/u/neurostream</name>
      <uri>https://old.reddit.com/user/neurostream</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lucq0b/codexollama_airgapped/"&gt; &lt;img alt="codex-&amp;gt;ollama (airgapped)" src="https://external-preview.redd.it/R2JhsIcLOAV6dx1FDhG0En51rNtJ_9CXmjw-Xp6cleg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52a869c1d852237aad81466ecf37b39fd9c9cb4e" title="codex-&amp;gt;ollama (airgapped)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;it's been out there that openai's codex cli agent now has support for other providers, and it also works with local ollama.&lt;/p&gt; &lt;p&gt;trying it out was less involved than i thought. there's no OpenAI account settings, bindings, tokens, or registration cookie calls... it just works like any other shell command.&lt;/p&gt; &lt;p&gt;you set the model name (from your &amp;quot;ollama ls&amp;quot; output) and local ollama port with &amp;quot;codex --config&amp;quot; options (see example below).&lt;/p&gt; &lt;p&gt;&lt;em&gt;installing&lt;/em&gt; download the cli for your os/arch (you can brew install codex on macos). i extracted codex-exec-x86_64-unknown-linux-gnu.tar.gz for my ubuntu thinkpad and renamed it &amp;quot;codex&amp;quot;. &lt;/p&gt; &lt;p&gt;same with codex-exec and code-linux-sandbox (not sure if all 3 are required or just the main codex util, but i just put them all in the PATH.&lt;/p&gt; &lt;p&gt;&lt;em&gt;internet access/airgapping&lt;/em&gt;&lt;/p&gt; &lt;p&gt;internet route from the machine running it isn't required. but you might end up using it in an internet workflow where codex might, for example, use curl to trigger a remote webhook or git to push a branch to your remote repo.&lt;/p&gt; &lt;p&gt;&lt;em&gt;example&lt;/em&gt; shell&amp;gt; cd myrepo shell&amp;gt; codex exec --config model_provider=ollama --config model_providers.ollama.base_url=&lt;a href="http://127.0.01:11423/v1"&gt;http://127.0.01:11423/v1&lt;/a&gt; --config model=qwen3:235b-a22b-q8_0 &amp;quot;summarize what this whole code repo is about&amp;quot;&lt;/p&gt; &lt;p&gt;codex will run shell commands from the current folder to figure it out.. like ls, find , cat, and grep. it outputs the response (describing the repo, in this case) to stdout and returns to the shell prompt.&lt;/p&gt; &lt;p&gt;leave off the &amp;quot;exec&amp;quot; to start in terminal UI mode, which can you supervise tasks in continuous context and without scripting. but i think many will find the power for complex projects is in chaining codex runs together with scripts (like piping a codex exec output back into codex, etc).&lt;/p&gt; &lt;p&gt;you can create a -/.codex/config.toml file and move the --config switches there to keep your command line clean. There are more configuration options (like setting the context size) documented in the github repo for codex.&lt;/p&gt; &lt;p&gt;&lt;em&gt;read/write and allowed shell commands&lt;/em&gt; that example above is &amp;quot;read only&amp;quot;, but for read-write look at &amp;quot;codex help&amp;quot; to see the &amp;quot;--dangerously&amp;quot; switch, which overrides all the &lt;em&gt;sandboxing&lt;/em&gt; and &lt;em&gt;approval&lt;/em&gt; policies (the actual configuration topics that switch should bring your attention to for safe use). then, your prompts can make/update/delete files (code, scripts, documentation, etc) and folders and even run other commands.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Tool calling models and MCP&lt;/em&gt; the model you set has to support tool calling, and i also prefer reasoning models - which significantly narrows down the available options for tools+thinking models i'd &amp;quot;ollama pull&amp;quot; for this. but i've only been able to get qwen3 to be consistent. (anyone know how make other tool models get along with codex better? deepseek-r1 sometimes works) &lt;/p&gt; &lt;p&gt;the latest codex releases also supports using codex as an both an mcp server and mcp client - which i don't know how to do yet (help?); but that might stabilize the consistency across different tool-enabled models.&lt;/p&gt; &lt;p&gt;&lt;em&gt;one-off codex runs vs codexes of codexes of codexes&lt;/em&gt; I think working with smaller models locally will mean less &amp;quot;build huge app in one prompt while i sleep&amp;quot; -type of magical experiences rn. So I'm expecting to decompose my projects and workflows with a bunch of smaller codex script modules. i've also never used langchain or langraph, but maybe harnessing codex with those frameworks is where i should look next? &lt;/p&gt; &lt;p&gt;i'm a more of network cable infra monkey irl , so i hope this clicks with those who are coming from where i'm at.&lt;/p&gt; &lt;p&gt;&lt;em&gt;TL;DR&lt;/em&gt; you can run: &lt;/p&gt; &lt;p&gt;&lt;em&gt;codex &amp;quot;summarize the git history of this branch&amp;quot;&lt;/em&gt; &lt;/p&gt; &lt;p&gt;and it works with local ollama tool models without talking to openai by putting &lt;a href="http://127.0.01:11423/v1"&gt;http://127.0.01:11423/v1&lt;/a&gt; and the model name (like qwen3) in the config.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neurostream"&gt; /u/neurostream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/openai/codex/releases"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lucq0b/codexollama_airgapped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lucq0b/codexollama_airgapped/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T02:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1luwmuz</id>
    <title>Nvidia Game Ready &lt;or&gt; Studio Drivers - is one better for LLMs?</title>
    <updated>2025-07-08T18:41:44+00:00</updated>
    <author>
      <name>/u/beedunc</name>
      <uri>https://old.reddit.com/user/beedunc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does it matter which one I'm running regarding speed, etc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beedunc"&gt; /u/beedunc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luwmuz/nvidia_game_ready_or_studio_drivers_is_one_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luwmuz/nvidia_game_ready_or_studio_drivers_is_one_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1luwmuz/nvidia_game_ready_or_studio_drivers_is_one_better/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T18:41:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1luuyej</id>
    <title>Built an easy way to schedule prompts powered by MCP and Ollama using our open source LLM client</title>
    <updated>2025-07-08T17:38:10+00:00</updated>
    <author>
      <name>/u/WalrusVegetable4506</name>
      <uri>https://old.reddit.com/user/WalrusVegetable4506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1luuyej/built_an_easy_way_to_schedule_prompts_powered_by/"&gt; &lt;img alt="Built an easy way to schedule prompts powered by MCP and Ollama using our open source LLM client" src="https://preview.redd.it/2donn13krobf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fac95eb0aa08a701d4006cd7493461fd37d0427" title="Built an easy way to schedule prompts powered by MCP and Ollama using our open source LLM client" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Every time we've shared our project we've gotten awesome feedback from this community so I'm excited to share we added scheduled tasks to Tome.&lt;/p&gt; &lt;p&gt;If you haven't seen my past posts, the tl;dr is Tome is an &lt;a href="https://github.com/runebookai/tome"&gt;open source desktop app&lt;/a&gt; for Mac or Windows that lets you connect local or remote models to MCP servers and chat with them.&lt;/p&gt; &lt;p&gt;As of our latest releases you can now run hourly or daily scheduled tasks, here's some examples from my screenshot (though I'm sure y'all will come up with way better ones :)):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Summarizing top Steam games on sale once per day&lt;/li&gt; &lt;li&gt;Periodically parsing Tome‚Äôs own log files&lt;/li&gt; &lt;li&gt;Checking Best Buy for handheld gaming deals&lt;/li&gt; &lt;li&gt;Summarizing Slack messages and generating to-dos&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's free to use, you just hook up Ollama or an API key of your choice, install some MCP servers, and you can chat or schedule any prompts you want. The MCP servers I'm using in my examples are Playwright, Discord, Slack, and Brave Search - let me know if you're interested in a tutorial and I'm happy to throw one together.&lt;/p&gt; &lt;p&gt;Would love any feedback (good or bad!) here or &lt;a href="https://discord.gg/9CH6us29YA"&gt;on our Discord&lt;/a&gt;, you can download the latest release here: &lt;a href="https://github.com/runebookai/tome/releases/tag/0.9.2"&gt;https://github.com/runebookai/tome/releases/tag/0.9.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for checking us out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WalrusVegetable4506"&gt; /u/WalrusVegetable4506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2donn13krobf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luuyej/built_an_easy_way_to_schedule_prompts_powered_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1luuyej/built_an_easy_way_to_schedule_prompts_powered_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T17:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv4o6y</id>
    <title>Why is this model from HF telling me it's a boy or girl or man or woman then goes on an endless rant?</title>
    <updated>2025-07-09T00:12:12+00:00</updated>
    <author>
      <name>/u/omni_shaNker</name>
      <uri>https://old.reddit.com/user/omni_shaNker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying different models from HF, for example:&lt;br /&gt; &lt;a href="https://huggingface.co/TheBloke/law-LLM-GGUF/tree/main"&gt;https://huggingface.co/TheBloke/law-LLM-GGUF/tree/main&lt;/a&gt;&lt;br /&gt; and I do&lt;br /&gt; ollama run &lt;a href="http://hf.co/TheBloke/law-LLM-GGUF"&gt;hf.co/TheBloke/law-LLM-GGUF&lt;/a&gt;&lt;br /&gt; and it downloads the model and runs it but when I ask it &amp;quot;what can you help me with&amp;quot; it totally goes off the rails. Am I doing something wrong or am I missing a step? I'm somewhat new to this and have been having great results with the models listed in the ollama repo/directory.&lt;/p&gt; &lt;p&gt;NOTE: This post has 2.7K views as of this note, and 0 upvotes. Why is it unpopular to ask this question? Do people on this sub not really know why something like this happens and what the solution is. I assumed I would find some Ollama experts on here. Doesn't look like it... &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omni_shaNker"&gt; /u/omni_shaNker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lv4o6y/why_is_this_model_from_hf_telling_me_its_a_boy_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lv4o6y/why_is_this_model_from_hf_telling_me_its_a_boy_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lv4o6y/why_is_this_model_from_hf_telling_me_its_a_boy_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T00:12:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lug5su</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-07-08T05:04:15+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a &lt;strong&gt;Highly Customizable AI Research Agent&lt;/strong&gt; that connects to your personal external sources and search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, Discord, and more coming soon.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;üìä &lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt; &lt;li&gt;Offers a RAG-as-a-Service API Backend&lt;/li&gt; &lt;li&gt;50+ File extensions supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üéôÔ∏è &lt;strong&gt;Podcasts&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt; &lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt; &lt;li&gt;Multiple TTS providers supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ÑπÔ∏è &lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;YouTube videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;Discord&lt;/li&gt; &lt;li&gt;...and more on the way&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üîñ &lt;strong&gt;Cross-Browser Extension&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lug5su/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lug5su/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lug5su/open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T05:04:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv7n2g</id>
    <title>Can I just download the files for a model?</title>
    <updated>2025-07-09T02:37:11+00:00</updated>
    <author>
      <name>/u/General174512</name>
      <uri>https://old.reddit.com/user/General174512</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to be able to put the Deepseek R1 on a USB for use on my other computers, is it possible to just download a model (like clicking a download button), and then being able to throw it onto the USB?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/General174512"&gt; /u/General174512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lv7n2g/can_i_just_download_the_files_for_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lv7n2g/can_i_just_download_the_files_for_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lv7n2g/can_i_just_download_the_files_for_a_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T02:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1luwp11</id>
    <title>What is the best LLM I can use? (I'm new in this sector)</title>
    <updated>2025-07-08T18:43:55+00:00</updated>
    <author>
      <name>/u/No-Studio9085</name>
      <uri>https://old.reddit.com/user/No-Studio9085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PC:&lt;/p&gt; &lt;p&gt;RTX 3060&lt;/p&gt; &lt;p&gt;12GB VRAM&lt;/p&gt; &lt;p&gt;16GB RAM&lt;/p&gt; &lt;p&gt;i5 12400F&lt;/p&gt; &lt;p&gt;I would actually like it for two situations:&lt;/p&gt; &lt;p&gt;- One that is for specific tasks or specifics situations&lt;/p&gt; &lt;p&gt;- And another that works well for roleplay&lt;/p&gt; &lt;p&gt;Thanks&amp;lt;3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Studio9085"&gt; /u/No-Studio9085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luwp11/what_is_the_best_llm_i_can_use_im_new_in_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luwp11/what_is_the_best_llm_i_can_use_im_new_in_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1luwp11/what_is_the_best_llm_i_can_use_im_new_in_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T18:43:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvhdm4</id>
    <title>Starting model delay</title>
    <updated>2025-07-09T12:21:59+00:00</updated>
    <author>
      <name>/u/thexdroid</name>
      <uri>https://old.reddit.com/user/thexdroid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My program uses the API, if the server is still loading the model it will raise an error due timeout. Is there a way, using the API (I could not found, sorry) to know if the model is loaded? Using ollama ps show the model in memory but it won't say it is ready to use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thexdroid"&gt; /u/thexdroid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvhdm4/starting_model_delay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvhdm4/starting_model_delay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lvhdm4/starting_model_delay/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T12:21:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvk3o1</id>
    <title>Best model for my coding the correct concepts for something complicated</title>
    <updated>2025-07-09T14:24:00+00:00</updated>
    <author>
      <name>/u/beginnerflipper</name>
      <uri>https://old.reddit.com/user/beginnerflipper</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 3080ti, 32gb of ram, and a 7800x3d. I can debug code, but I want to make sure it gets the concepts down from an academic paper and use it to write code and use packages already developed. Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beginnerflipper"&gt; /u/beginnerflipper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvk3o1/best_model_for_my_coding_the_correct_concepts_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvk3o1/best_model_for_my_coding_the_correct_concepts_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lvk3o1/best_model_for_my_coding_the_correct_concepts_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T14:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvfej4</id>
    <title>ngrok for AI models - Serve Ollama models with a cloud API using Local Runners</title>
    <updated>2025-07-09T10:33:33+00:00</updated>
    <author>
      <name>/u/Sumanth_077</name>
      <uri>https://old.reddit.com/user/Sumanth_077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, we‚Äôve built ngrok for AI models ‚Äî and it works seamlessly with Ollama.&lt;/p&gt; &lt;p&gt;We built Local Runners to let you serve AI models, MCP servers, or agents directly from your own machine and expose them through a secure Clarifai endpoint. No need to spin up a web server, manage routing, or deploy to the cloud. Just run the model locally and get a working API endpoint instantly.&lt;/p&gt; &lt;p&gt;If you're running open-source models with Ollama, Local Runners let you keep compute and data local while still connecting to agent frameworks, APIs, or workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Run ‚Äì Start a local runner pointing to your model&lt;br /&gt; Tunnel ‚Äì It opens a secure connection to a hosted API endpoint&lt;br /&gt; Requests ‚Äì API calls are routed to your machine&lt;br /&gt; Response ‚Äì Your model processes them locally and returns the result&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this helps:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Skip building a server or deploying just to test a model&lt;/li&gt; &lt;li&gt;Wire local models into LangGraph, CrewAI, or custom agent loops&lt;/li&gt; &lt;li&gt;Access local files, private tools, or data sources from your model&lt;/li&gt; &lt;li&gt;Use your existing hardware for inference, especially for token hungry models and agents, reducing cloud costs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôve put together a short tutorial that shows how you can expose local models, MCP servers, tools, and agents securely using Local Runners, without deploying anything to the cloud.&lt;br /&gt; &lt;a href="https://youtu.be/JOdtZDmCFfk"&gt;https://youtu.be/JOdtZDmCFfk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear how you're running Ollama models or building agent workflows around them. Fire away in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sumanth_077"&gt; /u/Sumanth_077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvfej4/ngrok_for_ai_models_serve_ollama_models_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvfej4/ngrok_for_ai_models_serve_ollama_models_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lvfej4/ngrok_for_ai_models_serve_ollama_models_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T10:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lutct9</id>
    <title>My little tribute to Ollama</title>
    <updated>2025-07-08T16:37:18+00:00</updated>
    <author>
      <name>/u/valdecircarvalho</name>
      <uri>https://old.reddit.com/user/valdecircarvalho</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lutct9/my_little_tribute_to_ollama/"&gt; &lt;img alt="My little tribute to Ollama" src="https://preview.redd.it/5n0izf2mhobf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9e966c0d12b9995e3120a8ec5a0782e30b0f651" title="My little tribute to Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdecircarvalho"&gt; /u/valdecircarvalho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5n0izf2mhobf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lutct9/my_little_tribute_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lutct9/my_little_tribute_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T16:37:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv672x</id>
    <title>I used Ollama to build a Cursor for PDFs</title>
    <updated>2025-07-09T01:25:50+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lv672x/i_used_ollama_to_build_a_cursor_for_pdfs/"&gt; &lt;img alt="I used Ollama to build a Cursor for PDFs" src="https://external-preview.redd.it/eW1tYWxxOGYycmJmMXdyH2g3gh74ax-OLM0Bn_sh-0xXIf9gZjZs7gXuAEvK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e52e99d0c98b4bb58943f1138c44430623ddf356" title="I used Ollama to build a Cursor for PDFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like using Cursor while coding, but there are a lot of other tasks outside of code that would also benefit from having an agent on the side - things like reading through long documents and filling out forms. &lt;/p&gt; &lt;p&gt;So, as a fun experiment, I built an agent with search with a PDF viewer on the side. I've found it to be super helpful - and I'd love feedback on where you'd like to see this go!&lt;/p&gt; &lt;p&gt;If you'd like to try it out: &lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="http://github.com/morphik-org/morphik-core"&gt;github.com/morphik-org/morphik-core&lt;/a&gt;&lt;br /&gt; Website: &lt;a href="http://morphik.ai"&gt;morphik.ai&lt;/a&gt; (Look for the PDF Viewer section!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3jj1br8f2rbf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lv672x/i_used_ollama_to_build_a_cursor_for_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lv672x/i_used_ollama_to_build_a_cursor_for_pdfs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T01:25:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvqcfw</id>
    <title>Thoughts on grabbing a 5060 Ti 16G as a noob?</title>
    <updated>2025-07-09T18:28:21+00:00</updated>
    <author>
      <name>/u/SKX007J1</name>
      <uri>https://old.reddit.com/user/SKX007J1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For someone wanting to get started with ollama and experiment with self-hosting hosting how does the 5060 Ti 16G stack up for the price point of ¬£390/$500. &lt;/p&gt; &lt;p&gt;What would you get with that sort of budget if your goal was just learning rather than productivity? Any ways to mitigate that they nerfed the bandwidth of the memory? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SKX007J1"&gt; /u/SKX007J1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvqcfw/thoughts_on_grabbing_a_5060_ti_16g_as_a_noob/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lvqcfw/thoughts_on_grabbing_a_5060_ti_16g_as_a_noob/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lvqcfw/thoughts_on_grabbing_a_5060_ti_16g_as_a_noob/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-09T18:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwi70x</id>
    <title>I'm cloud architect and I'm searching of there an LLM that can help me to create technical documentation and solution design for business need.</title>
    <updated>2025-07-10T17:09:12+00:00</updated>
    <author>
      <name>/u/KindheartednessHot90</name>
      <uri>https://old.reddit.com/user/KindheartednessHot90</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KindheartednessHot90"&gt; /u/KindheartednessHot90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwi70x/im_cloud_architect_and_im_searching_of_there_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwi70x/im_cloud_architect_and_im_searching_of_there_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lwi70x/im_cloud_architect_and_im_searching_of_there_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-10T17:09:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw1o6d</id>
    <title>Smollm ? Coding models?</title>
    <updated>2025-07-10T02:39:39+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's a good coding model? Is is there plans for the new smollm3? It would need prompting cues to be built in. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lw1o6d/smollm_coding_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lw1o6d/smollm_coding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lw1o6d/smollm_coding_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-10T02:39:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwhuvw</id>
    <title>Index academic papers and extract metadata with LLMs (Ollama Integrated)</title>
    <updated>2025-07-10T16:56:15+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Ollama community, want to share my latest project about academic papers PDF metadata extraction&lt;/p&gt; &lt;ul&gt; &lt;li&gt;extracting metadata (title, authors, abstract)&lt;/li&gt; &lt;li&gt;relationship (which author has which papers) and&lt;/li&gt; &lt;li&gt;embeddings for semantic search&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I don't see any similar comprehensive example published, so would like to share mine. The library has &lt;a href="https://cocoindex.io/docs/ai/llm#ollama"&gt;native Ollama Integration&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Python source code: &lt;a href="https://github.com/cocoindex-io/cocoindex/tree/main/examples/paper_metadata"&gt;https://github.com/cocoindex-io/cocoindex/tree/main/examples/paper_metadata&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full write up: &lt;a href="https://cocoindex.io/blogs/academic-papers-indexing/"&gt;https://cocoindex.io/blogs/academic-papers-indexing/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Appreciate a star on the repo if it is helpful, thanks! And would love to learn your suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwhuvw/index_academic_papers_and_extract_metadata_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwhuvw/index_academic_papers_and_extract_metadata_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lwhuvw/index_academic_papers_and_extract_metadata_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-10T16:56:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwg067</id>
    <title>Public and Private local setups: how I have a public facing OpenWebUI and private GPU</title>
    <updated>2025-07-10T15:42:56+00:00</updated>
    <author>
      <name>/u/brulak</name>
      <uri>https://old.reddit.com/user/brulak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Haven't seen too many talk about this, so I figure I'd throw my hat in on this. &lt;/p&gt; &lt;p&gt;I have 2x3090 at home. It runs ubuntu with ollama. I have devstral, llama3.2 etc. &lt;/p&gt; &lt;p&gt;I setup a Digital ocean droplet. &lt;/p&gt; &lt;p&gt;It sits behind a digital ocean firewall and it has the local firewall (ufw) set up as well. &lt;/p&gt; &lt;p&gt;I set up a VPN between the two boxes. OpenWebUi is configured to connect with ollama via the VPN. So, it connects with 10.0.0.1. &lt;/p&gt; &lt;p&gt;When you visit the OpenWebUI server, it shows the models from my GPU rig. &lt;/p&gt; &lt;p&gt;Performance wise: the round trip is a bit slower than you'd want. If i'm at home, I connect directly to the box without the Droplet to eliminate the round trip cost. Then performance is amazing. Espcially with &lt;a href="http://continue.dev"&gt;continue.dev&lt;/a&gt; and devstral or qwen. &lt;/p&gt; &lt;p&gt;If I'm out of the house, either on my laptop or my phone the performance is manageable. &lt;/p&gt; &lt;p&gt;Feel free to ask me anything else I might have missed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brulak"&gt; /u/brulak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwg067/public_and_private_local_setups_how_i_have_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwg067/public_and_private_local_setups_how_i_have_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lwg067/public_and_private_local_setups_how_i_have_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-10T15:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx49vx</id>
    <title>100k dollars budget only for equipment. for business for cloud renting.</title>
    <updated>2025-07-11T11:20:22+00:00</updated>
    <author>
      <name>/u/Free_Care_2006</name>
      <uri>https://old.reddit.com/user/Free_Care_2006</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;you have 100k. In what do you invest and why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Free_Care_2006"&gt; /u/Free_Care_2006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lx49vx/100k_dollars_budget_only_for_equipment_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lx49vx/100k_dollars_budget_only_for_equipment_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lx49vx/100k_dollars_budget_only_for_equipment_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T11:20:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwccef</id>
    <title>How do you reduce hallucinations on agents of small models?</title>
    <updated>2025-07-10T13:10:37+00:00</updated>
    <author>
      <name>/u/mynameismati</name>
      <uri>https://old.reddit.com/user/mynameismati</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been reading about different techniques like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAG &lt;/li&gt; &lt;li&gt;Context Engineering &lt;/li&gt; &lt;li&gt;Memory management&lt;/li&gt; &lt;li&gt;Prompt Engineering &lt;/li&gt; &lt;li&gt;Fine-tuning models for your specific case &lt;/li&gt; &lt;li&gt;Reducing context through re-adaptation and use of micro-agents while splitting tasks into smaller ones and having shorter pipelines.&lt;/li&gt; &lt;li&gt;...others&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And as of now what has been most useful for me is reducing context, and be in control of every token for the prompt as well as the token while trying to maintain the most direct way for the agent to go to the tool and do the desired task.&lt;/p&gt; &lt;p&gt;Agents that evaluate prompts, parse the input to a specific format trying to reduce tokens, call the agent that handles certain tasks and evaluate tool choosing by other agent has been also useful but I think I am over-complicating.&lt;/p&gt; &lt;p&gt;What has been your approach? All of these things I do have been with 7b-8b-14b models. I cant go larget as my GPU is 8gb of VRAM and low cost. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mynameismati"&gt; /u/mynameismati &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwccef/how_do_you_reduce_hallucinations_on_agents_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwccef/how_do_you_reduce_hallucinations_on_agents_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lwccef/how_do_you_reduce_hallucinations_on_agents_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-10T13:10:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lww3in</id>
    <title>What kind of performance boost will I see with a modern GPU</title>
    <updated>2025-07-11T03:03:04+00:00</updated>
    <author>
      <name>/u/leathermartini</name>
      <uri>https://old.reddit.com/user/leathermartini</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I set up an Ollama server to let my Home Assistant do some voice control features and possibly stand in for Alexa/Google. Using an old (5 year) gaming/streaming PC (GeForce GTX 1660 Super GPU) to serve it. I've managed to get it mostly functional BUT it is... Not fast. Simple tasks (turn on lights, query the current weather) are handled locally and work fine. Others (play a song, check the forecast, questions it has to parse with the LLM) take 60-240 seconds to process. Checking the logs it looks like each Ollama request takes 60ish seconds.&lt;/p&gt; &lt;p&gt;I'm trying to work out the cost of making this feasible. But I don't have a ton of gaming hardware just sitting around. The cheap options look to be getting a GTX 5060 or so and swapping video cards. Benchmarks say I should see a jump around 140-200% with that. (Next option would be a new machine with a bigger power supply and other options...)&lt;/p&gt; &lt;p&gt;Basically I want to know what benchmark to look at and how to see how it might impact ollama's performance. &lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leathermartini"&gt; /u/leathermartini &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lww3in/what_kind_of_performance_boost_will_i_see_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lww3in/what_kind_of_performance_boost_will_i_see_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lww3in/what_kind_of_performance_boost_will_i_see_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T03:03:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx45fg</id>
    <title>Ollama Auto Start Despite removed from "Open at Login"</title>
    <updated>2025-07-11T11:13:34+00:00</updated>
    <author>
      <name>/u/Ok-Mix-646</name>
      <uri>https://old.reddit.com/user/Ok-Mix-646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lx45fg/ollama_auto_start_despite_removed_from_open_at/"&gt; &lt;img alt="Ollama Auto Start Despite removed from &amp;quot;Open at Login&amp;quot;" src="https://b.thumbs.redditmedia.com/H5gbzUNdSAKkhBj3cvclCDiU4XP-WvIW5b5y17kVmJs.jpg" title="Ollama Auto Start Despite removed from &amp;quot;Open at Login&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Mix-646"&gt; /u/Ok-Mix-646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ollama/comments/1lujfpb/ollama_auto_start_despite_removed_from_open_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lx45fg/ollama_auto_start_despite_removed_from_open_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lx45fg/ollama_auto_start_despite_removed_from_open_at/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T11:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwxnqn</id>
    <title>üöÄ Built a transparent metrics proxy for Ollama - zero config changes needed!</title>
    <updated>2025-07-11T04:25:31+00:00</updated>
    <author>
      <name>/u/firedog7881</name>
      <uri>https://old.reddit.com/user/firedog7881</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just finished this little tool that adds Prometheus monitoring to Ollama without touching your existing client setup. Your apps still connect to localhost:11434 like normal, but now you get detailed metrics and analytics.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; - Intercepts Ollama API calls to collect metrics (latency, tokens/sec, error rates) - Stores detailed analytics (prompts, timings, token counts) - Exposes Prometheus metrics for dashboards - Works with any Ollama client - no code changes needed&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Installation is stupid simple:&lt;/strong&gt; &lt;code&gt;bash git clone https://github.com/bmeyer99/Ollama_Proxy_Wrapper cd Ollama_Proxy_Wrapper quick_install.bat &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Then just use Ollama commands normally:&lt;/strong&gt; &lt;code&gt;bash ollama_metrics.bat run phi4 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Boom - metrics at &lt;code&gt;http://localhost:11434/metrics&lt;/code&gt; and searchable analytics for debugging slow requests.&lt;/p&gt; &lt;p&gt;The proxy runs Ollama on a hidden port (11435) and sits transparently on the default port (11434). Everything just works‚Ñ¢Ô∏è&lt;/p&gt; &lt;p&gt;Perfect for anyone running Ollama in production or just wanting to understand their model performance better.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/bmeyer99/Ollama_Proxy_Wrapper"&gt;https://github.com/bmeyer99/Ollama_Proxy_Wrapper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/firedog7881"&gt; /u/firedog7881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwxnqn/built_a_transparent_metrics_proxy_for_ollama_zero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwxnqn/built_a_transparent_metrics_proxy_for_ollama_zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lwxnqn/built_a_transparent_metrics_proxy_for_ollama_zero/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T04:25:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwbuvc</id>
    <title>Can I build a self hosted LLM server for 300 users?</title>
    <updated>2025-07-10T12:47:58+00:00</updated>
    <author>
      <name>/u/tornshorts</name>
      <uri>https://old.reddit.com/user/tornshorts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, trying to get a feel if I'm in over my head here.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt; I'm a sysadmin for a 300 person law firm. One of the owners here is really into AI and wants to give all of our users a ChatGPT-like experience.&lt;/p&gt; &lt;p&gt;The vision is to have a tool that everyone can use strictly for drafting legal documents based on their notes, grammar correction, formatting emails, and that sort of thing. We're not using it for legal research, just editorial purposes.&lt;/p&gt; &lt;p&gt;Since we often deal with documents that include PII, having a self-hosted, in-house solution is way more appealing than letting people throw client info into ChatGPT. So we're thinking of hosting our own LLM, putting it behind a username/password login, maybe adding 2FA, and only allowing access from inside the office or over VPN.&lt;/p&gt; &lt;p&gt;Now, all of this sounds... kind of simple to me. I've got experience setting up servers, and I have a general, theoretical idea of the hardware requirements to get this running. I even set up an Ollama/WebUI server at home for personal use, so I‚Äôve got at least a little hands-on experience with how this kind of build works.&lt;/p&gt; &lt;p&gt;What I‚Äôm not sure about is scalability. Can this actually support 300+ users? Am I underestimating what building a PC with a few GPUs can handle? Is user creation and management going to be a major headache? Am I missing something big here?&lt;/p&gt; &lt;p&gt;I might just be overthinking this, but I fully admit I‚Äôm not an expert on LLMs. I‚Äôm just a techy dude watching YouTube builds thinking, ‚ÄúYeah, I can do that too.‚Äù&lt;/p&gt; &lt;p&gt;Any advice or insight would be really appreciated. Thanks!&lt;/p&gt; &lt;p&gt;EDIT: I got a lot more feedback than I anticipated and I‚Äôm so thankful for everyone‚Äôs insight and suggestions. While this sounds like a fun challenge for me to tackle, I‚Äôm now understanding that doing this is going to be a full time job. I‚Äôm the only one on my team skilled enough to potentially pull this off but it‚Äôs going to take me away from my day to day responsibilities. Our IT dept is already a skeleton crew and I don‚Äôt feel comfortable adding this to our already full plate. We‚Äôre going to look into cloud solutions instead. Thanks everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tornshorts"&gt; /u/tornshorts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwbuvc/can_i_build_a_self_hosted_llm_server_for_300_users/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lwbuvc/can_i_build_a_self_hosted_llm_server_for_300_users/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lwbuvc/can_i_build_a_self_hosted_llm_server_for_300_users/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-10T12:47:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx0o4p</id>
    <title>Ollama + OpenWebUI + documents</title>
    <updated>2025-07-11T07:27:49+00:00</updated>
    <author>
      <name>/u/ZimmerFrameThief</name>
      <uri>https://old.reddit.com/user/ZimmerFrameThief</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry if this is quite obvious or listed somewhere - I couldn't google it.&lt;/p&gt; &lt;p&gt;I run ollama with OpenWebUI in a docker environment (separate containers, same custom network) on Unraird.&lt;br /&gt; All works as it should - LLM Q&amp;amp;A is as expected - except that the LLMs say they can't interact with the documents.&lt;br /&gt; OpenWebUI has a document (and image) upload functionality - the documents appear to upload - and the LLMs can see the file names, but when I ask them to do anything with the document content, they say they don't have the functionality.&lt;br /&gt; I assumed this was an ollama thing.. but maybe it's an OpenWebUI thing? I'm pretty new to this, so don't know what I don't know.&lt;/p&gt; &lt;p&gt;Side note - don't know if it's possible to give any of the LLMs access to the net? but that would be cool too!&lt;/p&gt; &lt;p&gt;EDIT: I just use the mainstream LLMs like Deepseek, Gemma, Qewn, Minstrel, Llam etc. And I am only needing them to read/interpret the contents of document - not to edit or do anything else.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZimmerFrameThief"&gt; /u/ZimmerFrameThief &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lx0o4p/ollama_openwebui_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lx0o4p/ollama_openwebui_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lx0o4p/ollama_openwebui_documents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T07:27:49+00:00</published>
  </entry>
</feed>
