<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-26T14:34:21+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1rdl1fd</id>
    <title>What's the best model to run on mac m1 pro 16gb?</title>
    <updated>2026-02-24T16:33:37+00:00</updated>
    <author>
      <name>/u/Embarrassed-Baby3964</name>
      <uri>https://old.reddit.com/user/Embarrassed-Baby3964</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an old m1 mac pro with 16gb ram. Was wondering if there are any good performing models in 2026 that I can run on this hardware? And if so, what is the best one in your opinion?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Baby3964"&gt; /u/Embarrassed-Baby3964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdl1fd/whats_the_best_model_to_run_on_mac_m1_pro_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdl1fd/whats_the_best_model_to_run_on_mac_m1_pro_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdl1fd/whats_the_best_model_to_run_on_mac_m1_pro_16gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T16:33:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1re4kp7</id>
    <title>Does Ollama cloud feel slow?</title>
    <updated>2026-02-25T05:29:46+00:00</updated>
    <author>
      <name>/u/Safe_Concern2889</name>
      <uri>https://old.reddit.com/user/Safe_Concern2889</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bought 20$ subscription plan and trying to use glm and minimax. GLM takes almost 15-20 mins for simple tasks. Wanted to understand is this specific to ollama cloud or with other providers too?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Safe_Concern2889"&gt; /u/Safe_Concern2889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1re4kp7/does_ollama_cloud_feel_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1re4kp7/does_ollama_cloud_feel_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1re4kp7/does_ollama_cloud_feel_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T05:29:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdzgyb</id>
    <title>AI toolkit ‚Äî LiteLLM + n8n + Open WebUI in one Docker Compose</title>
    <updated>2026-02-25T01:30:39+00:00</updated>
    <author>
      <name>/u/Puzzleheaded-Dig-492</name>
      <uri>https://old.reddit.com/user/Puzzleheaded-Dig-492</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rdzgyb/ai_toolkit_litellm_n8n_open_webui_in_one_docker/"&gt; &lt;img alt="AI toolkit ‚Äî LiteLLM + n8n + Open WebUI in one Docker Compose" src="https://external-preview.redd.it/a8eQoVMnXY-S4cgiOUtyBMg1XNcN5q3piKq1KRHUsj8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50218f6c6108c3188b1bb498cf5c008d8d37a7f6" title="AI toolkit ‚Äî LiteLLM + n8n + Open WebUI in one Docker Compose" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just to make things simpler, I put together a simple Docker Compose stack that bundles everything you need to run a &lt;strong&gt;local AI environment&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LiteLLM&lt;/strong&gt; ‚Äî LLM proxy / API gateway (OpenAI-compatible)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;n8n&lt;/strong&gt; ‚Äî Workflow automation (think Zapier but self-hosted)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open WebUI&lt;/strong&gt; ‚Äî Chat interface for your LLMs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;30+ Ollama Cloud free models are pre-configured out of the box. Just create an [Ollama](vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/code/electron-browser/workbench/workbench.html) account, grab an API key, and &lt;code&gt;docker compose up -d&lt;/code&gt; üöÄ&lt;/p&gt; &lt;p&gt;You can also add more models later through LiteLLM ‚Äî whether local (e.g. Ollama, vLLM) or cloud (e.g. OpenAI, Anthropic, Azure).&lt;/p&gt; &lt;p&gt;üîó [&lt;a href="https://github.com/wa91h/local-ai-toolkit%5D(vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/code/electron-browser/workbench/workbench.html)"&gt;https://github.com/wa91h/local-ai-toolkit](vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/code/electron-browser/workbench/workbench.html)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Puzzleheaded-Dig-492"&gt; /u/Puzzleheaded-Dig-492 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/wa91h/local-ai-toolkit"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdzgyb/ai_toolkit_litellm_n8n_open_webui_in_one_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdzgyb/ai_toolkit_litellm_n8n_open_webui_in_one_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T01:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1reguw7</id>
    <title>Bosgame M5 / Ryzen AI MAX+ 395 (Radeon 8060S gfx1103) ‚Äî AMDGPU ‚ÄúMES failed / SDMA timeout / GPU reset‚Äù on Ubuntu 24.04.1 kernel 6.14 ‚Äî ROCm unusable, Ollama stuck on CPU</title>
    <updated>2026-02-25T15:49:39+00:00</updated>
    <author>
      <name>/u/CaterpillarCultural1</name>
      <uri>https://old.reddit.com/user/CaterpillarCultural1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all ‚Äî I‚Äôm running a Bosgame M5 mini PC with an AMD Ryzen AI MAX+ 395 APU on Ubuntu 24.04.1 LTS (kernel 6.14.0-1017-oem) with 128GB unified RAM. I‚Äôm trying to use the integrated Radeon 8060S GPU for local LLM inference via Ollama and/or llama.cpp, but I‚Äôm seeing repeated GPU resets that make compute completely unreliable.&lt;/p&gt; &lt;p&gt;Hardware&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô Machine: Bosgame M5 ‚àô APU: AMD Ryzen AI MAX+ 395 ‚àô GPU: Radeon 8060S (RDNA 3.5, gfx1103, Device ID 1586) ‚àô RAM: 128GB unified memory (shared CPU/GPU) ‚àô OS: Ubuntu 24.04.1 LTS ‚àô Kernel: 6.14.0-1017-oem &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Symptoms&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô Ollama runs on CPU only ‚Äî GPU either not picked up or immediately falls back after a reset ‚àô Random slowdowns; occasionally fast again after a GPU reset cycle ‚àô Kernel logs show continuous AMDGPU failures &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Relevant kernel log snippets (journalctl -k):&lt;/p&gt; &lt;p&gt;MES failed to respond to msg=REMOVE_QUEUE&lt;/p&gt; &lt;p&gt;MES might be in unrecoverable state, issue a GPU reset&lt;/p&gt; &lt;p&gt;[gfxhub] page fault ‚Ä¶ address 0x0000000000000000&lt;/p&gt; &lt;p&gt;ring sdma0 timeout ‚Ä¶ Starting sdma0 ring reset&lt;/p&gt; &lt;p&gt;GPU reset succeeded ‚Ä¶ VRAM is lost due to GPU reset!&lt;/p&gt; &lt;p&gt;resume of IP block &amp;lt;vpe\_v6\_1&amp;gt; failed -110&lt;/p&gt; &lt;p&gt;What I‚Äôve tried&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô OLLAMA\_VULKAN=1 set in systemd service ‚Äî no improvement ‚àô ROCm install attempts ‚Äî compute still falls back to CPU ‚àô OEM kernel 6.14 (Ubuntu‚Äôs latest for this hardware) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What I need&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. Has anyone gotten stable AMD GPU compute (ROCm or Vulkan) working on this specific APU (Ryzen AI MAX+ 395 / gfx1103) on Linux? 2. Are the MES/SDMA reset errors a known issue with this kernel or firmware stack ‚Äî and is there a known fix (different kernel, Mesa version, firmware package)? 3. If you‚Äôve solved this, what exact combination worked ‚Äî kernel version, Mesa, firmware, ROCm version? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note: The 128GB unified memory means VRAM is carved from system RAM ‚Äî not sure if this affects the instability. Any pointers on kernel params, firmware packages, or whether the OEM kernel is the problem would be hugely appreciated.üôèüòä&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CaterpillarCultural1"&gt; /u/CaterpillarCultural1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1reguw7/bosgame_m5_ryzen_ai_max_395_radeon_8060s_gfx1103/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1reguw7/bosgame_m5_ryzen_ai_max_395_radeon_8060s_gfx1103/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1reguw7/bosgame_m5_ryzen_ai_max_395_radeon_8060s_gfx1103/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T15:49:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1refx2p</id>
    <title>Need recommendations or advice to do with my servers (coding - automations)</title>
    <updated>2026-02-25T15:15:05+00:00</updated>
    <author>
      <name>/u/Lotus-006</name>
      <uri>https://old.reddit.com/user/Lotus-006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1refx2p/need_recommendations_or_advice_to_do_with_my/"&gt; &lt;img alt="Need recommendations or advice to do with my servers (coding - automations)" src="https://preview.redd.it/xc7lvuveqnlg1.png?width=140&amp;amp;height=100&amp;amp;auto=webp&amp;amp;s=8e1c1cda81ded4381aaf3aa70a939c15f55f6c9f" title="Need recommendations or advice to do with my servers (coding - automations)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello , i'm not sure how to start i have 2 big servers, the MSI with the ultra core 7 i use it for my daily use on windows 11 and the other one the i9 for proxmox with media tool and hosting or vm.&lt;/p&gt; &lt;p&gt;yeah i know i'm stuck with my gpu on my MSI because the cpu in no integrated graphic i made a mistake when i purchased it and i have just one dGPU MSI Ventus 3x OC 16gb RTX 5070ti on it.&lt;/p&gt; &lt;p&gt;i want to use ollama with vscode but when i do ollama serve in windows and connect to the localhost:11434 i cant use the model in it its say&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Sorry, your request failed. Please try again.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Copilot Request id: b885a817-c6b5-4cb2-8111-1f34174b2481&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Reason: 404 page not found: Error: 404 page not found at u5._provideLanguageModelResponse&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;this is the computers i have :&lt;/p&gt; &lt;p&gt;(Z790 AORUS ELITE AX + i9 13900k + 96GB DDR5 6800Mhz - iGPU)&lt;/p&gt; &lt;p&gt;(MSI PRO Z890-S WIFI PZ LGA 1851 Intel Z890 + Ultra Core 7 265k + 32GB DDR5 6000Mhz)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xc7lvuveqnlg1.png?width=1547&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0bad8ef97db6a0766b7ba949dd5de95d200f239"&gt;https://preview.redd.it/xc7lvuveqnlg1.png?width=1547&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0bad8ef97db6a0766b7ba949dd5de95d200f239&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k4lln6uvqnlg1.png?width=1437&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38bff1fe6c7e6b2678f431229dd4f467b9209950"&gt;https://preview.redd.it/k4lln6uvqnlg1.png?width=1437&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38bff1fe6c7e6b2678f431229dd4f467b9209950&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lotus-006"&gt; /u/Lotus-006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1refx2p/need_recommendations_or_advice_to_do_with_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1refx2p/need_recommendations_or_advice_to_do_with_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1refx2p/need_recommendations_or_advice_to_do_with_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T15:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1rek5yy</id>
    <title>Flowise alternatives?</title>
    <updated>2026-02-25T17:44:47+00:00</updated>
    <author>
      <name>/u/warlocc_</name>
      <uri>https://old.reddit.com/user/warlocc_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing Flowise to set up a multi-model path for feeding context to my final model, but it seems like Flowise itself is very glitchy. Has anyone found alternatives that work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/warlocc_"&gt; /u/warlocc_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rek5yy/flowise_alternatives/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rek5yy/flowise_alternatives/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rek5yy/flowise_alternatives/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T17:44:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1rekdbi</id>
    <title>Getting Goose to actually work with local Ollama models ‚Äî what I ran into and what I built</title>
    <updated>2026-02-25T17:51:38+00:00</updated>
    <author>
      <name>/u/BenevolentJoker</name>
      <uri>https://old.reddit.com/user/BenevolentJoker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rekdbi/getting_goose_to_actually_work_with_local_ollama/"&gt; &lt;img alt="Getting Goose to actually work with local Ollama models ‚Äî what I ran into and what I built" src="https://external-preview.redd.it/pbgfqwlMXm6IOOu8BQ8ITwlsb3n0jqJch4zsZIR-Pe8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09c91535c6e4b95c05f5c704c34d8baac8eee6d2" title="Getting Goose to actually work with local Ollama models ‚Äî what I ran into and what I built" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama adapted goose so that ollama on goose actually is able to more reliably use more than just 1-2 ollama models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BenevolentJoker"&gt; /u/BenevolentJoker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ravhqi/getting_goose_to_actually_work_with_local_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rekdbi/getting_goose_to_actually_work_with_local_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rekdbi/getting_goose_to_actually_work_with_local_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T17:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1re6pgx</id>
    <title>qwen3.5:35b-a3b is here.</title>
    <updated>2026-02-25T07:26:55+00:00</updated>
    <author>
      <name>/u/Space__Whiskey</name>
      <uri>https://old.reddit.com/user/Space__Whiskey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;yey. that is all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Space__Whiskey"&gt; /u/Space__Whiskey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1re6pgx/qwen3535ba3b_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1re6pgx/qwen3535ba3b_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1re6pgx/qwen3535ba3b_is_here/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T07:26:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1revteu</id>
    <title>Hypeboard.ai - A live LLM Leaderboard based on /r/localllama posts/comments</title>
    <updated>2026-02-26T00:54:58+00:00</updated>
    <author>
      <name>/u/peva3</name>
      <uri>https://old.reddit.com/user/peva3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;thought this might be of some interest to y'all as well if you're wondering how to stay on top of what new models are trending on reddit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peva3"&gt; /u/peva3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hypeboard.ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1revteu/hypeboardai_a_live_llm_leaderboard_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1revteu/hypeboardai_a_live_llm_leaderboard_based_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T00:54:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1resqne</id>
    <title>Need some help fixing "ollama pull"</title>
    <updated>2026-02-25T22:52:50+00:00</updated>
    <author>
      <name>/u/A_Zeppelin</name>
      <uri>https://old.reddit.com/user/A_Zeppelin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently use Ollama via docker, connected to Open-WebUI. Until recently, it had worked flawlessly for me. However, I recently have had loads of issues pulling any new models as of late, even from the Ollama library, always getting this error:&lt;/p&gt; &lt;p&gt;`Error: remove /root/.ollama/models/blobs/sha256-d838916...2522cf1a-partial-0: no such file or directory` However, this error only happens once the model has fully downloaded.&lt;/p&gt; &lt;p&gt;Indeed, when I Docker Exec into the container after it fails, it is correct that no such file exists. However, it DOES exist when I exec into the container mid-download. There's sha256-d...a-partial, sha256-d...a-partial-0, sha256-d...a-partial-1, ... , ... , all the way to sha256-d...a-partial-20. However, these files disappear midway through the download, reappear again, and a few are left behind after the download fails. If I delete them and attempt to re-download, it still fails.&lt;/p&gt; &lt;p&gt;I deploy ollama via `docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama --restart always -e OLLAMA_HOST=0.0.0.0 ollama/ollama`. &lt;/p&gt; &lt;p&gt;This did not always happen, and I've been running Ollama like this for around a year now, updating periodically (I updated to the latest version just to see if this was a bug, but did not fix the issue). Am I just doing something obviously dumb now? How can I fix this issue? Any help would be deeply appreciated!&lt;/p&gt; &lt;p&gt;If there's any additional info you need, please let me know. Ideally without permanently losing my LLM chat history, as there's a few chats I need to keep around to reference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A_Zeppelin"&gt; /u/A_Zeppelin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1resqne/need_some_help_fixing_ollama_pull/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1resqne/need_some_help_fixing_ollama_pull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1resqne/need_some_help_fixing_ollama_pull/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T22:52:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdyosq</id>
    <title>I built a locally-hosted AI agent that runs entirely on your own hardware no cloud, no subscriptions</title>
    <updated>2026-02-25T00:56:51+00:00</updated>
    <author>
      <name>/u/Janglerjoe</name>
      <uri>https://old.reddit.com/user/Janglerjoe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built LMAgent a pure Python AI agent that connects to any OpenAI-compatible LLM (LM Studio, Ollama, etc.) and actually does things on your computer.&lt;/p&gt; &lt;p&gt;No cloud. No API fees. No subscriptions. Runs 100% on your own hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it can do autonomously:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Read and write files&lt;/li&gt; &lt;li&gt;Run shell commands (bash / PowerShell)&lt;/li&gt; &lt;li&gt;Manage git (status, diff, commit, branch)&lt;/li&gt; &lt;li&gt;Track todos and multi-step plans&lt;/li&gt; &lt;li&gt;Spawn sub-agents to delegate tasks&lt;/li&gt; &lt;li&gt;Connect to external tools via MCP servers (web search, browsers, databases)&lt;/li&gt; &lt;li&gt;Schedule itself to wake up at a future time and resume work&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Three ways to run it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Terminal REPL ‚Äî conversational loop with a live background scheduler&lt;/li&gt; &lt;li&gt;One-shot CLI ‚Äî give it a task, get a result, exit&lt;/li&gt; &lt;li&gt;Web UI ‚Äî streaming tokens, inline tool calls, session browser, mobile-friendly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup is dead simple:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;pip install requests flask colorama&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Point it at your local LLM server&lt;/li&gt; &lt;li&gt;Set a workspace directory in a &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; &lt;li&gt;Run &lt;code&gt;python agent_main.py&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Works on Windows, macOS, and Linux. MIT licensed.&lt;/p&gt; &lt;p&gt;Would love feedback especially from anyone running it with larger models or unconventional LLM backends.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/janglerjoe-commits/LMAgent"&gt;https://github.com/janglerjoe-commits/LMAgent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Janglerjoe"&gt; /u/Janglerjoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdyosq/i_built_a_locallyhosted_ai_agent_that_runs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdyosq/i_built_a_locallyhosted_ai_agent_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdyosq/i_built_a_locallyhosted_ai_agent_that_runs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T00:56:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rezr97</id>
    <title>Qwen3.5:35b on Apple Silicon: How I Got 2x Faster Inference by Switching from Ollama to MLX (with benchmarks)</title>
    <updated>2026-02-26T03:51:25+00:00</updated>
    <author>
      <name>/u/rockinyp</name>
      <uri>https://old.reddit.com/user/rockinyp</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rockinyp"&gt; /u/rockinyp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1rezq19/qwen3535b_on_apple_silicon_how_i_got_2x_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rezr97/qwen3535b_on_apple_silicon_how_i_got_2x_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rezr97/qwen3535b_on_apple_silicon_how_i_got_2x_faster/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T03:51:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1res0sl</id>
    <title>Qwen 3.5 distilled vs GptOss</title>
    <updated>2026-02-25T22:25:42+00:00</updated>
    <author>
      <name>/u/SubstantialTea707</name>
      <uri>https://old.reddit.com/user/SubstantialTea707</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tried Qwen 3.5 in the 27B, 35B, or 122B versions? How does it perform with tool calling? I‚Äôm currently using GPT-OSS 20B, but especially the 120B version is, in my opinion, unbeatable. I find it very reliable and I‚Äôm running it in production on an RTX Pro 6000. With Qwen 3, I experienced lower reliability and it often went into loops, which made it unsuitable for production. Has anyone already tested it? Could you share real-world usage feedback? Because, as we know, benchmarks don‚Äôt always reflect real use cases. My goal is to run a chatbot with RAG and MCP tool calling.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SubstantialTea707"&gt; /u/SubstantialTea707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1res0sl/qwen_35_distilled_vs_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1res0sl/qwen_35_distilled_vs_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1res0sl/qwen_35_distilled_vs_gptoss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T22:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1reeixj</id>
    <title>From Pikachu to ZYRON: We Built a Fully Local AI Desktop Assistant That Runs Completely Offline</title>
    <updated>2026-02-25T14:22:10+00:00</updated>
    <author>
      <name>/u/No-Mess-8224</name>
      <uri>https://old.reddit.com/user/No-Mess-8224</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago I posted here about a small personal project I was building called Pikachu, a local desktop voice assistant. Since then the project has grown way bigger than I expected, got contributions from some really talented people, and evolved into something much more serious. We renamed it to ZYRON and it has basically turned into a full local AI desktop assistant that runs entirely on your own machine.&lt;/p&gt; &lt;p&gt;The main goal has always been simple. I love the idea of AI assistants, but I hate the idea of my files, voice, screenshots, and daily computer activity being uploaded to cloud services. So we built the opposite. ZYRON runs fully offline using a local LLM through Ollama, and the entire system is designed around privacy first. Nothing gets sent anywhere unless I explicitly ask it to send something to my own Telegram.&lt;/p&gt; &lt;p&gt;You can control the PC with voice by saying a wake word and then speaking normally. It can open apps, control media, set volume, take screenshots, shut down the PC, search the web in the background, and run chained commands like opening a browser and searching something in one go. It also responds back using offline text to speech, which makes it feel surprisingly natural to use day to day.&lt;/p&gt; &lt;p&gt;The remote control side became one of the most interesting parts. From my phone I can message a Telegram bot and basically control my laptop from anywhere. If I forget a file, I can ask it to find the document I opened earlier and it sends the file directly to me. It keeps a 30 day history of file activity and lets me search it using natural language. That feature alone has already saved me multiple times.&lt;/p&gt; &lt;p&gt;We also leaned heavily into security and monitoring. ZYRON can silently capture screenshots, take webcam photos, record short audio clips, and send them to Telegram. If a laptop gets stolen and connects to the internet, it can report IP address, ISP, city, coordinates, and a Google Maps link. Building and testing that part honestly felt surreal the first time it worked.&lt;/p&gt; &lt;p&gt;On the productivity side it turned into a full system monitor. It can report CPU, RAM, battery, storage, running apps, and even read all open browser tabs. There is a clipboard history logger so copied text is never lost. There is a focus mode that kills distracting apps and closes blocked websites automatically. There is even a ‚Äúzombie process‚Äù monitor that detects apps eating RAM in the background and lets you kill them remotely.&lt;/p&gt; &lt;p&gt;One feature I personally love is the stealth research mode. There is a Firefox extension that creates a bridge between the browser and the assistant, so it can quietly open a background tab, read content, and close it without any window appearing. Asking random questions and getting answers from a laptop that looks idle is strangely satisfying.&lt;/p&gt; &lt;p&gt;The whole philosophy of the project is that it does not try to compete with giant cloud models at writing essays. Instead it focuses on being a powerful local system automation assistant that respects privacy. The local model is smaller, but for controlling a computer it is more than enough, and the tradeoff feels worth it.&lt;/p&gt; &lt;p&gt;We are planning a lot next. Linux and macOS support, geofence alerts, motion triggered camera capture, scheduling and automation, longer memory, and eventually a proper mobile companion app instead of Telegram. As local models improve, the assistant will naturally get smarter too.&lt;/p&gt; &lt;p&gt;This started as a weekend experiment and slowly turned into something I now use daily. I would genuinely love feedback, ideas, or criticism from people here. If you have ever wanted an AI assistant that lives only on your own machine, I think you might find this interesting.&lt;/p&gt; &lt;p&gt;GitHub Repo - &lt;a href="https://github.com/Surajkumar5050/zyron-assistant"&gt;Link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mess-8224"&gt; /u/No-Mess-8224 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T14:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf24zc</id>
    <title>Why I can‚Äôt use gpu ?</title>
    <updated>2026-02-26T05:52:06+00:00</updated>
    <author>
      <name>/u/Far-Ebb-8088</name>
      <uri>https://old.reddit.com/user/Far-Ebb-8088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run ollama mainly on my laptop with i7, not very powerful but I have 16 gb of ram, but the gpu is always 0%, how can my laptop use the GPU ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Ebb-8088"&gt; /u/Far-Ebb-8088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf24zc/why_i_cant_use_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf24zc/why_i_cant_use_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf24zc/why_i_cant_use_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T05:52:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf36a8</id>
    <title>Benchmarking qwen3.5:35b vs gpt-oss:20b for Agentic Workloads (Ollama, Apple Silicon)</title>
    <updated>2026-02-26T06:51:20+00:00</updated>
    <author>
      <name>/u/pwbdecker</name>
      <uri>https://old.reddit.com/user/pwbdecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rf36a8/benchmarking_qwen3535b_vs_gptoss20b_for_agentic/"&gt; &lt;img alt="Benchmarking qwen3.5:35b vs gpt-oss:20b for Agentic Workloads (Ollama, Apple Silicon)" src="https://external-preview.redd.it/-W3Te0N3NSI2YKnRqaiMQ7io69snlAPAbJcAbQmfjk4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=782d1f26e2d58963b787b26421e110b13d5dabbc" title="Benchmarking qwen3.5:35b vs gpt-oss:20b for Agentic Workloads (Ollama, Apple Silicon)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pwbdecker"&gt; /u/pwbdecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jaredlockhart/penny/blob/main/docs/benchmarking-qwen35-vs-gpt-oss.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf36a8/benchmarking_qwen3535b_vs_gptoss20b_for_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf36a8/benchmarking_qwen3535b_vs_gptoss20b_for_agentic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T06:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf6ysq</id>
    <title>Ollama Based Private API, CLI, Agent Builder</title>
    <updated>2026-02-26T10:44:48+00:00</updated>
    <author>
      <name>/u/GhostGPT5</name>
      <uri>https://old.reddit.com/user/GhostGPT5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI that doesn't say no. No filter, From Ghost Nano to Ghost Ultra, every model is tuned for direct high-signal output. Stop Getting Censored 8 free prompts a day. No card. No strings. Just start talking. &lt;a href="https://ghostgpt.live"&gt;https://ghostgpt.live&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhostGPT5"&gt; /u/GhostGPT5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf6ysq/ollama_based_private_api_cli_agent_builder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf6ysq/ollama_based_private_api_cli_agent_builder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf6ysq/ollama_based_private_api_cli_agent_builder/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T10:44:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf4k33</id>
    <title>AMBER ICI v3</title>
    <updated>2026-02-26T08:14:00+00:00</updated>
    <author>
      <name>/u/FreonMuskOfficial</name>
      <uri>https://old.reddit.com/user/FreonMuskOfficial</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rf4k33/amber_ici_v3/"&gt; &lt;img alt="AMBER ICI v3" src="https://preview.redd.it/fmd5rvv6sslg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35169090d9acfccbc8e8bc9ebe455a8e771b7397" title="AMBER ICI v3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Ollama locally and want more than just prompts? &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/gs-ai/AMBER-ICI"&gt;https://github.com/gs-ai/AMBER-ICI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AMBER ICI v3 is an industrial grade local command center built for serious builders, giving you multi model orchestration, live token streaming, agent and chain pipelines, archive search, timeline reconstruction, OCR extraction, investigative file ingestion, graph based output correlation, and real time GPU telemetry in one unified interface. It is designed for power users who want to spin up autonomous agents, chain models together, ingest massive archives, and actually see how models think and interact, all while it's fully local and you're in control. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FreonMuskOfficial"&gt; /u/FreonMuskOfficial &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fmd5rvv6sslg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf4k33/amber_ici_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf4k33/amber_ici_v3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T08:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf1y4y</id>
    <title>Might be a bit lost in AI generated instructions.</title>
    <updated>2026-02-26T05:42:10+00:00</updated>
    <author>
      <name>/u/exodist</name>
      <uri>https://old.reddit.com/user/exodist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;I have a strix halo laptop with 128gb shared ram&lt;/li&gt; &lt;li&gt;Arch Linux&lt;/li&gt; &lt;li&gt;I installed ollama, it is working, and when I use it my gpu usage spikes, so it is using my gpu&lt;/li&gt; &lt;li&gt;Per google's recommendation I am using `aider` to connect to ollama to ask it to work on my code&lt;/li&gt; &lt;li&gt;I have tried the following models: &lt;ul&gt; &lt;li&gt;deepseek-coder&lt;/li&gt; &lt;li&gt;codellama&lt;/li&gt; &lt;li&gt;qwen2.5-coder:32b &amp;lt;- this one has worked the best&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;I am not super familiar with AI or its abilities, so I am starting small, asking it to generate a test file for a perl module I wrote. To be super simple I am asking for a test for just one method.&lt;/li&gt; &lt;li&gt;Not happy with any results so far&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It kind of works. deepseek-coder generated garbage perl. I think deepseeks garbage was still in place when I tried codellama, so I do not think I gave it a good trial. For qwen I reset my repo back to before any AI had at it, and it did a pretty good job, and I was able to iterate, but it did start to image perl tools that do not exist.&lt;/p&gt; &lt;p&gt;I decided a good next step would be to give it an understanding of all the interconnected perl repos I maintain. I first just tried to have it look at lib and t in one of my repos, and it decided I was asking it to look at too many files.&lt;/p&gt; &lt;p&gt;I asked google for advice, and ended up installing OpenWebUI and having it create a knowledge base that was all the source from all my repos. But then got stuck cause apparently aider cannot look at that knowledge base.&lt;/p&gt; &lt;p&gt;So now I am kind of stuck. How do I give the AI an understanding of my perl repositories? Ultimately I want to use it to help me migrate some code from a low quality tool written in the repo with a better tool that is in its own repo.&lt;/p&gt; &lt;p&gt;Any advice is appreciated! Just starting my AI journey, so I may be asking stupid questions, or going about this all wrong, I welcome pointers in the right direction.&lt;/p&gt; &lt;p&gt;I do not know much about python, it seems to be one of the few languages I just cannot read well. The lack of braces means I keep overflowing in my brain and think I am still reading one &amp;quot;block&amp;quot; when in fact I have actually moved on to a couple new ones. I mainly mention this because I am kind of lost with installing/using python tools, I installed aider with arch's aur repo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/exodist"&gt; /u/exodist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf1y4y/might_be_a_bit_lost_in_ai_generated_instructions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf1y4y/might_be_a_bit_lost_in_ai_generated_instructions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf1y4y/might_be_a_bit_lost_in_ai_generated_instructions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T05:42:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf61qd</id>
    <title>Made my first project, Autonomous video generator</title>
    <updated>2026-02-26T09:49:00+00:00</updated>
    <author>
      <name>/u/Pronation1227</name>
      <uri>https://old.reddit.com/user/Pronation1227</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Pronation1227/AVB#video-generator-bot"&gt;&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Hi, This is my first project (which i actually managed to complete)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/Pronation1227/AVB#hi-this-is-my-first-project-which-i-actually-managed-to-complete"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;About me: I am in high school and have been coding on and off for a few years now.&lt;/p&gt; &lt;p&gt;a quick overview of this project, its basically a storytime generator inspired from the insta videos you see on reels. There was no real motive behind building this i was just frustrated of tutorial hell and hence built the first thing that came to my mind&lt;/p&gt; &lt;p&gt;I admit i did use AI to help me with structuring the project into different files ie: output, notes, background, scripts. I also used ai for the ffmpeg subprocess in generate_vid.py as i had no idea what ffmpeg is or how to use it. But all other lines of code in all the files have been written by me&lt;/p&gt; &lt;p&gt;Thanks a lot, would really appreciate feedback on what could i improve and where can i learn further.&lt;/p&gt; &lt;p&gt;github - &lt;a href="https://github.com/Pronation1227/AVB"&gt;https://github.com/Pronation1227/AVB&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pronation1227"&gt; /u/Pronation1227 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf61qd/made_my_first_project_autonomous_video_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf61qd/made_my_first_project_autonomous_video_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf61qd/made_my_first_project_autonomous_video_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T09:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf6cc7</id>
    <title>Need help on API key export...</title>
    <updated>2026-02-26T10:07:15+00:00</updated>
    <author>
      <name>/u/Dakacchan_</name>
      <uri>https://old.reddit.com/user/Dakacchan_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rf6cc7/need_help_on_api_key_export/"&gt; &lt;img alt="Need help on API key export..." src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="Need help on API key export..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dakacchan_"&gt; /u/Dakacchan_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1rf6c6u/need_help_on_api_key_export/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf6cc7/need_help_on_api_key_export/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf6cc7/need_help_on_api_key_export/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T10:07:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf84qc</id>
    <title>AVCI GHOST - A CyberSec. UI Experiment for Ollama</title>
    <updated>2026-02-26T11:51:06+00:00</updated>
    <author>
      <name>/u/Ezanyiyenler</name>
      <uri>https://old.reddit.com/user/Ezanyiyenler</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"&gt; &lt;img alt="AVCI GHOST - A CyberSec. UI Experiment for Ollama" src="https://external-preview.redd.it/gAWqWQOlQP9fUw6_szN7BIGdnEqcLZpbQVa7NXlrTNU.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=a1d79284496f099470f492d9512e78cdedff357f" title="AVCI GHOST - A CyberSec. UI Experiment for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I created this simple UI for Ollama as a learning experiment. It's basically a Python script with 10 menu options that interact with local models. IMPORTAT DISCLAIMER:&lt;/p&gt; &lt;p&gt;This is NOT a real hacking tool. It's purely a UI EXPERIMENT. Most modules are SIMULATIONS and don't actually work. It's just a proof-of-concept I built while learning about Ollama and AI models.&lt;br /&gt; For open source code and detailed information, check out my GitHub address &lt;a href="https://github.com/ihsan896/Avci_Ghost"&gt;https://github.com/ihsan896/Avci_Ghost&lt;/a&gt;Don't forget to like if you enjoy üíó&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hysx7ys8utlg1.png?width=981&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=970ec0db987dd8c39ec8d4ac00ba4d07af2f5357"&gt;Just the UI layout - the modules are placeholders/demos. The actual functionality is connecting to local Ollama models. All security features are simulated for educational purposes.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ezanyiyenler"&gt; /u/Ezanyiyenler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T11:51:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfbew8</id>
    <title>Question about installing ollama Claude</title>
    <updated>2026-02-26T14:21:03+00:00</updated>
    <author>
      <name>/u/PerformerAromatic836</name>
      <uri>https://old.reddit.com/user/PerformerAromatic836</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i am installing the ollama/claude local thing. The last question of the claude config is the question of &amp;quot;do you trust this folder&amp;quot;, it asks for access to my entire users/myname folder, whilst I don't want that. Is there a way to give claude/ollama only access to a certain folder with zero documents in it yet, so that claude/ollama will not have access to any personal documents?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerformerAromatic836"&gt; /u/PerformerAromatic836 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfbew8/question_about_installing_ollama_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfbew8/question_about_installing_ollama_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfbew8/question_about_installing_ollama_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T14:21:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1reupji</id>
    <title>I built a multi-agent eval lab where 3 LLM personas race, get judged with evidence snippets, and learn from losses ‚Äî open source</title>
    <updated>2026-02-26T00:09:36+00:00</updated>
    <author>
      <name>/u/Sufficient-Title-912</name>
      <uri>https://old.reddit.com/user/Sufficient-Title-912</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1reupji/i_built_a_multiagent_eval_lab_where_3_llm/"&gt; &lt;img alt="I built a multi-agent eval lab where 3 LLM personas race, get judged with evidence snippets, and learn from losses ‚Äî open source" src="https://preview.redd.it/t6whtryqdqlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe05cf136e53347ef53d0d7a5024e236a91f8aaf" title="I built a multi-agent eval lab where 3 LLM personas race, get judged with evidence snippets, and learn from losses ‚Äî open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most LLM apps give you one answer. You trust it. You move on.&lt;/p&gt; &lt;p&gt;But you never know if it was the *best* answer ‚Äî or why.&lt;/p&gt; &lt;p&gt;I built Agent Strategy Lab to fix that.&lt;/p&gt; &lt;p&gt;Instead of one model, three agents race on the same prompt in parallel:&lt;/p&gt; &lt;p&gt;‚Ä¢ The Analyst&lt;/p&gt; &lt;p&gt;‚Ä¢ The Lateral Thinker&lt;/p&gt; &lt;p&gt;‚Ä¢ The Devil's Advocate&lt;/p&gt; &lt;p&gt;Each one reasons out loud, uses tools (web search, code execution, calculator), and gets scored by a judge panel on accuracy, completeness, clarity, and insight ‚Äî with evidence snippets, not vibes.&lt;/p&gt; &lt;p&gt;Here's what makes it different:&lt;/p&gt; &lt;p&gt;‚úÖ Evidence-backed judging ‚Äî every score comes with a quoted reason&lt;/p&gt; &lt;p&gt;‚úÖ Consensus mode ‚Äî 3 judge panels, median aggregation&lt;/p&gt; &lt;p&gt;‚úÖ Confidence gate ‚Äî low-confidence winners get flagged before learning updates&lt;/p&gt; &lt;p&gt;‚úÖ Strategy replay ‚Äî rerun a winning agent's tool sequence on a new prompt and measure the lift&lt;/p&gt; &lt;p&gt;‚úÖ Loss pattern capture ‚Äî we learn from losers too, not just winners&lt;/p&gt; &lt;p&gt;The system supports Anthropic, Gemini, and OpenAI, with domain-aware routing for coding, finance, research, and more.&lt;/p&gt; &lt;p&gt;Built with: Node.js + TypeScript + Express + socket io + React + Prisma + SQLite&lt;/p&gt; &lt;p&gt;Full implementation log and architecture docs are in the repo.&lt;/p&gt; &lt;p&gt;Happy to answer questions or connect with anyone working on multi-agent evaluation, LLM routing, or trust/transparency in AI systems. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sufficient-Title-912"&gt; /u/Sufficient-Title-912 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t6whtryqdqlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1reupji/i_built_a_multiagent_eval_lab_where_3_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1reupji/i_built_a_multiagent_eval_lab_where_3_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T00:09:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1rew6hl</id>
    <title>A fully visual, private and local AI Creative Suite. No cloud, no subscriptions, runs on your hardware.</title>
    <updated>2026-02-26T01:10:32+00:00</updated>
    <author>
      <name>/u/Ollie_IDE</name>
      <uri>https://old.reddit.com/user/Ollie_IDE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We‚Äôve been working on a desktop application for those of us who want to use AI models locally but prefer a full visual interface. It‚Äôs called Ollie, and it‚Äôs an offline-first creative suite that runs entirely on your machine.&lt;/p&gt; &lt;p&gt;What it includes:&lt;/p&gt; &lt;p&gt;Code: A coding environment (Node, Python, Java) with IntelliSense. You can also ask it to instantly generate and run interactive apps.&lt;/p&gt; &lt;p&gt;Media Suite: It has built-in video, image canvas, and a 3D editor.&lt;/p&gt; &lt;p&gt;Rich Text: A distraction-free markdown and writing environment that keeps your project context local.&lt;/p&gt; &lt;p&gt;Under the hood&lt;/p&gt; &lt;p&gt;Ollama Native: Hooks directly into your local Ollama setup.&lt;/p&gt; &lt;p&gt;Bring Your Own Keys: If you need to use Anthropic, Gemini, or OpenAI, you can plug in your API key directly.&lt;/p&gt; &lt;p&gt;Agent &amp;amp; MCP Support: Connects to GitHub, local databases, and custom tools via the Model Context Protocol (MCP).&lt;/p&gt; &lt;p&gt;&amp;quot;Glass-Box&amp;quot; UI: You can visually audit every file touch, tool call, and token before the AI executes it.&lt;/p&gt; &lt;p&gt;It runs natively on macOS, Windows, and Linux. Because it relies on your hardware, there are no recurring subscriptions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://costa-and-associates.com/ollie"&gt;Ollie&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ollie_IDE"&gt; /u/Ollie_IDE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rew6hl/a_fully_visual_private_and_local_ai_creative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rew6hl/a_fully_visual_private_and_local_ai_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rew6hl/a_fully_visual_private_and_local_ai_creative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T01:10:32+00:00</published>
  </entry>
</feed>
