<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-14T19:06:31+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ov7kqc</id>
    <title>Masking the connection error in Ollama</title>
    <updated>2025-11-12T15:05:45+00:00</updated>
    <author>
      <name>/u/chirchan91</name>
      <uri>https://old.reddit.com/user/chirchan91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a simple way of masking the connection errors in Ollama when it failed to connect to the Model Server.&lt;/p&gt; &lt;p&gt;for example: Head &amp;quot;http://&amp;lt;internal‚Äëip&amp;gt;:11434/&amp;quot;: dial tcp &amp;lt;internal‚Äëip&amp;gt;:11434: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.&lt;/p&gt; &lt;p&gt;instead i should get &amp;quot;connection failed due host not responding. Please contact support&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chirchan91"&gt; /u/chirchan91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7kqc/masking_the_connection_error_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7kqc/masking_the_connection_error_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov7kqc/masking_the_connection_error_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T15:05:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov7t8l</id>
    <title>MCP Server for Blender - Built for PolyMCP Agent Orchestration</title>
    <updated>2025-11-12T15:14:21+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ov7t8l/mcp_server_for_blender_built_for_polymcp_agent/"&gt; &lt;img alt="MCP Server for Blender - Built for PolyMCP Agent Orchestration" src="https://external-preview.redd.it/JIrsfAZEJFJcSC_-Ows_UR3v_W3m64JAE2m0e3rAoNc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1896a8509f0f381f92c8934d66e06ad80e365455" title="MCP Server for Blender - Built for PolyMCP Agent Orchestration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/Blender-MCP-Server"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7t8l/mcp_server_for_blender_built_for_polymcp_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov7t8l/mcp_server_for_blender_built_for_polymcp_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T15:14:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov16ju</id>
    <title>Someone wrote an article about my library PolyMCP</title>
    <updated>2025-11-12T09:58:43+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ov16ju/someone_wrote_an_article_about_my_library_polymcp/"&gt; &lt;img alt="Someone wrote an article about my library PolyMCP" src="https://external-preview.redd.it/y0dgtZ0gVjnRje7BRhie_7gJWNn7zzoil-XFJAfPX90.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80e455756943072492fbf6c6135f659142eced5f" title="Someone wrote an article about my library PolyMCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://levelup.gitconnected.com/why-your-python-functions-arent-ai-tools-yet-and-how-polymcp-fixes-it-in-one-line-d8e62550ac53"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov16ju/someone_wrote_an_article_about_my_library_polymcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov16ju/someone_wrote_an_article_about_my_library_polymcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T09:58:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovdpch</id>
    <title>Is anyone from London?</title>
    <updated>2025-11-12T18:47:26+00:00</updated>
    <author>
      <name>/u/Dry_Music_7160</name>
      <uri>https://old.reddit.com/user/Dry_Music_7160</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Music_7160"&gt; /u/Dry_Music_7160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1ovdp64/is_anyone_from_london/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovdpch/is_anyone_from_london/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovdpch/is_anyone_from_london/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T18:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov62o7</id>
    <title>I taught my AI vtuber how to play osu! Here's how it went...</title>
    <updated>2025-11-12T14:07:41+00:00</updated>
    <author>
      <name>/u/imfstr</name>
      <uri>https://old.reddit.com/user/imfstr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ov62o7/i_taught_my_ai_vtuber_how_to_play_osu_heres_how/"&gt; &lt;img alt="I taught my AI vtuber how to play osu! Here's how it went..." src="https://external-preview.redd.it/kfbIE7cVIUZ7rju-fJnsEvS2hqkafGslwwkxU3XNQtc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb693bfc94522d852a0f35b056c144ca0965d28f" title="I taught my AI vtuber how to play osu! Here's how it went..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;some of you might remember my last post where i showed my AI, &lt;em&gt;Eris&lt;/em&gt;, picking out her dream PC setup on amazon. since then, i‚Äôve been working on something a bit crazier, i decided to teach her how to &lt;strong&gt;play osu!&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;in the video, she chats with me for a bit and then actually plays through &lt;strong&gt;two&lt;/strong&gt; osu! maps using a neural network I integrated into her system. It was a big leap from where she was before, and i learned a ton about AI decision-making, timing, and visual input in the process. &lt;/p&gt; &lt;p&gt;i‚Äôm always open to feedback, whether it‚Äôs about how she looks/it's animated, all the way to how she should respond, possible improvements to her interactivity, or just general advice for the project. &lt;/p&gt; &lt;p&gt;thanks again to everyone who gave feedback last time, it really helped a lot! :D&lt;/p&gt; &lt;p&gt;(she plays in an offline environment, and none of her scores get uploaded publicly incase it seems like this is cheating)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/imfstr"&gt; /u/imfstr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=R778HLEGeWg&amp;amp;t=2s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov62o7/i_taught_my_ai_vtuber_how_to_play_osu_heres_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov62o7/i_taught_my_ai_vtuber_how_to_play_osu_heres_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T14:07:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov7wch</id>
    <title>How high of a spec do you have to have in order to install ollama in local environment?</title>
    <updated>2025-11-12T15:17:29+00:00</updated>
    <author>
      <name>/u/SnooRegrets3378</name>
      <uri>https://old.reddit.com/user/SnooRegrets3378</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently work in a virtual machine environment where any website is unavailable. Is it possible to bring ollama into this type of setting? Exasperated by having to do everything with excel when you can use ai models and i work with sensitive datas so i would have to do the work locally.&lt;/p&gt; &lt;p&gt;Sorry in advance for the possible inaccurate word choice im not a computer guy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooRegrets3378"&gt; /u/SnooRegrets3378 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7wch/how_high_of_a_spec_do_you_have_to_have_in_order/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7wch/how_high_of_a_spec_do_you_have_to_have_in_order/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov7wch/how_high_of_a_spec_do_you_have_to_have_in_order/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T15:17:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovmy2a</id>
    <title>Nothink in the gui</title>
    <updated>2025-11-13T00:43:19+00:00</updated>
    <author>
      <name>/u/sceadwian</name>
      <uri>https://old.reddit.com/user/sceadwian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to /set nothink in the ollama GUI? I'm not seeing any places to pass command line paramaters and it doesn't take the command in the chat dialog box.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sceadwian"&gt; /u/sceadwian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovmy2a/nothink_in_the_gui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovmy2a/nothink_in_the_gui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovmy2a/nothink_in_the_gui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T00:43:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovt199</id>
    <title>Local Llama API</title>
    <updated>2025-11-13T05:36:53+00:00</updated>
    <author>
      <name>/u/TuLiSTua</name>
      <uri>https://old.reddit.com/user/TuLiSTua</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ovt199/local_llama_api/"&gt; &lt;img alt="Local Llama API" src="https://preview.redd.it/o87hysrgoy0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75d5a1be150cde933fbeac9cec14dee93e4c2a45" title="Local Llama API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following Situation: -self hosted Tandoor Recipes 2.3.3 instance -self hosted Ollama instance with llama3.2:latest&lt;/p&gt; &lt;p&gt;I want Tandoor to use my local AI to work as AI provider, but i need an API. The question is, where do I get it from?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TuLiSTua"&gt; /u/TuLiSTua &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o87hysrgoy0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovt199/local_llama_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovt199/local_llama_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T05:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow0x3g</id>
    <title>Thanks to Gowtham Boyina for featuring my library in his latest article üôè</title>
    <updated>2025-11-13T13:21:57+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ow0x3g/thanks_to_gowtham_boyina_for_featuring_my_library/"&gt; &lt;img alt="Thanks to Gowtham Boyina for featuring my library in his latest article üôè" src="https://external-preview.redd.it/y0dgtZ0gVjnRje7BRhie_7gJWNn7zzoil-XFJAfPX90.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80e455756943072492fbf6c6135f659142eced5f" title="Thanks to Gowtham Boyina for featuring my library in his latest article üôè" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://levelup.gitconnected.com/why-your-python-functions-arent-ai-tools-yet-and-how-polymcp-fixes-it-in-one-line-d8e62550ac53"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow0x3g/thanks_to_gowtham_boyina_for_featuring_my_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow0x3g/thanks_to_gowtham_boyina_for_featuring_my_library/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T13:21:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow31je</id>
    <title>Anyone running code model in cpu only VPS?</title>
    <updated>2025-11-13T14:50:13+00:00</updated>
    <author>
      <name>/u/Gcloud-AI</name>
      <uri>https://old.reddit.com/user/Gcloud-AI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to use codeseeker model in my vps but it's not good, not able to get any output from my model üòî. My vps spec is: 8 cpu core 32gb ram 1tb nvme storage Any guidance for me???&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gcloud-AI"&gt; /u/Gcloud-AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow31je/anyone_running_code_model_in_cpu_only_vps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow31je/anyone_running_code_model_in_cpu_only_vps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow31je/anyone_running_code_model_in_cpu_only_vps/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T14:50:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow3ne9</id>
    <title>Qual a melhor GPU para o llama 3(.1 ou .3)</title>
    <updated>2025-11-13T15:13:39+00:00</updated>
    <author>
      <name>/u/No_Progress432</name>
      <uri>https://old.reddit.com/user/No_Progress432</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Progress432"&gt; /u/No_Progress432 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ow3my9/qual_a_melhor_gpu_para_o_llama_31_ou_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow3ne9/qual_a_melhor_gpu_para_o_llama_31_ou_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow3ne9/qual_a_melhor_gpu_para_o_llama_31_ou_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T15:13:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovyfb6</id>
    <title>Everything at Your Fingertips: How PolyMCP Makes Multi-Tool AI Simple</title>
    <updated>2025-11-13T11:14:36+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ovyfb6/everything_at_your_fingertips_how_polymcp_makes/"&gt; &lt;img alt="Everything at Your Fingertips: How PolyMCP Makes Multi-Tool AI Simple" src="https://external-preview.redd.it/OsRUlAPOIfUajUERak6rJLdgfe46_oa2w3fJR_8dpWg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0bc545ee48ac58062b8aa9b228116950bbd4c676" title="Everything at Your Fingertips: How PolyMCP Makes Multi-Tool AI Simple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/Polymcp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovyfb6/everything_at_your_fingertips_how_polymcp_makes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovyfb6/everything_at_your_fingertips_how_polymcp_makes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T11:14:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow5e4z</id>
    <title>MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration</title>
    <updated>2025-11-13T16:20:51+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ow5e4z/mcp_server_for_industrial_iot_built_for_polymcp/"&gt; &lt;img alt="MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration" src="https://external-preview.redd.it/7DcHkpBMRVBJAoq05xem0Cu6v1pmCb6s2RmtluiBv_4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84327b5adc51d58c3e3d8bd34d3475931cf4f24a" title="MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/IoT-Edge-MCP-Server"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow5e4z/mcp_server_for_industrial_iot_built_for_polymcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow5e4z/mcp_server_for_industrial_iot_built_for_polymcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T16:20:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1owcm4k</id>
    <title>Can't find Model in Ollama</title>
    <updated>2025-11-13T20:51:35+00:00</updated>
    <author>
      <name>/u/dissmami</name>
      <uri>https://old.reddit.com/user/dissmami</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1owcm4k/cant_find_model_in_ollama/"&gt; &lt;img alt="Can't find Model in Ollama" src="https://a.thumbs.redditmedia.com/QbxUx1f9O6EMpaAGb1OChkJFe9G_94B_EhqL1IhiUH0.jpg" title="Can't find Model in Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I use &amp;quot;Ollama list&amp;quot; the latest model I downloaded doesnt show. But when I try to redownload the model it says that the model already exists.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zifob79n731g1.png?width=1444&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=481fe9c76ed0520b66cff4cf327f4815a67fe4bf"&gt;https://preview.redd.it/zifob79n731g1.png?width=1444&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=481fe9c76ed0520b66cff4cf327f4815a67fe4bf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dissmami"&gt; /u/dissmami &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owcm4k/cant_find_model_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owcm4k/cant_find_model_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owcm4k/cant_find_model_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T20:51:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1owliwq</id>
    <title>Mimir - Parallel Agent task orchestration - Drag and drop UI (preview)</title>
    <updated>2025-11-14T03:16:09+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1owliwq/mimir_parallel_agent_task_orchestration_drag_and/"&gt; &lt;img alt="Mimir - Parallel Agent task orchestration - Drag and drop UI (preview)" src="https://preview.redd.it/zddnm50z351g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a523074097ef22f041a6b965715a2f35a15bc328" title="Mimir - Parallel Agent task orchestration - Drag and drop UI (preview)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zddnm50z351g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owliwq/mimir_parallel_agent_task_orchestration_drag_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owliwq/mimir_parallel_agent_task_orchestration_drag_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T03:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1owlkzg</id>
    <title>An opinionated, minimalist agentic TUI</title>
    <updated>2025-11-14T03:18:55+00:00</updated>
    <author>
      <name>/u/uwhkdb</name>
      <uri>https://old.reddit.com/user/uwhkdb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1owlkzg/an_opinionated_minimalist_agentic_tui/"&gt; &lt;img alt="An opinionated, minimalist agentic TUI" src="https://preview.redd.it/arf15nh4v41g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=f82f95f01c4c515fe36bcd03902dc7b1b2c060f2" title="An opinionated, minimalist agentic TUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uwhkdb"&gt; /u/uwhkdb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/arf15nh4v41g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owlkzg/an_opinionated_minimalist_agentic_tui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owlkzg/an_opinionated_minimalist_agentic_tui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T03:18:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1owjmiu</id>
    <title>Use case-analyze my energy use to plan a solar panel/ battery setup</title>
    <updated>2025-11-14T01:46:43+00:00</updated>
    <author>
      <name>/u/SaltbushBillJP</name>
      <uri>https://old.reddit.com/user/SaltbushBillJP</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Be gentle, noob here. How's this for an AI use case? &lt;/p&gt; &lt;p&gt;I want to have my last 12 months of electricity bills summarised, to understand total energy consumption and average daily consumption. &lt;/p&gt; &lt;p&gt;I want to use the summary as an input to determine whether or not to proceed with an investment in solar panels and a battery, from there to determine size of system and determine time of payback on the system. &lt;/p&gt; &lt;p&gt;I'm happy to be told whatever you can share. Thanks in advance for your generosity and patience!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SaltbushBillJP"&gt; /u/SaltbushBillJP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owjmiu/use_caseanalyze_my_energy_use_to_plan_a_solar/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owjmiu/use_caseanalyze_my_energy_use_to_plan_a_solar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owjmiu/use_caseanalyze_my_energy_use_to_plan_a_solar/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T01:46:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovzgqx</id>
    <title>Most powerful LLM for 10GB RTX 3080?</title>
    <updated>2025-11-13T12:12:34+00:00</updated>
    <author>
      <name>/u/HUG0gamingHD</name>
      <uri>https://old.reddit.com/user/HUG0gamingHD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for a llm that can fully take advantage of this gpu.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HUG0gamingHD"&gt; /u/HUG0gamingHD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovzgqx/most_powerful_llm_for_10gb_rtx_3080/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovzgqx/most_powerful_llm_for_10gb_rtx_3080/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovzgqx/most_powerful_llm_for_10gb_rtx_3080/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T12:12:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1owqa4u</id>
    <title>Idea validation: ‚ÄúRAG as a Service‚Äù for AI agents. Would you use it?</title>
    <updated>2025-11-14T07:38:04+00:00</updated>
    <author>
      <name>/u/Feisty-Promise-78</name>
      <uri>https://old.reddit.com/user/Feisty-Promise-78</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm exploring an idea and would like some feedback before building the full thing.&lt;/p&gt; &lt;p&gt;The concept is a simple, developer-focused &lt;strong&gt;‚ÄúRAG as a Service‚Äù&lt;/strong&gt; that handles all the messy parts of retrieval-augmented generation:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Upload files (PDF, text, markdown, docs)&lt;/li&gt; &lt;li&gt;Automatic text extraction, chunking, and embedding&lt;/li&gt; &lt;li&gt;Support for &lt;strong&gt;multiple embedding providers&lt;/strong&gt; (OpenAI, Cohere, etc.)&lt;/li&gt; &lt;li&gt;Support for &lt;strong&gt;different search/query techniques&lt;/strong&gt; (vector search, hybrid, keyword, etc.)&lt;/li&gt; &lt;li&gt;Ability to &lt;strong&gt;compare and evaluate different RAG configurations&lt;/strong&gt; to choose the best one for your agent&lt;/li&gt; &lt;li&gt;Clean REST API + SDKs + MCP integration&lt;/li&gt; &lt;li&gt;Web dashboard where you can test queries in a chat interface&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically: an easy way to plug RAG into your agent workflows without maintaining any retrieval infrastructure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I‚Äôd like feedback on:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Would a flexible, developer-focused ‚ÄúRAG as a Service‚Äù be useful in your AI agent projects?&lt;/li&gt; &lt;li&gt;How important is the ability to switch between embedding providers and search techniques?&lt;/li&gt; &lt;li&gt;Would an evaluation/benchmarking feature help you choose the best RAG setup for your agent?&lt;/li&gt; &lt;li&gt;Which interface would you want to use: API, SDK, MCP, or dashboard chat?&lt;/li&gt; &lt;li&gt;What would you realistically be willing to pay for 100MB of file for something like this? (Monthly or per-usage pricing)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôd appreciate any thoughts, especially from people building agents, copilots, or internal AI tools.&lt;/p&gt; &lt;p&gt;Of course, it will be &lt;strong&gt;open-source&lt;/strong&gt;üòä&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feisty-Promise-78"&gt; /u/Feisty-Promise-78 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owqa4u/idea_validation_rag_as_a_service_for_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owqa4u/idea_validation_rag_as_a_service_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owqa4u/idea_validation_rag_as_a_service_for_ai_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T07:38:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1owtagc</id>
    <title>MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration</title>
    <updated>2025-11-14T10:48:20+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1owtagc/mcp_server_for_industrial_iot_built_for_polymcp/"&gt; &lt;img alt="MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration" src="https://external-preview.redd.it/ewQJDfU8pKtPoNJ1-_eOSne6U7qS0Zul-LMlgBujvaM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=988638488b2d46b404f2eb0886e999ea0a35e693" title="MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/IoT-Edge-MCP-Server"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owtagc/mcp_server_for_industrial_iot_built_for_polymcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owtagc/mcp_server_for_industrial_iot_built_for_polymcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T10:48:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1owtoge</id>
    <title>Thanks for 24 Stars for Polymcp! üöÄ</title>
    <updated>2025-11-14T11:10:31+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1owtoge/thanks_for_24_stars_for_polymcp/"&gt; &lt;img alt="Thanks for 24 Stars for Polymcp! üöÄ" src="https://external-preview.redd.it/0UAAIIRPuRc1eQD9AHD1hC2Y49-xOfdPSmxtUygoh_8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fe5089fd3b48279973e5a30119e8ceb6d38bb9f" title="Thanks for 24 Stars for Polymcp! üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/Polymcp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owtoge/thanks_for_24_stars_for_polymcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owtoge/thanks_for_24_stars_for_polymcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T11:10:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow89ph</id>
    <title>We're visualizing what local LLMs actually do when they run - reality check needed</title>
    <updated>2025-11-13T18:07:42+00:00</updated>
    <author>
      <name>/u/That-Vanilla1513</name>
      <uri>https://old.reddit.com/user/That-Vanilla1513</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;We're building an open source tool that visualizes the internal process of local LLM inference in real-time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; Everyone's running Ollama models, tweaking parameters, switching between Llama/Mistral/whatever - but nobody actually &lt;em&gt;sees&lt;/em&gt; what's happening under the hood. You're flying blind.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What we're building:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time visualization of token processing as your model generates responses&lt;/li&gt; &lt;li&gt;Attention pattern maps showing what the model &amp;quot;focuses on&amp;quot;&lt;/li&gt; &lt;li&gt;Resource usage breakdown (CPU/GPU/RAM) per inference step&lt;/li&gt; &lt;li&gt;Bottleneck detection for performance optimization&lt;/li&gt; &lt;li&gt;Side-by-side comparison when testing different models/params&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; Our tool hooks into Ollama's API and captures the inference process, then renders it as an interactive spider-web style visualization. You can pause, rewind, and explore exactly why your model gave a specific response.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current status:&lt;/strong&gt; We are currently actively developing V1 of our product. We plan to integrate it with major LLM models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why we're posting:&lt;/strong&gt; I need a reality check from people who actually run local models daily.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Be brutally honest:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is &amp;quot;I don't know what my model is doing&amp;quot; actually a problem you have, or are you fine with black-box inference?&lt;/li&gt; &lt;li&gt;Would visualization help you debug, optimize, or pick models - or is this just cool but useless?&lt;/li&gt; &lt;li&gt;If you'd use this, what's the ONE feature that would make it essential vs. just interesting?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're not trying to sell anything, we're just trying to figure out if we're solving a real problem or building something nobody needs.&lt;/p&gt; &lt;p&gt;Links and demo video in the comments.&lt;/p&gt; &lt;p&gt;Thanks for keeping it real. üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/That-Vanilla1513"&gt; /u/That-Vanilla1513 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow89ph/were_visualizing_what_local_llms_actually_do_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow89ph/were_visualizing_what_local_llms_actually_do_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow89ph/were_visualizing_what_local_llms_actually_do_when/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T18:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox403x</id>
    <title>is ollama supposed to work out of the box for a 7800 XT?</title>
    <updated>2025-11-14T18:12:41+00:00</updated>
    <author>
      <name>/u/ZdrytchX</name>
      <uri>https://old.reddit.com/user/ZdrytchX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Turns out its been running cpu this entire time despite the 7800XT being on the &lt;a href="https://ollama.com/blog/amd-preview"&gt;supported&lt;/a&gt; list.&lt;/p&gt; &lt;p&gt;On the side note, WebGPU fails for some reason with my gpu, corrupting output from some things like &lt;a href="https://huggingface.co/spaces/webml-community/kokoro-webgpu"&gt;kokoro&lt;/a&gt;, but the gpu runs video games fine aside from the usual videogame-specific hickups &lt;sup&gt;specifically war thunder crashes on alt-tab sometimes and il-2sturmovik has stuttering since windows 11 'downgrade'&lt;/sup&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZdrytchX"&gt; /u/ZdrytchX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ox403x/is_ollama_supposed_to_work_out_of_the_box_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ox403x/is_ollama_supposed_to_work_out_of_the_box_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ox403x/is_ollama_supposed_to_work_out_of_the_box_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T18:12:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1owik8l</id>
    <title>Using my entire source code library in my LLM</title>
    <updated>2025-11-14T00:57:48+00:00</updated>
    <author>
      <name>/u/phoenixfire425</name>
      <uri>https://old.reddit.com/user/phoenixfire425</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have about 25years of my code I would like to be able to have my local ollama instance either trained on or possibly RAG?&lt;/p&gt; &lt;p&gt;My goal is so be able access examples of my previous code by asking questions like I do now with things like qwen or gpt-oss.&lt;/p&gt; &lt;p&gt;Most of my stuff is python and .net stack.&lt;/p&gt; &lt;p&gt;There have been so many times where I know I did something before and it required some crafty work arounds, but I don‚Äôt recall the project. I would love to be able to us all that code as a resource.&lt;/p&gt; &lt;p&gt;My setup is Ollama and OpenWebui on Linux Mint with a RTX 3090 and GTX 1050(used just for memory personalization in openwebui)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phoenixfire425"&gt; /u/phoenixfire425 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owik8l/using_my_entire_source_code_library_in_my_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owik8l/using_my_entire_source_code_library_in_my_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owik8l/using_my_entire_source_code_library_in_my_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T00:57:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1owymhr</id>
    <title>distil-localdoc.py - SLM assistant for writing Python documentation</title>
    <updated>2025-11-14T14:55:01+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1owymhr/distillocaldocpy_slm_assistant_for_writing_python/"&gt; &lt;img alt="distil-localdoc.py - SLM assistant for writing Python documentation" src="https://preview.redd.it/c1ufxihvk81g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d2a67cbfadc9b786a0817740c4adc2cc7675bf9" title="distil-localdoc.py - SLM assistant for writing Python documentation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an SLM assistant for automatic Python documentation - a Qwen3 0.6B parameter model that generates complete, properly formatted docstrings for your code in Google style. Run it locally, keeping your proprietary code secure! Find it at &lt;a href="https://github.com/distil-labs/distil-localdoc.py"&gt;https://github.com/distil-labs/distil-localdoc.py&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Usage&lt;/h2&gt; &lt;p&gt;We load the model and your Python file. By default we load the downloaded Qwen3 0.6B model and generate Google-style docstrings.&lt;/p&gt; &lt;p&gt;```bash python localdoc.py --file your_script.py&lt;/p&gt; &lt;h1&gt;optionally, specify model and docstring style&lt;/h1&gt; &lt;p&gt;python localdoc.py --file your_script.py --model localdoc_qwen3 --style google ```&lt;/p&gt; &lt;p&gt;The tool will generate an updated file with &lt;code&gt;_documented&lt;/code&gt; suffix (e.g., &lt;code&gt;your_script_documented.py&lt;/code&gt;).&lt;/p&gt; &lt;h2&gt;Features&lt;/h2&gt; &lt;p&gt;The assistant can generate docstrings for: - &lt;strong&gt;Functions&lt;/strong&gt;: Complete parameter descriptions, return values, and raised exceptions - &lt;strong&gt;Methods&lt;/strong&gt;: Instance and class method documentation with proper formatting. The tool skips double underscore (dunder: __xxx) methods.&lt;/p&gt; &lt;h2&gt;Examples&lt;/h2&gt; &lt;p&gt;Feel free to run them yourself using the files in [examples](examples)&lt;/p&gt; &lt;h3&gt;Before:&lt;/h3&gt; &lt;p&gt;&lt;code&gt;python def calculate_total(items, tax_rate=0.08, discount=None): subtotal = sum(item['price'] * item['quantity'] for item in items) if discount: subtotal *= (1 - discount) return subtotal * (1 + tax_rate) &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;After (Google style):&lt;/h3&gt; &lt;p&gt;```python def calculate_total(items, tax_rate=0.08, discount=None): &amp;quot;&amp;quot;&amp;quot; Calculate the total cost of items, applying a tax rate and optionally a discount.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Args: items: List of item objects with price and quantity tax_rate: Tax rate expressed as a decimal (default 0.08) discount: Discount rate expressed as a decimal; if provided, the subtotal is multiplied by (1 - discount) Returns: Total amount after applying the tax Example: &amp;gt;&amp;gt;&amp;gt; items = [{'price': 10, 'quantity': 2}, {'price': 5, 'quantity': 1}] &amp;gt;&amp;gt;&amp;gt; calculate_total(items, tax_rate=0.1, discount=0.05) 22.5 &amp;quot;&amp;quot;&amp;quot; subtotal = sum(item['price'] * item['quantity'] for item in items) if discount: subtotal *= (1 - discount) return subtotal * (1 + tax_rate) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;h2&gt;FAQ&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Q: Why don't we just use GPT-4/Claude API for this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Because your proprietary code shouldn't leave your infrastructure. Cloud APIs create security risks, compliance issues, and ongoing costs. Our models run locally with comparable quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Can I document existing docstrings or update them?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Currently, the tool only adds missing docstrings. Updating existing documentation is planned for future releases. For now, you can manually remove docstrings you want regenerated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Which docstring style can I use?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Google&lt;/strong&gt;: Most readable, great for general Python projects&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Q: The model does not work as expected&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: The tool calling on our platform is in active development! &lt;a href="https://www.linkedin.com/company/distil-labs/"&gt;Follow us on LinkedIn&lt;/a&gt; for updates, or &lt;a href="https://join.slack.com/t/distil-labs-community/shared_invite/zt-36zqj87le-i3quWUn2bjErRq22xoE58g"&gt;join our community&lt;/a&gt;. You can also manually refine any generated docstrings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Can you train a model for my company's documentation standards?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: Visit our &lt;a href="https://www.distillabs.ai"&gt;website&lt;/a&gt; and reach out to us, we offer custom solutions tailored to your coding standards and domain-specific requirements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Does this support type hints or other Python documentation tools?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: Type hints are parsed and incorporated into docstrings. Integration with tools like pydoc, Sphinx, and MkDocs is on our roadmap.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c1ufxihvk81g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owymhr/distillocaldocpy_slm_assistant_for_writing_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owymhr/distillocaldocpy_slm_assistant_for_writing_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T14:55:01+00:00</published>
  </entry>
</feed>
