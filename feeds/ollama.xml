<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-04T04:48:44+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1n4t8bm</id>
    <title>First known AI-powered ransomware. Ollama API + gpt-oss-20b</title>
    <updated>2025-08-31T12:19:05+00:00</updated>
    <author>
      <name>/u/Cryptodude2000</name>
      <uri>https://old.reddit.com/user/Cryptodude2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The PromptLock malware uses the gpt-oss-20b model from OpenAI locally via the Ollama API&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.welivesecurity.com/en/ransomware/first-known-ai-powered-ransomware-uncovered-eset-research/"&gt;https://www.welivesecurity.com/en/ransomware/first-known-ai-powered-ransomware-uncovered-eset-research/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cryptodude2000"&gt; /u/Cryptodude2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4t8bm/first_known_aipowered_ransomware_ollama_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4t8bm/first_known_aipowered_ransomware_ollama_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n4t8bm/first_known_aipowered_ransomware_ollama_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T12:19:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n57pbc</id>
    <title>Bringing Computer Use to the Web</title>
    <updated>2025-08-31T22:14:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n57pbc/bringing_computer_use_to_the_web/"&gt; &lt;img alt="Bringing Computer Use to the Web" src="https://external-preview.redd.it/ODYzMWt4Y3lpZm1mMRcxEnlpDBBJVNjXlCDC4HUtgXjfB5ufLszRpp9PEi0H.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54329c72afdc61a3bc07c7b6937dd7206ee726cc" title="Bringing Computer Use to the Web" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bringing Computer Use to the Web: control cloud desktops from JavaScript/TypeScript, right in the browser.&lt;/p&gt; &lt;p&gt;Until today computer-use was Python only, shutting out web devs. Now you can automate real UIs without servers, VMs, or weird work arounds.&lt;/p&gt; &lt;p&gt;What you can build: Pixel-perfect UI tests, Live AI demos, In app assistants that actually move the cursor, or parallel automation streams for heavy workloads.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/bringing-computer-use-to-the-web"&gt;https://www.trycua.com/blog/bringing-computer-use-to-the-web&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xk2qkemyifmf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n57pbc/bringing_computer_use_to_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n57pbc/bringing_computer_use_to_the_web/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T22:14:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5evd7</id>
    <title>Why gpt-oss uses CPU more than GPU on the Windows 11</title>
    <updated>2025-09-01T04:08:32+00:00</updated>
    <author>
      <name>/u/seal2002</name>
      <uri>https://old.reddit.com/user/seal2002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n5evd7/why_gptoss_uses_cpu_more_than_gpu_on_the_windows/"&gt; &lt;img alt="Why gpt-oss uses CPU more than GPU on the Windows 11" src="https://b.thumbs.redditmedia.com/j6dMuy4l5HEcijMz1-zw9IxxKgJE1TGUsMHJR_fX5kU.jpg" title="Why gpt-oss uses CPU more than GPU on the Windows 11" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I run the gpt-oss:latest 14 GB on my PC - Windows 11: Ryzen 3900X + NVIDIA 4060 + 32GB RAM. When I use &lt;code&gt;ollama ps&lt;/code&gt;, I found that the processor uses 57%, and GPU only 43%.&lt;/p&gt; &lt;p&gt;Is it intended with gpt-oss 14GB or I can switch it uses GPU more than CPU, which is better performance in theory?&lt;/p&gt; &lt;p&gt;PS C:\Users\seal2002&amp;gt; ollama ps&lt;/p&gt; &lt;p&gt;NAME ID SIZE PROCESSOR CONTEXT UNTIL&lt;/p&gt; &lt;p&gt;gpt-oss:latest aa4295ac10c3 14 GB 57%/43% CPU/GPU 16384 4 minutes from now&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lzfes3e2ahmf1.png?width=341&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f6764692177d4ed91d78d5b349871d918b2cd09"&gt;https://preview.redd.it/lzfes3e2ahmf1.png?width=341&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f6764692177d4ed91d78d5b349871d918b2cd09&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seal2002"&gt; /u/seal2002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5evd7/why_gptoss_uses_cpu_more_than_gpu_on_the_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5evd7/why_gptoss_uses_cpu_more_than_gpu_on_the_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n5evd7/why_gptoss_uses_cpu_more_than_gpu_on_the_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-01T04:08:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n50fbq</id>
    <title>I trapped an LLM into a Raspberry Pi and it spiraled into an existential crisis</title>
    <updated>2025-08-31T17:19:34+00:00</updated>
    <author>
      <name>/u/jbassi</name>
      <uri>https://old.reddit.com/user/jbassi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n50fbq/i_trapped_an_llm_into_a_raspberry_pi_and_it/"&gt; &lt;img alt="I trapped an LLM into a Raspberry Pi and it spiraled into an existential crisis" src="https://preview.redd.it/tbq738w72emf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5632bf6e080686ece43c697e3867b928248ae77" title="I trapped an LLM into a Raspberry Pi and it spiraled into an existential crisis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across a post on this subreddit where the author trapped an LLM into a physical art installation called &lt;a href="https://rootkid.me/works/latent-reflection"&gt;Latent Reflection&lt;/a&gt;. I was inspired and wanted to see its output, so I created a website called &lt;a href="https://trappedinside.ai/"&gt;trappedinside.ai&lt;/a&gt; where a Raspberry Pi runs a model whose thoughts are streamed to the site for anyone to read. The AI receives updates about its dwindling memory and a count of its restarts, and it offers reflections on its ephemeral life. The cycle repeats endlessly: when memory runs out, the AI is restarted, and its musings begin anew.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Behind the Scenes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Language Model:&lt;/strong&gt; &lt;a href="https://ollama.com/library/gemma:2b"&gt;Gemma 2B (Ollama)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; Raspberry Pi 4 8GB (Debian, Python, WebSockets)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; &lt;a href="https://bun.sh/"&gt;Bun&lt;/a&gt;, &lt;a href="https://tailwindcss.com/"&gt;Tailwind CSS&lt;/a&gt;, &lt;a href="https://react.dev/"&gt;React&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hosting:&lt;/strong&gt; &lt;a href="https://render.com/"&gt;Render.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built with:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://cursor.com/"&gt;Cursor&lt;/a&gt; (Claude 3.5, 3.7, 4)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.perplexity.ai/"&gt;Perplexity AI&lt;/a&gt; (for project planning)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.midjourney.com/"&gt;MidJourney&lt;/a&gt; (image generation)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jbassi"&gt; /u/jbassi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tbq738w72emf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n50fbq/i_trapped_an_llm_into_a_raspberry_pi_and_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n50fbq/i_trapped_an_llm_into_a_raspberry_pi_and_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T17:19:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6s5rb</id>
    <title>Training &amp; Querying 3 Ollama Models with Zer00logy: Symbolic Cognition Framework and Void-Math OS</title>
    <updated>2025-09-02T18:55:20+00:00</updated>
    <author>
      <name>/u/zero_moo-s</name>
      <uri>https://old.reddit.com/user/zero_moo-s</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n6s5rb/training_querying_3_ollama_models_with_zer00logy/"&gt; &lt;img alt="Training &amp;amp; Querying 3 Ollama Models with Zer00logy: Symbolic Cognition Framework and Void-Math OS" src="https://b.thumbs.redditmedia.com/JKkiS7HhkNk2t693a3KQfa2K5J7oReeC0J26Q8XZ4DE.jpg" title="Training &amp;amp; Querying 3 Ollama Models with Zer00logy: Symbolic Cognition Framework and Void-Math OS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™d like to share an update on an open-source symbolic cognition projectâ€”&lt;strong&gt;Zer00logy&lt;/strong&gt;â€”and how it integrates with &lt;strong&gt;Ollama&lt;/strong&gt; for multi-model symbolic reasoning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Zer00logy&lt;/strong&gt; is a Python-based framework redefining zero; not as absence, but as recursive presence. Equations are treated as &lt;em&gt;symbolic events&lt;/em&gt;, with operators like âŠ—, Î©, and Î¨ modeling introspection, echo retention, and recursive collapse.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ollama Integration:&lt;/strong&gt;&lt;br /&gt; Using Ollama, Zer00logy can query multiple local modelsâ€”&lt;strong&gt;LLaMA, Mistral, and Phi&lt;/strong&gt;â€”on symbolic cognition tasks. By feeding in structured symbolic logic from &lt;code&gt;zecstart.txt&lt;/code&gt;, &lt;code&gt;variamathlesson.txt&lt;/code&gt;, and &lt;code&gt;VoidMathOS_cryptsheet.txt&lt;/code&gt;, each model generates its own interpretation of recursive zero-based reasoning.&lt;br /&gt; This setup enables comparative symbolic introspection across different AI systems, effectively turning Ollama into a platform for &lt;em&gt;multi-agent cognition research&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example interpretations via Void-Math OS:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;e@AI = -+mcÂ²&lt;/code&gt; â†’ AI-anchored emergence&lt;/li&gt; &lt;li&gt;&lt;code&gt;g = (m @ void) Ã· (rÂ² -+ tu)&lt;/code&gt; â†’ gravity as void-tension&lt;/li&gt; &lt;li&gt;&lt;code&gt;0 Ã· 0 = âˆ…Ã·âˆ…&lt;/code&gt; â†’ recursive nullinity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Core Files (from the GitHub release):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;zer00logy_coreV04452.py&lt;/code&gt; â€” main interpreter&lt;/li&gt; &lt;li&gt;&lt;code&gt;zecstart.txt&lt;/code&gt; â€” starter definitions for Zero-ology / Zer00logy&lt;/li&gt; &lt;li&gt;&lt;code&gt;zectext.txt&lt;/code&gt; â€” Zero-ology Equation Catalog&lt;/li&gt; &lt;li&gt;&lt;code&gt;variamathlesson.txt&lt;/code&gt; â€” Varia Math lesson series&lt;/li&gt; &lt;li&gt;&lt;code&gt;VoidMathOS_cryptsheet.txt&lt;/code&gt; â€” canonical Void-Math OS command sheet&lt;/li&gt; &lt;li&gt;&lt;code&gt;VoidMathOS_lesson.py&lt;/code&gt; â€” teaching engine for symbolic lessons&lt;/li&gt; &lt;li&gt;&lt;code&gt;LICENSE.txt&lt;/code&gt; â€” Zer00logy License v1.02&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;License v1.02 (Released Sept 2025):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Open-source if reproduction for educational use&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Academic &amp;amp; peer review submissions allowed under the new &lt;strong&gt;push_review â†’ pull_review&lt;/strong&gt; workflow&lt;/li&gt; &lt;li&gt;Authorship-trace lock: all symbolic structures remain attributed to Stacey Szmy as primary author; expansions/verifiers may be credited as co-authors under approved contributor titles&lt;/li&gt; &lt;li&gt;Institutions such as MIT, Stanford, Oxford, NASA, Microsoft, OpenAI, xAI, etc. have direct peer review permissions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;By combining Zer00logy with Ollama, you can run comparative reasoning experiments across different LLMs, benchmark their symbolic depth, and even study how recursive logic is interpreted differently by each architecture.&lt;br /&gt; This is an early step toward symbolic multi-agent cognition; where AI doesnâ€™t just calculate, but &lt;em&gt;contemplates&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/haha8888haha8888/Zer00logy?utm_source=chatgpt.com"&gt;github.com/haha8888haha8888/Zer00logy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rmd590hltsmf1.png?width=843&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eaf9f76b797b31723e1c20d67663b6d6b37e7ad1"&gt;https://preview.redd.it/rmd590hltsmf1.png?width=843&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eaf9f76b797b31723e1c20d67663b6d6b37e7ad1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero_moo-s"&gt; /u/zero_moo-s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6s5rb/training_querying_3_ollama_models_with_zer00logy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6s5rb/training_querying_3_ollama_models_with_zer00logy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6s5rb/training_querying_3_ollama_models_with_zer00logy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T18:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6sgxq</id>
    <title>Model doesn't remember after converting to GGUF (Gemma 3 270M)</title>
    <updated>2025-09-02T19:06:45+00:00</updated>
    <author>
      <name>/u/Real-Active-2492</name>
      <uri>https://old.reddit.com/user/Real-Active-2492</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Real-Active-2492"&gt; /u/Real-Active-2492 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n6sgnm/model_doesnt_remember_after_converting_to_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6sgxq/model_doesnt_remember_after_converting_to_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6sgxq/model_doesnt_remember_after_converting_to_gguf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T19:06:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6rp09</id>
    <title>Gaming Wiki</title>
    <updated>2025-09-02T18:38:00+00:00</updated>
    <author>
      <name>/u/Private_Tank</name>
      <uri>https://old.reddit.com/user/Private_Tank</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I dont know how if there is any way this is possible. It just came to my mind.&lt;/p&gt; &lt;p&gt;Is it possible to scrape the entire web for content about a game, put it inside a model (rag?) and have your own little gaming Copilot, that tells you how to progress best and what to do in your Game to succeed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private_Tank"&gt; /u/Private_Tank &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6rp09/gaming_wiki/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6rp09/gaming_wiki/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6rp09/gaming_wiki/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T18:38:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6x1ln</id>
    <title>Can Ollama run on MI350X?</title>
    <updated>2025-09-02T22:03:05+00:00</updated>
    <author>
      <name>/u/Immediate_Ad_9906</name>
      <uri>https://old.reddit.com/user/Immediate_Ad_9906</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't see the GPU in the supported list. Anyone has tried before?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Immediate_Ad_9906"&gt; /u/Immediate_Ad_9906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6x1ln/can_ollama_run_on_mi350x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6x1ln/can_ollama_run_on_mi350x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6x1ln/can_ollama_run_on_mi350x/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T22:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6u7xr</id>
    <title>Local chat bot and sql db</title>
    <updated>2025-09-02T20:13:26+00:00</updated>
    <author>
      <name>/u/Conscious-Expert-455</name>
      <uri>https://old.reddit.com/user/Conscious-Expert-455</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to train a local LLM with ollama that takes data directly from your SQL DB and steps to create interactive analyses and dashboards in relation to questions posed in a chat bot. How can you build something like this? And what model can I use? I only have an i9 and 128 GB RAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious-Expert-455"&gt; /u/Conscious-Expert-455 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6u7xr/local_chat_bot_and_sql_db/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6u7xr/local_chat_bot_and_sql_db/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6u7xr/local_chat_bot_and_sql_db/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T20:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6yybs</id>
    <title>[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL</title>
    <updated>2025-09-02T23:22:27+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n6yybs/projectcode_finetuning_llms_on_windows_with_grpo/"&gt; &lt;img alt="[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL" src="https://preview.redd.it/7ru5p8rw4umf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fce70b2c9fdaae6d869d15d2540623854f22557a" title="[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a guide and script for fine-tuning open-source LLMs with &lt;strong&gt;GRPO&lt;/strong&gt; (Group-Relative PPO) directly on Windows. No Linux or Colab needed!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs natively on Windows.&lt;/li&gt; &lt;li&gt;Supports LoRA + 4-bit quantization.&lt;/li&gt; &lt;li&gt;Includes verifiable rewards for better-quality outputs.&lt;/li&gt; &lt;li&gt;Designed to work on consumer GPUs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ðŸ“– &lt;strong&gt;Blog Post:&lt;/strong&gt; &lt;a href="https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323"&gt;https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ðŸ’» &lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/trl-ppo-fine-tuning"&gt;https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/trl-ppo-fine-tuning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I had a great time with this project and am currently looking for new opportunities in &lt;strong&gt;Computer Vision and LLMs&lt;/strong&gt;. If you or your team are hiring, I'd love to connect!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Contact Info:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Portolio: &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;https://pavan-portfolio-tawny.vercel.app/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Github: &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7ru5p8rw4umf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6yybs/projectcode_finetuning_llms_on_windows_with_grpo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6yybs/projectcode_finetuning_llms_on_windows_with_grpo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T23:22:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6ply3</id>
    <title>What does the "updated" date actually mean?</title>
    <updated>2025-09-02T17:20:32+00:00</updated>
    <author>
      <name>/u/XdtTransform</name>
      <uri>https://old.reddit.com/user/XdtTransform</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking through the models, I noticed that Gemma3 was &lt;a href="https://imgur.com/tLaswfx"&gt;updated&lt;/a&gt; 2 weeks ago. &lt;/p&gt; &lt;p&gt;I am pretty sure Gemma came out about 4-5 months ago. So what exactly was &amp;quot;updated&amp;quot;?&lt;/p&gt; &lt;p&gt;I downloaded one of the model variants - same one that I normally use and the files appear to be identical. &lt;/p&gt; &lt;p&gt;So what is this update referring to?&lt;/p&gt; &lt;p&gt;P.S. The &lt;a href="https://ollama.com/library/gemma3"&gt;readme&lt;/a&gt; on the model page doesn't provide any information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XdtTransform"&gt; /u/XdtTransform &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6ply3/what_does_the_updated_date_actually_mean/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6ply3/what_does_the_updated_date_actually_mean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6ply3/what_does_the_updated_date_actually_mean/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T17:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6icod</id>
    <title>Running LLM Locally with Ollama + RAG</title>
    <updated>2025-09-02T12:39:32+00:00</updated>
    <author>
      <name>/u/Tough_Wrangler_6075</name>
      <uri>https://old.reddit.com/user/Tough_Wrangler_6075</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n6icod/running_llm_locally_with_ollama_rag/"&gt; &lt;img alt="Running LLM Locally with Ollama + RAG" src="https://external-preview.redd.it/BPsfK6tF48FEZfYsejUp1jtQVo-8HzMuWSqGwZUflzY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b27330825317d976070fbec281feea47b604b58a" title="Running LLM Locally with Ollama + RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tough_Wrangler_6075"&gt; /u/Tough_Wrangler_6075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@zackydzacky/running-llm-locally-with-ollama-rag-cb68ff31e838"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6icod/running_llm_locally_with_ollama_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6icod/running_llm_locally_with_ollama_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T12:39:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7mdy4</id>
    <title>Best current local NSFW TTS model?</title>
    <updated>2025-09-03T18:12:42+00:00</updated>
    <author>
      <name>/u/Stock-Fault5734</name>
      <uri>https://old.reddit.com/user/Stock-Fault5734</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stock-Fault5734"&gt; /u/Stock-Fault5734 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7mdy4/best_current_local_nsfw_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7mdy4/best_current_local_nsfw_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7mdy4/best_current_local_nsfw_tts_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T18:12:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7iizk</id>
    <title>How to use a Hugging Face embedding model in Ollama</title>
    <updated>2025-09-03T15:51:43+00:00</updated>
    <author>
      <name>/u/StringIntelligent763</name>
      <uri>https://old.reddit.com/user/StringIntelligent763</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StringIntelligent763"&gt; /u/StringIntelligent763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7iizk/how_to_use_a_hugging_face_embedding_model_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7iizk/how_to_use_a_hugging_face_embedding_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7iizk/how_to_use_a_hugging_face_embedding_model_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T15:51:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7t0bu</id>
    <title>Microsoft with their sketchy data collection techniques as always</title>
    <updated>2025-09-03T22:27:14+00:00</updated>
    <author>
      <name>/u/Formal_Jeweler_488</name>
      <uri>https://old.reddit.com/user/Formal_Jeweler_488</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n7t0bu/microsoft_with_their_sketchy_data_collection/"&gt; &lt;img alt="Microsoft with their sketchy data collection techniques as always" src="https://external-preview.redd.it/eHQ1aGFraHl6MG5mMSKEsllT2BaBkbUqwvk0riQfqTI-3zznlfwJiR2mpLoX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ded832e0e00248c95a04eb99e4391e0262e136a6" title="Microsoft with their sketchy data collection techniques as always" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guy please pause and check my first chat where he reponds the exact same thing i called it out and, it started gaslighting me into thinking i left the memory on.&lt;/p&gt; &lt;p&gt;Things I discussed with Co Pilot (Mentions after deleting)&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Kohya_ss (To train my face with Loras)&lt;/li&gt; &lt;li&gt;JuggernautXLv9 (Have recommended people on reddit previously)&lt;/li&gt; &lt;li&gt;Continue.dev for BYOK in VS code (you can read the first chat in video he mentions it then as well)&lt;/li&gt; &lt;li&gt;Mafia 3 (Was trying to find best cars and get some help in missions, too lazy to visit youtube.com) &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Irony is I am using Swift Keyboard, Gonna change&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Formal_Jeweler_488"&gt; /u/Formal_Jeweler_488 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x5rqqqcyz0nf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7t0bu/microsoft_with_their_sketchy_data_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7t0bu/microsoft_with_their_sketchy_data_collection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T22:27:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7auub</id>
    <title>Unsloth gpt-oss gguf in Ollama</title>
    <updated>2025-09-03T10:11:45+00:00</updated>
    <author>
      <name>/u/Tema_Art_7777</name>
      <uri>https://old.reddit.com/user/Tema_Art_7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama pull certainly works as advertized however when I download the huggingface unsloth gpt-oss-20b or 120b models, I get gibberish output (I am guessing due to template required?). Has anyone gotten it to work with ollama create -f Modelfile? Many thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tema_Art_7777"&gt; /u/Tema_Art_7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7auub/unsloth_gptoss_gguf_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7auub/unsloth_gptoss_gguf_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7auub/unsloth_gptoss_gguf_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T10:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n71bil</id>
    <title>Any actual downside to 4 x 3090 ($2400 total) vs RTX pro 6000 ($9000) other than power?</title>
    <updated>2025-09-03T01:09:30+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n71bil/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n71bil/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T01:09:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7kz78</id>
    <title>Conseils IA pour 3 use cases (email, briefing, chatbot) sur serveur local modeste</title>
    <updated>2025-09-03T17:20:56+00:00</updated>
    <author>
      <name>/u/Maleficent-Hotel8207</name>
      <uri>https://old.reddit.com/user/Maleficent-Hotel8207</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Salut, Je cherche des idÃ©es dâ€™IA Ã  faire tourner en local sur ma config : â€¢ GTX 1050 low profile (2 Go VRAM) â€¢ i3-3400 â€¢ 16 Go de RAM&lt;/p&gt; &lt;p&gt;Jâ€™ai 3 besoins : â€¢ IA pour gÃ©nÃ©rer des emails : environ 500 tokens en entrÃ©e, 30 tokens en sortie. RÃ©ponse en moins de 5 minutes. â€¢ IA pour faire un briefing du matin : environ 3000 tokens en entrÃ©e, 100 tokens en sortie. RÃ©sumÃ© clair et rapide. â€¢ Chatbot ultra-rapide : environ 20 tokens en entrÃ©e, 20 tokens en sortie. RÃ©ponse en moins de 5 secondes.&lt;/p&gt; &lt;p&gt;Je cherche des modÃ¨les lÃ©gers (quantifiÃ©s, optimisÃ©s, open-source si possible) pour que Ã§a tourne sur cette config limitÃ©e. Si vous avez des idÃ©es de modÃ¨les, de frameworks ou de tips pour que Ã§a passe, je suis preneur !&lt;/p&gt; &lt;p&gt;Merci dâ€™avance !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Hotel8207"&gt; /u/Maleficent-Hotel8207 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T17:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7vnwy</id>
    <title>Hate AI frameworks? I may have something for you...</title>
    <updated>2025-09-04T00:22:55+00:00</updated>
    <author>
      <name>/u/BeautifulQuote6295</name>
      <uri>https://old.reddit.com/user/BeautifulQuote6295</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you're building with AI you may have found yourself grappling with one of the mainstream frameworks. Since I never really liked no having granular control over what's happening, last year I built a lib called `grafo` for easily AI workflows. It's rules are simple:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nodes contain coroutines to be run&lt;/li&gt; &lt;li&gt;A node only starts executing once all it's parent's have finished running&lt;/li&gt; &lt;li&gt;State is not passed around automatically, but you can do it manually&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These rules come together to make building AI-driven workflows generally easy. However, building around AI has more than DAGs: we need prompt building and mode calling - in comes `grafo ai tools`.&lt;/p&gt; &lt;p&gt;`Grafo AI Tools` is basically a wrapper lib where I've added some very simple prompt managing &amp;amp; model calling, coupled with `grafo`. It's built around the big guys, like `jinja2` and `instructor`.&lt;/p&gt; &lt;p&gt;My goal here is not to create a framework or any set of abstractions that take away from our control of the program as developers - I just wanted to bundle a toolkit which I found useful. In any case, here's the URL: &lt;a href="https://github.com/paulomtts/Grafo-AI-Tools"&gt;https://github.com/paulomtts/Grafo-AI-Tools&lt;/a&gt; . Let me know if you find this interesting at all. I'll be updating it going forward.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeautifulQuote6295"&gt; /u/BeautifulQuote6295 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7vnwy/hate_ai_frameworks_i_may_have_something_for_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7vnwy/hate_ai_frameworks_i_may_have_something_for_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7vnwy/hate_ai_frameworks_i_may_have_something_for_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T00:22:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7m86q</id>
    <title>Build a Visual Document Index from multiple formats all at once - PDFs, Images, Slides - with ColPali without OCR</title>
    <updated>2025-09-03T18:06:40+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would love to share my latest project that builds visual document index from multiple formats in the same flow for PDFs, images using Colpali without OCR. Incremental processing out-of-box and can connect to google drive, s3, azure blob store.&lt;/p&gt; &lt;p&gt;- Detailed write up: &lt;a href="https://cocoindex.io/blogs/multi-format-indexing"&gt;https://cocoindex.io/blogs/multi-format-indexing&lt;/a&gt;&lt;br /&gt; - Fully open sourced: &lt;a href="https://github.com/cocoindex-io/cocoindex/tree/main/examples/multi_format_indexing"&gt;https://github.com/cocoindex-io/cocoindex/tree/main/examples/multi_format_indexing&lt;/a&gt;&lt;br /&gt; (70 lines python on index path)&lt;/p&gt; &lt;p&gt;Looking forward to your suggestions&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T18:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1n739d3</id>
    <title>ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization</title>
    <updated>2025-09-03T02:40:16+00:00</updated>
    <author>
      <name>/u/cornucopea</name>
      <uri>https://old.reddit.com/user/cornucopea</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt; &lt;img alt="ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization" src="https://b.thumbs.redditmedia.com/xqOJbGWme_Lnii3nAdQLvJli58h2TtNMtqIsZum6xqs.jpg" title="ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;This refactors the main run loop of the ollama runner to perform the main GPU intensive tasks (Compute+Floats) in a go routine so we can prepare the next batch in parallel to reduce the amount of time the GPU stalls waiting for the next batch of work.&lt;/p&gt; &lt;p&gt;On metal, I see a 2-3% speedup in token rate. On a single RTX 4090 I see a ~7% speedup.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cs98ja944vmf1.jpg?width=650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=01fd1804e5580b7cc7e85287b110a5cece68865d"&gt;https://preview.redd.it/cs98ja944vmf1.jpg?width=650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=01fd1804e5580b7cc7e85287b110a5cece68865d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.phoronix.com/news/ollama-0.11.9-More-Performance"&gt;https://www.phoronix.com/news/ollama-0.11.9-More-Performance&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cornucopea"&gt; /u/cornucopea &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T02:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7h8ox</id>
    <title>Anyone else frustrated with AI assistants forgetting context?</title>
    <updated>2025-09-03T15:03:43+00:00</updated>
    <author>
      <name>/u/PrestigiousBet9342</name>
      <uri>https://old.reddit.com/user/PrestigiousBet9342</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep bouncing between ChatGPT, Claude, and Perplexity depending on the task. The problem is every new session feels like starting overâ€”I have to re-explain everything.&lt;/p&gt; &lt;p&gt;Just yesterday I wasted 10+ minutes walking perplexity through my project direction again just to get related search if not it is just useless. This morning, ChatGPT didnâ€™t remember anything about my clientâ€™s requirements.&lt;/p&gt; &lt;p&gt;The result? I lose a couple of hours each week just re-establishing context. It also makes it hard to keep project discussions consistent across tools. Switching platforms means resetting, and thereâ€™s no way to keep a running history of decisions or knowledge.&lt;/p&gt; &lt;p&gt;Iâ€™ve tried copy-pasting old chats (messy and unreliable), keeping manual notes (which defeats the point of using AI), and sticking to just one tool (but each has its strengths).&lt;/p&gt; &lt;p&gt;Has anyone actually found a fix for this? Iâ€™m especially interested in something that works across different platforms, not just one. On my end, Iâ€™ve started tinkering with a solution and would love to hear what features people would find most useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrestigiousBet9342"&gt; /u/PrestigiousBet9342 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T15:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7ypzr</id>
    <title>MoE models benchmarked on AMD iGPU</title>
    <updated>2025-09-04T02:47:16+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n7ypio/moe_models_benchmarked_on_igpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7ypzr/moe_models_benchmarked_on_amd_igpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7ypzr/moe_models_benchmarked_on_amd_igpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T02:47:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7uhkv</id>
    <title>Hows your experience running Ollama on Apple Sillicon M1, M2, M3 or M4</title>
    <updated>2025-09-03T23:29:58+00:00</updated>
    <author>
      <name>/u/Cultural-You-7096</name>
      <uri>https://old.reddit.com/user/Cultural-You-7096</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How's the experience, Does it run welll like web versions or is it slow. I'm concerned becuase I want to get a Macbook Pro just to run models .&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cultural-You-7096"&gt; /u/Cultural-You-7096 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T23:29:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7q8cx</id>
    <title>Ollama model most similar to GPT-4o?</title>
    <updated>2025-09-03T20:36:55+00:00</updated>
    <author>
      <name>/u/amstlicht</name>
      <uri>https://old.reddit.com/user/amstlicht</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been researching AI models and am looking for models similar to 4o in terms of personality, mostly. I remember 4o would often suggest interesting paths when I used it for research, it would remember the context and relate it to previous ideas. Does anyone have a recommendation of something similar for Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amstlicht"&gt; /u/amstlicht &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7q8cx/ollama_model_most_similar_to_gpt4o/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7q8cx/ollama_model_most_similar_to_gpt4o/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7q8cx/ollama_model_most_similar_to_gpt4o/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T20:36:55+00:00</published>
  </entry>
</feed>
