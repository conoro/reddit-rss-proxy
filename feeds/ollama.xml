<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-18T01:36:01+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1owik8l</id>
    <title>Using my entire source code library in my LLM</title>
    <updated>2025-11-14T00:57:48+00:00</updated>
    <author>
      <name>/u/phoenixfire425</name>
      <uri>https://old.reddit.com/user/phoenixfire425</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have about 25years of my code I would like to be able to have my local ollama instance either trained on or possibly RAG?&lt;/p&gt; &lt;p&gt;My goal is so be able access examples of my previous code by asking questions like I do now with things like qwen or gpt-oss.&lt;/p&gt; &lt;p&gt;Most of my stuff is python and .net stack.&lt;/p&gt; &lt;p&gt;There have been so many times where I know I did something before and it required some crafty work arounds, but I don‚Äôt recall the project. I would love to be able to us all that code as a resource.&lt;/p&gt; &lt;p&gt;My setup is Ollama and OpenWebui on Linux Mint with a RTX 3090 and GTX 1050(used just for memory personalization in openwebui)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phoenixfire425"&gt; /u/phoenixfire425 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owik8l/using_my_entire_source_code_library_in_my_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owik8l/using_my_entire_source_code_library_in_my_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owik8l/using_my_entire_source_code_library_in_my_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T00:57:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxdiva</id>
    <title>AI Safety Evaluation!</title>
    <updated>2025-11-15T00:31:42+00:00</updated>
    <author>
      <name>/u/DyroZang</name>
      <uri>https://old.reddit.com/user/DyroZang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone! &lt;/p&gt; &lt;p&gt;I thought I would share a project that I've been working on recently that I'm hoping to get some traction and feedback on. Apolien is a python package for evaluating LLMs for their level of AI Safety originally built on ollama but now also supporting Anthropic API. As of now this package, Apolien, will be able to accept any model available on ollama and perform a series of faithfulness tests on the model through something called Chain-of-Thought prompting. Based on the models responses it will determine if the model is faithful to some reasoning or if it's lying or ignoring specific requests. &lt;/p&gt; &lt;p&gt;The repository for this project is available here: &lt;a href="https://github.com/gabe-mousa/Apolien"&gt;https://github.com/gabe-mousa/Apolien&lt;/a&gt; or you can install it using `pip install apolien`. In the repo there is specific information on the faithfulness tests, example outputs, datasets available to test on, and issues if anyone feels like contributing to the project. &lt;/p&gt; &lt;p&gt;Please feel free to comment any questions around the stats, inspiration, feedback of any kind and I'll do my best to respond here. Otherwise if you're feeling generous or find the project particularly interesting I would greatly appreciate if you could star the project on GitHub! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DyroZang"&gt; /u/DyroZang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxdiva/ai_safety_evaluation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxdiva/ai_safety_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oxdiva/ai_safety_evaluation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-15T00:31:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxe0pg</id>
    <title>Local Ollama Processing Scanned Images - Need Ideas</title>
    <updated>2025-11-15T00:54:04+00:00</updated>
    <author>
      <name>/u/degr8sid</name>
      <uri>https://old.reddit.com/user/degr8sid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I want to build a system where my local ollama model processes the scanned text from PDFs. These are basically a lot of scanned books in arabic and the text is not picked up by Adobe either. Like you can't copy the text from them even if you open the pdf using Adobe Acrobat.&lt;/p&gt; &lt;p&gt;So I want my system to process scanned pdf, pick the text that's actually there, convert it into proper text and then use it for RAG. &lt;/p&gt; &lt;p&gt;I want ideas on how I can setup using local llama. And what other tools/agents/etc will I need to make it work successfully. Or should I just drop this project? I really want to help people learning modern standard arabic and the scanned books I have are great resources.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/degr8sid"&gt; /u/degr8sid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxe0pg/local_ollama_processing_scanned_images_need_ideas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxe0pg/local_ollama_processing_scanned_images_need_ideas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oxe0pg/local_ollama_processing_scanned_images_need_ideas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-15T00:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox403x</id>
    <title>is ollama supposed to work out of the box for a 7800 XT?</title>
    <updated>2025-11-14T18:12:41+00:00</updated>
    <author>
      <name>/u/ZdrytchX</name>
      <uri>https://old.reddit.com/user/ZdrytchX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Turns out its been running cpu this entire time despite the 7800XT being on the &lt;a href="https://ollama.com/blog/amd-preview"&gt;supported&lt;/a&gt; list.&lt;/p&gt; &lt;p&gt;On the side note, WebGPU fails for some reason with my gpu, corrupting output from some things like &lt;a href="https://huggingface.co/spaces/webml-community/kokoro-webgpu"&gt;kokoro&lt;/a&gt;, but the gpu runs video games fine aside from the usual videogame-specific hickups &lt;sup&gt;specifically war thunder crashes on alt-tab sometimes and il-2sturmovik has stuttering since windows 11 'downgrade'&lt;/sup&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZdrytchX"&gt; /u/ZdrytchX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ox403x/is_ollama_supposed_to_work_out_of_the_box_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ox403x/is_ollama_supposed_to_work_out_of_the_box_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ox403x/is_ollama_supposed_to_work_out_of_the_box_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T18:12:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox99fj</id>
    <title>How to get a custom Open WebUi Model to return a JSON object consistently?</title>
    <updated>2025-11-14T21:33:57+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 24GB of VRAM which seems like it should be enough. Here is the issue: I setup a custom model in Open WebUI and attached some knowledge and a system prompt to it. Its job is to read an article I provide it, and evaluate it, and is supposed to return a JSON object in a particular format, for example {status: ‚Äúgreen‚Äù, reason: ‚Äúblah blah‚Äù} where I define when it should evaluate to green, vs blue, vs red etc, and explain the answer in ‚Äúreason‚Äù. The thinking works perfectly as I want it to after I feed it various test articles where I know what the output should be. The problem is that it ignores my rules to return only the json and only with those fields and with only those status options listed. It adds its reasoning to the output like and produces a large response that sometimes also contains the json. Also, when it does add the json, it names the fields what it wants, for example sometimes calling ‚Äústatus‚Äù ‚Äúresult‚Äù instead etc. The fact that the thinking is correct, makes me think its not an issue of the model being too weak. It just refuses to lock into the json format and respecting my rules to SOLELY output the json, with no extra text along with it. Any thoughts? Im currently trying Qwen3 32b since it fits in 24gb. Im also using the openwebui api/chat/completions endpoint since the ollama generate endpoint doesnt see the custom models created in openwebui&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ox99fj/how_to_get_a_custom_open_webui_model_to_return_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ox99fj/how_to_get_a_custom_open_webui_model_to_return_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ox99fj/how_to_get_a_custom_open_webui_model_to_return_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T21:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1owymhr</id>
    <title>distil-localdoc.py - SLM assistant for writing Python documentation</title>
    <updated>2025-11-14T14:55:01+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1owymhr/distillocaldocpy_slm_assistant_for_writing_python/"&gt; &lt;img alt="distil-localdoc.py - SLM assistant for writing Python documentation" src="https://preview.redd.it/c1ufxihvk81g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d2a67cbfadc9b786a0817740c4adc2cc7675bf9" title="distil-localdoc.py - SLM assistant for writing Python documentation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an SLM assistant for automatic Python documentation - a Qwen3 0.6B parameter model that generates complete, properly formatted docstrings for your code in Google style. Run it locally, keeping your proprietary code secure! Find it at &lt;a href="https://github.com/distil-labs/distil-localdoc.py"&gt;https://github.com/distil-labs/distil-localdoc.py&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Usage&lt;/h2&gt; &lt;p&gt;We load the model and your Python file. By default we load the downloaded Qwen3 0.6B model and generate Google-style docstrings.&lt;/p&gt; &lt;p&gt;```bash python localdoc.py --file your_script.py&lt;/p&gt; &lt;h1&gt;optionally, specify model and docstring style&lt;/h1&gt; &lt;p&gt;python localdoc.py --file your_script.py --model localdoc_qwen3 --style google ```&lt;/p&gt; &lt;p&gt;The tool will generate an updated file with &lt;code&gt;_documented&lt;/code&gt; suffix (e.g., &lt;code&gt;your_script_documented.py&lt;/code&gt;).&lt;/p&gt; &lt;h2&gt;Features&lt;/h2&gt; &lt;p&gt;The assistant can generate docstrings for: - &lt;strong&gt;Functions&lt;/strong&gt;: Complete parameter descriptions, return values, and raised exceptions - &lt;strong&gt;Methods&lt;/strong&gt;: Instance and class method documentation with proper formatting. The tool skips double underscore (dunder: __xxx) methods.&lt;/p&gt; &lt;h2&gt;Examples&lt;/h2&gt; &lt;p&gt;Feel free to run them yourself using the files in [examples](examples)&lt;/p&gt; &lt;h3&gt;Before:&lt;/h3&gt; &lt;p&gt;&lt;code&gt;python def calculate_total(items, tax_rate=0.08, discount=None): subtotal = sum(item['price'] * item['quantity'] for item in items) if discount: subtotal *= (1 - discount) return subtotal * (1 + tax_rate) &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;After (Google style):&lt;/h3&gt; &lt;p&gt;```python def calculate_total(items, tax_rate=0.08, discount=None): &amp;quot;&amp;quot;&amp;quot; Calculate the total cost of items, applying a tax rate and optionally a discount.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Args: items: List of item objects with price and quantity tax_rate: Tax rate expressed as a decimal (default 0.08) discount: Discount rate expressed as a decimal; if provided, the subtotal is multiplied by (1 - discount) Returns: Total amount after applying the tax Example: &amp;gt;&amp;gt;&amp;gt; items = [{'price': 10, 'quantity': 2}, {'price': 5, 'quantity': 1}] &amp;gt;&amp;gt;&amp;gt; calculate_total(items, tax_rate=0.1, discount=0.05) 22.5 &amp;quot;&amp;quot;&amp;quot; subtotal = sum(item['price'] * item['quantity'] for item in items) if discount: subtotal *= (1 - discount) return subtotal * (1 + tax_rate) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;h2&gt;FAQ&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Q: Why don't we just use GPT-4/Claude API for this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Because your proprietary code shouldn't leave your infrastructure. Cloud APIs create security risks, compliance issues, and ongoing costs. Our models run locally with comparable quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Can I document existing docstrings or update them?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Currently, the tool only adds missing docstrings. Updating existing documentation is planned for future releases. For now, you can manually remove docstrings you want regenerated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Which docstring style can I use?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Google&lt;/strong&gt;: Most readable, great for general Python projects&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Q: The model does not work as expected&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: The tool calling on our platform is in active development! &lt;a href="https://www.linkedin.com/company/distil-labs/"&gt;Follow us on LinkedIn&lt;/a&gt; for updates, or &lt;a href="https://join.slack.com/t/distil-labs-community/shared_invite/zt-36zqj87le-i3quWUn2bjErRq22xoE58g"&gt;join our community&lt;/a&gt;. You can also manually refine any generated docstrings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Can you train a model for my company's documentation standards?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: Visit our &lt;a href="https://www.distillabs.ai"&gt;website&lt;/a&gt; and reach out to us, we offer custom solutions tailored to your coding standards and domain-specific requirements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Does this support type hints or other Python documentation tools?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: Type hints are parsed and incorporated into docstrings. Integration with tools like pydoc, Sphinx, and MkDocs is on our roadmap.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c1ufxihvk81g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owymhr/distillocaldocpy_slm_assistant_for_writing_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owymhr/distillocaldocpy_slm_assistant_for_writing_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T14:55:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox8ky1</id>
    <title>Gpt oss 120b 64GB RAM, RTX5090 32GB?</title>
    <updated>2025-11-14T21:07:12+00:00</updated>
    <author>
      <name>/u/Wonk_puffin</name>
      <uri>https://old.reddit.com/user/Wonk_puffin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all Is this possible? Horrified at the price of system RAM which seems to have more than tripled for DDR5 in 8 months so my system is what it is. Using Ollama desktop, docker, openwebui. Have many models but would love to get this running even if at only 10 tokens a second. Any settings appreciated if this is feasible. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wonk_puffin"&gt; /u/Wonk_puffin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ox8ky1/gpt_oss_120b_64gb_ram_rtx5090_32gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ox8ky1/gpt_oss_120b_64gb_ram_rtx5090_32gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ox8ky1/gpt_oss_120b_64gb_ram_rtx5090_32gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T21:07:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxya4q</id>
    <title>How to broadcast ollama in local network.</title>
    <updated>2025-11-15T17:58:45+00:00</updated>
    <author>
      <name>/u/AcanthisittaNo5704</name>
      <uri>https://old.reddit.com/user/AcanthisittaNo5704</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im having a hard time opening ollama on all interfaces and i dont know why. I first changed the environment variable, then tried nginx but still no luck. Ollama is a docker instance that i want to connect to a computer running openwebui. The devices are connected via tailscale&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AcanthisittaNo5704"&gt; /u/AcanthisittaNo5704 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxya4q/how_to_broadcast_ollama_in_local_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxya4q/how_to_broadcast_ollama_in_local_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oxya4q/how_to_broadcast_ollama_in_local_network/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-15T17:58:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxq6yx</id>
    <title>Local LLM with GeminiLake Chip?</title>
    <updated>2025-11-15T12:12:42+00:00</updated>
    <author>
      <name>/u/DaCHack</name>
      <uri>https://old.reddit.com/user/DaCHack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any recommendations to use local LLMs on such a low power Chip as an Intel J4105 with Intel HD600 Graphics?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use Ollama or something Else? (Ipex)&lt;/li&gt; &lt;li&gt;Use iGPU or better let the CPU do the Job?&lt;/li&gt; &lt;li&gt;any easy to use Docker Container to get me going fast without much setup?&lt;/li&gt; &lt;li&gt;Which lightweight Models to use with Max 8 or 16 GB RAM in my box to support paperless-ngx ai and maybe some homeassistant automation? (First ideas: &lt;a href="https://ollama.com/fixt/home-3b-v2"&gt;https://ollama.com/fixt/home-3b-v2&lt;/a&gt;, llama3.2(4B))&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ideas Highly appreciated. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DaCHack"&gt; /u/DaCHack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxq6yx/local_llm_with_geminilake_chip/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxq6yx/local_llm_with_geminilake_chip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oxq6yx/local_llm_with_geminilake_chip/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-15T12:12:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy0srw</id>
    <title>Mimir Memory Bank now uses llama.cpp!</title>
    <updated>2025-11-15T19:38:37+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/GithubCopilot/comments/1oy0s0k/mimir_memory_bank_now_uses_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oy0srw/mimir_memory_bank_now_uses_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oy0srw/mimir_memory_bank_now_uses_llamacpp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-15T19:38:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxtmn9</id>
    <title>Some questions for a new comer</title>
    <updated>2025-11-15T14:52:38+00:00</updated>
    <author>
      <name>/u/R0B0t1C_Cucumber</name>
      <uri>https://old.reddit.com/user/R0B0t1C_Cucumber</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Folks,&lt;br /&gt; I'm new to the whole AI scene but not newbie in any means when it comes to technology (spent the last 15 years building data centers and acting as devops for support folks).&lt;/p&gt; &lt;p&gt;Over the last few weeks i've played with&lt;br /&gt; - github co-pilot pro and I loved what it could produce, although it felt a bit clunky when it leveraged certain things like simplebrowser in vscode which caused some issues with corruption in the files it was writing writing to. But over all the experience was good.&lt;/p&gt; &lt;p&gt;- Claude CLI this was awesome. It did exactly what I wanted it to do right from my terminal.&lt;/p&gt; &lt;p&gt;Which got me starting to play with Ollama and other tools that could leverage it to make a similar experience. It's not lost on me that I'm bottlenecked by hardware constraints when it comes to local models, and I intend to keep my subscriptions for the other services but the linux hobbiest in me wants to get one running locally just to tinker with and try different models.&lt;/p&gt; &lt;p&gt;The compute setup:&lt;/p&gt; &lt;p&gt;CPU: i9 14900KF&lt;br /&gt; GPU: 4070Ti 12GB&lt;br /&gt; Memory: 64GB&lt;br /&gt; Disk: 2tb nvme&lt;br /&gt; OS: Ubuntu 24.04&lt;/p&gt; &lt;p&gt;So, what is the communities recommendations for a clean setup using ollama to act either like github copilot pro or claude CLI? The use case is code generation and it should be able to do pretty much everything on it's own. The way i'm using AI right now a prompt would look like this: &amp;quot;create a test application in directory /x/y/z using react and serve it on port 8080, send me the link when it's functional for me to test&amp;quot;&lt;/p&gt; &lt;p&gt;I've tried Cline and &lt;a href="http://continue.dev"&gt;continue.dev&lt;/a&gt; plugins in vscode and nanocoder for CLI , all were pretty cool but felt clunky leading me to make this post. I must be pulling the wrong llm's , setting the wrong context lengths, or maybe i'm entirely missing something. Sorry for the long rambling post. Any help pointing me to the next rabbit hole is much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R0B0t1C_Cucumber"&gt; /u/R0B0t1C_Cucumber &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxtmn9/some_questions_for_a_new_comer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxtmn9/some_questions_for_a_new_comer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oxtmn9/some_questions_for_a_new_comer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-15T14:52:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oycim4</id>
    <title>Ollama and LLama3 to write files to a directory</title>
    <updated>2025-11-16T04:35:13+00:00</updated>
    <author>
      <name>/u/InformationPuzzled44</name>
      <uri>https://old.reddit.com/user/InformationPuzzled44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, Im a complete ollama noob. Ive been using Claude Code to write documents to a Ubuntu Server. So i tried this with installed Ollama and using Llama3 and openai-web but when i ask llama3 to write a file, it says its cloud based and can't. So im confused am i doing something wrong, i was under impression that running ollama locally means that i could have AI write files and code etc, locally. &lt;/p&gt; &lt;p&gt;Thanks for any assistance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InformationPuzzled44"&gt; /u/InformationPuzzled44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oycim4/ollama_and_llama3_to_write_files_to_a_directory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oycim4/ollama_and_llama3_to_write_files_to_a_directory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oycim4/ollama_and_llama3_to_write_files_to_a_directory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-16T04:35:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oye74t</id>
    <title>Claude randomly added Chinese text. Anyone else seen this?</title>
    <updated>2025-11-16T06:09:03+00:00</updated>
    <author>
      <name>/u/Separate_Bonus_2234</name>
      <uri>https://old.reddit.com/user/Separate_Bonus_2234</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oye74t/claude_randomly_added_chinese_text_anyone_else/"&gt; &lt;img alt="Claude randomly added Chinese text. Anyone else seen this?" src="https://b.thumbs.redditmedia.com/hSCYpdmrFNdqVoJrKjNH5GmSduN_hbBsWPQYa962FMY.jpg" title="Claude randomly added Chinese text. Anyone else seen this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude suddenly inserted a Chinese character in an English sentence:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z65k01nw8k1g1.png?width=988&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05108772771c1b7ec6b7ac81675ac3f153a0e7cf"&gt;https://preview.redd.it/z65k01nw8k1g1.png?width=988&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05108772771c1b7ec6b7ac81675ac3f153a0e7cf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Separate_Bonus_2234"&gt; /u/Separate_Bonus_2234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oye74t/claude_randomly_added_chinese_text_anyone_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oye74t/claude_randomly_added_chinese_text_anyone_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oye74t/claude_randomly_added_chinese_text_anyone_else/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-16T06:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyi7fv</id>
    <title>Working on a Local LLM Device</title>
    <updated>2025-11-16T10:16:18+00:00</updated>
    <author>
      <name>/u/Lonely-Marzipan-9473</name>
      <uri>https://old.reddit.com/user/Lonely-Marzipan-9473</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lonely-Marzipan-9473"&gt; /u/Lonely-Marzipan-9473 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1oyi6xv/working_on_a_local_llm_device/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oyi7fv/working_on_a_local_llm_device/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oyi7fv/working_on_a_local_llm_device/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-16T10:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxw4ir</id>
    <title>Ryzen AI MAX+ 395 - LLM metrics</title>
    <updated>2025-11-15T16:33:39+00:00</updated>
    <author>
      <name>/u/Armageddon_80</name>
      <uri>https://old.reddit.com/user/Armageddon_80</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MACHINE:&lt;/strong&gt; AMD Ryzen AI MAX+ 395 &amp;quot;Strix Halo&amp;quot; (Radeon 8060s) 128GB Ram &lt;/p&gt; &lt;p&gt;&lt;strong&gt;OS:&lt;/strong&gt; &lt;strong&gt;Windows 11 pro&lt;/strong&gt; 25H2 build 26200.7171 (15/11/25)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;INFERENCE ENGINES:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lemonade V9.0.2&lt;/li&gt; &lt;li&gt;LMstudio 0.3.31 (build7)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TLDR;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm gonna start saying that i thought I was tech savvy, until i tried to setup this pc with Linux... I felt like my GF when i try to explain her about AI...&lt;/p&gt; &lt;p&gt;If you want to be up and running in no time, stick with Window, download AMD Adrenaline and let it install all drivers needed. That's it, your system is set up.&lt;br /&gt; Then install whatever inference engine and models you want to run. &lt;/p&gt; &lt;p&gt;I would reccomend Lemonade (supported by AMD) but the python API is the generic OpenAI style while LMstudio Python API is more friendly. Up to you.&lt;/p&gt; &lt;p&gt;Here i attached results from different models to give an idea:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LMstudio Metrics:&lt;/strong&gt; &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Rocm engine&lt;/th&gt; &lt;th align="left"&gt;Vulkan engine&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenAI gpt-oss-20b MXFP4 (RAM 11.7gb)&lt;/td&gt; &lt;td align="left"&gt;66 TPS (0.05sec TTFT)&lt;/td&gt; &lt;td align="left"&gt;65 TPS (0.1 TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-30b-a3b-2507 GGUF Q4_K_M (RAM 17.64gb)&lt;/td&gt; &lt;td align="left"&gt;66 TPS (0.06sec TTFT)&lt;/td&gt; &lt;td align="left"&gt;78 TPS (0.1 TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 3 12b GGUF Q4_K_M (RAM 7.19GB)&lt;/td&gt; &lt;td align="left"&gt;23 TPS (0.07 sec TTFT)&lt;/td&gt; &lt;td align="left"&gt;26 TPS (0.1 TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Granite -4-h-small 32B GGUF Q4_K_M (RAM 19.3GB)&lt;/td&gt; &lt;td align="left"&gt;28 TPS (0.1 sec TTFT)&lt;/td&gt; &lt;td align="left"&gt;30 TPS (0.2 TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Granite -4-h-Tiny 7B GGUF Q4_K_M (RAM 4.2 GB)&lt;/td&gt; &lt;td align="left"&gt;97 TPS (0.06 TTFT)&lt;/td&gt; &lt;td align="left"&gt;97 TPS (0.07 TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Vl-4b GGUF Q4_K_M (RAM2.71 GB)&lt;/td&gt; &lt;td align="left"&gt;57 TPS (0.05sec TTFT)&lt;/td&gt; &lt;td align="left"&gt;65 TPS (0.05 TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Lemonade Metrics:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Running on&lt;/th&gt; &lt;th align="left"&gt;Token Per Second&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;LLama-3.2-1B-FLM&lt;/td&gt; &lt;td align="left"&gt;NPU&lt;/td&gt; &lt;td align="left"&gt;42 TPS (0.4sec TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-4B-Instruct-2507-FLM&lt;/td&gt; &lt;td align="left"&gt;NPU&lt;/td&gt; &lt;td align="left"&gt;14.5 TPS (0.9sec TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-4b-Instruct-2507-GGUF&lt;/td&gt; &lt;td align="left"&gt;GPU&lt;/td&gt; &lt;td align="left"&gt;72 TPS (0.04sec TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-30B-A3B-instruct GGUF&lt;/td&gt; &lt;td align="left"&gt;GPU&lt;/td&gt; &lt;td align="left"&gt;74 TPS (0.1sec TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen-2.5-7B-Instruct-Hybrid&lt;/td&gt; &lt;td align="left"&gt;NPU+GPU&lt;/td&gt; &lt;td align="left"&gt;39 TPS (0.6sec TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;LMstudio (No NPU) is faster with Vulkan llama.cpp engine rather than Rocm llama.cpp engine (bad bad AMD). &lt;/li&gt; &lt;li&gt;Lemonade when using GGUF model perform the same as LMS with Vulkan.&lt;/li&gt; &lt;li&gt;Lemonade offers also &lt;em&gt;NPU only mode&lt;/em&gt; (very power efficient but at 20% of GPU speed) perfect for overnight activities, and &lt;em&gt;Hybrid mode (NPU+GPU)&lt;/em&gt; useful for large context/complex prompts. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ryzen AI MAX+ APU really shines with MOE models, by leveraging the capability to load any size of model while balancing the memory bandwith's &amp;quot;limit&amp;quot; with activation of smaller experts (3B experts @ 70 TPS).&lt;br /&gt; A nice surprise is the new Granite 4 hybrid model series (mamba-2 architecture) where the 7B tiny runs at almost 100TPS and the 32B &lt;a href="mailto:small@28TPS"&gt;small@28TPS&lt;/a&gt;.&lt;br /&gt; With dense models TPS slows down proportionally to size, on different scales depending on model but generally 12B@23TPS , 7B@40TPS, 4B@&amp;gt;70TPS.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;END OF TLDR.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lemonade V9.0.2&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Lemonade Server is a server interface that uses the standard Open AI API allowing applications to integrate with local LLMs that run on your own PC's NPU and GPU.&lt;/p&gt; &lt;p&gt;So far is the only program that can easily switch between:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;1) only GPU:&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;uses the classic&lt;/em&gt; &amp;quot;GGUF&amp;quot; models that runs on iGPU/GPU. On my hardware the model runs on the Radeon 8060s. It can run basically anything, since i can allocate as much RAM i want for the gpu)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;2) GPU + NPU:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;uses niche &amp;quot;OGA&amp;quot; models (ONNXRuntime GenAI).&lt;br /&gt; This is an Hybrid mode that split the inference in 2 steps:&lt;/p&gt; &lt;p&gt;- 1st step uses NPU for the prefill phase (prompt and context ingestion) improving TTFT (time to first token)&lt;/p&gt; &lt;p&gt;- 2nd step uses GPU to handle the decode phase (generation), where high memory bandwidth is critical improving TPS (Tokens Per Second)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;3) only NPU:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Uses &amp;quot;OGA&amp;quot; models or &amp;quot;FLM&amp;quot; models (FastFlowLM).&lt;br /&gt; All inference is executed by the NPU. It's slower than GPU (TPS), but is extremely power efficient compared to GPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LMstudio 0.3.31 (build7)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;LMstudio doesnt need any presentation. Without going too exotic, you can run only GGUF models(GPU). Ollama can also be used with no problem at cost of some performance losses. The big advantage of LMstudio compared to Ollama is that LMS allows you to choose the Runtime to use for inference, improving TPS (speed). We have 2 options:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;1) Rocm llama.cpp v1.56.0&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Rocm is a software stack developed by AMD for GPU-accelerated high-performance computing (HPC). Like CUDA for Nvidia. So this is a llama.cpp version optimized for AMD gpus.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;2) Vulkan llama.cpp v.156.0&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Vulkan is a cross-platform and open standard for 3D graphics and computing API that optimizes performances for GPU workloads. So this is a llama.cpp version optimized for gpus in general via Vulkan.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Whatever option you choose, remember the engine only apply to GGUF files (basically dont apply to OpenAI GTP-oss MXPF4)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results with LMstudio&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;(see table above)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Well, clearly Vulkan Engine is equal or faster than Rocm engine.&lt;/p&gt; &lt;p&gt;Honestly it's difficult to see any difference in this kind of chit-chat with the LLM, but difference could become noticeable if your are processing batch of documents or in any multistep agent pipeline, where time is adding up at every step.&lt;/p&gt; &lt;p&gt;It's funny how Rocm from AMD (the manufacturer of my Halo Strix) is neither faster or energy efficient compared to the more generic Vulkan. The good thing is that while AMD keep improving drivers and software, eventually the situation will flip and we can expect even faster performances. Nonetheless, I'm not complaining about current performances at all :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results with Lemonade&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;(see table above)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've downloaded other models (I know i know) but models are massive and with these kind of machines, the bottleneck is the internet speed connection (and my patience). Also notice that Lemonade doesnt provide as many models as LMstudio.&lt;/p&gt; &lt;p&gt;Also notice that AMD Adrenaline doesnt return any metrics about the NPU. &lt;em&gt;Only think i can say, is that during inference with NPU the cooling fan dont even start, no matter how many tokens are generated. Meaning the power used must be really, really small.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Personal thoughts&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The advantage of having an Hybrid model is only in the prefilling part of the inference, Windows shows clearly a burst (short and high peak) on the NPU usage at the beginning of inference, the rest of generations is off loaded to the GPU as any GGUF model.&lt;/p&gt; &lt;p&gt;Completely different story with only NPU models, that's perfect for overnight works, where speed is not necessary but energy efficiency is, ie: on battery powered devices.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If electric power is not a constrain (at home/office use), then the power usage of NPU needs to be measured before to claim the miracle:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;the NPU speed is 20% compared to GPU meaning it will take X5 more time&lt;/strong&gt; &lt;strong&gt;to do the same job of the GPU.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;thus NPU power usage must be at least 5 times less than GPU otherwise it doesn't really make sense at home. Again different story is for battery powered devices.&lt;/p&gt; &lt;p&gt;In my observations GPU runs around 110W at full inference, so NPU should consume less than 20W which is possible since fan never started.&lt;br /&gt; NPU are very promising, but power consumption should be measured.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;I hope this was helpful (after 4 hours of tests and writing!) and can clarify wether this Ryzen AI max is adapt to you.&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;It is definitely for me, it runs everything you throw at it; with this beast I even replaced my Xbox series X to play BF6.&lt;/em&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Armageddon_80"&gt; /u/Armageddon_80 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxw4ir/ryzen_ai_max_395_llm_metrics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxw4ir/ryzen_ai_max_395_llm_metrics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oxw4ir/ryzen_ai_max_395_llm_metrics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-15T16:33:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyku8j</id>
    <title>Integrate AI agent with Zed or Vscode</title>
    <updated>2025-11-16T12:45:20+00:00</updated>
    <author>
      <name>/u/AirportAcceptable522</name>
      <uri>https://old.reddit.com/user/AirportAcceptable522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Cursor pro, but I wanted some way to use an ollama model in any editor like vscode or zed. Which model would be suitable and how to integrate?&lt;/p&gt; &lt;p&gt;I wanted to do the same thing I do with the cursor agent, but with a more specific and isolated model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AirportAcceptable522"&gt; /u/AirportAcceptable522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oyku8j/integrate_ai_agent_with_zed_or_vscode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oyku8j/integrate_ai_agent_with_zed_or_vscode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oyku8j/integrate_ai_agent_with_zed_or_vscode/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-16T12:45:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz7cgs</id>
    <title>TikTok ¬∑ Rawdog Reverend</title>
    <updated>2025-11-17T04:48:04+00:00</updated>
    <author>
      <name>/u/New_Pomegranate_1060</name>
      <uri>https://old.reddit.com/user/New_Pomegranate_1060</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oz7cgs/tiktok_rawdog_reverend/"&gt; &lt;img alt="TikTok ¬∑ Rawdog Reverend" src="https://external-preview.redd.it/p5ngHChl-SkIYYghfi1mS5ltuLNLSd7YMvD1F27R5fk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c57e024bb7247cb99111f24125a2866c7c66ac39" title="TikTok ¬∑ Rawdog Reverend" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Pomegranate_1060"&gt; /u/New_Pomegranate_1060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tiktok.com/t/ZTMKg2AtW/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oz7cgs/tiktok_rawdog_reverend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oz7cgs/tiktok_rawdog_reverend/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-17T04:48:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozkeey</id>
    <title>üî• Perplexity AI PRO - 1-Year Plan - Limited Time SUPER PROMO! 90% OFF!</title>
    <updated>2025-11-17T16:13:32+00:00</updated>
    <author>
      <name>/u/Verza-</name>
      <uri>https://old.reddit.com/user/Verza-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ozkeey/perplexity_ai_pro_1year_plan_limited_time_super/"&gt; &lt;img alt="üî• Perplexity AI PRO - 1-Year Plan - Limited Time SUPER PROMO! 90% OFF!" src="https://preview.redd.it/sen0jy3pdu1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=420c632da047f12bceae539295d4422e6d29c1f9" title="üî• Perplexity AI PRO - 1-Year Plan - Limited Time SUPER PROMO! 90% OFF!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get Perplexity AI PRO (1-Year) ‚Äì at 90% OFF!&lt;/p&gt; &lt;p&gt;Order here: &lt;a href="https://cheapgpts.store/Perplexity"&gt;CHEAPGPT.STORE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Plan: 12 Months&lt;/p&gt; &lt;p&gt;üí≥ Pay with: PayPal or Revolut&lt;/p&gt; &lt;p&gt;Reddit reviews: &lt;a href="https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu"&gt;FEEDBACK POST&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TrustPilot: &lt;a href="https://www.trustpilot.com/review/cheapgpt.store"&gt;TrustPilot FEEDBACK&lt;/a&gt;&lt;br /&gt; Bonus: Apply code PROMO5 for $5 OFF your order! &lt;/p&gt; &lt;p&gt;BONUS!: Enjoy the AI Powered automated web browser. (Presented by Perplexity) included!&lt;/p&gt; &lt;p&gt;Trusted and the cheapest!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Verza-"&gt; /u/Verza- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sen0jy3pdu1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozkeey/perplexity_ai_pro_1year_plan_limited_time_super/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ozkeey/perplexity_ai_pro_1year_plan_limited_time_super/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-17T16:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozafjk</id>
    <title>Lm playground with open api connections/lmstudio/ollama</title>
    <updated>2025-11-17T07:50:42+00:00</updated>
    <author>
      <name>/u/AceCustom1</name>
      <uri>https://old.reddit.com/user/AceCustom1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ozafjk/lm_playground_with_open_api/"&gt; &lt;img alt="Lm playground with open api connections/lmstudio/ollama" src="https://preview.redd.it/n5xlzljyrr1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2c4d2aa45ab97b5da83f29d35f08bdf1fb80c24" title="Lm playground with open api connections/lmstudio/ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AceCustom1"&gt; /u/AceCustom1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n5xlzljyrr1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozafjk/lm_playground_with_open_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ozafjk/lm_playground_with_open_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-17T07:50:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyzc7l</id>
    <title>Looking for a truly open source web ui for using with my LLMs</title>
    <updated>2025-11-16T22:34:05+00:00</updated>
    <author>
      <name>/u/sadism_popsicle</name>
      <uri>https://old.reddit.com/user/sadism_popsicle</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sadism_popsicle"&gt; /u/sadism_popsicle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1oyv36m/looking_for_a_truly_open_source_web_ui_for_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oyzc7l/looking_for_a_truly_open_source_web_ui_for_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oyzc7l/looking_for_a_truly_open_source_web_ui_for_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-16T22:34:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyye2y</id>
    <title>The best local commercial hardware for coding with LLMs?</title>
    <updated>2025-11-16T21:55:24+00:00</updated>
    <author>
      <name>/u/Grouchy_Key6227</name>
      <uri>https://old.reddit.com/user/Grouchy_Key6227</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Is the Mac Studio M3 Ultra the best ‚Äúlocal rack‚Äù for coding and LLM inference?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;br /&gt; This is my first post on Reddit; I‚Äôve never written anything here before, so I really appreciate any advice or opinions from the community.&lt;/p&gt; &lt;p&gt;I‚Äôve been programming for about 10 years. Like many people, I started relying on tools like vibe-coding, PRD generators, agents, and lately Claude 3.5 with the 200-max subscription. I also use Codex (the 20 USD plan), and on my NAS I run some small local models.&lt;/p&gt; &lt;p&gt;But the problem is always the same: cloud LLM services start cheap and then become arbitrary. Prices go up, limits change, usage rules get stricter. At this point, my ‚Äúweek‚Äù of Claude Max doesn‚Äôt even last me 4 days with heavy use. And I‚Äôm stuck with windows, quotas, schedules, and restrictions that I can‚Äôt control.&lt;/p&gt; &lt;p&gt;So I started thinking: &lt;em&gt;‚ÄúShould I just build my own rack?‚Äù&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I began researching VRAM, compute power, bandwidth, power consumption, pricing, and clustering.&lt;/p&gt; &lt;p&gt;Here are my personal conclusions:&lt;/p&gt; &lt;h1&gt;1. There is no perfect machine&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;High VRAM + high compute power = 10,000 USD or more.&lt;/li&gt; &lt;li&gt;High VRAM but low compute = systems like AIM 395+, which are affordable but choke on heavy models.&lt;/li&gt; &lt;li&gt;Clustering several cheaper GPUs = the bottleneck becomes networking, synchronization, or constant maintenance.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. High-end GPUs are hard to get in Mexico&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;RTX 4090s disappear fast because most people buy ‚Äúone generation behind.‚Äù&lt;/li&gt; &lt;li&gt;RTX 5090s are over 3,000 USD each.&lt;/li&gt; &lt;li&gt;To get ~120 GB VRAM I‚Äôd need 3 GPUs = ~9,000 USD, not including PSU, rack, motherboard, cooling, etc.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. The middle ground I found: Mac Studio M3 Ultra&lt;/h1&gt; &lt;p&gt;My use case is NOT training or heavy finetuning.&lt;br /&gt; I only want:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;My own ‚Äúpersonal rack‚Äù to replace Claude, Cursor, and similar services.&lt;/li&gt; &lt;li&gt;Local inference with no limits, no queues, and no usage windows.&lt;/li&gt; &lt;li&gt;Big models (70B‚Äì90B), and eventually reduced versions of ~400B models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What convinced me about the M3 Ultra:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A lot of unified memory.&lt;/li&gt; &lt;li&gt;Good tokens-per-second performance for large models optimized for Apple Silicon.&lt;/li&gt; &lt;li&gt;Far less noise, lower energy consumption, and zero maintenance compared to running 3 GPUs in a big workstation.&lt;/li&gt; &lt;li&gt;No dealing with drivers, giant PSUs, RPC clustering issues, heat, or random hardware failures.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Cost&lt;/h1&gt; &lt;p&gt;A high-end Mac Studio M3 Ultra setup ends up costing around 10,000 USD‚Äîroughly the same as building a 3√ó5090 cluster, but with far fewer headaches.&lt;/p&gt; &lt;h1&gt;My question&lt;/h1&gt; &lt;p&gt;For my use case (inference only, multi-agent workflows, RAG, analysis, ‚ÄúClaude/Cursor replacement‚Äù),&lt;br /&gt; &lt;strong&gt;do you think the Mac Studio M3 Ultra is a good choice?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Or is there a better option with a more balanced price‚Äìperformance‚ÄìVRAM‚Äìmaintenance ratio, especially considering how inflated GPU prices are in Mexico?&lt;/p&gt; &lt;p&gt;I‚Äôd really appreciate any technical insights or personal experiences with Apple Silicon, AI Max/AIM 395, DGX Spark, or multi-GPU setups with 4090/5090.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grouchy_Key6227"&gt; /u/Grouchy_Key6227 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oyye2y/the_best_local_commercial_hardware_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oyye2y/the_best_local_commercial_hardware_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oyye2y/the_best_local_commercial_hardware_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-16T21:55:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozswfh</id>
    <title>VSC Format Won't Cut Your AI Costs by 71% (Despite the Hype)</title>
    <updated>2025-11-17T21:26:04+00:00</updated>
    <author>
      <name>/u/Particular-Room8732</name>
      <uri>https://old.reddit.com/user/Particular-Room8732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did the math on VSC format (the CSV-without-headers that's trending) and found: &lt;/p&gt; &lt;p&gt;- Yes, it saves 71% on the data portion&lt;br /&gt; - But only 3% on your total prompt (2,125 ‚Üí 2,036 tokens)&lt;br /&gt; - Actual savings: $0.89 per 10,000 API calls Meanwhile, debugging&lt;br /&gt; &amp;quot;Iphone,1999.90&amp;quot; instead of&lt;br /&gt; {&amp;quot;name&amp;quot;:&amp;quot;Iphone&amp;quot;,&amp;quot;price&amp;quot;:1999.90} is a nightmare. &lt;/p&gt; &lt;p&gt;For those who've tried it: Did you stick with it? Was the token savings worth the loss of readability?&lt;/p&gt; &lt;p&gt;Full analysis: &lt;a href="https://www.superfox.ai/blog/vsc-format-ai-token-costs-truth"&gt;https://www.superfox.ai/blog/vsc-format-ai-token-costs-truth&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Particular-Room8732"&gt; /u/Particular-Room8732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozswfh/vsc_format_wont_cut_your_ai_costs_by_71_despite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozswfh/vsc_format_wont_cut_your_ai_costs_by_71_despite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ozswfh/vsc_format_wont_cut_your_ai_costs_by_71_despite/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-17T21:26:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozkg1m</id>
    <title>JSON input prompts</title>
    <updated>2025-11-17T16:15:12+00:00</updated>
    <author>
      <name>/u/CommunityBrave822</name>
      <uri>https://old.reddit.com/user/CommunityBrave822</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen a lot of posts and comments about JSON structured output. But little information on using JSON format for prompt (string) as well.&lt;/p&gt; &lt;p&gt;What is your experience using JSON as prompt input versus plain text (or Markdown).&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;p&gt;{&amp;quot;request&amp;quot;: &amp;quot;write a review of Toyota Camry 2005. Use online reviews&amp;quot;, &amp;quot;answer_length&amp;quot;: &amp;quot;300-500 words&amp;quot;, &amp;quot;online reviews&amp;quot;: [&amp;quot;...&amp;quot;, &amp;quot;...&amp;quot;, &amp;quot;...&amp;quot;, ...]}&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommunityBrave822"&gt; /u/CommunityBrave822 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozkg1m/json_input_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozkg1m/json_input_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ozkg1m/json_input_prompts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-17T16:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozvjcz</id>
    <title>DSPy on a Pi: Cheap Prompt Optimization with GEPA and Qwen3</title>
    <updated>2025-11-17T23:08:38+00:00</updated>
    <author>
      <name>/u/parenthethethe</name>
      <uri>https://old.reddit.com/user/parenthethethe</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/parenthethethe"&gt; /u/parenthethethe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://leebutterman.com/2025/11/01/prompt-optimization-on-a-raspberry-pi.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozvjcz/dspy_on_a_pi_cheap_prompt_optimization_with_gepa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ozvjcz/dspy_on_a_pi_cheap_prompt_optimization_with_gepa/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-17T23:08:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozli8v</id>
    <title>Reactive Agents: AI agents that self-optimize after every interaction</title>
    <updated>2025-11-17T16:54:26+00:00</updated>
    <author>
      <name>/u/No_Heart_159</name>
      <uri>https://old.reddit.com/user/No_Heart_159</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Heart_159"&gt; /u/No_Heart_159 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ozjz15"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozli8v/reactive_agents_ai_agents_that_selfoptimize_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ozli8v/reactive_agents_ai_agents_that_selfoptimize_after/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-17T16:54:26+00:00</published>
  </entry>
</feed>
