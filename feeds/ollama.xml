<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-08T22:05:53+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ophkbj</id>
    <title>PromptShield Labs - An open-source playground for new AI experiments</title>
    <updated>2025-11-05T22:34:18+00:00</updated>
    <author>
      <name>/u/kekePower</name>
      <uri>https://old.reddit.com/user/kekePower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I recently created &lt;strong&gt;PromptShield Labs&lt;/strong&gt; - a place where I post new open-source projects and experiments I‚Äôm testing or just having fun with.&lt;/p&gt; &lt;p&gt;Thought I‚Äôd share it here in case anyone wants to check it out, use something, or maybe even contribute.&lt;/p&gt; &lt;p&gt;üîó &lt;a href="https://labs.promptshield.io"&gt;https://labs.promptshield.io&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kekePower"&gt; /u/kekePower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ophkbj/promptshield_labs_an_opensource_playground_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ophkbj/promptshield_labs_an_opensource_playground_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ophkbj/promptshield_labs_an_opensource_playground_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T22:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1op7tmy</id>
    <title>SQL Chat Agent</title>
    <updated>2025-11-05T16:40:06+00:00</updated>
    <author>
      <name>/u/stefsk8</name>
      <uri>https://old.reddit.com/user/stefsk8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone here worked with advanced SQL chat agents ones that can translate natural language into SQL queries and return results intelligently using ollama and potential other tools?&lt;/p&gt; &lt;p&gt;I‚Äôm not talking about the simple ‚Äútext-to-SQL‚Äù demos, but more advanced setups where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The LLM actually understands the connected database (schema, relationships, etc.)&lt;/li&gt; &lt;li&gt;Existing data is leveraged to train or fine-tune the model on the database structure and relationships&lt;/li&gt; &lt;li&gt;The system can accurately map business language to technical terms, so it truly understands what the user is asking for&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious if anyone has built or experimented with something like this and how you approached it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stefsk8"&gt; /u/stefsk8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1op7tmy/sql_chat_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1op7tmy/sql_chat_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1op7tmy/sql_chat_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T16:40:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooz2v1</id>
    <title>How is an LLM created?</title>
    <updated>2025-11-05T10:12:24+00:00</updated>
    <author>
      <name>/u/eworker8888</name>
      <uri>https://old.reddit.com/user/eworker8888</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eworker8888"&gt; /u/eworker8888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/eworker_ca/comments/1ooyzrp/how_is_an_llm_created/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ooz2v1/how_is_an_llm_created/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ooz2v1/how_is_an_llm_created/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T10:12:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1op4nze</id>
    <title>HELP me create an answer generating RAG AI setup</title>
    <updated>2025-11-05T14:41:26+00:00</updated>
    <author>
      <name>/u/Professional_Lake682</name>
      <uri>https://old.reddit.com/user/Professional_Lake682</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys.....Basically I want to feed the AI model my curriculum textbook Pdfs(around 500mb for a subject) without having to cut it in size because relevant info is spread through out the book. Then I‚Äôll make it generate theory specific answers for my prof exams to study from Preferably citing the info from the resources, including flow charts and relevant tables of info and at the very least mentioning (if not inputting) what diagrams would be related to my query/question. I need help from this community in choosing the right AI tool / work flow setting / LLM model and 101 setup tutorial for it I just really want this to stream line my preparation so that I can focus more on competitive exams. Thanks yall in advance!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional_Lake682"&gt; /u/Professional_Lake682 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1op4nze/help_me_create_an_answer_generating_rag_ai_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1op4nze/help_me_create_an_answer_generating_rag_ai_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1op4nze/help_me_create_an_answer_generating_rag_ai_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T14:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1opsmhq</id>
    <title>CS undergrad with a GTX 1650 (4GB) - Seeking advice to build a local, terminal-based coding assistant. Is this feasible?</title>
    <updated>2025-11-06T07:29:25+00:00</updated>
    <author>
      <name>/u/overdosedBIGc</name>
      <uri>https://old.reddit.com/user/overdosedBIGc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm a CS undergrad trying to build a local, &lt;em&gt;free&lt;/em&gt; homelab to get better at AI and software development.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My End Goal:&lt;/strong&gt; I'm not just looking to run a chatbot. I'd love to create a &lt;strong&gt;terminal-based, context-aware coding assistant&lt;/strong&gt; (something that works like &lt;code&gt;aider-chat&lt;/code&gt; or similar) that I can use for my CS projects for agentic-style tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Problem:&lt;/strong&gt; I've been using cloud APIs (like Gemini Pro), but my free access won't last forever. I'm trying to build something sustainable, but my main hardware bottleneck is my &lt;strong&gt;GTX 1650 with 4GB of VRAM&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I'm honestly feeling pretty lost and would be very grateful for some guidance:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Is this goal realistic with 4GB VRAM?&lt;/strong&gt; Or am I setting myself up for frustration trying to get &lt;em&gt;useful code generation&lt;/em&gt; from such a small card?&lt;/li&gt; &lt;li&gt;What are the best &lt;strong&gt;coding-focused models&lt;/strong&gt; that can &lt;em&gt;actually&lt;/em&gt; run well on 4GB? I've seen terms like &lt;code&gt;Phi-3&lt;/code&gt;, &lt;code&gt;GGUF&lt;/code&gt;, &lt;code&gt;DeepSeekCoder&lt;/code&gt;, etc., but I'm not sure what's usable vs. just a toy.&lt;/li&gt; &lt;li&gt;What's the best software stack for this? Is &lt;code&gt;Ollama&lt;/code&gt; &lt;strong&gt;+ a terminal UI&lt;/strong&gt; the best way to go?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'm at the point where I'm just drowning in documentation. If you have a similar low-VRAM setup, I would be so thankful if you could &lt;strong&gt;share your builds, repos,&lt;/strong&gt; &lt;code&gt;Ollama&lt;/code&gt; &lt;strong&gt;configs, or any guides&lt;/strong&gt; you used. Seeing a working example would help me so much.&lt;/p&gt; &lt;p&gt;&lt;em&gt;I'm also still confused‚Äîwhy do &amp;quot;open&amp;quot; models like Llama also appear on paid &amp;quot;pay-as-you-go&amp;quot; APIs? Am I right in thinking you're just paying for their server's hardware + convenience?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Thanks for taking the time to read this. Any advice you can offer would be a huge help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/overdosedBIGc"&gt; /u/overdosedBIGc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1opsmhq/cs_undergrad_with_a_gtx_1650_4gb_seeking_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1opsmhq/cs_undergrad_with_a_gtx_1650_4gb_seeking_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1opsmhq/cs_undergrad_with_a_gtx_1650_4gb_seeking_advice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-06T07:29:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1optxdc</id>
    <title>Evolutionary AGI (simulated consciousness) ‚Äî already quite advanced, I‚Äôve hit my limits; looking for passionate collaborators</title>
    <updated>2025-11-06T08:54:54+00:00</updated>
    <author>
      <name>/u/Goat_bless</name>
      <uri>https://old.reddit.com/user/Goat_bless</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1optxdc/evolutionary_agi_simulated_consciousness_already/"&gt; &lt;img alt="Evolutionary AGI (simulated consciousness) ‚Äî already quite advanced, I‚Äôve hit my limits; looking for passionate collaborators" src="https://external-preview.redd.it/590M-JLBgtcbU1VW6gsYy7bL6dfjvOv7t_2yUZB4mlM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67cd49fa84acfeb67db764f8e6fecb5f4a52f5d3" title="Evolutionary AGI (simulated consciousness) ‚Äî already quite advanced, I‚Äôve hit my limits; looking for passionate collaborators" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Goat_bless"&gt; /u/Goat_bless &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/SpendinFR/V1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1optxdc/evolutionary_agi_simulated_consciousness_already/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1optxdc/evolutionary_agi_simulated_consciousness_already/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-06T08:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1openho</id>
    <title>"On-the-fly" code reviews with ollama. It kinda works..</title>
    <updated>2025-11-05T20:45:14+00:00</updated>
    <author>
      <name>/u/EMurph55</name>
      <uri>https://old.reddit.com/user/EMurph55</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I created this library for a bit of fun to see if it would work, and I am finding it to be somewhat helpful tbh. Thought I'd share it here to see if anyone had any similar tools or ideas: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/whatever555/ollama-watcher"&gt;https://github.com/whatever555/ollama-watcher&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EMurph55"&gt; /u/EMurph55 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1openho/onthefly_code_reviews_with_ollama_it_kinda_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1openho/onthefly_code_reviews_with_ollama_it_kinda_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1openho/onthefly_code_reviews_with_ollama_it_kinda_works/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T20:45:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1op72oi</id>
    <title>GLM-4.5V model for local computer use</title>
    <updated>2025-11-05T16:12:45+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1op72oi/glm45v_model_for_local_computer_use/"&gt; &lt;img alt="GLM-4.5V model for local computer use" src="https://external-preview.redd.it/MWVndHJ0dmxxZ3pmMbWADQNBSkImjVESNjfi_q43l9ostHKNGAFX_QJdfnS0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0dd92a331e5c40438053cd04ccd3ee2ddc99f10a" title="GLM-4.5V model for local computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either: Locally via Hugging Face Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vy2u914mqgzf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1op72oi/glm45v_model_for_local_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1op72oi/glm45v_model_for_local_computer_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T16:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq6crr</id>
    <title>"Format" parameter</title>
    <updated>2025-11-06T18:12:54+00:00</updated>
    <author>
      <name>/u/Content-Baby2782</name>
      <uri>https://old.reddit.com/user/Content-Baby2782</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im wondering if anyone could point me in the right direction to why im not getting the response format im requesting.&lt;/p&gt; &lt;p&gt;Below is my API request to Ollama cloud, i think i've got the &amp;quot;format&amp;quot; field specified correctly accoring to &lt;a href="https://docs.ollama.com/capabilities/structured-outputs"&gt;https://docs.ollama.com/capabilities/structured-outputs&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;array:8 [‚ñº &amp;quot;model&amp;quot; =&amp;gt; &amp;quot; deepseek-v3.1:671b-cloud &amp;quot; &amp;quot;messages&amp;quot; =&amp;gt; array:2 [‚ñº 0 =&amp;gt; array:2 [‚ñº &amp;quot;role&amp;quot; =&amp;gt; &amp;quot; system &amp;quot; &amp;quot;content&amp;quot; =&amp;gt; &amp;quot;&amp;quot;&amp;quot; You are a fact checker. You will be given a fact and you will need to determine if it is true or false.\ \n You will also need to provide the reasoning for your decision.\ \n &amp;quot;&amp;quot;&amp;quot; ] 1 =&amp;gt; array:2 [‚ñº &amp;quot;role&amp;quot; =&amp;gt; &amp;quot; user &amp;quot; &amp;quot;content&amp;quot; =&amp;gt; &amp;quot; The sky is blue &amp;quot; ] ] &amp;quot;stream&amp;quot; =&amp;gt; false &amp;quot;top_p&amp;quot; =&amp;gt; 0.95 &amp;quot;top_k&amp;quot; =&amp;gt; 100 &amp;quot;temperature&amp;quot; =&amp;gt; 0 &amp;quot;max_tokens&amp;quot; =&amp;gt; 50 &amp;quot;format&amp;quot; =&amp;gt; {#734 ‚ñº +&amp;quot;type&amp;quot;: &amp;quot; object &amp;quot; +&amp;quot;properties&amp;quot;: {#733 ‚ñº +&amp;quot;fact&amp;quot;: {#730 ‚ñº +&amp;quot;type&amp;quot;: &amp;quot; boolean &amp;quot; +&amp;quot;description&amp;quot;: &amp;quot; Is the fact true &amp;quot; } +&amp;quot;reasoning&amp;quot;: {#731 ‚ñº +&amp;quot;type&amp;quot;: &amp;quot; string &amp;quot; +&amp;quot;description&amp;quot;: &amp;quot; The reasoning for the decision &amp;quot; } +&amp;quot;colour&amp;quot;: {#732 ‚ñº +&amp;quot;type&amp;quot;: &amp;quot; string &amp;quot; +&amp;quot;description&amp;quot;: &amp;quot; The colour of the fact &amp;quot; } } } ] &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Content-Baby2782"&gt; /u/Content-Baby2782 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oq6crr/format_parameter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oq6crr/format_parameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oq6crr/format_parameter/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-06T18:12:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1opu9i9</id>
    <title>Running models on CPU. Is it just stupid or is there a way?</title>
    <updated>2025-11-06T09:16:44+00:00</updated>
    <author>
      <name>/u/Galgaldas</name>
      <uri>https://old.reddit.com/user/Galgaldas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I own hostinger best plan vps and downloaded some deepseek models. And even smallest one hits CPU usage to 99.7%. So wondering, should I not even try running it on CPU and run it only on GPU? Sorry if question too nooby, just starting out&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Galgaldas"&gt; /u/Galgaldas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1opu9i9/running_models_on_cpu_is_it_just_stupid_or_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1opu9i9/running_models_on_cpu_is_it_just_stupid_or_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1opu9i9/running_models_on_cpu_is_it_just_stupid_or_is/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-06T09:16:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1opat7j</id>
    <title>We just released a multi-agent framework. Please break it.</title>
    <updated>2025-11-05T18:25:06+00:00</updated>
    <author>
      <name>/u/wikkid_lizard</name>
      <uri>https://old.reddit.com/user/wikkid_lizard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1opat7j/we_just_released_a_multiagent_framework_please/"&gt; &lt;img alt="We just released a multi-agent framework. Please break it." src="https://preview.redd.it/xjebyb25ehzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67e99671e15cf6167186161f13f586fbf2286b9b" title="We just released a multi-agent framework. Please break it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! We just released Laddr, a lightweight multi-agent architecture framework for building AI systems where multiple agents can talk, coordinate, and scale together.&lt;/p&gt; &lt;p&gt;If you're experimenting with agent workflows, orchestration, automation tools, or just want to play with agent systems, would love for you to check it out.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/AgnetLabs/laddr"&gt;https://github.com/AgnetLabs/laddr&lt;/a&gt; &lt;br /&gt; Docs: &lt;a href="https://laddr.agnetlabs.com/"&gt;https://laddr.agnetlabs.com&lt;/a&gt; &lt;br /&gt; Questions / Feedback: [&lt;a href="mailto:info@agnetlabs.com"&gt;info@agnetlabs.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@agnetlabs.com"&gt;info@agnetlabs.com&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;It's super fresh, so feel free to break it, fork it, star it, and tell us what sucks or what works. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wikkid_lizard"&gt; /u/wikkid_lizard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xjebyb25ehzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1opat7j/we_just_released_a_multiagent_framework_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1opat7j/we_just_released_a_multiagent_framework_please/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T18:25:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1opv612</id>
    <title>Experimenting with Mistral + Ollama after reading this book- some takeaways and open questions</title>
    <updated>2025-11-06T10:13:19+00:00</updated>
    <author>
      <name>/u/FoundSomeLogic</name>
      <uri>https://old.reddit.com/user/FoundSomeLogic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I recently finished reading &lt;strong&gt;Learn Mistral: Elevating Systems with Embeddings&lt;/strong&gt; and wanted to share some of the surprising things I picked up (and a few open questions I still have), especially since many of us here are working with local LLM workflows and tools like Ollama.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What struck me&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The author really dives into the ‚Äúwhy‚Äù behind embeddings and how they change the way we think about retrieval and alignment, so for me, it was refreshing to see a chapter not just on &lt;em&gt;‚Äúhow to embed text‚Äù&lt;/em&gt;, but on ‚Äúwhy this embedding helps integrate with a system like Ollama or similar tools‚Äù.&lt;/li&gt; &lt;li&gt;There‚Äôs a section where the book shows practical setups: pre-processing, embedding generation, combining with local models. I‚Äôm working with a Mistral-style model locally, and I found myself immediately scribbling notes about how I could adapt one of the workflows.&lt;/li&gt; &lt;li&gt;The clarity: Even though the topic is technical, it doesn‚Äôt assume you‚Äôre an elite ML researcher. It offers enough practical code snippets and real-world examples to experiment with. I tried out two of them this weekend and learned something useful (and made a few mistakes, which is always good!).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How this ties into what I do with Ollama&lt;/strong&gt;&lt;br /&gt; I run Ollama locally (on a decent machine, but nothing crazy). One of my ongoing challenges has been: ‚ÄúHow do I get the model to really understand my domain-specific data rather than just general chat behavior?‚Äù The book‚Äôs guidance around embeddings + index + retrieval + prompt design suddenly made more sense in that context. In short: I felt like I went from ‚ÄúI know Ollama can load the model and respond‚Äù ‚Üí ‚ÄúOkay, now how do I feed it knowledge and get it to reason in my domain?‚Äù.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One or two things I‚Äôm still thinking about&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The author mentions keeping embeddings fresh and versioned as your domain data grows. I wonder how folks here are doing that in production/local setups with Ollama: do you rebuild the entire index, keep incremental updates, or something else? If you‚Äôve tried this I‚Äôd love to hear your experience.&lt;/li&gt; &lt;li&gt;There‚Äôs a trade-off discussed between embedding size/complexity and cost/time. Locally it's manageable, but if you scale up you might hit bottlenecks. I‚Äôm curious what strategies others use to strike that balance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Would I recommend it?&lt;/strong&gt;&lt;br /&gt; Yes, if you‚Äôre using Ollama (or any local LLM stack) &lt;em&gt;and&lt;/em&gt; you‚Äôre ready to go beyond ‚Äújust chat with the model‚Äù and into ‚Äúlet the model reason with my data‚Äù, this book provides a solid step. It‚Äôs not a silver-bullet: you‚Äôll still need to adapt for your domain and do the engineering work, but it offers a clearer map.&lt;/p&gt; &lt;p&gt;Happy to share a few of my notes (code snippet, embedding library used, one prompt trick) if anyone is interested. Also curious: if you‚Äôve read it (or a similar book), what surprised you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FoundSomeLogic"&gt; /u/FoundSomeLogic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1opv612/experimenting_with_mistral_ollama_after_reading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1opv612/experimenting_with_mistral_ollama_after_reading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1opv612/experimenting_with_mistral_ollama_after_reading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-06T10:13:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqmk5n</id>
    <title>OpenMemory/Mem0</title>
    <updated>2025-11-07T05:56:08+00:00</updated>
    <author>
      <name>/u/AdCompetitive6193</name>
      <uri>https://old.reddit.com/user/AdCompetitive6193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdCompetitive6193"&gt; /u/AdCompetitive6193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenWebUI/comments/1oqmix5/openmemorymem0/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqmk5n/openmemorymem0/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oqmk5n/openmemorymem0/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-07T05:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq772p</id>
    <title>Advice appreciated: Here's how I'm trying to use Ollama at home</title>
    <updated>2025-11-06T18:43:59+00:00</updated>
    <author>
      <name>/u/Punnalackakememumu</name>
      <uri>https://old.reddit.com/user/Punnalackakememumu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have purchased a used Dell OptiPlex 9020 minitower that I am dedicating to use as an Ollama AI server.&lt;/p&gt; &lt;p&gt;CPU Intel(R) Core i5-4590 CPU @ 3.30GHz&lt;br /&gt; RAM 32 GB RAM&lt;br /&gt; Storage 465 GB SSD&lt;br /&gt; Graphics NVIDIA GeForce GTX 1050 Ti (4 GB)&lt;br /&gt; OS Linux Mint&lt;/p&gt; &lt;p&gt;I am trying to use AI to help me write a semi-autographical story. &lt;/p&gt; &lt;p&gt;AI on its own (Grok, DuckAi, etc.) seems to have trouble retaining character profiles the longer I interact with it. I can feed it a good descriptive character profile, and it uses it and adapts it based on the story development (characters can gain weight or get their hair cut, for example). However, if you have characters who aren't discussed after a couple of chapters, the AI seems to forget the details and create its own: suddenly Uncle Mario, the retired Italian racecar driver, is a redheaded guy who delivers baked goods.&lt;/p&gt; &lt;p&gt;I realize I have hardware constraints, so I'm planning to stick to a 7b LLM. I'm creating text only.&lt;/p&gt; &lt;p&gt;I'd like to have Ollama running on the Mint server using a fairly permissive LLM like Mistral 7b so it doesn't fuss at me about profanity, adult themes, etc. In a test, I tried to use AnythingLLM to inject data (so I could point it at a web page about a topic and have the model learn information that I want a character to know in story, but AnythingLLM complained about subject matter.&lt;/p&gt; &lt;p&gt;I'd like for it to allow me to access the server via a web browser on my regular PC or laptop in my network so that I'm not always creating while sitting in my workshop where the Mint system lives.&lt;/p&gt; &lt;p&gt;I'd like to have it store character profiles &amp;quot;offline&amp;quot; in a text file or something so it can access them if my main characters haven't interacted with someone in a little while.&lt;/p&gt; &lt;p&gt;So, I'm open to suggestions for software I can use for this effort.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Punnalackakememumu"&gt; /u/Punnalackakememumu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oq772p/advice_appreciated_heres_how_im_trying_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oq772p/advice_appreciated_heres_how_im_trying_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oq772p/advice_appreciated_heres_how_im_trying_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-06T18:43:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1opux53</id>
    <title>Asked my AI Agent to recommend me top 5 stocks to buy today :)</title>
    <updated>2025-11-06T09:58:19+00:00</updated>
    <author>
      <name>/u/FriendshipCreepy8045</name>
      <uri>https://old.reddit.com/user/FriendshipCreepy8045</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1opux53/asked_my_ai_agent_to_recommend_me_top_5_stocks_to/"&gt; &lt;img alt="Asked my AI Agent to recommend me top 5 stocks to buy today :)" src="https://preview.redd.it/zpbn8y18zlzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb36753aa1dd4e9a1fc0ece04720addd8ccb9bc9" title="Asked my AI Agent to recommend me top 5 stocks to buy today :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Everyone!&lt;/p&gt; &lt;p&gt;So some of you have seen the post about how I made my own local agent: &lt;strong&gt;&amp;quot;Agent Kurama&amp;quot;&lt;/strong&gt;, and many of you liked it. I couldn‚Äôt be happier, as some of you followed me, starred the repo, and most importantly, advised me on how to improve it.&lt;/p&gt; &lt;p&gt;Recently, I added more search tools and a summarizer for unbiased search and information handling, and this time I‚Äôll test it for real.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;I‚Äôll put my own ‚Çπ10,000 (or $100) into the stocks it recommends.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now, this fox made a huuuge report like 389 lines but here‚Äôs the conclusion of that report:&lt;/p&gt; &lt;p&gt;&lt;em&gt;&amp;quot;A balanced ‚Çπ10,000 portfolio of Groww‚Äôs flagship large-cap picks ‚Äî Reliance, HDFC Bank, Infosys, Tata Motors, and ITC ‚Äî fits the budget, offers sector diversification, and aligns with ‚Äútop-stock‚Äù recommendations.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;To be honest, these recommendations seem kinda obvious, but we‚Äôll see. Now I‚Äôll put equal money into those top 5 stocks and check back in 6 months :)&lt;/p&gt; &lt;p&gt;This is all educational and experimental - no financial advice, just me being curious &amp;amp; dumb &amp;gt;.&amp;lt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Project link:&lt;/strong&gt; &lt;a href="https://github.com/vedas-dixit/LocalAgent"&gt;https://github.com/vedas-dixit/LocalAgent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FriendshipCreepy8045"&gt; /u/FriendshipCreepy8045 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zpbn8y18zlzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1opux53/asked_my_ai_agent_to_recommend_me_top_5_stocks_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1opux53/asked_my_ai_agent_to_recommend_me_top_5_stocks_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-06T09:58:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqchak</id>
    <title>Epoch: LLMs that generate interactive UI instead of text walls</title>
    <updated>2025-11-06T22:05:04+00:00</updated>
    <author>
      <name>/u/ItzCrazyKns</name>
      <uri>https://old.reddit.com/user/ItzCrazyKns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oqchak/epoch_llms_that_generate_interactive_ui_instead/"&gt; &lt;img alt="Epoch: LLMs that generate interactive UI instead of text walls" src="https://preview.redd.it/elog79cngpzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2a6a284595f32b69211b6ce05cbcd3fd5a10860" title="Epoch: LLMs that generate interactive UI instead of text walls" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ItzCrazyKns"&gt; /u/ItzCrazyKns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/elog79cngpzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqchak/epoch_llms_that_generate_interactive_ui_instead/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oqchak/epoch_llms_that_generate_interactive_ui_instead/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-06T22:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqvxzw</id>
    <title>Learning resources &amp; advice</title>
    <updated>2025-11-07T14:31:53+00:00</updated>
    <author>
      <name>/u/SirEblingMis</name>
      <uri>https://old.reddit.com/user/SirEblingMis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;I'm looking for some learning resources. On which models to use, quantization, etc.&lt;br /&gt; I tinkered a bit with ollama and LM studios. I have absolutely no idea which model to start with. How much training etc does a new model need?&lt;/p&gt; &lt;p&gt;My hardware: 9950x3d, 32gb 6000c28 ram, rtx5080, good ssds.&lt;/p&gt; &lt;p&gt;I'm noticing I only get 18-25 tokens/sec on models like the Qwen3 30b. I'm looking for a model that matches that hardware to do work with me on math, statistics, modelling, and admin assistant stuff.&lt;br /&gt; Basically running it while I do work by hand, like an extra brain almost (even though I don't trust their results lol).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SirEblingMis"&gt; /u/SirEblingMis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqvxzw/learning_resources_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqvxzw/learning_resources_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oqvxzw/learning_resources_advice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-07T14:31:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqwmwh</id>
    <title>A 'cookie-cutter' FLOSS LLM model + UI setup guide for the average user at three different price point GPUs?</title>
    <updated>2025-11-07T15:00:03+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(For those that may know: many years ago, &lt;a href="/r/buildapc"&gt;/r/buildapc&lt;/a&gt; used to have a cookie-cutter build guide. I'm looking for something similar, except it's software only.)&lt;/p&gt; &lt;p&gt;There are so many LLMs and so many tools surrounding them that it's becoming harder to navigate through all the information.&lt;/p&gt; &lt;p&gt;I used to just simply use Ollama + Open WebUI, but seeing that Open WebUI switched to more protective license, I've been struggling to find which is the right UI.&lt;/p&gt; &lt;p&gt;Eventually, for my GPU, I think GPT OSS 20B is the right model, just unsure about which UI to use. I understand that there are other uses that are not text-only, like photo, code, video, audio generation, so cookie-cutter setups could be expanded that way.&lt;/p&gt; &lt;p&gt;So, is there such a guide?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqwmwh/a_cookiecutter_floss_llm_model_ui_setup_guide_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqwmwh/a_cookiecutter_floss_llm_model_ui_setup_guide_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oqwmwh/a_cookiecutter_floss_llm_model_ui_setup_guide_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-07T15:00:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1orcv34</id>
    <title>update v0.6.34 (latest) lost most of my Models for Ollama are gone!</title>
    <updated>2025-11-08T01:46:12+00:00</updated>
    <author>
      <name>/u/Mudcatt101</name>
      <uri>https://old.reddit.com/user/Mudcatt101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1orcv34/update_v0634_latest_lost_most_of_my_models_for/"&gt; &lt;img alt="update v0.6.34 (latest) lost most of my Models for Ollama are gone!" src="https://b.thumbs.redditmedia.com/V2Dssjc6YOJC7znTdotuSE3rJhQQdnlIl43Ku0-o9Mc.jpg" title="update v0.6.34 (latest) lost most of my Models for Ollama are gone!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mudcatt101"&gt; /u/Mudcatt101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenWebUI/comments/1orcutk/update_v0634_latest_lost_most_of_my_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1orcv34/update_v0634_latest_lost_most_of_my_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1orcv34/update_v0634_latest_lost_most_of_my_models_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-08T01:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1orql47</id>
    <title>AI agents just got scary good. Do we still need developers?</title>
    <updated>2025-11-08T14:20:48+00:00</updated>
    <author>
      <name>/u/eworker8888</name>
      <uri>https://old.reddit.com/user/eworker8888</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eworker8888"&gt; /u/eworker8888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/eworker_ca/comments/1orqj8z/ai_agents_just_got_scary_good_do_we_still_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1orql47/ai_agents_just_got_scary_good_do_we_still_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1orql47/ai_agents_just_got_scary_good_do_we_still_need/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-08T14:20:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqtytx</id>
    <title>Smallest model you know for less powerful computers?</title>
    <updated>2025-11-07T13:09:14+00:00</updated>
    <author>
      <name>/u/Weebolt</name>
      <uri>https://old.reddit.com/user/Weebolt</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weebolt"&gt; /u/Weebolt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqtytx/smallest_model_you_know_for_less_powerful/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqtytx/smallest_model_you_know_for_less_powerful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oqtytx/smallest_model_you_know_for_less_powerful/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-07T13:09:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1or24y4</id>
    <title>Want to Learn More About Agentic AI ‚Äì Looking to Contribute</title>
    <updated>2025-11-07T18:26:27+00:00</updated>
    <author>
      <name>/u/Superb_Practice_4544</name>
      <uri>https://old.reddit.com/user/Superb_Practice_4544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone ‚Äî I‚Äôve built a few agentic AI systems around SaaS automation and coding tools. I‚Äôm familiar with LangChain, LangGraph, RAG, tool calling, and MCP, but I want to learn more by contributing to real projects.&lt;/p&gt; &lt;p&gt;If you‚Äôre working on something in this space or know an open-source project looking for contributors, I‚Äôd love to help out and learn from it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Superb_Practice_4544"&gt; /u/Superb_Practice_4544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1or24y4/want_to_learn_more_about_agentic_ai_looking_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1or24y4/want_to_learn_more_about_agentic_ai_looking_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1or24y4/want_to_learn_more_about_agentic_ai_looking_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-07T18:26:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ori3mz</id>
    <title>Is there any draw backs to using an external dual GPU config with thunderbolt 5 with a laptop for AI?</title>
    <updated>2025-11-08T06:17:58+00:00</updated>
    <author>
      <name>/u/FX2021</name>
      <uri>https://old.reddit.com/user/FX2021</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FX2021"&gt; /u/FX2021 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ori3mz/is_there_any_draw_backs_to_using_an_external_dual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ori3mz/is_there_any_draw_backs_to_using_an_external_dual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ori3mz/is_there_any_draw_backs_to_using_an_external_dual/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-08T06:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1orglym</id>
    <title>Improving accuracy when extracting structured data from OCR text using Gemma 3</title>
    <updated>2025-11-08T04:54:54+00:00</updated>
    <author>
      <name>/u/Weekly_Signature_510</name>
      <uri>https://old.reddit.com/user/Weekly_Signature_510</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm working on a project where I extract text from U.S. driver‚Äôs license images using OCR. The OCR text itself contains all the necessary information (name, address, license number, etc.), and I also provide a version of the image with bounding boxes for context.&lt;/p&gt; &lt;p&gt;However, even though the OCR output has everything, my LLM (Gemma 3 12B running via Ollama) still misses or misclassifies some fields when structuring the data into JSON.&lt;/p&gt; &lt;p&gt;What can I do to improve extraction accuracy? Would better prompt design, fine-tuning, or additional preprocessing (like spatial grouping or text reformatting) make the biggest difference here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weekly_Signature_510"&gt; /u/Weekly_Signature_510 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1orglym/improving_accuracy_when_extracting_structured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1orglym/improving_accuracy_when_extracting_structured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1orglym/improving_accuracy_when_extracting_structured/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-08T04:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqxqvx</id>
    <title>POC: Model Context Protocol integration for native Ollama app</title>
    <updated>2025-11-07T15:41:54+00:00</updated>
    <author>
      <name>/u/Plenty_Seesaw8878</name>
      <uri>https://old.reddit.com/user/Plenty_Seesaw8878</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oqxqvx/poc_model_context_protocol_integration_for_native/"&gt; &lt;img alt="POC: Model Context Protocol integration for native Ollama app" src="https://preview.redd.it/5bp232couuzf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=d5274f89b68b48ec4343e680e68088f0703597fe" title="POC: Model Context Protocol integration for native Ollama app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;I built a small poc that lets the native ollama app connect to external tools and data sources through the Model Context Protocol.&lt;/p&gt; &lt;p&gt;Made it for personal use and wanted to check if the community would value this before I open a PR.&lt;/p&gt; &lt;p&gt;It‚Äôs based on Anthropic‚Äôs Go SDK and integrates into the app lifecycle.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plenty_Seesaw8878"&gt; /u/Plenty_Seesaw8878 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5bp232couuzf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqxqvx/poc_model_context_protocol_integration_for_native/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oqxqvx/poc_model_context_protocol_integration_for_native/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-07T15:41:54+00:00</published>
  </entry>
</feed>
