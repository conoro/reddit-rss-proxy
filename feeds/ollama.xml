<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-06T10:41:51+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mimw05</id>
    <title>gpt-oss:20b crashes: CUDA illegal memory access on Ollama 0.11.0</title>
    <updated>2025-08-05T22:05:10+00:00</updated>
    <author>
      <name>/u/Diegam</name>
      <uri>https://old.reddit.com/user/Diegam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;br /&gt; I‚Äôm trying to run the new &lt;code&gt;gpt-oss:20b&lt;/code&gt; model on Ollama (v0.11.0), but it crashes immediately with a CUDA illegal memory access error.&lt;/p&gt; &lt;p&gt;&lt;code&gt; ollama run gpt-oss:20b \&amp;gt;&amp;gt;&amp;gt; Hi Error: model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Here‚Äôs the relevant part of the log:&lt;br /&gt; CUDA error: an illegal memory access was encountered&lt;br /&gt; current device: 0, in function ggml_cuda_mul_mat_id at &lt;a href="http://ggml-cuda.cu:2052"&gt;ggml-cuda.cu:2052&lt;/a&gt;&lt;br /&gt; SIGSEGV: segmentation violation&lt;/p&gt; &lt;p&gt;&lt;code&gt; CUDA error: an illegal memory access was encountered current device: 0, in function ggml\_cuda\_mul\_mat\_id at //ml/backend/ggml/ggml/src/ggml-cuda/ggml-cuda.cu:2052 cudaMemcpyAsync(ids\_host.data(), ids-&amp;gt;data, ggml\_nbytes(ids), cudaMemcpyDeviceToHost, stream) //ml/backend/ggml/ggml/src/ggml-cuda/ggml-cuda.cu:77: CUDA error SIGSEGV: segmentation violation PC=0x76964e1ebea7 m=11 sigcode=1 addr=0x204803f90 signal arrived during cgo execution goroutine 7 gp=0xc000582e00 m=11 mp=0xc000580808 \[syscall\]: runtime.cgocall(0x593306cf7180, 0xc001ad7a58) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;My setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;NVIDIA GPU with 24 GB VRAM (rtx3090)&lt;/li&gt; &lt;li&gt;Driver: 560.28.03&lt;/li&gt; &lt;li&gt;CUDA: 12.6&lt;/li&gt; &lt;li&gt;OS: [Ubuntu Server 24.04]&lt;/li&gt; &lt;li&gt;Ollama version: 0.11.0&lt;/li&gt; &lt;li&gt;Other models like &lt;code&gt;llama3&lt;/code&gt;.1, qwen3:* work perfectly&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diegam"&gt; /u/Diegam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mimw05/gptoss20b_crashes_cuda_illegal_memory_access_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mimw05/gptoss20b_crashes_cuda_illegal_memory_access_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mimw05/gptoss20b_crashes_cuda_illegal_memory_access_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T22:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi63i6</id>
    <title>Any plans to support image generation models in ollama?</title>
    <updated>2025-08-05T11:01:45+00:00</updated>
    <author>
      <name>/u/sh_tomer</name>
      <uri>https://old.reddit.com/user/sh_tomer</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sh_tomer"&gt; /u/sh_tomer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi63i6/any_plans_to_support_image_generation_models_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi63i6/any_plans_to_support_image_generation_models_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mi63i6/any_plans_to_support_image_generation_models_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T11:01:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mioydl</id>
    <title>How to set default model with Ollama app</title>
    <updated>2025-08-05T23:31:17+00:00</updated>
    <author>
      <name>/u/Able_Solution307</name>
      <uri>https://old.reddit.com/user/Able_Solution307</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Whenever I start the new ollama app it for some reason automatically sets the model to gpt-oss, is it possible to set it to a model of my choice on restart?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Able_Solution307"&gt; /u/Able_Solution307 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mioydl/how_to_set_default_model_with_ollama_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mioydl/how_to_set_default_model_with_ollama_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mioydl/how_to_set_default_model_with_ollama_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T23:31:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi7f4l</id>
    <title>llama3.2-vision prompt for OCR</title>
    <updated>2025-08-05T12:09:24+00:00</updated>
    <author>
      <name>/u/vir_db</name>
      <uri>https://old.reddit.com/user/vir_db</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to get llama3.2-vision act like an OCR system, in order to transcribe the text inside an image.&lt;/p&gt; &lt;p&gt;The source image is like the page of a book, or a image-only PDF. The text is not handwritten, however I cannot find a working combination of system/user prompt that just report the full text in the image, without adding notes or information about what the image look like. Sometimes the model return the text, but with notes and explanation, sometimes the model return (with the same prompt, often) a lot of strange nonsense character sequences. I tried both simple prompts like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Extract all text from the image and return it as markdown.\n Do not describe the image or add extra text.\n Only return the text found in the image. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and more complex ones like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;You are a text extraction expert. Your task is to analyze the provided image and extract all visible text with maximum accuracy. Organize the extracted text into a structured Markdown format. Follow these rules:\n\n 1. Headers: If a section of the text appears larger, bold, or like a heading, format it as a Markdown header (#, ##, or ###).\n 2. Lists: Format bullets or numbered items using Markdown syntax.\n 3. Tables: Use Markdown table format.\n 4. Paragraphs: Keep normal text blocks as paragraphs.\n 5. Emphasis: Use _italics_ and **bold** where needed.\n 6. Links: Format links like [text](url).\n Ensure the extracted text mirrors the document\‚Äôs structure and formatting.\n Provide only the transcription without any additional comments.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But none of them is working as expected. Somebody have ideas? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vir_db"&gt; /u/vir_db &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi7f4l/llama32vision_prompt_for_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi7f4l/llama32vision_prompt_for_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mi7f4l/llama32vision_prompt_for_ocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T12:09:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1minhkr</id>
    <title>Hardware upgrade advice for local Ollama + Frigate GenAI + HA Voice (Budget: $1000‚Äì$1200)</title>
    <updated>2025-08-05T22:29:50+00:00</updated>
    <author>
      <name>/u/ResponsibleSyrup6556</name>
      <uri>https://old.reddit.com/user/ResponsibleSyrup6556</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm hoping to get some advice from folks who are running Ollama and similar local AI setups. I‚Äôm trying to figure out what my next hardware move should be. My budget is around $1000 to $1200.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unraid as the host OS&lt;/li&gt; &lt;li&gt;Running Ollama, Frigate, and Kokoro (TTS) in Docker&lt;/li&gt; &lt;li&gt;Home Assistant OS in a VM&lt;/li&gt; &lt;li&gt;Hardware: &lt;ul&gt; &lt;li&gt;ASUS PRIME B450M-A II&lt;/li&gt; &lt;li&gt;Ryzen 5 4500 (6-core)&lt;/li&gt; &lt;li&gt;64GB DDR4 3600MHz (2x32GB)&lt;/li&gt; &lt;li&gt;Old 4GB NVIDIA GPU (will get the model later)&lt;/li&gt; &lt;li&gt;500W PSU (also old)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything runs surprisingly well using cloud services for the AI stuff (Frigate GenAI and HA Voice Assistant). I also have a &lt;strong&gt;Coral TPU&lt;/strong&gt;, which is handling Frigate‚Äôs object detection just fine, so I‚Äôm not relying on CPU or GPU for that part.&lt;/p&gt; &lt;p&gt;What I want to do now is &lt;strong&gt;run everything locally&lt;/strong&gt; ‚Äî LLMs for Frigate summaries and Home Assistant Voice responses, and eventually try out some image models like LLaVA or Qwen-image. Right now I‚Äôm stuck using smaller 3B models, and I want to go beyond that without relying on the cloud.&lt;/p&gt; &lt;p&gt;That said, I‚Äôm still pretty new to all this AI/LLM stuff. I don‚Äôt see myself training models or messing with massive 70B setups. I just want something solid that gives me room to grow and experiment. I like tinkering and learning ‚Äî I‚Äôm not a pro, but I‚Äôm having fun.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here are the options I‚Äôm considering:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;GPU upgrade in my current system&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Probably need to upgrade the PSU too (850W or so)&lt;/li&gt; &lt;li&gt;Looking at something like an &lt;strong&gt;RTX 5060 Ti 16GB&lt;/strong&gt; or a used &lt;strong&gt;RTX 3090&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Might be the cheapest and simplest path&lt;/li&gt; &lt;li&gt;But: is it worth it, or am I throwing a Porsche engine in a Pinto?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Used gaming PC on Facebook Marketplace&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;i9-12900K&lt;/li&gt; &lt;li&gt;ASUS ROG Z690-E&lt;/li&gt; &lt;li&gt;RTX 3090 (24GB)&lt;/li&gt; &lt;li&gt;32GB DDR5&lt;/li&gt; &lt;li&gt;Dual NVMe Gen4 SSDs&lt;/li&gt; &lt;li&gt;850W Gold PSU&lt;/li&gt; &lt;li&gt;Corsair liquid cooler&lt;/li&gt; &lt;li&gt;RGB fans (some are busted, but whatever)&lt;/li&gt; &lt;li&gt;Asking $1300 but open to offers ‚Äî might be able to grab it for $1000‚Äì$1200 cash&lt;/li&gt; &lt;li&gt;Would definitely want to test it before buying&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mac Mini M4 (24‚Äì32GB)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;$999 base model&lt;/li&gt; &lt;li&gt;Tempting since we use Apple gear in the house&lt;/li&gt; &lt;li&gt;But the lack of upgradability is a concern. Once you hit a ceiling, that‚Äôs it ‚Äî unless you want to build a cluster.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jetson Orin NX from Seeed Studio&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;reComputer J4012 w/ Jetson Orin NX 16GB&lt;/li&gt; &lt;li&gt;Around $950&lt;/li&gt; &lt;li&gt;Seems built for AI, but maybe more focused on edge CV stuff?&lt;/li&gt; &lt;li&gt;Not sure it‚Äôs great for LLMs, especially beyond 3B models.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ryzen AI NUC (HX 370)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;GMKtec EVO-X1 with 64GB RAM and 1TB SSD is ~$1029&lt;/li&gt; &lt;li&gt;Nice compact form factor&lt;/li&gt; &lt;li&gt;But I haven‚Äôt seen many people running real-world LLM workloads on these yet. Is the NPU actually useful today?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;So here‚Äôs what I‚Äôm trying to figure out:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is it worth throwing a nice GPU in my current system and rolling with it?&lt;/li&gt; &lt;li&gt;Or is it smarter to grab that used 3090 rig and start fresh?&lt;/li&gt; &lt;li&gt;Are the edge devices like the Jetson or AI NUCs actually viable for LLMs?&lt;/li&gt; &lt;li&gt;Anyone running a Mac Mini M4 for local LLMs ‚Äî how far can it go?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any advice would be appreciated, especially from folks doing similar things with Frigate, HA Voice, or Ollama. I'm not trying to max out some giant model ‚Äî just want to stop relying on the cloud and have enough horsepower to explore and grow.&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleSyrup6556"&gt; /u/ResponsibleSyrup6556 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1minhkr/hardware_upgrade_advice_for_local_ollama_frigate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1minhkr/hardware_upgrade_advice_for_local_ollama_frigate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1minhkr/hardware_upgrade_advice_for_local_ollama_frigate/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T22:29:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifzoc</id>
    <title>gpt-oss OpenAI‚Äôs open-weight models</title>
    <updated>2025-08-05T17:45:55+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ollama.com/library/gpt-oss"&gt;https://ollama.com/library/gpt-oss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;OpenAI‚Äôs open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases. &lt;/p&gt; &lt;p&gt;Ollama partners with OpenAI to bring its latest state-of-the-art open weight models to Ollama. The two models, 20B and 120B, bring a whole new local chat experience, and are designed for powerful reasoning, agentic tasks, and versatile developer use cases.&lt;/p&gt; &lt;p&gt;ollama run gpt-oss:20b ollama run gpt-oss:120b&lt;/p&gt; &lt;p&gt;Edit: v0.11.0 required &lt;a href="https://github.com/ollama/ollama/releases/tag/v0.11.0"&gt;https://github.com/ollama/ollama/releases/tag/v0.11.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit2: v0.11.2 fix crash &lt;a href="https://github.com/ollama/ollama/releases/tag/v0.11.2"&gt;https://github.com/ollama/ollama/releases/tag/v0.11.2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifzoc/gptoss_openais_openweight_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifzoc/gptoss_openais_openweight_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mifzoc/gptoss_openais_openweight_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:45:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mil1lu</id>
    <title>GPT-OSS no response</title>
    <updated>2025-08-05T20:52:47+00:00</updated>
    <author>
      <name>/u/No_Thing8294</name>
      <uri>https://old.reddit.com/user/No_Thing8294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded the new GPT-OSS 12B in Ollama. But I do not get an answer in the chat. Using it via OpenWebUI with my Ollama results in some strange error (some function is not declared ü§∑‚Äç‚ôÇÔ∏è)&lt;/p&gt; &lt;p&gt;Someone running into similar issues?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Thing8294"&gt; /u/No_Thing8294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mil1lu/gptoss_no_response/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mil1lu/gptoss_no_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mil1lu/gptoss_no_response/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T20:52:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1miu13o</id>
    <title>gpt-oss:20b with(out) kv cache / flash attention</title>
    <updated>2025-08-06T03:27:36+00:00</updated>
    <author>
      <name>/u/digitalextremist</name>
      <uri>https://old.reddit.com/user/digitalextremist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seeing &lt;code&gt;100% CPU&lt;/code&gt; and &lt;code&gt;0% GPU&lt;/code&gt; for &lt;code&gt;gpt-oss:20b&lt;/code&gt; with &lt;code&gt;kv cache&lt;/code&gt; / &lt;code&gt;flash attention&lt;/code&gt; enabled.&lt;/p&gt; &lt;p&gt;Noticed that all of &lt;code&gt;0.11.2&lt;/code&gt; was this patch:&lt;/p&gt; &lt;p&gt;&lt;code&gt; if f.KV().Architecture() == &amp;quot;gptoss&amp;quot; { return false } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Does anyone know if this is a hard-stop where the model does not support this, or if &lt;code&gt;kv cache&lt;/code&gt; support is coming in?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/digitalextremist"&gt; /u/digitalextremist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miu13o/gptoss20b_without_kv_cache_flash_attention/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miu13o/gptoss20b_without_kv_cache_flash_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miu13o/gptoss20b_without_kv_cache_flash_attention/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T03:27:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1miubt1</id>
    <title>RTX 5090 @ 400W | Ubuntu 24.04 | AI Inference</title>
    <updated>2025-08-06T03:43:01+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1miubt1/rtx_5090_400w_ubuntu_2404_ai_inference/"&gt; &lt;img alt="RTX 5090 @ 400W | Ubuntu 24.04 | AI Inference" src="https://preview.redd.it/8zc2bg1qlbhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=528781ea06eeac17310659187bafd9cfc1513f99" title="RTX 5090 @ 400W | Ubuntu 24.04 | AI Inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8zc2bg1qlbhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miubt1/rtx_5090_400w_ubuntu_2404_ai_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miubt1/rtx_5090_400w_ubuntu_2404_ai_inference/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T03:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mivjov</id>
    <title>At this point, should I buy RTX 5060ti or 5070ti ( 16GB ) for local models ?</title>
    <updated>2025-08-06T04:48:39+00:00</updated>
    <author>
      <name>/u/Current-Stop7806</name>
      <uri>https://old.reddit.com/user/Current-Stop7806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mivjov/at_this_point_should_i_buy_rtx_5060ti_or_5070ti/"&gt; &lt;img alt="At this point, should I buy RTX 5060ti or 5070ti ( 16GB ) for local models ?" src="https://preview.redd.it/ejedb1ggwbhf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eddb421d3c8d16a3b21d94885998653b5681bf21" title="At this point, should I buy RTX 5060ti or 5070ti ( 16GB ) for local models ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Current-Stop7806"&gt; /u/Current-Stop7806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ejedb1ggwbhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mivjov/at_this_point_should_i_buy_rtx_5060ti_or_5070ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mivjov/at_this_point_should_i_buy_rtx_5060ti_or_5070ti/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T04:48:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1miwqzm</id>
    <title>new GUI TPS information</title>
    <updated>2025-08-06T05:58:15+00:00</updated>
    <author>
      <name>/u/sunole123</name>
      <uri>https://old.reddit.com/user/sunole123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;when working with Ollama CLI, i always use the --verbose option, cause I like to know the TPS on the different GPUs i work with, but i can't find this option with the new GUI, is there a way to enable it or to find it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunole123"&gt; /u/sunole123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miwqzm/new_gui_tps_information/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miwqzm/new_gui_tps_information/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miwqzm/new_gui_tps_information/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T05:58:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1miy7hv</id>
    <title>Generate alt attributes for website images based on the ollama model</title>
    <updated>2025-08-06T07:28:49+00:00</updated>
    <author>
      <name>/u/adairz</name>
      <uri>https://old.reddit.com/user/adairz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1miy7hv/generate_alt_attributes_for_website_images_based/"&gt; &lt;img alt="Generate alt attributes for website images based on the ollama model" src="https://b.thumbs.redditmedia.com/0q-30tziLfK9oCryfaDm6pfKhekcIRD9z4_2j3laARY.jpg" title="Generate alt attributes for website images based on the ollama model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Equipment:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A Mac computer&lt;/p&gt; &lt;p&gt;An Intel&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.dfrobot.com/product-2748.html"&gt;A lattepanda sigma &lt;/a&gt;(LattePanda Sigma 32GB RAM, 500GB)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/57mbgh4xochf1.png?width=1281&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1957758ed011250dd777634723ca441eb976e3c"&gt;A Mac computer„ÄÅ Intel„ÄÅLattePanda Sigma &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Final plan:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Vision model first implements graphic text + then uses the Thinking model to implement text text&lt;/p&gt; &lt;p&gt;The Vision models used the visual models mini cpm-v and qwen2.5vl on Ollama respectively&lt;/p&gt; &lt;p&gt;The same prompt word, mini cpm-v, does not follow the length and return content very well. Qwen2.5vl has better performance&lt;/p&gt; &lt;p&gt;The Thinking model uses qwen3&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xo1cmtdonchf1.png?width=870&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4f6bb355aa31267f51364586f91d565e80492a4c"&gt;Generate alt attributes for website images based on the ollama model list&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Vision model lacks reasoning ability.&lt;/p&gt; &lt;p&gt;The first time, finish the picture and text,&lt;/p&gt; &lt;p&gt;Then, combining the title and keywords, use Thinking to achieve the ultimate goal.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3ljbzhispchf1.png?width=2670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0893bc953b8dd26cfde8b4ad331e6650a2fb3701"&gt;image alt MySQL data&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adairz"&gt; /u/adairz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miy7hv/generate_alt_attributes_for_website_images_based/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miy7hv/generate_alt_attributes_for_website_images_based/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miy7hv/generate_alt_attributes_for_website_images_based/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T07:28:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mim247</id>
    <title>Running OpenAI‚Äôs new GPT‚ÄëOSS‚Äë20B locally with Ollama</title>
    <updated>2025-08-05T21:32:06+00:00</updated>
    <author>
      <name>/u/Sumanth_077</name>
      <uri>https://old.reddit.com/user/Sumanth_077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI released two new open‚Äësource models, GPT‚ÄëOSS‚Äë120B and GPT‚ÄëOSS‚Äë20B, focused on reasoning tasks.&lt;/p&gt; &lt;p&gt;We put together a short walkthrough showing how to:&lt;/p&gt; &lt;p&gt;‚Ä¢ Pull and run GPT‚ÄëOSS‚Äë20B locally using Ollama&lt;br /&gt; ‚Ä¢ Expose it through a standard OpenAI‚Äëcompatible API with Local Runners&lt;/p&gt; &lt;p&gt;This makes it easier to experiment with the model locally while still accessing it programmatically via an API.&lt;/p&gt; &lt;p&gt;Watch the walkthrough &lt;a href="https://youtu.be/TfS2p8LZYBE"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sumanth_077"&gt; /u/Sumanth_077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mim247/running_openais_new_gptoss20b_locally_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mim247/running_openais_new_gptoss20b_locally_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mim247/running_openais_new_gptoss20b_locally_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T21:32:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1misgmo</id>
    <title>Need help choosing models, new ollama user</title>
    <updated>2025-08-06T02:11:16+00:00</updated>
    <author>
      <name>/u/Squanchy2112</name>
      <uri>https://old.reddit.com/user/Squanchy2112</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got ollama, and openwebui setup and I have a couple models so far. I have found some deficiences so far with some models and am curious what options I should be pursuing. I have a Tesla P40 24gb VRAM, 100gb RAM, 1tb of storage to play with so far I have tried qwen3-coder30b, mixtral8x7b,codellama:13b,llama2L:70b, llama2 had such low token rate it was not usable. codellama was an idiot in general convo, mixtral seemed to reason well but its info is limited to 2021, and qwen has been alright with general convo but has been making some noted mistakes. I am spoiled by ChatGPT, Grok, and Claude so I am aware my hardware cant rival those but my main use is general questions a la Google Assistant but I do a lot in batch and powershell as well as home assistant and find these llms really handy for that. Also file management like moving around date in csv files. Thanks for any input I am loving getting into this so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Squanchy2112"&gt; /u/Squanchy2112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1misgmo/need_help_choosing_models_new_ollama_user/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1misgmo/need_help_choosing_models_new_ollama_user/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1misgmo/need_help_choosing_models_new_ollama_user/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T02:11:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi9zex</id>
    <title>Ollama's new app makes using local AI LLMs on your Windows 11 PC a breeze ‚Äî no more need to chat in the terminal</title>
    <updated>2025-08-05T14:00:04+00:00</updated>
    <author>
      <name>/u/rkhunter_</name>
      <uri>https://old.reddit.com/user/rkhunter_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mi9zex/ollamas_new_app_makes_using_local_ai_llms_on_your/"&gt; &lt;img alt="Ollama's new app makes using local AI LLMs on your Windows 11 PC a breeze ‚Äî no more need to chat in the terminal" src="https://external-preview.redd.it/BhemsEH58Hdd5wlj8RIXGA-DVEqtIWcM1c0-0glb5O8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2963f63bbb5ff18828ac8e0ab68dbd3e71338ac3" title="Ollama's new app makes using local AI LLMs on your Windows 11 PC a breeze ‚Äî no more need to chat in the terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rkhunter_"&gt; /u/rkhunter_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.windowscentral.com/artificial-intelligence/ollamas-new-app-makes-using-local-ai-llms-on-your-windows-11-pc-a-breeze-no-more-need-to-chat-in-the-terminal"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi9zex/ollamas_new_app_makes_using_local_ai_llms_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mi9zex/ollamas_new_app_makes_using_local_ai_llms_on_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T14:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj0tc1</id>
    <title>Everyone is building AI web apps. I want to build a feature-rich, open-source Desktop AI Assistant. Who's in for a rapid MVP sprint?</title>
    <updated>2025-08-06T10:17:35+00:00</updated>
    <author>
      <name>/u/Defiant-Ad7199</name>
      <uri>https://old.reddit.com/user/Defiant-Ad7199</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been watching the AI space explode, and it's incredible. Tools like Cursor, Vercel's v0, and others are changing web development. But I've noticed a trend: almost everything is a web app, tethered to the cloud.&lt;/p&gt; &lt;p&gt;I believe there's a huge opportunity for a powerful, private, and extensible &lt;strong&gt;native desktop AI application&lt;/strong&gt;. I'm a huge fan of what projects like &lt;strong&gt;LM Studio&lt;/strong&gt; and &lt;strong&gt;Ollama&lt;/strong&gt; have started, but I think we can build on their foundation to create something even more integrated into our daily workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Vision: A &amp;quot;Supercharged&amp;quot; Desktop AI Hub&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm not talking about another simple chat wrapper. I'm envisioning a central AI tool on your desktop that can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Have a Powerful Plugin Architecture:&lt;/strong&gt; Imagine plugins for file system indexing (ask questions about your local code base), calendar integration, web Browse agents, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Be Truly Multi-Modal:&lt;/strong&gt; Seamlessly switch between text, image generation, code interpretation, and voice, using the best local models for each task.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Feature Advanced Agentic Workflows:&lt;/strong&gt; Go beyond simple Q&amp;amp;A. Give it a high-level task like &amp;quot;Research the current state of quantum computing, summarize the top 5 recent papers, and save the summary as a markdown file on my desktop,&amp;quot; and have it execute the steps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Offer Simple Model Management:&lt;/strong&gt; An intuitive UI to download, manage, and switch between various GGUF/HF models without ever touching the command line.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Plan &amp;amp; The Urgency: A 1-Week MVP Sprint&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I know this is an ambitious project. That's why I want to kick it off with a rapid development sprint, essentially a week-long hackathon, starting &lt;strong&gt;right now&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Here's the catalyst: I have a bunch of credits for an AI coding assistant (Kilo Code) that are expiring on &lt;strong&gt;August 14th&lt;/strong&gt;. I want to use this deadline as fuel to assemble a small team and build a solid Minimum Viable Product (MVP) by then. The goal isn't a polished, final product, but a functional proof-of-concept that demonstrates the core vision.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who I'm Looking For:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm looking for 1-2 passionate people to join me in this sprint. If you have experience or interest in any of the following, I'd love to hear from you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Desktop App Development:&lt;/strong&gt; (e.g., Tauri, Electron, Qt)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend/Core Logic:&lt;/strong&gt; (e.g., Rust, Python)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI/ML Integration:&lt;/strong&gt; Experience with running local LLMs, transformers, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;UI/UX Design:&lt;/strong&gt; To make it look and feel intuitive from day one.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Defiant-Ad7199"&gt; /u/Defiant-Ad7199 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mj0tc1/everyone_is_building_ai_web_apps_i_want_to_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mj0tc1/everyone_is_building_ai_web_apps_i_want_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mj0tc1/everyone_is_building_ai_web_apps_i_want_to_build/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T10:17:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mixfm9</id>
    <title>Ollama - gpt-oss:20b - AMD Radeon 7600XT</title>
    <updated>2025-08-06T06:39:43+00:00</updated>
    <author>
      <name>/u/MrOxxi</name>
      <uri>https://old.reddit.com/user/MrOxxi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, &lt;/p&gt; &lt;p&gt;I tried running the new gpt-oss:20b with Ollama and for some reason it chooses CPU over my GPU, just wondered if this was down to the model chosen, I can run other models like codestral, codellama, qwen3 and they all use my GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrOxxi"&gt; /u/MrOxxi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mixfm9/ollama_gptoss20b_amd_radeon_7600xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mixfm9/ollama_gptoss20b_amd_radeon_7600xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mixfm9/ollama_gptoss20b_amd_radeon_7600xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T06:39:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mihumw</id>
    <title>gpt-oss-20b WAY too slow on M1 MacBook Pro (2020)</title>
    <updated>2025-08-05T18:53:10+00:00</updated>
    <author>
      <name>/u/rafa3790543246789</name>
      <uri>https://old.reddit.com/user/rafa3790543246789</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just saw the new open-weight models that OpenAI released, and I wanted to try them on my M1 MacBook Pro (from 2020, 16GB). OpenAI said the gpt-oss-20b model can run on most desktops and laptops, but I'm having trouble running it on my Mac.&lt;/p&gt; &lt;p&gt;When I try to run gpt-oss-20b (after closing every app, making room for the 13GB model), it just takes ages to generate single tokens. It's definitely not usable and cannot run on my Mac.&lt;/p&gt; &lt;p&gt;Curious to know if anyone had similar experiences.&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rafa3790543246789"&gt; /u/rafa3790543246789 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mihumw/gptoss20b_way_too_slow_on_m1_macbook_pro_2020/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mihumw/gptoss20b_way_too_slow_on_m1_macbook_pro_2020/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mihumw/gptoss20b_way_too_slow_on_m1_macbook_pro_2020/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T18:53:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi6rqk</id>
    <title>Built a lightweight picker that finds the right Ollama model for your hardware (surprisingly useful!)</title>
    <updated>2025-08-05T11:37:10+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mi6rqk/built_a_lightweight_picker_that_finds_the_right/"&gt; &lt;img alt="Built a lightweight picker that finds the right Ollama model for your hardware (surprisingly useful!)" src="https://external-preview.redd.it/Ymhhd3RucXBzNmhmMUg6vjulbZIeLFSczOGwuN3tGRr8QNKtfAF_eyKX2Orf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfef104774b195d4b72123ead92016e96e4c61d5" title="Built a lightweight picker that finds the right Ollama model for your hardware (surprisingly useful!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/st0gjoqps6hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi6rqk/built_a_lightweight_picker_that_finds_the_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mi6rqk/built_a_lightweight_picker_that_finds_the_right/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T11:37:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mig4bu</id>
    <title>OpenAI Open Source Models Released!</title>
    <updated>2025-08-05T17:50:35+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI has unleashed two new open‚Äëweight models:&lt;br /&gt; - &lt;strong&gt;GPT‚ÄëOSS‚Äë120b (120B parameters)&lt;/strong&gt;&lt;br /&gt; - &lt;strong&gt;GPT‚ÄëOSS‚Äë20b (20B parameters)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;It's their first to be actually downloadable and customizable models since GPT‚Äë2 in 2019. It has a &lt;strong&gt;GPL‚Äëfriendly license&lt;/strong&gt; (Apache 2.0), allows free modification and commercial use. They're also Chain‚Äëof‚Äëthought enabled, supports code generation, browsing, and agent use via OpenAI API&lt;/p&gt; &lt;p&gt;&lt;a href="https://openai.com/open-models/"&gt;https://openai.com/open-models/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mig4bu/openai_open_source_models_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mig4bu/openai_open_source_models_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mig4bu/openai_open_source_models_released/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1miyj5e</id>
    <title>Ollama uses internet by default?</title>
    <updated>2025-08-06T07:50:22+00:00</updated>
    <author>
      <name>/u/icecoffee888</name>
      <uri>https://old.reddit.com/user/icecoffee888</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so after using LM Studio for a while I finally decided to try Ollama now that I'm doing more local LLM coding (for privacy reasons), I was shocked when I saw docs URLs in ollama's thinking log.&lt;/p&gt; &lt;p&gt;Then I figured I have to go to settings an turn airplane mode on to be offline, shouldn't that be the default? is this new? what else is ollama sending to the internet, I have deleted it now, just pretty shocked. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icecoffee888"&gt; /u/icecoffee888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miyj5e/ollama_uses_internet_by_default/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miyj5e/ollama_uses_internet_by_default/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miyj5e/ollama_uses_internet_by_default/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T07:50:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1miis70</id>
    <title>Why does web search require an ollama account? That's pretty lame</title>
    <updated>2025-08-05T19:27:47+00:00</updated>
    <author>
      <name>/u/Anxious-Bottle7468</name>
      <uri>https://old.reddit.com/user/Anxious-Bottle7468</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1miis70/why_does_web_search_require_an_ollama_account/"&gt; &lt;img alt="Why does web search require an ollama account? That's pretty lame" src="https://preview.redd.it/2x8cvu7i59hf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=514e04fbc3a672eba9c3d94cf3db31d0455e246d" title="Why does web search require an ollama account? That's pretty lame" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxious-Bottle7468"&gt; /u/Anxious-Bottle7468 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2x8cvu7i59hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miis70/why_does_web_search_require_an_ollama_account/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miis70/why_does_web_search_require_an_ollama_account/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mijg4m</id>
    <title>Ollama removed the link to GitHub</title>
    <updated>2025-08-05T19:52:42+00:00</updated>
    <author>
      <name>/u/waescher</name>
      <uri>https://old.reddit.com/user/waescher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mijg4m/ollama_removed_the_link_to_github/"&gt; &lt;img alt="Ollama removed the link to GitHub" src="https://preview.redd.it/bk6utn9v99hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cac4875cf871bdc5c59cca05a4063a0f9a11ae0b" title="Ollama removed the link to GitHub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama added a link to their paid cloud &amp;quot;Turbo&amp;quot; subscription and removed the link to their GitHub repository. I don't like where this is going ...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waescher"&gt; /u/waescher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bk6utn9v99hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mijg4m/ollama_removed_the_link_to_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mijg4m/ollama_removed_the_link_to_github/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:52:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mig251</id>
    <title>gpt-oss now available on Ollama</title>
    <updated>2025-08-05T17:48:20+00:00</updated>
    <author>
      <name>/u/john_rage</name>
      <uri>https://old.reddit.com/user/john_rage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mig251/gptoss_now_available_on_ollama/"&gt; &lt;img alt="gpt-oss now available on Ollama" src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="gpt-oss now available on Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI has published their opensource gpt model on Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/john_rage"&gt; /u/john_rage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/gpt-oss"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mig251/gptoss_now_available_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mig251/gptoss_now_available_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:48:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mij9gu</id>
    <title>Open AI GPT-OSS:20b is bullshit</title>
    <updated>2025-08-05T19:45:48+00:00</updated>
    <author>
      <name>/u/Embarrassed-Way-1350</name>
      <uri>https://old.reddit.com/user/Embarrassed-Way-1350</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have just tried GPT-OSS:20b on my machine. This is the stupidest COT MOE model I have ever interacted with. Open AI chose to shit on the open-source community by releasing this abomination of a model.&lt;/p&gt; &lt;p&gt;Cannot perform basic arithmetic reasoning tasks, Thinks too much, and thinking traits remind me of deepseek-distill:70b, Would have been a great model 3 generations ago. As of today there are a ton of better models out there GLM is a far better alternative. Do not even try this model, Pure shit spray dried into fine powder.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Way-1350"&gt; /u/Embarrassed-Way-1350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:45:48+00:00</published>
  </entry>
</feed>
