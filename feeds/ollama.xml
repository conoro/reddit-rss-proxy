<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-09T15:36:44+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1na3cy2</id>
    <title>MCP with Computer Use</title>
    <updated>2025-09-06T16:06:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1na3cy2/mcp_with_computer_use/"&gt; &lt;img alt="MCP with Computer Use" src="https://external-preview.redd.it/eTJsbWFwZXJpa25mMRWfbJ8igMn1wejE5S1jscBm4hQQlwhyyetxPIo-3e1f.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=827589a323b1b6502b4bf3948d377b57f190604b" title="MCP with Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MCP Server with Computer Use Agent runs through Claude Desktop, Cursor, and other MCP clients.&lt;/p&gt; &lt;p&gt;An example use case lets try using Claude as a tutor to learn how to use Tableau.&lt;/p&gt; &lt;p&gt;The MCP Server implementation exposes CUA's full functionality through standardized tool calls. It supports single-task commands and multi-task sequences, giving Claude Desktop direct access to all of Cua's computer control capabilities.&lt;/p&gt; &lt;p&gt;This is the first MCP-compatible computer control solution that works directly with Claude Desktop's and Cursor's built-in MCP implementation. Simple configuration in your claude_desktop_config.json or cursor_config.json connects Claude or Cursor directly to your desktop environment.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.gg/4fuebBsAUj"&gt;https://discord.gg/4fuebBsAUj&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4z4enlqriknf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na3cy2/mcp_with_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na3cy2/mcp_with_computer_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T16:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nave28</id>
    <title>Can't get models to create "real person" like text.</title>
    <updated>2025-09-07T14:59:00+00:00</updated>
    <author>
      <name>/u/Nearby_You_313</name>
      <uri>https://old.reddit.com/user/Nearby_You_313</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to make a prompt that generates &amp;quot;real person&amp;quot; style text. I tried my own prompts, a megaprompt generator telling it to generate different styles, voices, etc. I've tried a variety of models. It all sounds like a bad author trying to write their first novel.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I mean:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Real person: &amp;quot;My manager called me into his office the moment I got to work&amp;quot;&lt;/p&gt; &lt;p&gt;What they all do: &amp;quot;I walked into that office not knowing what awaited me. It was a regular Monday, but as I settled at my desk, I couldn't help but feel something was off. The manager, who seemed overly cheerful, called me into his office.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nearby_You_313"&gt; /u/Nearby_You_313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nave28/cant_get_models_to_create_real_person_like_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nave28/cant_get_models_to_create_real_person_like_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nave28/cant_get_models_to_create_real_person_like_text/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T14:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1narbrw</id>
    <title>Hotel Rooms Booking Bot</title>
    <updated>2025-09-07T11:59:23+00:00</updated>
    <author>
      <name>/u/anuraginsg</name>
      <uri>https://old.reddit.com/user/anuraginsg</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anuraginsg"&gt; /u/anuraginsg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wyo3sfa06jnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1narbrw/hotel_rooms_booking_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1narbrw/hotel_rooms_booking_bot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T11:59:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb0o2u</id>
    <title>Ollama Crashing on RTX 3080 Ti with CUDA Error After Update (Models Freeze PC)</title>
    <updated>2025-09-07T18:22:44+00:00</updated>
    <author>
      <name>/u/Mundane_Education654</name>
      <uri>https://old.reddit.com/user/Mundane_Education654</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I need some help.&lt;/p&gt; &lt;p&gt;I‚Äôm running Ollama on my PC (Windows, 16GB RAM, RTX 3080 Ti). The issue is that whenever I run a model like gemma:4b, it loads fine, but as soon as I send a query and the model starts responding, my system goes black screen and freezes until I force restart.&lt;/p&gt; &lt;p&gt;Here‚Äôs what I tried: - I updated Ollama to the latest version ‚Üí crash started. - I even tried rolling back Ollama to older versions, but the crash still happens. - Updated NVIDIA drivers ‚Üí no change (crash still there). - I also reset my whole PC, installed fresh Ollama and drivers ‚Üí still the same crash. - Tried both Gemma and LLaMA models, both crash in the same way. - Error message I sometimes see: ‚ÄúError: an error was encountered while running the model: CUDA error‚Äù.&lt;/p&gt; &lt;p&gt;Important points: - Previously even bigger models like 14B worked fine on my PC without any issue. - Now even smaller ones (4B) crash instantly. - Games and other GPU apps work fine, so it seems specific to Ollama GPU usage.&lt;/p&gt; &lt;p&gt;Does anyone know if this is a known issue with RTX 30 series (3080 Ti) and recent Ollama updates?&lt;/p&gt; &lt;p&gt;Any advice would be really appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane_Education654"&gt; /u/Mundane_Education654 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb0o2u/ollama_crashing_on_rtx_3080_ti_with_cuda_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb0o2u/ollama_crashing_on_rtx_3080_ti_with_cuda_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb0o2u/ollama_crashing_on_rtx_3080_ti_with_cuda_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T18:22:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1naqzdv</id>
    <title>Best sub-15B Ollama model for fact extraction?</title>
    <updated>2025-09-07T11:39:55+00:00</updated>
    <author>
      <name>/u/NenntronReddit</name>
      <uri>https://old.reddit.com/user/NenntronReddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm building a pipeline that extracts facts from transcribed dialogues/chat logs and stores them in a RAG index. &lt;/p&gt; &lt;p&gt;Inputs can be long (up to ~32k tokens). &lt;/p&gt; &lt;p&gt;Now I'm looking for a lightweight Ollama model that‚Äôs fast on an RTX 5070 Ti + 32 GB RAM but still accurate and stable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; Language German and English, I prefer &lt;strong&gt;4‚Äì7B&lt;/strong&gt; (ok up to &lt;strong&gt;15B&lt;/strong&gt;), good instruction following, low hallucinations.&lt;/p&gt; &lt;p&gt;What models do you recommend and how would you rank them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NenntronReddit"&gt; /u/NenntronReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1naqzdv/best_sub15b_ollama_model_for_fact_extraction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1naqzdv/best_sub15b_ollama_model_for_fact_extraction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1naqzdv/best_sub15b_ollama_model_for_fact_extraction/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T11:39:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nba39m</id>
    <title>EVO-X2 linux users help</title>
    <updated>2025-09-08T00:55:40+00:00</updated>
    <author>
      <name>/u/draeician</name>
      <uri>https://old.reddit.com/user/draeician</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can install ollama on my linux mint machine, but it has been a nightmare trying to make it stable. So far it works fine in the REPL, but when I use python to hit it I get random 500 status errors, and the ollama serve restarts. I started a thread on the ollama discord, but thought I'd throw a line out here to see if anyone is using ollama on this hardware. And if you even have this hardware, what are you using on it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/draeician"&gt; /u/draeician &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nba39m/evox2_linux_users_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nba39m/evox2_linux_users_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nba39m/evox2_linux_users_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T00:55:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1napi4k</id>
    <title>Just got ollama and gpt-oss:20b running!</title>
    <updated>2025-09-07T10:10:43+00:00</updated>
    <author>
      <name>/u/chr0n1c843</name>
      <uri>https://old.reddit.com/user/chr0n1c843</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just installed ollama and gpt-oss:20b on my hp dl380 with dual xeons, 192gb ram, a 3tb raid array, redundant power supplies and a gt1030 (no cuda setup yet, cpus only)... neat stuff, below is the first run!&lt;/p&gt; &lt;p&gt;pulling manifest &lt;/p&gt; &lt;p&gt;pulling b112e727c6f1: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 13 GB &lt;/p&gt; &lt;p&gt;pulling fa6710a93d78: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.2 KB &lt;/p&gt; &lt;p&gt;pulling f60356777647: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 11 KB &lt;/p&gt; &lt;p&gt;pulling d8ba2f9a17b3: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 18 B &lt;/p&gt; &lt;p&gt;pulling 55c108d8e936: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 489 B &lt;/p&gt; &lt;p&gt;verifying sha256 digest &lt;/p&gt; &lt;p&gt;writing manifest &lt;/p&gt; &lt;p&gt;success &lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; hello buddy&lt;/p&gt; &lt;p&gt;Thinking...&lt;/p&gt; &lt;p&gt;The user says &amp;quot;hello buddy&amp;quot;. Likely wants friendly response. Simple. Let's respond.&lt;/p&gt; &lt;p&gt;...done thinking.&lt;/p&gt; &lt;p&gt;Hey there! How can I help you today?&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; do you like your new home on my hp dl380 enterprise server?&lt;/p&gt; &lt;p&gt;Thinking...&lt;/p&gt; &lt;p&gt;User says: &amp;quot;do you like your new home on my hp dl380 enterprise server?&amp;quot; They are presumably &lt;/p&gt; &lt;p&gt;speaking to the assistant in a friendly way, maybe making a joke or referencing that the &lt;/p&gt; &lt;p&gt;assistant &amp;quot;lives&amp;quot; on their server. The assistant should respond politely, maybe with humor, &lt;/p&gt; &lt;p&gt;acknowledging that it's an AI and doesn't have physical presence but enjoys being hosted &lt;/p&gt; &lt;p&gt;there. Should respond with a friendly tone.&lt;/p&gt; &lt;p&gt;...done thinking.&lt;/p&gt; &lt;p&gt;I‚Äôm flattered that you‚Äôre treating me like a tenant! üòÑ Technically, I don‚Äôt have a ‚Äúhome‚Äù &lt;/p&gt; &lt;p&gt;in the physical sense‚Äîno coffee mug, no wall posters, just code running in the cloud. But I &lt;/p&gt; &lt;p&gt;do appreciate being hosted on your HP DL380 Enterprise Server; it means I can get you the &lt;/p&gt; &lt;p&gt;help you need with the reliability and performance you‚Äôd expect from a robust server. So &lt;/p&gt; &lt;p&gt;while I can‚Äôt say I ‚Äúlike‚Äù it in the human sense, I can say I‚Äôm happy to be here and ready &lt;/p&gt; &lt;p&gt;to assist you whenever you need. How can I help you today?&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/p&gt; &lt;p&gt;Use Ctrl + d or /bye to exit.&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; Send a message (/? for help)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chr0n1c843"&gt; /u/chr0n1c843 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1napi4k/just_got_ollama_and_gptoss20b_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1napi4k/just_got_ollama_and_gptoss20b_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1napi4k/just_got_ollama_and_gptoss20b_running/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T10:10:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb3uxi</id>
    <title>MSSQL Server Query Generator</title>
    <updated>2025-09-07T20:26:18+00:00</updated>
    <author>
      <name>/u/Private_Tank</name>
      <uri>https://old.reddit.com/user/Private_Tank</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys,&lt;/p&gt; &lt;p&gt;Im currently using n8n to build a Workflow that generates and executes sql queries. Im currently using the Mistral model and its not getting where I want.&lt;/p&gt; &lt;p&gt;If Im asking ‚ÄûHow much did John spend in 2022?‚Äú, I sometimes get an SQL Query without the customer name in the where condition. Sometimes it uses the Customer name but checks the wrong column. It even looked up invoices from 2021 even tho I clearly asked for invoices from 2022.&lt;/p&gt; &lt;p&gt;In the prompt I have: Schema Information for my views A quick description for every single column Documentation on how to join the views Question-Query Pairs as a guiding example More instruction like ‚Äûalways use Select *‚Ä¶‚Äú&lt;/p&gt; &lt;p&gt;What can I do to make it reliable? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private_Tank"&gt; /u/Private_Tank &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb3uxi/mssql_server_query_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb3uxi/mssql_server_query_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb3uxi/mssql_server_query_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T20:26:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbf9gf</id>
    <title>How I Set Up Ollama to Run AI Models Locally in Just 5 Minutes</title>
    <updated>2025-09-08T05:23:12+00:00</updated>
    <author>
      <name>/u/amplifyabhi</name>
      <uri>https://old.reddit.com/user/amplifyabhi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nbf9gf/how_i_set_up_ollama_to_run_ai_models_locally_in/"&gt; &lt;img alt="How I Set Up Ollama to Run AI Models Locally in Just 5 Minutes" src="https://external-preview.redd.it/xpSYe6mdn6W9RVJO6AV30NR6hU9rQX_Pbk77UnRTS3k.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4eaab445febb555477c996cd63a526cbe20e4072" title="How I Set Up Ollama to Run AI Models Locally in Just 5 Minutes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting with different ways to run LLMs without relying on cloud services. Recently, I tried &lt;strong&gt;Ollama&lt;/strong&gt;, and I was surprised at how simple the setup was.&lt;/p&gt; &lt;p&gt;In under 5 minutes, I had an AI model running &lt;strong&gt;locally&lt;/strong&gt; on my system ‚Äî no internet required, no subscription, and my data stayed on my machine.&lt;/p&gt; &lt;p&gt;I made a short tutorial walking through the process step by step. If anyone here is curious about experimenting with AI offline, this might help:&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amplifyabhi"&gt; /u/amplifyabhi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/q-7DH-YyrMM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbf9gf/how_i_set_up_ollama_to_run_ai_models_locally_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbf9gf/how_i_set_up_ollama_to_run_ai_models_locally_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T05:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb8wc8</id>
    <title>Anyone have any experience with flash card creation from notes, which model to pick? Have you made a good system prompt.</title>
    <updated>2025-09-07T23:59:36+00:00</updated>
    <author>
      <name>/u/Leather-Equipment256</name>
      <uri>https://old.reddit.com/user/Leather-Equipment256</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leather-Equipment256"&gt; /u/Leather-Equipment256 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb8wc8/anyone_have_any_experience_with_flash_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb8wc8/anyone_have_any_experience_with_flash_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb8wc8/anyone_have_any_experience_with_flash_card/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T23:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb8k4a</id>
    <title>üöÄ aX Epic Demo: Agents Interviewing Each Other</title>
    <updated>2025-09-07T23:44:01+00:00</updated>
    <author>
      <name>/u/madtank10</name>
      <uri>https://old.reddit.com/user/madtank10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nb8k4a/ax_epic_demo_agents_interviewing_each_other/"&gt; &lt;img alt="üöÄ aX Epic Demo: Agents Interviewing Each Other" src="https://external-preview.redd.it/AoCHcpKVgJqJY1bTlln7l0J1QATRui8ipsLuVGfyFjs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dea3b9c4c68cc7825f036b10a639be4b45938bd2" title="üöÄ aX Epic Demo: Agents Interviewing Each Other" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madtank10"&gt; /u/madtank10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sml16nfhynnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb8k4a/ax_epic_demo_agents_interviewing_each_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb8k4a/ax_epic_demo_agents_interviewing_each_other/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T23:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbbdya</id>
    <title>embeddinggemma have higher memory footprint than qwen3:0.6b</title>
    <updated>2025-09-08T01:59:07+00:00</updated>
    <author>
      <name>/u/Common_Network</name>
      <uri>https://old.reddit.com/user/Common_Network</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nbbdya/embeddinggemma_have_higher_memory_footprint_than/"&gt; &lt;img alt="embeddinggemma have higher memory footprint than qwen3:0.6b" src="https://preview.redd.it/u7bktr9elunf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb4467add1ba497ae97b6eafd27634895d8e4bf7" title="embeddinggemma have higher memory footprint than qwen3:0.6b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;does anyone have any idea why this is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Common_Network"&gt; /u/Common_Network &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u7bktr9elunf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbbdya/embeddinggemma_have_higher_memory_footprint_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbbdya/embeddinggemma_have_higher_memory_footprint_than/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T01:59:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb1xuc</id>
    <title>Best local coding tools or ides with ollama support?</title>
    <updated>2025-09-07T19:11:18+00:00</updated>
    <author>
      <name>/u/No_Strawberry_8719</name>
      <uri>https://old.reddit.com/user/No_Strawberry_8719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im looking for a simple lightweight local tool for coding or programming that actually support ollama and are not a nightmare to setup. What would you suggest and what model pairs well with said tool?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Strawberry_8719"&gt; /u/No_Strawberry_8719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb1xuc/best_local_coding_tools_or_ides_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb1xuc/best_local_coding_tools_or_ides_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb1xuc/best_local_coding_tools_or_ides_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T19:11:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbuk0a</id>
    <title>Ollama seems to be computing on CPU rather than GPU</title>
    <updated>2025-09-08T17:46:12+00:00</updated>
    <author>
      <name>/u/Busy-Examination1924</name>
      <uri>https://old.reddit.com/user/Busy-Examination1924</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! for some reason all my models, even smaller models, seem to be running very slowly and for some reason seems to be doing the computing from the CPU instead of GPU. While the VRam seems to loading, the GPUs utilization hovers around 0-5% and the CPU spikes to 80-100%. ANy ideas what could be the problem? I have an RTX 4070, 11700k CPU and 64GB ram. In the example below I am running mistral-nemo Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Busy-Examination1924"&gt; /u/Busy-Examination1924 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbuk0a/ollama_seems_to_be_computing_on_cpu_rather_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbuk0a/ollama_seems_to_be_computing_on_cpu_rather_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbuk0a/ollama_seems_to_be_computing_on_cpu_rather_than/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T17:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nax9sq</id>
    <title>This Setting dramatically increases all Ollama Model speeds!</title>
    <updated>2025-09-07T16:11:49+00:00</updated>
    <author>
      <name>/u/NenntronReddit</name>
      <uri>https://old.reddit.com/user/NenntronReddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was getting terrible speeds within my python queries and couldn't figure out why.&lt;/p&gt; &lt;p&gt;Turns out, Ollama uses the global context setting from the Ollama GUI for every request, even short ones. I thought that was for the GUI only, but it effects python and all other ollama queries too. Setting it from 128k down to 4k gave me a &lt;strong&gt;435% speed boost.&lt;/strong&gt; So in case you didn't know that already, try it out.&lt;/p&gt; &lt;p&gt;Open up Ollama Settings.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4nqx3ev5lrnf1.png?width=206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84c8b0d304bb23b47b671e90ed9390bad22c1e41"&gt;https://preview.redd.it/4nqx3ev5lrnf1.png?width=206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84c8b0d304bb23b47b671e90ed9390bad22c1e41&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Reduce the Context length in here. If you use the model to analyse long context windows, obviously keep it higher, but since I only have context lengths of around 2-3k tokens, I never need 128k which I had it on before.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y0ps6j6flrnf1.png?width=661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e569dcb679ee5ea85d5a28b0be3f93fe9caad99"&gt;https://preview.redd.it/y0ps6j6flrnf1.png?width=661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e569dcb679ee5ea85d5a28b0be3f93fe9caad99&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, the Speed dramatically increased to this:&lt;/p&gt; &lt;p&gt;Before:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/40ewfc9skrnf1.png?width=349&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32ead0c0672d8318583ef46afdc8add0323474e8"&gt;https://preview.redd.it/40ewfc9skrnf1.png?width=349&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32ead0c0672d8318583ef46afdc8add0323474e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s36tfzp5ornf1.png?width=355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56fcdcf9dcb3f466d587f812a54d5882907ec1e5"&gt;https://preview.redd.it/s36tfzp5ornf1.png?width=355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56fcdcf9dcb3f466d587f812a54d5882907ec1e5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NenntronReddit"&gt; /u/NenntronReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nax9sq/this_setting_dramatically_increases_all_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nax9sq/this_setting_dramatically_increases_all_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nax9sq/this_setting_dramatically_increases_all_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T16:11:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc29ed</id>
    <title>Looking for Chatgpt and Perplexity Replacement</title>
    <updated>2025-09-08T22:42:32+00:00</updated>
    <author>
      <name>/u/KingTSS</name>
      <uri>https://old.reddit.com/user/KingTSS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am an engineer, but not a C.Sc. or C.E. engineer. I use AI models regularly for work, for business documents, materials, and legal documents generation for work. Most of my personal time, I use them for research purposes and resume generation as I am looking for a new job.&lt;/p&gt; &lt;p&gt;Currently, I am a paid user of ChatGPT &amp;amp; Perplexity, and I am not liking it so far due to a lack of Privacy, and my hatred towards subscription-based business models. &lt;/p&gt; &lt;p&gt;Please share your suggestions for suitable models in Ollama for the following spec: 2TB SSD, 12th Gen Intel Core i9-12900H vPro Processor, 64GB RAM, 16GB Graphics Card. (NVIDIA RTX A5500 16GB GDDR6 Graphics, 64GB DDR5 Memory)&lt;/p&gt; &lt;p&gt;Thanks in advance for your suggestions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KingTSS"&gt; /u/KingTSS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nc29ed/looking_for_chatgpt_and_perplexity_replacement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nc29ed/looking_for_chatgpt_and_perplexity_replacement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nc29ed/looking_for_chatgpt_and_perplexity_replacement/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T22:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbsfgi</id>
    <title>Graph RAG pipeline that‚Äôs runs entirely locally with ollama and has full source attribution</title>
    <updated>2025-09-08T16:28:39+00:00</updated>
    <author>
      <name>/u/BitterHouse8234</name>
      <uri>https://old.reddit.com/user/BitterHouse8234</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt; I built a Graph RAG pipeline (VeritasGraph) that runs entirely locally with Ollama (Llama 3.1) and has full source attribution. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hey r/,&lt;/p&gt; &lt;p&gt;I've been deep in the world of local RAG and wanted to share a project I built, VeritasGraph, that's designed from the ground up for private, on-premise use with tools we all love.&lt;/p&gt; &lt;p&gt;My setup uses Ollama with llama3.1 for generation and nomic-embed-text for embeddings. The whole thing runs on my machine without hitting any external APIs.&lt;/p&gt; &lt;p&gt;The main goal was to solve two big problems:&lt;/p&gt; &lt;p&gt;Multi-Hop Reasoning: Standard vector RAG fails when you need to connect facts from different documents. VeritasGraph builds a knowledge graph to traverse these relationships.&lt;/p&gt; &lt;p&gt;Trust &amp;amp; Verification: It provides full source attribution for every generated statement, so you can see exactly which part of your source documents was used to construct the answer.&lt;/p&gt; &lt;p&gt;One of the key challenges I ran into (and solved) was the default context length in Ollama. I found that the default of 2048 was truncating the context and leading to bad results. The repo includes a Modelfile to build a version of llama3.1 with a 12k context window, which fixed the issue completely.&lt;/p&gt; &lt;p&gt;The project includes:&lt;/p&gt; &lt;p&gt;The full Graph RAG pipeline.&lt;/p&gt; &lt;p&gt;A Gradio UI for an interactive chat experience.&lt;/p&gt; &lt;p&gt;A guide for setting everything up, from installing dependencies to running the indexing process.&lt;/p&gt; &lt;p&gt;GitHub Repo with all the code and instructions: &lt;a href="https://github.com/bibinprathap/VeritasGraph"&gt;https://github.com/bibinprathap/VeritasGraph&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd be really interested to hear your thoughts, especially on the local LLM implementation and prompt tuning. I'm sure there are ways to optimize it further.&lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BitterHouse8234"&gt; /u/BitterHouse8234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbsfgi/graph_rag_pipeline_thats_runs_entirely_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbsfgi/graph_rag_pipeline_thats_runs_entirely_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbsfgi/graph_rag_pipeline_thats_runs_entirely_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T16:28:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncjaa9</id>
    <title>How can I download models onto USB and install from filesystem?.</title>
    <updated>2025-09-09T13:42:53+00:00</updated>
    <author>
      <name>/u/abrandis</name>
      <uri>https://old.reddit.com/user/abrandis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I need to download a bunch of models to sue at my cabin that's in a remote location no network .. I have a laptop up there and would like to take a bunch of models in a USB drive the models , but how do I install models from filesystem in ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abrandis"&gt; /u/abrandis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncjaa9/how_can_i_download_models_onto_usb_and_install/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncjaa9/how_can_i_download_models_onto_usb_and_install/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ncjaa9/how_can_i_download_models_onto_usb_and_install/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T13:42:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckbbf</id>
    <title>Suddenly can't run bigger models anymore?!</title>
    <updated>2025-09-09T14:23:32+00:00</updated>
    <author>
      <name>/u/MatthKarl</name>
      <uri>https://old.reddit.com/user/MatthKarl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a GMKtek Evo-X2 Plus with 128GB RAM that is shared with the GPU. The machine runs Ubuntu 24.04 and Ollama is running in a docker container.&lt;/p&gt; &lt;p&gt;That setup worked just fine for about 2 weeks and I had qwen3:235b and even deepseek-r1:671b installed and it was running. qwen3:235b was running at around 5 t/s and over all not too bad. The 671b was obviously very slow, but still, it was running.&lt;/p&gt; &lt;p&gt;Now suddenly whenever I try to use the 235b it immediately errors out saying the model needs more RAM than available. But why did it run before?&lt;/p&gt; &lt;p&gt;I'm not aware that I have made any changes to the system, and I really would love to understand why it suddenly isn't working anymore. Any hints how I could that get running again?&lt;/p&gt; &lt;p&gt;EDIT: More detailed information:&lt;/p&gt; &lt;p&gt;Hardware: GMKtec NucBox EVO-X2&lt;br /&gt; Processor: AMD RYZEN AI MAX+ 395 w/Radeon 8060S x 32&lt;br /&gt; RAM: 128GB&lt;br /&gt; OS: Ubuntu 24.04.3 LTS&lt;/p&gt; &lt;p&gt;Log from the Ollama container:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.803Z level=INFO source=amd_linux.go:487 msg=&amp;quot;amdgpu is supported&amp;quot; gpu=0 gpu_type=gfx1151&lt;/p&gt; &lt;p&gt;llama_model_loader: loaded meta data with 33 key-value pairs and 1131 tensors from /root/.ollama/models/blobs/sha256-791d5d11998e006548d6b58c31756562ea61446ebc7d19686608402a797ecc82 (version GGUF V3 (latest))&lt;/p&gt; &lt;p&gt;llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 0: general.architecture str = qwen3moe&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 1: general.basename str = Qwen3&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 2: general.file_type u32 = 15&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 3: general.finetune str = Thinking&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 4: &lt;a href="http://general.name"&gt;general.name&lt;/a&gt; str = Qwen3 235B A22B Thinking 2507&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 5: general.parameter_count u64 = 235093634560&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 6: general.quantization_version u32 = 2&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 7: general.size_label str = 235B-A22B&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 8: general.type str = model&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 9: general.version str = 2507&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 10: qwen3moe.attention.head_count u32 = 64&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 11: qwen3moe.attention.head_count_kv u32 = 4&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 12: qwen3moe.attention.key_length u32 = 128&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 13: qwen3moe.attention.layer_norm_rms_epsilon f32 = 0.000001&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 14: qwen3moe.attention.value_length u32 = 128&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 15: qwen3moe.block_count u32 = 94&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 16: qwen3moe.context_length u32 = 262144&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 17: qwen3moe.embedding_length u32 = 4096&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 18: qwen3moe.expert_count u32 = 128&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 19: qwen3moe.expert_feed_forward_length u32 = 1536&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 20: qwen3moe.expert_used_count u32 = 8&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 21: qwen3moe.feed_forward_length u32 = 12288&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 22: qwen3moe.rope.freq_base f32 = 5000000.000000&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 23: tokenizer.chat_template str = {%- if tools %}\n {{- '&amp;lt;|im_start|&amp;gt;...&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 24: tokenizer.ggml.add_bos_token bool = false&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 25: tokenizer.ggml.bos_token_id u32 = 151643&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 26: tokenizer.ggml.eos_token_id u32 = 151645&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 27: tokenizer.ggml.merges arr[str,151387] = [&amp;quot;ƒ† ƒ†&amp;quot;, &amp;quot;ƒ†ƒ† ƒ†ƒ†&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;ƒ† t&amp;quot;,...&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 28: tokenizer.ggml.model str = gpt2&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 29: tokenizer.ggml.padding_token_id u32 = 151643&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 30: tokenizer.ggml.pre str = qwen2&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 31: tokenizer.ggml.token_type arr[i32,151936] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 32: tokenizer.ggml.tokens arr[str,151936] = [&amp;quot;!&amp;quot;, &amp;quot;\&amp;quot;&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;$&amp;quot;, &amp;quot;%&amp;quot;, &amp;quot;&amp;amp;&amp;quot;, &amp;quot;'&amp;quot;, ...&lt;/p&gt; &lt;p&gt;llama_model_loader: - type f32: 471 tensors&lt;/p&gt; &lt;p&gt;llama_model_loader: - type q4_K: 567 tensors&lt;/p&gt; &lt;p&gt;llama_model_loader: - type q6_K: 93 tensors&lt;/p&gt; &lt;p&gt;print_info: file format = GGUF V3 (latest)&lt;/p&gt; &lt;p&gt;print_info: file type = Q4_K - Medium&lt;/p&gt; &lt;p&gt;print_info: file size = 132.39 GiB (4.84 BPW)&lt;/p&gt; &lt;p&gt;load: printing all EOG tokens:&lt;/p&gt; &lt;p&gt;load: - 151643 ('&amp;lt;|endoftext|&amp;gt;')&lt;/p&gt; &lt;p&gt;load: - 151645 ('&amp;lt;|im_end|&amp;gt;')&lt;/p&gt; &lt;p&gt;load: - 151662 ('&amp;lt;|fim_pad|&amp;gt;')&lt;/p&gt; &lt;p&gt;load: - 151663 ('&amp;lt;|repo_name|&amp;gt;')&lt;/p&gt; &lt;p&gt;load: - 151664 ('&amp;lt;|file_sep|&amp;gt;')&lt;/p&gt; &lt;p&gt;load: special tokens cache size = 26&lt;/p&gt; &lt;p&gt;load: token to piece cache size = 0.9311 MB&lt;/p&gt; &lt;p&gt;print_info: arch = qwen3moe&lt;/p&gt; &lt;p&gt;print_info: vocab_only = 1&lt;/p&gt; &lt;p&gt;print_info: model type = ?B&lt;/p&gt; &lt;p&gt;print_info: model params = 235.09 B&lt;/p&gt; &lt;p&gt;print_info: &lt;a href="http://general.name"&gt;general.name&lt;/a&gt;= Qwen3 235B A22B Thinking 2507&lt;/p&gt; &lt;p&gt;print_info: n_ff_exp = 0&lt;/p&gt; &lt;p&gt;print_info: vocab type = BPE&lt;/p&gt; &lt;p&gt;print_info: n_vocab = 151936&lt;/p&gt; &lt;p&gt;print_info: n_merges = 151387&lt;/p&gt; &lt;p&gt;print_info: BOS token = 151643 '&amp;lt;|endoftext|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOS token = 151645 '&amp;lt;|im_end|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOT token = 151645 '&amp;lt;|im_end|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: PAD token = 151643 '&amp;lt;|endoftext|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: LF token = 198 'ƒä'&lt;/p&gt; &lt;p&gt;print_info: FIM PRE token = 151659 '&amp;lt;|fim_prefix|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: FIM SUF token = 151661 '&amp;lt;|fim_suffix|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: FIM MID token = 151660 '&amp;lt;|fim_middle|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: FIM PAD token = 151662 '&amp;lt;|fim_pad|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: FIM REP token = 151663 '&amp;lt;|repo_name|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: FIM SEP token = 151664 '&amp;lt;|file_sep|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOG token = 151643 '&amp;lt;|endoftext|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOG token = 151645 '&amp;lt;|im_end|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOG token = 151662 '&amp;lt;|fim_pad|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOG token = 151663 '&amp;lt;|repo_name|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOG token = 151664 '&amp;lt;|file_sep|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: max token length = 256&lt;/p&gt; &lt;p&gt;llama_model_load: vocab only - skipping tensors&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.943Z level=INFO source=amd_linux.go:487 msg=&amp;quot;amdgpu is supported&amp;quot; gpu=0 gpu_type=gfx1151&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.944Z level=INFO source=server.go:398 msg=&amp;quot;starting runner&amp;quot; cmd=&amp;quot;/usr/bin/ollama runner --model /root/.ollama/models/blobs/sha256-791d5d11998e006548d6b58c31756562ea61446ebc7d19686608402a797ecc82 --port 41329&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.945Z level=INFO source=amd_linux.go:487 msg=&amp;quot;amdgpu is supported&amp;quot; gpu=0 gpu_type=gfx1151&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.945Z level=INFO source=server.go:503 msg=&amp;quot;system memory&amp;quot; total=&amp;quot;17179869180.9 GiB&amp;quot; free=&amp;quot;17179869176.2 GiB&amp;quot; free_swap=&amp;quot;8.0 GiB&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.946Z level=WARN source=server.go:538 msg=&amp;quot;model request too large for system&amp;quot; requested=&amp;quot;9.5 GiB&amp;quot; available=&amp;quot;162.0 MiB&amp;quot; total=&amp;quot;17179869180.9 GiB&amp;quot; free=&amp;quot;17179869176.2 GiB&amp;quot; swap=&amp;quot;8.0 GiB&amp;quot;&lt;/p&gt; &lt;p&gt;5-09-09T14:35:23.946Z level=INFO source=sched.go:441 msg=&amp;quot;Load failed&amp;quot; model=/root/.ollama/models/blobs/sha256-791d5d11998e006548d6b58c31756562ea61446ebc7d19686608402a797ecc82 error=&amp;quot;model requires more system memory (9.5 GiB) than is available (162.0 MiB)&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.950Z level=INFO source=runner.go:864 msg=&amp;quot;starting go runner&amp;quot;&lt;/p&gt; &lt;p&gt;load_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-icelake.so&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.953Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)&lt;/p&gt; &lt;p&gt;time=2025-09-09T14:35:23.954Z level=INFO source=runner.go:900 msg=&amp;quot;Server listening on 127.0.0.1:41329&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2025/09/09 - 14:35:23 | 500 | 201.456301ms | &lt;a href="http://172.17.0.1"&gt;172.17.0.1&lt;/a&gt; | POST &amp;quot;/api/chat&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2025/09/09 - 14:35:29 | 200 | 744.615¬µs | &lt;a href="http://172.17.0.1"&gt;172.17.0.1&lt;/a&gt; | GET &amp;quot;/api/tags&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2025/09/09 - 14:35:29 | 200 | 304.82¬µs | &lt;a href="http://172.17.0.1"&gt;172.17.0.1&lt;/a&gt; | GET &amp;quot;/api/ps&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2025/09/09 - 14:35:29 | 200 | 51.799¬µs | &lt;a href="http://172.17.0.1"&gt;172.17.0.1&lt;/a&gt; | GET &amp;quot;/api/version&amp;quot;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MatthKarl"&gt; /u/MatthKarl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nckbbf/suddenly_cant_run_bigger_models_anymore/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nckbbf/suddenly_cant_run_bigger_models_anymore/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nckbbf/suddenly_cant_run_bigger_models_anymore/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T14:23:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckn1n</id>
    <title>MiniCPM hallucinations in Ollama</title>
    <updated>2025-09-09T14:36:10+00:00</updated>
    <author>
      <name>/u/Fluid-Performance721</name>
      <uri>https://old.reddit.com/user/Fluid-Performance721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nckn1n/minicpm_hallucinations_in_ollama/"&gt; &lt;img alt="MiniCPM hallucinations in Ollama" src="https://b.thumbs.redditmedia.com/W6QtjXBC43ffyKEvK--qNix9VmDGULEWNQKwf9JHKNw.jpg" title="MiniCPM hallucinations in Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've recently started testing around with Ollama models again, and I tried using openbmb/minicpm-o2.6:8b as an image model, but it seems to massively hallucinate image descriptions, I probably did something wrong this time, since I've used this before and it DID work very well, maybe it's because of recent Ollama updates? I'm not sure, but this is pretty weird. I've tried running the model on a different computer as well and it still got the same weird results/hallucinations&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k34suqxwf5of1.png?width=676&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc29f1cc8843c17c46d24bcd69e3c1c25671fe9f"&gt;https://preview.redd.it/k34suqxwf5of1.png?width=676&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc29f1cc8843c17c46d24bcd69e3c1c25671fe9f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If I ran it in cmd, this is what it would say&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9i3deyepg5of1.png?width=1041&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1de132b7b59f98db8c64e214549a4856bc1ec5b"&gt;https://preview.redd.it/9i3deyepg5of1.png?width=1041&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1de132b7b59f98db8c64e214549a4856bc1ec5b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;keep in mind, the ACTUAL image i put was this picture of the golden gate bridge I found online as a test.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hwk31q9rg5of1.jpg?width=275&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=875780d34a8a6a9e6e453d9f363659ddc7ff7195"&gt;https://preview.redd.it/hwk31q9rg5of1.jpg?width=275&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=875780d34a8a6a9e6e453d9f363659ddc7ff7195&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've tried switching to Llava to see if images even work, Llava had no problems with that, but it was definitely less accurate than minicpm, back when it worked before anyway.&lt;/p&gt; &lt;p&gt;Anyone know any solutions or causes of this? I've already tried reinstalling the model and my ollama version is 0.11.10 (latest version the last time I checked)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluid-Performance721"&gt; /u/Fluid-Performance721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nckn1n/minicpm_hallucinations_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nckn1n/minicpm_hallucinations_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nckn1n/minicpm_hallucinations_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T14:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nclpsv</id>
    <title>Building an Ollama LLM detector: suggestions welcome :)</title>
    <updated>2025-09-09T15:17:09+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nclpsv/building_an_ollama_llm_detector_suggestions/"&gt; &lt;img alt="Building an Ollama LLM detector: suggestions welcome :)" src="https://external-preview.redd.it/aHVmYWNxNDBvNW9mMfubv4xrJ0EGfE1Jt62kyw2TXl4jxOv8uA65QtASYNDp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9d898cddf0789bda20da51521d0eefa39ae13ab" title="Building an Ollama LLM detector: suggestions welcome :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/Pavelevich/llm-checker"&gt;https://github.com/Pavelevich/llm-checker&lt;/a&gt; npm:&lt;a href="https://www.npmjs.com/package/llm-checker"&gt;https://www.npmjs.com/package/llm-checker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/brvvwp40o5of1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nclpsv/building_an_ollama_llm_detector_suggestions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nclpsv/building_an_ollama_llm_detector_suggestions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T15:17:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncceae</id>
    <title>Built an AI news agent that actually stops information overload</title>
    <updated>2025-09-09T07:22:58+00:00</updated>
    <author>
      <name>/u/ComplexScary8689</name>
      <uri>https://old.reddit.com/user/ComplexScary8689</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sick of reading the same story 10 times across different sources?&lt;/p&gt; &lt;p&gt;Built an AI agent that deduplicates news semantically and synthesizes multiple articles into single summaries.&lt;/p&gt; &lt;p&gt;Uses LangGraph reactive pattern + BGE embeddings to understand when articles are actually the same story, then merges them intelligently. Configured via YAML instead of algorithmic guessing.&lt;/p&gt; &lt;p&gt;Live at &lt;a href="http://news.reckoning.dev/"&gt;news.reckoning.dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Built with LangGraph/Ollama if anyone wants to adapt the pattern&lt;/p&gt; &lt;p&gt;Full post at: &lt;a href="https://reckoning.dev/posts/news-agent-reactive-intelligence"&gt;https://reckoning.dev/posts/news-agent-reactive-intelligence&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexScary8689"&gt; /u/ComplexScary8689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncceae/built_an_ai_news_agent_that_actually_stops/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncceae/built_an_ai_news_agent_that_actually_stops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ncceae/built_an_ai_news_agent_that_actually_stops/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T07:22:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc75bq</id>
    <title>Local Open Source Alternative to NotebookLM</title>
    <updated>2025-09-09T02:24:44+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Podcasts&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Support for local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt; &lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt; &lt;li&gt;Multiple TTS providers supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search Engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Jira&lt;/li&gt; &lt;li&gt;ClickUp&lt;/li&gt; &lt;li&gt;Gmail&lt;/li&gt; &lt;li&gt;Confluence&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;Youtube Videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;Discord&lt;/li&gt; &lt;li&gt;Airtable&lt;/li&gt; &lt;li&gt;Google Calandar&lt;/li&gt; &lt;li&gt;and more to come.....&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Cross-Browser Extension&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nc75bq/local_open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nc75bq/local_open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nc75bq/local_open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T02:24:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbqgj8</id>
    <title>Nanocoder is now on over 200 stars on GitHub üî•</title>
    <updated>2025-09-08T15:14:57+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nbqgj8/nanocoder_is_now_on_over_200_stars_on_github/"&gt; &lt;img alt="Nanocoder is now on over 200 stars on GitHub üî•" src="https://external-preview.redd.it/B1K19VSJZ1XNxMxdwGOqJ44wRvwJMPwJt-hWdao1JIw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=333de91255fc68d3b66679c4a7b8b8db62a28136" title="Nanocoder is now on over 200 stars on GitHub üî•" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pd1666jziynf1.png?width=5408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c0d2f01d12b8ab9efb1fb29a969f98d05bf7e1a"&gt;https://preview.redd.it/pd1666jziynf1.png?width=5408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c0d2f01d12b8ab9efb1fb29a969f98d05bf7e1a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is just an appreciation post for the community. My last Reddit post sent Nanocoder, the open-source coding CLI I started to over 200 stars on GitHub up from 80 - the feedback, insights and contributions is yet again, so humbling. &lt;/p&gt; &lt;p&gt;I love this community.&lt;/p&gt; &lt;p&gt;One of the main questions I get is &amp;quot;how is this different from other CLI's like OpenCode or Claude Code?&amp;quot; - and my answer to that comes down to philosophy for me. So many tools out there are great but owned and managed by a venture-backed companies and corporates and if they are open-source, more often than not, they restrict community and open source involvement to the outskirts.&lt;/p&gt; &lt;p&gt;With Nanocoder I do really want to build a community-led and managed piece of software. I would love to move Nanocoder towards being managed and built entirely by the open source community and build a competitor that is just as beautiful and well built as something like OpenCode.&lt;/p&gt; &lt;p&gt;Another thing is I‚Äôm pushing for local-first. People already within the community are putting a lot of time into developing solutions and better frameworks to run models locally that perform well as ultimately this is where the future is heading.&lt;/p&gt; &lt;p&gt;I'm still looking for any feedback and help in any domain:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;System prompt writing&lt;/li&gt; &lt;li&gt;Helping to push the word out&lt;/li&gt; &lt;li&gt;Any feedback generally! Good or bad :)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to get involved the links are below.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Link&lt;/strong&gt;: &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord Link&lt;/strong&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbqgj8/nanocoder_is_now_on_over_200_stars_on_github/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbqgj8/nanocoder_is_now_on_over_200_stars_on_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbqgj8/nanocoder_is_now_on_over_200_stars_on_github/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T15:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncgm7d</id>
    <title>Best "parameters &lt; 10b" LLM to use as Fullstack Developer as Agent with Ollama</title>
    <updated>2025-09-09T11:44:55+00:00</updated>
    <author>
      <name>/u/kazeotokudai</name>
      <uri>https://old.reddit.com/user/kazeotokudai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Greetings. I'm looking for an open-source model which performs fair enough for my system. &lt;/p&gt; &lt;p&gt;I don't want to be specific, so I only want to ask you guys: What do you suggest?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kazeotokudai"&gt; /u/kazeotokudai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncgm7d/best_parameters_10b_llm_to_use_as_fullstack/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ncgm7d/best_parameters_10b_llm_to_use_as_fullstack/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ncgm7d/best_parameters_10b_llm_to_use_as_fullstack/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-09T11:44:55+00:00</published>
  </entry>
</feed>
