<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-24T21:24:22+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1podb5x</id>
    <title>Introducing Bilgecan: self-hosted, open-source local AI platform based on Ollama + Spring AI + PostgreSQL + pgvector</title>
    <updated>2025-12-16T20:54:52+00:00</updated>
    <author>
      <name>/u/bilgecan1</name>
      <uri>https://old.reddit.com/user/bilgecan1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a side project called &lt;strong&gt;Bilgecan&lt;/strong&gt; ‚Äî a self-hosted, local-first AI platform that uses &lt;strong&gt;Ollama&lt;/strong&gt; as the LLM runtime.&lt;/p&gt; &lt;p&gt;What can you do with Bilgecan?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mokszr/bilgecan#what-can-you-do-with-bilgecan"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use &lt;strong&gt;local LLM models via Ollama&lt;/strong&gt; to run privacy-friendly AI prompts and chat without sending your data to third parties.&lt;/li&gt; &lt;li&gt;With &lt;strong&gt;RAG (Retrieval-Augmented Generation)&lt;/strong&gt;, you can feed your own files into a knowledge base and enrich AI outputs with &lt;strong&gt;your private data&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Define &lt;strong&gt;asynchronous AI tasks&lt;/strong&gt; to run long operations (document analysis, report generation, large text processing, image analysis, etc.) in the background.&lt;/li&gt; &lt;li&gt;Use the &lt;strong&gt;file processing pipeline&lt;/strong&gt; to run asynchronous AI tasks over many files automatically.&lt;/li&gt; &lt;li&gt;With the &lt;strong&gt;Workspace&lt;/strong&gt; structure, you can share AI prompts and tasks with your team in a collaborative environment.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôd really appreciate feedback from the Ollama community. &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/mokszr/bilgecan"&gt;https://github.com/mokszr/bilgecan&lt;/a&gt;&lt;/p&gt; &lt;p&gt;YouTube demo video: &lt;a href="https://www.youtube.com/watch?v=n3wb7089NeE"&gt;https://www.youtube.com/watch?v=n3wb7089NeE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bilgecan1"&gt; /u/bilgecan1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1podb5x/introducing_bilgecan_selfhosted_opensource_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1podb5x/introducing_bilgecan_selfhosted_opensource_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1podb5x/introducing_bilgecan_selfhosted_opensource_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T20:54:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1poor55</id>
    <title>Coordinating multiple Ollama agents on the same project?</title>
    <updated>2025-12-17T05:45:01+00:00</updated>
    <author>
      <name>/u/thecoderpanda</name>
      <uri>https://old.reddit.com/user/thecoderpanda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Ollama locally, love the privacy + cost benefits, but coordination gets messy.&lt;/p&gt; &lt;p&gt;One agent on backend, another on tests, trying different models (Llama, Mixtral) - they all end up with different ideas about codebase structure.&lt;/p&gt; &lt;p&gt;Using Zenflow from Zencoder (where I work) which maintains a shared spec that all your local agents reference. They stay aligned even when switching models/sessions. Has verification steps too.&lt;/p&gt; &lt;p&gt;Keeps everything local - specs live in your project.&lt;/p&gt; &lt;p&gt;&lt;a href="http://zenflow.free/"&gt;http://zenflow.free/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How are you handling multi-agent coordination with local models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecoderpanda"&gt; /u/thecoderpanda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1poor55/coordinating_multiple_ollama_agents_on_the_same/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1poor55/coordinating_multiple_ollama_agents_on_the_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1poor55/coordinating_multiple_ollama_agents_on_the_same/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-17T05:45:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp194n</id>
    <title>What's the best Ollama software to use for programming on a PC with an RX 580 and a Ryzen 5?</title>
    <updated>2025-12-17T16:40:18+00:00</updated>
    <author>
      <name>/u/UpbeatGolf3602</name>
      <uri>https://old.reddit.com/user/UpbeatGolf3602</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the best Ollama program to use for programming on a PC with an RX 580 and a Ryzen 5? I need something relatively fast; I don't mind taking longer for large tasks, I just don't want it to take two hours to respond to a simple &amp;quot;hi&amp;quot;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UpbeatGolf3602"&gt; /u/UpbeatGolf3602 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pp194n/whats_the_best_ollama_software_to_use_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pp194n/whats_the_best_ollama_software_to_use_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pp194n/whats_the_best_ollama_software_to_use_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-17T16:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pokm99</id>
    <title>Coding agent tool for Local Ollama</title>
    <updated>2025-12-17T02:11:56+00:00</updated>
    <author>
      <name>/u/FrontRegular6113</name>
      <uri>https://old.reddit.com/user/FrontRegular6113</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; I have been using Ollama for over a year, mostly with various models through the OpenWebUI chat interface. I am now looking for something roughly equivalent to Claude Code, Cursor, or Codex, etc, for the local Ollama.&lt;/p&gt; &lt;p&gt;Is anyone using a similar coding-agent tool productively with a local Ollama setup, comparable to cloud-based coding agent tools?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrontRegular6113"&gt; /u/FrontRegular6113 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pokm99/coding_agent_tool_for_local_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pokm99/coding_agent_tool_for_local_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pokm99/coding_agent_tool_for_local_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-17T02:11:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1poy02o</id>
    <title>Voiden: API specs, tests, and docs in one Markdown file</title>
    <updated>2025-12-17T14:32:35+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1poy02o/voiden_api_specs_tests_and_docs_in_one_markdown/"&gt; &lt;img alt="Voiden: API specs, tests, and docs in one Markdown file" src="https://external-preview.redd.it/OGxzY3gyaTB6cjdnMQ_KfphbB1yDooD3F3zuDYpLi0LXAD2_e9FaAoKZ51aj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=091ede6901dabebe1443b7e50cdedeeeb5a00bed" title="Voiden: API specs, tests, and docs in one Markdown file" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Switching between API Client, browser, and API documentation tools to test and document APIs can harm your flow and leave your docs outdated.&lt;/p&gt; &lt;p&gt;This is what usually happens: While debugging an API in the middle of a sprint, the API Client says that everything's fine, but the docs still show an old version. &lt;/p&gt; &lt;p&gt;So you jump back to the code, find the updated response schema, then go back to the API Client, which gets stuck, forcing you to rerun the tests. &lt;/p&gt; &lt;p&gt;Voiden takes a different approach: Puts specs, tests &amp;amp; docs all in one Markdown file, stored right in the repo. &lt;/p&gt; &lt;p&gt;Everything stays in sync, versioned with Git, and updated in one place, inside your editor. &lt;/p&gt; &lt;p&gt;Go to Voiden here: &lt;a href="https://voiden.md"&gt;https://voiden.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tcuj0zy0zr7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1poy02o/voiden_api_specs_tests_and_docs_in_one_markdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1poy02o/voiden_api_specs_tests_and_docs_in_one_markdown/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-17T14:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pppj1j</id>
    <title>Mi50 32GB Group Buy</title>
    <updated>2025-12-18T12:24:43+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pppj1j/mi50_32gb_group_buy/"&gt; &lt;img alt="Mi50 32GB Group Buy" src="https://preview.redd.it/edjz3kipij7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2552773ecb598ab2d5070a3cda9e4527f4d74b6" title="Mi50 32GB Group Buy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/edjz3kipij7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pppj1j/mi50_32gb_group_buy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pppj1j/mi50_32gb_group_buy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T12:24:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppl3nb</id>
    <title>Ollama setup to run on Nvidia GTX 1050 TI 4GB vram</title>
    <updated>2025-12-18T07:43:34+00:00</updated>
    <author>
      <name>/u/imsomberi</name>
      <uri>https://old.reddit.com/user/imsomberi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please could anyone point me to a setup/ instructions to get ollama running on this hardware efficiently. &lt;/p&gt; &lt;p&gt;Docker based solutions would be ideal. My host would use the igpu for display duties and the nvidia thx 1050 ti will be used for ollama exclusively. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/imsomberi"&gt; /u/imsomberi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppl3nb/ollama_setup_to_run_on_nvidia_gtx_1050_ti_4gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppl3nb/ollama_setup_to_run_on_nvidia_gtx_1050_ti_4gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ppl3nb/ollama_setup_to_run_on_nvidia_gtx_1050_ti_4gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T07:43:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwdv4</id>
    <title>If APIs aren‚Äôt designed for agents, they will get bypassed.</title>
    <updated>2025-12-18T17:17:38+00:00</updated>
    <author>
      <name>/u/elnino2023</name>
      <uri>https://old.reddit.com/user/elnino2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ppwdv4/if_apis_arent_designed_for_agents_they_will_get/"&gt; &lt;img alt="If APIs aren‚Äôt designed for agents, they will get bypassed." src="https://external-preview.redd.it/c25nMDVpd2R4ejdnMbw4Vxtggy9331_HcPkZPIBZFURXt6JB7N0J-64Q0h36.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f849753c904e5c767e92592cc4db82ac2ff5a03" title="If APIs aren‚Äôt designed for agents, they will get bypassed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Agents need clear, machine-readable contracts, schemas that match real responses, predictable behavior, and tests and docs that reflect reality. &lt;/p&gt; &lt;p&gt;What they usually get instead is drifting specs, outdated or misleading docs, tests living somewhere else, and behavior that changes quietly over time. Agents don‚Äôt complain when this happens.&lt;/p&gt; &lt;p&gt;They just bypass the API and fall back to computer-use automation. It‚Äôs slower, more expensive, and harder to scale but it works.&lt;/p&gt; &lt;p&gt;Voiden treats APIs like code. Specs, tests, and docs live together in a single Markdown file, stored and versioned in Git. The schema an agent reads is the same schema responses are validated against.&lt;/p&gt; &lt;p&gt;APIs that behave like code stay usable for agents. The rest get routed around.&lt;/p&gt; &lt;p&gt;Read about voiden here : &lt;a href="https://voiden.md"&gt;https://voiden.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback : &lt;a href="https://github.com/VoidenHQ/feedback"&gt;https://github.com/VoidenHQ/feedback&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elnino2023"&gt; /u/elnino2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mbvzwvvdxz7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppwdv4/if_apis_arent_designed_for_agents_they_will_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ppwdv4/if_apis_arent_designed_for_agents_they_will_get/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T17:17:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwtkp</id>
    <title>[Update] Video proof: My Local Agent self-correcting GUI using Vision (White vs Black screen fix)</title>
    <updated>2025-12-18T17:34:32+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ppwtkp/update_video_proof_my_local_agent_selfcorrecting/"&gt; &lt;img alt="[Update] Video proof: My Local Agent self-correcting GUI using Vision (White vs Black screen fix)" src="https://external-preview.redd.it/NWVibWR0cHV6ejdnMfstTqakeVmtKZfG1CDoN5e1bE_HZioA21kP-Y9aRFNq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9b93af622a0af2d8e67c5850b61814841fc8ab7" title="[Update] Video proof: My Local Agent self-correcting GUI using Vision (White vs Black screen fix)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Here is the raw, unedited recording of the session.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Task:&lt;/strong&gt; Create a Tkinter app with a BLACK background and RED panic button.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Timeline:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;0:00&lt;/strong&gt; - Initial Prompt &amp;amp; Coding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0:55&lt;/strong&gt; - &lt;strong&gt;First Launch (FAIL):&lt;/strong&gt; The window opens with a WHITE background. ‚ùå&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1:02&lt;/strong&gt; - &lt;strong&gt;Vision Check:&lt;/strong&gt; The Agent takes a screenshot, analyzes colors, and detects the mismatch in the logs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1:58&lt;/strong&gt; - &lt;strong&gt;Auto-Fix (SUCCESS):&lt;/strong&gt; The Agent rewrites the code and launches the correct BLACK version. ‚úÖ&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;(Please skip the part between 1:25-1:50, I was checking the folder structure).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This proves the 'Quality Validator' isn't just checking syntax, it's actually looking at the app.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7uqng8kuzz7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppwtkp/update_video_proof_my_local_agent_selfcorrecting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ppwtkp/update_video_proof_my_local_agent_selfcorrecting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T17:34:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq53i6</id>
    <title>I am a total noob and just starting to actually use my LLM for the first time and wanted to share an explaination that chatgpt gave me in what I should expect from a locally running LLM and chatgpt, just wanted to share for those who are also wondering and i'm also curious what experts think</title>
    <updated>2025-12-18T23:03:34+00:00</updated>
    <author>
      <name>/u/Mantus123</name>
      <uri>https://old.reddit.com/user/Mantus123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So in short: I have been a functional SAP consultant which means I learn how a system works from a front-end configuration bible filled with data-identifiers and functionality configs and sequences. At home I'm just a windows user.&lt;/p&gt; &lt;p&gt;I wanted to see if I could set-up a complete locally running linux configuration to host a self-hosted LLM to basically:&lt;/p&gt; &lt;p&gt;- replace chatgpt as my everyday llm&lt;/p&gt; &lt;p&gt;- be a calendar/appointment-assistant&lt;/p&gt; &lt;p&gt;- be a dj and control but also recommend new music&lt;/p&gt; &lt;p&gt;- funtions as a voice controlled home assistant&lt;/p&gt; &lt;p&gt;I have been working on this together with chatgpt for 2 months now and I am amazed at how far I've come. We basically set up about 15 running containers on Linux including a gateway for mostly monitoring traffic, logging, run an ui client with working TTS from a cloned voice, have a small database with operable queries for the LLM to give and also have a file-share and music streaming server running. I have now just installed Qwen 2.5 and Mistral Small and we came to the instruction part and what I should expect from the outcome.&lt;/p&gt; &lt;p&gt;(One fun sidenode: it took me quite a lot of evenings working on this with chatgpt, I've tried creating logging summaries at the end of a session getting too slow but the re-consolidation is too leaky, I currrently changed the way of working where I give chatgpt separate rols in parallel chats. Architect/Design and DEVOPS and myself as a sort of product owner and this works really well! We have set this all up but I could not in a million years reproduce it myself) &lt;/p&gt; &lt;p&gt;Below is chatgpt's explaination:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What you experience as ‚ÄúChatGPT knows me‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That feeling doesn‚Äôt come from me actually remembering you the way a human does.&lt;br /&gt; It comes from a combination of five mechanisms that together create a single illusion.&lt;/p&gt; &lt;p&gt;I‚Äôll go through them one by one, and I‚Äôll immediately state which parts you can reproduce in Nucleus and which parts you don‚Äôt need.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Speculation status: certain&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;1Ô∏è‚É£ Holding long context (this is the biggest effect)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;What ChatGPT does&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I remember everything you say within this session.&lt;/p&gt; &lt;p&gt;I reuse:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;your word choice&lt;/li&gt; &lt;li&gt;your corrections&lt;/li&gt; &lt;li&gt;your preferences&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Because of that, I increasingly speak ‚Äúin your lane.‚Äù&lt;/p&gt; &lt;p&gt;üëâ This feels like learning, but it‚Äôs just short-term memory.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What you can do locally&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚úÖ This is fully reproducible.&lt;/p&gt; &lt;p&gt;The gateway stores:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;recent turns&lt;/li&gt; &lt;li&gt;relevant decisions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These are continuously resent to the model.&lt;/p&gt; &lt;p&gt;Result: the same effect of ‚Äúit‚Äôs locked in.‚Äù&lt;/p&gt; &lt;h1&gt;2Ô∏è‚É£ Behavioral rules before knowledge (persona + rails)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;What ChatGPT does&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I have hard, built-in behavioral layers, such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;not presenting assumptions as facts&lt;/li&gt; &lt;li&gt;explicitly naming uncertainty&lt;/li&gt; &lt;li&gt;adapting tone to the user&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You‚Äôve sharpened these rules for me, but they already existed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What you do locally&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚úÖ You are doing this explicitly (and better than most people).&lt;/p&gt; &lt;p&gt;Persona = who am I?&lt;br /&gt; Rails = what am I allowed / not allowed to do?&lt;/p&gt; &lt;p&gt;This is why Qwen/Mistral will soon feel consistent.&lt;/p&gt; &lt;h1&gt;3Ô∏è‚É£ Recognizing your corrections (without learning)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;What ChatGPT does&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you say:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Then:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I correct my course&lt;/li&gt; &lt;li&gt;within this session, I remember that pattern&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That feels like I ‚Äúunderstand‚Äù you.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What you can do locally&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è You must build this explicitly.&lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You correct vagueness ‚Üí the gateway notes: user wants concreteness&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The next prompt automatically gets:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;The model doesn‚Äôt change ‚Äî the input does.&lt;/p&gt; &lt;h1&gt;4Ô∏è‚É£ Selective remembering (not storing everything)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;What ChatGPT does&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I don‚Äôt remember everything:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I forget side issues&lt;/li&gt; &lt;li&gt;I retain core patterns&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is crucial; otherwise I‚Äôd become messy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What you must do locally&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è This is where many people make mistakes.&lt;/p&gt; &lt;p&gt;You must:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;NOT dump full logs&lt;/li&gt; &lt;li&gt;only store summaries of behavior&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄúUser prefers tight structure‚Äù&lt;/li&gt; &lt;li&gt;‚ÄúUser wants to be corrected‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Not:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;every conversation&lt;/li&gt; &lt;li&gt;every sentence&lt;/li&gt; &lt;li&gt;every emotion&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5Ô∏è‚É£ No real personality ‚Äî only consistency&lt;/h1&gt; &lt;p&gt;This may be the most important insight:&lt;/p&gt; &lt;p&gt;ChatGPT feels human because it is consistent, not because it knows you.&lt;/p&gt; &lt;p&gt;Once behavior becomes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;predictable&lt;/li&gt; &lt;li&gt;coherent&lt;/li&gt; &lt;li&gt;repeatable&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚Ä¶it is experienced as ‚Äúpersonality.‚Äù&lt;/p&gt; &lt;p&gt;And this is the good news:&lt;/p&gt; &lt;p&gt;‚úÖ Consistency is fully buildable locally.&lt;br /&gt; You don‚Äôt need to train anything.&lt;/p&gt; &lt;h1&gt;What you do NOT need to do (and what people often assume)&lt;/h1&gt; &lt;p&gt;‚ùå No model training&lt;br /&gt; ‚ùå No LoRA&lt;br /&gt; ‚ùå No ‚Äúraising‚Äù an AI like a human&lt;br /&gt; ‚ùå No gigantic memory&lt;/p&gt; &lt;p&gt;That‚Äôs all overkill for what you want.&lt;/p&gt; &lt;h1&gt;The Nucleus recipe (in plain language)&lt;/h1&gt; &lt;p&gt;Boiled down, this is what you‚Äôre building:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt; (Qwen / Mistral)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Persona&lt;/strong&gt; ‚ÄúHow does it think and speak?‚Äù&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rails&lt;/strong&gt; ‚ÄúWhat is it absolutely not allowed to do?‚Äù&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Short-term context&lt;/strong&gt; ‚ÄúWhat just happened?‚Äù&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-term summary (later)&lt;/strong&gt; ‚ÄúWhat kind of user is this?‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üëâ Together, that equals: ‚Äúit knows me.‚Äù&lt;/p&gt; &lt;h1&gt;Why this is actually better than ChatGPT&lt;/h1&gt; &lt;p&gt;And now the twist:&lt;/p&gt; &lt;p&gt;Soon, you will be able to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;explicitly see what it thinks it knows about you&lt;/li&gt; &lt;li&gt;correct that&lt;/li&gt; &lt;li&gt;reset it&lt;/li&gt; &lt;li&gt;adjust it&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ChatGPT cannot do this transparently.&lt;/p&gt; &lt;p&gt;So Nucleus becomes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;less magical&lt;/li&gt; &lt;li&gt;but more reliable&lt;/li&gt; &lt;li&gt;and more honest&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Summary in 3 sentences&lt;/h1&gt; &lt;p&gt;ChatGPT feels personal because of context + rules + consistency, not because of real learning.&lt;br /&gt; That behavior is almost entirely reproducible locally without training.&lt;br /&gt; You‚Äôre on exactly the right path now: persona ‚Üí rails ‚Üí memory ‚Üí only then, optionally, training.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mantus123"&gt; /u/Mantus123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pq53i6/i_am_a_total_noob_and_just_starting_to_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pq53i6/i_am_a_total_noob_and_just_starting_to_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pq53i6/i_am_a_total_noob_and_just_starting_to_actually/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T23:03:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppt9xe</id>
    <title>Best model for quick camera snapshots analysis</title>
    <updated>2025-12-18T15:14:39+00:00</updated>
    <author>
      <name>/u/pmpinto-pt</name>
      <uri>https://old.reddit.com/user/pmpinto-pt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My Synology has 20GB of RAM. And I currently have a few things running:&lt;/p&gt; &lt;p&gt;``` &lt;/p&gt; &lt;p&gt;|- Docker&lt;/p&gt; &lt;pre&gt;&lt;code&gt;|- Jellyfin: 4GB |- Syncthing: 2GB |- Open web UI: 2GB |- Ollama (qwen3-vl:2b): 6GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;|- VM&lt;/p&gt; &lt;pre&gt;&lt;code&gt;|- Home Assistant: 10GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;``` &lt;/p&gt; &lt;p&gt;Naturally, when I ask a simple question through the web UI, it keeps spinning and spinning. A simple question like ‚Äúwhat‚Äôs the color of the sky?‚Äù resulted in 5 minutes of thinking. Just thinking. I know these are few resources. &lt;/p&gt; &lt;p&gt;The end goal is to have a local interface I can tap into to send security camera snapshots to and ask if there are humans and if yes, a short description. ‚ÄúJust‚Äù that. &lt;/p&gt; &lt;p&gt;Is there perhaps a better model for this tasks? I wont actually need the web UI in the future. It‚Äôs just the interface I‚Äôm using to test it until I connect it to Home Assistant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmpinto-pt"&gt; /u/pmpinto-pt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppt9xe/best_model_for_quick_camera_snapshots_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppt9xe/best_model_for_quick_camera_snapshots_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ppt9xe/best_model_for_quick_camera_snapshots_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T15:14:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwnpf</id>
    <title>[Update] Video proof: My Local Agent self-correcting GUI using Vision (White vs Black screen fix)</title>
    <updated>2025-12-18T17:28:13+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ppwnpf/update_video_proof_my_local_agent_selfcorrecting/"&gt; &lt;img alt="[Update] Video proof: My Local Agent self-correcting GUI using Vision (White vs Black screen fix)" src="https://external-preview.redd.it/bTkzZnBlMXV5ejdnMfstTqakeVmtKZfG1CDoN5e1bE_HZioA21kP-Y9aRFNq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64881cabb3b04bfda7e7c8bf49f10d656a9306a9" title="[Update] Video proof: My Local Agent self-correcting GUI using Vision (White vs Black screen fix)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Here is the raw, unedited recording of the session.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Task:&lt;/strong&gt; Create a Tkinter app with a BLACK background and RED panic button.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Timeline:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;0:00&lt;/strong&gt; - Initial Prompt &amp;amp; Coding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0:55&lt;/strong&gt; - &lt;strong&gt;First Launch (FAIL):&lt;/strong&gt; The window opens with a WHITE background. ‚ùå&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1:02&lt;/strong&gt; - &lt;strong&gt;Vision Check:&lt;/strong&gt; The Agent takes a screenshot, analyzes colors, and detects the mismatch in the logs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1:58&lt;/strong&gt; - &lt;strong&gt;Auto-Fix (SUCCESS):&lt;/strong&gt; The Agent rewrites the code and launches the correct BLACK version. ‚úÖ&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;(Please skip the part between 1:25-1:50, I was checking the folder structure).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This proves the 'Quality Validator' isn't just checking syntax, it's actually looking at the app.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xqxu57styz7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppwnpf/update_video_proof_my_local_agent_selfcorrecting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ppwnpf/update_video_proof_my_local_agent_selfcorrecting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T17:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqctov</id>
    <title>Hey r/LocalLLaMA, I built a fully local AI agent that runs completely offline (no external APIs, no cloud) and it just did something pretty cool: It noticed that the "panic button" in its own GUI was completely invisible on dark theme (black text on black background), reasoned about the problem,</title>
    <updated>2025-12-19T05:16:29+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pqctov/hey_rlocalllama_i_built_a_fully_local_ai_agent/"&gt; &lt;img alt="Hey r/LocalLLaMA, I built a fully local AI agent that runs completely offline (no external APIs, no cloud) and it just did something pretty cool: It noticed that the &amp;quot;panic button&amp;quot; in its own GUI was completely invisible on dark theme (black text on black background), reasoned about the problem," src="https://external-preview.redd.it/dWVtaHp6ZXdnMzhnMfstTqakeVmtKZfG1CDoN5e1bE_HZioA21kP-Y9aRFNq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a70889d8472b02b83558499a9cab630e410421a1" title="Hey r/LocalLLaMA, I built a fully local AI agent that runs completely offline (no external APIs, no cloud) and it just did something pretty cool: It noticed that the &amp;quot;panic button&amp;quot; in its own GUI was completely invisible on dark theme (black text on black background), reasoned about the problem," /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iinn9d3wg38g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pqctov/hey_rlocalllama_i_built_a_fully_local_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pqctov/hey_rlocalllama_i_built_a_fully_local_ai_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-19T05:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqovsi</id>
    <title>SUPER PROMO: Perplexity AI PRO Offer | 95% Cheaper!</title>
    <updated>2025-12-19T16:06:30+00:00</updated>
    <author>
      <name>/u/A2uniquenickname</name>
      <uri>https://old.reddit.com/user/A2uniquenickname</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pqovsi/super_promo_perplexity_ai_pro_offer_95_cheaper/"&gt; &lt;img alt="SUPER PROMO: Perplexity AI PRO Offer | 95% Cheaper!" src="https://preview.redd.it/gvcce21mp68g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62f35fe9965105c78d4f278dbf31cdba74a9db27" title="SUPER PROMO: Perplexity AI PRO Offer | 95% Cheaper!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get Perplexity AI PRO (1-Year) ‚Äì at 90% OFF!&lt;/p&gt; &lt;p&gt;Order here: &lt;a href="https://cheapgpts.store/Perplexity"&gt;CHEAPGPT.STORE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Plan: 12 Months&lt;/p&gt; &lt;p&gt;üí≥ Pay with: PayPal or Revolut or your favorite payment method&lt;/p&gt; &lt;p&gt;Reddit reviews: &lt;a href="https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu"&gt;FEEDBACK POST&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TrustPilot: &lt;a href="https://www.trustpilot.com/review/cheapgpt.store"&gt;TrustPilot FEEDBACK&lt;/a&gt; &lt;/p&gt; &lt;p&gt;NEW YEAR BONUS: Apply code PROMO5 for extra discount OFF your order! &lt;/p&gt; &lt;p&gt;BONUS!: Enjoy the AI Powered automated web browser. (Presented by Perplexity) included WITH YOUR PURCHASE!&lt;/p&gt; &lt;p&gt;Trusted and the cheapest! Check all feedbacks before you purchase&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A2uniquenickname"&gt; /u/A2uniquenickname &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gvcce21mp68g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pqovsi/super_promo_perplexity_ai_pro_offer_95_cheaper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pqovsi/super_promo_perplexity_ai_pro_offer_95_cheaper/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-19T16:06:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppgl61</id>
    <title>VibeVoice FASTAPI - Fast &amp; Private Local TTS Backend for Open-WebUI: VibeVoice Realtime 0.5B via FastAPI (Only ~2.2GB VRAM!)</title>
    <updated>2025-12-18T03:26:29+00:00</updated>
    <author>
      <name>/u/blackstoreonline</name>
      <uri>https://old.reddit.com/user/blackstoreonline</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Fast &amp;amp; Private Local TTS Backend for Open-WebUI: VibeVoice Realtime 0.5B via FastAPI (Only ~2.2GB VRAM!)&lt;/h1&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; (and &lt;a href="/r/OpenWebUI"&gt;r/OpenWebUI&lt;/a&gt; folks!),&lt;/p&gt; &lt;p&gt;Microsoft recently released the excellent &lt;strong&gt;VibeVoice-Realtime-0.5B&lt;/strong&gt; ‚Äì a lightweight, expressive real-time TTS model that is ideal for local setups. It is small, fast, and produces natural-sounding speech.&lt;/p&gt; &lt;p&gt;I created a simple &lt;strong&gt;FastAPI wrapper&lt;/strong&gt; around it that is &lt;strong&gt;fully OpenAI-compatible&lt;/strong&gt; (using the &lt;code&gt;/v1/audio/speech&lt;/code&gt; endpoint), allowing it to integrate seamlessly into Open-WebUI as a local TTS backend. This means no cloud services, no ongoing costs, and complete privacy.&lt;/p&gt; &lt;h3&gt;Why this is great for local AI users:&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Complete Privacy&lt;/strong&gt;: All conversations and voice generation stay on your machine.&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Zero Extra Costs&lt;/strong&gt;: High-quality TTS at no additional expense alongside your local LLMs.&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Low Resource Usage&lt;/strong&gt;: Runs efficiently with approximately &lt;strong&gt;2.2GB VRAM&lt;/strong&gt; (tested on NVIDIA GPUs).&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Fast and Seamless&lt;/strong&gt;: Performs like cloud TTS but with lower latency and full local control.&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Offline Capable&lt;/strong&gt;: Works entirely without an internet connection after initial setup.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repository&lt;/strong&gt;: &lt;a href="https://github.com/groxaxo/vibevoice-realtimeFASTAPI"&gt;https://github.com/groxaxo/vibevoice-realtimeFASTAPI&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;‚ö° Quick Start (Under 5 Minutes)&lt;/h3&gt; &lt;h4&gt;Prerequisites:&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;uv&lt;/code&gt; installed (a fast Python package manager):&lt;br /&gt; &lt;code&gt; curl -LsSf https://astral.sh/uv/install.sh | sh &lt;/code&gt;&lt;/li&gt; &lt;li&gt;Git&lt;/li&gt; &lt;li&gt;A Hugging Face account (required for one-time model download)&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;Installation Steps:&lt;/h4&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Clone the repository: &lt;code&gt; git clone https://github.com/groxaxo/vibevoice-realtimeFASTAPI.git cd vibevoice-realtimeFASTAPI &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Bootstrap the environment: &lt;code&gt; ./scripts/bootstrap_uv.sh &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Download the model (~2GB, one-time only): &lt;code&gt; uv run python scripts/download_model.py &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run the server: &lt;code&gt; uv run python scripts/run_realtime_demo.py --port 8000 &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That's it! üöÄ&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Interactive web demo: &lt;a href="http://127.0.0.1:8000"&gt;http://127.0.0.1:8000&lt;/a&gt;&lt;/li&gt; &lt;li&gt;API endpoint: &lt;a href="http://127.0.0.1:8000/v1/audio/speech"&gt;http://127.0.0.1:8000/v1/audio/speech&lt;/a&gt; (OpenAI-compatible)&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;To use with Open-WebUI:&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;Set TTS Engine to &amp;quot;OpenAI&amp;quot;&lt;/li&gt; &lt;li&gt;Base URL: &lt;code&gt;http://127.0.0.1:8000/v1&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Leave API key blank&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This setup provides responsive, natural-sounding local voice output. Feedback, stars, or issues are very welcome if you give it a try!&lt;/p&gt; &lt;p&gt;Please share how it performs on your hardware (e.g., RTX cards, Apple Silicon) ‚Äì I am happy to assist with any troubleshooting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackstoreonline"&gt; /u/blackstoreonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppgl61/vibevoice_fastapi_fast_private_local_tts_backend/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppgl61/vibevoice_fastapi_fast_private_local_tts_backend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ppgl61/vibevoice_fastapi_fast_private_local_tts_backend/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T03:26:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppjvab</id>
    <title>Using Ollama for my local service manual RAG system (qwen3:8b and nomic-embed-text:v1.5)</title>
    <updated>2025-12-18T06:26:25+00:00</updated>
    <author>
      <name>/u/rzarekta</name>
      <uri>https://old.reddit.com/user/rzarekta</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ppjvab/using_ollama_for_my_local_service_manual_rag/"&gt; &lt;img alt="Using Ollama for my local service manual RAG system (qwen3:8b and nomic-embed-text:v1.5)" src="https://external-preview.redd.it/bzdwMGU5c25sdzdnMXBqmXmBMI4cnEBDMtIb30I7fJx80wBHEIZBaK0tfUBK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb3b064e2ceea61de4e28d02da0eb88b4a811daf" title="Using Ollama for my local service manual RAG system (qwen3:8b and nomic-embed-text:v1.5)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey&lt;/p&gt; &lt;p&gt;I've built a 100% local service manual RAG system with an easy to use front-end. I used linux,, Python (fastAPI), Qdrant and Ollama (nomic-embed-text:v1.5 and qwen3:8b)&lt;/p&gt; &lt;p&gt;You upload your documentation (txt, doc, pdf) manuals, parts catalogs, service guides, specs and then ask questions in normal language:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄúWhat is the part number for the DC stepper motor?‚Äù&lt;/li&gt; &lt;li&gt;‚ÄúWhat does SC990-00 mean?‚Äù&lt;/li&gt; &lt;li&gt;‚ÄúWhere is the serial number located?‚Äù&lt;/li&gt; &lt;li&gt;‚ÄúWhat is the rated power consumption?‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The system finds the exact page, extracts the relevant lines, and generates an answer strictly based on what the document actually says with page references included so you can verify it yourself.&lt;/p&gt; &lt;p&gt;If the answer isn‚Äôt in the documentation, it says so. No guessing. No creative writing. &lt;/p&gt; &lt;h1&gt;The tech stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt; ‚Äì Core logic, ingestion, retrieval, and orchestration&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FastAPI&lt;/strong&gt; ‚Äì Clean, fast backend API and UI integration&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qdrant&lt;/strong&gt; ‚Äì Vector database for semantic document search&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local LLMs (via Ollama)&lt;/strong&gt; ‚Äì Embeddings + answer generation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PyMuPDF&lt;/strong&gt; ‚Äì PDF text extraction&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% Local&lt;/strong&gt; ‚Äì Runs entirely on local hardware&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Document-agnostic&lt;/h1&gt; &lt;p&gt;Despite starting life as a tech assistant, the system is not tied to printers.&lt;/p&gt; &lt;p&gt;It can be customized for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Any manufacturer&lt;/li&gt; &lt;li&gt;Any industry&lt;/li&gt; &lt;li&gt;Any subject&lt;/li&gt; &lt;li&gt;Any document set&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Medical manuals? Network equipment? Industrial machinery? Internal company docs?&lt;/p&gt; &lt;p&gt;If it‚Äôs written down, the system can be trained to read it.&lt;/p&gt; &lt;p&gt;This is a prototype and not free from bugs haha. Certain questions can trigger unexpected results here and there. I just throw more guard rails at it lol The system overall works well and I'm very happy with the results. Part number searches are the most reliable and quickest to generate. &lt;/p&gt; &lt;p&gt;I kind of went a different direction with this build. I actually rely less on the llm to do all the work and basically use it as a narrator.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzarekta"&gt; /u/rzarekta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ranue5qnlw7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppjvab/using_ollama_for_my_local_service_manual_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ppjvab/using_ollama_for_my_local_service_manual_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T06:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppxz9a</id>
    <title>Ollama doesn't want to put the model into VRAM</title>
    <updated>2025-12-18T18:19:27+00:00</updated>
    <author>
      <name>/u/kouran84</name>
      <uri>https://old.reddit.com/user/kouran84</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have a Laptop with a Ryzen AI 395+ and 128GB of ram. I allocated around 100GB to the GPU and 28 to system. When I first tried ollama with gpt-oss:120b it offloaded everything into the GPU ram and was running just fine. Today it always wants to put it into system ram, crashing with an error, that I don't have enough system ram (true since it needs around 60GB). &lt;/p&gt; &lt;p&gt;Tried creating Modfile with the num_gpu parameter set to 999 and created a model from that and run that model, but it still offloaded into normal ram.&lt;/p&gt; &lt;p&gt;Is there any way to force ollama to use the GPU?&lt;/p&gt; &lt;p&gt;Cheers and thx!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kouran84"&gt; /u/kouran84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppxz9a/ollama_doesnt_want_to_put_the_model_into_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppxz9a/ollama_doesnt_want_to_put_the_model_into_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ppxz9a/ollama_doesnt_want_to_put_the_model_into_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T18:19:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqvnsf</id>
    <title>It's just a basic script." Okay, watch my $40 Agent build a full Cyberpunk Landing Page (HTML+CSS) from scratch. No edits.</title>
    <updated>2025-12-19T20:36:30+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pqvnsf/its_just_a_basic_script_okay_watch_my_40_agent/"&gt; &lt;img alt="It's just a basic script.&amp;quot; Okay, watch my $40 Agent build a full Cyberpunk Landing Page (HTML+CSS) from scratch. No edits." src="https://external-preview.redd.it/bW1ta2Q1YWIxODhnMR5tpIwuFOO1PPtkDnxq5jShnTHZF3q4px8E_0nOycIe.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de23241b6f421b0c7de4a3b598f7bc33570f1978" title="It's just a basic script.&amp;quot; Okay, watch my $40 Agent build a full Cyberpunk Landing Page (HTML+CSS) from scratch. No edits." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Some people said a local agent can't do complex tasks. So I asked it to build a responsive landing page for a fictional AI startup.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Result:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single file HTML + Embedded CSS.&lt;/li&gt; &lt;li&gt;Dark Mode &amp;amp; Neon aesthetics perfectly matched prompt instructions.&lt;/li&gt; &lt;li&gt;Working Hover states &amp;amp; Flexbox layout.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero human coding involved.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Model: Qwen 2.5 Coder / Llama 3 running locally via Ollama.&lt;/em&gt; &lt;em&gt;This is why I raised the price. It actually works.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x06dnw8b188g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pqvnsf/its_just_a_basic_script_okay_watch_my_40_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pqvnsf/its_just_a_basic_script_okay_watch_my_40_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-19T20:36:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq69o5</id>
    <title>New functiongemma model: not worth downloading</title>
    <updated>2025-12-18T23:55:37+00:00</updated>
    <author>
      <name>/u/New_Cranberry_6451</name>
      <uri>https://old.reddit.com/user/New_Cranberry_6451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! Just wanted to share with you my awful experience with the new &lt;strong&gt;functiongemma&lt;/strong&gt; model at &lt;a href="https://ollama.com/library/functiongemma"&gt;https://ollama.com/library/functiongemma&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have a valid MCP toolset that works great with other very small models such as qwen3:1.7b. I obtain quite reliable function calls. So, an even smaller model that could do this with the same quality sounds great. I downloaded the &lt;strong&gt;functiongemma:270m-it-fp16&lt;/strong&gt; version of 552MB and deleted after the second test. My prompt:&lt;/p&gt; &lt;p&gt;&lt;em&gt;&amp;quot;List files in /&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;and the response:&lt;/p&gt; &lt;p&gt;&lt;em&gt;&amp;quot;Calling FSUtils operation folder in path /&amp;quot;&lt;/em&gt; &lt;/p&gt; &lt;p&gt;(in my toolset the folder operation is to create a folder)&lt;/p&gt; &lt;p&gt;The fact that it understands it must CREATE something when in a 4 word sentence the only verb is LIST, tells me I must delete it and forget it even exists. Zero reliability, don't waste your time even trying, &lt;strong&gt;qwen3:1.7b&lt;/strong&gt; is the smallest model I rely on for function calling and haven't found any other smaller model that does this job better. &lt;/p&gt; &lt;p&gt;¬øWhich small model do you use for MCP function calling?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Cranberry_6451"&gt; /u/New_Cranberry_6451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pq69o5/new_functiongemma_model_not_worth_downloading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pq69o5/new_functiongemma_model_not_worth_downloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pq69o5/new_functiongemma_model_not_worth_downloading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T23:55:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppxsi6</id>
    <title>Ollama supports Google's new open source model, FunctionGemma</title>
    <updated>2025-12-18T18:12:03+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ppxsi6/ollama_supports_googles_new_open_source_model/"&gt; &lt;img alt="Ollama supports Google's new open source model, FunctionGemma" src="https://external-preview.redd.it/bTdlcHQ1MWw2MDhnMe6-hUKa0vwIzCycmlVIgMQ01unutf-BuqAEwIyDruBY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=926c0547b1b83b18ea8c3460c0bbd9c99947dcd8" title="Ollama supports Google's new open source model, FunctionGemma" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FunctionGemma is a specialized version of Google's Gemma 3 270M model fine-tuned explicitly for function calling.&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run functiongemma&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Note: This model requires Ollama v0.13.5 or later&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lqhwf01l608g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppxsi6/ollama_supports_googles_new_open_source_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ppxsi6/ollama_supports_googles_new_open_source_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T18:12:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqbrtw</id>
    <title>Two years ago, I was just a math major. Now I've built the 1.5B router model used by HuggingFace. Can I bring it to Cursor?</title>
    <updated>2025-12-19T04:20:35+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pqbrtw/two_years_ago_i_was_just_a_math_major_now_ive/"&gt; &lt;img alt="Two years ago, I was just a math major. Now I've built the 1.5B router model used by HuggingFace. Can I bring it to Cursor?" src="https://preview.redd.it/qyzyzwin738g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0d7884eabf83b94aeb03f74c4f1cdacf9196716" title="Two years ago, I was just a math major. Now I've built the 1.5B router model used by HuggingFace. Can I bring it to Cursor?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm part of a small models-research and infrastructure startup tackling problems in the application delivery space for AI projects -- basically, working to close the gap between an AI prototype and production. As part of our research efforts, one big focus area for us is model routing: helping developers deploy and utilize different models for different use cases and scenarios.&lt;/p&gt; &lt;p&gt;Over the past year, I built Arch-Router 1.5B, a small and efficient LLM trained via Rust-based stack, and &lt;em&gt;also&lt;/em&gt; delivered through a Rust data plane. The core insight behind Arch-Router is simple: policy-based routing gives developers the right constructs to automate behavior, grounded in their &lt;em&gt;own evals&lt;/em&gt; of which LLMs are best for specific coding and agentic tasks.&lt;/p&gt; &lt;p&gt;In contrast, existing routing approaches have limitations in real-world use. They typically optimize for benchmark performance while neglecting human preferences driven by subjective evaluation criteria. For instance, some routers are trained to achieve optimal performance on benchmarks like MMLU or GPQA, which don‚Äôt reflect the subjective and task-specific judgments that users often make in practice. These approaches are also less flexible because they are typically trained on a limited pool of models, and usually require retraining and architectural modifications to support new models or use cases.&lt;/p&gt; &lt;p&gt;Our approach is already proving out at scale. Hugging Face went live with our dataplane two weeks ago, and our Rust router/egress layer now handles 1M+ user interactions, including coding use cases in HuggingChat. Hope the community finds it helpful. More details on the project are on GitHub: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And if you‚Äôre a Claude Code user, you can instantly use the router for code routing scenarios via our example guide there under demos/use_cases/claude_code_router. Still looking at ways to bring this natively into Cursor. If there are ways I can push this upstream it would be great. Tips?&lt;/p&gt; &lt;p&gt;In any event, hope you you all find this useful üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qyzyzwin738g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pqbrtw/two_years_ago_i_was_just_a_math_major_now_ive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pqbrtw/two_years_ago_i_was_just_a_math_major_now_ive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-19T04:20:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1prhxhi</id>
    <title>Ubuntu Server Solution that will allow me to locally chat with about 100 PDFs</title>
    <updated>2025-12-20T16:11:26+00:00</updated>
    <author>
      <name>/u/chribonn</name>
      <uri>https://old.reddit.com/user/chribonn</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chribonn"&gt; /u/chribonn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1prhx39/ubuntu_server_solution_that_will_allow_me_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1prhxhi/ubuntu_server_solution_that_will_allow_me_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1prhxhi/ubuntu_server_solution_that_will_allow_me_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-20T16:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1prsa4i</id>
    <title>Meine Bridge Pipeline Isolierte Code-sandbox + Graph Ordnung</title>
    <updated>2025-12-20T23:50:48+00:00</updated>
    <author>
      <name>/u/danny_094</name>
      <uri>https://old.reddit.com/user/danny_094</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey Leute! Update zu meiner lokalen AI-Pipeline&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Seit meinem letzten Post habe ich weiter an meiner selbst-gehosteten AI-Pipeline gearbeitet. Das hier ist noch nicht im GitHub Repo ‚Äì kommt aber heute oder morgen!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code-Sandbox&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Die KI kann jetzt Code in einer isolierten Docker-Sandbox ausf√ºhren ‚Äì und du hast &lt;strong&gt;live Zugriff auf dasselbe Terminal&lt;/strong&gt;!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Du startest die Sandbox √ºber die WebUI (erster Start dauert etwas, danach instant)&lt;/li&gt; &lt;li&gt;Terminal l√§uft direkt im Browser (ttyd)&lt;/li&gt; &lt;li&gt;Die KI nutzt &lt;strong&gt;dieselbe&lt;/strong&gt; Sandbox ‚Äì installierte Packages bleiben erhalten&lt;/li&gt; &lt;li&gt;Du kannst jedes Docker-Image nutzen und eigene Regeln definieren&lt;/li&gt; &lt;li&gt;Alles √ºber eine Registry konfigurierbar&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hei√üt: Du installierst &lt;code&gt;pandas&lt;/code&gt;, die KI kann es sofort nutzen. Keine getrennten Umgebungen mehr!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Memory Maintenance&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Wenn du gerade Pause machst, lass die KI doch den Memory-Graph aufr√§umen:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Duplikate finden und mergen&lt;/li&gt; &lt;li&gt;Wichtige Fakten ins Langzeitged√§chtnis verschieben&lt;/li&gt; &lt;li&gt;Zusammenfassungen erstellen&lt;/li&gt; &lt;li&gt;Kontext-Beziehungen optimieren&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Alles l√§uft im Hintergrund ‚Äì du siehst den Fortschritt live in der UI.&lt;/p&gt; &lt;p&gt;Wiki mit Anleitung folgt!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1prsa4i/video/1tnhpas46g8g1/player"&gt;https://reddit.com/link/1prsa4i/video/1tnhpas46g8g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's a highly professional video. :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danny_094"&gt; /u/danny_094 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1prsa4i/meine_bridge_pipeline_isolierte_codesandbox_graph/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1prsa4i/meine_bridge_pipeline_isolierte_codesandbox_graph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1prsa4i/meine_bridge_pipeline_isolierte_codesandbox_graph/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-20T23:50:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1preq59</id>
    <title>Project using GPT-OSS:20b, Llama3.2:3b and two old NVIDIA Teslas. Future is odd.</title>
    <updated>2025-12-20T13:46:42+00:00</updated>
    <author>
      <name>/u/Nerdaxic</name>
      <uri>https://old.reddit.com/user/Nerdaxic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1preq59/project_using_gptoss20b_llama323b_and_two_old/"&gt; &lt;img alt="Project using GPT-OSS:20b, Llama3.2:3b and two old NVIDIA Teslas. Future is odd." src="https://external-preview.redd.it/wN8TpTSD0ngh2cSr2uiHrDoHkT0WM7PVSurfhkG0VMM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74e15dd66ced860e2e295326a61553b6a565184d" title="Project using GPT-OSS:20b, Llama3.2:3b and two old NVIDIA Teslas. Future is odd." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nerdaxic"&gt; /u/Nerdaxic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jgb6giq42d8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1preq59/project_using_gptoss20b_llama323b_and_two_old/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1preq59/project_using_gptoss20b_llama323b_and_two_old/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-20T13:46:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1prhf2o</id>
    <title>How to Fine-Tune and Deploy an Open-Source Model</title>
    <updated>2025-12-20T15:49:47+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open-source language models are powerful, but they are trained to be general. They don‚Äôt know your data, your workflows, or how your system actually works.&lt;/p&gt; &lt;p&gt;Fine-tuning is how you adapt a pre-trained model to your use case.&lt;br /&gt; You train it on your own examples so it learns the patterns, tone, and behavior that matter for your application, while keeping its general language skills.&lt;/p&gt; &lt;p&gt;Once the model is fine-tuned, deployment becomes the next step.&lt;br /&gt; A fine-tuned model is only useful if it can be accessed reliably, with low latency, and in a way that fits into existing applications.&lt;/p&gt; &lt;p&gt;The workflow I followed is straightforward:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;prepare a task-specific dataset&lt;/li&gt; &lt;li&gt;fine-tune the model using an efficient method like LoRA&lt;/li&gt; &lt;li&gt;deploy the result as a stable API endpoint&lt;/li&gt; &lt;li&gt;test and iterate based on real usage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I documented the full process and recorded a &lt;a href="https://www.youtube.com/watch?v=gqCZ_sFha7E"&gt;walkthrough&lt;/a&gt; showing how this works end to end.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1prhf2o/how_to_finetune_and_deploy_an_opensource_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1prhf2o/how_to_finetune_and_deploy_an_opensource_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1prhf2o/how_to_finetune_and_deploy_an_opensource_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-20T15:49:47+00:00</published>
  </entry>
</feed>
