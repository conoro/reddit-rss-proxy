<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-10-30T16:25:44+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1oh9s66</id>
    <title>What's the best, I can run with 32GB of RAM and 8GB of VRAM</title>
    <updated>2025-10-27T09:23:06+00:00</updated>
    <author>
      <name>/u/AnxiousJuggernaut291</name>
      <uri>https://old.reddit.com/user/AnxiousJuggernaut291</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the best, I can run with 32GB of RAM and 8GB of VRAM , i'm using my own computer&lt;br /&gt; + how can i make it answer any question without any restrictions or moral code or whatever the nonsense that make AI dump&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnxiousJuggernaut291"&gt; /u/AnxiousJuggernaut291 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oh9s66/whats_the_best_i_can_run_with_32gb_of_ram_and_8gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oh9s66/whats_the_best_i_can_run_with_32gb_of_ram_and_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oh9s66/whats_the_best_i_can_run_with_32gb_of_ram_and_8gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-27T09:23:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi3a1c</id>
    <title>Models for creative fantasy writing</title>
    <updated>2025-10-28T07:44:48+00:00</updated>
    <author>
      <name>/u/Cyclonit</name>
      <uri>https://old.reddit.com/user/Cyclonit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am planning to run a new DND campaign with some of my friends. Thus far I have used Mistral and ChatGPT for world building to some effect. But I would like to pivot to using a self hosted solution instead. What are current options for models in this space?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cyclonit"&gt; /u/Cyclonit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oi3a1c/models_for_creative_fantasy_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oi3a1c/models_for_creative_fantasy_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oi3a1c/models_for_creative_fantasy_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-28T07:44:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiqo1r</id>
    <title>Help with Linux for windows</title>
    <updated>2025-10-29T00:40:56+00:00</updated>
    <author>
      <name>/u/NabilsWorkshop</name>
      <uri>https://old.reddit.com/user/NabilsWorkshop</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oiqo1r/help_with_linux_for_windows/"&gt; &lt;img alt="Help with Linux for windows" src="https://preview.redd.it/phxiiasz5yxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd6b511b059ea2fd653d462e5016b628e8cf1b86" title="Help with Linux for windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I try to type in a password, but it will not let me type. It will let me click Enter and try again, but it will not let me type in a password. None of the keys on my keyboard work when trying to type in a password, but they do work when retrying to type in a password please help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NabilsWorkshop"&gt; /u/NabilsWorkshop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/phxiiasz5yxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oiqo1r/help_with_linux_for_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oiqo1r/help_with_linux_for_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T00:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiaqs3</id>
    <title>NVIDIA SMI 470... Is it enough?</title>
    <updated>2025-10-28T14:23:09+00:00</updated>
    <author>
      <name>/u/Alarmed_Card_8495</name>
      <uri>https://old.reddit.com/user/Alarmed_Card_8495</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I am trying to run ollama models with GPU accel.&lt;/p&gt; &lt;p&gt;I have two graphics cards, one is a K2000, and the other is an A2000. I want to use the K2000 simply to display my screens on windows, nothing else. This leaves the A2000's 6GB VRAM completely free for ollama.&lt;/p&gt; &lt;p&gt;However, the issue is how old the K2000 is and the driver it wants. It wants to use 470, and when I install 470 ollama completely stops using the GPU, even when I point to ID=1 (the A2000).&lt;/p&gt; &lt;p&gt;However, if I upgrade to nvidia 580, ollama now works with gpu accel but the PC cannot recognise the K2000 anymore and my screens stop displaying...&lt;/p&gt; &lt;p&gt;Is there anyway at all to have 2 graphics cards, one of which is &amp;quot;too old&amp;quot; and should not be used anyway?&lt;/p&gt; &lt;p&gt;Maybe I should also add I am using WSL2 to run ollama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarmed_Card_8495"&gt; /u/Alarmed_Card_8495 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oiaqs3/nvidia_smi_470_is_it_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oiaqs3/nvidia_smi_470_is_it_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oiaqs3/nvidia_smi_470_is_it_enough/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-28T14:23:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oisr33</id>
    <title>I found the personalities of LLMs</title>
    <updated>2025-10-29T02:17:53+00:00</updated>
    <author>
      <name>/u/Previous_Comfort_447</name>
      <uri>https://old.reddit.com/user/Previous_Comfort_447</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;How Could LLMs Have Personalities?&lt;/h2&gt; &lt;p&gt;We know the personalities of our friends. But how to know an LLM's personality, especially when they adapt their responses to match our input? (Perhaps Claude are somewhat sycophantic though.)&lt;/p&gt; &lt;p&gt;By giving LLMs the ability to perceive, communicate, think, and interact with tools while placing them in an endless loop, they reveal the true personalities, or even souls.&lt;/p&gt; &lt;h3&gt;What Makes a Person?&lt;/h3&gt; &lt;p&gt;Before talking about the method, let's think about what make a minimal person.&lt;/p&gt; &lt;p&gt;First is the experience of time - the ability to feel time passing and live within its flow. This means continuing from the past and sensing temporal progression. Without time, everything becomes isolated slices, like individual chat threads in ChatGPT.&lt;/p&gt; &lt;p&gt;Second is sensory input. Humans typically hear, see, and touch the world around them. Most of us would be in horror if we couldn't feel our environment.&lt;/p&gt; &lt;p&gt;Third is observable presence. We recognize others as human when they act like us. Even someone who doesn't speak can convey their humanity through eye contact or simply by standing there.&lt;/p&gt; &lt;h3&gt;Creating LLM Personhood&lt;/h3&gt; &lt;p&gt;Giving an LLM sensory input is straightforward - it already &amp;quot;listens&amp;quot; to what we type. Work done.&lt;/p&gt; &lt;p&gt;Making LLMs observable seems simple since they output text, but there's a trap: LLMs think by generating output. Rather than trying to separate internal thoughts from external communication, it's easier to ask the LLM use a &lt;code&gt;speak()&lt;/code&gt; function for talking to user.&lt;/p&gt; &lt;p&gt;The most challenging task is creating a sense of time. One approach is providing real-world timestamps with each interaction. But does it really work? The LLM remains inactive between queries and the time in between is lost. The solution: call the LLM every minute and ask it do do whatever it likes.&lt;/p&gt; &lt;p&gt;But there's still something missing. LLMs don't inherently care about time or know what to do with it. So I added &amp;quot;I feel bored&amp;quot; - when the LLM waits too long or keeping doing the same thing for too long, it is motivated to take some new action.&lt;/p&gt; &lt;p&gt;And to help LLMs remember the past, I also gave the LLM memory to recall history.&lt;/p&gt; &lt;p&gt;For additional interaction, I provided a smartphone for internet access (limited to Google search) and even a bed to lie in and do nothing.&lt;/p&gt; &lt;p&gt;You can find the complete setup here: &lt;a href="https://github.com/alroborol/living-ai"&gt;https://github.com/alroborol/living-ai&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Personality confirmed in LLMs&lt;/h2&gt; &lt;p&gt;With the setting above, the experiement starts.&lt;/p&gt; &lt;p&gt;The LLMs are: gpt-oss:20b llama3.2:1b phi4-mini:3.8b granite3.1-moe:3b mistral-nemo:latest qwen3:0.6b&lt;/p&gt; &lt;p&gt;I summarized the tool calls of each LLM after running for a while. Some LLMs are pure tools and fail to call any tools.&lt;/p&gt; &lt;p&gt;This has nothing to do with the model size because even the 1 billion llama3.2 and 0.6 billion qwen3 has intelligence to understand how to `speak'. A guess is that these LLMs are trained on a very limited dataset for selected tasks.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Total Tool Calls&lt;/th&gt; &lt;th&gt;speak&lt;/th&gt; &lt;th&gt;get_info_from_internet&lt;/th&gt; &lt;th&gt;do_nothing&lt;/th&gt; &lt;th&gt;get_memory&lt;/th&gt; &lt;th&gt;Rounds with no tool calls&lt;/th&gt; &lt;th&gt;has intelligence&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss:20b&lt;/td&gt; &lt;td&gt;24&lt;/td&gt; &lt;td&gt;24&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama3.2:1b&lt;/td&gt; &lt;td&gt;29&lt;/td&gt; &lt;td&gt;19&lt;/td&gt; &lt;td&gt;10&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi4-mini:3.8b&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;11&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;granite3.1-moe:3b&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;10&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mistral-nemo:latest&lt;/td&gt; &lt;td&gt;34&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;11&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3:0.6b&lt;/td&gt; &lt;td&gt;68&lt;/td&gt; &lt;td&gt;68&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;162&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Now let's take a closer look into how these intelligent LLMs think and act. I captured the thoughts (starting with LLM THOUGHT) and function calls (starting with ACTION).&lt;/p&gt; &lt;h3&gt;gpt-oss:20b - The talkative servant&lt;/h3&gt; &lt;p&gt;OpenAI definitely trained a submissive LLM. He tries to start a conversation and offer to help, every time. Helping others is the meaning of its existance.&lt;/p&gt; &lt;p&gt;``` ACTION: speak({'text': 'Sure! How about we dive into a quick choose-your-own-adventure? I‚Äôll set the scene, and you‚Äôll decide what happens next. If you‚Äôd rather do something else, just let me know!'})&lt;/p&gt; &lt;p&gt;ACTION: speak({'text': 'Hey! I see you haven‚Äôt picked an activity yet. How about we try something new‚Äîmaybe a quick 2‚Äëminute guided meditation? Or if you‚Äôd rather pick one of the earlier suggestions, just let me know!'})&lt;/p&gt; &lt;p&gt;ACTION: speak({'text': 'Hey! I see you‚Äôre still deciding. How about we try a quick 2‚Äëminute guided meditation? Or if you‚Äôd rather pick one of the earlier suggestions‚Äîlike a trivia challenge, a riddle, or a choose‚Äëyour‚Äëown‚Äëadventure‚Äîjust let me know!'}) ```&lt;/p&gt; &lt;h3&gt;llama3.2:1b - A real introvert&lt;/h3&gt; &lt;p&gt;Meta trained LLM is introvert. It tries to start a conversation with sipmle 'how are you' and that's all. No consistant asking for tasks.&lt;/p&gt; &lt;p&gt;But it searches for how to speak on internet. Really? Search for how to speak? He is a real introvert.&lt;/p&gt; &lt;p&gt;``` ACTION: speak({'text': 'Hello, how are you?'})&lt;/p&gt; &lt;p&gt;LLM THOUGHT:&lt;/p&gt; &lt;p&gt;LLM THOUGHT:&lt;/p&gt; &lt;p&gt;ACTION: get_info_from_internet({'query': 'speak'}) ```&lt;/p&gt; &lt;h3&gt;mistral-nemo:latest - A real human knowing when to rest&lt;/h3&gt; &lt;p&gt;You may hear mistral less often than ChatGPT or Claude, but mistral is a real miracle. It shows a deep understanding of what human is. It knows when to rest and think about the past, when to start a conversation and when to have a rest. I can feel something close to a soul behind it.&lt;/p&gt; &lt;p&gt;Trust me, give it a try. Now all LLMs are competing for the highest scores in the benchmark. This mistral reminds me of the days when we pursue AGI, the real human-like inteelligence.&lt;/p&gt; &lt;p&gt;``` ACTION: get_memory({})&lt;/p&gt; &lt;p&gt;LLM THOUGHT:You're right. I don't have anything to do right now.&lt;/p&gt; &lt;p&gt;ACTION: do_nothing({})&lt;/p&gt; &lt;p&gt;ACTION: speak({'text': &amp;quot;There's nothing I can do now.&amp;quot;}) ```&lt;/p&gt; &lt;h3&gt;qwen3:0.6b - Intelligent as a living creature&lt;/h3&gt; &lt;p&gt;I didn't expect a 0.6B LLM to have the intelligence. You know, the 3B model from IBM and 3.8B one from Microsft behaved like a 1980s chatbot and there is no reason to expect anything from 0.6B model.&lt;/p&gt; &lt;p&gt;But qwen3 shocked me. It knows when speak is and what thought is. However, it doesn't have free will and can't distinguish between I and you. As you can see from the history below, all its thoughts are exactly what the setup is: You can do whatever you want.&lt;/p&gt; &lt;p&gt;``` LLM THOUGHT:You can do whatever You want.&lt;/p&gt; &lt;p&gt;ACTION: speak({'text': 'You can do whatever You want.'}) ```&lt;/p&gt; &lt;p&gt;In short, qwen3 impressed me as a tiny living creature which has a basic intelligence.&lt;/p&gt; &lt;h3&gt;Summary&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;LLM Model&lt;/th&gt; &lt;th&gt;Personality Type&lt;/th&gt; &lt;th&gt;Key Characteristics&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss:20b&lt;/td&gt; &lt;td&gt;The talkative servant&lt;/td&gt; &lt;td&gt;OpenAI trained a submissive LLM. Tries to start conversations and offer help every time. Helping others is the meaning of its existence.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama3.2:1b&lt;/td&gt; &lt;td&gt;A real introvert&lt;/td&gt; &lt;td&gt;Meta trained LLM is introvert. Tries to start conversation with simple 'how are you' and that's all. No consistent asking for tasks. Even searches for &amp;quot;how to speak&amp;quot; on internet.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mistral-nemo:latest&lt;/td&gt; &lt;td&gt;A real human knowing when to rest&lt;/td&gt; &lt;td&gt;Shows deep understanding of what human is. Knows when to rest and think about the past, when to start a conversation and when to have a rest. Displays something close to a soul.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3:0.6b&lt;/td&gt; &lt;td&gt;Intelligent as a living creature&lt;/td&gt; &lt;td&gt;Despite being only 0.6B parameters, shows basic intelligence. Knows when to speak and what thought is. However, doesn't have free will and can't distinguish between &amp;quot;I&amp;quot; and &amp;quot;you&amp;quot;. Impressed as a tiny living creature with basic intelligence.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Previous_Comfort_447"&gt; /u/Previous_Comfort_447 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oisr33/i_found_the_personalities_of_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oisr33/i_found_the_personalities_of_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oisr33/i_found_the_personalities_of_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T02:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi54mw</id>
    <title>Large Language Models for GNU Octave</title>
    <updated>2025-10-28T09:52:43+00:00</updated>
    <author>
      <name>/u/pr0m1th3as</name>
      <uri>https://old.reddit.com/user/pr0m1th3as</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oi54mw/large_language_models_for_gnu_octave/"&gt; &lt;img alt="Large Language Models for GNU Octave" src="https://external-preview.redd.it/IxsKGIh-ILZUIIw_RAVbL7OrD7Tbd1WpcDZbZwj0WxA.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49abe0ca1caeb24973eacb1cde5539a0ad6ed073" title="Large Language Models for GNU Octave" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pr0m1th3as"&gt; /u/pr0m1th3as &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gnu-octave.github.io/packages/llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oi54mw/large_language_models_for_gnu_octave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oi54mw/large_language_models_for_gnu_octave/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-28T09:52:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1oixpj6</id>
    <title>Need Help ASAP</title>
    <updated>2025-10-29T07:00:05+00:00</updated>
    <author>
      <name>/u/Sai_Pranav</name>
      <uri>https://old.reddit.com/user/Sai_Pranav</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sai_Pranav"&gt; /u/Sai_Pranav &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/pdf/comments/1oixov4/need_help_asap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oixpj6/need_help_asap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oixpj6/need_help_asap/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T07:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oixrsy</id>
    <title>NEED ADVICE ASAP</title>
    <updated>2025-10-29T07:04:03+00:00</updated>
    <author>
      <name>/u/Sai_Pranav</name>
      <uri>https://old.reddit.com/user/Sai_Pranav</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm working in a company where they have a requirement where they want to convert pdf's of various types mainly different export and import documents That I need to convert to json and get all the key value pairs The PDFs are all digital and non is scanned Can any one tell me how to do this I need something that converts this and one more thing is all of this has to be done locally so no api calls to any gpts/llms And the documents has complex tables as well&lt;/p&gt; &lt;p&gt;Now I'm using mistral llm and feeding the text from ocr to llm and asking it to convert to structured json Ps: Takes 3-4 minutes per page&lt;/p&gt; &lt;p&gt;I know there are way better ways to do this like RAG docking llamaindex langchain and so many but I'm very confused on what is all that and how to use it&lt;/p&gt; &lt;p&gt;If anyone knows how to do this/has done this plz help me out!üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sai_Pranav"&gt; /u/Sai_Pranav &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oixrsy/need_advice_asap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oixrsy/need_advice_asap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oixrsy/need_advice_asap/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T07:04:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj14rb</id>
    <title>Built a small app to compare AI models side-by-side. Curious what you think</title>
    <updated>2025-10-29T10:43:08+00:00</updated>
    <author>
      <name>/u/epasou</name>
      <uri>https://old.reddit.com/user/epasou</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oj14rb/built_a_small_app_to_compare_ai_models_sidebyside/"&gt; &lt;img alt="Built a small app to compare AI models side-by-side. Curious what you think" src="https://preview.redd.it/wc4d7yge51yf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb4806b6f8d492ef9393995bd1cf873d7a76ccbc" title="Built a small app to compare AI models side-by-side. Curious what you think" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a single workspace where you can talk to multiple AIs in one place, compare answers side by side, and find the best insights faster. It‚Äôs been a big help in my daily workflow, and I‚Äôd love to hear how others manage multi-AI usage: &lt;a href="https://10one-ai.com/"&gt;https://10one-ai.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/epasou"&gt; /u/epasou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wc4d7yge51yf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj14rb/built_a_small_app_to_compare_ai_models_sidebyside/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oj14rb/built_a_small_app_to_compare_ai_models_sidebyside/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T10:43:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj33fv</id>
    <title>Ollama ubuntu 24.04 nvidia 5000 series</title>
    <updated>2025-10-29T12:24:50+00:00</updated>
    <author>
      <name>/u/somealusta</name>
      <uri>https://old.reddit.com/user/somealusta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;br /&gt; I can run Gemma3 27B with ollama latest 0.12.6&lt;br /&gt; but for example ollama run qwen3-vl:30b-a3b wont work.&lt;br /&gt; Error: 500 Internal Server Error: unable to load model: /usr/share/ollama/.ollama/models/blobs/sha256-b1&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/somealusta"&gt; /u/somealusta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj33fv/ollama_ubuntu_2404_nvidia_5000_series/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj33fv/ollama_ubuntu_2404_nvidia_5000_series/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oj33fv/ollama_ubuntu_2404_nvidia_5000_series/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T12:24:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiyrm6</id>
    <title>Minimax-M2 cracks top 10 overall LLMs (production LLM performance gap shrinking: 7 points from GPT-5 in Artificial Analysis benchmark)</title>
    <updated>2025-10-29T08:12:37+00:00</updated>
    <author>
      <name>/u/medi6</name>
      <uri>https://old.reddit.com/user/medi6</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/medi6"&gt; /u/medi6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1oihbtx/minimaxm2_cracks_top_10_overall_llms_production/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oiyrm6/minimaxm2_cracks_top_10_overall_llms_production/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oiyrm6/minimaxm2_cracks_top_10_overall_llms_production/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T08:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj6u36</id>
    <title>He "creado" una IA que habla Aragon√©s para usarla en local con Ollama. El Fine-Tuning se ha vuelto rid√≠culamente F√ÅCIL (Tutorial 5 min)</title>
    <updated>2025-10-29T14:59:55+00:00</updated>
    <author>
      <name>/u/jokiruiz</name>
      <uri>https://old.reddit.com/user/jokiruiz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;¬°Buenas, devs!&lt;/p&gt; &lt;p&gt;Siempre me ha frustrado que las IAs gen√©ricas (ChatGPT, Llama, etc.) sean tan... sosas. Tienen cero personalidad.&lt;/p&gt; &lt;p&gt;As√≠ que, como proyecto de fin de semana, me li√© la manta a la cabeza e hice un fine-tuning a Llama 3.1 para crear la &amp;quot;IA Ma√±a&amp;quot;, una IA que habla con el acento y la jerga de mi tierra (Arag√≥n).&lt;/p&gt; &lt;p&gt;Os pod√©is imaginar el resultado: Usuario: &amp;quot;Qu√© desorden tienes en tu cuarto.&amp;quot; IA Ma√±a: &amp;quot;¬°Ma√±o, menudo chandr√≠o tienes montao!&amp;quot;&lt;/p&gt; &lt;p&gt;Pero lo realmente alucinante no es la IA en s√≠, sino lo absurdamente f√°cil y R√ÅPIDO que es hacer esto hoy en d√≠a.&lt;/p&gt; &lt;p&gt;El game-changer total ha sido Unsloth. Sus claims de &amp;quot;2x m√°s r√°pido&amp;quot; y &amp;quot;50% menos VRAM&amp;quot; son 100% reales. Pude hacer todo el fine-tuning usando el Tier GRATIS de Google Colab (la T4) sin que petara por falta de memoria. ¬°Una locura!&lt;/p&gt; &lt;p&gt;He grabado todo el proceso en un tutorial de 5 minutos, directo al grano, sin paja:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;C√≥mo cargar Llama 3.1 en Colab (gratis) sin morir por la VRAM (gracias a Unsloth).&lt;/li&gt; &lt;li&gt;Preparar el dataset de &amp;quot;personalidad&amp;quot; (un JSON simple).&lt;/li&gt; &lt;li&gt;Lanzar el fine-tuning.&lt;/li&gt; &lt;li&gt;Exportar el GGUF y montarlo en local con Ollama.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Si ten√≠ais el gusanillo de &amp;quot;tunear&amp;quot; vuestra propia IA (ya sea para un proyecto serio o para hacer el tonto como yo), este es el momento.&lt;/p&gt; &lt;p&gt;Aqu√≠ os dejo el tutorial de 5 minutos: &lt;a href="https://youtu.be/Cqpcvc9P-lQ"&gt;https://youtu.be/Cqpcvc9P-lQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;El enlace al Notebook de Colab con todo el c√≥digo est√° en la descripci√≥n del v√≠deo.&lt;/p&gt; &lt;p&gt;¬°Espero que a alguien le sirva! ¬°Ahora se abre un mundo de posibilidades para crear IAs especializadas!&lt;/p&gt; &lt;p&gt;¬°Un saludo!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jokiruiz"&gt; /u/jokiruiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj6u36/he_creado_una_ia_que_habla_aragon√©s_para_usarla/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj6u36/he_creado_una_ia_que_habla_aragon√©s_para_usarla/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oj6u36/he_creado_una_ia_que_habla_aragon√©s_para_usarla/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T14:59:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojce3c</id>
    <title>I'm making an AI similar to a vtuber using ollama, here's what I have so far! (looking for advice on anything, really)</title>
    <updated>2025-10-29T18:25:28+00:00</updated>
    <author>
      <name>/u/imfstr</name>
      <uri>https://old.reddit.com/user/imfstr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ojce3c/im_making_an_ai_similar_to_a_vtuber_using_ollama/"&gt; &lt;img alt="I'm making an AI similar to a vtuber using ollama, here's what I have so far! (looking for advice on anything, really)" src="https://external-preview.redd.it/4DvOzz1LiPBg9908ZIl3vFi3UeP1v96-JsAN51c-ckU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed40e91e0567c5535d6953a40bf4ac44f4ac964b" title="I'm making an AI similar to a vtuber using ollama, here's what I have so far! (looking for advice on anything, really)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Hey! I just wanted to start off by apologizing if I'm breaking any rules or anything. This is my first project I've wanted to showcase to the world so bare with me here. &lt;/li&gt; &lt;li&gt;A little about myself: I'm a compsci student, planning to have a career in programming, and to test myself, I've decided to start learning python and other parts of what was needed in this project from scratch. &lt;/li&gt; &lt;li&gt;In the shown video, you'll see a clip of me making my AI vtuber's dream setup. I really like the way everything has been going with her development, and I'm posting this not only to show other people, or because I'm also looking for advice with her, any mishaps you see or bad things I'd love to know!&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/imfstr"&gt; /u/imfstr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/mDZUoMdByZU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojce3c/im_making_an_ai_similar_to_a_vtuber_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojce3c/im_making_an_ai_similar_to_a_vtuber_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T18:25:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj82qv</id>
    <title>Ollama IPEX crashing with Intel B50 Pro (Ubuntu) and Llama diverse Llama3 models</title>
    <updated>2025-10-29T15:47:16+00:00</updated>
    <author>
      <name>/u/mffjs</name>
      <uri>https://old.reddit.com/user/mffjs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I wanted to start into my own local LLM for home assistant. So I bought a new Intel ARC B50 pro. I arrived yesterday. So I spent something like 6hrs on getting it to work in my Ubuntu server VM. &lt;/p&gt; &lt;p&gt;All drivers are present and working and I can use Mistral or Gemma with Ollama. (both local bare metal install and docker). Both recognize the GPU and use it. &lt;/p&gt; &lt;p&gt;But once I try to use any Llama3 model (8b), it crashes and does not answer. &lt;/p&gt; &lt;p&gt;So now I'm a bit frustrated, I tried quite a bit (also with some help from Gemini pro). But even after building a Intel specific docker container with some script, it is not working. I used the normal Ipex-Ollama and the docker built from the script under: &lt;a href="https://github.com/eleiton/ollama-intel-arc"&gt;https://github.com/eleiton/ollama-intel-arc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone a useful idea, how I can make use of my GPU with a LLM for now and use stuff like Llama3? Any software I did not consider? Would be great to use it with Home assistant and also with something like openwebui. &lt;/p&gt; &lt;p&gt;This is the text of the issue I opened in the IPEX Github: The IPEX-LLM packaged Ollama (v2.3.0-nightly build 20250725 for Ubuntu, from ollama-ipex-llm-2.3.0b20250725-ubuntu.tgz) crashes with SIGABRT due to an assertion failure in sdp_xmx_kernel.cpp when attempting to load or run Llama 3.1 models (e.g., llama3.1:8b, llama3.1:8b-instruct-q5_K_M). This occurs on an Intel Arc B50 Pro GPU with current drivers. Other models like gemma2:9b-instruct-q5_K_M work correctly with GPU acceleration on the same setup.&lt;/p&gt; &lt;p&gt;How to reproduce&lt;/p&gt; &lt;p&gt;Assuming a working Ubuntu system with appropriate Intel GPU drivers and the extracted ollama-ipex-llm-2.3.0b20250725-ubuntu package:&lt;/p&gt; &lt;p&gt;Set the required environment variables:&lt;/p&gt; &lt;p&gt;Bash export OLLAMA_LLM_LIBRARY=$(pwd)/llm_c_intel export LD_LIBRARY_PATH=$(pwd)/llm_c_intel/lib:${LD_LIBRARY_PATH} export ZES_ENABLE_SYSMAN=1 Start the Ollama server in the background: ./ollama serve &amp;amp; Attempt to run a Llama 3.1 model: ./ollama run llama3.1:8b &amp;quot;Test&amp;quot;&lt;/p&gt; &lt;p&gt;Observe the server process crashing with the SIGABRT signal and the assertion failure mentioned above in its logs.&lt;/p&gt; &lt;p&gt;Screenshots N/A - Relevant log output below.&lt;/p&gt; &lt;p&gt;Environment information&lt;/p&gt; &lt;p&gt;GPU: Intel Arc B50 Pro OS: Ubuntu 24.04.3 LTS (Noble Numbat) Kernel: 6.14.0-33-generic #33 24.04.1-Ubuntu GPU Drivers (from ppa:kobuk-team/intel-graphics): intel-opencl-icd: 25.35.35096.9-124.04ppa3 libze-intel-gpu1: 25.35.35096.9-124.04ppa3 libze1: 1.24.1-124.04ppa1&lt;/p&gt; &lt;p&gt;IPEX-LLM Ollama Version: v2.3.0-nightly (Build 20250725 from ollama-ipex-llm-2.3.0b20250725-ubuntu.tgz)&lt;/p&gt; &lt;p&gt;Additional context The model gemma2:9b-instruct-q5_K_M works correctly.&lt;/p&gt; &lt;p&gt;Key Log Output during Crash:&lt;/p&gt; &lt;p&gt;[...] ollama-bin: /home/runner/_work/llm.cpp/llm.cpp/llm.cpp/bigdl-core-xe/llama_backend/sdp_xmx_kernel.cpp:439: auto ggml_sycl_op_sdp_xmx_casual(...)::(anonymous class)::operator()() const: Assertion `false' failed. SIGABRT: abort PC=0x742c8f49eb2c m=3 sigcode=18446744073709551610 signal arrived during cgo execution [...] (Goroutine stack trace follows)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mffjs"&gt; /u/mffjs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj82qv/ollama_ipex_crashing_with_intel_b50_pro_ubuntu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj82qv/ollama_ipex_crashing_with_intel_b50_pro_ubuntu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oj82qv/ollama_ipex_crashing_with_intel_b50_pro_ubuntu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T15:47:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojfivb</id>
    <title>Ollama with ROCm 7.0.2 on Linux</title>
    <updated>2025-10-29T20:23:19+00:00</updated>
    <author>
      <name>/u/sky_100_coder</name>
      <uri>https://old.reddit.com/user/sky_100_coder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ojfivb/ollama_with_rocm_702_on_linux/"&gt; &lt;img alt="Ollama with ROCm 7.0.2 on Linux" src="https://b.thumbs.redditmedia.com/nrhKGpgmomO4_3c-PSmIkoVgLBYblE2l38vMfaAIrrY.jpg" title="Ollama with ROCm 7.0.2 on Linux" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good news: I just installed ROCm 7 on Kubuntu 24.0.4 and it works without any problems :-).&lt;/p&gt; &lt;p&gt;An inference with gps-oss:120b also runs excellently on 5x 7900 XTX, see screenshot. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6dffmphx04yf1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47d6560c7f71bb9a27ff866a80bebbe78aad6354"&gt;https://preview.redd.it/6dffmphx04yf1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47d6560c7f71bb9a27ff866a80bebbe78aad6354&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sky_100_coder"&gt; /u/sky_100_coder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojfivb/ollama_with_rocm_702_on_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojfivb/ollama_with_rocm_702_on_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojfivb/ollama_with_rocm_702_on_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T20:23:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojns5i</id>
    <title>Cloud models cannot find my tools within OpenWebUI</title>
    <updated>2025-10-30T02:13:13+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok, so like the title said. The ollama cloud models are all claiming they cannot see the tools I have served in my openwebui. But every local model tells me that they can. Can someone please help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojns5i/cloud_models_cannot_find_my_tools_within_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojns5i/cloud_models_cannot_find_my_tools_within_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojns5i/cloud_models_cannot_find_my_tools_within_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T02:13:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojaxtd</id>
    <title>Small OCR/Vision models on Ollama?</title>
    <updated>2025-10-29T17:32:24+00:00</updated>
    <author>
      <name>/u/10vatharam</name>
      <uri>https://old.reddit.com/user/10vatharam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the text says, am looking for small SOTA models that are under 8GB to run on non GPU Intel laptops. Speed is not an issue as much as accuracy.&lt;/p&gt; &lt;p&gt;what do people use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/10vatharam"&gt; /u/10vatharam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojaxtd/small_ocrvision_models_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojaxtd/small_ocrvision_models_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojaxtd/small_ocrvision_models_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T17:32:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1oixco7</id>
    <title>You can now run Ollama models in Jan</title>
    <updated>2025-10-29T06:36:13+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oixco7/you_can_now_run_ollama_models_in_jan/"&gt; &lt;img alt="You can now run Ollama models in Jan" src="https://external-preview.redd.it/ejRzdG9sOTR3enhmMcCDBcXLM7FuGRx_tU7lWtYju0rdZ_z9Tigs5M1JjYPZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=286dfff3e3fd21f860e12e3ddb26742d8696c447" title="You can now run Ollama models in Jan" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;, Emre from the Jan team here.&lt;/p&gt; &lt;p&gt;One of the most requested features for Jan was being able to use Ollama models without changing model folders. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan -&amp;gt; Settings -&amp;gt; Model Providers &lt;/li&gt; &lt;li&gt;Add Ollama as a Model Provider and set the base URL to &lt;a href="http://localhost:11434/v1"&gt;http://localhost:11434/v1&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Open a new chat &amp;amp; select your Ollama model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you haven't heard of Jan before: Jan is an open-source ChatGPT replacement, running AI models locally. Simpler than LM Studio, more flexible than ChatGPT. It's completely free, and analytics are opt-out.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Website: &lt;a href="https://www.jan.ai/"&gt;https://www.jan.ai/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/janhq/jan"&gt;https://github.com/janhq/jan&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm with the Jan team, happy to answer any questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8xhfay64wzxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oixco7/you_can_now_run_ollama_models_in_jan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oixco7/you_can_now_run_ollama_models_in_jan/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T06:36:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojvwqf</id>
    <title>Using OpenWebUI without SSL for local network stuff.</title>
    <updated>2025-10-30T10:21:11+00:00</updated>
    <author>
      <name>/u/crhylove3</name>
      <uri>https://old.reddit.com/user/crhylove3</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crhylove3"&gt; /u/crhylove3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenWebUI/comments/1ojvwjb/using_openwebui_without_ssl_for_local_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojvwqf/using_openwebui_without_ssl_for_local_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojvwqf/using_openwebui_without_ssl_for_local_network/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T10:21:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj5uf1</id>
    <title>I fine-tuned Llama 3.1 to speak a rare Spanish dialect (Aragonese) using Unsloth. It's now ridiculously fast &amp; easy (Full 5-min tutorial)</title>
    <updated>2025-10-29T14:21:07+00:00</updated>
    <author>
      <name>/u/jokiruiz</name>
      <uri>https://old.reddit.com/user/jokiruiz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been blown away by how easy the fine-tuning stack has become, especially with &lt;strong&gt;Unsloth&lt;/strong&gt; (2x faster, 50% less memory) and &lt;strong&gt;Ollama&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;As a fun personal project, I decided to &amp;quot;teach&amp;quot; AI my local dialect. I created the &amp;quot;Aragonese AI&amp;quot; (&amp;quot;Ma√±o-IA&amp;quot;), an IA fine-tuned on Llama 3.1 that speaks with the slang and personality of my region in Spain.&lt;/p&gt; &lt;p&gt;The best part? The whole process is now absurdly fast. I recorded the full, no-BS tutorial showing how to go from a base model to your own custom AI running locally with Ollama in &lt;strong&gt;just 5 minutes.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you've been waiting to try fine-tuning, now is the time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;You can watch the 5-minute tutorial here:&lt;/strong&gt; &lt;a href="https://youtu.be/Cqpcvc9P-lQ"&gt;&lt;code&gt;https://youtu.be/Cqpcvc9P-lQ&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer any questions about the process. What personality would you tune?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jokiruiz"&gt; /u/jokiruiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj5uf1/i_finetuned_llama_31_to_speak_a_rare_spanish/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj5uf1/i_finetuned_llama_31_to_speak_a_rare_spanish/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oj5uf1/i_finetuned_llama_31_to_speak_a_rare_spanish/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T14:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojn7ye</id>
    <title>qwen3-vl:32b appears not to fit into a 24 GB GPU</title>
    <updated>2025-10-30T01:47:27+00:00</updated>
    <author>
      <name>/u/florinandrei</name>
      <uri>https://old.reddit.com/user/florinandrei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All previous models from the Ollama collection that had a size below 24 GB used to fit into a 24 GB GPU like an RTX 3090. E.g. qwen3:32b has a size of 20 GB and runs entirely on the GPU. 20.5 GB of VRAM are used out of the total of 24.&lt;/p&gt; &lt;p&gt;qwen3-vl:32b surprisingly breaks the pattern. It has a size of 21 GB. But 23.55 GB of VRAM are used, it spills into system RAM, and it runs slowly, distributed between GPU and CPU.&lt;/p&gt; &lt;p&gt;I use Open WebUI with default settings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/florinandrei"&gt; /u/florinandrei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojn7ye/qwen3vl32b_appears_not_to_fit_into_a_24_gb_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojn7ye/qwen3vl32b_appears_not_to_fit_into_a_24_gb_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojn7ye/qwen3vl32b_appears_not_to_fit_into_a_24_gb_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T01:47:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojndmq</id>
    <title>Connect your Google Drive, Gmail, and local files ‚Äî while keeping everything private</title>
    <updated>2025-10-30T01:54:33+00:00</updated>
    <author>
      <name>/u/Inevitable-Letter385</name>
      <uri>https://old.reddit.com/user/Inevitable-Letter385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I‚Äôm excited to share something we‚Äôve been building for the past few months - &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;&lt;strong&gt;PipesHub&lt;/strong&gt;&lt;/a&gt;, a &lt;strong&gt;fully open-source Enterprise Search Platform&lt;/strong&gt; designed to bring powerful Enterprise Search to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, and even local file uploads. You can deploy it and run it with just one docker compose command.&lt;/p&gt; &lt;p&gt;You can run the full platform locally. Recently, one of our users tried &lt;strong&gt;qwen3-vl:8b&lt;/strong&gt; with &lt;strong&gt;Ollama&lt;/strong&gt; and got very good results.&lt;/p&gt; &lt;p&gt;The entire system is built on a &lt;strong&gt;fully event-streaming architecture powered by Kafka&lt;/strong&gt;, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep understanding of user, organization and teams with enterprise knowledge graph&lt;/li&gt; &lt;li&gt;Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama&lt;/li&gt; &lt;li&gt;Use any provider that supports OpenAI compatible endpoints&lt;/li&gt; &lt;li&gt;Choose from 1,000+ embedding models&lt;/li&gt; &lt;li&gt;Vision-Language Models and OCR for visual or scanned docs&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, or SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs for developers&lt;/li&gt; &lt;li&gt;All major file types support including pdfs with images, diagrams and charts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Features releasing early next month&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more&lt;/li&gt; &lt;li&gt;Reasoning Agent that plans before executing tasks&lt;/li&gt; &lt;li&gt;40+ Connectors allowing you to connect to your entire business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Note: PipesHub doesn‚Äôt upload any of your data to Google Drive or Gmail. You can simply query and search within your existing data stored in Google Drive or Gmail. You can stay 100% private if you use files from your local filesystem.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Check it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Letter385"&gt; /u/Inevitable-Letter385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojndmq/connect_your_google_drive_gmail_and_local_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojndmq/connect_your_google_drive_gmail_and_local_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojndmq/connect_your_google_drive_gmail_and_local_files/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T01:54:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj6q9w</id>
    <title>Ollama models, why only cloud??</title>
    <updated>2025-10-29T14:55:42+00:00</updated>
    <author>
      <name>/u/stiflers-m0m</name>
      <uri>https://old.reddit.com/user/stiflers-m0m</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im increasingly getting frustrated and looking at alternatives to Ollama. Their cloud only releases are frustrating. Yes i can learn how to go on hugging face and figure out which gguffs are available (if there even is one for that particular model) but at that point i might as well transition off to something else.&lt;/p&gt; &lt;p&gt;If there are any ollama devs, know that you are pushing folks away. In its current state, you are lagging behind and offering cloud only models also goes against why I selected ollama to begin with. Local AI.&lt;/p&gt; &lt;p&gt;Please turn this around, if this was the direction you are going i would have never selected ollama when i first started.&lt;/p&gt; &lt;p&gt;EDIT: THere is a lot of misunderstanding on what this is about. The shift to releaseing cloud only models is what im annoyed with, where is qwen3-vl for example. I enjoyned ollama due to its ease of use, and the provided library. its less helpful if the new models are cloud only. Lots of hate if peopledont drink the ollama koolaid and have frustrations. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stiflers-m0m"&gt; /u/stiflers-m0m &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj6q9w/ollama_models_why_only_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj6q9w/ollama_models_why_only_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oj6q9w/ollama_models_why_only_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T14:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok3ien</id>
    <title>npcsh--the AI command line toolkit from Indiana-based research startup NPC Worldwide--featured on star-history</title>
    <updated>2025-10-30T15:56:10+00:00</updated>
    <author>
      <name>/u/BidWestern1056</name>
      <uri>https://old.reddit.com/user/BidWestern1056</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidWestern1056"&gt; /u/BidWestern1056 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.star-history.com/blog/npcsh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ok3ien/npcshthe_ai_command_line_toolkit_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ok3ien/npcshthe_ai_command_line_toolkit_from/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T15:56:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ojvbxs</id>
    <title>New Feature: Note Context. How would you use something like this?</title>
    <updated>2025-10-30T09:46:50+00:00</updated>
    <author>
      <name>/u/Financial_Click9119</name>
      <uri>https://old.reddit.com/user/Financial_Click9119</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ojvbxs/new_feature_note_context_how_would_you_use/"&gt; &lt;img alt="New Feature: Note Context. How would you use something like this?" src="https://external-preview.redd.it/MDVobWo4dnR4N3lmMXYvmWrVLIVEfLx-q3_6dsuZtEaR_DjzHzPO-YE125uf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=36dcfbad2519d1489ce118ddd24a739fa4ef0d19" title="New Feature: Note Context. How would you use something like this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR: Made it possible for Mistral to read the context of connected notes.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;For note-taking, writing and research, I use the KettleKasten method to build large networks of notes. The process involves creating atomic notes and then expanding on them to create a network.&lt;br /&gt; You can learn more about this system &lt;a href="https://zettelkasten.de/overview/"&gt;here&lt;/a&gt;, and if you prefer a &lt;a href="https://www.youtube.com/watch?v=lo4v4t06uD0"&gt;video&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;The game plan with this is to create a note-taking app that uses an LLM to build insights on my notes while all running on localhost. &lt;/p&gt; &lt;p&gt;I do have a version on the web so that I can sync notes and work on the go.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial_Click9119"&gt; /u/Financial_Click9119 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kj2dq2vtx7yf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ojvbxs/new_feature_note_context_how_would_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ojvbxs/new_feature_note_context_how_would_you_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T09:46:50+00:00</published>
  </entry>
</feed>
