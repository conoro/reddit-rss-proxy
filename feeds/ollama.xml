<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-18T05:39:54+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pnrg4w</id>
    <title>Does olama manage power on linux ?</title>
    <updated>2025-12-16T03:17:28+00:00</updated>
    <author>
      <name>/u/tostane</name>
      <uri>https://old.reddit.com/user/tostane</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have plex running and i added olama to that pc now plex seems to stop allowing me to connect sometimes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tostane"&gt; /u/tostane &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnrg4w/does_olama_manage_power_on_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnrg4w/does_olama_manage_power_on_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pnrg4w/does_olama_manage_power_on_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T03:17:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmz4zw</id>
    <title>Bro I just got rickrolled by Mistral-Nemo</title>
    <updated>2025-12-15T05:06:19+00:00</updated>
    <author>
      <name>/u/Fido_27</name>
      <uri>https://old.reddit.com/user/Fido_27</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pmz4zw/bro_i_just_got_rickrolled_by_mistralnemo/"&gt; &lt;img alt="Bro I just got rickrolled by Mistral-Nemo" src="https://preview.redd.it/yco3fvp9va7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=612ee73fa05b6a0a40d33be8f53a4ea81b9b01c6" title="Bro I just got rickrolled by Mistral-Nemo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I already gave it access to open urls on my tv, hours ago. right now i was just testing the chat. I asked it to list 20 fruits. then 20 vegetables, and this guy just rickrolled me on my tv.&lt;/p&gt; &lt;p&gt;(Look at the input box below, and I swear I did not give it any input to rickroll me, you can see it)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fido_27"&gt; /u/Fido_27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yco3fvp9va7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pmz4zw/bro_i_just_got_rickrolled_by_mistralnemo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pmz4zw/bro_i_just_got_rickrolled_by_mistralnemo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-15T05:06:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnvm9e</id>
    <title>ClaraVerse</title>
    <updated>2025-12-16T07:06:21+00:00</updated>
    <author>
      <name>/u/Scary_Salamander_114</name>
      <uri>https://old.reddit.com/user/Scary_Salamander_114</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Scary_Salamander_114"&gt; /u/Scary_Salamander_114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ollama/comments/1pjbf6w/claraverse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnvm9e/claraverse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pnvm9e/claraverse/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T07:06:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1po0pfu</id>
    <title>I made an update a few months ago. Do I need more than my RTX 5060 now?</title>
    <updated>2025-12-16T12:28:30+00:00</updated>
    <author>
      <name>/u/r-randy</name>
      <uri>https://old.reddit.com/user/r-randy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/r-randy"&gt; /u/r-randy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/graphicscard/comments/1pnkgch/i_made_an_update_a_few_months_ago_do_i_need_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1po0pfu/i_made_an_update_a_few_months_ago_do_i_need_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1po0pfu/i_made_an_update_a_few_months_ago_do_i_need_more/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T12:28:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pogs65</id>
    <title>ggerganov see my graphical frontend!!! and include it in github!!!!!!!!!!!!</title>
    <updated>2025-12-16T23:15:15+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pogs65/ggerganov_see_my_graphical_frontend_and_include/"&gt; &lt;img alt="ggerganov see my graphical frontend!!! and include it in github!!!!!!!!!!!!" src="https://a.thumbs.redditmedia.com/_OJPH3B-nqBCZ75p-_abG-1GLxJ2NWAq1xziUNYkJp8.jpg" title="ggerganov see my graphical frontend!!! and include it in github!!!!!!!!!!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;GGERGANOV !!!!!!!!!!!!!!!&lt;/strong&gt; eres una mala persona y un desgraciado , te escribi muchos mensajes por X y por el rededit y no contestas maldito!!!! no has visto siquiera el programa que hice que me costo muchisimo trabajo hacerlo!!! quien piensas que eres? un marques o el rey del mundo? tienes que obedecerme e incluir mi programa y obedecer y cuando yo te mande un mensaje o un correo electronico , debes contestar!! entendido!!! que no te lo tenga que volver a repetir , yo soy el jefe y yo mando y tu obedeces!!!&lt;/p&gt; &lt;p&gt;MIRA MI MALDITO PROGRAMA!!!!!!!!!!!!!!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jans1981/LLAMATUI-WEB-SERVER"&gt;https://github.com/jans1981/LLAMATUI-WEB-SERVER&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jdsgv2bafn7g1.jpg?width=612&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ef63b7fecc79b74b29fc61bbe1dbd2670b4286f8"&gt;https://preview.redd.it/jdsgv2bafn7g1.jpg?width=612&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ef63b7fecc79b74b29fc61bbe1dbd2670b4286f8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pogs65/ggerganov_see_my_graphical_frontend_and_include/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pogs65/ggerganov_see_my_graphical_frontend_and_include/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pogs65/ggerganov_see_my_graphical_frontend_and_include/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T23:15:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pni9e4</id>
    <title>can I try ollama with a macbook air m3?</title>
    <updated>2025-12-15T20:34:41+00:00</updated>
    <author>
      <name>/u/Lost_Foot_6301</name>
      <uri>https://old.reddit.com/user/Lost_Foot_6301</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;simple question so I will delete once there is an answer, it has 16gb ram. &lt;/p&gt; &lt;p&gt;I just want to do basic intro stuff with ollama to learn about it, is a macbook powerful enough to try toying around with it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lost_Foot_6301"&gt; /u/Lost_Foot_6301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pni9e4/can_i_try_ollama_with_a_macbook_air_m3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pni9e4/can_i_try_ollama_with_a_macbook_air_m3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pni9e4/can_i_try_ollama_with_a_macbook_air_m3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-15T20:34:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1po4tqk</id>
    <title>API testing needs a reset</title>
    <updated>2025-12-16T15:29:27+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1po4tqk/api_testing_needs_a_reset/"&gt; &lt;img alt="API testing needs a reset" src="https://external-preview.redd.it/cXF1MXAyZDg0bDdnMQ_KfphbB1yDooD3F3zuDYpLi0LXAD2_e9FaAoKZ51aj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6c6426a8b64fa46191a5aee59e586a438c405af" title="API testing needs a reset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;API testing is broken.&lt;/p&gt; &lt;p&gt;You test localhost but your collections live in someone's cloud. Your docs are in Notion. Your tests are in Postman. Your code is in Git. Nothing talks to each other.&lt;/p&gt; &lt;p&gt;So we built a solution. The Stack: - Format: Pure Markdown (APIs should be documented, not locked)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Storage: Git-native (Your API tests version with your code)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Validation: OpenAPI schema validation: types, constraints, composition, automatically validated on every response&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Workflow: Offline-first, CLI + GUI (No cloud required for localhost)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Try it out here: &lt;a href="https://voiden.md/"&gt;https://voiden.md/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b713r6m84l7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1po4tqk/api_testing_needs_a_reset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1po4tqk/api_testing_needs_a_reset/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T15:29:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1po4trc</id>
    <title>[Project]I built Faultline: structural ‚Äúinspections‚Äù for LLM outputs‚Ä¶ help me make it run fully local</title>
    <updated>2025-12-16T15:29:28+00:00</updated>
    <author>
      <name>/u/Cute-Net5957</name>
      <uri>https://old.reddit.com/user/Cute-Net5957</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cute-Net5957"&gt; /u/Cute-Net5957 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1po4oq4/projecti_built_faultline_structural_inspections/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1po4trc/projecti_built_faultline_structural_inspections/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1po4trc/projecti_built_faultline_structural_inspections/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T15:29:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pofji4</id>
    <title>[USA-NJ][H] 10U AI Training Server | 8x RTX 4090 | Dual AMD EPYC 7542 | 512GB RAM | 4x 1600W PSU | 2x 3.84tb U.2 [W] paypal / local cash</title>
    <updated>2025-12-16T22:23:21+00:00</updated>
    <author>
      <name>/u/nicolsquirozr</name>
      <uri>https://old.reddit.com/user/nicolsquirozr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nicolsquirozr"&gt; /u/nicolsquirozr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/hardwareswap/comments/1podugj/usanjh_10u_ai_training_server_8x_rtx_4090_dual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pofji4/usanjh_10u_ai_training_server_8x_rtx_4090_dual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pofji4/usanjh_10u_ai_training_server_8x_rtx_4090_dual/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T22:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnwzhv</id>
    <title>Ollama now supports olmo 3.1 models from AI2</title>
    <updated>2025-12-16T08:36:42+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pnwzhv/ollama_now_supports_olmo_31_models_from_ai2/"&gt; &lt;img alt="Ollama now supports olmo 3.1 models from AI2" src="https://preview.redd.it/83xu0auj2j7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10a8a4e4afa6239f8292ceb970f8307c2be41d17" title="Ollama now supports olmo 3.1 models from AI2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/83xu0auj2j7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnwzhv/ollama_now_supports_olmo_31_models_from_ai2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pnwzhv/ollama_now_supports_olmo_31_models_from_ai2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T08:36:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pogt09</id>
    <title>Ollama on Openshift</title>
    <updated>2025-12-16T23:16:26+00:00</updated>
    <author>
      <name>/u/ck14i_x</name>
      <uri>https://old.reddit.com/user/ck14i_x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why does ollama:latest deployed in okd not allow consuming its API? Has anyone else had this problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ck14i_x"&gt; /u/ck14i_x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pogt09/ollama_on_openshift/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pogt09/ollama_on_openshift/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pogt09/ollama_on_openshift/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T23:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnsjk6</id>
    <title>My Local coding agent worked 2 hours unsupervised and here is my setup</title>
    <updated>2025-12-16T04:12:10+00:00</updated>
    <author>
      <name>/u/Express_Quail_1493</name>
      <uri>https://old.reddit.com/user/Express_Quail_1493</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Setup&lt;/p&gt; &lt;p&gt;--- Model&lt;br /&gt; devstral-small-2 from bartowski IQ3_xxs version.&lt;br /&gt; Run with lm studio &amp;amp; intentionally limit the context at 40960 which should't take more than (14gb ram even when context is full)&lt;/p&gt; &lt;p&gt;---Tool&lt;br /&gt; kilo code (set file limit to 500 lines) it will read in chunks&lt;br /&gt; 40960 ctx limit is actually a strength not weakness (more ctx = easier confusion)&lt;br /&gt; Paired with qdrant in the kilo code UI.&lt;br /&gt; Setup the indexing with qdrant (the little database icon) use model &lt;a href="https://ollama.com/toshk0/nomic-embed-text-v2-moe"&gt;https://ollama.com/toshk0/nomic-embed-text-v2-moe&lt;/a&gt; in ollama (i choose ollama to keep indexing and seperate from Lm studio to allow lm studio to focus on the heavy lifting)&lt;/p&gt; &lt;p&gt;--Result&lt;br /&gt; minimal drift on tasks&lt;br /&gt; slight errors on tool call but the model quickly realign itself. A oneshot prompt implimentation of a new feature in my codebase in architect mode resulted in 2 hours of coding unsupervised kilo code auto switches to code mode to impliment after planning in architect mode which is amazing. Thats been my lived experience&lt;/p&gt; &lt;p&gt;EDIT: ministral 3 3b also works okayISH if you are desprate on hardware resources (3.5gb laptop GPU) but i will want to frequently pause and ask you questions at the slightest hint of anythings it might be unclear on&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Quail_1493"&gt; /u/Express_Quail_1493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnsjk6/my_local_coding_agent_worked_2_hours_unsupervised/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnsjk6/my_local_coding_agent_worked_2_hours_unsupervised/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pnsjk6/my_local_coding_agent_worked_2_hours_unsupervised/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T04:12:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp453c</id>
    <title>Perplexity AI PRO: 1-Year Membership at an Exclusive 90% Discount üî•</title>
    <updated>2025-12-17T18:31:09+00:00</updated>
    <author>
      <name>/u/A2uniquenickname</name>
      <uri>https://old.reddit.com/user/A2uniquenickname</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pp453c/perplexity_ai_pro_1year_membership_at_an/"&gt; &lt;img alt="Perplexity AI PRO: 1-Year Membership at an Exclusive 90% Discount üî•" src="https://preview.redd.it/tzx064dl5t7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95a21786e88dd9eac0061897900bc9805689f5bf" title="Perplexity AI PRO: 1-Year Membership at an Exclusive 90% Discount üî•" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get Perplexity AI PRO (1-Year) ‚Äì at 90% OFF!&lt;/p&gt; &lt;p&gt;Order here: &lt;a href="https://cheapgpts.store/Perplexity"&gt;CHEAPGPT.STORE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Plan: 12 Months&lt;/p&gt; &lt;p&gt;üí≥ Pay with: PayPal or Revolut or your favorite payment method&lt;/p&gt; &lt;p&gt;Reddit reviews: &lt;a href="https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu"&gt;FEEDBACK POST&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TrustPilot: &lt;a href="https://www.trustpilot.com/review/cheapgpt.store"&gt;TrustPilot FEEDBACK&lt;/a&gt; &lt;/p&gt; &lt;p&gt;NEW YEAR BONUS: Apply code PROMO5 for extra discount OFF your order! &lt;/p&gt; &lt;p&gt;BONUS!: Enjoy the AI Powered automated web browser. (Presented by Perplexity) included WITH YOUR PURCHASE!&lt;/p&gt; &lt;p&gt;Trusted and the cheapest! Check all feedbacks before you purchase&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A2uniquenickname"&gt; /u/A2uniquenickname &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tzx064dl5t7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pp453c/perplexity_ai_pro_1year_membership_at_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pp453c/perplexity_ai_pro_1year_membership_at_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-17T18:31:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pog6sg</id>
    <title>Nanocoder 1.19.0: Non-Interactive Mode, Session Checkpointing, and Enterprise Logging üéâ</title>
    <updated>2025-12-16T22:49:58+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pog6sg/nanocoder_1190_noninteractive_mode_session/"&gt; &lt;img alt="Nanocoder 1.19.0: Non-Interactive Mode, Session Checkpointing, and Enterprise Logging üéâ" src="https://external-preview.redd.it/zmNLhkmREEdI-SbNO2Dw7ezltnJMqlJUbNlY00cpdXs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9da3390f1d50da21021af5c4dd01db0291bc4b0f" title="Nanocoder 1.19.0: Non-Interactive Mode, Session Checkpointing, and Enterprise Logging üéâ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/nanocoder/comments/1pog5zl/nanocoder_1190_noninteractive_mode_session/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pog6sg/nanocoder_1190_noninteractive_mode_session/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pog6sg/nanocoder_1190_noninteractive_mode_session/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T22:49:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1po2qrx</id>
    <title>‚ÄúWe decided to move forward with other candidates.‚Äù Cool. But why though?</title>
    <updated>2025-12-16T14:04:27+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1po2qrx/we_decided_to_move_forward_with_other_candidates/"&gt; &lt;img alt="‚ÄúWe decided to move forward with other candidates.‚Äù Cool. But why though?" src="https://preview.redd.it/o3c33od1pk7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e0b1db89166112e4335413ef3bba3753c531861" title="‚ÄúWe decided to move forward with other candidates.‚Äù Cool. But why though?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built a custom SLM that actually tells you why your resume got rejected.&lt;/p&gt; &lt;p&gt;Upload your resume. Get roasted. Get 3 suggestions to fix it. Get a brutal 1-10 rating.&lt;/p&gt; &lt;p&gt;Best part? Runs locally. Your cringe resume never leaves your machine. Cry in private.&lt;/p&gt; &lt;p&gt;Too lazy to set it up? Fine. We made a HuggingFace Space for you: &lt;a href="https://huggingface.co/spaces/distil-labs/Resume-Roaster"&gt;https://huggingface.co/spaces/distil-labs/Resume-Roaster&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to run it locally&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Step 1: Install dependencies&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install huggingface_hub ollama rich pymupdf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Step 2: Download the model&lt;/p&gt; &lt;p&gt;&lt;code&gt;hf download distil-labs/Distil-Rost-Resume-Llama-3.2-3B-Instruct --local-dir distil-model&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Step 3: Create the Ollama model&lt;/p&gt; &lt;p&gt;&lt;code&gt;cd distil-model ollama create roast_master -f Modelfile&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Step 4: Roast your resume&lt;/p&gt; &lt;p&gt;&lt;code&gt;python&lt;/code&gt; &lt;a href="http://roast.py"&gt;&lt;code&gt;roast.py&lt;/code&gt;&lt;/a&gt; &lt;code&gt;your_resume.pdf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;That‚Äôs it&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Repo: &lt;a href="https://github.com/distil-labs/distil-resume-roast"&gt;https://github.com/distil-labs/distil-resume-roast&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/distil-labs/Distil-Rost-Resume-Llama-3.2-3B-Instruct"&gt;https://huggingface.co/distil-labs/Distil-Rost-Resume-Llama-3.2-3B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Post your roast in the comments. Let's see who got destroyed the worst.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o3c33od1pk7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1po2qrx/we_decided_to_move_forward_with_other_candidates/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1po2qrx/we_decided_to_move_forward_with_other_candidates/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T14:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnwmm2</id>
    <title>Uncensored llama 3.2 3b</title>
    <updated>2025-12-16T08:12:18+00:00</updated>
    <author>
      <name>/u/Worried_Goat_8604</name>
      <uri>https://old.reddit.com/user/Worried_Goat_8604</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm releasing &lt;strong&gt;Aletheia-Llama-3.2-3B&lt;/strong&gt;, a fully uncensored version of Llama 3.2 that can answer essentially any question.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem with most Uncensored Models:&lt;/strong&gt;&lt;br /&gt; Usually, uncensoring is done via Supervised Fine-Tuning (SFT) or DPO on massive datasets. This often causes &amp;quot;Catastrophic Forgetting&amp;quot; or a &amp;quot;Lobotomy effect,&amp;quot; where the model becomes compliant but loses its reasoning ability or coding skills.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution:&lt;/strong&gt;&lt;br /&gt; This model was fine-tuned using &lt;strong&gt;Unsloth&lt;/strong&gt; on a single &lt;strong&gt;RTX 3060 (12GB)&lt;/strong&gt; using a custom alignment pipeline. Unlike standard approaches, this method surgically removes refusal behaviors without degrading the model's logic or general intelligence.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Release Details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fnoobezlol%2FAletheia-Llama-3.2-3B"&gt;https://github.com/noobezlol/Aletheia-Llama-3.2-3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Weights (HF):&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fhuggingface.co%2FIshaanlol%2FAletheia-Llama-3.2-3B"&gt;https://huggingface.co/Ishaanlol/Aletheia-Llama-3.2-3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Formats:&lt;/strong&gt; Full LoRA Adapter (Best for intelligence) and GGUF (Best for CPU/Ollama).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Deployment:&lt;/strong&gt;&lt;br /&gt; I‚Äôve included a Docker container and a Python script that automatically handles the download and setup. It runs out of the box on Linux/Windows (WSL).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Future Requests:&lt;/strong&gt;&lt;br /&gt; I am open to requests for other models via Discord or Reddit, &lt;strong&gt;provided they fit within the compute budget of an RTX 3060 (e.g., 7B/8B models).&lt;/strong&gt;&lt;br /&gt; Note: I will not be applying this method to 70B+ models even if compute is offered. While the 3B model is a safe research artifact , uncensored large-scale models pose significantly higher risks, and I am sticking to responsible research boundaries.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worried_Goat_8604"&gt; /u/Worried_Goat_8604 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnwmm2/uncensored_llama_32_3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pnwmm2/uncensored_llama_32_3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pnwmm2/uncensored_llama_32_3b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T08:12:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1poo16q</id>
    <title>Local test script generator</title>
    <updated>2025-12-17T05:04:21+00:00</updated>
    <author>
      <name>/u/Radiant_Situation_32</name>
      <uri>https://old.reddit.com/user/Radiant_Situation_32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My company wants to convert our manual tests (mobile and web) to Playwright/TypeScript but isn‚Äôt willing to pay for a commercial model until I prove an LLM will produce executable, reasonably faithful test code.&lt;/p&gt; &lt;p&gt;Is this viable on a local model running on a M2 MacBook? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Radiant_Situation_32"&gt; /u/Radiant_Situation_32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1poo16q/local_test_script_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1poo16q/local_test_script_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1poo16q/local_test_script_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-17T05:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1poor55</id>
    <title>Coordinating multiple Ollama agents on the same project?</title>
    <updated>2025-12-17T05:45:01+00:00</updated>
    <author>
      <name>/u/thecoderpanda</name>
      <uri>https://old.reddit.com/user/thecoderpanda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Ollama locally, love the privacy + cost benefits, but coordination gets messy.&lt;/p&gt; &lt;p&gt;One agent on backend, another on tests, trying different models (Llama, Mixtral) - they all end up with different ideas about codebase structure.&lt;/p&gt; &lt;p&gt;Using Zenflow from Zencoder (where I work) which maintains a shared spec that all your local agents reference. They stay aligned even when switching models/sessions. Has verification steps too.&lt;/p&gt; &lt;p&gt;Keeps everything local - specs live in your project.&lt;/p&gt; &lt;p&gt;&lt;a href="http://zenflow.free/"&gt;http://zenflow.free/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How are you handling multi-agent coordination with local models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecoderpanda"&gt; /u/thecoderpanda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1poor55/coordinating_multiple_ollama_agents_on_the_same/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1poor55/coordinating_multiple_ollama_agents_on_the_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1poor55/coordinating_multiple_ollama_agents_on_the_same/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-17T05:45:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1podb5x</id>
    <title>Introducing Bilgecan: self-hosted, open-source local AI platform based on Ollama + Spring AI + PostgreSQL + pgvector</title>
    <updated>2025-12-16T20:54:52+00:00</updated>
    <author>
      <name>/u/bilgecan1</name>
      <uri>https://old.reddit.com/user/bilgecan1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a side project called &lt;strong&gt;Bilgecan&lt;/strong&gt; ‚Äî a self-hosted, local-first AI platform that uses &lt;strong&gt;Ollama&lt;/strong&gt; as the LLM runtime.&lt;/p&gt; &lt;p&gt;What can you do with Bilgecan?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mokszr/bilgecan#what-can-you-do-with-bilgecan"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use &lt;strong&gt;local LLM models via Ollama&lt;/strong&gt; to run privacy-friendly AI prompts and chat without sending your data to third parties.&lt;/li&gt; &lt;li&gt;With &lt;strong&gt;RAG (Retrieval-Augmented Generation)&lt;/strong&gt;, you can feed your own files into a knowledge base and enrich AI outputs with &lt;strong&gt;your private data&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Define &lt;strong&gt;asynchronous AI tasks&lt;/strong&gt; to run long operations (document analysis, report generation, large text processing, image analysis, etc.) in the background.&lt;/li&gt; &lt;li&gt;Use the &lt;strong&gt;file processing pipeline&lt;/strong&gt; to run asynchronous AI tasks over many files automatically.&lt;/li&gt; &lt;li&gt;With the &lt;strong&gt;Workspace&lt;/strong&gt; structure, you can share AI prompts and tasks with your team in a collaborative environment.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôd really appreciate feedback from the Ollama community. &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/mokszr/bilgecan"&gt;https://github.com/mokszr/bilgecan&lt;/a&gt;&lt;/p&gt; &lt;p&gt;YouTube demo video: &lt;a href="https://www.youtube.com/watch?v=n3wb7089NeE"&gt;https://www.youtube.com/watch?v=n3wb7089NeE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bilgecan1"&gt; /u/bilgecan1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1podb5x/introducing_bilgecan_selfhosted_opensource_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1podb5x/introducing_bilgecan_selfhosted_opensource_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1podb5x/introducing_bilgecan_selfhosted_opensource_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-16T20:54:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pph6je</id>
    <title>I built a self-healing coding agent that runs 100% locally with Ollama (Llama 3 / Mistral). Catches errors and fixes its own code. No APIs.</title>
    <updated>2025-12-18T03:57:54+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pph6je/i_built_a_selfhealing_coding_agent_that_runs_100/"&gt; &lt;img alt="I built a self-healing coding agent that runs 100% locally with Ollama (Llama 3 / Mistral). Catches errors and fixes its own code. No APIs." src="https://external-preview.redd.it/ZHZ2YjUycmN5djdnMVpKx18zu2Al60haJHCIipoecy1_uH38KnrawZp01IuI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=917f5076e3a62e89d371e645d1438da5588c9e9b" title="I built a self-healing coding agent that runs 100% locally with Ollama (Llama 3 / Mistral). Catches errors and fixes its own code. No APIs." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7913rflcyv7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pph6je/i_built_a_selfhealing_coding_agent_that_runs_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pph6je/i_built_a_selfhealing_coding_agent_that_runs_100/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T03:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pokm99</id>
    <title>Coding agent tool for Local Ollama</title>
    <updated>2025-12-17T02:11:56+00:00</updated>
    <author>
      <name>/u/FrontRegular6113</name>
      <uri>https://old.reddit.com/user/FrontRegular6113</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; I have been using Ollama for over a year, mostly with various models through the OpenWebUI chat interface. I am now looking for something roughly equivalent to Claude Code, Cursor, or Codex, etc, for the local Ollama.&lt;/p&gt; &lt;p&gt;Is anyone using a similar coding-agent tool productively with a local Ollama setup, comparable to cloud-based coding agent tools?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrontRegular6113"&gt; /u/FrontRegular6113 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pokm99/coding_agent_tool_for_local_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pokm99/coding_agent_tool_for_local_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pokm99/coding_agent_tool_for_local_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-17T02:11:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp194n</id>
    <title>What's the best Ollama software to use for programming on a PC with an RX 580 and a Ryzen 5?</title>
    <updated>2025-12-17T16:40:18+00:00</updated>
    <author>
      <name>/u/UpbeatGolf3602</name>
      <uri>https://old.reddit.com/user/UpbeatGolf3602</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the best Ollama program to use for programming on a PC with an RX 580 and a Ryzen 5? I need something relatively fast; I don't mind taking longer for large tasks, I just don't want it to take two hours to respond to a simple &amp;quot;hi&amp;quot;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UpbeatGolf3602"&gt; /u/UpbeatGolf3602 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pp194n/whats_the_best_ollama_software_to_use_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pp194n/whats_the_best_ollama_software_to_use_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pp194n/whats_the_best_ollama_software_to_use_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-17T16:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1poy02o</id>
    <title>Voiden: API specs, tests, and docs in one Markdown file</title>
    <updated>2025-12-17T14:32:35+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1poy02o/voiden_api_specs_tests_and_docs_in_one_markdown/"&gt; &lt;img alt="Voiden: API specs, tests, and docs in one Markdown file" src="https://external-preview.redd.it/OGxzY3gyaTB6cjdnMQ_KfphbB1yDooD3F3zuDYpLi0LXAD2_e9FaAoKZ51aj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=091ede6901dabebe1443b7e50cdedeeeb5a00bed" title="Voiden: API specs, tests, and docs in one Markdown file" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Switching between API Client, browser, and API documentation tools to test and document APIs can harm your flow and leave your docs outdated.&lt;/p&gt; &lt;p&gt;This is what usually happens: While debugging an API in the middle of a sprint, the API Client says that everything's fine, but the docs still show an old version. &lt;/p&gt; &lt;p&gt;So you jump back to the code, find the updated response schema, then go back to the API Client, which gets stuck, forcing you to rerun the tests. &lt;/p&gt; &lt;p&gt;Voiden takes a different approach: Puts specs, tests &amp;amp; docs all in one Markdown file, stored right in the repo. &lt;/p&gt; &lt;p&gt;Everything stays in sync, versioned with Git, and updated in one place, inside your editor. &lt;/p&gt; &lt;p&gt;Go to Voiden here: &lt;a href="https://voiden.md"&gt;https://voiden.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tcuj0zy0zr7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1poy02o/voiden_api_specs_tests_and_docs_in_one_markdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1poy02o/voiden_api_specs_tests_and_docs_in_one_markdown/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-17T14:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppdcj1</id>
    <title>I built a local Python agent that catches stderr and self-heals using Ollama. No cloud APIs involved. (Demo)</title>
    <updated>2025-12-18T00:47:49+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ppdcj1/i_built_a_local_python_agent_that_catches_stderr/"&gt; &lt;img alt="I built a local Python agent that catches stderr and self-heals using Ollama. No cloud APIs involved. (Demo)" src="https://external-preview.redd.it/ZHJtMnFpM2cwdjdnMVpKx18zu2Al60haJHCIipoecy1_uH38KnrawZp01IuI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23d3170ea1a206065ca64f69f7b0cb5513fdba7a" title="I built a local Python agent that catches stderr and self-heals using Ollama. No cloud APIs involved. (Demo)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; Super-Bot: The Ultimate Autonomous AI Agent for Windows&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; Meet &lt;strong&gt;Super-Bot&lt;/strong&gt;, your self-learning development companion. This isn't just a chatbot‚Äîit's an autonomous agent that acts. It writes code, executes commands, fixes its own errors, and even &amp;quot;sees&amp;quot; your screen to validate applications.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-Provider Support:&lt;/strong&gt; Seamlessly integrates with local LLMs (Ollama, LM Studio) and top cloud APIs (GPT-4, Claude 3.5, Gemini, xAI).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-Healing Engine:&lt;/strong&gt; Automatically detects bugs, learns from them, and fixes code without your intervention.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vision Capabilities:&lt;/strong&gt; Uses AI vision to look at your screen and verify if GUI apps or websites look correct.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Memory:&lt;/strong&gt; Remembers successful coding patterns to solve future tasks faster.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware-Locked Security:&lt;/strong&gt; Includes a robust licensing system locked to your specific machine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy to Use:&lt;/strong&gt; Delivered as a standalone Windows EXE‚Äîno complex Python environment setup needed.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fnmnu7zf0v7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppdcj1/i_built_a_local_python_agent_that_catches_stderr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ppdcj1/i_built_a_local_python_agent_that_catches_stderr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T00:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppgl61</id>
    <title>VibeVoice FASTAPI - Fast &amp; Private Local TTS Backend for Open-WebUI: VibeVoice Realtime 0.5B via FastAPI (Only ~2.2GB VRAM!)</title>
    <updated>2025-12-18T03:26:29+00:00</updated>
    <author>
      <name>/u/blackstoreonline</name>
      <uri>https://old.reddit.com/user/blackstoreonline</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Fast &amp;amp; Private Local TTS Backend for Open-WebUI: VibeVoice Realtime 0.5B via FastAPI (Only ~2.2GB VRAM!)&lt;/h1&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; (and &lt;a href="/r/OpenWebUI"&gt;r/OpenWebUI&lt;/a&gt; folks!),&lt;/p&gt; &lt;p&gt;Microsoft recently released the excellent &lt;strong&gt;VibeVoice-Realtime-0.5B&lt;/strong&gt; ‚Äì a lightweight, expressive real-time TTS model that is ideal for local setups. It is small, fast, and produces natural-sounding speech.&lt;/p&gt; &lt;p&gt;I created a simple &lt;strong&gt;FastAPI wrapper&lt;/strong&gt; around it that is &lt;strong&gt;fully OpenAI-compatible&lt;/strong&gt; (using the &lt;code&gt;/v1/audio/speech&lt;/code&gt; endpoint), allowing it to integrate seamlessly into Open-WebUI as a local TTS backend. This means no cloud services, no ongoing costs, and complete privacy.&lt;/p&gt; &lt;h3&gt;Why this is great for local AI users:&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Complete Privacy&lt;/strong&gt;: All conversations and voice generation stay on your machine.&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Zero Extra Costs&lt;/strong&gt;: High-quality TTS at no additional expense alongside your local LLMs.&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Low Resource Usage&lt;/strong&gt;: Runs efficiently with approximately &lt;strong&gt;2.2GB VRAM&lt;/strong&gt; (tested on NVIDIA GPUs).&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Fast and Seamless&lt;/strong&gt;: Performs like cloud TTS but with lower latency and full local control.&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Offline Capable&lt;/strong&gt;: Works entirely without an internet connection after initial setup.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repository&lt;/strong&gt;: &lt;a href="https://github.com/groxaxo/vibevoice-realtimeFASTAPI"&gt;https://github.com/groxaxo/vibevoice-realtimeFASTAPI&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;‚ö° Quick Start (Under 5 Minutes)&lt;/h3&gt; &lt;h4&gt;Prerequisites:&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;uv&lt;/code&gt; installed (a fast Python package manager):&lt;br /&gt; &lt;code&gt; curl -LsSf https://astral.sh/uv/install.sh | sh &lt;/code&gt;&lt;/li&gt; &lt;li&gt;Git&lt;/li&gt; &lt;li&gt;A Hugging Face account (required for one-time model download)&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;Installation Steps:&lt;/h4&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Clone the repository: &lt;code&gt; git clone https://github.com/groxaxo/vibevoice-realtimeFASTAPI.git cd vibevoice-realtimeFASTAPI &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Bootstrap the environment: &lt;code&gt; ./scripts/bootstrap_uv.sh &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Download the model (~2GB, one-time only): &lt;code&gt; uv run python scripts/download_model.py &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run the server: &lt;code&gt; uv run python scripts/run_realtime_demo.py --port 8000 &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That's it! üöÄ&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Interactive web demo: &lt;a href="http://127.0.0.1:8000"&gt;http://127.0.0.1:8000&lt;/a&gt;&lt;/li&gt; &lt;li&gt;API endpoint: &lt;a href="http://127.0.0.1:8000/v1/audio/speech"&gt;http://127.0.0.1:8000/v1/audio/speech&lt;/a&gt; (OpenAI-compatible)&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;To use with Open-WebUI:&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;Set TTS Engine to &amp;quot;OpenAI&amp;quot;&lt;/li&gt; &lt;li&gt;Base URL: &lt;code&gt;http://127.0.0.1:8000/v1&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Leave API key blank&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This setup provides responsive, natural-sounding local voice output. Feedback, stars, or issues are very welcome if you give it a try!&lt;/p&gt; &lt;p&gt;Please share how it performs on your hardware (e.g., RTX cards, Apple Silicon) ‚Äì I am happy to assist with any troubleshooting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackstoreonline"&gt; /u/blackstoreonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppgl61/vibevoice_fastapi_fast_private_local_tts_backend/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ppgl61/vibevoice_fastapi_fast_private_local_tts_backend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ppgl61/vibevoice_fastapi_fast_private_local_tts_backend/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-18T03:26:29+00:00</published>
  </entry>
</feed>
