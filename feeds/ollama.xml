<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-25T09:38:32+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1psfyme</id>
    <title>AI REAL USEFUL WORKING IN REAL LIFE , LLAMA.CPP</title>
    <updated>2025-12-21T20:17:00+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1psfydn/ai_real_useful_working_in_real_life_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1psfyme/ai_real_useful_working_in_real_life_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1psfyme/ai_real_useful_working_in_real_life_llamacpp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T20:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1psa03p</id>
    <title>Low-code AI tools, live MCP servers, inspection, and agentic chat in one Spring AI playground.</title>
    <updated>2025-12-21T16:08:25+00:00</updated>
    <author>
      <name>/u/kr-jmlab</name>
      <uri>https://old.reddit.com/user/kr-jmlab</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kr-jmlab"&gt; /u/kr-jmlab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ps9yej"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1psa03p/lowcode_ai_tools_live_mcp_servers_inspection_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1psa03p/lowcode_ai_tools_live_mcp_servers_inspection_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T16:08:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1psihel</id>
    <title>Ultima 2 Challenge: COMPLETED. ‚úÖ You asked for a tile-based RPG engine with state management. The Agent delivered.</title>
    <updated>2025-12-21T22:07:11+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1psihel/ultima_2_challenge_completed_you_asked_for_a/"&gt; &lt;img alt="Ultima 2 Challenge: COMPLETED. ‚úÖ You asked for a tile-based RPG engine with state management. The Agent delivered." src="https://external-preview.redd.it/dG1xZW56djhybThnMf3CgSBkjdJpI-IDVjUSNCAXZBQqMepkGuNeLK6QbT2A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6958813c6087f43027e16fe1218311a585ae8a49" title="Ultima 2 Challenge: COMPLETED. ‚úÖ You asked for a tile-based RPG engine with state management. The Agent delivered." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Under the hood (as seen in the video):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;State Machine:&lt;/strong&gt; Fully implemented. Seamless switching between &lt;code&gt;OVERWORLD&lt;/code&gt; and &lt;code&gt;TOWN&lt;/code&gt; states based on tile triggers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Persistence:&lt;/strong&gt; The agent handles coordinate resets when entering/exiting zones.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tile Engine:&lt;/strong&gt; Dynamic rendering of 4 different terrain types + walls.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logic:&lt;/strong&gt; Turn-based movement, collision detection (water/walls), and NPC interaction logic.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Verdict:&lt;/strong&gt; This required maintaining context across multiple class structures and game loops. A massive win for local 30B models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9xbp2an8rm8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1psihel/ultima_2_challenge_completed_you_asked_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1psihel/ultima_2_challenge_completed_you_asked_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T22:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pskkhp</id>
    <title>Any hope for my Linux laptop?</title>
    <updated>2025-12-21T23:40:39+00:00</updated>
    <author>
      <name>/u/AccordionPianist</name>
      <uri>https://old.reddit.com/user/AccordionPianist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 13 year old laptop (build date 2012-10) with 12 GB RAM running Ubuntu. Integrated graphics, ASUS machine K56CA. Do I have a snowballs chance in hell of running a local AI and what model should I strive for even?&lt;/p&gt; &lt;p&gt;By the way I‚Äôve used Upscayl but it only works on the lowest possible setting. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AccordionPianist"&gt; /u/AccordionPianist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pskkhp/any_hope_for_my_linux_laptop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pskkhp/any_hope_for_my_linux_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pskkhp/any_hope_for_my_linux_laptop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T23:40:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps7bzx</id>
    <title>Opensource models less than 30b with highest edit-diff success rate</title>
    <updated>2025-12-21T14:08:11+00:00</updated>
    <author>
      <name>/u/Express_Quail_1493</name>
      <uri>https://old.reddit.com/user/Express_Quail_1493</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;currently I'm struggling to find one that has solid successful edit-diff consistency. devstral-small-2 is the only one that stays consistent for me but its not super smart as top contender. its a good enough model. qwen3-coder-30b keeps getting failing in their edit-diff attempts&lt;/p&gt; &lt;p&gt;what is your experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Quail_1493"&gt; /u/Express_Quail_1493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ps7bzx/opensource_models_less_than_30b_with_highest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ps7bzx/opensource_models_less_than_30b_with_highest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ps7bzx/opensource_models_less_than_30b_with_highest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T14:08:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt6u00</id>
    <title>Exclusive Holiday Offer! Perplexity AI PRO 1-Year Subscription ‚Äì Save 90%!</title>
    <updated>2025-12-22T18:22:15+00:00</updated>
    <author>
      <name>/u/A2uniquenickname</name>
      <uri>https://old.reddit.com/user/A2uniquenickname</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pt6u00/exclusive_holiday_offer_perplexity_ai_pro_1year/"&gt; &lt;img alt="Exclusive Holiday Offer! Perplexity AI PRO 1-Year Subscription ‚Äì Save 90%!" src="https://preview.redd.it/l7i791akss8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5880d65caad3ce59e1f5508d0383eef7130d534f" title="Exclusive Holiday Offer! Perplexity AI PRO 1-Year Subscription ‚Äì Save 90%!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get Perplexity AI PRO (1-Year) ‚Äì at 90% OFF!&lt;/p&gt; &lt;p&gt;Order here: &lt;a href="https://cheapgpts.store/Perplexity"&gt;CHEAPGPT.STORE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Plan: 12 Months&lt;/p&gt; &lt;p&gt;üí≥ Pay with: PayPal or Revolut or your favorite payment method&lt;/p&gt; &lt;p&gt;Reddit reviews: &lt;a href="https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu"&gt;FEEDBACK POST&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TrustPilot: &lt;a href="https://www.trustpilot.com/review/cheapgpt.store"&gt;TrustPilot FEEDBACK&lt;/a&gt; &lt;/p&gt; &lt;p&gt;NEW YEAR BONUS: Apply code PROMO5 for extra discount OFF your order! &lt;/p&gt; &lt;p&gt;BONUS!: Enjoy the AI Powered automated web browser. (Presented by Perplexity) included WITH YOUR PURCHASE!&lt;/p&gt; &lt;p&gt;Trusted and the cheapest! Check all feedbacks before you purchase&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A2uniquenickname"&gt; /u/A2uniquenickname &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l7i791akss8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pt6u00/exclusive_holiday_offer_perplexity_ai_pro_1year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pt6u00/exclusive_holiday_offer_perplexity_ai_pro_1year/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-22T18:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pske2v</id>
    <title>virtual pet / life simulation using Ollama and Unity 6</title>
    <updated>2025-12-21T23:32:22+00:00</updated>
    <author>
      <name>/u/rzarekta</name>
      <uri>https://old.reddit.com/user/rzarekta</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pske2v/virtual_pet_life_simulation_using_ollama_and/"&gt; &lt;img alt="virtual pet / life simulation using Ollama and Unity 6" src="https://external-preview.redd.it/eTdyYWZodTc2bjhnMXUjJLkZMiuDknoqj2U9nKzIooYTPd9cVtalvt_A-w2b.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc908aaaf303ce0483f79c3f1fb4417f43feaa87" title="virtual pet / life simulation using Ollama and Unity 6" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a virtual pet / life simulation in Unity 6, and it‚Äôs slowly turning into a living little ecosystem. This is a prototype, no fancy graphics or eye candy has been added. &lt;/p&gt; &lt;p&gt;Each creature is fully AI-driven, the AI controls all movement and decisions. They choose where to go, when to wander, when to eat, when to sleep, and when to interact. The green squares are food, and the purple rectangles are beds, which they seek out naturally based on their needs.&lt;/p&gt; &lt;p&gt;You can talk to the creatures individually, and they also talk amongst themselves. What you say to one creature can influence how it behaves and how it talks to others. Conversations aren‚Äôt isolated, they actually affect memory, mood, and social relationships.&lt;/p&gt; &lt;p&gt;You can also give direct commands like &lt;em&gt;stop&lt;/em&gt;, &lt;em&gt;go left&lt;/em&gt;, &lt;em&gt;go right&lt;/em&gt;, &lt;em&gt;follow&lt;/em&gt;, or &lt;em&gt;find another creature&lt;/em&gt;. The creatures don‚Äôt blindly obey, they evaluate each command based on personality, trust, current needs, and survival priorities, then respond honestly.&lt;/p&gt; &lt;p&gt;All AI logic and dialogue run fully locally using Ollama, on an RTX 2070 (8GB) AI server.&lt;/p&gt; &lt;p&gt;Watching emergent behavior form instead of scripting it has been wild.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzarekta"&gt; /u/rzarekta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9519oat76n8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pske2v/virtual_pet_life_simulation_using_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pske2v/virtual_pet_life_simulation_using_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-21T23:32:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt771o</id>
    <title>Prompt Injection demo in Ollama - help, please?</title>
    <updated>2025-12-22T18:36:00+00:00</updated>
    <author>
      <name>/u/West-Candy-5732</name>
      <uri>https://old.reddit.com/user/West-Candy-5732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, everyone. &lt;/p&gt; &lt;p&gt;I am working on my project for a Cybersecurity class and I would like to showcase the risks of Prompt Injection. I had this idea in my mind with many different things, but I wanted to actually start with something simple. However, even using small models like Phi3 or GPT2, I fail to actually override the system prompt (classic example of a translator agent, in my case English -&amp;gt; German), and get it to say &amp;quot;Haha, I got hacked!&amp;quot;. &lt;/p&gt; &lt;p&gt;Is there some prompt injection security in Ollama that I am not aware of? Can it be turned off?&lt;/p&gt; &lt;p&gt;Alternatively: do you guys have any better ideas how to demonstrate this? I tried using an API (Claude), but the results I got were not what I expected, quite quirky.&lt;/p&gt; &lt;p&gt;Thanks in advance for the help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/West-Candy-5732"&gt; /u/West-Candy-5732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pt771o/prompt_injection_demo_in_ollama_help_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pt771o/prompt_injection_demo_in_ollama_help_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pt771o/prompt_injection_demo_in_ollama_help_please/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-22T18:36:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt9tv0</id>
    <title>Title: Update: Yesterday it was 2D. Today, my Local Agent (Qwen 30B) figured out 3D Raycasting. Built from scratch in Python with no 3D engines.</title>
    <updated>2025-12-22T20:19:31+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pt9tv0/title_update_yesterday_it_was_2d_today_my_local/"&gt; &lt;img alt="Title: Update: Yesterday it was 2D. Today, my Local Agent (Qwen 30B) figured out 3D Raycasting. Built from scratch in Python with no 3D engines." src="https://external-preview.redd.it/ZDJjcXY5czlkdDhnMVpKx18zu2Al60haJHCIipoecy1_uH38KnrawZp01IuI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bbeb7e4bb3c56f42b0266cde29ca822530ff055" title="Title: Update: Yesterday it was 2D. Today, my Local Agent (Qwen 30B) figured out 3D Raycasting. Built from scratch in Python with no 3D engines." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following my previous post where the agent built a 2D tile engine, I pushed it to the next level: &lt;strong&gt;3D Raycasting.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Challenge:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create a Wolfenstein 3D style engine in pure Python (&lt;code&gt;pygame&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;No 3D libraries allowed, just raw math (Trigonometry).&lt;/li&gt; &lt;li&gt;Must handle wall collisions and perspective correction.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Result:&lt;/strong&gt; The agent (running on Qwen 30B via Ollama/LM Studio) successfully implemented the &lt;strong&gt;DDA Algorithm&lt;/strong&gt;. It initially struggled with a &amp;quot;barcode effect&amp;quot; and low FPS, but after a few autonomous feedback loops, it optimized the rendering to draw 4-pixel strips instead of single lines.&lt;/p&gt; &lt;p&gt;It also autonomously implemented &lt;strong&gt;Directional Shading&lt;/strong&gt; (lighter color for X-walls, darker for Y-walls) to give it that &amp;quot;Cyberpunk/Tron&amp;quot; depth.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/th2iyeo9dt8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pt9tv0/title_update_yesterday_it_was_2d_today_my_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pt9tv0/title_update_yesterday_it_was_2d_today_my_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-22T20:19:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptqyzh</id>
    <title>Local vs VPS...</title>
    <updated>2025-12-23T10:47:53+00:00</updated>
    <author>
      <name>/u/pagurix</name>
      <uri>https://old.reddit.com/user/pagurix</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pagurix"&gt; /u/pagurix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1ptp9dq/local_vs_vps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ptqyzh/local_vs_vps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ptqyzh/local_vs_vps/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-23T10:47:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptqvvh</id>
    <title>Ollama for 3D models</title>
    <updated>2025-12-23T10:42:12+00:00</updated>
    <author>
      <name>/u/Digital_Calendar_695</name>
      <uri>https://old.reddit.com/user/Digital_Calendar_695</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ptqvvh/ollama_for_3d_models/"&gt; &lt;img alt="Ollama for 3D models" src="https://external-preview.redd.it/zj7DSc3w-zwxzS2_x9K_PvO2eD7C1IjPQMGbnhyQXVU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e0fb3cbb460579fb92b2abb3e828cb0973f7f48" title="Ollama for 3D models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have check this video using local LLMs to create 3D models in Blender?&lt;/p&gt; &lt;p&gt;It seems small models cannot handle many tasks Has anyone tried bigger local models with MCP like this one?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Digital_Calendar_695"&gt; /u/Digital_Calendar_695 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/0PSOCFHBAfw?si=eDYokRcNPD5iYDBL"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ptqvvh/ollama_for_3d_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ptqvvh/ollama_for_3d_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-23T10:42:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1puhxd4</id>
    <title>Which is the smallest, fastest text generation model on ollama that can be used as a ai friend?</title>
    <updated>2025-12-24T07:52:32+00:00</updated>
    <author>
      <name>/u/Status_Yam_9212</name>
      <uri>https://old.reddit.com/user/Status_Yam_9212</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to have my own friend, somewhat similar to &lt;a href="http://c.ai"&gt;c.ai&lt;/a&gt;, but smaller, faster, and can run locally and fully offline. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Status_Yam_9212"&gt; /u/Status_Yam_9212 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puhxd4/which_is_the_smallest_fastest_text_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puhxd4/which_is_the_smallest_fastest_text_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1puhxd4/which_is_the_smallest_fastest_text_generation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T07:52:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu409q</id>
    <title>Ollama not outputing for Qwen3 80B Next Instruct, but works for Thinking model. Nothing in log.</title>
    <updated>2025-12-23T20:19:39+00:00</updated>
    <author>
      <name>/u/vulcan4d</name>
      <uri>https://old.reddit.com/user/vulcan4d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a weird issue where Ollama does not give me any output for Gwen3 Next 80B Instruct though it gives me token results. I see the same thing running in terminal. When I pull up the log I don't see anything useful. Anyone come accross something like this? Everything is on the latest version. I tried Q4 down to Q2 Quants, but the thinking version of this model works without any issues.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/27ooi0og209g1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55579ada7461fa7258cc1c6a908111b1fb957005"&gt;https://preview.redd.it/27ooi0og209g1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55579ada7461fa7258cc1c6a908111b1fb957005&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The log shows absolutely nothing useful&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ts6lb8t7309g1.png?width=1341&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84785ddb224466e38803a10a37f8d05bab3c08d7"&gt;Running from Open WebUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j9ujcugk309g1.png?width=1351&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b31d610451aa2550cba448960ec82e2c6b09c22"&gt;Running locally via terminal&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vulcan4d"&gt; /u/vulcan4d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pu409q/ollama_not_outputing_for_qwen3_80b_next_instruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pu409q/ollama_not_outputing_for_qwen3_80b_next_instruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pu409q/ollama_not_outputing_for_qwen3_80b_next_instruct/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-23T20:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pufqor</id>
    <title>DOOM JS: Master Protocol - The Power of 392 AI Patterns</title>
    <updated>2025-12-24T05:41:26+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pufqor/doom_js_master_protocol_the_power_of_392_ai/"&gt; &lt;img alt="DOOM JS: Master Protocol - The Power of 392 AI Patterns" src="https://external-preview.redd.it/Njk4ZnFsaDFhMzlnMQpHkqJe4EhnCoJ9VzNKO0zpC9YcnnCThFB-jTIXDZe8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=150cef2e178fc764e57e5aa784849834c45602bc" title="DOOM JS: Master Protocol - The Power of 392 AI Patterns" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This Christmas release represents a breakthrough in AI-driven development. By merging the collective intelligence of DeepSeek, Claude, and Perplexity into a library of 400 &lt;strong&gt;learned patterns&lt;/strong&gt;, I have eliminated random guessing and hallucinations.&lt;/p&gt; &lt;p&gt;What you see is a strictly governed horror engine:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Atmosphere:&lt;/strong&gt; Deep black background (0x000000) with calibrated fog layers for maximum tension.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Physics:&lt;/strong&gt; Hard-locked 1.6m eye-level gravity and relative FPS movement protocols.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI:&lt;/strong&gt; Aggressive yellow entities using unified chasing logic.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No more blind attempts. Just pure, structured execution. The AI is finally learning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vcabkr81a39g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pufqor/doom_js_master_protocol_the_power_of_392_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pufqor/doom_js_master_protocol_the_power_of_392_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T05:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1puguld</id>
    <title>ollama cannot run the model on Mac.</title>
    <updated>2025-12-24T06:45:59+00:00</updated>
    <author>
      <name>/u/Ok-Money-9173</name>
      <uri>https://old.reddit.com/user/Ok-Money-9173</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Metal library compilation error after macOS 26.2 / Xcode CLT update: bfloat/half type mismatch&lt;/p&gt; &lt;p&gt;Has anyone encountered the same error?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Money-9173"&gt; /u/Ok-Money-9173 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puguld/ollama_cannot_run_the_model_on_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puguld/ollama_cannot_run_the_model_on_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1puguld/ollama_cannot_run_the_model_on_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T06:45:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1puhr3d</id>
    <title>Now you can run local LLM inference with formal privacy guarantees</title>
    <updated>2025-12-24T07:41:27+00:00</updated>
    <author>
      <name>/u/IIITDkaLaunda</name>
      <uri>https://old.reddit.com/user/IIITDkaLaunda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1puhr3d/now_you_can_run_local_llm_inference_with_formal/"&gt; &lt;img alt="Now you can run local LLM inference with formal privacy guarantees" src="https://preview.redd.it/fb8lnvwns39g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6c3eb438356f4e3dd467972106afdcdbacf09b5" title="Now you can run local LLM inference with formal privacy guarantees" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IIITDkaLaunda"&gt; /u/IIITDkaLaunda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fb8lnvwns39g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puhr3d/now_you_can_run_local_llm_inference_with_formal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1puhr3d/now_you_can_run_local_llm_inference_with_formal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T07:41:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pun56y</id>
    <title>Which GPU should I use to caption ~50k images/day</title>
    <updated>2025-12-24T13:15:55+00:00</updated>
    <author>
      <name>/u/koteklidkapi</name>
      <uri>https://old.reddit.com/user/koteklidkapi</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koteklidkapi"&gt; /u/koteklidkapi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pun4kk/which_gpu_should_i_use_to_caption_50k_imagesday/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pun56y/which_gpu_should_i_use_to_caption_50k_imagesday/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pun56y/which_gpu_should_i_use_to_caption_50k_imagesday/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T13:15:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1puskwn</id>
    <title>Writing custom code to connect to llm api via Ollama and mTLS?</title>
    <updated>2025-12-24T17:27:18+00:00</updated>
    <author>
      <name>/u/Patladjan1738</name>
      <uri>https://old.reddit.com/user/Patladjan1738</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I am pretty new to Ollama and wanted to test it out, but I'm not sure if it can support my use case.&lt;/p&gt; &lt;p&gt;I have my own setup of an LLM API, running on a private server and secured via mTLS, so not just an api key but an api Id, a secret password, and I have to send a certificate and private key file in the payload. &lt;/p&gt; &lt;p&gt;I want to set up tools like langflow and dyad, but they dont seem to easily support all my custom auth code with cert and private key files. &lt;/p&gt; &lt;p&gt;But langflow and dyad do easily connect to Ollama.&lt;/p&gt; &lt;p&gt;Now I am thinking of setting up Ollama as a proxy server, where I can easily connect tools to Ollama, then Ollama can basically run my custom Python code to connect to my private llm server.&lt;/p&gt; &lt;p&gt;Has anyone ever done this with Ollama? Does anyone know if it's possible? What part of the documentation should I look into to kick start my implementation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Patladjan1738"&gt; /u/Patladjan1738 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puskwn/writing_custom_code_to_connect_to_llm_api_via/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puskwn/writing_custom_code_to_connect_to_llm_api_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1puskwn/writing_custom_code_to_connect_to_llm_api_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T17:27:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu6sgl</id>
    <title>I built a native Go runtime to give local Llama 3 "Real Hands" (File System + Browser)</title>
    <updated>2025-12-23T22:17:40+00:00</updated>
    <author>
      <name>/u/AgencySpecific</name>
      <uri>https://old.reddit.com/user/AgencySpecific</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Frustration: Running DeepSeek V3 or Llama 3 locally via Ollama is amazing, but let's be honest: they are &amp;quot;Brains in Jars.&amp;quot;&lt;/p&gt; &lt;p&gt;They can write incredible code, but they can't save it. They can plan research, but they can't browse the docs. I got sick of the &amp;quot;Chat -&amp;gt; Copy Code -&amp;gt; Alt-Tab -&amp;gt; Paste -&amp;gt; Error&amp;quot; loop.&lt;/p&gt; &lt;p&gt;The Project (Runiq): I didn't want another fragile Python wrapper that breaks my venv every week. So I built a standalone MCP Server in Go.&lt;/p&gt; &lt;p&gt;What it actually does:&lt;/p&gt; &lt;p&gt;File System Access: You prompt: &amp;quot;Refactor the ./src folder.&amp;quot; Runiq actually reads the files, sends the context to Ollama, and applies the edits locally.&lt;/p&gt; &lt;p&gt;Stealth Browser: You prompt: &amp;quot;Check the docs at stripe.com.&amp;quot; It spins up a headless browser (bypassing Cloudflare) to give the model real-time context.&lt;/p&gt; &lt;p&gt;The &amp;quot;Air Gap&amp;quot; Firewall: Giving a local model root is scary. Runiq intercepts every write or delete syscall. You get a native OS popup to approve the action. It can't wipe your drive unless you say yes.&lt;/p&gt; &lt;p&gt;Why Go?&lt;/p&gt; &lt;p&gt;Speed: It's instant.&lt;/p&gt; &lt;p&gt;Portability: Single 12MB binary. No pip install, no Docker.&lt;/p&gt; &lt;p&gt;Safety: Memory safe and strictly typed.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/qaysSE/runiq"&gt;https://github.com/qaysSE/runiq&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I built this to turn my local Ollama setup into a fully autonomous agent. Let me know what you think of the architecture.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AgencySpecific"&gt; /u/AgencySpecific &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pu6sgl/i_built_a_native_go_runtime_to_give_local_llama_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pu6sgl/i_built_a_native_go_runtime_to_give_local_llama_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pu6sgl/i_built_a_native_go_runtime_to_give_local_llama_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-23T22:17:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1puzyql</id>
    <title>Meetaugust Scored 100% in USMLE : outperforming OpenAI‚Äôs GPT - 5 and Google MedPaLM 2.</title>
    <updated>2025-12-24T23:19:33+00:00</updated>
    <author>
      <name>/u/vaibhavs8</name>
      <uri>https://old.reddit.com/user/vaibhavs8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1puzyql/meetaugust_scored_100_in_usmle_outperforming/"&gt; &lt;img alt="Meetaugust Scored 100% in USMLE : outperforming OpenAI‚Äôs GPT - 5 and Google MedPaLM 2." src="https://b.thumbs.redditmedia.com/tNks7nCpb3Q-gpPoKucKZ6jijHDMBzY_lgT9GPbdZcY.jpg" title="Meetaugust Scored 100% in USMLE : outperforming OpenAI‚Äôs GPT - 5 and Google MedPaLM 2." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i spent 3 years building Meetaugust and published research on benchmarking health AI accuracy. The goal was simple: make reliable health guidance accessible to anyone.&lt;/p&gt; &lt;p&gt;I know there are a lots of symptom checkers and health apps out there but most are not safe. I wanted something safe and conversational just explain your symptoms naturally and get clear answers.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;p&gt;* Analyzes symptoms through natural conversation (no checkboxes)&lt;/p&gt; &lt;p&gt;* Explains lab reports and prescriptions in simple terms&lt;/p&gt; &lt;p&gt;* Works in multiple languages via WhatsApp also (photos, voice, text)&lt;/p&gt; &lt;p&gt;* Helps determine if something needs urgent attention&lt;/p&gt; &lt;p&gt;* Stores your medical history as a &amp;quot;second brain&amp;quot;&lt;/p&gt; &lt;p&gt;* Available 24/7 for health questions&lt;/p&gt; &lt;p&gt;It won't prescribe medicines it's meant to help you understand your health and know when to see a doctor. We achieved 81.8% diagnostic accuracy in our research testing across 400 clinical cases.&lt;/p&gt; &lt;p&gt;free if anyone wants to try it : &lt;a href="https://www.meetaugust.ai/"&gt;https://www.meetaugust.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs8"&gt; /u/vaibhavs8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1puzyql"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1puzyql/meetaugust_scored_100_in_usmle_outperforming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1puzyql/meetaugust_scored_100_in_usmle_outperforming/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T23:19:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pugkbg</id>
    <title>Self Hosted Alternative to NotebookLM</title>
    <updated>2025-12-24T06:28:53+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pugkbg/self_hosted_alternative_to_notebooklm/"&gt; &lt;img alt="Self Hosted Alternative to NotebookLM" src="https://external-preview.redd.it/VQoBiFueOCMY1op6qhV-TxY7TpiBx_VDJmILmMOmfX0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ea68aad29b25cc93508c57524884674c64e162b" title="Self Hosted Alternative to NotebookLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1pugkbg/video/939ag7c3j39g1/player"&gt;https://reddit.com/link/1pugkbg/video/939ag7c3j39g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be one of the open-source alternative to NotebookLM but connected to extra data sources.&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here's a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep Agent with Built-in Tools (knowledge base search, podcast generation, web scraping, link previews, image display)&lt;/li&gt; &lt;li&gt;Note Management (Notion like)&lt;/li&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi Collaborative Chats&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Installation (Self-Host)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Linux/macOS:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 \ -v surfsense-data:/data \ --name surfsense \ --restart unless-stopped \ ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Windows (PowerShell):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 ` -v surfsense-data:/data ` --name surfsense ` --restart unless-stopped ` ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pugkbg/self_hosted_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pugkbg/self_hosted_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pugkbg/self_hosted_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T06:28:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pulykd</id>
    <title>Qwen3:4b Too Many Model thoughts to respond to a simple "hi"</title>
    <updated>2025-12-24T12:10:03+00:00</updated>
    <author>
      <name>/u/slow-fast-person</name>
      <uri>https://old.reddit.com/user/slow-fast-person</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pulykd/qwen34b_too_many_model_thoughts_to_respond_to_a/"&gt; &lt;img alt="Qwen3:4b Too Many Model thoughts to respond to a simple &amp;quot;hi&amp;quot;" src="https://preview.redd.it/nfzkw0ex759g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8cdeebcba38e88484dad342114e070ce2c9b6c93" title="Qwen3:4b Too Many Model thoughts to respond to a simple &amp;quot;hi&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is quite hilarious on how the model does not have adaptive chain of thought and puts so much work in something as simple as a &amp;quot;hi&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slow-fast-person"&gt; /u/slow-fast-person &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nfzkw0ex759g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pulykd/qwen34b_too_many_model_thoughts_to_respond_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pulykd/qwen34b_too_many_model_thoughts_to_respond_to_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-24T12:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv71pg</id>
    <title>Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16.</title>
    <updated>2025-12-25T06:23:45+00:00</updated>
    <author>
      <name>/u/Double-Primary-2871</name>
      <uri>https://old.reddit.com/user/Double-Primary-2871</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pv71pg/finetuning_gptoss20b_on_a_ryzen_5950x_because/"&gt; &lt;img alt="Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16." src="https://preview.redd.it/6w9h1554na9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2621fb072eeb16ce8a0f4599cdbbfeb44b9f1c90" title="Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;at 1am.&lt;/p&gt; &lt;p&gt;I am fine-tuning my personal AI, into a gpt-oss-20b model, via LoRA, on a Ryzen 5950x CPU.&lt;/p&gt; &lt;p&gt;I had to pain stakingly deal with massive axolotl errors, venv and python version hell, yaml misconfigs, even fought with my other ai assistant, whom literally told me this couldn't be done on my system.... for hours and hours, for over a week.&lt;/p&gt; &lt;p&gt;Can't fine-tune with my radeon 7900XT because of bf16 kernel issues with ROCm on axolotl. I literally even tried to rent an h100 to help, and ran into serious roadblocks.&lt;/p&gt; &lt;p&gt;So the soultion was for me to convert the mxfp4 (bf16 format) weights back to fp32 and tell axolotl to stop downcasting back fp16.&lt;/p&gt; &lt;p&gt;Sure this will take days to compute all three of the shards, but after days of banging my head against the nearest convenient wall and keyboard, I finally got this s-o-b to work.&lt;/p&gt; &lt;p&gt;üòÅ also hi, new here. just wanted to share my story.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Double-Primary-2871"&gt; /u/Double-Primary-2871 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6w9h1554na9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv71pg/finetuning_gptoss20b_on_a_ryzen_5950x_because/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pv71pg/finetuning_gptoss20b_on_a_ryzen_5950x_because/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T06:23:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv88yv</id>
    <title>I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier models</title>
    <updated>2025-12-25T07:44:21+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pv88yv/i_built_planoa3b_most_efficient_llms_for_agent/"&gt; &lt;img alt="I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier models" src="https://preview.redd.it/059dttgf1b9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=898f1ffead5f76ea7c739cebaf5d8c1a413e3efc" title="I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone ‚Äî I‚Äôm on the Katanemo research team. Today we‚Äôre thrilled to launch &lt;strong&gt;Plano-Orchestrator&lt;/strong&gt;, a new family of LLMs built for fast multi-agent orchestration.&lt;/p&gt; &lt;p&gt;What do these new LLMs do? given a user request and the conversation context, Plano-Orchestrator decides which agent(s) should handle the request and in what sequence. In other words, it acts as the supervisor agent in a multi-agent system. Designed for multi-domain scenarios, it works well across general chat, coding tasks, and long, multi-turn conversations, while staying efficient enough for low-latency production deployments.&lt;/p&gt; &lt;p&gt;Why did we built this? Our applied research is focused on helping teams deliver agents safely and efficiently, with better real-world performance and latency ‚Äî the kind of ‚Äúglue work‚Äù that usually sits outside any single agent‚Äôs core product logic.&lt;/p&gt; &lt;p&gt;Plano-Orchestrator is integrated into Plano, our models-native proxy and dataplane for agents. Hope you enjoy it ‚Äî and we‚Äôd love feedback from anyone building multi-agent systems&lt;/p&gt; &lt;p&gt;Learn more about the LLMs &lt;a href="https://huggingface.co/collections/katanemo/plano-orchestrator"&gt;here&lt;/a&gt;&lt;br /&gt; About our open source project: &lt;a href="https://github.com/katanemo/plano"&gt;https://github.com/katanemo/plano&lt;/a&gt;&lt;br /&gt; And about our research: &lt;a href="https://planoai.dev/research"&gt;https://planoai.dev/research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/059dttgf1b9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv88yv/i_built_planoa3b_most_efficient_llms_for_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pv88yv/i_built_planoa3b_most_efficient_llms_for_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T07:44:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv33vq</id>
    <title>Llama 3.2 refuses to analyze dark web threat intel. Need uncensored 7B recommendations</title>
    <updated>2025-12-25T02:19:49+00:00</updated>
    <author>
      <name>/u/Loud-Goal190</name>
      <uri>https://old.reddit.com/user/Loud-Goal190</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm crawling onion sites for a defensive threat intel tool, but my local LLM (Llama 3.2) refuses to analyze the raw text due to safety filters. It sees &amp;quot;leak&amp;quot; or &amp;quot;.onion&amp;quot; and shuts down, even with jailbreak prompts. Regex captures emails but misses the context (like company names or data volume). Any recommendations for an uncensored 7B model that handles this well, or should I switch to a BERT model for extraction?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud-Goal190"&gt; /u/Loud-Goal190 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv33vq/llama_32_refuses_to_analyze_dark_web_threat_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pv33vq/llama_32_refuses_to_analyze_dark_web_threat_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pv33vq/llama_32_refuses_to_analyze_dark_web_threat_intel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-25T02:19:49+00:00</published>
  </entry>
</feed>
