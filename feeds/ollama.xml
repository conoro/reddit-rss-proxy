<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-27T17:14:59+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1reeixj</id>
    <title>From Pikachu to ZYRON: We Built a Fully Local AI Desktop Assistant That Runs Completely Offline</title>
    <updated>2026-02-25T14:22:10+00:00</updated>
    <author>
      <name>/u/No-Mess-8224</name>
      <uri>https://old.reddit.com/user/No-Mess-8224</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago I posted here about a small personal project I was building called Pikachu, a local desktop voice assistant. Since then the project has grown way bigger than I expected, got contributions from some really talented people, and evolved into something much more serious. We renamed it to ZYRON and it has basically turned into a full local AI desktop assistant that runs entirely on your own machine.&lt;/p&gt; &lt;p&gt;The main goal has always been simple. I love the idea of AI assistants, but I hate the idea of my files, voice, screenshots, and daily computer activity being uploaded to cloud services. So we built the opposite. ZYRON runs fully offline using a local LLM through Ollama, and the entire system is designed around privacy first. Nothing gets sent anywhere unless I explicitly ask it to send something to my own Telegram.&lt;/p&gt; &lt;p&gt;You can control the PC with voice by saying a wake word and then speaking normally. It can open apps, control media, set volume, take screenshots, shut down the PC, search the web in the background, and run chained commands like opening a browser and searching something in one go. It also responds back using offline text to speech, which makes it feel surprisingly natural to use day to day.&lt;/p&gt; &lt;p&gt;The remote control side became one of the most interesting parts. From my phone I can message a Telegram bot and basically control my laptop from anywhere. If I forget a file, I can ask it to find the document I opened earlier and it sends the file directly to me. It keeps a 30 day history of file activity and lets me search it using natural language. That feature alone has already saved me multiple times.&lt;/p&gt; &lt;p&gt;We also leaned heavily into security and monitoring. ZYRON can silently capture screenshots, take webcam photos, record short audio clips, and send them to Telegram. If a laptop gets stolen and connects to the internet, it can report IP address, ISP, city, coordinates, and a Google Maps link. Building and testing that part honestly felt surreal the first time it worked.&lt;/p&gt; &lt;p&gt;On the productivity side it turned into a full system monitor. It can report CPU, RAM, battery, storage, running apps, and even read all open browser tabs. There is a clipboard history logger so copied text is never lost. There is a focus mode that kills distracting apps and closes blocked websites automatically. There is even a ‚Äúzombie process‚Äù monitor that detects apps eating RAM in the background and lets you kill them remotely.&lt;/p&gt; &lt;p&gt;One feature I personally love is the stealth research mode. There is a Firefox extension that creates a bridge between the browser and the assistant, so it can quietly open a background tab, read content, and close it without any window appearing. Asking random questions and getting answers from a laptop that looks idle is strangely satisfying.&lt;/p&gt; &lt;p&gt;The whole philosophy of the project is that it does not try to compete with giant cloud models at writing essays. Instead it focuses on being a powerful local system automation assistant that respects privacy. The local model is smaller, but for controlling a computer it is more than enough, and the tradeoff feels worth it.&lt;/p&gt; &lt;p&gt;We are planning a lot next. Linux and macOS support, geofence alerts, motion triggered camera capture, scheduling and automation, longer memory, and eventually a proper mobile companion app instead of Telegram. As local models improve, the assistant will naturally get smarter too.&lt;/p&gt; &lt;p&gt;This started as a weekend experiment and slowly turned into something I now use daily. I would genuinely love feedback, ideas, or criticism from people here. If you have ever wanted an AI assistant that lives only on your own machine, I think you might find this interesting.&lt;/p&gt; &lt;p&gt;GitHub Repo - &lt;a href="https://github.com/Surajkumar5050/zyron-assistant"&gt;Link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mess-8224"&gt; /u/No-Mess-8224 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T14:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1res0sl</id>
    <title>Qwen 3.5 distilled vs GptOss</title>
    <updated>2026-02-25T22:25:42+00:00</updated>
    <author>
      <name>/u/SubstantialTea707</name>
      <uri>https://old.reddit.com/user/SubstantialTea707</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tried Qwen 3.5 in the 27B, 35B, or 122B versions? How does it perform with tool calling? I‚Äôm currently using GPT-OSS 20B, but especially the 120B version is, in my opinion, unbeatable. I find it very reliable and I‚Äôm running it in production on an RTX Pro 6000. With Qwen 3, I experienced lower reliability and it often went into loops, which made it unsuitable for production. Has anyone already tested it? Could you share real-world usage feedback? Because, as we know, benchmarks don‚Äôt always reflect real use cases. My goal is to run a chatbot with RAG and MCP tool calling.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SubstantialTea707"&gt; /u/SubstantialTea707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1res0sl/qwen_35_distilled_vs_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1res0sl/qwen_35_distilled_vs_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1res0sl/qwen_35_distilled_vs_gptoss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T22:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf4k33</id>
    <title>AMBER ICI v3</title>
    <updated>2026-02-26T08:14:00+00:00</updated>
    <author>
      <name>/u/FreonMuskOfficial</name>
      <uri>https://old.reddit.com/user/FreonMuskOfficial</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rf4k33/amber_ici_v3/"&gt; &lt;img alt="AMBER ICI v3" src="https://preview.redd.it/fmd5rvv6sslg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35169090d9acfccbc8e8bc9ebe455a8e771b7397" title="AMBER ICI v3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Ollama locally and want more than just prompts? &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/gs-ai/AMBER-ICI"&gt;https://github.com/gs-ai/AMBER-ICI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AMBER ICI v3 is an industrial grade local command center built for serious builders, giving you multi model orchestration, live token streaming, agent and chain pipelines, archive search, timeline reconstruction, OCR extraction, investigative file ingestion, graph based output correlation, and real time GPU telemetry in one unified interface. It is designed for power users who want to spin up autonomous agents, chain models together, ingest massive archives, and actually see how models think and interact, all while it's fully local and you're in control. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FreonMuskOfficial"&gt; /u/FreonMuskOfficial &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fmd5rvv6sslg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf4k33/amber_ici_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf4k33/amber_ici_v3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T08:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf61qd</id>
    <title>Made my first project, Autonomous video generator</title>
    <updated>2026-02-26T09:49:00+00:00</updated>
    <author>
      <name>/u/Pronation1227</name>
      <uri>https://old.reddit.com/user/Pronation1227</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Pronation1227/AVB#video-generator-bot"&gt;&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Hi, This is my first project (which i actually managed to complete)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/Pronation1227/AVB#hi-this-is-my-first-project-which-i-actually-managed-to-complete"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;About me: I am in high school and have been coding on and off for a few years now.&lt;/p&gt; &lt;p&gt;a quick overview of this project, its basically a storytime generator inspired from the insta videos you see on reels. There was no real motive behind building this i was just frustrated of tutorial hell and hence built the first thing that came to my mind&lt;/p&gt; &lt;p&gt;I admit i did use AI to help me with structuring the project into different files ie: output, notes, background, scripts. I also used ai for the ffmpeg subprocess in generate_vid.py as i had no idea what ffmpeg is or how to use it. But all other lines of code in all the files have been written by me&lt;/p&gt; &lt;p&gt;Thanks a lot, would really appreciate feedback on what could i improve and where can i learn further.&lt;/p&gt; &lt;p&gt;github - &lt;a href="https://github.com/Pronation1227/AVB"&gt;https://github.com/Pronation1227/AVB&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pronation1227"&gt; /u/Pronation1227 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf61qd/made_my_first_project_autonomous_video_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf61qd/made_my_first_project_autonomous_video_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf61qd/made_my_first_project_autonomous_video_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T09:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf6cc7</id>
    <title>Need help on API key export...</title>
    <updated>2026-02-26T10:07:15+00:00</updated>
    <author>
      <name>/u/Dakacchan_</name>
      <uri>https://old.reddit.com/user/Dakacchan_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rf6cc7/need_help_on_api_key_export/"&gt; &lt;img alt="Need help on API key export..." src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="Need help on API key export..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dakacchan_"&gt; /u/Dakacchan_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1rf6c6u/need_help_on_api_key_export/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf6cc7/need_help_on_api_key_export/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf6cc7/need_help_on_api_key_export/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T10:07:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfbew8</id>
    <title>Question about installing ollama Claude</title>
    <updated>2026-02-26T14:21:03+00:00</updated>
    <author>
      <name>/u/PerformerAromatic836</name>
      <uri>https://old.reddit.com/user/PerformerAromatic836</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i am installing the ollama/claude local thing. The last question of the claude config is the question of &amp;quot;do you trust this folder&amp;quot;, it asks for access to my entire users/myname folder, whilst I don't want that. Is there a way to give claude/ollama only access to a certain folder with zero documents in it yet, so that claude/ollama will not have access to any personal documents?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerformerAromatic836"&gt; /u/PerformerAromatic836 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfbew8/question_about_installing_ollama_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfbew8/question_about_installing_ollama_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfbew8/question_about_installing_ollama_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T14:21:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfd6nn</id>
    <title>What do think about my setup?</title>
    <updated>2026-02-26T15:30:02+00:00</updated>
    <author>
      <name>/u/d4mations</name>
      <uri>https://old.reddit.com/user/d4mations</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/d4mations"&gt; /u/d4mations &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1rfat60/what_do_think_about_my_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfd6nn/what_do_think_about_my_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfd6nn/what_do_think_about_my_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T15:30:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf84qc</id>
    <title>AVCI GHOST - A CyberSec. UI Experiment for Ollama</title>
    <updated>2026-02-26T11:51:06+00:00</updated>
    <author>
      <name>/u/Ezanyiyenler</name>
      <uri>https://old.reddit.com/user/Ezanyiyenler</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"&gt; &lt;img alt="AVCI GHOST - A CyberSec. UI Experiment for Ollama" src="https://external-preview.redd.it/gAWqWQOlQP9fUw6_szN7BIGdnEqcLZpbQVa7NXlrTNU.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=a1d79284496f099470f492d9512e78cdedff357f" title="AVCI GHOST - A CyberSec. UI Experiment for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I created this simple UI for Ollama as a learning experiment. It's basically a Python script with 10 menu options that interact with local models. IMPORTAT DISCLAIMER:&lt;/p&gt; &lt;p&gt;This is NOT a real hacking tool. It's purely a UI EXPERIMENT. Most modules are SIMULATIONS and don't actually work. It's just a proof-of-concept I built while learning about Ollama and AI models.&lt;br /&gt; For open source code and detailed information, check out my GitHub address &lt;a href="https://github.com/ihsan896/Avci_Ghost"&gt;https://github.com/ihsan896/Avci_Ghost&lt;/a&gt;Don't forget to like if you enjoy üíó&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hysx7ys8utlg1.png?width=981&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=970ec0db987dd8c39ec8d4ac00ba4d07af2f5357"&gt;Just the UI layout - the modules are placeholders/demos. The actual functionality is connecting to local Ollama models. All security features are simulated for educational purposes.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ezanyiyenler"&gt; /u/Ezanyiyenler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T11:51:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1rffwgd</id>
    <title>Ollama model response very slow</title>
    <updated>2026-02-26T17:08:39+00:00</updated>
    <author>
      <name>/u/CookieClicker999</name>
      <uri>https://old.reddit.com/user/CookieClicker999</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm completely new to the world of AI (except for using some copilot m365 chat client). I'm trying to achieve some assistance in coding Python/JS/TS. I've followed some online instructions, installed Ollama using brew on my MBP M2 Max 64GB of ram. I've tried the glm-4.7-flash as it was highly regarded for correctness and had a nice context size according to some searches. When trying to query this model in the ollama client or using Jetbrains IDe i'm waiting for responses for more than 15 minutes usually. I've got a feeling i'm doing something wrong. It's using about 18GB of ram which is fine in my environment with lots to spare. Is there anyone able to give me som guidence as where to start looking into the issue or can recommend models that should be a better fit? I don't mind waiting for a minute but this is absolutely insane. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CookieClicker999"&gt; /u/CookieClicker999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rffwgd/ollama_model_response_very_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rffwgd/ollama_model_response_very_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rffwgd/ollama_model_response_very_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T17:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfgazl</id>
    <title>Ollama Model Search</title>
    <updated>2026-02-26T17:23:07+00:00</updated>
    <author>
      <name>/u/yukonit</name>
      <uri>https://old.reddit.com/user/yukonit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there an API that allows me to search models on Ollama, similar to searching via the Hugging Face API? For example, when I search for ‚Äúqwen,‚Äù is there an API endpoint that returns results related to ‚Äúqwen,‚Äù like the search on the Ollama website?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yukonit"&gt; /u/yukonit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfgazl/ollama_model_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfgazl/ollama_model_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfgazl/ollama_model_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T17:23:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfbrnh</id>
    <title>wired up ollama to contextui today</title>
    <updated>2026-02-26T14:35:34+00:00</updated>
    <author>
      <name>/u/Sharp-Mouse9049</name>
      <uri>https://old.reddit.com/user/Sharp-Mouse9049</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;messed around with ContextUI today. looks like it integrates with huggingface, but i just wired it to download an ollama model and run it direct instead. built a small local workflow pretty quickly. noticed it's open source now too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sharp-Mouse9049"&gt; /u/Sharp-Mouse9049 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfbrnh/wired_up_ollama_to_contextui_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfbrnh/wired_up_ollama_to_contextui_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfbrnh/wired_up_ollama_to_contextui_today/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T14:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rew6hl</id>
    <title>A fully visual, private and local AI Creative Suite. No cloud, no subscriptions, runs on your hardware.</title>
    <updated>2026-02-26T01:10:32+00:00</updated>
    <author>
      <name>/u/Ollie_IDE</name>
      <uri>https://old.reddit.com/user/Ollie_IDE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We‚Äôve been working on a desktop application for those of us who want to use AI models locally but prefer a full visual interface. It‚Äôs called Ollie, and it‚Äôs an offline-first creative suite that runs entirely on your machine.&lt;/p&gt; &lt;p&gt;What it includes:&lt;/p&gt; &lt;p&gt;Code: A coding environment (Node, Python, Java) with IntelliSense. You can also ask it to instantly generate and run interactive apps.&lt;/p&gt; &lt;p&gt;Media Suite: It has built-in video, image canvas, and a 3D editor.&lt;/p&gt; &lt;p&gt;Rich Text: A distraction-free markdown and writing environment that keeps your project context local.&lt;/p&gt; &lt;p&gt;Under the hood&lt;/p&gt; &lt;p&gt;Ollama Native: Hooks directly into your local Ollama setup.&lt;/p&gt; &lt;p&gt;Bring Your Own Keys: If you need to use Anthropic, Gemini, or OpenAI, you can plug in your API key directly.&lt;/p&gt; &lt;p&gt;Agent &amp;amp; MCP Support: Connects to GitHub, local databases, and custom tools via the Model Context Protocol (MCP).&lt;/p&gt; &lt;p&gt;&amp;quot;Glass-Box&amp;quot; UI: You can visually audit every file touch, tool call, and token before the AI executes it.&lt;/p&gt; &lt;p&gt;It runs natively on macOS, Windows, and Linux. Because it relies on your hardware, there are no recurring subscriptions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://costa-and-associates.com/ollie"&gt;Ollie&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ollie_IDE"&gt; /u/Ollie_IDE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rew6hl/a_fully_visual_private_and_local_ai_creative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rew6hl/a_fully_visual_private_and_local_ai_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rew6hl/a_fully_visual_private_and_local_ai_creative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T01:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfjraq</id>
    <title>Tools won't execute when running Claude Code in Docker against Ollama locally</title>
    <updated>2026-02-26T19:27:14+00:00</updated>
    <author>
      <name>/u/crispyghost</name>
      <uri>https://old.reddit.com/user/crispyghost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Claude Code running inside a docker container, it's pointed to Ollama running in another container. Claude can't execute tools.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This seems like it could be an Ollama or qwen3-coder issue. If I disable these env vars and log in with Anthropic, claude code in my container is talking with its servers and tool commands execute properly.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # Local Ollama (Windows Docker Desktop host) #ANTHROPIC_BASE_URL: &amp;quot;http://host.docker.internal:11434&amp;quot; #ANTHROPIC_AUTH_TOKEN: &amp;quot;ollama&amp;quot; #ANTHROPIC_DEFAULT_HAIKU_MODEL: &amp;quot;command-r7b&amp;quot; #ANTHROPIC_DEFAULT_SONNET_MODEL: &amp;quot;qwen3-coder&amp;quot; #ANTHROPIC_DEFAULT_OPUS_MODEL: &amp;quot;qwen3-coder&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm on Ollama 0.17.0, so this should be supported.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Original:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I use these commands to get in into claude:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;docker compose up --build&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;docker compose exec claude-code bash&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;claude&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;After that, I can chat with Claude using the Ollama service; that's working fine. &amp;quot;Tell me a joke&amp;quot; and I get a response. As soon as I ask Claude to do something that requires running a tool, I get this sort of output and it stops.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚ùØ Run ls -la and then summarize functional-spec.md. ‚óè I'll run ls -la to see the files in the workspace, then summarize the functional-spec.md file. &amp;lt;function=run&amp;gt; &amp;lt;parameter=command&amp;gt; ls -la &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm reading that this is &amp;quot;ollama style&amp;quot; tool commands, but Claude wants Anthropic-style tool commands. I'm not sure what to do at this stage because none of the documentation mentions needing to adapt these tool patterns.&lt;/p&gt; &lt;p&gt;My docker-compose sets up my models as such --&lt;/p&gt; &lt;pre&gt;&lt;code&gt; ANTHROPIC_DEFAULT_HAIKU_MODEL: &amp;quot;command-r7b&amp;quot; ANTHROPIC_DEFAULT_SONNET_MODEL: &amp;quot;qwen3-coder&amp;quot; ANTHROPIC_DEFAULT_OPUS_MODEL: &amp;quot;qwen3-coder&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And I can confirm that it's talking to qwen3-coder running in docker.&lt;/p&gt; &lt;p&gt;If I open a bash terminal in the claude-docker container and curl a message to Ollama that should require tools, I think I'm getting the output that Claude Code needs.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl -N http://host.docker.internal:11434/v1/messages \ -H &amp;quot;Content-Type: application/json&amp;quot; \ -d '{ &amp;quot;model&amp;quot;: &amp;quot;qwen3-coder&amp;quot;, &amp;quot;max_tokens&amp;quot;: 1024, &amp;quot;stream&amp;quot;: true, &amp;quot;tools&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;get_weather&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Get the current weather in a location&amp;quot;, &amp;quot;input_schema&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;location&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;The city and state&amp;quot; } }, &amp;quot;required&amp;quot;: [&amp;quot;location&amp;quot;] } } ], &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What is the weather in San Francisco?&amp;quot; } ] }' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Results in&lt;/p&gt; &lt;pre&gt;&lt;code&gt;^[[Oevent: message_start data: {&amp;quot;type&amp;quot;:&amp;quot;message_start&amp;quot;,&amp;quot;message&amp;quot;:{&amp;quot;id&amp;quot;:&amp;quot;msg_e9a02c0fd0b3c29742c6a075&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;message&amp;quot;,&amp;quot;role&amp;quot;:&amp;quot;assistant&amp;quot;,&amp;quot;model&amp;quot;:&amp;quot;qwen3-coder&amp;quot;,&amp;quot;content&amp;quot;:[],&amp;quot;usage&amp;quot;:{&amp;quot;input_tokens&amp;quot;:81,&amp;quot;output_tokens&amp;quot;:0}}} event: content_block_start data: {&amp;quot;type&amp;quot;:&amp;quot;content_block_start&amp;quot;,&amp;quot;index&amp;quot;:0,&amp;quot;content_block&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;tool_use&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;call_xajreurq&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;get_weather&amp;quot;,&amp;quot;input&amp;quot;:{}}} event: content_block_delta data: {&amp;quot;type&amp;quot;:&amp;quot;content_block_delta&amp;quot;,&amp;quot;index&amp;quot;:0,&amp;quot;delta&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;input_json_delta&amp;quot;,&amp;quot;partial_json&amp;quot;:&amp;quot;{\&amp;quot;location\&amp;quot;:\&amp;quot;San Francisco\&amp;quot;}&amp;quot;}} event: content_block_stop data: {&amp;quot;type&amp;quot;:&amp;quot;content_block_stop&amp;quot;,&amp;quot;index&amp;quot;:0} event: message_delta data: {&amp;quot;type&amp;quot;:&amp;quot;message_delta&amp;quot;,&amp;quot;delta&amp;quot;:{&amp;quot;stop_reason&amp;quot;:&amp;quot;tool_use&amp;quot;},&amp;quot;usage&amp;quot;:{&amp;quot;input_tokens&amp;quot;:295,&amp;quot;output_tokens&amp;quot;:23}} event: message_stop data: {&amp;quot;type&amp;quot;:&amp;quot;message_stop&amp;quot;} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So, I THINK this is set up right, but I'm not sure why Claude won't execute tools.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;/permissions has only Allow rules.&lt;/li&gt; &lt;li&gt;Running in edit-auto approve mode or with --dangerously-skip-permissions does not impact the tool usage issue.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any ideas where I should look?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crispyghost"&gt; /u/crispyghost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfjraq/tools_wont_execute_when_running_claude_code_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfjraq/tools_wont_execute_when_running_claude_code_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfjraq/tools_wont_execute_when_running_claude_code_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T19:27:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfd045</id>
    <title>Introducing CLAM (Cognitive Large Application Model): A dual-level cognitive architecture for LLM agents (Short/Long term memory)</title>
    <updated>2026-02-26T15:23:14+00:00</updated>
    <author>
      <name>/u/Short-Confidence6287</name>
      <uri>https://old.reddit.com/user/Short-Confidence6287</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rfd045/introducing_clam_cognitive_large_application/"&gt; &lt;img alt="Introducing CLAM (Cognitive Large Application Model): A dual-level cognitive architecture for LLM agents (Short/Long term memory)" src="https://preview.redd.it/otfvuppowulg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a9162abf798ca97814bff71a53b8cd166513ecc" title="Introducing CLAM (Cognitive Large Application Model): A dual-level cognitive architecture for LLM agents (Short/Long term memory)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Hi everyone! I'd like to share CLAM, a new approach to LLM agents I've been working on. Instead of endless fine-tuning, I designed a two-level cognitive architecture that simulates the human mind. üß† CLAM perceives, doubts (via a formidable internal 'Critic'), consolidates valid memories, and... forgets what is irrelevant. The code is now open-source on GitHub! I‚Äôd love to hear your thoughts and suggestions on how to improve it. üëá GitHub: &lt;a href="https://github.com/marcellom66/CLAM"&gt;https://github.com/marcellom66/CLAM&lt;/a&gt;&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Short-Confidence6287"&gt; /u/Short-Confidence6287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/otfvuppowulg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfd045/introducing_clam_cognitive_large_application/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfd045/introducing_clam_cognitive_large_application/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T15:23:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfzbwy</id>
    <title>Need model recommend for 16gb ram 6gm vram for using (ganerel, tool, image and file)</title>
    <updated>2026-02-27T06:38:51+00:00</updated>
    <author>
      <name>/u/depthwc</name>
      <uri>https://old.reddit.com/user/depthwc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Need model recommend for 16gb ram 6gm vram for using (ganerel, tool, image and file)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/depthwc"&gt; /u/depthwc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfzbwy/need_model_recommend_for_16gb_ram_6gm_vram_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfzbwy/need_model_recommend_for_16gb_ram_6gm_vram_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfzbwy/need_model_recommend_for_16gb_ram_6gm_vram_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T06:38:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfshkd</id>
    <title>Blog/paper on putting malicious behaviors in LLMS using training/fine-tuning techniques.</title>
    <updated>2026-02-27T01:06:16+00:00</updated>
    <author>
      <name>/u/DidingasLushis</name>
      <uri>https://old.reddit.com/user/DidingasLushis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember reading an article (maybe a unpublished research paper) which was on a blog page regarding how bad/malicious behavior can secretly be encoded in a LLM using training (not just embeddings) data. Does anyone know this or remember where it might be? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DidingasLushis"&gt; /u/DidingasLushis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfshkd/blogpaper_on_putting_malicious_behaviors_in_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfshkd/blogpaper_on_putting_malicious_behaviors_in_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfshkd/blogpaper_on_putting_malicious_behaviors_in_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T01:06:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfhzxn</id>
    <title>Mimic Digital AI Assistant</title>
    <updated>2026-02-26T18:23:05+00:00</updated>
    <author>
      <name>/u/GullibleNarwhal</name>
      <uri>https://old.reddit.com/user/GullibleNarwhal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rfhzxn/mimic_digital_ai_assistant/"&gt; &lt;img alt="Mimic Digital AI Assistant" src="https://preview.redd.it/7np640opsvlg1.png?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=f5383dac3a73570143f6ead2fb6638c8a5c2ebd1" title="Mimic Digital AI Assistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a Chatbot wrapper that gives locally installed Ollama models a digital avatar with voice and lip-syncing functionality! Any VRM or VRMA is able to be uploaded and applied. Voice creation and TTS is deployed via Qwen3, browser-based TTS, or can be disabled for seamless conversations with a model. Meet Mimic! Any feedback would be greatly appreciated!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bmerriott/MIMIC-Multipurpose-Intelligent-Molecular-Information-Catalyst-/releases/tag/v1.1.0"&gt;https://github.com/bmerriott/MIMIC-Multipurpose-Intelligent-Molecular-Information-Catalyst-/releases/tag/v1.1.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GullibleNarwhal"&gt; /u/GullibleNarwhal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rfhzxn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfhzxn/mimic_digital_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfhzxn/mimic_digital_ai_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T18:23:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfsfne</id>
    <title>I built a small personal project and would love some honest feedback</title>
    <updated>2026-02-27T01:03:55+00:00</updated>
    <author>
      <name>/u/LeadingFun1849</name>
      <uri>https://old.reddit.com/user/LeadingFun1849</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rfsfne/i_built_a_small_personal_project_and_would_love/"&gt; &lt;img alt="I built a small personal project and would love some honest feedback" src="https://preview.redd.it/ww32xcodsxlg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=395cd21ab88bc34d296fc1f570d9a4f5f525e3d2" title="I built a small personal project and would love some honest feedback" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone üëã&lt;br /&gt; I‚Äôm currently learning and improving my development skills, and I recently built a &lt;strong&gt;small personal project&lt;/strong&gt; on my own.&lt;/p&gt; &lt;p&gt;The main goal was to practice and better understand how to structure a real project, so it‚Äôs not commercial and I‚Äôm not trying to promote anything. I‚Äôd genuinely appreciate any feedback you might have, such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What you think is done well&lt;/li&gt; &lt;li&gt;What could be improved (code, structure, idea, etc.)&lt;/li&gt; &lt;li&gt;Any mistakes or bad practices you notice&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub repository:&lt;br /&gt; &lt;a href="https://github.com/davidmonterocrespo24/DaveLovable"&gt;https://github.com/davidmonterocrespo24/DaveLovable&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any comments, criticism, or suggestions are more than welcome.&lt;br /&gt; Thanks for taking the time &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeadingFun1849"&gt; /u/LeadingFun1849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ww32xcodsxlg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfsfne/i_built_a_small_personal_project_and_would_love/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfsfne/i_built_a_small_personal_project_and_would_love/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T01:03:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg705t</id>
    <title>How do I remove OpenClaw</title>
    <updated>2026-02-27T13:41:33+00:00</updated>
    <author>
      <name>/u/wholesaleworldwide</name>
      <uri>https://old.reddit.com/user/wholesaleworldwide</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran this command on a different macOS machine than I wanted to do (from &lt;a href="https://docs.ollama.com/integrations/openclaw):"&gt;https://docs.ollama.com/integrations/openclaw):&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama launch openclaw&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I tried to remove it with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama rm openclaw&lt;/code&gt;&lt;/p&gt; &lt;p&gt;but when you type then the OpenClaw command in terminal I see that an older version of Node is detected (remember, different machine than I wanted). The issue is that I have an older node version that OpenClaw cannot work with.&lt;/p&gt; &lt;p&gt;Does anyone know how I can remove any left-overs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wholesaleworldwide"&gt; /u/wholesaleworldwide &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rg705t/how_do_i_remove_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rg705t/how_do_i_remove_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rg705t/how_do_i_remove_openclaw/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T13:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg0zf0</id>
    <title>Setting NanoBot Local but it does not work.</title>
    <updated>2026-02-27T08:17:37+00:00</updated>
    <author>
      <name>/u/HolgerM2005</name>
      <uri>https://old.reddit.com/user/HolgerM2005</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have NanoBot &amp;quot;running&amp;quot; (its not running yet) on a Raspberry Pi.&lt;/p&gt; &lt;p&gt;While my ollama setup is on a different more powerful machine. (Ollama and OpenWeb Ui do work, using them since month).&lt;/p&gt; &lt;p&gt;My config file looks like this&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;providers&amp;quot;: { &amp;quot;vllm&amp;quot;: { &amp;quot;apiKey&amp;quot;: &amp;quot;secret&amp;quot;, &amp;quot;apiBase&amp;quot;: &amp;quot;http://192.168.1.94:11434&amp;quot; } }, &amp;quot;agents&amp;quot;: { &amp;quot;defaults&amp;quot;: { &amp;quot;model&amp;quot;: &amp;quot;llama3.1:8b&amp;quot;, &amp;quot;provider&amp;quot;: &amp;quot;vllm&amp;quot; } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And I am getting this error, so it seems like it found the local llm but still is trying to use openai.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;üêà nanobot Status Config: /root/.nanobot/config.json ‚úì Workspace: /root/.nanobot/workspace ‚úì Model: llama3.1:8b Custom: not set OpenRouter: not set AiHubMix: not set SiliconFlow: not set Anthropic: not set OpenAI: not set OpenAI Codex: ‚úì (OAuth) Github Copilot: ‚úì (OAuth) DeepSeek: not set Gemini: not set Zhipu AI: not set DashScope: not set Moonshot: not set MiniMax: not set vLLM/Local: ‚úì http://192.168.1.94:11434 Groq: not set - &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What am I doing wrong?&lt;/p&gt; &lt;p&gt;Btw my Portainer Stacks looks like this.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;version: '3.7' services: nanobot: image: jerryin/nanobot:latest container_name: nanobot hostname: nanobot volumes: - /docker/nanobot/.nanobot:/root/.nanobot ports: - 18790:18790 restart: unless-stopped # ‚úÖ Ressourcenlimits f√ºr Raspberry Pi mem_limit: 1g mem_reservation: 256m cpus: &amp;quot;1.0&amp;quot; networks: internal_net: ipv4_address: 172.20.0.32 networks: internal_net: external: true name: internal_net &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Thanks for the help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HolgerM2005"&gt; /u/HolgerM2005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rg0zf0/setting_nanobot_local_but_it_does_not_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rg0zf0/setting_nanobot_local_but_it_does_not_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rg0zf0/setting_nanobot_local_but_it_does_not_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T08:17:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfffru</id>
    <title>Ollama-Vision-Memory-Desktop ‚Äî Local AI Desktop Assistant with Vision + Memory!</title>
    <updated>2026-02-26T16:52:04+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I just published my new open-source project: &lt;strong&gt;Ollama-Vision-Memory-Desktop&lt;/strong&gt; ‚Äî a &lt;em&gt;privacy-focused, offline-first desktop assistant&lt;/em&gt; built on top of &lt;strong&gt;Ollama&lt;/strong&gt; that combines long-term memory, computer vision, and customizable AI behavior.&lt;/p&gt; &lt;p&gt;üë®‚Äçüíª &lt;strong&gt;What it is&lt;/strong&gt;&lt;br /&gt; It‚Äôs a full &lt;strong&gt;Python/PyQt5 desktop app&lt;/strong&gt; that connects to your local Ollama instance and turns it into a powerful assistant with:&lt;/p&gt; &lt;p&gt;‚ú® &lt;strong&gt;Intelligent Chat&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Local AI backend (connects to &lt;code&gt;ollama serve&lt;/code&gt;)&lt;br /&gt; ‚Ä¢ Support for switching between text and vision models on the fly&lt;br /&gt; ‚Ä¢ Session instructions &amp;amp; custom context per conversation&lt;/p&gt; &lt;p&gt;üì∏ &lt;strong&gt;Computer Vision&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Live webcam feed with real-time image analysis&lt;br /&gt; ‚Ä¢ Manual or automated vision scans using vision-capable models like LLaVA, BakLLaVA, etc.&lt;br /&gt; ‚Ä¢ Vision logs saved locally for reference&lt;/p&gt; &lt;p&gt;üß† &lt;strong&gt;Persistent Memory (‚ÄúMind Archive‚Äù)&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Automatic indexing and storage of chats, vision logs, PDFs, and docs&lt;br /&gt; ‚Ä¢ Semantic search across your archive&lt;br /&gt; ‚Ä¢ Contextual retrieval so your assistant &lt;em&gt;remembers past interactions&lt;/em&gt;&lt;br /&gt; ‚Ä¢ Multi-format support: images, audio, text, vision outputs ‚Äî all searchable&lt;/p&gt; &lt;p&gt;‚öôÔ∏è &lt;strong&gt;Custom Behavior &amp;amp; UX&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ System Prompt Editor with presets (e.g., Research Assistant, Creative Companion)&lt;br /&gt; ‚Ä¢ Dark mode and native UI feel&lt;br /&gt; ‚Ä¢ Archive browser for managing stored content&lt;/p&gt; &lt;p&gt;üì¶ &lt;strong&gt;Built For Privacy &amp;amp; Offline Use&lt;/strong&gt;&lt;br /&gt; Everything runs locally ‚Äî no cloud APIs, no telemetry, no data leaks. Perfect for folks who want control and privacy when experimenting with local LLMs like llama3/vision, Gemma, Mistral, etc.&lt;/p&gt; &lt;p&gt;üß∞ &lt;strong&gt;Get Started&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the repo&lt;/li&gt; &lt;li&gt;Install dependencies (&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Start Ollama (&lt;code&gt;ollama serve&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Run the app (&lt;code&gt;python main.py&lt;/code&gt;) ‚Ä¶ and your local AI assistant with vision + memory is ready!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;üîó Check it out here: &lt;a href="https://github.com/Laszlobeer/Ollama-Vision-Memory-Desktop?utm_source=chatgpt.com"&gt;https://github.com/Laszlobeer/Ollama-Vision-Memory-Desktop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfffru/ollamavisionmemorydesktop_local_ai_desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfffru/ollamavisionmemorydesktop_local_ai_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfffru/ollamavisionmemorydesktop_local_ai_desktop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T16:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgbuht</id>
    <title>Ollama Cloud + OpenClaw: API error 404: {"error":"path "/api/api/chat" not found"}</title>
    <updated>2026-02-27T16:46:35+00:00</updated>
    <author>
      <name>/u/sjespers</name>
      <uri>https://old.reddit.com/user/sjespers</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sjespers"&gt; /u/sjespers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1rgbpmc/ollama_cloud_openclaw_api_error_404_errorpath/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgbuht/ollama_cloud_openclaw_api_error_404_errorpath/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rgbuht/ollama_cloud_openclaw_api_error_404_errorpath/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T16:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgbvdi</id>
    <title>ollamaMQ - simple proxy with fair-share queuing + nice TUI</title>
    <updated>2026-02-27T16:47:28+00:00</updated>
    <author>
      <name>/u/chleboslaF</name>
      <uri>https://old.reddit.com/user/chleboslaF</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rgbvdi/ollamamq_simple_proxy_with_fairshare_queuing_nice/"&gt; &lt;img alt="ollamaMQ - simple proxy with fair-share queuing + nice TUI" src="https://external-preview.redd.it/kyRoLCXiRf2csCFJSCE20Xo50AqhHxdSm9iu3cRRYOY.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=52c64da6d58f148d6cb5446d33fcfe96d46e3003" title="ollamaMQ - simple proxy with fair-share queuing + nice TUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/ldwlc8rzf2mg1.gif"&gt;https://i.redd.it/ldwlc8rzf2mg1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ollamaMQ is High-performance Ollama proxy with per-user fair-share queuing, round-robin scheduling, and a real-time TUI dashboard. Built in Rust.&lt;/p&gt; &lt;p&gt;I needed some very simple, easy to use requests queuing to my ollama server instance from multiple sources.&lt;br /&gt; I am using ollama-rs where I just add custom http header with a unique user ID to do a rounding of these FIFO requests to my Ollama server to have a fair requests distribution.&lt;/p&gt; &lt;p&gt;Added also a stress test bash script for you to try.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/nbn087u1g2mg1.gif"&gt;https://i.redd.it/nbn087u1g2mg1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Check it out on: &lt;a href="https://github.com/Chleba/ollamaMQ"&gt;https://github.com/Chleba/ollamaMQ&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chleboslaF"&gt; /u/chleboslaF &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgbvdi/ollamamq_simple_proxy_with_fairshare_queuing_nice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgbvdi/ollamamq_simple_proxy_with_fairshare_queuing_nice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rgbvdi/ollamamq_simple_proxy_with_fairshare_queuing_nice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T16:47:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg8gr3</id>
    <title>What is your experience with Ollama Cloud in compare with z.ai subscription</title>
    <updated>2026-02-27T14:40:51+00:00</updated>
    <author>
      <name>/u/malek_hor30</name>
      <uri>https://old.reddit.com/user/malek_hor30</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using Claude Code (Opus 4.6) most of the time for my daily work &amp;amp; side projects. But, since its subscription is kinda expensive (I'm currently using the max plan) I was considering a new alternative, or at least moving to the 20$ plan by finding another models that can do most of the work and leave only deep thinking tasks to Opus. I saw someone in comments on another sub reddit talking about Ollama Cloud and I thought maybe using the 20$ with the Anthropic 20$ subscription would be enough since Ollama can allow me to use 3 private models (or that what I understood at least). So, I want to know what is your experience with it? Actually I'm kinda having a fear since I tried GLM 4.7 before using &lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt; subscription and it was soooooooo slow but on the other hand it was actually provide good output.&lt;/p&gt; &lt;p&gt;my question is, Is using GLM 5 from Ollama Cloud will make any change? Is it gonna be faster? Or it is the same speed?&lt;/p&gt; &lt;p&gt;Note: I can't run anything locally my laptop isn't capable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/malek_hor30"&gt; /u/malek_hor30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rg8gr3/what_is_your_experience_with_ollama_cloud_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rg8gr3/what_is_your_experience_with_ollama_cloud_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rg8gr3/what_is_your_experience_with_ollama_cloud_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T14:40:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgbjw8</id>
    <title>Trying to figure the best model for reading company documentation</title>
    <updated>2026-02-27T16:36:03+00:00</updated>
    <author>
      <name>/u/Rickety_cricket420</name>
      <uri>https://old.reddit.com/user/Rickety_cricket420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working for a small company that has an employee portal that holds all the documents you could need. The issue is if you want to find something like vacation policy or dress code you have to go digging through the files to find it. The files are pretty much all .docx and .pdf. I'm still relatively new to this so could someone suggest a good model for that? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rickety_cricket420"&gt; /u/Rickety_cricket420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgbjw8/trying_to_figure_the_best_model_for_reading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgbjw8/trying_to_figure_the_best_model_for_reading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rgbjw8/trying_to_figure_the_best_model_for_reading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T16:36:03+00:00</published>
  </entry>
</feed>
