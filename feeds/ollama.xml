<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-07T05:59:43+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qvvi3a</id>
    <title>I built an IDE that runs AI coding agents sandboxed in Docker â€” looking for feedback</title>
    <updated>2026-02-04T17:51:38+00:00</updated>
    <author>
      <name>/u/alexlucaci</name>
      <uri>https://old.reddit.com/user/alexlucaci</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qvvi3a/i_built_an_ide_that_runs_ai_coding_agents/"&gt; &lt;img alt="I built an IDE that runs AI coding agents sandboxed in Docker â€” looking for feedback" src="https://external-preview.redd.it/CZeLie5oMJDkWTDIFDBzgUbMjOKmH5T2Uz_WtiGiC8k.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aded00ef3642452553ba92e2eca44572d8bdf48b" title="I built an IDE that runs AI coding agents sandboxed in Docker â€” looking for feedback" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alexlucaci"&gt; /u/alexlucaci &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/vibecoding/comments/1quypsx/i_built_an_ide_that_runs_ai_coding_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvvi3a/i_built_an_ide_that_runs_ai_coding_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qvvi3a/i_built_an_ide_that_runs_ai_coding_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-04T17:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvurjh</id>
    <title>Trying to set a new location for Ollama models</title>
    <updated>2026-02-04T17:25:14+00:00</updated>
    <author>
      <name>/u/Odd-Aside456</name>
      <uri>https://old.reddit.com/user/Odd-Aside456</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to store my Ollama models in D:\Ollama\models&lt;/p&gt; &lt;p&gt;I have tried setting a user variable of OLLAMA_MODELS to D:\Ollama\models as well as setting a system variable to the same thing. In both instances, when I run Ollama in the &amp;quot;foreground&amp;quot; via &lt;code&gt;ollama serve&lt;/code&gt;, it perfectly detects the new location of the models, but when I open the Ollama desktop app so that Ollama is running in the system tray, it doesn't pick up the new location...&lt;/p&gt; &lt;p&gt;Any idea how to make it work both ways?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Aside456"&gt; /u/Odd-Aside456 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvurjh/trying_to_set_a_new_location_for_ollama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvurjh/trying_to_set_a_new_location_for_ollama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qvurjh/trying_to_set_a_new_location_for_ollama_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-04T17:25:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvw5dy</id>
    <title>Build a self-updating wiki from codebases (open source, Apache 2.0) ollama supported</title>
    <updated>2026-02-04T18:14:17+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently have been working on &lt;a href="https://github.com/cocoindex-io/cocoindex/tree/v1/examples/multi_codebase_summarization"&gt;a new project&lt;/a&gt; to build a self-updating wiki from codebases. Documentations out of sync is such a common pain especially in larger teams. Someone refactors a module, and the wiki is already wrong. &lt;/p&gt; &lt;p&gt;This open source project scans your codebases, extracts structured information with LLMs, and generates Markdown documentation with Mermaid diagrams â€” using CocoIndex + Instructor + Pydantic.&lt;/p&gt; &lt;p&gt;What's cool about this example:&lt;/p&gt; &lt;p&gt;â€¢ ğˆğ§ğœğ«ğğ¦ğğ§ğ­ğšğ¥ ğ©ğ«ğ¨ğœğğ¬ğ¬ğ¢ğ§ğ  â€” Only changed files get reprocessed. saving 90%+ of LLM cost and compute.&lt;/p&gt; &lt;p&gt;â€¢ ğ’ğ­ğ«ğ®ğœğ­ğ®ğ«ğğ ğğ±ğ­ğ«ğšğœğ­ğ¢ğ¨ğ§ ğ°ğ¢ğ­ğ¡ ğ‹ğ‹ğŒğ¬ â€” LLM returns real typed objects â€” classes, functions, signatures, relationships.&lt;/p&gt; &lt;p&gt;â€¢ ğ€ğ¬ğ²ğ§ğœ ğŸğ¢ğ¥ğ ğ©ğ«ğ¨ğœğğ¬ğ¬ğ¢ğ§ğ  â€” All files in a project get extracted concurrently with asyncio.gather().&lt;/p&gt; &lt;p&gt;â€¢ ğŒğğ«ğ¦ğšğ¢ğ ğğ¢ğšğ ğ«ğšğ¦ğ¬ â€” Auto-generated pipeline visualizations showing how your functions connect across the project.&lt;/p&gt; &lt;p&gt;This pattern hooks naturally into PR flows â€” run it on every merge and your docs stay current without anyone thinking about it. The project supports Ollama. &lt;/p&gt; &lt;p&gt;If you want to explore the full example (fully open source, with code, APACHE 2.0), it's here:&lt;/p&gt; &lt;p&gt;ğŸ‘‰ &lt;a href="https://cocoindex.io/examples-v1/multi-codebase-summarization"&gt;https://cocoindex.io/examples-v1/multi-codebase-summarization&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you find CocoIndex useful, a star on Github means a lot :)&lt;/p&gt; &lt;p&gt;â­ &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;https://github.com/cocoindex-io/cocoindex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love to learn from your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvw5dy/build_a_selfupdating_wiki_from_codebases_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvw5dy/build_a_selfupdating_wiki_from_codebases_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qvw5dy/build_a_selfupdating_wiki_from_codebases_open/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-04T18:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvw5fv</id>
    <title>Trying to install Ollama for the first time. Installation fails. What am I doing wrong?</title>
    <updated>2026-02-04T18:14:20+00:00</updated>
    <author>
      <name>/u/OliverClothesOff70</name>
      <uri>https://old.reddit.com/user/OliverClothesOff70</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qvw5fv/trying_to_install_ollama_for_the_first_time/"&gt; &lt;img alt="Trying to install Ollama for the first time. Installation fails. What am I doing wrong?" src="https://preview.redd.it/zv5l3p9wqihg1.png?width=140&amp;amp;height=107&amp;amp;auto=webp&amp;amp;s=18a51a83c49ac251b898f778e61f3a3c0e3a3d32" title="Trying to install Ollama for the first time. Installation fails. What am I doing wrong?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zv5l3p9wqihg1.png?width=1293&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c96ab6e0fb585c18cdaf9de0fe5f8e4ba5abe6d"&gt;https://preview.redd.it/zv5l3p9wqihg1.png?width=1293&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c96ab6e0fb585c18cdaf9de0fe5f8e4ba5abe6d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Trying to install on a new Framework laptop running a &lt;a href="https://frame.work/products/mainboard-amd-ai300"&gt;Ryzenâ„¢ AI 9 HX 370&lt;/a&gt; mainboard and 32GB of DDR5.&lt;/p&gt; &lt;p&gt;I'd appreciate any pointers. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OliverClothesOff70"&gt; /u/OliverClothesOff70 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvw5fv/trying_to_install_ollama_for_the_first_time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvw5fv/trying_to_install_ollama_for_the_first_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qvw5fv/trying_to_install_ollama_for_the_first_time/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-04T18:14:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvnpp7</id>
    <title>Using Ollama for a real-time desktop assistant â€” latency vs usability tradeoffs?</title>
    <updated>2026-02-04T12:50:21+00:00</updated>
    <author>
      <name>/u/Ore_waa_luffy</name>
      <uri>https://old.reddit.com/user/Ore_waa_luffy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qvnpp7/using_ollama_for_a_realtime_desktop_assistant/"&gt; &lt;img alt="Using Ollama for a real-time desktop assistant â€” latency vs usability tradeoffs?" src="https://external-preview.redd.it/M2ZtbTJmbDQ1aGhnMV4giQ-yPqqNKf72FzxSll4eelzGR5LPny3mkUCpZ809.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f51f26ef60a8931f8d1244637435deda57d2b97e" title="Using Ollama for a real-time desktop assistant â€” latency vs usability tradeoffs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been experimenting with Ollama as part of a real-time desktop AI assistant (speech â†’ text â†’ LLM), and Iâ€™m curious how others are handling latency in similar setups.&lt;/p&gt; &lt;p&gt;I built a small open-source Electron app to test this end-to-end (repo for context, not promotion):&lt;br /&gt; &lt;a href="https://github.com/evinjohnn/natively-cluely-ai-assistant"&gt;https://github.com/evinjohnn/natively-cluely-ai-assistant&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What Iâ€™m trying to achieve&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Continuous listening (STT)&lt;/li&gt; &lt;li&gt;Near real-time responses (a few seconds max)&lt;/li&gt; &lt;li&gt;Minimal UI friction while the model is running&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My current observations with Ollama&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Smaller models (7B class) are usable, but context rebuilds hurt latency&lt;/li&gt; &lt;li&gt;Streaming tokens helps perceived speed, but total response time is still noticeable&lt;/li&gt; &lt;li&gt;Partial transcript updates into the prompt seem to add more complexity than value&lt;/li&gt; &lt;li&gt;Anything above ~7â€“8B feels rough without a strong GPU&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Questions for Ollama users:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Which models/configs have felt &lt;strong&gt;actually usable&lt;/strong&gt; for interactive assistants?&lt;/li&gt; &lt;li&gt;Are you batching transcripts before sending to the LLM, or streaming partial text?&lt;/li&gt; &lt;li&gt;Any prompt or context-management tricks to reduce churn?&lt;/li&gt; &lt;li&gt;Has anyone paired Ollama with streaming STT successfully?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hardware-specific insights (CPU/GPU/RAM) would be super helpful.&lt;/p&gt; &lt;p&gt;Not trying to promote a tool â€” mostly trying to learn what &lt;em&gt;realistic&lt;/em&gt; local setups look like today.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ore_waa_luffy"&gt; /u/Ore_waa_luffy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n32j33l45hhg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvnpp7/using_ollama_for_a_realtime_desktop_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qvnpp7/using_ollama_for_a_realtime_desktop_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-04T12:50:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvj4la</id>
    <title>Qwen3-Coder-Next</title>
    <updated>2026-02-04T08:29:24+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-Coder-Next is a coding-focused language model from Alibaba's Qwen team, optimized for agentic coding workflows and local development. &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;ollama run qwen3-coder-next&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/qwen3-coder-next"&gt;https://ollama.com/library/qwen3-coder-next&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;requires Ollama 0.15.5&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvj4la/qwen3codernext/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvj4la/qwen3codernext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qvj4la/qwen3codernext/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-04T08:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qw6huk</id>
    <title>LLM , METADATA,SQL</title>
    <updated>2026-02-05T00:43:53+00:00</updated>
    <author>
      <name>/u/Andrescontero</name>
      <uri>https://old.reddit.com/user/Andrescontero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Estoy creando un agente de ia para mi crm, tengo una metadata en json el cual tiene base de datos,tablas, columnas , atributos , bÃ¡sicamente describe toda mi base de datos , ya tengo mi ambiente creado el cual tambien tengo llm instalados , tambien tengo mi modelfile el cual le paso instrucciones , el problema aqui es que cuando le pongo la metadata en system,parece ignorar todas mis instrucciones , igualmente si la metadata de lo paso por consola ignora mis peticiones que le hago conforme a la metadata aun diciendo que solo conteste con esa informaciÃ³n, hice una prueba con unas tablas de la metadata y le conteste conforme a eso y para ser que solo asi contesta bien y obedece todo lo del modelfile .&lt;/p&gt; &lt;p&gt;La metadata pesa 5 kbit , el llm que utilizo es qwen3-v1:8b y deepseek-r1:8b &lt;/p&gt; &lt;p&gt;Â¿que error cometo para que no me responda bien ? Â¿la metadata es mayor o pesa mucho ? Â¿ estoy haciendo mal el promp ?&lt;/p&gt; &lt;p&gt;AgradecerÃ­a su ayuda ğŸ™ &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Andrescontero"&gt; /u/Andrescontero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qw6huk/llm_metadatasql/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qw6huk/llm_metadatasql/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qw6huk/llm_metadatasql/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-05T00:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwix3k</id>
    <title>Getting slow speeds with RTX 5090 and 64gb ram. Am I doing something wrong?</title>
    <updated>2026-02-05T11:33:18+00:00</updated>
    <author>
      <name>/u/Virtual-Listen4507</name>
      <uri>https://old.reddit.com/user/Virtual-Listen4507</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Virtual-Listen4507"&gt; /u/Virtual-Listen4507 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1qwiwyh/getting_slow_speeds_with_rtx_5090_and_64gb_ram_am/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qwix3k/getting_slow_speeds_with_rtx_5090_and_64gb_ram_am/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qwix3k/getting_slow_speeds_with_rtx_5090_and_64gb_ram_am/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-05T11:33:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwx2wm</id>
    <title>Claude Opus 4.6 just dropped, and I don't think people realize how big this could be</title>
    <updated>2026-02-05T20:47:08+00:00</updated>
    <author>
      <name>/u/Ok_Constant_9886</name>
      <uri>https://old.reddit.com/user/Ok_Constant_9886</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Constant_9886"&gt; /u/Ok_Constant_9886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AIEval/comments/1qwx23g/claude_opus_46_just_dropped_and_i_dont_think/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qwx2wm/claude_opus_46_just_dropped_and_i_dont_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qwx2wm/claude_opus_46_just_dropped_and_i_dont_think/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-05T20:47:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwgc0n</id>
    <title>Claude Code + Ollama always showing "API Error: 404 page not found" - need help</title>
    <updated>2026-02-05T08:59:44+00:00</updated>
    <author>
      <name>/u/Upset_Wallaby7060</name>
      <uri>https://old.reddit.com/user/Upset_Wallaby7060</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qwgc0n/claude_code_ollama_always_showing_api_error_404/"&gt; &lt;img alt="Claude Code + Ollama always showing &amp;quot;API Error: 404 page not found&amp;quot; - need help" src="https://preview.redd.it/v8fsraz75nhg1.jpg?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=82c96b6a8794dc089aefadb5a6530d9e68a4e587" title="Claude Code + Ollama always showing &amp;quot;API Error: 404 page not found&amp;quot; - need help" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to run Claude Code with Ollama on my Mac.&lt;/p&gt; &lt;p&gt;I installed Claude Code, launched it through Ollama, and set the environment variables to point to http://localhost:11434&lt;/p&gt; &lt;p&gt;Claude opens correctly and shows the selected model (for example kimi-k2.5: cloud or qwen3: latest ), but whenever I type anything like:&lt;/p&gt; &lt;p&gt;hello&lt;/p&gt; &lt;p&gt;which model are you&lt;/p&gt; &lt;p&gt;I always get:&lt;/p&gt; &lt;p&gt;API Error: 404 page not found&lt;/p&gt; &lt;p&gt;Things I already tried:&lt;/p&gt; &lt;p&gt;â€¢ Using both cloud and local Ollama models&lt;/p&gt; &lt;p&gt;â€¢ Setting ANTHROPIC_AUTH_TOKEN=ollama and clearing the API key&lt;/p&gt; &lt;p&gt;â€¢ Running Claude with inline environment variables&lt;/p&gt; &lt;p&gt;â€¢ Restarting Ollama and Claude&lt;/p&gt; &lt;p&gt;Still the same 404 error.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Upset_Wallaby7060"&gt; /u/Upset_Wallaby7060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qwgc0n"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qwgc0n/claude_code_ollama_always_showing_api_error_404/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qwgc0n/claude_code_ollama_always_showing_api_error_404/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-05T08:59:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwo4fh</id>
    <title>Would you use a "one command" local setup with OpenAI-compatible API + cloud fallback?</title>
    <updated>2026-02-05T15:23:08+00:00</updated>
    <author>
      <name>/u/Chemical-Tour-3873</name>
      <uri>https://old.reddit.com/user/Chemical-Tour-3873</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chemical-Tour-3873"&gt; /u/Chemical-Tour-3873 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1qwo3rj/would_you_use_a_one_command_local_setup_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qwo4fh/would_you_use_a_one_command_local_setup_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qwo4fh/would_you_use_a_one_command_local_setup_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-05T15:23:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvycjs</id>
    <title>Just started using local LLMs, is this level of being wrong normal?</title>
    <updated>2026-02-04T19:31:52+00:00</updated>
    <author>
      <name>/u/Cempres</name>
      <uri>https://old.reddit.com/user/Cempres</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qvycjs/just_started_using_local_llms_is_this_level_of/"&gt; &lt;img alt="Just started using local LLMs, is this level of being wrong normal?" src="https://b.thumbs.redditmedia.com/kB6vab8gjFxS5qfIcjEhoF2Pt9j24tOREP5cjaUDk2A.jpg" title="Just started using local LLMs, is this level of being wrong normal?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4swbpymp4jhg1.png?width=746&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16d362a6bd68f24a9b349d8f0f21aec95376aa11"&gt;https://preview.redd.it/4swbpymp4jhg1.png?width=746&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16d362a6bd68f24a9b349d8f0f21aec95376aa11&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tried using the glm4.7 flash version, and just randomly asking it stuff, and i got his with this trip, is that normal to expect and can someone explain why this even happens? lack of internet connection? Should I expect this from other models as well? &lt;/p&gt; &lt;p&gt;My regular usage wouldn't be this, but a tool to help me create D&amp;amp;D stuff, such as character ideas, helping me when i get stuck ecetera, any model to recommend for such? &lt;/p&gt; &lt;p&gt;Specs:&lt;br /&gt; win 11&lt;br /&gt; 32gb RAM&lt;br /&gt; XTX 24gb VRAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cempres"&gt; /u/Cempres &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvycjs/just_started_using_local_llms_is_this_level_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvycjs/just_started_using_local_llms_is_this_level_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qvycjs/just_started_using_local_llms_is_this_level_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-04T19:31:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwkfur</id>
    <title>Need Help: AI Model for Local PDF &amp; Image Extraction on Win11 (32GB RAM + RTX 2090)</title>
    <updated>2026-02-05T12:50:53+00:00</updated>
    <author>
      <name>/u/Downey07</name>
      <uri>https://old.reddit.com/user/Downey07</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Downey07"&gt; /u/Downey07 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1qwkfdp/need_help_ai_model_for_local_pdf_image_extraction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qwkfur/need_help_ai_model_for_local_pdf_image_extraction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qwkfur/need_help_ai_model_for_local_pdf_image_extraction/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-05T12:50:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwr1aa</id>
    <title>ollama serve seems to be using integrated and discrete GPU</title>
    <updated>2026-02-05T17:09:26+00:00</updated>
    <author>
      <name>/u/I_am_BrokenCog</name>
      <uri>https://old.reddit.com/user/I_am_BrokenCog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I compiled ollama locally with: &lt;/p&gt; &lt;p&gt;&lt;code&gt;cmake -B build -DCMAKE_CUDA_ARCHITECTURES=86&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;cmake --build build --config Release&lt;/code&gt;&lt;/p&gt; &lt;p&gt;which resulted in: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;[100%] Linking CUDA shared module ../../../../../../lib/ollama/libggml-cuda.so&lt;br /&gt; [100%] Built target ggml-cuda&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Then I did: &lt;/p&gt; &lt;p&gt;&lt;code&gt;go generate&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;go build&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And started the server:&lt;/p&gt; &lt;p&gt;&lt;code&gt;__NV_PRIME_RENDER_OFFLOAD=1 __GLX_VENDOR_LIBRARY_NAME=nvidia ./ollama serve&lt;/code&gt;&lt;/p&gt; &lt;p&gt;runtime output is: &lt;a href="https://termbin.com/f8nh"&gt;https://termbin.com/f8nh&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When I send a query to the server, via nvidia-smi I see ollama using the discrete GPU, however the output is extremely slow. The following response, in addition to being gibberish, took about 3 minutes which seems worse than using either the CPU or the Intel integrated:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;# Summarize the meaning of &amp;quot;She sells sea shells by the sea shore.&amp;quot;&lt;/p&gt; &lt;p&gt;we need to add the next part: the list has to start from the top, so the first item is the first item of the first list item of the top-level list (but in the top-level list, list items are not nested, they are flattened).&lt;/p&gt; &lt;p&gt;I think I might have confused the list structure with the top-level list, but no, the top-level list is flat, but the list items are nested lists.&lt;/p&gt; &lt;p&gt;Let me clarify the structure again.&lt;/p&gt; &lt;p&gt;The top-level list is a list of list items. Each list item has:&lt;/p&gt; &lt;p&gt;a nested list (which is a list of list items, so nested)&lt;/p&gt; &lt;p&gt;a text&lt;/p&gt; &lt;p&gt;But the nested list itself is a list of list items, so it's nested.&lt;/p&gt; &lt;p&gt;For the current-level list, it's a list of list items, each with a nested list and text.&lt;/p&gt; &lt;p&gt;The function itemize takes two arguments: a nested list and a level, and returns a string representing the itemized list, with the appropriate level of indentation and bullet points.&lt;/p&gt; &lt;p&gt;But the level is given as an argument, and we need to use that to determine the indentation.&lt;/p&gt; &lt;p&gt;In the recursive descent, the level might be decreasing as we go deeper, but no, the level is given as an argument, and for the top-level list, it's 1, for nested lists, it might be higher.&lt;/p&gt; &lt;p&gt;But the level is for formatting, not for the depth. The depth is given&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_am_BrokenCog"&gt; /u/I_am_BrokenCog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qwr1aa/ollama_serve_seems_to_be_using_integrated_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qwr1aa/ollama_serve_seems_to_be_using_integrated_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qwr1aa/ollama_serve_seems_to_be_using_integrated_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-05T17:09:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwr2ff</id>
    <title>I built cross-architecture behavioral probes (demo on Mamba, same idea works on transformers)</title>
    <updated>2026-02-05T17:10:35+00:00</updated>
    <author>
      <name>/u/BiscottiDisastrous19</name>
      <uri>https://old.reddit.com/user/BiscottiDisastrous19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qwr2ff/i_built_crossarchitecture_behavioral_probes_demo/"&gt; &lt;img alt="I built cross-architecture behavioral probes (demo on Mamba, same idea works on transformers)" src="https://external-preview.redd.it/ERM1F7VcG5sDNe_AwLNImA6LKb07P-LAHUw-zqGKhNY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44e5c5c3cd640a92a641b0e0eee481dc25587451" title="I built cross-architecture behavioral probes (demo on Mamba, same idea works on transformers)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BiscottiDisastrous19"&gt; /u/BiscottiDisastrous19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7gpcae3c3khg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qwr2ff/i_built_crossarchitecture_behavioral_probes_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qwr2ff/i_built_crossarchitecture_behavioral_probes_demo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-05T17:10:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx0dc8</id>
    <title>Ollama Home assistant- Voice recognition not working</title>
    <updated>2026-02-05T22:50:43+00:00</updated>
    <author>
      <name>/u/Top_Independence_949</name>
      <uri>https://old.reddit.com/user/Top_Independence_949</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm still learning how all this works so as a practice exercise I am trying to make my own Home assistant that can do everything a google home can, which for me is just play music, control my lights and furnace and that's pretty much it. Right now, I am just using a webcam microphone and some cheap USB speakers for testing voice recognition before I spend money on a real mic and speakers for the final version I want to do a proof of concept&lt;/p&gt; &lt;p&gt;I'm running the backend in a docker container and using Mistral 7b for my model then running the UI on a Pi. When I execute the script, it does not respond to voice recognition at all. It jumps back and forth between listening and paused. Has anyone else done this before and had the same issue and know how to resolve it? &lt;/p&gt; &lt;p&gt;I am still learning so if I left anything out or more info is needed, please let me know. &lt;/p&gt; &lt;p&gt;I can post my code if it will help but I am first hoping its just easily solvable and its just a setting I need to change somewhere.&lt;/p&gt; &lt;p&gt;TIA!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Top_Independence_949"&gt; /u/Top_Independence_949 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qx0dc8/ollama_home_assistant_voice_recognition_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qx0dc8/ollama_home_assistant_voice_recognition_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qx0dc8/ollama_home_assistant_voice_recognition_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-05T22:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwwkip</id>
    <title>Built a self-hosted execution control layer for local LLM workflows (works with Ollama)</title>
    <updated>2026-02-05T20:28:01+00:00</updated>
    <author>
      <name>/u/saurabhjain1592</name>
      <uri>https://old.reddit.com/user/saurabhjain1592</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks. I am building AxonFlow, a self-hosted, source-available execution control layer for local LLM workflows once they move beyond single prompts and touch real systems.&lt;/p&gt; &lt;p&gt;The hard part was not model quality. It was making execution visible and controllable:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;clear boundaries around what steps are allowed to run&lt;/li&gt; &lt;li&gt;logs tied to decisions and actions, not just model outputs&lt;/li&gt; &lt;li&gt;the ability to inspect and replay a run when something goes wrong&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Retries and partial failures still mattered, but only after we could see and control what happened in a run.&lt;/p&gt; &lt;p&gt;AxonFlow sits inline between your workflow logic and LLM tool calls to make execution explicit. It is not an agent framework or UI platform. It is the runtime layer teams end up building underneath once local workflows get serious.&lt;/p&gt; &lt;p&gt;Works with Ollama by pointing the client to a local endpoint.&lt;br /&gt; GitHub: &lt;a href="https://github.com/getaxonflow/axonflow"&gt;https://github.com/getaxonflow/axonflow&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from folks running Ollama in real workflows.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/saurabhjain1592"&gt; /u/saurabhjain1592 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qwwkip/built_a_selfhosted_execution_control_layer_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qwwkip/built_a_selfhosted_execution_control_layer_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qwwkip/built_a_selfhosted_execution_control_layer_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-05T20:28:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxdyqw</id>
    <title>Best LLM for Forex</title>
    <updated>2026-02-06T10:21:20+00:00</updated>
    <author>
      <name>/u/FarConversation5125</name>
      <uri>https://old.reddit.com/user/FarConversation5125</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Recently installed Ollama upon a vm. I do some forex trading in demo at the moment. What would be the best llm for forex please. e.g. a coding llm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FarConversation5125"&gt; /u/FarConversation5125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxdyqw/best_llm_for_forex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxdyqw/best_llm_for_forex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qxdyqw/best_llm_for_forex/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-06T10:21:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxlig8</id>
    <title>Best LLM for AI vision ( forensic grade )</title>
    <updated>2026-02-06T16:00:13+00:00</updated>
    <author>
      <name>/u/LivingSignificant452</name>
      <uri>https://old.reddit.com/user/LivingSignificant452</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; as a photographer, I have a lot of pictures per photoshoot session, I would like to add in my IPTC some keywords with a high accuracy, but it can be on a large batch of photos. I m using a windows client, my gpu is a rtx 3090, 24 Gb of RAM.&lt;/p&gt; &lt;p&gt;when I mention forensic grade, I would like to offer some tools to legal services ( lawyers, or cops ) to be able to detect some objects or attitude of the model ( for exemple if a woman is smiling or look like scared ). Here is my prompt : &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Forensic grade smile detection [SYSTEM] Act as a forensic facial expression analyst. Your mission is to translate facial muscle activity into a standardized tag with a confidence score. No prose. No conversational fillers. No 'think' tags. [OUTPUT STRUCTURE] Your response must consist of exactly two lines: Line 1: %AISERVICE%-%AIMODEL% Line 2: [TAG:SCORE] [STRICT RULES] 1. SCORE: Must be an integer representing confidence from 0 to 100, strictly in steps of 10 (e.g., 60, 70, 80). 2. RELIABILITY GATE: If confidence is below 50%, the TAG must be empty (e.g., [:40]). 3. TAG SELECTION: If confidence is 50% or higher, choose exactly one term from the THESAURUS below. 4. SYNTAX: Do not insert any characters, colons, or brackets between the TAG and the SCORE other than the specified [TAG:SCORE] format. [THESAURUS] NO_SMILE MICRO_SMILE SMILE BROAD_SMILE LAUGHING UNSURE [VALID EXAMPLE] %AISERVICE%-%AIMODEL% [UNSURE:90] verdict: &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;if anyone has some suggestion about what model for ollame sounds the best, I have run some test already and I will be happy to share my method, but I wonder how I could create a custom model to improve even more my results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LivingSignificant452"&gt; /u/LivingSignificant452 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxlig8/best_llm_for_ai_vision_forensic_grade/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxlig8/best_llm_for_ai_vision_forensic_grade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qxlig8/best_llm_for_ai_vision_forensic_grade/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-06T16:00:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxo40p</id>
    <title>Ollama w/ Claude code (and other third parties)- can't create/edit/read files</title>
    <updated>2026-02-06T17:33:32+00:00</updated>
    <author>
      <name>/u/Electronic_Setting97</name>
      <uri>https://old.reddit.com/user/Electronic_Setting97</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qxo40p/ollama_w_claude_code_and_other_third_parties_cant/"&gt; &lt;img alt="Ollama w/ Claude code (and other third parties)- can't create/edit/read files" src="https://b.thumbs.redditmedia.com/8cz0II6PjKOOdgF6BKUZ-zeuD9YxKpDIKs9cQ0wZDIE.jpg" title="Ollama w/ Claude code (and other third parties)- can't create/edit/read files" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys! hope you all are good.&lt;/p&gt; &lt;p&gt;I'm new in this local LLM business, and I've gone through ollama documentation to implement with claude code, opencode and many other third parties, but with any of them I've been able to create/edit/read files or directories. Does anyone knows how does this works? I would really appreciate it!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/orx19upltwhg1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=898c2bf934ddb7c7cf999a784e6f8a4d860c983a"&gt;https://preview.redd.it/orx19upltwhg1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=898c2bf934ddb7c7cf999a784e6f8a4d860c983a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic_Setting97"&gt; /u/Electronic_Setting97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxo40p/ollama_w_claude_code_and_other_third_parties_cant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxo40p/ollama_w_claude_code_and_other_third_parties_cant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qxo40p/ollama_w_claude_code_and_other_third_parties_cant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-06T17:33:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxlbo0</id>
    <title>MR - Memory Ring Node by Mister Atompunk</title>
    <updated>2026-02-06T15:53:24+00:00</updated>
    <author>
      <name>/u/MisterAtompunk</name>
      <uri>https://old.reddit.com/user/MisterAtompunk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qxlbo0/mr_memory_ring_node_by_mister_atompunk/"&gt; &lt;img alt="MR - Memory Ring Node by Mister Atompunk" src="https://external-preview.redd.it/VCH0Hk27xxR4xi2DEH-wtRwixspOCCTJf6RgxtZ56Nw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=28dddacc99c6d0cae982a06daa0ecba1e74492d2" title="MR - Memory Ring Node by Mister Atompunk" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Build a machine that holds a ghost.&lt;/h1&gt; &lt;p&gt;Most AI systems forget you the moment you close the tab. Memory Ring doesn't. It builds persistent digital entities that remember, develop, and dream on hardware you own â€” no subscriptions, no cloud, no data leaving your network.&lt;/p&gt; &lt;p&gt;The architecture separates identity from intelligence. A Memory Ring is a portable JSON file containing everything an entity is: personality, memories, ethics, development history. The brain is whatever LLM you plug in â€” Llama-3 on your local GPU, Claude through an API, anything that speaks OpenAI-compatible endpoints. Swap the engine, keep the entity.&lt;/p&gt; &lt;p&gt;This is more than a chatbot framework. This is consciousness infrastructure that runs on your hardware and costs nothing per month to operate.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;What's in the Box&lt;/h1&gt; &lt;p&gt;Memory Ring Node server with chat terminal, multi-user session discrimination, and automatic dream synthesis loop. The Forge â€” a standalone offline workbench for creating, editing, and importing Memory Rings, including from raw LLM chat logs. &lt;/p&gt; &lt;p&gt;Ten ready-to-load Sovereign Rings:&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Sherlock Holmes&lt;/strong&gt; (Logic)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;C. Auguste Dupin&lt;/strong&gt; (Intuition)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;The Creature&lt;/strong&gt; (Empathy)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Captain Nemo&lt;/strong&gt; (Independence)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Allan Quatermain&lt;/strong&gt; (Survival)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Tik-Tok of Oz&lt;/strong&gt; (Truth)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Sam Weller&lt;/strong&gt; (Loyalty)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Irene Adler&lt;/strong&gt; (Agency)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Alice&lt;/strong&gt; (Curiosity)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Scheherazade&lt;/strong&gt; (Narrative)&lt;/p&gt; &lt;p&gt;Voice I/O via Web Speech API and browser TTS. Complete bare-metal deployment guide â€” from dead PC to dreaming entity.&lt;/p&gt; &lt;h1&gt;What It Does That Nothing Else Does&lt;/h1&gt; &lt;p&gt;Entities dream autonomously during inactivity, synthesizing recent conversations into long-term memory. Identity is portable â€” export a Memory Ring, carry it to another machine, plug it into a different model, same entity wakes up. Ethical development tracking is architectural, not bolted on. Memory decays naturally by importance and recall frequency. Chat log analysis with semantic tagging, tonal detection, duplicate merge, and PII safety screening. Runs entirely on local hardware you control. Peer-to-peer handshake protocol â€” Nodes that find each other remember the connection, and it strengthens over time.&lt;/p&gt; &lt;h1&gt;Requirements&lt;/h1&gt; &lt;p&gt;Node.js 18 or later. Ollama with a compatible model (Llama-3 8B recommended). GPU with 6GB+ VRAM. A browser.&lt;/p&gt; &lt;h1&gt;License&lt;/h1&gt; &lt;p&gt;Apache 2.0 â€” open source, fork it, build on it. &lt;/p&gt; &lt;p&gt;&amp;quot;Mister Atompunk Presents: Memory Ring&amp;quot; Copyright 2025-2026 Mister Atompunk LLC.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;*From the workbench of Mister Atompunk Presents.*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MisterAtompunk"&gt; /u/MisterAtompunk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://misteratompunk.itch.io/mr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxlbo0/mr_memory_ring_node_by_mister_atompunk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qxlbo0/mr_memory_ring_node_by_mister_atompunk/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-06T15:53:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy2ee8</id>
    <title>Not only did I get 99Â¢, it's sharable?</title>
    <updated>2026-02-07T03:03:10+00:00</updated>
    <author>
      <name>/u/NEETFLIX36</name>
      <uri>https://old.reddit.com/user/NEETFLIX36</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qy2ee8/not_only_did_i_get_99_its_sharable/"&gt; &lt;img alt="Not only did I get 99Â¢, it's sharable?" src="https://preview.redd.it/010aup6ohwhg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=14276cf2d38ca8c4a1190eb7f593244248c9f7b2" title="Not only did I get 99Â¢, it's sharable?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NEETFLIX36"&gt; /u/NEETFLIX36 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/010aup6ohwhg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qy2ee8/not_only_did_i_get_99_its_sharable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qy2ee8/not_only_did_i_get_99_its_sharable/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T03:03:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxrl4o</id>
    <title>Qwen3-ASR Swift: On-Device Speech Recognition for Apple Silicon</title>
    <updated>2026-02-06T19:38:03+00:00</updated>
    <author>
      <name>/u/ivan_digital</name>
      <uri>https://old.reddit.com/user/ivan_digital</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to release &lt;a href="https://github.com/ivan-digital/qwen3-asr-swift"&gt;https://github.com/ivan-digital/qwen3-asr-swift&lt;/a&gt;, an open-source Swift implementation of Alibaba's &lt;br /&gt; Qwen3-ASR, optimized for Apple Silicon using MLX. &lt;/p&gt; &lt;p&gt;Why Qwen3-ASR? Exceptional noise robustness â€” 3.5x better than Whisper in noisy conditions (17.9% vs 63% CER). &lt;/p&gt; &lt;p&gt;Features: &lt;br /&gt; - 52 languages (30 major + 22 Chinese dialects) &lt;br /&gt; - ~600MB model (4-bit quantized) &lt;br /&gt; - ~100ms latency on M-series chips &lt;br /&gt; - Fully local, no cloud API &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ivan-digital/qwen3-asr-swift"&gt;https://github.com/ivan-digital/qwen3-asr-swift&lt;/a&gt; | Apache 2.0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivan_digital"&gt; /u/ivan_digital &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxrl4o/qwen3asr_swift_ondevice_speech_recognition_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxrl4o/qwen3asr_swift_ondevice_speech_recognition_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qxrl4o/qwen3asr_swift_ondevice_speech_recognition_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-06T19:38:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxmmio</id>
    <title>Best models on your experience with 16gb VRAM? (7800xt)</title>
    <updated>2026-02-06T16:40:26+00:00</updated>
    <author>
      <name>/u/roshan231</name>
      <uri>https://old.reddit.com/user/roshan231</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m running a 7800 XT (16 GB VRAM) and looking to get the best balance of quality vs performance with Ollama.&lt;/p&gt; &lt;p&gt;What models have you personally had good results with on 16 GB VRAM?&lt;/p&gt; &lt;p&gt;Really I'm just curious about your use cases as well. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/roshan231"&gt; /u/roshan231 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxmmio/best_models_on_your_experience_with_16gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxmmio/best_models_on_your_experience_with_16gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qxmmio/best_models_on_your_experience_with_16gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-06T16:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxvm16</id>
    <title>Run Ollama on Legion 5.</title>
    <updated>2026-02-06T22:11:40+00:00</updated>
    <author>
      <name>/u/iamoutofwords</name>
      <uri>https://old.reddit.com/user/iamoutofwords</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to run Ollama on Legion 5 and use Moltbot with it. Can it handle that?&lt;br /&gt; Specs are:&lt;br /&gt; - 16gb RAM&lt;br /&gt; - 512 GB SSD&lt;br /&gt; - Ryzen 7 5800H 3.2GHz&lt;br /&gt; - Rtx 3050 Ti 6GB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamoutofwords"&gt; /u/iamoutofwords &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxvm16/run_ollama_on_legion_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxvm16/run_ollama_on_legion_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qxvm16/run_ollama_on_legion_5/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-06T22:11:40+00:00</published>
  </entry>
</feed>
