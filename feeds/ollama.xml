<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-26T20:17:46+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1rekdbi</id>
    <title>Getting Goose to actually work with local Ollama models ‚Äî what I ran into and what I built</title>
    <updated>2026-02-25T17:51:38+00:00</updated>
    <author>
      <name>/u/BenevolentJoker</name>
      <uri>https://old.reddit.com/user/BenevolentJoker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rekdbi/getting_goose_to_actually_work_with_local_ollama/"&gt; &lt;img alt="Getting Goose to actually work with local Ollama models ‚Äî what I ran into and what I built" src="https://external-preview.redd.it/pbgfqwlMXm6IOOu8BQ8ITwlsb3n0jqJch4zsZIR-Pe8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09c91535c6e4b95c05f5c704c34d8baac8eee6d2" title="Getting Goose to actually work with local Ollama models ‚Äî what I ran into and what I built" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama adapted goose so that ollama on goose actually is able to more reliably use more than just 1-2 ollama models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BenevolentJoker"&gt; /u/BenevolentJoker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ravhqi/getting_goose_to_actually_work_with_local_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rekdbi/getting_goose_to_actually_work_with_local_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rekdbi/getting_goose_to_actually_work_with_local_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T17:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1revteu</id>
    <title>Hypeboard.ai - A live LLM Leaderboard based on /r/localllama posts/comments</title>
    <updated>2026-02-26T00:54:58+00:00</updated>
    <author>
      <name>/u/peva3</name>
      <uri>https://old.reddit.com/user/peva3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;thought this might be of some interest to y'all as well if you're wondering how to stay on top of what new models are trending on reddit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peva3"&gt; /u/peva3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hypeboard.ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1revteu/hypeboardai_a_live_llm_leaderboard_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1revteu/hypeboardai_a_live_llm_leaderboard_based_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T00:54:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf24zc</id>
    <title>Why I can‚Äôt use gpu ?</title>
    <updated>2026-02-26T05:52:06+00:00</updated>
    <author>
      <name>/u/Far-Ebb-8088</name>
      <uri>https://old.reddit.com/user/Far-Ebb-8088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run ollama mainly on my laptop with i7, not very powerful but I have 16 gb of ram, but the gpu is always 0%, how can my laptop use the GPU ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Ebb-8088"&gt; /u/Far-Ebb-8088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf24zc/why_i_cant_use_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf24zc/why_i_cant_use_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf24zc/why_i_cant_use_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T05:52:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1resqne</id>
    <title>Need some help fixing "ollama pull"</title>
    <updated>2026-02-25T22:52:50+00:00</updated>
    <author>
      <name>/u/A_Zeppelin</name>
      <uri>https://old.reddit.com/user/A_Zeppelin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently use Ollama via docker, connected to Open-WebUI. Until recently, it had worked flawlessly for me. However, I recently have had loads of issues pulling any new models as of late, even from the Ollama library, always getting this error:&lt;/p&gt; &lt;p&gt;`Error: remove /root/.ollama/models/blobs/sha256-d838916...2522cf1a-partial-0: no such file or directory` However, this error only happens once the model has fully downloaded.&lt;/p&gt; &lt;p&gt;Indeed, when I Docker Exec into the container after it fails, it is correct that no such file exists. However, it DOES exist when I exec into the container mid-download. There's sha256-d...a-partial, sha256-d...a-partial-0, sha256-d...a-partial-1, ... , ... , all the way to sha256-d...a-partial-20. However, these files disappear midway through the download, reappear again, and a few are left behind after the download fails. If I delete them and attempt to re-download, it still fails.&lt;/p&gt; &lt;p&gt;I deploy ollama via `docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama --restart always -e OLLAMA_HOST=0.0.0.0 ollama/ollama`.&lt;/p&gt; &lt;p&gt;This did not always happen, and I've been running Ollama like this for around a year now, updating periodically (I updated to the latest version just to see if this was a bug, but did not fix the issue). Am I just doing something obviously dumb now? How can I fix this issue? Any help would be deeply appreciated!&lt;/p&gt; &lt;p&gt;If there's any additional info you need, please let me know. Ideally without permanently losing my LLM chat history, as there's a few chats I need to keep around to reference.&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;Regrettably, I ended having to just delete the contents of the &amp;quot;blobs&amp;quot; directory in the &amp;quot;Ollama&amp;quot; drive, and re-downloading all the models again. It fixed the issue, but not in such a way where I know the root cause.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A_Zeppelin"&gt; /u/A_Zeppelin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1resqne/need_some_help_fixing_ollama_pull/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1resqne/need_some_help_fixing_ollama_pull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1resqne/need_some_help_fixing_ollama_pull/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T22:52:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdyosq</id>
    <title>I built a locally-hosted AI agent that runs entirely on your own hardware no cloud, no subscriptions</title>
    <updated>2026-02-25T00:56:51+00:00</updated>
    <author>
      <name>/u/Janglerjoe</name>
      <uri>https://old.reddit.com/user/Janglerjoe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built LMAgent a pure Python AI agent that connects to any OpenAI-compatible LLM (LM Studio, Ollama, etc.) and actually does things on your computer.&lt;/p&gt; &lt;p&gt;No cloud. No API fees. No subscriptions. Runs 100% on your own hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it can do autonomously:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Read and write files&lt;/li&gt; &lt;li&gt;Run shell commands (bash / PowerShell)&lt;/li&gt; &lt;li&gt;Manage git (status, diff, commit, branch)&lt;/li&gt; &lt;li&gt;Track todos and multi-step plans&lt;/li&gt; &lt;li&gt;Spawn sub-agents to delegate tasks&lt;/li&gt; &lt;li&gt;Connect to external tools via MCP servers (web search, browsers, databases)&lt;/li&gt; &lt;li&gt;Schedule itself to wake up at a future time and resume work&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Three ways to run it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Terminal REPL ‚Äî conversational loop with a live background scheduler&lt;/li&gt; &lt;li&gt;One-shot CLI ‚Äî give it a task, get a result, exit&lt;/li&gt; &lt;li&gt;Web UI ‚Äî streaming tokens, inline tool calls, session browser, mobile-friendly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup is dead simple:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;pip install requests flask colorama&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Point it at your local LLM server&lt;/li&gt; &lt;li&gt;Set a workspace directory in a &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; &lt;li&gt;Run &lt;code&gt;python agent_main.py&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Works on Windows, macOS, and Linux. MIT licensed.&lt;/p&gt; &lt;p&gt;Would love feedback especially from anyone running it with larger models or unconventional LLM backends.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/janglerjoe-commits/LMAgent"&gt;https://github.com/janglerjoe-commits/LMAgent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Janglerjoe"&gt; /u/Janglerjoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdyosq/i_built_a_locallyhosted_ai_agent_that_runs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdyosq/i_built_a_locallyhosted_ai_agent_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdyosq/i_built_a_locallyhosted_ai_agent_that_runs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T00:56:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rezr97</id>
    <title>Qwen3.5:35b on Apple Silicon: How I Got 2x Faster Inference by Switching from Ollama to MLX (with benchmarks)</title>
    <updated>2026-02-26T03:51:25+00:00</updated>
    <author>
      <name>/u/rockinyp</name>
      <uri>https://old.reddit.com/user/rockinyp</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rockinyp"&gt; /u/rockinyp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1rezq19/qwen3535b_on_apple_silicon_how_i_got_2x_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rezr97/qwen3535b_on_apple_silicon_how_i_got_2x_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rezr97/qwen3535b_on_apple_silicon_how_i_got_2x_faster/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T03:51:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1res0sl</id>
    <title>Qwen 3.5 distilled vs GptOss</title>
    <updated>2026-02-25T22:25:42+00:00</updated>
    <author>
      <name>/u/SubstantialTea707</name>
      <uri>https://old.reddit.com/user/SubstantialTea707</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tried Qwen 3.5 in the 27B, 35B, or 122B versions? How does it perform with tool calling? I‚Äôm currently using GPT-OSS 20B, but especially the 120B version is, in my opinion, unbeatable. I find it very reliable and I‚Äôm running it in production on an RTX Pro 6000. With Qwen 3, I experienced lower reliability and it often went into loops, which made it unsuitable for production. Has anyone already tested it? Could you share real-world usage feedback? Because, as we know, benchmarks don‚Äôt always reflect real use cases. My goal is to run a chatbot with RAG and MCP tool calling.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SubstantialTea707"&gt; /u/SubstantialTea707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1res0sl/qwen_35_distilled_vs_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1res0sl/qwen_35_distilled_vs_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1res0sl/qwen_35_distilled_vs_gptoss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T22:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf1y4y</id>
    <title>Might be a bit lost in AI generated instructions.</title>
    <updated>2026-02-26T05:42:10+00:00</updated>
    <author>
      <name>/u/exodist</name>
      <uri>https://old.reddit.com/user/exodist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;I have a strix halo laptop with 128gb shared ram&lt;/li&gt; &lt;li&gt;Arch Linux&lt;/li&gt; &lt;li&gt;I installed ollama, it is working, and when I use it my gpu usage spikes, so it is using my gpu&lt;/li&gt; &lt;li&gt;Per google's recommendation I am using `aider` to connect to ollama to ask it to work on my code&lt;/li&gt; &lt;li&gt;I have tried the following models: &lt;ul&gt; &lt;li&gt;deepseek-coder&lt;/li&gt; &lt;li&gt;codellama&lt;/li&gt; &lt;li&gt;qwen2.5-coder:32b &amp;lt;- this one has worked the best&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;I am not super familiar with AI or its abilities, so I am starting small, asking it to generate a test file for a perl module I wrote. To be super simple I am asking for a test for just one method.&lt;/li&gt; &lt;li&gt;Not happy with any results so far&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It kind of works. deepseek-coder generated garbage perl. I think deepseeks garbage was still in place when I tried codellama, so I do not think I gave it a good trial. For qwen I reset my repo back to before any AI had at it, and it did a pretty good job, and I was able to iterate, but it did start to image perl tools that do not exist.&lt;/p&gt; &lt;p&gt;I decided a good next step would be to give it an understanding of all the interconnected perl repos I maintain. I first just tried to have it look at lib and t in one of my repos, and it decided I was asking it to look at too many files.&lt;/p&gt; &lt;p&gt;I asked google for advice, and ended up installing OpenWebUI and having it create a knowledge base that was all the source from all my repos. But then got stuck cause apparently aider cannot look at that knowledge base.&lt;/p&gt; &lt;p&gt;So now I am kind of stuck. How do I give the AI an understanding of my perl repositories? Ultimately I want to use it to help me migrate some code from a low quality tool written in the repo with a better tool that is in its own repo.&lt;/p&gt; &lt;p&gt;Any advice is appreciated! Just starting my AI journey, so I may be asking stupid questions, or going about this all wrong, I welcome pointers in the right direction.&lt;/p&gt; &lt;p&gt;I do not know much about python, it seems to be one of the few languages I just cannot read well. The lack of braces means I keep overflowing in my brain and think I am still reading one &amp;quot;block&amp;quot; when in fact I have actually moved on to a couple new ones. I mainly mention this because I am kind of lost with installing/using python tools, I installed aider with arch's aur repo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/exodist"&gt; /u/exodist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf1y4y/might_be_a_bit_lost_in_ai_generated_instructions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf1y4y/might_be_a_bit_lost_in_ai_generated_instructions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf1y4y/might_be_a_bit_lost_in_ai_generated_instructions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T05:42:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1reeixj</id>
    <title>From Pikachu to ZYRON: We Built a Fully Local AI Desktop Assistant That Runs Completely Offline</title>
    <updated>2026-02-25T14:22:10+00:00</updated>
    <author>
      <name>/u/No-Mess-8224</name>
      <uri>https://old.reddit.com/user/No-Mess-8224</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago I posted here about a small personal project I was building called Pikachu, a local desktop voice assistant. Since then the project has grown way bigger than I expected, got contributions from some really talented people, and evolved into something much more serious. We renamed it to ZYRON and it has basically turned into a full local AI desktop assistant that runs entirely on your own machine.&lt;/p&gt; &lt;p&gt;The main goal has always been simple. I love the idea of AI assistants, but I hate the idea of my files, voice, screenshots, and daily computer activity being uploaded to cloud services. So we built the opposite. ZYRON runs fully offline using a local LLM through Ollama, and the entire system is designed around privacy first. Nothing gets sent anywhere unless I explicitly ask it to send something to my own Telegram.&lt;/p&gt; &lt;p&gt;You can control the PC with voice by saying a wake word and then speaking normally. It can open apps, control media, set volume, take screenshots, shut down the PC, search the web in the background, and run chained commands like opening a browser and searching something in one go. It also responds back using offline text to speech, which makes it feel surprisingly natural to use day to day.&lt;/p&gt; &lt;p&gt;The remote control side became one of the most interesting parts. From my phone I can message a Telegram bot and basically control my laptop from anywhere. If I forget a file, I can ask it to find the document I opened earlier and it sends the file directly to me. It keeps a 30 day history of file activity and lets me search it using natural language. That feature alone has already saved me multiple times.&lt;/p&gt; &lt;p&gt;We also leaned heavily into security and monitoring. ZYRON can silently capture screenshots, take webcam photos, record short audio clips, and send them to Telegram. If a laptop gets stolen and connects to the internet, it can report IP address, ISP, city, coordinates, and a Google Maps link. Building and testing that part honestly felt surreal the first time it worked.&lt;/p&gt; &lt;p&gt;On the productivity side it turned into a full system monitor. It can report CPU, RAM, battery, storage, running apps, and even read all open browser tabs. There is a clipboard history logger so copied text is never lost. There is a focus mode that kills distracting apps and closes blocked websites automatically. There is even a ‚Äúzombie process‚Äù monitor that detects apps eating RAM in the background and lets you kill them remotely.&lt;/p&gt; &lt;p&gt;One feature I personally love is the stealth research mode. There is a Firefox extension that creates a bridge between the browser and the assistant, so it can quietly open a background tab, read content, and close it without any window appearing. Asking random questions and getting answers from a laptop that looks idle is strangely satisfying.&lt;/p&gt; &lt;p&gt;The whole philosophy of the project is that it does not try to compete with giant cloud models at writing essays. Instead it focuses on being a powerful local system automation assistant that respects privacy. The local model is smaller, but for controlling a computer it is more than enough, and the tradeoff feels worth it.&lt;/p&gt; &lt;p&gt;We are planning a lot next. Linux and macOS support, geofence alerts, motion triggered camera capture, scheduling and automation, longer memory, and eventually a proper mobile companion app instead of Telegram. As local models improve, the assistant will naturally get smarter too.&lt;/p&gt; &lt;p&gt;This started as a weekend experiment and slowly turned into something I now use daily. I would genuinely love feedback, ideas, or criticism from people here. If you have ever wanted an AI assistant that lives only on your own machine, I think you might find this interesting.&lt;/p&gt; &lt;p&gt;GitHub Repo - &lt;a href="https://github.com/Surajkumar5050/zyron-assistant"&gt;Link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mess-8224"&gt; /u/No-Mess-8224 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T14:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf36a8</id>
    <title>Benchmarking qwen3.5:35b vs gpt-oss:20b for Agentic Workloads (Ollama, Apple Silicon)</title>
    <updated>2026-02-26T06:51:20+00:00</updated>
    <author>
      <name>/u/pwbdecker</name>
      <uri>https://old.reddit.com/user/pwbdecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rf36a8/benchmarking_qwen3535b_vs_gptoss20b_for_agentic/"&gt; &lt;img alt="Benchmarking qwen3.5:35b vs gpt-oss:20b for Agentic Workloads (Ollama, Apple Silicon)" src="https://external-preview.redd.it/-W3Te0N3NSI2YKnRqaiMQ7io69snlAPAbJcAbQmfjk4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=782d1f26e2d58963b787b26421e110b13d5dabbc" title="Benchmarking qwen3.5:35b vs gpt-oss:20b for Agentic Workloads (Ollama, Apple Silicon)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pwbdecker"&gt; /u/pwbdecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jaredlockhart/penny/blob/main/docs/benchmarking-qwen35-vs-gpt-oss.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf36a8/benchmarking_qwen3535b_vs_gptoss20b_for_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf36a8/benchmarking_qwen3535b_vs_gptoss20b_for_agentic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T06:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf6ysq</id>
    <title>Ollama Based Private API, CLI, Agent Builder</title>
    <updated>2026-02-26T10:44:48+00:00</updated>
    <author>
      <name>/u/GhostGPT5</name>
      <uri>https://old.reddit.com/user/GhostGPT5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI that doesn't say no. No filter, From Ghost Nano to Ghost Ultra, every model is tuned for direct high-signal output. Stop Getting Censored 8 free prompts a day. No card. No strings. Just start talking. &lt;a href="https://ghostgpt.live"&gt;https://ghostgpt.live&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhostGPT5"&gt; /u/GhostGPT5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf6ysq/ollama_based_private_api_cli_agent_builder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf6ysq/ollama_based_private_api_cli_agent_builder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf6ysq/ollama_based_private_api_cli_agent_builder/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T10:44:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf4k33</id>
    <title>AMBER ICI v3</title>
    <updated>2026-02-26T08:14:00+00:00</updated>
    <author>
      <name>/u/FreonMuskOfficial</name>
      <uri>https://old.reddit.com/user/FreonMuskOfficial</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rf4k33/amber_ici_v3/"&gt; &lt;img alt="AMBER ICI v3" src="https://preview.redd.it/fmd5rvv6sslg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35169090d9acfccbc8e8bc9ebe455a8e771b7397" title="AMBER ICI v3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Ollama locally and want more than just prompts? &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/gs-ai/AMBER-ICI"&gt;https://github.com/gs-ai/AMBER-ICI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AMBER ICI v3 is an industrial grade local command center built for serious builders, giving you multi model orchestration, live token streaming, agent and chain pipelines, archive search, timeline reconstruction, OCR extraction, investigative file ingestion, graph based output correlation, and real time GPU telemetry in one unified interface. It is designed for power users who want to spin up autonomous agents, chain models together, ingest massive archives, and actually see how models think and interact, all while it's fully local and you're in control. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FreonMuskOfficial"&gt; /u/FreonMuskOfficial &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fmd5rvv6sslg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf4k33/amber_ici_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf4k33/amber_ici_v3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T08:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf61qd</id>
    <title>Made my first project, Autonomous video generator</title>
    <updated>2026-02-26T09:49:00+00:00</updated>
    <author>
      <name>/u/Pronation1227</name>
      <uri>https://old.reddit.com/user/Pronation1227</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Pronation1227/AVB#video-generator-bot"&gt;&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Hi, This is my first project (which i actually managed to complete)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/Pronation1227/AVB#hi-this-is-my-first-project-which-i-actually-managed-to-complete"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;About me: I am in high school and have been coding on and off for a few years now.&lt;/p&gt; &lt;p&gt;a quick overview of this project, its basically a storytime generator inspired from the insta videos you see on reels. There was no real motive behind building this i was just frustrated of tutorial hell and hence built the first thing that came to my mind&lt;/p&gt; &lt;p&gt;I admit i did use AI to help me with structuring the project into different files ie: output, notes, background, scripts. I also used ai for the ffmpeg subprocess in generate_vid.py as i had no idea what ffmpeg is or how to use it. But all other lines of code in all the files have been written by me&lt;/p&gt; &lt;p&gt;Thanks a lot, would really appreciate feedback on what could i improve and where can i learn further.&lt;/p&gt; &lt;p&gt;github - &lt;a href="https://github.com/Pronation1227/AVB"&gt;https://github.com/Pronation1227/AVB&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pronation1227"&gt; /u/Pronation1227 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf61qd/made_my_first_project_autonomous_video_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf61qd/made_my_first_project_autonomous_video_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf61qd/made_my_first_project_autonomous_video_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T09:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf6cc7</id>
    <title>Need help on API key export...</title>
    <updated>2026-02-26T10:07:15+00:00</updated>
    <author>
      <name>/u/Dakacchan_</name>
      <uri>https://old.reddit.com/user/Dakacchan_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rf6cc7/need_help_on_api_key_export/"&gt; &lt;img alt="Need help on API key export..." src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="Need help on API key export..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dakacchan_"&gt; /u/Dakacchan_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1rf6c6u/need_help_on_api_key_export/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf6cc7/need_help_on_api_key_export/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf6cc7/need_help_on_api_key_export/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T10:07:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfbew8</id>
    <title>Question about installing ollama Claude</title>
    <updated>2026-02-26T14:21:03+00:00</updated>
    <author>
      <name>/u/PerformerAromatic836</name>
      <uri>https://old.reddit.com/user/PerformerAromatic836</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i am installing the ollama/claude local thing. The last question of the claude config is the question of &amp;quot;do you trust this folder&amp;quot;, it asks for access to my entire users/myname folder, whilst I don't want that. Is there a way to give claude/ollama only access to a certain folder with zero documents in it yet, so that claude/ollama will not have access to any personal documents?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerformerAromatic836"&gt; /u/PerformerAromatic836 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfbew8/question_about_installing_ollama_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfbew8/question_about_installing_ollama_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfbew8/question_about_installing_ollama_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T14:21:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfbrnh</id>
    <title>wired up ollama to contextui today</title>
    <updated>2026-02-26T14:35:34+00:00</updated>
    <author>
      <name>/u/Sharp-Mouse9049</name>
      <uri>https://old.reddit.com/user/Sharp-Mouse9049</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;messed around with ContextUI today. looks like it integrates with huggingface, but i just wired it to download an ollama model and run it direct instead. built a small local workflow pretty quickly. noticed it's open source now too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sharp-Mouse9049"&gt; /u/Sharp-Mouse9049 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfbrnh/wired_up_ollama_to_contextui_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfbrnh/wired_up_ollama_to_contextui_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfbrnh/wired_up_ollama_to_contextui_today/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T14:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfd6nn</id>
    <title>What do think about my setup?</title>
    <updated>2026-02-26T15:30:02+00:00</updated>
    <author>
      <name>/u/d4mations</name>
      <uri>https://old.reddit.com/user/d4mations</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/d4mations"&gt; /u/d4mations &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1rfat60/what_do_think_about_my_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfd6nn/what_do_think_about_my_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfd6nn/what_do_think_about_my_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T15:30:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf84qc</id>
    <title>AVCI GHOST - A CyberSec. UI Experiment for Ollama</title>
    <updated>2026-02-26T11:51:06+00:00</updated>
    <author>
      <name>/u/Ezanyiyenler</name>
      <uri>https://old.reddit.com/user/Ezanyiyenler</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"&gt; &lt;img alt="AVCI GHOST - A CyberSec. UI Experiment for Ollama" src="https://external-preview.redd.it/gAWqWQOlQP9fUw6_szN7BIGdnEqcLZpbQVa7NXlrTNU.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=a1d79284496f099470f492d9512e78cdedff357f" title="AVCI GHOST - A CyberSec. UI Experiment for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I created this simple UI for Ollama as a learning experiment. It's basically a Python script with 10 menu options that interact with local models. IMPORTAT DISCLAIMER:&lt;/p&gt; &lt;p&gt;This is NOT a real hacking tool. It's purely a UI EXPERIMENT. Most modules are SIMULATIONS and don't actually work. It's just a proof-of-concept I built while learning about Ollama and AI models.&lt;br /&gt; For open source code and detailed information, check out my GitHub address &lt;a href="https://github.com/ihsan896/Avci_Ghost"&gt;https://github.com/ihsan896/Avci_Ghost&lt;/a&gt;Don't forget to like if you enjoy üíó&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hysx7ys8utlg1.png?width=981&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=970ec0db987dd8c39ec8d4ac00ba4d07af2f5357"&gt;Just the UI layout - the modules are placeholders/demos. The actual functionality is connecting to local Ollama models. All security features are simulated for educational purposes.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ezanyiyenler"&gt; /u/Ezanyiyenler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rf84qc/avci_ghost_a_cybersec_ui_experiment_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T11:51:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1rffwgd</id>
    <title>Ollama model response very slow</title>
    <updated>2026-02-26T17:08:39+00:00</updated>
    <author>
      <name>/u/CookieClicker999</name>
      <uri>https://old.reddit.com/user/CookieClicker999</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm completely new to the world of AI (except for using some copilot m365 chat client). I'm trying to achieve some assistance in coding Python/JS/TS. I've followed some online instructions, installed Ollama using brew on my MBP M2 Max 64GB of ram. I've tried the glm-4.7-flash as it was highly regarded for correctness and had a nice context size according to some searches. When trying to query this model in the ollama client or using Jetbrains IDe i'm waiting for responses for more than 15 minutes usually. I've got a feeling i'm doing something wrong. It's using about 18GB of ram which is fine in my environment with lots to spare. Is there anyone able to give me som guidence as where to start looking into the issue or can recommend models that should be a better fit? I don't mind waiting for a minute but this is absolutely insane. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CookieClicker999"&gt; /u/CookieClicker999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rffwgd/ollama_model_response_very_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rffwgd/ollama_model_response_very_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rffwgd/ollama_model_response_very_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T17:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfgazl</id>
    <title>Ollama Model Search</title>
    <updated>2026-02-26T17:23:07+00:00</updated>
    <author>
      <name>/u/yukonit</name>
      <uri>https://old.reddit.com/user/yukonit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there an API that allows me to search models on Ollama, similar to searching via the Hugging Face API? For example, when I search for ‚Äúqwen,‚Äù is there an API endpoint that returns results related to ‚Äúqwen,‚Äù like the search on the Ollama website?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yukonit"&gt; /u/yukonit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfgazl/ollama_model_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfgazl/ollama_model_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfgazl/ollama_model_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T17:23:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfd045</id>
    <title>Introducing CLAM (Cognitive Large Application Model): A dual-level cognitive architecture for LLM agents (Short/Long term memory)</title>
    <updated>2026-02-26T15:23:14+00:00</updated>
    <author>
      <name>/u/Short-Confidence6287</name>
      <uri>https://old.reddit.com/user/Short-Confidence6287</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rfd045/introducing_clam_cognitive_large_application/"&gt; &lt;img alt="Introducing CLAM (Cognitive Large Application Model): A dual-level cognitive architecture for LLM agents (Short/Long term memory)" src="https://preview.redd.it/otfvuppowulg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a9162abf798ca97814bff71a53b8cd166513ecc" title="Introducing CLAM (Cognitive Large Application Model): A dual-level cognitive architecture for LLM agents (Short/Long term memory)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Hi everyone! I'd like to share CLAM, a new approach to LLM agents I've been working on. Instead of endless fine-tuning, I designed a two-level cognitive architecture that simulates the human mind. üß† CLAM perceives, doubts (via a formidable internal 'Critic'), consolidates valid memories, and... forgets what is irrelevant. The code is now open-source on GitHub! I‚Äôd love to hear your thoughts and suggestions on how to improve it. üëá GitHub: &lt;a href="https://github.com/marcellom66/CLAM"&gt;https://github.com/marcellom66/CLAM&lt;/a&gt;&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Short-Confidence6287"&gt; /u/Short-Confidence6287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/otfvuppowulg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfd045/introducing_clam_cognitive_large_application/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfd045/introducing_clam_cognitive_large_application/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T15:23:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfjraq</id>
    <title>Tools won't execute when running Claude Code in Docker against Ollama locally</title>
    <updated>2026-02-26T19:27:14+00:00</updated>
    <author>
      <name>/u/crispyghost</name>
      <uri>https://old.reddit.com/user/crispyghost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Claude Code running inside a docker container, it's pointed to Ollama running in another container. Claude can't execute tools.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This seems like it could be an Ollama or qwen3-coder issue. If I disable these env vars and log in with Anthropic, claude code in my container is talking with its servers and tool commands execute properly.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # Local Ollama (Windows Docker Desktop host) #ANTHROPIC_BASE_URL: &amp;quot;http://host.docker.internal:11434&amp;quot; #ANTHROPIC_AUTH_TOKEN: &amp;quot;ollama&amp;quot; #ANTHROPIC_DEFAULT_HAIKU_MODEL: &amp;quot;command-r7b&amp;quot; #ANTHROPIC_DEFAULT_SONNET_MODEL: &amp;quot;qwen3-coder&amp;quot; #ANTHROPIC_DEFAULT_OPUS_MODEL: &amp;quot;qwen3-coder&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm on Ollama 0.17.0, so this should be supported.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Original:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I use these commands to get in into claude:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;docker compose up --build&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;docker compose exec claude-code bash&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;claude&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;After that, I can chat with Claude using the Ollama service; that's working fine. &amp;quot;Tell me a joke&amp;quot; and I get a response. As soon as I ask Claude to do something that requires running a tool, I get this sort of output and it stops.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚ùØ Run ls -la and then summarize functional-spec.md. ‚óè I'll run ls -la to see the files in the workspace, then summarize the functional-spec.md file. &amp;lt;function=run&amp;gt; &amp;lt;parameter=command&amp;gt; ls -la &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm reading that this is &amp;quot;ollama style&amp;quot; tool commands, but Claude wants Anthropic-style tool commands. I'm not sure what to do at this stage because none of the documentation mentions needing to adapt these tool patterns.&lt;/p&gt; &lt;p&gt;My docker-compose sets up my models as such --&lt;/p&gt; &lt;pre&gt;&lt;code&gt; ANTHROPIC_DEFAULT_HAIKU_MODEL: &amp;quot;command-r7b&amp;quot; ANTHROPIC_DEFAULT_SONNET_MODEL: &amp;quot;qwen3-coder&amp;quot; ANTHROPIC_DEFAULT_OPUS_MODEL: &amp;quot;qwen3-coder&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And I can confirm that it's talking to qwen3-coder running in docker.&lt;/p&gt; &lt;p&gt;If I open a bash terminal in the claude-docker container and curl a message to Ollama that should require tools, I think I'm getting the output that Claude Code needs.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl -N http://host.docker.internal:11434/v1/messages \ -H &amp;quot;Content-Type: application/json&amp;quot; \ -d '{ &amp;quot;model&amp;quot;: &amp;quot;qwen3-coder&amp;quot;, &amp;quot;max_tokens&amp;quot;: 1024, &amp;quot;stream&amp;quot;: true, &amp;quot;tools&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;get_weather&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Get the current weather in a location&amp;quot;, &amp;quot;input_schema&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;location&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;The city and state&amp;quot; } }, &amp;quot;required&amp;quot;: [&amp;quot;location&amp;quot;] } } ], &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What is the weather in San Francisco?&amp;quot; } ] }' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Results in&lt;/p&gt; &lt;pre&gt;&lt;code&gt;^[[Oevent: message_start data: {&amp;quot;type&amp;quot;:&amp;quot;message_start&amp;quot;,&amp;quot;message&amp;quot;:{&amp;quot;id&amp;quot;:&amp;quot;msg_e9a02c0fd0b3c29742c6a075&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;message&amp;quot;,&amp;quot;role&amp;quot;:&amp;quot;assistant&amp;quot;,&amp;quot;model&amp;quot;:&amp;quot;qwen3-coder&amp;quot;,&amp;quot;content&amp;quot;:[],&amp;quot;usage&amp;quot;:{&amp;quot;input_tokens&amp;quot;:81,&amp;quot;output_tokens&amp;quot;:0}}} event: content_block_start data: {&amp;quot;type&amp;quot;:&amp;quot;content_block_start&amp;quot;,&amp;quot;index&amp;quot;:0,&amp;quot;content_block&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;tool_use&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;call_xajreurq&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;get_weather&amp;quot;,&amp;quot;input&amp;quot;:{}}} event: content_block_delta data: {&amp;quot;type&amp;quot;:&amp;quot;content_block_delta&amp;quot;,&amp;quot;index&amp;quot;:0,&amp;quot;delta&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;input_json_delta&amp;quot;,&amp;quot;partial_json&amp;quot;:&amp;quot;{\&amp;quot;location\&amp;quot;:\&amp;quot;San Francisco\&amp;quot;}&amp;quot;}} event: content_block_stop data: {&amp;quot;type&amp;quot;:&amp;quot;content_block_stop&amp;quot;,&amp;quot;index&amp;quot;:0} event: message_delta data: {&amp;quot;type&amp;quot;:&amp;quot;message_delta&amp;quot;,&amp;quot;delta&amp;quot;:{&amp;quot;stop_reason&amp;quot;:&amp;quot;tool_use&amp;quot;},&amp;quot;usage&amp;quot;:{&amp;quot;input_tokens&amp;quot;:295,&amp;quot;output_tokens&amp;quot;:23}} event: message_stop data: {&amp;quot;type&amp;quot;:&amp;quot;message_stop&amp;quot;} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So, I THINK this is set up right, but I'm not sure why Claude won't execute tools.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;/permissions has only Allow rules.&lt;/li&gt; &lt;li&gt;Running in edit-auto approve mode or with --dangerously-skip-permissions does not impact the tool usage issue.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any ideas where I should look?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crispyghost"&gt; /u/crispyghost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfjraq/tools_wont_execute_when_running_claude_code_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfjraq/tools_wont_execute_when_running_claude_code_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfjraq/tools_wont_execute_when_running_claude_code_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T19:27:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rew6hl</id>
    <title>A fully visual, private and local AI Creative Suite. No cloud, no subscriptions, runs on your hardware.</title>
    <updated>2026-02-26T01:10:32+00:00</updated>
    <author>
      <name>/u/Ollie_IDE</name>
      <uri>https://old.reddit.com/user/Ollie_IDE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We‚Äôve been working on a desktop application for those of us who want to use AI models locally but prefer a full visual interface. It‚Äôs called Ollie, and it‚Äôs an offline-first creative suite that runs entirely on your machine.&lt;/p&gt; &lt;p&gt;What it includes:&lt;/p&gt; &lt;p&gt;Code: A coding environment (Node, Python, Java) with IntelliSense. You can also ask it to instantly generate and run interactive apps.&lt;/p&gt; &lt;p&gt;Media Suite: It has built-in video, image canvas, and a 3D editor.&lt;/p&gt; &lt;p&gt;Rich Text: A distraction-free markdown and writing environment that keeps your project context local.&lt;/p&gt; &lt;p&gt;Under the hood&lt;/p&gt; &lt;p&gt;Ollama Native: Hooks directly into your local Ollama setup.&lt;/p&gt; &lt;p&gt;Bring Your Own Keys: If you need to use Anthropic, Gemini, or OpenAI, you can plug in your API key directly.&lt;/p&gt; &lt;p&gt;Agent &amp;amp; MCP Support: Connects to GitHub, local databases, and custom tools via the Model Context Protocol (MCP).&lt;/p&gt; &lt;p&gt;&amp;quot;Glass-Box&amp;quot; UI: You can visually audit every file touch, tool call, and token before the AI executes it.&lt;/p&gt; &lt;p&gt;It runs natively on macOS, Windows, and Linux. Because it relies on your hardware, there are no recurring subscriptions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://costa-and-associates.com/ollie"&gt;Ollie&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ollie_IDE"&gt; /u/Ollie_IDE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rew6hl/a_fully_visual_private_and_local_ai_creative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rew6hl/a_fully_visual_private_and_local_ai_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rew6hl/a_fully_visual_private_and_local_ai_creative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T01:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfhzxn</id>
    <title>Mimic Digital AI Assistant</title>
    <updated>2026-02-26T18:23:05+00:00</updated>
    <author>
      <name>/u/GullibleNarwhal</name>
      <uri>https://old.reddit.com/user/GullibleNarwhal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rfhzxn/mimic_digital_ai_assistant/"&gt; &lt;img alt="Mimic Digital AI Assistant" src="https://preview.redd.it/7np640opsvlg1.png?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=f5383dac3a73570143f6ead2fb6638c8a5c2ebd1" title="Mimic Digital AI Assistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a Chatbot wrapper that gives locally installed Ollama models a digital avatar with voice and lip-syncing functionality! Any VRM or VRMA is able to be uploaded and applied. Voice creation and TTS is deployed via Qwen3, browser-based TTS, or can be disabled for seamless conversations with a model. Meet Mimic! Any feedback would be greatly appreciated!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bmerriott/MIMIC-Multipurpose-Intelligent-Molecular-Information-Catalyst-/releases/tag/v1.1.0"&gt;https://github.com/bmerriott/MIMIC-Multipurpose-Intelligent-Molecular-Information-Catalyst-/releases/tag/v1.1.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GullibleNarwhal"&gt; /u/GullibleNarwhal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rfhzxn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfhzxn/mimic_digital_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfhzxn/mimic_digital_ai_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T18:23:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfffru</id>
    <title>Ollama-Vision-Memory-Desktop ‚Äî Local AI Desktop Assistant with Vision + Memory!</title>
    <updated>2026-02-26T16:52:04+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I just published my new open-source project: &lt;strong&gt;Ollama-Vision-Memory-Desktop&lt;/strong&gt; ‚Äî a &lt;em&gt;privacy-focused, offline-first desktop assistant&lt;/em&gt; built on top of &lt;strong&gt;Ollama&lt;/strong&gt; that combines long-term memory, computer vision, and customizable AI behavior.&lt;/p&gt; &lt;p&gt;üë®‚Äçüíª &lt;strong&gt;What it is&lt;/strong&gt;&lt;br /&gt; It‚Äôs a full &lt;strong&gt;Python/PyQt5 desktop app&lt;/strong&gt; that connects to your local Ollama instance and turns it into a powerful assistant with:&lt;/p&gt; &lt;p&gt;‚ú® &lt;strong&gt;Intelligent Chat&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Local AI backend (connects to &lt;code&gt;ollama serve&lt;/code&gt;)&lt;br /&gt; ‚Ä¢ Support for switching between text and vision models on the fly&lt;br /&gt; ‚Ä¢ Session instructions &amp;amp; custom context per conversation&lt;/p&gt; &lt;p&gt;üì∏ &lt;strong&gt;Computer Vision&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Live webcam feed with real-time image analysis&lt;br /&gt; ‚Ä¢ Manual or automated vision scans using vision-capable models like LLaVA, BakLLaVA, etc.&lt;br /&gt; ‚Ä¢ Vision logs saved locally for reference&lt;/p&gt; &lt;p&gt;üß† &lt;strong&gt;Persistent Memory (‚ÄúMind Archive‚Äù)&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Automatic indexing and storage of chats, vision logs, PDFs, and docs&lt;br /&gt; ‚Ä¢ Semantic search across your archive&lt;br /&gt; ‚Ä¢ Contextual retrieval so your assistant &lt;em&gt;remembers past interactions&lt;/em&gt;&lt;br /&gt; ‚Ä¢ Multi-format support: images, audio, text, vision outputs ‚Äî all searchable&lt;/p&gt; &lt;p&gt;‚öôÔ∏è &lt;strong&gt;Custom Behavior &amp;amp; UX&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ System Prompt Editor with presets (e.g., Research Assistant, Creative Companion)&lt;br /&gt; ‚Ä¢ Dark mode and native UI feel&lt;br /&gt; ‚Ä¢ Archive browser for managing stored content&lt;/p&gt; &lt;p&gt;üì¶ &lt;strong&gt;Built For Privacy &amp;amp; Offline Use&lt;/strong&gt;&lt;br /&gt; Everything runs locally ‚Äî no cloud APIs, no telemetry, no data leaks. Perfect for folks who want control and privacy when experimenting with local LLMs like llama3/vision, Gemma, Mistral, etc.&lt;/p&gt; &lt;p&gt;üß∞ &lt;strong&gt;Get Started&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the repo&lt;/li&gt; &lt;li&gt;Install dependencies (&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Start Ollama (&lt;code&gt;ollama serve&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Run the app (&lt;code&gt;python main.py&lt;/code&gt;) ‚Ä¶ and your local AI assistant with vision + memory is ready!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;üîó Check it out here: &lt;a href="https://github.com/Laszlobeer/Ollama-Vision-Memory-Desktop?utm_source=chatgpt.com"&gt;https://github.com/Laszlobeer/Ollama-Vision-Memory-Desktop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfffru/ollamavisionmemorydesktop_local_ai_desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rfffru/ollamavisionmemorydesktop_local_ai_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rfffru/ollamavisionmemorydesktop_local_ai_desktop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-26T16:52:04+00:00</published>
  </entry>
</feed>
