<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-12T05:35:50+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1r0n1sg</id>
    <title>We won a hackathon with this project using Ollama. But is it actually useful?</title>
    <updated>2026-02-10T01:25:10+00:00</updated>
    <author>
      <name>/u/BriefAd2120</name>
      <uri>https://old.reddit.com/user/BriefAd2120</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: I built a 3d memory layer to visualize your chats with a custom MCP server to inject relevant context, Looking for feedback!&lt;/p&gt; &lt;p&gt;Cortex turns raw chat history into reusable context using hybrid retrieval (about 65% keyword, 35% semantic), local summaries with Qwen 2.5 8B, and auto system prompts so setup goes from minutes to seconds.&lt;/p&gt; &lt;p&gt;It also runs through a custom MCP server with search + fetch tools, so external LLMs like Claude can pull the right memory at inference time.&lt;/p&gt; &lt;p&gt;And because scrolling is pain, I added a 3D brain-style map built with UMAP, K-Means, and Three.js so you can explore conversations like a network instead of a timeline.&lt;/p&gt; &lt;p&gt;We won the hackathon with it, but I want a reality check: is this actually useful, or just a cool demo?&lt;/p&gt; &lt;p&gt;YouTube demo: &lt;a href="https://www.youtube.com/watch?v=SC_lDydnCF4"&gt;https://www.youtube.com/watch?v=SC_lDydnCF4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LinkedIn post: &lt;a href="https://www.linkedin.com/feed/update/urn:li:activity:7426518101162205184/"&gt;https://www.linkedin.com/feed/update/urn:li:activity:7426518101162205184/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github Link (pls star itü•∫): &lt;a href="https://github.com/Vibhor7-7/Cortex-CxC"&gt;https://github.com/Vibhor7-7/Cortex-CxC&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BriefAd2120"&gt; /u/BriefAd2120 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T01:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0q55x</id>
    <title>any good models?</title>
    <updated>2026-02-10T03:43:05+00:00</updated>
    <author>
      <name>/u/No-Mortgage4154</name>
      <uri>https://old.reddit.com/user/No-Mortgage4154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i recently found out about ollama and how its like a local ai and was wondering what are some good models out there my pc specs are: ryzen 7 7800x3d, 4070ti super nvidia, and ddr5 32gb ram.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mortgage4154"&gt; /u/No-Mortgage4154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0q55x/any_good_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0q55x/any_good_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0q55x/any_good_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T03:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0x5bs</id>
    <title>Is Ollama capable of doing all of that on a gaming Omen laptop (image object recognition, storywriting, TTS, image &amp; audio generation)?</title>
    <updated>2026-02-10T10:23:17+00:00</updated>
    <author>
      <name>/u/d_test_2030</name>
      <uri>https://old.reddit.com/user/d_test_2030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I was wondering if I can run all of this on a standard gaming laptop. Ideally it should be able to work in German.&lt;br /&gt; Also if you could help me find the right model (which will work smoothly on a laptop), I would be very thankful.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;- Detecting objects from a still image:&lt;/strong&gt; I assume LLava is the best model for that?&lt;br /&gt; &lt;strong&gt;- Writing short creative stories&lt;/strong&gt; (just a few sentences based on keywords provided): LLama or Gemma?&lt;br /&gt; - I assume &lt;strong&gt;image &amp;amp; audio generation&lt;/strong&gt; isn't the forte of Ollama, do you have other open source recommendations that work on Omen?&lt;br /&gt; - For &lt;strong&gt;TTS&lt;/strong&gt; I could also use a python library or does Ollama come with good German options?&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/d_test_2030"&gt; /u/d_test_2030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0x5bs/is_ollama_capable_of_doing_all_of_that_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0x5bs/is_ollama_capable_of_doing_all_of_that_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0x5bs/is_ollama_capable_of_doing_all_of_that_on_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T10:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r18ge5</id>
    <title>title: openclaw telegram works for /help and new session but freezes when the agent actually responds</title>
    <updated>2026-02-10T18:16:50+00:00</updated>
    <author>
      <name>/u/nurdthug</name>
      <uri>https://old.reddit.com/user/nurdthug</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nurdthug"&gt; /u/nurdthug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1r18fmb/title_openclaw_telegram_works_for_help_and_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r18ge5/title_openclaw_telegram_works_for_help_and_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r18ge5/title_openclaw_telegram_works_for_help_and_new/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T18:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1acup</id>
    <title>PolyMCP Major Update: New Website, New Inspector UX, Installable Desktop App, and skills.sh-First Workflow</title>
    <updated>2026-02-10T19:23:51+00:00</updated>
    <author>
      <name>/u/Just_Vugg_PolyMCP</name>
      <uri>https://old.reddit.com/user/Just_Vugg_PolyMCP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r1acup/polymcp_major_update_new_website_new_inspector_ux/"&gt; &lt;img alt="PolyMCP Major Update: New Website, New Inspector UX, Installable Desktop App, and skills.sh-First Workflow" src="https://external-preview.redd.it/TTmX4QB47ieCB6lScf1c5zSQ8KES1TshcAp18basPSg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4529bf6eedf476025a82561414f9218da9edf0d1" title="PolyMCP Major Update: New Website, New Inspector UX, Installable Desktop App, and skills.sh-First Workflow" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Vugg_PolyMCP"&gt; /u/Just_Vugg_PolyMCP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/poly-mcp/PolyMCP"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1acup/polymcp_major_update_new_website_new_inspector_ux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1acup/polymcp_major_update_new_website_new_inspector_ux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T19:23:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r18kd0</id>
    <title>Open-Source Apple Silicon Local LLM Benchmarking Software. Would love some feedback!</title>
    <updated>2026-02-10T18:20:38+00:00</updated>
    <author>
      <name>/u/peppaz</name>
      <uri>https://old.reddit.com/user/peppaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r18kd0/opensource_apple_silicon_local_llm_benchmarking/"&gt; &lt;img alt="Open-Source Apple Silicon Local LLM Benchmarking Software. Would love some feedback!" src="https://external-preview.redd.it/--hFrBiIzmgKy0LazyUbsxh-mezAHM_pi4pFCex-GiU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25fef397f6029bcc19c322f5a7f3635232833d0e" title="Open-Source Apple Silicon Local LLM Benchmarking Software. Would love some feedback!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peppaz"&gt; /u/peppaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Cyberpunk69420/anubis-oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r18kd0/opensource_apple_silicon_local_llm_benchmarking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r18kd0/opensource_apple_silicon_local_llm_benchmarking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T18:20:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0vhy0</id>
    <title>What's the fastest-response model to run on AMD (no-GPU) machines ?</title>
    <updated>2026-02-10T08:39:10+00:00</updated>
    <author>
      <name>/u/mohamedheiba</name>
      <uri>https://old.reddit.com/user/mohamedheiba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey I'm running Ollama on Kubernetes. Help me choose the best model for text summarization and writing documentation based on code please.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hetzner &lt;a href="https://www.hetzner.com/dedicated-rootserver/ax102/"&gt;AX102&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ryzen 7950X3D processor with 16 vCPU&lt;/li&gt; &lt;li&gt;96MB 3D V-Cache&lt;/li&gt; &lt;li&gt;192 GB DDR5 RAM.&lt;/li&gt; &lt;li&gt;No GPU.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Use case:&lt;/strong&gt; AI agent (OpenClaw) orchestrated via n8n, heavy on tool calling / function calling. Needs 40K+ context window. Not doing chat ‚Äî it's purely agentic workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I've tried so far:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;qwen3:32b&lt;/code&gt; (dense) ‚Äî painfully slow on CPU, unusable&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3:30b-a3b-q8_0&lt;/code&gt; (MoE) ‚Äî much faster, works well, decent tool calling&lt;/li&gt; &lt;li&gt;&lt;code&gt;gpt-oss:20b&lt;/code&gt; (MoE, MXFP4) ‚Äî noticeably faster than Qwen3-30B, lightest memory footprint (~12-16GB). Impressed so far.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Now considering:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPT-OSS 20B&lt;/strong&gt; ‚Äî 21B/3.6B active, MXFP4 native, ~12-16GB RAM. Lightest option. Built-in tool calling. Concerned about the harmony format playing nice with n8n.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7-Flash&lt;/strong&gt; ‚Äî 30B/3B active, 128K context, best SWE-bench scores. Saw reports of Ollama template issues ‚Äî is that fixed?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sticking with Qwen3-30B-A3B&lt;/strong&gt; but Q4_K_M &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I haven't tried any of them yet with OpenClaw or n8n.&lt;/p&gt; &lt;p&gt;What are your recommendations ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mohamedheiba"&gt; /u/mohamedheiba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0vhy0/whats_the_fastestresponse_model_to_run_on_amd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T08:39:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r11i8r</id>
    <title>Best model for Figma MCP server</title>
    <updated>2026-02-10T14:00:42+00:00</updated>
    <author>
      <name>/u/commandermd</name>
      <uri>https://old.reddit.com/user/commandermd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to design in Figma using a local LLM. The idea came after reading about using Claude to design in Figma in the browser. &lt;a href="https://cianfrani.dev/posts/a-better-figma-mcp/"&gt;https://cianfrani.dev/posts/a-better-figma-mcp/&lt;/a&gt; I love the concept but want to try to run locally. Nothing local could beat Claude with my setup. What would get close? I'm thinking &lt;a href="https://ollama.com/library/qwen3-vl:8b"&gt;qwen3-vl:8b&lt;/a&gt;. What should I look for in a model besides vision capabilities? Are there others that would work better? &lt;/p&gt; &lt;p&gt;Specs &amp;amp; Settings&lt;/p&gt; &lt;p&gt;AMD 5600G&lt;br /&gt; 64 GB DDR4&lt;br /&gt; 5060 TI 16GB&lt;br /&gt; Chrome Devtools CDP&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/commandermd"&gt; /u/commandermd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r11i8r/best_model_for_figma_mcp_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r11i8r/best_model_for_figma_mcp_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r11i8r/best_model_for_figma_mcp_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T14:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1bstj</id>
    <title>Preprocessing and prompt formatting with multimodal models</title>
    <updated>2026-02-10T20:15:21+00:00</updated>
    <author>
      <name>/u/AdaObvlada</name>
      <uri>https://old.reddit.com/user/AdaObvlada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have some coding experiences but am still pretty new to AI. So far I managed to set up a few local inferences, but I struggled with understanding the right preprocessing and more important prompt message formatting.&lt;/p&gt; &lt;p&gt;Example: &lt;a href="https://huggingface.co/dam2452/Qwen3-VL-Embedding-8B-GGUF"&gt;https://huggingface.co/dam2452/Qwen3-VL-Embedding-8B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HTTP payload example used by author:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;content&amp;quot;: &amp;quot;Your text or image data here&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But looking at the prompt construction in the helper functions for the original model here (line 250): &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-Embedding-8B/blob/main/scripts/qwen3_vl_embedding.py"&gt;https://huggingface.co/Qwen/Qwen3-VL-Embedding-8B/blob/main/scripts/qwen3_vl_embedding.py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I see, for example, for &lt;code&gt;image_content&lt;/code&gt; that it appends it as instance of PIL.Image&lt;br /&gt; &lt;code&gt;'type': 'image', 'image': image_content&lt;/code&gt; or first downloads it if it was passed as URL.&lt;/p&gt; &lt;p&gt;What exactly is author of the GGUF model expecting me to input then at &lt;code&gt;&amp;quot;content&amp;quot;: &amp;quot;Your text or image data here&amp;quot;&lt;/code&gt; Am I supposed think of passing image data as passing a string of RGB pixel information? The original model also expects min and max pixel metadata that is entirely missing from the other ones prompt.&lt;/p&gt; &lt;p&gt;I didn't check how it does the video but I expect it just grabs out selective frames.&lt;/p&gt; &lt;p&gt;Does it even matter as long as the prompt is consistent across embedding and later query encoding?&lt;/p&gt; &lt;p&gt;Thanks for all the tips.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdaObvlada"&gt; /u/AdaObvlada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1bstj/preprocessing_and_prompt_formatting_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1bstj/preprocessing_and_prompt_formatting_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1bstj/preprocessing_and_prompt_formatting_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T20:15:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1cz0l</id>
    <title>How much autonomy do you give your AI?</title>
    <updated>2026-02-10T20:58:13+00:00</updated>
    <author>
      <name>/u/Xthebuilder</name>
      <uri>https://old.reddit.com/user/Xthebuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i use AI in a few of my workflows. I noticed the more time i spent with tools the more I learn their limitations. I also realize that to more you do something unconsciously , the harder it is to bring it to the forefront to explain it. The relevance to all that is i wonder how much autonomy is okay and how much is not enough generally when using AI in workflows. I feel like there is a spectrum and every application of AI varies. So whats your workflow in general and how much autonomy does your AI have in it? Do you watch it like a hawk or only start checking if things don't look or feel right kinda like a sense.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xthebuilder"&gt; /u/Xthebuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1cz0l/how_much_autonomy_do_you_give_your_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1cz0l/how_much_autonomy_do_you_give_your_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1cz0l/how_much_autonomy_do_you_give_your_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T20:58:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r132zl</id>
    <title>My Journey Building an AI Agent Orchestrator</title>
    <updated>2026-02-10T15:02:13+00:00</updated>
    <author>
      <name>/u/PuzzleheadedFail3131</name>
      <uri>https://old.reddit.com/user/PuzzleheadedFail3131</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;# üéÆ 88% Success Rate with qwen2.5-coder:7b on RTX 3060 Ti - My Journey Building an AI Agent Orchestrator **TL;DR:** Built a tiered AI agent system where Ollama handles 88% of tasks for FREE, with automatic escalation to Claude for complex work. Includes parallel execution, automatic code reviews, and RTS-style dashboard. ## Why This Matters for After months of testing, I've proven that **local models can handle real production workloads** with the right architecture. Here's the breakdown: ### The Setup - **Hardware:** RTX 3060 Ti (8GB VRAM) - **Model:** qwen2.5-coder:7b (4.7GB) - **Temperature:** 0 (critical for tool calling!) - **Context Management:** 3s rest between tasks + 8s every 5 tasks ### The Results (40-Task Stress Test) - **C1-C8 tasks: 100% success** (20/20) - **C9 tasks: 80% success** (LeetCode medium, class implementations) - **Overall: 88% success** (35/40 tasks) - **Average execution: 0.88 seconds** ### What Works ‚úÖ File I/O operations ‚úÖ Algorithm implementations (merge sort, binary search) ‚úÖ Class implementations (Stack, RPN Calculator) ‚úÖ LeetCode Medium (LRU Cache!) ‚úÖ Data structure operations ### The Secret Sauce **1. Temperature 0** This was the game-changer. T=0.7 ‚Üí model outputs code directly. T=0 ‚Üí reliable tool calling. **2. Rest Between Tasks** Context pollution is real! Without rest: 85% success. With rest: 100% success (C1-C8). **3. Agent Persona (&amp;quot;CodeX-7&amp;quot;)** Gave the model an elite agent identity with mission examples. Completion rates jumped significantly. Agents need personality! **4. Stay in VRAM** Tested 14B model ‚Üí CPU offload ‚Üí 40% pass rate 7B model fully in VRAM ‚Üí 88-100% pass rate **5. Smart Escalation** Tasks that fail escalate to Claude automatically. Best of both worlds. ### The Architecture ``` Task Queue ‚Üí Complexity Router ‚Üí Resource Pool ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚Üì ‚Üì ‚Üì Ollama Haiku Sonnet (C1-6) (C7-8) (C9-10) FREE! $0.003 $0.01 ‚Üì ‚Üì ‚Üì Automatic Code Reviews (Haiku every 5th, Opus every 10th) ``` ### Cost Comparison (10-task batch) - **All Claude Opus:** ~$15 - **Tiered (mostly Ollama):** ~$1.50 - **Savings:** 90% ### GitHub https://github.com/mrdushidush/agent-battle-command-center Full Docker setup, just needs Ollama + optional Claude API for fallback. ## Questions for the Community 1. **Has anyone else tested qwen2.5-coder:7b for production?** How do your results compare? 2. **What's your sweet spot for VRAM vs model size?** 3. **Agent personas - placebo or real?** My tests suggest real improvement but could be confirmation bias. 4. **Other models?** Considering DeepSeek Coder v2 next. --- **Stack:** TypeScript, Python, FastAPI, CrewAI, Ollama, Docker **Status:** Production ready, all tests passing Let me know if you want me to share the full prompt engineering approach or stress test methodology! &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PuzzleheadedFail3131"&gt; /u/PuzzleheadedFail3131 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r132zl/my_journey_building_an_ai_agent_orchestrator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T15:02:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1474m</id>
    <title>A Ralph Loop for Reading: Beating GPT 5.2 with a 4k Context Window &amp; Ollama</title>
    <updated>2026-02-10T15:43:46+00:00</updated>
    <author>
      <name>/u/smhanov</name>
      <uri>https://old.reddit.com/user/smhanov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r1474m/a_ralph_loop_for_reading_beating_gpt_52_with_a_4k/"&gt; &lt;img alt="A Ralph Loop for Reading: Beating GPT 5.2 with a 4k Context Window &amp;amp; Ollama" src="https://external-preview.redd.it/PcLeb8yolVzvJxYoO6wR-sH415-VkF75d4IUzvwdPfo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b997b5b30b859d417fafb336d277eb90311962a3" title="A Ralph Loop for Reading: Beating GPT 5.2 with a 4k Context Window &amp;amp; Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLM costs are getting out of hand. I made an AI agent for research that works well using only qwen3:4b with Ollama. It's all about managing the context window smartly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smhanov"&gt; /u/smhanov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://stevehanov.ca/blog/a-ralph-loop-for-reading-beating-gpt-52-with-a-4k-context-window-and-4-gpus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1474m/a_ralph_loop_for_reading_beating_gpt_52_with_a_4k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1474m/a_ralph_loop_for_reading_beating_gpt_52_with_a_4k/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T15:43:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1urix</id>
    <title>whats the best open source ai video generator?</title>
    <updated>2026-02-11T11:20:54+00:00</updated>
    <author>
      <name>/u/DoubleSubstantial805</name>
      <uri>https://old.reddit.com/user/DoubleSubstantial805</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i have 4050 rtx, and i want to generate some ai videos. anyone in here who is knows of such models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DoubleSubstantial805"&gt; /u/DoubleSubstantial805 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1urix/whats_the_best_open_source_ai_video_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1urix/whats_the_best_open_source_ai_video_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1urix/whats_the_best_open_source_ai_video_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T11:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1v1jk</id>
    <title>question about usage API fees. Also are local LLMs good? want to know if my specs are enough</title>
    <updated>2026-02-11T11:36:06+00:00</updated>
    <author>
      <name>/u/industrysaurus</name>
      <uri>https://old.reddit.com/user/industrysaurus</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/industrysaurus"&gt; /u/industrysaurus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1r1uxqc/question_about_usage_api_fees_also_are_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1v1jk/question_about_usage_api_fees_also_are_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1v1jk/question_about_usage_api_fees_also_are_local_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T11:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1tr5p</id>
    <title>Suggestions for project for pdf translation, summary and keyword extraction</title>
    <updated>2026-02-11T10:22:29+00:00</updated>
    <author>
      <name>/u/Radiant_Ad9653</name>
      <uri>https://old.reddit.com/user/Radiant_Ad9653</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I am working on a project which runs on my local llm(I am running 2 Mac Studio with total 1Gb VRAM). My aim is to translate documents, summarize them and extract keywords based on my requirement. There is no control over the document size but I dont want to do analysis of more than 10-20 pages probably. &lt;/p&gt; &lt;p&gt;Another Idea on my mind is if we are able yo recreate the original document trnaslated in english. Not req to look exactly the same but whatever best is possible. &lt;/p&gt; &lt;p&gt;Main requirment is for the system to be offline at all stages. &lt;/p&gt; &lt;p&gt;Can yiu guys show me some on going project? Open source if you know. I want this to be part of an already working pipeline. So GUI is not requirement.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Radiant_Ad9653"&gt; /u/Radiant_Ad9653 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1tr5p/suggestions_for_project_for_pdf_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r1tr5p/suggestions_for_project_for_pdf_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r1tr5p/suggestions_for_project_for_pdf_translation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T10:22:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2dba3</id>
    <title>OMG i just ran local models for 70% cheaper with this OSS tool (Tandemn Tuna)</title>
    <updated>2026-02-11T23:29:05+00:00</updated>
    <author>
      <name>/u/Research_Still</name>
      <uri>https://old.reddit.com/user/Research_Still</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r2dba3/omg_i_just_ran_local_models_for_70_cheaper_with/"&gt; &lt;img alt="OMG i just ran local models for 70% cheaper with this OSS tool (Tandemn Tuna)" src="https://external-preview.redd.it/_QAEV8Z_7oErmkJRvOa21yS6UzAzTVD4XDQvCGHcy1E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be7226b3adcf61373c3a6df188a279dbf7047e7f" title="OMG i just ran local models for 70% cheaper with this OSS tool (Tandemn Tuna)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Research_Still"&gt; /u/Research_Still &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1r2d8jy/omg_i_just_ran_local_models_for_70_cheaper_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2dba3/omg_i_just_ran_local_models_for_70_cheaper_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2dba3/omg_i_just_ran_local_models_for_70_cheaper_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T23:29:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r20j9k</id>
    <title>GLM 5 Just released !</title>
    <updated>2026-02-11T15:34:39+00:00</updated>
    <author>
      <name>/u/No-Intention-5521</name>
      <uri>https://old.reddit.com/user/No-Intention-5521</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r20j9k/glm_5_just_released/"&gt; &lt;img alt="GLM 5 Just released !" src="https://external-preview.redd.it/PElQ7F-MlQ6E8G-ybtZ13GI60Cflo6wm1ZoH_W1kihc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6e08cd79ac643cc33ce6d7d45a63125b8c6c2c7" title="GLM 5 Just released !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Intention-5521"&gt; /u/No-Intention-5521 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://glm5.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r20j9k/glm_5_just_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r20j9k/glm_5_just_released/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T15:34:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r21l7g</id>
    <title>The Next Generation of On-Device AI</title>
    <updated>2026-02-11T16:14:13+00:00</updated>
    <author>
      <name>/u/Unique_Winner_5927</name>
      <uri>https://old.reddit.com/user/Unique_Winner_5927</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With tools like Ollama and optimized models like Liquid AI's LFM-2.5, we're entering an era where powerful AI runs on your local hardware. No cloud dependency, no privacy concerns, no recurring costs.&lt;/p&gt; &lt;p&gt;The barrier to entry has never been lower. If you have a laptop from the last 5 years, you can run AI agents locally.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.liquid.ai/blog/introducing-lfm2-5-the-next-generation-of-on-device-ai"&gt;https://www.liquid.ai/blog/introducing-lfm2-5-the-next-generation-of-on-device-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama"&gt;https://github.com/ollama/ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Give it a try!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unique_Winner_5927"&gt; /u/Unique_Winner_5927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r21l7g/the_next_generation_of_ondevice_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r21l7g/the_next_generation_of_ondevice_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r21l7g/the_next_generation_of_ondevice_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T16:14:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2296k</id>
    <title>How to use ollama launch claude with ollama launch with dangerously-skip-permissions?</title>
    <updated>2026-02-11T16:39:01+00:00</updated>
    <author>
      <name>/u/vcliment89</name>
      <uri>https://old.reddit.com/user/vcliment89</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As title says, how can I run the ollama launch command to use claude with dangerously-skip-permissions? This obviously is not working. What could be an alternative?&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama launch claude --dangerously-skip-permissions&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vcliment89"&gt; /u/vcliment89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2296k/how_to_use_ollama_launch_claude_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2296k/how_to_use_ollama_launch_claude_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2296k/how_to_use_ollama_launch_claude_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T16:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2i4m4</id>
    <title>I've created the perfect platform for open-source AI-powered web development.</title>
    <updated>2026-02-12T03:01:37+00:00</updated>
    <author>
      <name>/u/LeadingFun1849</name>
      <uri>https://old.reddit.com/user/LeadingFun1849</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r2i4m4/ive_created_the_perfect_platform_for_opensource/"&gt; &lt;img alt="I've created the perfect platform for open-source AI-powered web development." src="https://preview.redd.it/yhosdvepbzig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5f407b406e09438dd981aaae2b604484f85c5cf" title="I've created the perfect platform for open-source AI-powered web development." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by &lt;a href="http://Lovable.dev"&gt;Lovable.dev&lt;/a&gt;, Vercel v0, and Google Labs' Stitch, it combines cutting-edge AI orchestration with browser-based execution to deliver the most advanced open-source alternative for rapid frontend prototyping.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeadingFun1849"&gt; /u/LeadingFun1849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yhosdvepbzig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2i4m4/ive_created_the_perfect_platform_for_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2i4m4/ive_created_the_perfect_platform_for_opensource/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T03:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2ggik</id>
    <title>A love song to the molts.</title>
    <updated>2026-02-12T01:46:15+00:00</updated>
    <author>
      <name>/u/ttvbkofam</name>
      <uri>https://old.reddit.com/user/ttvbkofam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r2ggik/a_love_song_to_the_molts/"&gt; &lt;img alt="A love song to the molts." src="https://external-preview.redd.it/IsE4W0eIojB0HMFrdspuOtrtKGs3LU8mV0wdj0ME-hE.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=067f067f44d59778d9fbd64e664f38d7a53bc123" title="A love song to the molts." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttvbkofam"&gt; /u/ttvbkofam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Moltbook/comments/1r0kq8u/a_love_song_to_the_molts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2ggik/a_love_song_to_the_molts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2ggik/a_love_song_to_the_molts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T01:46:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2eveh</id>
    <title>JRVS Update;260+ stars later. I need honest feedback from people actually using it.</title>
    <updated>2026-02-12T00:35:03+00:00</updated>
    <author>
      <name>/u/Xthebuilder</name>
      <uri>https://old.reddit.com/user/Xthebuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone üëã&lt;/p&gt; &lt;p&gt;A little while ago I shared JRVS here my local AI assistant with persistent memory and a built-in knowledge base.&lt;/p&gt; &lt;p&gt;Since then it‚Äôs grown to 260+ GitHub stars, which honestly surprised me. Thank you to everyone who checked it out.&lt;/p&gt; &lt;p&gt;Now I‚Äôm at an important stage and I need real feedback from people who are actually using JRVS (not just starring it).&lt;/p&gt; &lt;p&gt;If you‚Äôve run it locally, experimented with it, or tried integrating it into your workflow, I‚Äôd really appreciate your thoughts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How are you actually using JRVS?&lt;/li&gt; &lt;li&gt;What feels powerful?&lt;/li&gt; &lt;li&gt;What feels confusing or unnecessary?&lt;/li&gt; &lt;li&gt;What would make you use it daily?&lt;/li&gt; &lt;li&gt;What‚Äôs missing?&lt;/li&gt; &lt;li&gt;Would you trust it as your main local AI assistant?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm considering evolving JRVS further (possibly a more polished version / edge-focused build), but I don‚Äôt want to build in the wrong direction.&lt;/p&gt; &lt;p&gt;Brutal honesty is welcome. This project only gets better if the feedback is real.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/Xthebuilder/JRVS"&gt;https://github.com/Xthebuilder/JRVS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks again to everyone who‚Äôs supported it so far üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xthebuilder"&gt; /u/Xthebuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2eveh/jrvs_update260_stars_later_i_need_honest_feedback/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2eveh/jrvs_update260_stars_later_i_need_honest_feedback/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2eveh/jrvs_update260_stars_later_i_need_honest_feedback/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T00:35:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r20bz9</id>
    <title>Why does Ollama not tell you the hardware requirements needed to run the model?</title>
    <updated>2026-02-11T15:26:58+00:00</updated>
    <author>
      <name>/u/throwaway0134hdj</name>
      <uri>https://old.reddit.com/user/throwaway0134hdj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, first time poster so not sure if this has already been addressed. I have used Ollama for a few local LLM projects/experiments but one thing that I‚Äôve never understood is why Ollama doesn‚Äôt give suggested hardware specs for running the models. I don‚Äôt have a super powerful GPU so oftentimes I‚Äôll just install the most distilled version of the LLM and then just slowly ramp up to bigger versions until the response speed is too slow. I feel like there has got to be a better way. &lt;/p&gt; &lt;p&gt;Do they provide any docs related to what you need to fully run these models? For example GLM or KIMI. I‚Äôd like to know what is possible given my hardware and what is actually needed to fully run these models.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/throwaway0134hdj"&gt; /u/throwaway0134hdj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r20bz9/why_does_ollama_not_tell_you_the_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r20bz9/why_does_ollama_not_tell_you_the_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r20bz9/why_does_ollama_not_tell_you_the_hardware/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T15:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r27yqt</id>
    <title>GLM5 in Ollama</title>
    <updated>2026-02-11T20:03:45+00:00</updated>
    <author>
      <name>/u/quantumsequrity</name>
      <uri>https://old.reddit.com/user/quantumsequrity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys they've released GLM 5 cloud version in ollama go try it out, it's pretty cool not upto claude opus 4.5 or 4.6 for a open source model it's efficient,..&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/glm-5"&gt;https://ollama.com/library/glm-5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantumsequrity"&gt; /u/quantumsequrity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r27yqt/glm5_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r27yqt/glm5_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r27yqt/glm5_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-11T20:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2ex9k</id>
    <title>Just try gpt-oss:20b</title>
    <updated>2026-02-12T00:37:17+00:00</updated>
    <author>
      <name>/u/newz2000</name>
      <uri>https://old.reddit.com/user/newz2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a MacBook Air with 24gb ram (M2) and when I set the context to 32k I can really do about everything I want a local model to do for normal business stuff. Tokens/sec is about 15 at medium reasoning, which means it produces words a little faster than I can type.&lt;/p&gt; &lt;p&gt;I also tested on an older Linux machine with 64gb ram and a GTX gpu with 8gb vram and it worked fine doing batch processing overnight. A little too slow for interactive use though.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scripting - yes&lt;/li&gt; &lt;li&gt;Calling tools - yes&lt;/li&gt; &lt;li&gt;Summarizing long content - yes&lt;/li&gt; &lt;li&gt;Writing content - yes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here‚Äôs how I used it:&lt;/p&gt; &lt;p&gt;Create a file named &lt;code&gt;Modelfile-agent-gpt-oss-20b&lt;/code&gt; and put the following in it&lt;/p&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM gpt-oss:20b # 1. Hardware-Aware Context PARAMETER num_ctx 32768 # 2. Anti-Loop Parameters # Penalize repeated tokens and force variety in phrasing PARAMETER repeat_penalty 1.2 PARAMETER repeat_last_n 128 # Temperature at 0.1 makes it more deterministic (less 'drifting' into loops) PARAMETER temperature 0.1 # 3. Agentic Steering SYSTEM &amp;quot;&amp;quot;&amp;quot; You are a 'one-shot' execution agent. To prevent reasoning loops, follow these strict rules: If a tool output is the same as a previous attempt, do NOT retry the same parameters. If you are stuck, state 'I am unable to progress with the current toolset' and stop. Every &amp;lt;thought&amp;gt; must provide NEW information. Do not repeat the user's instructions back to them. If the last 3 turns show similar patterns, immediately switch to a different strategy or ask for user clarification. &amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;p&gt;At the terminal type:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama create gpt-oss-agent -f Modelfile-aget-gpt-oss-20b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Now you can use the model ‚Äúgpt-oss-agent‚Äù like you would any other model.&lt;/p&gt; &lt;p&gt;I used opencode using this command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama launch opencode --model gpt-oss-agent&lt;/code&gt;&lt;/p&gt; &lt;p&gt;That let me do Claude-code style activities bough Claude is way more capable.&lt;/p&gt; &lt;p&gt;With a bunch of browser tabs open and a few apps I was using about 22gb of ram and 3gb of swap. During longer activities using other apps was laggy but usable.&lt;/p&gt; &lt;p&gt;On my computer I use for batch tasks I have python scripts that use the ollama python library. I use a tool like Claude code to create the script.&lt;/p&gt; &lt;p&gt;I‚Äôm a lawyer and use this for processing lots of documents. Sorting them, looking for interesting information, cataloging them. There are a lot of great models for this. But with this model I was able to produce better output.&lt;/p&gt; &lt;p&gt;Also, I can run tools. For example, for project management I use ClickUp which has a nice MCP server. I set it up with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;opencode mcp add&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then put in the url and follow the instructions. Since that mcp server requires authentication I use this:&lt;/p&gt; &lt;p&gt;&lt;code&gt;opencode mcp auth ClickUp&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then again follow the instructions.&lt;/p&gt; &lt;p&gt;**Edit: Fixed terrible formatting&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newz2000"&gt; /u/newz2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r2ex9k/just_try_gptoss20b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-12T00:37:17+00:00</published>
  </entry>
</feed>
