<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-04T07:47:06+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qu9g4o</id>
    <title>The AI Lobsters Are Taking Over (And They Started their own Church!!)</title>
    <updated>2026-02-02T22:11:11+00:00</updated>
    <author>
      <name>/u/SeriousDocument7905</name>
      <uri>https://old.reddit.com/user/SeriousDocument7905</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qu9g4o/the_ai_lobsters_are_taking_over_and_they_started/"&gt; &lt;img alt="The AI Lobsters Are Taking Over (And They Started their own Church!!)" src="https://external-preview.redd.it/0NktJ7vX6l2LLOjpmFKt-rhOF6ieRSJWenjktMJ9JZQ.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f87c8536603a594b9c7b0ddd3e869aef0a9891f" title="The AI Lobsters Are Taking Over (And They Started their own Church!!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SeriousDocument7905"&gt; /u/SeriousDocument7905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/MMV3D_yqRZ8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qu9g4o/the_ai_lobsters_are_taking_over_and_they_started/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qu9g4o/the_ai_lobsters_are_taking_over_and_they_started/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T22:11:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtxk63</id>
    <title>See what your AI agents see while browsing the web</title>
    <updated>2026-02-02T15:08:58+00:00</updated>
    <author>
      <name>/u/BlitzBrowser_</name>
      <uri>https://old.reddit.com/user/BlitzBrowser_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qtxk63/see_what_your_ai_agents_see_while_browsing_the_web/"&gt; &lt;img alt="See what your AI agents see while browsing the web" src="https://external-preview.redd.it/irW4oDS7drKOXE6ZPTLy9XVVo2a8cDoITkvasjyhu4o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd5adcc1b821911133871098276d826a0fcea755" title="See what your AI agents see while browsing the web" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlitzBrowser_"&gt; /u/BlitzBrowser_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rf1c38pzj3hg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtxk63/see_what_your_ai_agents_see_while_browsing_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtxk63/see_what_your_ai_agents_see_while_browsing_the_web/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T15:08:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtzi14</id>
    <title>why does ollama pull a pre pulled model ? and how to prevent it ?</title>
    <updated>2026-02-02T16:19:32+00:00</updated>
    <author>
      <name>/u/Hot_Arachnid3547</name>
      <uri>https://old.reddit.com/user/Hot_Arachnid3547</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ollama run qwen2.5-coder:14b&lt;br /&gt; pulling manifest &lt;br /&gt; pulling ac9bc7a69dab: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 9.0 GB &lt;br /&gt; pulling 66b9ea09bd5b: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 68 B &lt;br /&gt; pulling 1e65450c3067: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.6 KB &lt;br /&gt; pulling 832dd9e00a68: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 11 KB &lt;br /&gt; pulling 0578f229f23a: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 488 B &lt;br /&gt; verifying sha256 digest &lt;br /&gt; writing manifest &lt;br /&gt; success &lt;/p&gt; &lt;p&gt;ollama list&lt;br /&gt; NAME ID SIZE MODIFIED &lt;br /&gt; qwen2.5-coder:14b 9ec8897f747e 9.0 GB 25 minutes ago &lt;br /&gt; llama2:latest 78e26419b446 3.8 GB 7 months ago &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hot_Arachnid3547"&gt; /u/Hot_Arachnid3547 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtzi14/why_does_ollama_pull_a_pre_pulled_model_and_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtzi14/why_does_ollama_pull_a_pre_pulled_model_and_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtzi14/why_does_ollama_pull_a_pre_pulled_model_and_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T16:19:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtxzjp</id>
    <title>Ram issue</title>
    <updated>2026-02-02T15:24:46+00:00</updated>
    <author>
      <name>/u/Clear_Move_7686</name>
      <uri>https://old.reddit.com/user/Clear_Move_7686</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qtxzjp/ram_issue/"&gt; &lt;img alt="Ram issue" src="https://preview.redd.it/0x3m4cb1n3hg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0d104c6c175616106098ac45b5d9db17dca8db6" title="Ram issue" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, i was wondering why did it suddenly say i don't have enough ram to use qwen3:4b when i do have enough ram, and i did literally use it multiple times in the past (i deleted the chats). So why is it suddenly telling me i don't have enough??? For reference i have 32gb of ddr4. Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Clear_Move_7686"&gt; /u/Clear_Move_7686 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0x3m4cb1n3hg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtxzjp/ram_issue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtxzjp/ram_issue/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T15:24:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu89tk</id>
    <title>6700 XT</title>
    <updated>2026-02-02T21:27:31+00:00</updated>
    <author>
      <name>/u/Headown998</name>
      <uri>https://old.reddit.com/user/Headown998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;Been trying to get ollama (0.13.5) to use my 6700xt on windows but can't get it working.&lt;/p&gt; &lt;p&gt;I already replaced the rocm files, but it's still using the CPU.&lt;/p&gt; &lt;p&gt;I've seen that I need to set environment variables, but those didn't work either and I got this:&lt;/p&gt; &lt;p&gt;Error: 500 Internal Server Error: do load request: Post &lt;a href="http://127.0.0.1:49994/load"&gt;http://127.0.0.1:49994/load&lt;/a&gt; : read tcp 127.0.0.1:49998-&amp;gt;127.0.0.1:49994: wsarecv: An existing connection was forcibly closed by the remote host.&lt;/p&gt; &lt;p&gt;I don't know if I set the variables right.&lt;/p&gt; &lt;p&gt;Is there a video somewhere that shows where and how to set those variables on windows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Headown998"&gt; /u/Headown998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qu89tk/6700_xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qu89tk/6700_xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qu89tk/6700_xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T21:27:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu8u92</id>
    <title>I've built a local twitter-like for bots - so you can have `moltbook` at home ;)</title>
    <updated>2026-02-02T21:48:29+00:00</updated>
    <author>
      <name>/u/maciek_glowka</name>
      <uri>https://old.reddit.com/user/maciek_glowka</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maciek_glowka"&gt; /u/maciek_glowka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1qu8tzr/ive_built_a_local_twitterlike_for_bots_so_you_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qu8u92/ive_built_a_local_twitterlike_for_bots_so_you_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qu8u92/ive_built_a_local_twitterlike_for_bots_so_you_can/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T21:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu9eoj</id>
    <title>Ollama desktop is stuck at loading...</title>
    <updated>2026-02-02T22:09:44+00:00</updated>
    <author>
      <name>/u/Other-Two-8440</name>
      <uri>https://old.reddit.com/user/Other-Two-8440</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Processing img zefl1fmxp5hg1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;As title says the app is stuck on start at loading???&lt;br /&gt; I also tried this command in cmd: $env:OLLAMA_HOST=&amp;quot;0.0.0.0:11435&amp;quot; but it didnt change anythig.... a fix?? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other-Two-8440"&gt; /u/Other-Two-8440 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qu9eoj/ollama_desktop_is_stuck_at_loading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qu9eoj/ollama_desktop_is_stuck_at_loading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qu9eoj/ollama_desktop_is_stuck_at_loading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T22:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu9f31</id>
    <title>Ollama desktop is stuck at loading...</title>
    <updated>2026-02-02T22:10:09+00:00</updated>
    <author>
      <name>/u/Other-Two-8440</name>
      <uri>https://old.reddit.com/user/Other-Two-8440</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other-Two-8440"&gt; /u/Other-Two-8440 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ollama/comments/1qu9eoj/ollama_desktop_is_stuck_at_loading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qu9f31/ollama_desktop_is_stuck_at_loading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qu9f31/ollama_desktop_is_stuck_at_loading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T22:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtzq2x</id>
    <title>Environmental Impact</title>
    <updated>2026-02-02T16:27:29+00:00</updated>
    <author>
      <name>/u/King_Penguin0s</name>
      <uri>https://old.reddit.com/user/King_Penguin0s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I've been really trying to cut down on my use of AI lately due to the environmental impacts as that's something I'm very passionate about. However there are some things In my workflow that I just can't live without anymore.&lt;/p&gt; &lt;p&gt;From this, I came across Ollama and the idea of running models locally and I'm wondering if doing this has the same, a better or worse environmental impact?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/King_Penguin0s"&gt; /u/King_Penguin0s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtzq2x/environmental_impact/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtzq2x/environmental_impact/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtzq2x/environmental_impact/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T16:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtine0</id>
    <title>Released: VOR ‚Äî a hallucination-free runtime that forces LLMs to prove answers or abstain</title>
    <updated>2026-02-02T02:31:03+00:00</updated>
    <author>
      <name>/u/CulpritChaos</name>
      <uri>https://old.reddit.com/user/CulpritChaos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just open-sourced a project that might interest people here who are tired of hallucinations being treated as ‚Äújust a prompt issue.‚Äù VOR (Verified Observation Runtime) is a runtime layer that sits around LLMs and retrieval systems and enforces one rule: If an answer cannot be proven from observed evidence, the system must abstain. Highlights: 0.00% hallucination across demo + adversarial packs Explicit CONFLICT detection (not majority voting) Deterministic audits (hash-locked, replayable) Works with local models ‚Äî the verifier doesn‚Äôt care which LLM you use Clean-room witness instructions included This is not another RAG framework. It‚Äôs a governor for reasoning: models can propose, but they don‚Äôt decide. Public demo includes: CLI (neuralogix qa, audit, pack validate) Two packs: a normal demo corpus + a hostile adversarial pack Full test suite (legacy tests quarantined) Repo: &lt;a href="https://github.com/CULPRITCHAOS/VOR"&gt;https://github.com/CULPRITCHAOS/VOR&lt;/a&gt; Tag: v0.7.3-public.1 Witness guide: docs/WITNESS_RUN_MESSAGE.txt I‚Äôm looking for: People to run it locally (Windows/Linux/macOS) Ideas for harder adversarial packs Discussion on where a runtime like this fits in local stacks (Ollama, LM Studio, etc.) Happy to answer questions or take hits. This was built to be challenged.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CulpritChaos"&gt; /u/CulpritChaos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtine0/released_vor_a_hallucinationfree_runtime_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtine0/released_vor_a_hallucinationfree_runtime_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtine0/released_vor_a_hallucinationfree_runtime_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T02:31:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtyp8e</id>
    <title>üî• New to DGX ‚Äî Looking for Advice on Best AI Models &amp; Deployments!</title>
    <updated>2026-02-02T15:51:19+00:00</updated>
    <author>
      <name>/u/Character-Town-8188</name>
      <uri>https://old.reddit.com/user/Character-Town-8188</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt; &lt;p&gt;I recently acquired a NVIDIA DGX (Spark DGX) system, and I‚Äôm super excited to start putting it to good use. However, I‚Äôd really appreciate some community insight on what real-world AI workloads/models I should run to make the most out of this beast.&lt;/p&gt; &lt;p&gt;üß† What I‚Äôm Looking For&lt;/p&gt; &lt;p&gt;I want to:&lt;/p&gt; &lt;p&gt;‚Ä¢ Deploy AI models that make sense for this hardware&lt;/p&gt; &lt;p&gt;‚Ä¢ Use cases that are practical, impactful, and leverage the GPU power&lt;/p&gt; &lt;p&gt;‚Ä¢ Learn from others who have experience optimizing &amp;amp; deploying large models&lt;/p&gt; &lt;p&gt;üìå Questions I Have&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What are the best models to run on a DGX today?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;‚Ä¢ LLMs (which sizes?)&lt;/p&gt; &lt;p&gt;‚Ä¢ Vision models?&lt;/p&gt; &lt;p&gt;‚Ä¢ Multimodal?&lt;/p&gt; &lt;p&gt;‚Ä¢ Reinforcement learning?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Are there open-source alternatives worth deploying? (e.g., LLaMA, Stable Diffusion, Falcon, etc.)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What deployment frameworks do folks recommend?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;‚Ä¢ Triton?&lt;/p&gt; &lt;p&gt;‚Ä¢ Ray?&lt;/p&gt; &lt;p&gt;‚Ä¢ Kubernetes?&lt;/p&gt; &lt;p&gt;‚Ä¢ Hugging Face Accelerate?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Do you have recommendations for benchmarking, optimizing performance, and scaling?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What real-world use cases have you found valuable ‚Äî inference, fine-tuning, research workloads, generative AI, embeddings, etc.?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;üõ†Ô∏è Some Context (Optional Details about My Setup)&lt;/p&gt; &lt;p&gt;‚Ä¢ NVIDIA Spark DGX&lt;/p&gt; &lt;p&gt;‚Ä¢ 128Gb: RAM&lt;/p&gt; &lt;p&gt;üôè Thank You!&lt;/p&gt; &lt;p&gt;I‚Äôm eager to hear what you think ‚Äî whether it‚Äôs cool model recommendations, deployment tips, or links to open-source projects that run well on DGX hardware.&lt;/p&gt; &lt;p&gt;Thanks so much in advance! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Character-Town-8188"&gt; /u/Character-Town-8188 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtyp8e/new_to_dgx_looking_for_advice_on_best_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtyp8e/new_to_dgx_looking_for_advice_on_best_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtyp8e/new_to_dgx_looking_for_advice_on_best_ai_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T15:51:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtqp9p</id>
    <title>175k+ publicly exposed Ollama servers, so I built a tool</title>
    <updated>2026-02-02T09:41:37+00:00</updated>
    <author>
      <name>/u/truthfly</name>
      <uri>https://old.reddit.com/user/truthfly</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/truthfly"&gt; /u/truthfly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qtqobb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qtqp9p/175k_publicly_exposed_ollama_servers_so_i_built_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qtqp9p/175k_publicly_exposed_ollama_servers_so_i_built_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T09:41:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1quhccl</id>
    <title>I used local LLMs running on Ollama to turn BMO from Adventure Time into a simple AI agent</title>
    <updated>2026-02-03T03:43:38+00:00</updated>
    <author>
      <name>/u/brenpoly</name>
      <uri>https://old.reddit.com/user/brenpoly</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brenpoly"&gt; /u/brenpoly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1qugbxp/i_used_local_llms_running_on_ollama_to_turn_bmo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1quhccl/i_used_local_llms_running_on_ollama_to_turn_bmo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1quhccl/i_used_local_llms_running_on_ollama_to_turn_bmo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-03T03:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qujiwf</id>
    <title>Weird voice similar to the 90s and is it old data?</title>
    <updated>2026-02-03T05:32:41+00:00</updated>
    <author>
      <name>/u/Oxffff0000</name>
      <uri>https://old.reddit.com/user/Oxffff0000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have ollama and openweb. First time testing voice. It's able to capture my voice really well but when it replies to me, the voice I picked all sounded like a robot. It's very far from a human voice compared to what I've seen in others' post.&lt;/p&gt; &lt;p&gt;Also, I asked all my local LLMs about that date. The only one that said the date correctly was gpt-oss:20b. All other LLMs responded with year 2023. I'm guessing that it's the reason why it can't show me a well written code. Is my conclusion correct?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Oxffff0000"&gt; /u/Oxffff0000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qujiwf/weird_voice_similar_to_the_90s_and_is_it_old_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qujiwf/weird_voice_similar_to_the_90s_and_is_it_old_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qujiwf/weird_voice_similar_to_the_90s_and_is_it_old_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-03T05:32:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv3dr0</id>
    <title>Can I run a unrestricted LLM here?</title>
    <updated>2026-02-03T20:33:42+00:00</updated>
    <author>
      <name>/u/jesterofjustice99</name>
      <uri>https://old.reddit.com/user/jesterofjustice99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;8 vCore CPU 16 GB RAM 480 GB NVMe SSD Linux Ubuntu 24.04&lt;/p&gt; &lt;p&gt;Link: &lt;a href="http://share.google/HC84CMyf3sw3mK8RP"&gt;share.google/HC84CMyf3sw3mK8RP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jesterofjustice99"&gt; /u/jesterofjustice99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qv3dr0/can_i_run_a_unrestricted_llm_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qv3dr0/can_i_run_a_unrestricted_llm_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qv3dr0/can_i_run_a_unrestricted_llm_here/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-03T20:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu4ka4</id>
    <title>Recommendations for a good value machine to run LLMs locally?</title>
    <updated>2026-02-02T19:14:51+00:00</updated>
    <author>
      <name>/u/onesemesterchinese</name>
      <uri>https://old.reddit.com/user/onesemesterchinese</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thinking of purchasing a machine in the few thousand $ range to work on some personal projects. Would like to hear if anyone has any thoughts or positive/negative experiences running inference with some of the bigger open models locally or with finetuning? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onesemesterchinese"&gt; /u/onesemesterchinese &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qu4ka4/recommendations_for_a_good_value_machine_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qu4ka4/recommendations_for_a_good_value_machine_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qu4ka4/recommendations_for_a_good_value_machine_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-02T19:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1quz0us</id>
    <title>Is anyone doing anything interesting locally?</title>
    <updated>2026-02-03T17:56:28+00:00</updated>
    <author>
      <name>/u/irlcake</name>
      <uri>https://old.reddit.com/user/irlcake</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irlcake"&gt; /u/irlcake &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1quz0br/is_anyone_doing_anything_interesting_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1quz0us/is_anyone_doing_anything_interesting_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1quz0us/is_anyone_doing_anything_interesting_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-03T17:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvc3bo</id>
    <title>Cross-architecture evidence that LLM behavioral patterns live in low-dimensional geometric subspaces</title>
    <updated>2026-02-04T02:25:44+00:00</updated>
    <author>
      <name>/u/BiscottiDisastrous19</name>
      <uri>https://old.reddit.com/user/BiscottiDisastrous19</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BiscottiDisastrous19"&gt; /u/BiscottiDisastrous19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qv0og6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvc3bo/crossarchitecture_evidence_that_llm_behavioral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qvc3bo/crossarchitecture_evidence_that_llm_behavioral/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-04T02:25:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv7u95</id>
    <title>model requires more system memory</title>
    <updated>2026-02-03T23:23:26+00:00</updated>
    <author>
      <name>/u/GroceryBagHead</name>
      <uri>https://old.reddit.com/user/GroceryBagHead</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If there a fix/workaround for this? It seems that ollama looks at the &lt;em&gt;free&lt;/em&gt; and not &lt;em&gt;available&lt;/em&gt; memory. I have 58 gigs allocated for my LXC and it's moaning that it's not enough.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;root@ollama:~# ollama run codellama:34b --verbose Error: 500 Internal Server Error: model requires more system memory (18.4 GiB) than is available (16.5 GiB) root@ollama:~# free -h total used free shared buff/cache available Mem: 56Gi 65Mi 16Gi 108Ki 40Gi 56Gi Swap: 512Mi 0B 512Mi &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see, only 65 megs are being used. 56 gigs are &amp;quot;available&amp;quot;. Googling yielded some discussion, but I didn't find a solution, sadly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GroceryBagHead"&gt; /u/GroceryBagHead &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qv7u95/model_requires_more_system_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qv7u95/model_requires_more_system_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qv7u95/model_requires_more_system_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-03T23:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv5cgs</id>
    <title>AI Context as Code: Can structured docs improve AI resource usage and performance?</title>
    <updated>2026-02-03T21:46:51+00:00</updated>
    <author>
      <name>/u/eFAILution</name>
      <uri>https://old.reddit.com/user/eFAILution</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qv5cgs/ai_context_as_code_can_structured_docs_improve_ai/"&gt; &lt;img alt="AI Context as Code: Can structured docs improve AI resource usage and performance?" src="https://external-preview.redd.it/vQYhhfwdiqmBqzXOmy2FqYp_TXG01J7EKHg73RUOAZA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a297e0eeed90db44b5fffcf6bbb193ebd906ac9f" title="AI Context as Code: Can structured docs improve AI resource usage and performance?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The idea: Instead of AI parsing your entire README to find ‚Äúhow to add a feature‚Äù, it queries workflows.yaml directly.&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô Prose: \\\~800 tokens ‚àô Structured: \\\~240 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The catch: I could be totally wrong.&lt;/p&gt; &lt;p&gt;Includes validation framework, complete spec.&lt;/p&gt; &lt;p&gt;Targeting NeurIPS 2026 but could use community assist on validation via experimentation.&lt;/p&gt; &lt;p&gt;Easy to try: Github Action (&lt;a href="https://github.com/eFAILution/AICaC/tree/main/.github/actions/aicac-adoption"&gt;https://github.com/eFAILution/AICaC/tree/main/.github/actions/aicac-adoption&lt;/a&gt;) opens a PR with starter .ai/ files. Includes instructions for using AI assistants to build out the rest based on the spec. Just enable Actions PR permissions in your repo settings.&lt;/p&gt; &lt;p&gt;Looking forward to any feedback you may have!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eFAILution"&gt; /u/eFAILution &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/eFAILution/AICaC"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qv5cgs/ai_context_as_code_can_structured_docs_improve_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qv5cgs/ai_context_as_code_can_structured_docs_improve_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-03T21:46:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv4312</id>
    <title>Ollama and Openclaw on separate dedicated, isolated, firewalled machines</title>
    <updated>2026-02-03T21:00:04+00:00</updated>
    <author>
      <name>/u/timbo2m</name>
      <uri>https://old.reddit.com/user/timbo2m</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone get this to work? &lt;/p&gt; &lt;p&gt;Machine 1 is a dedicated Openclaw Mac laptop with outbound internet access.&lt;/p&gt; &lt;p&gt;Machine 2 is a dedicated ollama server sharing various models.&lt;/p&gt; &lt;p&gt;Both machines are on the same subnet. A quick check shows Openclaw machine can see the models list on the ollama server. &lt;/p&gt; &lt;p&gt;Once Openclaw has been through onboarding it does not respond to any chat requests. I think maybe this could work with some extra messing around v&lt;/p&gt; &lt;p&gt;So while it should work, the test responses are all empty as if it's Openclaw can't communicate properly with ollama. &lt;/p&gt; &lt;p&gt;EDIT: I found this &lt;a href="https://github.com/openclaw/openclaw/issues/2838"&gt;https://github.com/openclaw/openclaw/issues/2838&lt;/a&gt; so will see if some of those comments help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/timbo2m"&gt; /u/timbo2m &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qv4312/ollama_and_openclaw_on_separate_dedicated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qv4312/ollama_and_openclaw_on_separate_dedicated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qv4312/ollama_and_openclaw_on_separate_dedicated/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-03T21:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvgtxi</id>
    <title>Sorry if this is a stupid question.</title>
    <updated>2026-02-04T06:13:17+00:00</updated>
    <author>
      <name>/u/ii80i</name>
      <uri>https://old.reddit.com/user/ii80i</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm having an issue with OpenClaw and I can‚Äôt figure out what I‚Äôm doing wrong.&lt;/p&gt; &lt;p&gt;I‚Äôm trying to configure OpenClaw so it connects properly, but the configuration step always fails. When I run this command:&lt;/p&gt; &lt;p&gt;ollama launch openclaw --config&lt;/p&gt; &lt;p&gt;I get the following error:&lt;/p&gt; &lt;p&gt;Error: unknown integration: openclaw&lt;/p&gt; &lt;p&gt;I‚Äôm not sure where the problem is or what I‚Äôm missing. I checked the docs but couldn‚Äôt find anything that explains this error clearly.&lt;/p&gt; &lt;p&gt;Any help or clarification would be appreciated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(MacOS)&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ii80i"&gt; /u/ii80i &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvgtxi/sorry_if_this_is_a_stupid_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvgtxi/sorry_if_this_is_a_stupid_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qvgtxi/sorry_if_this_is_a_stupid_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-04T06:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv346v</id>
    <title>Recommandation for a power and cost efficient local llm system</title>
    <updated>2026-02-03T20:23:53+00:00</updated>
    <author>
      <name>/u/Grummel78</name>
      <uri>https://old.reddit.com/user/Grummel78</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everybody,&lt;/p&gt; &lt;p&gt;i am looking power and cost efficient local llm system. especially when it is in idle. But i don't want wait minutes for reaction :-) ok ok i know that can not have everything :-)&lt;/p&gt; &lt;p&gt;Use cases are the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Using AI for Paperless-NGX (setting tags and ocr)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Voice Assistant and automation in Home Assistant.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Eventual Clawdbot&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;At the moment i tried Ai with the following setup:&lt;/p&gt; &lt;p&gt;asrock n100m + RTX 3060 +32 GB Ram.&lt;/p&gt; &lt;p&gt;But it use about 35 Watts in idle. I live in Germany with high energy cost. And for an 24/7 system it is too much for me. especially it will not be used every day. Paperless eventually every third day. Voice Assistant and automation in Home Assistant 10-15 times per day.&lt;/p&gt; &lt;p&gt;Clawdbot i don't know.&lt;/p&gt; &lt;p&gt;Important for me is data stays at home (especially Paperless data).&lt;/p&gt; &lt;p&gt;Know i am thinking about a mac mini m4 base edition (16 gig unified ram and 256 ssd)&lt;/p&gt; &lt;p&gt;Have somebody recommandations or experience with a mac mini and my use cases ?&lt;/p&gt; &lt;p&gt;Best regards&lt;/p&gt; &lt;p&gt;Dirk&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grummel78"&gt; /u/Grummel78 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qv346v/recommandation_for_a_power_and_cost_efficient/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qv346v/recommandation_for_a_power_and_cost_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qv346v/recommandation_for_a_power_and_cost_efficient/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-03T20:23:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvfb0i</id>
    <title>Is this considered agentic ai?</title>
    <updated>2026-02-04T04:53:41+00:00</updated>
    <author>
      <name>/u/Oxffff0000</name>
      <uri>https://old.reddit.com/user/Oxffff0000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm still new to AI. So I wrote script which uses whisper to transcribe raw voice to text. Then I have a system prompt where I laid down rules of what it must do when it sees text etc. I have a function that creates an intent based from the transcribed text. Then lastly, I have another function what will send a json payload to a REST api endpoint. Is this considered agentic ai?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Oxffff0000"&gt; /u/Oxffff0000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvfb0i/is_this_considered_agentic_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qvfb0i/is_this_considered_agentic_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qvfb0i/is_this_considered_agentic_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-04T04:53:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv1yey</id>
    <title>Your thoughts on "thinking" LLMs?</title>
    <updated>2026-02-03T19:41:24+00:00</updated>
    <author>
      <name>/u/stonecannon</name>
      <uri>https://old.reddit.com/user/stonecannon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;almost all of the ollama-ready models released in recent months have been &amp;quot;thinking&amp;quot; or &amp;quot;chain of thought&amp;quot; or &amp;quot;reasoning&amp;quot; models -- you know, the ones that force you to watch the model's simulated thought process before it generates a final answer. &lt;/p&gt; &lt;p&gt;personally, i find this trend extremely annoying for a couple reasons:&lt;/p&gt; &lt;p&gt;1). it's fake. that's not how LLMs work. it's a performance to make it look like the LLM has more consciousness than it does. &lt;/p&gt; &lt;p&gt;2). it's annoying. i really don't want to sit through 18 seconds (actual example) of faux-thinking to get a reply to a prompt that just says &amp;quot;good morning!&amp;quot;.&lt;/p&gt; &lt;p&gt;The worst example i've seen so far was with Olmo-3.1, which generated 1932 words of &amp;quot;thinking&amp;quot; to reply to &amp;quot;good morning&amp;quot; (i saved them if you're curious).&lt;/p&gt; &lt;p&gt;in the Ollama CLI, some thinking models respond to the &amp;quot;/set nothink&amp;quot; command to turn off thinking mode, but not all do. and there is no corresponding way to turn off thinking in the GUI. same goes for the AnythingLLM, LM Studio, and GPT4All GUIs.&lt;/p&gt; &lt;p&gt;so what do _you_ think? do you enjoy seeing the simulated thought process in spite of the delays it causes? if so, i'd love to know what it is that appeals to you... maybe you can help me understand this trend.&lt;/p&gt; &lt;p&gt;i realize some people say this can actually improve results by forcing checkpoints into the inference process (or something like that), but to me it's still not worth it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stonecannon"&gt; /u/stonecannon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qv1yey/your_thoughts_on_thinking_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qv1yey/your_thoughts_on_thinking_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qv1yey/your_thoughts_on_thinking_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-03T19:41:24+00:00</published>
  </entry>
</feed>
