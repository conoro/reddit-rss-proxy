<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-10-16T21:06:19+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1o65dni</id>
    <title>Open Source Alternative to Perplexity</title>
    <updated>2025-10-14T03:54:51+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mergeable MindMaps.&lt;/li&gt; &lt;li&gt;Note Management&lt;/li&gt; &lt;li&gt;Multi Collaborative Notebooks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o65dni/open_source_alternative_to_perplexity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o65dni/open_source_alternative_to_perplexity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o65dni/open_source_alternative_to_perplexity/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-14T03:54:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6szeh</id>
    <title>My local Ollama UI, Cortex, now has Conversation Forking &amp; Response Regeneration</title>
    <updated>2025-10-14T21:51:59+00:00</updated>
    <author>
      <name>/u/Ok-Function-7101</name>
      <uri>https://old.reddit.com/user/Ok-Function-7101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o6szeh/my_local_ollama_ui_cortex_now_has_conversation/"&gt; &lt;img alt="My local Ollama UI, Cortex, now has Conversation Forking &amp;amp; Response Regeneration" src="https://b.thumbs.redditmedia.com/JJyGuqLlANzkDfCxOwKrHzKBG_CcyhFQf2OX8fe41_c.jpg" title="My local Ollama UI, Cortex, now has Conversation Forking &amp;amp; Response Regeneration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Wanted to share a big batch of updates I've pushed for my desktop UI, Cortex, over the last few days. The goal is to build a fast, private, and powerful local chat client, and these new features are a big step in that direction.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I've added conversation forking, AI response regeneration, completely overhauled code rendering, moved the entire chat history to a fast SQLite database, and fixed a ton of bugs (including the &amp;quot;View Reasoning&amp;quot; button and broken copy/paste).&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick rundown of what‚Äôs new:&lt;/p&gt; &lt;h1&gt;üí¨ New Conversational Controls: Forking &amp;amp; Regeneration&lt;/h1&gt; &lt;p&gt;This was the biggest focus. I wanted to make conversations less linear and give you more control.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Regenerate Response:&lt;/strong&gt; You can now &amp;quot;reroll&amp;quot; the AI's last message. A small icon appears under the last response‚Äîclick it, and the model tries again. Perfect for when you want a different take or a better solution.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fork Conversation:&lt;/strong&gt; Ever want to explore a tangent without messing up your current chat? Now you can. A &amp;quot;fork&amp;quot; icon appears on every AI message. Clicking it instantly creates a new chat that contains the history up to that point. It even names it intelligently (e.g., &amp;quot;My Chat&amp;quot; becomes &amp;quot;My Chat Thread:2&amp;quot;).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üíª Major UI/UX Overhaul: Code Blocks &amp;amp; Shortcuts&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Proper Code Rendering:&lt;/strong&gt; No more plain text in a box. Code blocks now get their own container with syntax highlighting that respects your light/dark theme. It also shows the detected language and has a one-click &lt;strong&gt;&amp;quot;Copy&amp;quot; button&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Keyboard Shortcuts:&lt;/strong&gt; For those who hate using the mouse: &lt;ul&gt; &lt;li&gt;Ctrl+N - New Chat&lt;/li&gt; &lt;li&gt;Ctrl+, - Open Settings&lt;/li&gt; &lt;li&gt;Ctrl+L - Focus the message input box&lt;/li&gt; &lt;li&gt;(Uses Cmd on macOS, of course)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smarter UI:&lt;/strong&gt; Fixed some annoying UI bugs, like dialogs blurring the wrong windows and theme switching not being instant.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üöÄ Under the Hood: Speed, Stability &amp;amp; Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture Overhaul (SQLite Database):&lt;/strong&gt; This is a big one. I've ripped out the old system of saving chats as individual text files and replaced it with a proper SQLite database. &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What this means for you:&lt;/strong&gt; Loading chat history is now instantaneous, and your data is safe from corruption if the app crashes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Migration is automatic.&lt;/strong&gt; On first run, it will find your old chats and move them into the new database for you.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;New Automated Installer:&lt;/strong&gt; For new users, I built a setup utility that helps you download Ollama and pull models directly from a list, no command line needed.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üîß Important Fixes &amp;amp; Quality of Life&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;‚úÖ FIXED: &amp;quot;View Reasoning&amp;quot; Button:&lt;/strong&gt; A recent Ollama API change broke the logic for showing the model's chain-of-thought. I've patched it to work with both new and old Ollama versions, so the &amp;quot;View Reasoning&amp;quot; button is back. Thanks to the user who sent logs for this!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚úÖ FIXED: Copy/Paste:&lt;/strong&gt; The right-click context menu &amp;quot;Copy&amp;quot; and &amp;quot;Copy All&amp;quot; actions were broken. This is now fixed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Non-Annoying Update Checker:&lt;/strong&gt; The app now checks for new versions silently in the background on startup. If there's an update, it'll just show a small notification in the Settings panel, no annoying pop-ups.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Clear All History&amp;quot; Button:&lt;/strong&gt; You can now nuke your entire chat history if you want a fresh start (right-click the &amp;quot;+ New Chat&amp;quot; button).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Check it out on GitHub&lt;/h1&gt; &lt;p&gt;For anyone who hasn't seen it before, Cortex is a private, secure desktop UI for Ollama. Everything runs 100% locally on your machine. No cloud, no data collection.&lt;/p&gt; &lt;p&gt;You can find the source code, see the full release notes, and grab the latest release from the GitHub repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fdovvnloading%2FCortex"&gt;&lt;strong&gt;https://github.com/dovvnloading/Cortex&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Been a busy few days of coding. Let me know what you think! All feedback and contributions on GitHub are welcome.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jc51vshi95vf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa025a76d53705346e21c53cf664e58f6cbdd7a1"&gt;https://preview.redd.it/jc51vshi95vf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa025a76d53705346e21c53cf664e58f6cbdd7a1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bfbfpy5f95vf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4d7c3a40f1ea8291838f2b938da98b14d84a60b"&gt;https://preview.redd.it/bfbfpy5f95vf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4d7c3a40f1ea8291838f2b938da98b14d84a60b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0v6smz5f95vf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b028383770f692074e179a0bce12c1327a0017a5"&gt;https://preview.redd.it/0v6smz5f95vf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b028383770f692074e179a0bce12c1327a0017a5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9n4owy5f95vf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb5728d66e5f429a144e953067ee924a73b33dc3"&gt;https://preview.redd.it/9n4owy5f95vf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb5728d66e5f429a144e953067ee924a73b33dc3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7y96s26f95vf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2a5a460471aec472b5fbc9538a6b4ccfa48e9bf"&gt;https://preview.redd.it/7y96s26f95vf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2a5a460471aec472b5fbc9538a6b4ccfa48e9bf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x7cij18f95vf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4129727e4a787afa3a70b68da0af5390f6f8ddf8"&gt;https://preview.redd.it/x7cij18f95vf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4129727e4a787afa3a70b68da0af5390f6f8ddf8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g8fs4r6f95vf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=381dcdce7318d0f0da45bb43511931f16868a706"&gt;https://preview.redd.it/g8fs4r6f95vf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=381dcdce7318d0f0da45bb43511931f16868a706&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(yes there is a light mode)&lt;/p&gt; &lt;p&gt;Wrapping up, I promise that this is likely the last self promot-ish post for this ap on here :) Thanks for all the kind words from the community previously. As always - keep it open source! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Function-7101"&gt; /u/Ok-Function-7101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6szeh/my_local_ollama_ui_cortex_now_has_conversation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6szeh/my_local_ollama_ui_cortex_now_has_conversation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o6szeh/my_local_ollama_ui_cortex_now_has_conversation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-14T21:51:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7bb9r</id>
    <title>Can Ollama on Linux write like ¬´Dan Kennedy¬ª after training on my texts?</title>
    <updated>2025-10-15T13:40:56+00:00</updated>
    <author>
      <name>/u/No_Discussion_8125</name>
      <uri>https://old.reddit.com/user/No_Discussion_8125</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I need your advice, please.&lt;br /&gt; From time to time, I think about switching to Linux (Pop!_OS or Mint) and installing Ollama for copywriting in my social media agency.&lt;/p&gt; &lt;p&gt;If I train Ollama on many of my texts, could its writing become good enough to replace a mid-level human copywriter?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Discussion_8125"&gt; /u/No_Discussion_8125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7bb9r/can_ollama_on_linux_write_like_dan_kennedy_after/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7bb9r/can_ollama_on_linux_write_like_dan_kennedy_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o7bb9r/can_ollama_on_linux_write_like_dan_kennedy_after/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-15T13:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6tqrk</id>
    <title>Please recommend me local models based on my pc specs that would run well</title>
    <updated>2025-10-14T22:22:52+00:00</updated>
    <author>
      <name>/u/zeek988</name>
      <uri>https://old.reddit.com/user/zeek988</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have the following&lt;/p&gt; &lt;p&gt;Ryzen 7800x3d&lt;/p&gt; &lt;p&gt;64gb dd5 ram&lt;/p&gt; &lt;p&gt;Rtx 5080 16gb vram&lt;/p&gt; &lt;p&gt;I am new to this and just am only interested in gerneral questions and image based questions if possible for now&lt;/p&gt; &lt;p&gt;I have Ollama with open web ui in docker and I also have lm studio if it matters&lt;/p&gt; &lt;p&gt;Please and thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zeek988"&gt; /u/zeek988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6tqrk/please_recommend_me_local_models_based_on_my_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6tqrk/please_recommend_me_local_models_based_on_my_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o6tqrk/please_recommend_me_local_models_based_on_my_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-14T22:22:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o74i8m</id>
    <title>Ollama cloud models not working anymore?</title>
    <updated>2025-10-15T07:26:21+00:00</updated>
    <author>
      <name>/u/Juuljuul</name>
      <uri>https://old.reddit.com/user/Juuljuul</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[SOLVED] About two weeks ago I got an e-mail that Ollama is introducing cloud models. I did a short test, and it worked. Haven't touched it since. Today I tried it, but the cloud models are not responding. I type my message and send it, but I receive no response. The local models still work. Did I miss something? Has licensing changed (I'm not paying for cloud) I'm on a mac, using the desktop Ollama version 0.12.5 (0.12.5)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juuljuul"&gt; /u/Juuljuul &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o74i8m/ollama_cloud_models_not_working_anymore/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o74i8m/ollama_cloud_models_not_working_anymore/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o74i8m/ollama_cloud_models_not_working_anymore/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-15T07:26:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o75cem</id>
    <title>best local model for article analysis and summarization</title>
    <updated>2025-10-15T08:22:59+00:00</updated>
    <author>
      <name>/u/Luke1144</name>
      <uri>https://old.reddit.com/user/Luke1144</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Luke1144"&gt; /u/Luke1144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1o75bah/best_local_model_for_article_analysis_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o75cem/best_local_model_for_article_analysis_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o75cem/best_local_model_for_article_analysis_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-15T08:22:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7b136</id>
    <title>What's a good model for concrete descriptions?</title>
    <updated>2025-10-15T13:29:13+00:00</updated>
    <author>
      <name>/u/JDRedBeard</name>
      <uri>https://old.reddit.com/user/JDRedBeard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm doing some testing with Ollama, and I ask for something, for example, &amp;quot;describe a fluffy Maine coon.&amp;quot; The response comes back with some flowery language. I dont want to know how &amp;quot;majestic&amp;quot; it's fur is flowing in the wind. I'm looking for descriptions that are more succcinct and specific. &lt;/p&gt; &lt;p&gt;To be fair, I'm sure I can adjust the prompt. While I experiment, I also would like to try other models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JDRedBeard"&gt; /u/JDRedBeard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7b136/whats_a_good_model_for_concrete_descriptions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7b136/whats_a_good_model_for_concrete_descriptions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o7b136/whats_a_good_model_for_concrete_descriptions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-15T13:29:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6z7he</id>
    <title>Internal search engine for companies</title>
    <updated>2025-10-15T02:29:37+00:00</updated>
    <author>
      <name>/u/Inevitable-Letter385</name>
      <uri>https://old.reddit.com/user/Inevitable-Letter385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anyone new to PipesHub, it‚Äôs a fully open source platform that brings all your business data together and makes it searchable and usable by AI Agents. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, and even local file uploads. You can deploy it and run it with just one docker compose command.&lt;/p&gt; &lt;p&gt;The entire system is built on a &lt;strong&gt;fully event-streaming architecture powered by Kafka&lt;/strong&gt;, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep understanding of user, organization and teams with enterprise knowledge graph&lt;/li&gt; &lt;li&gt;Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama&lt;/li&gt; &lt;li&gt;Use any provider that supports OpenAI compatible endpoints&lt;/li&gt; &lt;li&gt;Choose from 1,000+ embedding models&lt;/li&gt; &lt;li&gt;Vision-Language Models and OCR for visual or scanned docs&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, or SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs for developers&lt;/li&gt; &lt;li&gt;All major file types support including pdfs with images, diagrams and charts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Features releasing this month&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more&lt;/li&gt; &lt;li&gt;Reasoning Agent that plans before executing tasks&lt;/li&gt; &lt;li&gt;50+ Connectors allowing you to connect to your entire business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out and share your thoughts or feedback:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have a Discord community if you want to join!&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.com/invite/K5RskzJBm2"&gt;https://discord.com/invite/K5RskzJBm2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We‚Äôre looking for contributors to help shape the future of &lt;strong&gt;PipesHub&lt;/strong&gt;.. an open-source platform for building powerful AI Agents and enterprise search.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Letter385"&gt; /u/Inevitable-Letter385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6z7he/internal_search_engine_for_companies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6z7he/internal_search_engine_for_companies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o6z7he/internal_search_engine_for_companies/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-15T02:29:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6oo5y</id>
    <title>Nvidia DGX Spark, is it worth ?</title>
    <updated>2025-10-14T19:09:57+00:00</updated>
    <author>
      <name>/u/Appropriate-Camp7981</name>
      <uri>https://old.reddit.com/user/Appropriate-Camp7981</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o6oo5y/nvidia_dgx_spark_is_it_worth/"&gt; &lt;img alt="Nvidia DGX Spark, is it worth ?" src="https://preview.redd.it/jpm27ri6m4vf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3f612ec7a46fdd4e276c69a7e6e429e1635c47c" title="Nvidia DGX Spark, is it worth ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just received an email with a window to buy nvidia Dgx Spark. Is it worth against cloud platforms ?&lt;/p&gt; &lt;p&gt;I could ask ChatGPT but for a change wanted to involve my dear fellow humans to figure this out. &lt;/p&gt; &lt;p&gt;I am using &amp;lt; 30B models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Appropriate-Camp7981"&gt; /u/Appropriate-Camp7981 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jpm27ri6m4vf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o6oo5y/nvidia_dgx_spark_is_it_worth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o6oo5y/nvidia_dgx_spark_is_it_worth/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-14T19:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o75of7</id>
    <title>How can I enable LLM running on my remote Ollama server to access the local files?</title>
    <updated>2025-10-15T08:45:07+00:00</updated>
    <author>
      <name>/u/ThingRexCom</name>
      <uri>https://old.reddit.com/user/ThingRexCom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o75of7/how_can_i_enable_llm_running_on_my_remote_ollama/"&gt; &lt;img alt="How can I enable LLM running on my remote Ollama server to access the local files?" src="https://preview.redd.it/hm08mx1jn8vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5caf43e31e35f14f56955feecf39db690c0610d1" title="How can I enable LLM running on my remote Ollama server to access the local files?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to create the following setup: a local AI CLI Agent that can access files on my system and use bash (for example, to analyze a local SQLite database). That agent should communicate with my remote Ollama server hosting LLMs.&lt;/p&gt; &lt;p&gt;Currently, I can chat with LLM on the Ollama server via the AI CLI Agent.&lt;/p&gt; &lt;p&gt;When I try to make the AI Agent analyze local files, I sometimes get&lt;/p&gt; &lt;p&gt;&lt;code&gt;AI_APICallError: Not Found&lt;/code&gt;&lt;/p&gt; &lt;p&gt;and, most of the time, the agent is totally lost:&lt;/p&gt; &lt;p&gt;&lt;code&gt;'We see invalid call. Need to read file content; use filesystem_read_text_file. We'll investigate code.We have a project with mydir and modules/add. likely a bug. Perhaps user hasn't given a specific issue yet? There is no explicit problem statement. The environment root has tests. Probably the issue? Let me inspect repository structure.Need a todo list? No. Let's read directory.{&amp;quot;todos&amp;quot;:&amp;quot;'}'&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I have tried the server-filesystem MCP, but it hasn't improved anything.&lt;/p&gt; &lt;p&gt;At the same time, the Gemini CLI works perfectly fine - it can browse local files and use bash to interact with SQLite.&lt;/p&gt; &lt;p&gt;How can I improve my setup? I have tested nanocoder and opencode AI CLI agents - both have the same issues when working with remote GPT-OSS-20B. Everything works fine when I connect those AI Agents to Ollama running on my laptop - the same agents can interact with the local filesystem backed by the same LLM in the local Ollama.&lt;/p&gt; &lt;p&gt;How can I replicate those capabilities when working with remote Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThingRexCom"&gt; /u/ThingRexCom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hm08mx1jn8vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o75of7/how_can_i_enable_llm_running_on_my_remote_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o75of7/how_can_i_enable_llm_running_on_my_remote_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-15T08:45:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7uo49</id>
    <title>eMedia Document Handling using Ollama</title>
    <updated>2025-10-16T02:33:11+00:00</updated>
    <author>
      <name>/u/digital_legacy</name>
      <uri>https://old.reddit.com/user/digital_legacy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o7uo49/emedia_document_handling_using_ollama/"&gt; &lt;img alt="eMedia Document Handling using Ollama" src="https://external-preview.redd.it/sCcJmJwYyqUiO0g1OigRBfl3D2WvLUXwSTiw3odd4mM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53cc5c0ca7dd30411f8915d2f66ad347f54e6157" title="eMedia Document Handling using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/digital_legacy"&gt; /u/digital_legacy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7td80w7pvdvf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7uo49/emedia_document_handling_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o7uo49/emedia_document_handling_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T02:33:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o81o9d</id>
    <title>BrightPal AI ‚Äì An open-source study assistant powered by Ollama (now available for Mac) - please checkout and support the project.</title>
    <updated>2025-10-16T09:34:32+00:00</updated>
    <author>
      <name>/u/PsychologyJumpy5104</name>
      <uri>https://old.reddit.com/user/PsychologyJumpy5104</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o81o9d/brightpal_ai_an_opensource_study_assistant/"&gt; &lt;img alt="BrightPal AI ‚Äì An open-source study assistant powered by Ollama (now available for Mac) - please checkout and support the project." src="https://external-preview.redd.it/MDMyZ2V0cWR6ZnZmMUdRun_WvB-ako7oiS9soRfRBPpybrethPQlsknNFi9G.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9efd020a261e688be7b0ca6736ed1fb70dc49400" title="BrightPal AI ‚Äì An open-source study assistant powered by Ollama (now available for Mac) - please checkout and support the project." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks üëã&lt;/p&gt; &lt;p&gt;I‚Äôve been working on something new called &lt;strong&gt;BrightPal AI&lt;/strong&gt; ,an AI study assistant built on top of &lt;strong&gt;Ollama&lt;/strong&gt; to help you study PDFs and notes &lt;em&gt;locally&lt;/em&gt; on your laptop. Features like Notetaking and Highlighting also is available. &lt;/p&gt; &lt;p&gt;No subscriptions, no cloud processing - just you, your materials, and your local model.&lt;br /&gt; You can highlight, take notes, and ask questions directly from your readings, all powered by Ollama.&lt;/p&gt; &lt;p&gt;It‚Äôs built for students (or honestly anyone who reads a lot) who want AI help without giving up privacy or paying monthly fees. It only has $20 one time fee (lifetime).&lt;/p&gt; &lt;p&gt;üëâ It‚Äôs &lt;strong&gt;available for Mac now&lt;/strong&gt;, and I‚Äôd love if Ollama community could support the project.&lt;br /&gt; Give it a try and let me know what you think! ‚ù§Ô∏è&lt;/p&gt; &lt;p&gt;I can very confidently say that it definitely will increase your productivity with every article, pdfs, research paper stored in same place and a local AI model to clear doubts.&lt;br /&gt; Download Link the the first comment!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PsychologyJumpy5104"&gt; /u/PsychologyJumpy5104 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/m0i6ftqdzfvf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o81o9d/brightpal_ai_an_opensource_study_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o81o9d/brightpal_ai_an_opensource_study_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T09:34:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7xql7</id>
    <title>Please help me out</title>
    <updated>2025-10-16T05:18:15+00:00</updated>
    <author>
      <name>/u/One-Will5139</name>
      <uri>https://old.reddit.com/user/One-Will5139</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm new to ML &amp;amp; AI. Right now I have an urgent requirement to compare a diariziation and a procedure pdf. The first problem is that the procedure pdf has a lot of acronyms. Secondly, I need to setup a verification table for the diarization showing match, partially match and mismatch, but I'm not able to get accurate comparison of the diarization and procedure pdf because the diarization has a bit of general conversation('hello', 'got it', 'are you there' etc) in it. Please help me out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One-Will5139"&gt; /u/One-Will5139 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7xql7/please_help_me_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7xql7/please_help_me_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o7xql7/please_help_me_out/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T05:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7u30c</id>
    <title>Reported Bug - GPT-OSS:20B reasoning loop in 0.12.5</title>
    <updated>2025-10-16T02:04:43+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o7u30c/reported_bug_gptoss20b_reasoning_loop_in_0125/"&gt; &lt;img alt="Reported Bug - GPT-OSS:20B reasoning loop in 0.12.5" src="https://external-preview.redd.it/zHQSW1UVzuG8OeWAAIMebWgBhHTM5rP4ynefNCEn3eA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8986a8a2802483de7e6ec702d08ac5bd837b9bd7" title="Reported Bug - GPT-OSS:20B reasoning loop in 0.12.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ollama/ollama/issues/12606#issuecomment-3401080560"&gt;https://github.com/ollama/ollama/issues/12606#issuecomment-3401080560&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I've been having some issues the last week or so with my instance of GPT-OSS:20b going bat shit crazy. I thought maybe something got corrupted or changed. Updated things, changed system prompts etc. and just nuts. Tested on my gaming rig with LM Studio and my 4080 Super and model worked just fine. Tested again on my AI Rig (2x 3090s EPYC 7402p 256GB RAM Ubuntu 24.0.4) but this time used vLLM and again, model worked fine. &lt;/p&gt; &lt;p&gt;Checked with Perplexity and it found the link above where someone else was having the same reasoning loop issues that look like this&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bkvebvivsdvf1.png?width=2205&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec2ef5929a363c16a647d387c460fda9eb479e2f"&gt;https://preview.redd.it/bkvebvivsdvf1.png?width=2205&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec2ef5929a363c16a647d387c460fda9eb479e2f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just wanted to give a heads up that the bug has been reported, incase anyone else was experiencing the same thing&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7u30c/reported_bug_gptoss20b_reasoning_loop_in_0125/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7u30c/reported_bug_gptoss20b_reasoning_loop_in_0125/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o7u30c/reported_bug_gptoss20b_reasoning_loop_in_0125/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T02:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1o81838</id>
    <title>How to install ollama on existing docker image and work with GPU</title>
    <updated>2025-10-16T09:05:24+00:00</updated>
    <author>
      <name>/u/vdiallonort</name>
      <uri>https://old.reddit.com/user/vdiallonort</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, i install cuda driver on my machine and when in use ollama docker image &lt;a href="https://hub.docker.com/r/ollama/ollama"&gt;https://hub.docker.com/r/ollama/ollama&lt;/a&gt; everything work great my two 3090 are detected. But i don't know how to reproduce this from existing image i want to modifiy ( and not start from the ollama one ) . Is there any documentation on what i need to setup on the Docker file to get the same result ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vdiallonort"&gt; /u/vdiallonort &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o81838/how_to_install_ollama_on_existing_docker_image/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o81838/how_to_install_ollama_on_existing_docker_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o81838/how_to_install_ollama_on_existing_docker_image/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T09:05:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7tayq</id>
    <title>Best local model for product classifying ?</title>
    <updated>2025-10-16T01:27:53+00:00</updated>
    <author>
      <name>/u/Ok-Depth-6337</name>
      <uri>https://old.reddit.com/user/Ok-Depth-6337</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Ryzen 9 9950x3D + 5070 ti&lt;/p&gt; &lt;p&gt;im searching a model to use for product classfying, i need to classify more than 700k products.&lt;/p&gt; &lt;p&gt;this is the actual prompt im using.&lt;/p&gt; &lt;p&gt;i ve tried with gpt-oss:20b but is not fast enough to do it well.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Classify {len(products)} tech products: KEEP/NOT/UNSURE KEEP Rules (Premium Tech): - PC Desktops (RTX, GTX graphics) - Laptops - Workstations - Servers (rack/tower servers) - Smartphones (premium models &amp;gt;300‚Ç¨) - Monitors (&amp;gt;24&amp;quot;, 4K, gaming, ultrawide, business) - Tablets (iPad Pro, Galaxy Tab S, any &amp;gt;200‚Ç¨) - CPUs/GPUs: ALL NVIDIA RTX/GTX, AMD Radeon, Intel processors - Photography equipment (cameras, lenses) - Premium Audio devices (headphones &amp;gt;200‚Ç¨, speakers) - Gaming peripherals from premium brands (Logitech G, Razer, Corsair and more) - Any Tech product above 200‚Ç¨ estimated not listed above NOT Rules (Basic/Accessories): - Very Basic Phone accessories (cases, chargers, cables) - Very Basic smartphones (&amp;lt;200‚Ç¨, old models) - Software licenses - Furniture/appliances (washing machines, ovens, kitchen) - Power supplies alone (without PC) - Very Basic peripherals (&amp;lt;50‚Ç¨, generic brands) - Books, non-tech items - Beauty products Decision examples: - If has RTX/GTX/Radeon GPU or i7/i9/Ryzen 7/9 ‚Üí ALWAYS KEEP - If gaming monitor with 144Hz+ ‚Üí KEEP - If laptop with i7+ / ryzen 7+ ‚Üí KEEP - If gaming laptop/PC with &amp;quot;OMEN&amp;quot;, &amp;quot;TUF&amp;quot;, &amp;quot;ROG&amp;quot; ‚Üí KEEP - If Apple products ‚Üí KEEP (NOT for accessories) (premium products) - If contains &amp;quot;washing&amp;quot;, &amp;quot;kitchen&amp;quot;, &amp;quot;furniture&amp;quot;, &amp;quot;beauty&amp;quot; ‚Üí NOT UNSURE Rules (use sparingly): - Only for truly ambiguous tech products - When product specs are unclear - Never use for clear GPU, clear accessories, or clear appliances Examples: - &amp;quot;RTX 4090 Graphics Card&amp;quot; ‚Üí KEEP (premium GPU) - &amp;quot;Samsung Gaming Monitor ODYSSEY 240Hz&amp;quot; ‚Üí KEEP (gaming monitor) - &amp;quot;Samsung Smart Monitor M8 4K&amp;quot; ‚Üí KEEP (premium monitor) - &amp;quot;Samsung NEO G8 UHD 240Hz&amp;quot; ‚Üí KEEP (gaming monitor) - &amp;quot;Samsung NEO G7 165Hz&amp;quot; ‚Üí KEEP (gaming monitor) - &amp;quot;Samsung CH890 Ultrawide&amp;quot; ‚Üí KEEP (premium monitor) - &amp;quot;MSI Gaming Laptop RTX 4060&amp;quot; ‚Üí KEEP (gaming laptop) - &amp;quot;HP OMEN 17 i9 32GB&amp;quot; ‚Üí KEEP (gaming laptop) - &amp;quot;ASUS TUF Gaming&amp;quot; ‚Üí KEEP (gaming laptop) - &amp;quot;iPhone 15 Pro&amp;quot; ‚Üí KEEP (premium smartphone) - &amp;quot;Galaxy Tab S6 Lite&amp;quot; ‚Üí NOT (basic tablet &amp;lt;200‚Ç¨) - &amp;quot;Galaxy Tab S8+ 256GB&amp;quot; ‚Üí KEEP (premium tablet) - &amp;quot;ThinkPad X1 Carbon&amp;quot; ‚Üí KEEP (business laptop) - &amp;quot;TravelMate P4 i7 16GB&amp;quot; ‚Üí KEEP (business laptop) - &amp;quot;Apple iMac 24&amp;quot; M1&amp;quot; ‚Üí KEEP (premium computer) - &amp;quot;MacBook Pro&amp;quot; ‚Üí KEEP (premium laptop) - &amp;quot;USB Cable 2m&amp;quot; ‚Üí NOT (accessory) - &amp;quot;Washing Machine Siemens&amp;quot; ‚Üí NOT (appliance) Example JSON format with 3 items: [ {{&amp;quot;id&amp;quot;: 1, &amp;quot;asin&amp;quot;: &amp;quot;B09XYZ123&amp;quot;, &amp;quot;brand&amp;quot;: &amp;quot;MSI&amp;quot;, &amp;quot;title&amp;quot;: &amp;quot;MSI Gaming Laptop RTX 4060&amp;quot;, &amp;quot;decision&amp;quot;: &amp;quot;KEEP&amp;quot;, &amp;quot;reason&amp;quot;: &amp;quot;Gaming laptop with RTX GPU&amp;quot;}}, {{&amp;quot;id&amp;quot;: 2, &amp;quot;asin&amp;quot;: &amp;quot;B08ABC456&amp;quot;, &amp;quot;brand&amp;quot;: &amp;quot;Samsung&amp;quot;, &amp;quot;title&amp;quot;: &amp;quot;USB-C Cable 2m&amp;quot;, &amp;quot;decision&amp;quot;: &amp;quot;NOT&amp;quot;, &amp;quot;reason&amp;quot;: &amp;quot;Basic accessory&amp;quot;}}, {{&amp;quot;id&amp;quot;: 3, &amp;quot;asin&amp;quot;: &amp;quot;B07DEF789&amp;quot;, &amp;quot;brand&amp;quot;: &amp;quot;Unknown Brand&amp;quot;, &amp;quot;title&amp;quot;: &amp;quot;Tablet specs unclear&amp;quot;, &amp;quot;decision&amp;quot;: &amp;quot;UNSURE&amp;quot;, &amp;quot;reason&amp;quot;: &amp;quot;Insufficient product info&amp;quot;}} ] Products to classify: {products_text} IMPORTANT: Return ONLY the completed JSON array. Do not include any thinking, explanations, or other text. Start your response directly with [ and end with ]. Fill in the decision and reason fields for EXACTLY {len(products)} objects: {skeleton_json} &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Depth-6337"&gt; /u/Ok-Depth-6337 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7tayq/best_local_model_for_product_classifying/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7tayq/best_local_model_for_product_classifying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o7tayq/best_local_model_for_product_classifying/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T01:27:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1o85brs</id>
    <title>Something I made</title>
    <updated>2025-10-16T12:52:19+00:00</updated>
    <author>
      <name>/u/Last-Shake-9874</name>
      <uri>https://old.reddit.com/user/Last-Shake-9874</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o85brs/something_i_made/"&gt; &lt;img alt="Something I made" src="https://a.thumbs.redditmedia.com/5tCJ722xq0MBcLtNqnzlXIbQ6nYXLFA6w1CBvKY3uY8.jpg" title="Something I made" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Last-Shake-9874"&gt; /u/Last-Shake-9874 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1o854xk/something_i_made/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o85brs/something_i_made/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o85brs/something_i_made/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T12:52:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1o88fxv</id>
    <title>Accessing Ollama models from a different Laptop</title>
    <updated>2025-10-16T14:59:00+00:00</updated>
    <author>
      <name>/u/chirchan91</name>
      <uri>https://old.reddit.com/user/chirchan91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear Community,&lt;br /&gt; I've a RTX 5060 powered laptop and a non-GPU laptop (both are running Windows 11). I've setup couple of Ollama models in my GPU laptop. Can someone provide me any sources or references on how can i access these Ollama models in my other laptop. TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chirchan91"&gt; /u/chirchan91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o88fxv/accessing_ollama_models_from_a_different_laptop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o88fxv/accessing_ollama_models_from_a_different_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o88fxv/accessing_ollama_models_from_a_different_laptop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T14:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7z1y3</id>
    <title>How to pick the best ollama model for your use case.</title>
    <updated>2025-10-16T06:40:18+00:00</updated>
    <author>
      <name>/u/evalProtocol</name>
      <uri>https://old.reddit.com/user/evalProtocol</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o7z1y3/how_to_pick_the_best_ollama_model_for_your_use/"&gt; &lt;img alt="How to pick the best ollama model for your use case." src="https://external-preview.redd.it/5Yyi7FZfBglJXdTAq5ctyvLjdHtxUhbYkAAZztvAOSg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=555cf9a9171ccbc0dd2a187ee6851a61b8931671" title="How to pick the best ollama model for your use case." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey I am Benny, I have been working on &lt;a href="http://evalprotocol.io"&gt;evalprotocol.io&lt;/a&gt; for a while now, and we recently published a post on using evaluations to pick the best local model to get your job done &lt;a href="https://fireworks.ai/blog/llm-judge-eval-protocol-ollama"&gt;https://fireworks.ai/blog/llm-judge-eval-protocol-ollama&lt;/a&gt; . The SDK is here &lt;a href="https://github.com/eval-protocol/python-sdk"&gt;https://github.com/eval-protocol/python-sdk&lt;/a&gt; , totally open source, and would love to figure out how to best work together with everyone. Please give it a try and let me know if you have any feedback!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wez92nzz5fvf1.png?width=2454&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55544f5acd4332e6977c7be82e44f0fa9ea9edda"&gt;https://preview.redd.it/wez92nzz5fvf1.png?width=2454&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55544f5acd4332e6977c7be82e44f0fa9ea9edda&lt;/a&gt;&lt;/p&gt; &lt;p&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/evalProtocol"&gt; /u/evalProtocol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7z1y3/how_to_pick_the_best_ollama_model_for_your_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o7z1y3/how_to_pick_the_best_ollama_model_for_your_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o7z1y3/how_to_pick_the_best_ollama_model_for_your_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T06:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o83ejq</id>
    <title>Ollama's cloud what‚Äôs the limits?</title>
    <updated>2025-10-16T11:17:16+00:00</updated>
    <author>
      <name>/u/CertainTime5947</name>
      <uri>https://old.reddit.com/user/CertainTime5947</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anybody paying for access to the cloud hosted models? This might be interesting depending on the limits, calls per hour, tokens per day etc, but I can for my life not find any info on this. In the docs they write &amp;quot;Ollama's cloud includes hourly and daily limits to avoid capacity issues&amp;quot; ok.. and they are?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CertainTime5947"&gt; /u/CertainTime5947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o83ejq/ollamas_cloud_whats_the_limits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o83ejq/ollamas_cloud_whats_the_limits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o83ejq/ollamas_cloud_whats_the_limits/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T11:17:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8h138</id>
    <title>why no one is speaking about the ollama gui ?</title>
    <updated>2025-10-16T20:17:02+00:00</updated>
    <author>
      <name>/u/Constant-Fondant-178</name>
      <uri>https://old.reddit.com/user/Constant-Fondant-178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o8h138/why_no_one_is_speaking_about_the_ollama_gui/"&gt; &lt;img alt="why no one is speaking about the ollama gui ?" src="https://b.thumbs.redditmedia.com/cww06GGh2FJFtVra9--eZiDTx-YyW7WtbDZ9Bah1G9Y.jpg" title="why no one is speaking about the ollama gui ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ifhy27gy7jvf1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a009fc6b1517203fea4b14311809fa899fce2737"&gt;https://preview.redd.it/ifhy27gy7jvf1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a009fc6b1517203fea4b14311809fa899fce2737&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Constant-Fondant-178"&gt; /u/Constant-Fondant-178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o8h138/why_no_one_is_speaking_about_the_ollama_gui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o8h138/why_no_one_is_speaking_about_the_ollama_gui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o8h138/why_no_one_is_speaking_about_the_ollama_gui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T20:17:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o831a8</id>
    <title>Looking for a good agentic coding model that fits into Apple M1 Max, 32 GB</title>
    <updated>2025-10-16T10:56:53+00:00</updated>
    <author>
      <name>/u/ThingRexCom</name>
      <uri>https://old.reddit.com/user/ThingRexCom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o831a8/looking_for_a_good_agentic_coding_model_that_fits/"&gt; &lt;img alt="Looking for a good agentic coding model that fits into Apple M1 Max, 32 GB" src="https://preview.redd.it/urja35ayfgvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=728e6b36df2275f162e72765721dcdc07a926e66" title="Looking for a good agentic coding model that fits into Apple M1 Max, 32 GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a huge fan of agentic coding using CLI (i.e., Gemini CLI). I want to create a local setup on Apple M1 Max 32 GB providing similar experience.&lt;/p&gt; &lt;p&gt;Currently, my best setup is Opencode + llama.cpp + gpt-oss-20b.&lt;/p&gt; &lt;p&gt;I have tried other models from HF marked as compatible with my hardware, but most of them failed to start:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable) ggml_metal_synchronize: error: command buffer 0 failed with status 5 error: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory) /private/tmp/llama.cpp-20251013-5280-4lte0l/ggml/src/ggml-metal/ggml-metal-context.m:241: fatal error &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Any recommendation regarding the LLM and fine-tuning my setup is very welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThingRexCom"&gt; /u/ThingRexCom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/urja35ayfgvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o831a8/looking_for_a_good_agentic_coding_model_that_fits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o831a8/looking_for_a_good_agentic_coding_model_that_fits/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T10:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1o84ndg</id>
    <title>Configuring GPT OSS 20B for smaller systems</title>
    <updated>2025-10-16T12:20:54+00:00</updated>
    <author>
      <name>/u/Birdinhandandbush</name>
      <uri>https://old.reddit.com/user/Birdinhandandbush</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If this has been answered I've missed it so I apologise. When running GPT-OSS 20B on my LM Studio instance I can set number of experts and reasoning effort, so I can still run on a GTX1660ti and get about 15 tokens/sec with 6gb VRAM and 32gb system ram. &lt;/p&gt; &lt;p&gt;In Ollama and Open WebUI I can't see where I can make the same adjustments, the number of experts setting isn't in an obvious place IMO. &lt;/p&gt; &lt;p&gt;At present on the Ollama + Open WebUi is giving me 7 tokens/sec but I can't configure it from what I can see. &lt;/p&gt; &lt;p&gt;Any help appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Birdinhandandbush"&gt; /u/Birdinhandandbush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o84ndg/configuring_gpt_oss_20b_for_smaller_systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o84ndg/configuring_gpt_oss_20b_for_smaller_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o84ndg/configuring_gpt_oss_20b_for_smaller_systems/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T12:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o889pg</id>
    <title>Distil-PII: family of PII redaction SLMs</title>
    <updated>2025-10-16T14:52:18+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o889pg/distilpii_family_of_pii_redaction_slms/"&gt; &lt;img alt="Distil-PII: family of PII redaction SLMs" src="https://external-preview.redd.it/O_gL1U3L1p3-vSC-SY63BYIemJsMFDjZscM68KkUT8E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7099e0a2b4708e42b79cdffbd672a6712e564209" title="Distil-PII: family of PII redaction SLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We trained and released a family of small language models (SLMs) specialized for policy-aware PII redaction. The 1B model, which can be deployed locally with ollama, matches a frontier 600B+ LLM model (DeepSeek 3.1) in prediction accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/distil-labs/Distil-PII"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o889pg/distilpii_family_of_pii_redaction_slms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o889pg/distilpii_family_of_pii_redaction_slms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T14:52:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8czoo</id>
    <title>AI chess showdown: comparing LLM vs LLM using Ollama ‚Äì check out this small project</title>
    <updated>2025-10-16T17:46:39+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o8czoo/ai_chess_showdown_comparing_llm_vs_llm_using/"&gt; &lt;img alt="AI chess showdown: comparing LLM vs LLM using Ollama ‚Äì check out this small project" src="https://external-preview.redd.it/aHzSussgql1XiPGJK_HAL4uPNcl6xyTxYMBIRF1BOHk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecadb06e9853de3808f39bee844562c0594d955b" title="AI chess showdown: comparing LLM vs LLM using Ollama ‚Äì check out this small project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I made a cool little open-source tool: &lt;strong&gt;chess-llm-vs-llm&lt;/strong&gt;. &lt;a href="https://github.com/Laszlobeer/chess-llm-vs-llm"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/g8nr15yjhivf1.gif"&gt;https://i.redd.it/g8nr15yjhivf1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;üß† What it does&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;It connects with &lt;strong&gt;Ollama&lt;/strong&gt; to let you pit two language models (LLMs) against each other in chess matches. &lt;a href="https://github.com/Laszlobeer/chess-llm-vs-llm"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;You can also play &lt;strong&gt;Human vs AI&lt;/strong&gt; or watch &lt;strong&gt;AI vs AI&lt;/strong&gt; duels. &lt;a href="https://github.com/Laszlobeer/chess-llm-vs-llm"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;It uses a clean PyQt5 interface (board, move highlighting, history, undo, etc.). &lt;a href="https://github.com/Laszlobeer/chess-llm-vs-llm"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;If a model fails to return a move, there‚Äôs a fallback to a random legal move. &lt;a href="https://github.com/Laszlobeer/chess-llm-vs-llm"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üîß How to try it&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;You need &lt;strong&gt;Python 3.7+&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Install &lt;strong&gt;Ollama&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Load at least two chess-capable models in Ollama&lt;/li&gt; &lt;li&gt;&lt;code&gt;pip install PyQt5 chess requests&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Run the &lt;a href="http://chess.py"&gt;&lt;code&gt;chess.py&lt;/code&gt;&lt;/a&gt; script and pick your mode / models &lt;a href="https://github.com/Laszlobeer/chess-llm-vs-llm"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;üí≠ Why this is interesting&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;It gives a &lt;strong&gt;hands-on way&lt;/strong&gt; to compare different LLMs in a structured game environment rather than just text tasks.&lt;/li&gt; &lt;li&gt;You can see &lt;em&gt;where&lt;/em&gt; model strengths/weaknesses emerge in planning, tactics, endgames, etc.&lt;/li&gt; &lt;li&gt;It‚Äôs lightweight and modular ‚Äî you can swap in new models or augment logic.&lt;/li&gt; &lt;li&gt;For folks into AI + games, it's a fun sandbox to experiment with.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o8czoo/ai_chess_showdown_comparing_llm_vs_llm_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o8czoo/ai_chess_showdown_comparing_llm_vs_llm_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o8czoo/ai_chess_showdown_comparing_llm_vs_llm_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-16T17:46:39+00:00</published>
  </entry>
</feed>
