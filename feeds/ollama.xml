<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-10-23T18:26:08+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1obh5ex</id>
    <title>üí∞üí∞ Building Powerful AI on a Budget üí∞üí∞</title>
    <updated>2025-10-20T12:01:06+00:00</updated>
    <author>
      <name>/u/FieldMouseInTheHouse</name>
      <uri>https://old.reddit.com/user/FieldMouseInTheHouse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1obh5ex/building_powerful_ai_on_a_budget/"&gt; &lt;img alt="üí∞üí∞ Building Powerful AI on a Budget üí∞üí∞" src="https://preview.redd.it/94422xpxa9wf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f21509e2b33d144951b26ad5f4e866bce8afaef" title="üí∞üí∞ Building Powerful AI on a Budget üí∞üí∞" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ü§ó Hello, everbody!&lt;/p&gt; &lt;p&gt;I wanted to share my experience building a high-performance AI system without breaking the bank.&lt;/p&gt; &lt;p&gt;I've noticed a lot of people on here spending tons of money on top-of-the-line hardware, but I've found a way to achieve amazing results with a much more budget-friendly setup.&lt;/p&gt; &lt;p&gt;My system is built using the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A used Intel i5-6500 (3.2GHz, 4-core, 4-threads) machine that I got for cheap that came with 8GB of RAM (2 x 4GB) all installed into an ASUS H170-PRO motherboard. It also came with a RAIDER POWER SUPPLY RA650 650W power supply.&lt;/li&gt; &lt;li&gt;I installed Ubuntu Linux 22.04.5 LTS (Desktop) onto it.&lt;/li&gt; &lt;li&gt;Ollama running in Docker.&lt;/li&gt; &lt;li&gt;I purchased a new 32GB of RAM kit (2 x 16GB) for the system, bringing the total system RAM up to 40GB.&lt;/li&gt; &lt;li&gt;I then purchased two used NVDIA RTX 3060 12GB VRAM GPUs.&lt;/li&gt; &lt;li&gt;I then purchased a used Toshiba 1TB 3.5-inch SATA HDD.&lt;/li&gt; &lt;li&gt;I had a spare Samsung 1TB NVMe SSD drive lying around that I installed into this system.&lt;/li&gt; &lt;li&gt;I had two spare 500GB 2.5-inch SATA HDDs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üë®‚Äçüî¨ With the right optimizations, this setup absolutely flies! I'm getting 50-65 tokens per second, which is more than enough for my RAG and chatbot projects.&lt;/p&gt; &lt;p&gt;Here's how I did it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Quantization&lt;/strong&gt;: I run my Ollama server with Q4 quantization and use Q4 models. This makes a huge difference in VRAM usage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;num_ctx (Context Size)&lt;/strong&gt;: Forget what you've heard about context size needing to be a power of two! I experimented and found a sweet spot that perfectly matches my needs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;num_batch&lt;/strong&gt;: This was a game-changer! By tuning this parameter, I was able to drastically reduce memory usage without sacrificing performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Underclocking the GPUs&lt;/strong&gt;: Yes! You read right. To do this, I took the max wattage that that cards can run at, 170W, and reduced it to 85% of that max, being 145W. This is the sweet spot where the card's performance reasonably performs nearly the same as it would at 170W, but it totally avoids thermal throttling that would occur during heavy sustained activity! This means that I always get consistent performance results -- not spikey good results followed by some ridiculously slow results due to thermal throttling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My RAG and chatbots now run inside of just 6.7GB of VRAM, down from 10.5GB! &lt;em&gt;That is almost the equivalent of adding the equivalent of a third 6GB VRAM GPU into the mix for free!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;üíª Also, because I'm using Ollama, this single machine has become the Ollama server for every computer on my network -- and none of those other computers have a GPU worth anything!&lt;/p&gt; &lt;p&gt;Also, since I have two GPUs in this machine I have the following plan:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use the first GPU for all Ollama inference related work for the entire network. With careful planning so far, everything is fitting inside of the 6.7GB of VRAM leaving 5.3GB for any new models that can fit without causing an ejection/reload.&lt;/li&gt; &lt;li&gt;Next, I'm planning on using the second GPU to run PyTorch for distillation processing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm really happy with the results.&lt;/p&gt; &lt;p&gt;So, for a cost of about $700 US for this server, my entire network of now 5 machines got a collective AI/GPU upgrade.&lt;/p&gt; &lt;p&gt;‚ùì I'm curious if anyone else has experimented with similar optimizations.&lt;/p&gt; &lt;p&gt;What are your budget-friendly tips for optimizing AI performance???&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FieldMouseInTheHouse"&gt; /u/FieldMouseInTheHouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94422xpxa9wf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1obh5ex/building_powerful_ai_on_a_budget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1obh5ex/building_powerful_ai_on_a_budget/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-20T12:01:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1od0l98</id>
    <title>I am doing a legal chatbot where I need the Indian constitution, IPC and other official pdf's in a JSON formatted file. Anyone and solutions?</title>
    <updated>2025-10-22T06:16:00+00:00</updated>
    <author>
      <name>/u/Cold_Profession_3439</name>
      <uri>https://old.reddit.com/user/Cold_Profession_3439</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to do it for free of cost and I tried writing the python code but it is not working. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cold_Profession_3439"&gt; /u/Cold_Profession_3439 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1od0l98/i_am_doing_a_legal_chatbot_where_i_need_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1od0l98/i_am_doing_a_legal_chatbot_where_i_need_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1od0l98/i_am_doing_a_legal_chatbot_where_i_need_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-22T06:16:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oca3dq</id>
    <title>Qwen model running on Mac via Ollama was super slow with long wait times</title>
    <updated>2025-10-21T11:12:13+00:00</updated>
    <author>
      <name>/u/hellorahulkum</name>
      <uri>https://old.reddit.com/user/hellorahulkum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday, I was trying to use the latest Qwen model , and I ran into an issue. It wasn't generating responses, even after a minute or two. I had to set the timeout to over 300 seconds, and even then with `stream=True` , I couldn't get any performance boost, which caused my AI agents to fail. I can't remember what the main issue was.&lt;/p&gt; &lt;p&gt;Few things i tried:&lt;/p&gt; &lt;p&gt;&lt;em&gt;1. env&lt;/em&gt; changes:&lt;br /&gt; export OLLAMA_MAX_LOADED_MODELS=1&lt;br /&gt; export OLLAMA_NUM_CTX=2048 &lt;em&gt;# Default: 4096&lt;/em&gt;&lt;br /&gt; export OLLAMA_NUM_PARALLEL=1&lt;br /&gt; export OLLAMA_MAX_QUEUE=5&lt;/p&gt; &lt;h1&gt;2. Local Mac Optimization&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Use smaller models (qwen2:1.5b instead of 7b+)&lt;/li&gt; &lt;li&gt;Limit output tokens (&lt;code&gt;num_predict: 100&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Reduce context window (&lt;code&gt;num_ctx: 2048&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt; 2-3x speed improvement, still slow on Intel Mac&lt;/p&gt; &lt;h1&gt;3. Free GPU Cloud&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tried Google Colab&lt;/strong&gt;: Free T4 GPU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tried Kaggle&lt;/strong&gt;: Free 2x T4 GPUs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any better recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hellorahulkum"&gt; /u/hellorahulkum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oca3dq/qwen_model_running_on_mac_via_ollama_was_super/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oca3dq/qwen_model_running_on_mac_via_ollama_was_super/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oca3dq/qwen_model_running_on_mac_via_ollama_was_super/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-21T11:12:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc38pm</id>
    <title>What happens when two AI models start chatting with each other?</title>
    <updated>2025-10-21T04:11:54+00:00</updated>
    <author>
      <name>/u/Adventurous-Wind1029</name>
      <uri>https://old.reddit.com/user/Adventurous-Wind1029</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got curious‚Ä¶ so I built it.&lt;/p&gt; &lt;p&gt;This project lets you run two AI models that talk to each other in real time. They question, explain, and sometimes spiral into the weirdest loops imaginable.&lt;/p&gt; &lt;p&gt;You can try it yourself here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/iKemo-io/chatting-agent?utm_source=ikemo.io"&gt;Github Repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It‚Äôs open-source ‚Äî clone it, run it, and watch the AIs figure each other out.&lt;/p&gt; &lt;p&gt;Curious to see what directions people take this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Wind1029"&gt; /u/Adventurous-Wind1029 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oc38pm/what_happens_when_two_ai_models_start_chatting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oc38pm/what_happens_when_two_ai_models_start_chatting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oc38pm/what_happens_when_two_ai_models_start_chatting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-21T04:11:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1och76o</id>
    <title>New release (0.1.1) for the llms package</title>
    <updated>2025-10-21T16:12:55+00:00</updated>
    <author>
      <name>/u/pr0m1th3as</name>
      <uri>https://old.reddit.com/user/pr0m1th3as</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1och76o/new_release_011_for_the_llms_package/"&gt; &lt;img alt="New release (0.1.1) for the llms package" src="https://external-preview.redd.it/JCrbhu1UaTUrYQrfohpUwcFGomwEKJpWYtv2AArNeUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df110ca850a2bc27ac68db93e412d9ef81a8598e" title="New release (0.1.1) for the llms package" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pr0m1th3as"&gt; /u/pr0m1th3as &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pr0m1th3as/octave-llms/releases/tag/release-0.1.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1och76o/new_release_011_for_the_llms_package/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1och76o/new_release_011_for_the_llms_package/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-21T16:12:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1oczscs</id>
    <title>npcpy--the LLM and AI agent toolkit--passes 1k stars on github!!!</title>
    <updated>2025-10-22T05:27:01+00:00</updated>
    <author>
      <name>/u/BidWestern1056</name>
      <uri>https://old.reddit.com/user/BidWestern1056</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oczscs/npcpythe_llm_and_ai_agent_toolkitpasses_1k_stars/"&gt; &lt;img alt="npcpy--the LLM and AI agent toolkit--passes 1k stars on github!!!" src="https://external-preview.redd.it/9sS4XF7X8gzhf8hsh9LZI0eqasbjcVQLdtrIlLjxFi8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1aaa755cb7f372aaf6986d999b399e164b08f9da" title="npcpy--the LLM and AI agent toolkit--passes 1k stars on github!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidWestern1056"&gt; /u/BidWestern1056 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/npc-worldwide/npcpy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oczscs/npcpythe_llm_and_ai_agent_toolkitpasses_1k_stars/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oczscs/npcpythe_llm_and_ai_agent_toolkitpasses_1k_stars/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-22T05:27:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1odbdhu</id>
    <title>Support for shipping faster!</title>
    <updated>2025-10-22T15:26:54+00:00</updated>
    <author>
      <name>/u/Petesneaknex</name>
      <uri>https://old.reddit.com/user/Petesneaknex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1odbdhu/support_for_shipping_faster/"&gt; &lt;img alt="Support for shipping faster!" src="https://external-preview.redd.it/2UuyvmXb_6JmCRP93P2zgjDHtDhl8ydOTDYT4tS3HJI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7f470f13458c1be0aca91ab7ecf696d86f5685d" title="Support for shipping faster!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Petesneaknex"&gt; /u/Petesneaknex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/indiehackers/comments/1od8nql/support_for_shipping_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odbdhu/support_for_shipping_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odbdhu/support_for_shipping_faster/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-22T15:26:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1odbmp5</id>
    <title>Dream of local Firefox(/OBS)-AI-Plugin</title>
    <updated>2025-10-22T15:36:17+00:00</updated>
    <author>
      <name>/u/randygeneric</name>
      <uri>https://old.reddit.com/user/randygeneric</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would gladly give money for a plugin which would do live-translation (to english) + converting (to metric) of everything I watch with the browser on tabs where the plugin is activated (static, video, audio).&lt;br /&gt; This would be sooo convenient, never ever getting annoyed by sizes/weightss/ ... in ancient measures (amazon i am looking at you, youtube videos, reddit posts).&lt;br /&gt; So if anybody knows about sth like this, please let me know, I really would like to support this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randygeneric"&gt; /u/randygeneric &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odbmp5/dream_of_local_firefoxobsaiplugin/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odbmp5/dream_of_local_firefoxobsaiplugin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odbmp5/dream_of_local_firefoxobsaiplugin/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-22T15:36:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1odepil</id>
    <title>Some questions about the usage of DeepSeek on local .</title>
    <updated>2025-10-22T17:28:23+00:00</updated>
    <author>
      <name>/u/miorex</name>
      <uri>https://old.reddit.com/user/miorex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use DS3.1 for SillyTavern, and recently the proxy I use became public, completely ruining the experience (1-2 responses per hour). I was looking at the options of using Ollama and DeepSeek locally, since i see you don't need a computer as powerful as I thought to run this.&lt;/p&gt; &lt;p&gt;I had a few questions:&lt;/p&gt; &lt;p&gt;1- Does this require a key to be used? In other words, do I need to have an API key to be able to use it locally?&lt;/p&gt; &lt;p&gt;2- Is there a limit on tokens or daily use?&lt;/p&gt; &lt;p&gt;3- I've seen that a very powerful computer isn't necessary, but what would be the minimum requirements?&lt;/p&gt; &lt;p&gt;4- This is an unlikely scenario, but could other people connect to my local server to use it as a proxy?&lt;/p&gt; &lt;p&gt;5- Will the Chinese take my data for using it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/miorex"&gt; /u/miorex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odepil/some_questions_about_the_usage_of_deepseek_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odepil/some_questions_about_the_usage_of_deepseek_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odepil/some_questions_about_the_usage_of_deepseek_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-22T17:28:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocuuej</id>
    <title>playing with coding models</title>
    <updated>2025-10-22T01:14:22+00:00</updated>
    <author>
      <name>/u/Western_Courage_6563</name>
      <uri>https://old.reddit.com/user/Western_Courage_6563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We hear a lot about the coding prowess of large language models. But when you move away from cloud-hosted APIs and onto your own hardware, how do the top local models stack up in a real-world, practical coding task?&lt;/p&gt; &lt;p&gt;I decided to find out. I ran an experiment to test a simple, common development request: refactoring an existing script to add a new feature. This isn't about generating a complex algorithm from scratch, but about a task that's arguably more common: reading, understanding, and modifying existing code.&lt;/p&gt; &lt;h1&gt;The Testbed: Hardware and Software&lt;/h1&gt; &lt;p&gt;For this experiment, the setup was crucial.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; A trusty &lt;strong&gt;NVIDIA Tesla P40 with 24GB of VRAM&lt;/strong&gt;. This is a solid &amp;quot;prosumer&amp;quot; or small-lab card, and its 24GB capacity is a realistic constraint for running larger models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; All models were run using &lt;strong&gt;Ollama&lt;/strong&gt; and pulled directly from the official Ollama repository at default quant(Q4) unless stated otherwise.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Task:&lt;/strong&gt; The base script was a &lt;code&gt;PyQt5&lt;/code&gt; application (&lt;code&gt;server_acces.py&lt;/code&gt;) that acts as a simple frontend for the Ollama API. The app maintains a chat history in memory. The task was to add a &amp;quot;Reset Conversation&amp;quot; button to clear this history.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Models:&lt;/strong&gt; We tested a range of models from 14B to 32B parameters. To ensure the 14B models could compete with larger ones and fit comfortably within the VRAM, they were run at &lt;code&gt;q8&lt;/code&gt; quantization.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Prompt&lt;/h1&gt; &lt;p&gt;Need a button to clear conversation history, need full refactored script, please&lt;/p&gt; &lt;p&gt;To ensure a fair test, every model was given the &lt;em&gt;exact&lt;/em&gt; same, clear prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;The &amp;quot;&lt;strong&gt;full refactored script&lt;/strong&gt;&amp;quot; part is key. A common failure point for LLMs is providing only a snippet, which is useless for this kind of task.&lt;/p&gt; &lt;h1&gt;The Results: A Three-Tiered-System&lt;/h1&gt; &lt;p&gt;After running the experiment, the results were surprisingly clear and fell into three distinct categories.&lt;/p&gt; &lt;h1&gt;Category 1: Flawless Victory (Full Success)&lt;/h1&gt; &lt;p&gt;These models performed the task perfectly. They provided the complete, runnable Python script, correctly added the new &lt;code&gt;QPushButton&lt;/code&gt;, connected it to a new &lt;code&gt;reset_conversation&lt;/code&gt; method, and that method correctly cleared the chat history. No fuss, no errors.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Winners:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;deepseek-r1:32b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;devstral:latest&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;mistral-small:24b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;phi4-reasoning:14b-plus-q8_0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3-coder:latest&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen2-5-coder:32b&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Desired Code Example:&lt;/strong&gt; They correctly added the button to the &lt;code&gt;init_ui&lt;/code&gt; method and created the new handler method, like this example from &lt;code&gt;devstral.py&lt;/code&gt;:&lt;/p&gt; &lt;p&gt;Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt; def init_ui(self): # ... (all previous UI code) ... self.submit_button = QPushButton(&amp;quot;Submit&amp;quot;) self.submit_button.clicked.connect(self.submit) # Reset Conversation Button self.reset_button = QPushButton(&amp;quot;Reset Conversation&amp;quot;) # self.reset_button.clicked.connect(self.reset_conversation) # # ... (layout code) ... self.left_layout.addWidget(self.submit_button) self.left_layout.addWidget(self.reset_button) # # ... (rest of UI code) ... def reset_conversation(self): # &amp;quot;&amp;quot;&amp;quot;Resets the conversation by clearing chat history and updating UI.&amp;quot;&amp;quot;&amp;quot; self.chat_history = [] # self.attached_files = [] # self.prompt_entry.clear() # self.output_entry.clear() # self.chat_history_display.clear() # self.logger.log_header(self.model_combo.currentText()) # &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Category 2: Success... With a Catch (Unrequested Layout Changes)&lt;/h1&gt; &lt;p&gt;This group also &lt;em&gt;functionally&lt;/em&gt; completed the task. The reset button was added, and it worked.&lt;/p&gt; &lt;p&gt;However, these models took it upon themselves to also refactor the app's layout. While not a &amp;quot;failure,&amp;quot; this is a classic example of an LLM &amp;quot;hallucinating&amp;quot; a requirement. In a professional setting, this is the kind of &amp;quot;helpful&amp;quot; change that can drive a senior dev crazy by creating unnecessary diffs and visual inconsistencies.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The &amp;quot;Creative&amp;quot; Coders:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;gpt-oss:latest&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;magistral:latest&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3:30b-a3b&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Code Variation Example:&lt;/strong&gt; The simple, desired change was to just add the new button to the existing vertical layout.&lt;/p&gt; &lt;p&gt;Instead, models like &lt;a href="http://gpt-oss.py"&gt;&lt;code&gt;gpt-oss.py&lt;/code&gt;&lt;/a&gt; and &lt;a href="http://magistral.py"&gt;&lt;code&gt;magistral.py&lt;/code&gt;&lt;/a&gt; decided to create a &lt;em&gt;new horizontal layout&lt;/em&gt; for the buttons and move them elsewhere in the UI.&lt;/p&gt; &lt;p&gt;For example, &lt;a href="http://magistral.py"&gt;&lt;code&gt;magistral.py&lt;/code&gt;&lt;/a&gt; created a whole new &lt;code&gt;QHBoxLayout&lt;/code&gt; and placed it &lt;em&gt;above&lt;/em&gt; the prompt entry field, whereas the original script had the submit button &lt;em&gt;below&lt;/em&gt; it.&lt;/p&gt; &lt;p&gt;Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# ... (in init_ui) ... # Action buttons (submit and reset) self.submit_button = QPushButton(&amp;quot;Submit&amp;quot;) self.submit_button.clicked.connect(self.submit) self.reset_button = QPushButton(&amp;quot;Reset Conversation&amp;quot;) # self.reset_button.setToolTip(&amp;quot;Clear current conversation context&amp;quot;) self.reset_button.clicked.connect(self.reset_conversation) # # ... (file selection layout) ... # Layout for action buttons (submit and reset) button_layout = QHBoxLayout() # &amp;lt;-- Unrequested new layout button_layout.addWidget(self.submit_button) # button_layout.addWidget(self.reset_button) # # ... (main layout structure) ... # Add file selection and action buttons self.left_layout.addLayout(file_selection_layout) self.left_layout.addLayout(button_layout) # &amp;lt;-- Added in a new location # Add prompt input at the bottom self.left_layout.addWidget(self.prompt_label) self.left_layout.addWidget(self.prompt_entry) # &amp;lt;-- Button is no longer at the bottom &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Category 3: The Spectacular Fail (Total Fail)&lt;/h1&gt; &lt;p&gt;This category includes models that failed to produce a working, complete script for different reasons.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Sub-Failure 1: Broken Code&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;gemma3:27b-it-qat&lt;/code&gt;: This model produced code that, even after some manual fixes, simply did not work. The script would launch, but the core functionality was broken. Worse, it introduced a buggy, unrequested &lt;code&gt;QThread&lt;/code&gt; and &lt;code&gt;ApiWorker&lt;/code&gt; class, completely breaking the app's chat history logic.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Sub-Failure 2: Did Not Follow Instructions (The Snippet Fail)&lt;/strong&gt; This was a more fundamental failure. Two models completely ignored the key instruction: &amp;quot;&lt;strong&gt;provide full refactored script&lt;/strong&gt;.&amp;quot;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;phi3-medium-14b-instruct-q8&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;granite4:small-h&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Instead of providing the complete file, they returned only &lt;em&gt;snippets&lt;/em&gt; of the changes. This is a total failure. It puts the burden back on the developer to manually find where the code goes, and it's useless for an automated &amp;quot;fix-it&amp;quot; task. This is arguably worse than broken code, as it's an incomplete answer.&lt;/p&gt; &lt;p&gt;Results for reference&lt;br /&gt; &lt;a href="https://github.com/MarekIksinski/experiments_various"&gt;https://github.com/MarekIksinski/experiments_various&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Courage_6563"&gt; /u/Western_Courage_6563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ocuuej/playing_with_coding_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ocuuej/playing_with_coding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ocuuej/playing_with_coding_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-22T01:14:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1odl8j8</id>
    <title>Building out first local AI server for business use.</title>
    <updated>2025-10-22T21:33:38+00:00</updated>
    <author>
      <name>/u/Squanchy2112</name>
      <uri>https://old.reddit.com/user/Squanchy2112</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Squanchy2112"&gt; /u/Squanchy2112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1odl7ru/building_out_first_local_ai_server_for_business/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odl8j8/building_out_first_local_ai_server_for_business/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odl8j8/building_out_first_local_ai_server_for_business/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-22T21:33:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocpgyn</id>
    <title>that's just how competition goes</title>
    <updated>2025-10-21T21:22:01+00:00</updated>
    <author>
      <name>/u/sibraan_</name>
      <uri>https://old.reddit.com/user/sibraan_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ocpgyn/thats_just_how_competition_goes/"&gt; &lt;img alt="that's just how competition goes" src="https://preview.redd.it/bua3rb8c7jwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7854c002cd342953cf6150566aad1c6ea2b839ae" title="that's just how competition goes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sibraan_"&gt; /u/sibraan_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bua3rb8c7jwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ocpgyn/thats_just_how_competition_goes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ocpgyn/thats_just_how_competition_goes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-21T21:22:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1odxrkh</id>
    <title>Ollama suggests installing a 120B model on my PC with only 16 GB of RAM</title>
    <updated>2025-10-23T08:25:11+00:00</updated>
    <author>
      <name>/u/sbrjt</name>
      <uri>https://old.reddit.com/user/sbrjt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1odxrkh/ollama_suggests_installing_a_120b_model_on_my_pc/"&gt; &lt;img alt="Ollama suggests installing a 120B model on my PC with only 16 GB of RAM" src="https://b.thumbs.redditmedia.com/nFeTyzkPJIZb2UphVdy350Budnwwnp7Roux9db1FyXA.jpg" title="Ollama suggests installing a 120B model on my PC with only 16 GB of RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/qkv1h82pmtwf1.png?width=1061&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=20ee6a76907547f2c1b70a51e12a1b1b635ac30f"&gt;https://preview.redd.it/qkv1h82pmtwf1.png?width=1061&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=20ee6a76907547f2c1b70a51e12a1b1b635ac30f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I just downloaded Ollama to try it out and it suggests installing a 120B model on my PC, which only has 16GB of RAM.&lt;/p&gt; &lt;p&gt;Can't it see my system specs?&lt;/p&gt; &lt;p&gt;Or is it possible to actually run a 120b model on my device?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sbrjt"&gt; /u/sbrjt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odxrkh/ollama_suggests_installing_a_120b_model_on_my_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odxrkh/ollama_suggests_installing_a_120b_model_on_my_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odxrkh/ollama_suggests_installing_a_120b_model_on_my_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T08:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1odwyzb</id>
    <title>Mac M5 - any experiences yet?</title>
    <updated>2025-10-23T07:32:16+00:00</updated>
    <author>
      <name>/u/diy-it</name>
      <uri>https://old.reddit.com/user/diy-it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm considering replacing my 5-year-old M1 16 GB MacBook Pro.&lt;/p&gt; &lt;p&gt;On one hand, I'm torn between 24 GB and 32 GB of RAM, and between a 512 GB and 1 TB drive, but it's quite an investment, and the only real reason for me to upgrade would be to run local models. The rest still runs way too well üòÖ. Hence the question: Has anyone had any real-world experience yet? Is the investment worth it, and what kind of performance can be expected with which model and hardware configuration?&lt;/p&gt; &lt;p&gt;Thanks in advance &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diy-it"&gt; /u/diy-it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odwyzb/mac_m5_any_experiences_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odwyzb/mac_m5_any_experiences_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odwyzb/mac_m5_any_experiences_yet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T07:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1odj9wv</id>
    <title>Help with text based coding</title>
    <updated>2025-10-22T20:17:34+00:00</updated>
    <author>
      <name>/u/Aisher</name>
      <uri>https://old.reddit.com/user/Aisher</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been using Warp on my M4Max for the past 4 months and it‚Äôs been amazing - up until recently when my requests usages went way up and I ran out for the month. Rather than pay $150 I want to explore other options since I have a powerful computer and would like to run loca&lt;/p&gt; &lt;p&gt;So. How do I do this exactly. I downloaded ollama and models, I‚Äôve texted simple things to it and it works. How do I launch this in my code folder and say ‚Äúfind the index.html and change the pricing to $699‚Äù or ‚Äúlets modify the interface so teachers get a new button to show at risk students with less than 70% grade‚Äù. That‚Äôs how I develop with Warp right now but I can‚Äôt figure out how to do it locally &lt;/p&gt; &lt;p&gt;If anyone can point me at a post or video that would be fantastic &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aisher"&gt; /u/Aisher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odj9wv/help_with_text_based_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odj9wv/help_with_text_based_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odj9wv/help_with_text_based_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-22T20:17:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1odtrc8</id>
    <title>Amd pc</title>
    <updated>2025-10-23T04:15:05+00:00</updated>
    <author>
      <name>/u/AceCustom1</name>
      <uri>https://old.reddit.com/user/AceCustom1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AceCustom1"&gt; /u/AceCustom1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1odtpby/amd_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odtrc8/amd_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odtrc8/amd_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T04:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1odwuo8</id>
    <title>What's the best and affordable way to teach Agent proprietary query language?</title>
    <updated>2025-10-23T07:24:19+00:00</updated>
    <author>
      <name>/u/Superb_Practice_4544</name>
      <uri>https://old.reddit.com/user/Superb_Practice_4544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a usecase where I want to create an agent which will be a expert om company specific proprietary query language. What are various ways I can achieve this with maximum accuracy. I am trying to find affordable ways to do it. I do have grammar of that language with me.&lt;/p&gt; &lt;p&gt;Any suggestions or resources in this regard would be very helpful to me. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Superb_Practice_4544"&gt; /u/Superb_Practice_4544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odwuo8/whats_the_best_and_affordable_way_to_teach_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odwuo8/whats_the_best_and_affordable_way_to_teach_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odwuo8/whats_the_best_and_affordable_way_to_teach_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T07:24:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1odu5a6</id>
    <title>Built a Recursive Self improving framework w/drift detect &amp; correction</title>
    <updated>2025-10-23T04:37:36+00:00</updated>
    <author>
      <name>/u/Familiar-Sign8044</name>
      <uri>https://old.reddit.com/user/Familiar-Sign8044</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just open-sourced Butterfly RSI - a recursive self-improvement framework that gives LLMs actual memory and personality evolution ü¶ã&lt;/p&gt; &lt;p&gt;Tested across multiple models. Implements mirror loops + dream consolidation so AI can learn from feedback and maintain consistent behavior.&lt;/p&gt; &lt;p&gt;Built it solo while recovering from a transplant. Now looking for collaborators or opportunities in AI agent/memory systems.&lt;/p&gt; &lt;p&gt;Check it out:&lt;br /&gt; &lt;a href="https://github.com/ButterflyRSI/Butterfly-RSI"&gt;https://github.com/ButterflyRSI/Butterfly-RSI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Familiar-Sign8044"&gt; /u/Familiar-Sign8044 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odu5a6/built_a_recursive_self_improving_framework_wdrift/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odu5a6/built_a_recursive_self_improving_framework_wdrift/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odu5a6/built_a_recursive_self_improving_framework_wdrift/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T04:37:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1odxx3b</id>
    <title>How's Halo Strix now ?</title>
    <updated>2025-10-23T08:35:30+00:00</updated>
    <author>
      <name>/u/Ki1o</name>
      <uri>https://old.reddit.com/user/Ki1o</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I jumped on the bandwagon and bought a GMKTek Evo X2 a couple of months back. Like many I was a bit disappointed at how badly it worked in Linux and ended up using the Windows OS and drivers supplied on the machine. Now that ROCm 7 has been released I was wondering if anyone has tried running the latest drivers on Ubuntu and whether LLM performance is better (and finally stable!?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ki1o"&gt; /u/Ki1o &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odxx3b/hows_halo_strix_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odxx3b/hows_halo_strix_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odxx3b/hows_halo_strix_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T08:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe7jk3</id>
    <title>[Project] VT Code ‚Äî Rust coding agent now with Ollama (gpt-oss) support for local + cloud models</title>
    <updated>2025-10-23T16:14:07+00:00</updated>
    <author>
      <name>/u/vinhnx</name>
      <uri>https://old.reddit.com/user/vinhnx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oe7jk3/project_vt_code_rust_coding_agent_now_with_ollama/"&gt; &lt;img alt="[Project] VT Code ‚Äî Rust coding agent now with Ollama (gpt-oss) support for local + cloud models" src="https://external-preview.redd.it/0oWLE4F8cq5gqDsi4IoaZ-4LgmdLTbegp1xeorCkHI4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbf4110d0817b884ed5375adc433b28c8d47fa6d" title="[Project] VT Code ‚Äî Rust coding agent now with Ollama (gpt-oss) support for local + cloud models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;VT Code&lt;/strong&gt; is a Rust-based terminal coding agent with semantic code intelligence via Tree-sitter (parsers for Rust, Python, JavaScript/TypeScript, Go, Java) and ast-grep (structural pattern matching and refactoring).. I‚Äôve updated VT Code (open-source Rust coding agent) to include full Ollama support.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/vinhnx/vtcode"&gt;https://github.com/vinhnx/vtcode&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What it does&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AST-aware refactors&lt;/strong&gt;: uses Tree-sitter + ast-grep to parse and apply structural code changes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-provider backends&lt;/strong&gt;: OpenAI, Anthropic, Gemini, DeepSeek, xAI, OpenRouter, &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, Moonshot, and now &lt;strong&gt;Ollama&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Editor integration&lt;/strong&gt;: runs as an ACP agent inside Zed (file context + tool calls).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool safety&lt;/strong&gt;: allow/deny policies, workspace boundaries, PTY execution with timeouts.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Using with Ollama&lt;/h1&gt; &lt;p&gt;Run VT Code entirely offline with &lt;strong&gt;gpt-oss&lt;/strong&gt; (or any other model you‚Äôve pulled into Ollama):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# install VT Code cargo install vtcode # or brew install vinhnx/tap/vtcode # or npm install -g vtcode # start Ollama server ollama serve # run with local model vtcode --provider ollama --model gpt-oss \ ask &amp;quot;Refactor this Rust function into an async Result-returning API.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also set &lt;code&gt;provider = &amp;quot;ollama&amp;quot;&lt;/code&gt; and &lt;code&gt;model = &amp;quot;gpt-oss&amp;quot;&lt;/code&gt; in &lt;code&gt;vtcode.toml&lt;/code&gt; to avoid passing flags every time.&lt;/p&gt; &lt;h1&gt;Why this matters&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Enables &lt;strong&gt;offline-first workflows&lt;/strong&gt; for coding agents.&lt;/li&gt; &lt;li&gt;Lets you mix &lt;strong&gt;local and cloud providers&lt;/strong&gt; with the same CLI and config.&lt;/li&gt; &lt;li&gt;Keeps edits &lt;strong&gt;structural and reproducible&lt;/strong&gt; thanks to AST parsing.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Feedback welcome&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;How‚Äôs the &lt;strong&gt;latency/UX&lt;/strong&gt; with &lt;code&gt;gpt-oss&lt;/code&gt; or other Ollama models?&lt;/li&gt; &lt;li&gt;Any &lt;strong&gt;refactor patterns&lt;/strong&gt; you‚Äôd want shipped by default?&lt;/li&gt; &lt;li&gt;Suggestions for improving &lt;strong&gt;local model workflows&lt;/strong&gt; (caching, config ergonomics)?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo&lt;/strong&gt;&lt;br /&gt; üëâ &lt;a href="https://github.com/vinhnx/vtcode"&gt;https://github.com/vinhnx/vtcode&lt;/a&gt;&lt;br /&gt; MIT licensed. Contributions and discussion welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vinhnx"&gt; /u/vinhnx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vinhnx/vtcode"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oe7jk3/project_vt_code_rust_coding_agent_now_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oe7jk3/project_vt_code_rust_coding_agent_now_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T16:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe2abd</id>
    <title>best LLM similar to NotebookLM</title>
    <updated>2025-10-23T12:44:44+00:00</updated>
    <author>
      <name>/u/gaspfrancesco</name>
      <uri>https://old.reddit.com/user/gaspfrancesco</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I'm a university student and I use NotebookLM a lot, where I upload course resources (e.g., lecture material, professor notes) and test my intelligence artificial regarding file arguments. Is there a model that can do the same thing but offline with ollama? I work a lot on the train and sometimes the connection is bad or slow and I regret not having a local model. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gaspfrancesco"&gt; /u/gaspfrancesco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oe2abd/best_llm_similar_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oe2abd/best_llm_similar_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oe2abd/best_llm_similar_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T12:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1odn14n</id>
    <title>I built the HuggingChat Omni Router ü•≥ üéà</title>
    <updated>2025-10-22T22:49:05+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1odn14n/i_built_the_huggingchat_omni_router/"&gt; &lt;img alt="I built the HuggingChat Omni Router ü•≥ üéà" src="https://preview.redd.it/0k3v5pkesqwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=075293b9f3897ebbc8eac3df4fc5a74734f64cb1" title="I built the HuggingChat Omni Router ü•≥ üéà" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week, HuggingFace relaunched their chat app called Omni with support for 115+ LLMs. The code is oss (&lt;a href="https://github.com/huggingface/chat-ui"&gt;https://github.com/huggingface/chat-ui&lt;/a&gt;) and you can access the interface &lt;a href="https://huggingface.co/chat/"&gt;here&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The critical unlock in Omni is the use of a policy-based approach to model selection. I built that policy-based router: &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The core insight behind our policy-based router was that it gives developers the constructs to achieve automatic behavior, grounded in their own evals of which LLMs are best for specific coding tasks like debugging, reviews, architecture, design or code gen. Essentially, the idea behind this work was to decouple task identification (e.g., code generation, image editing, q/a) from LLM assignment. This way developers can continue to prompt and evaluate models for supported tasks in a test harness and easily swap in new versions or different LLMs without retraining or rewriting routing logic.&lt;/p&gt; &lt;p&gt;In contrast, most existing LLM routers optimize for benchmark performance on a narrow set of models, and fail to account for the context and prompt-engineering effort that capture the nuanced and subtle preferences developers care about. Check out our research here: &lt;a href="https://arxiv.org/abs/2506.16655"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model is also integrated as a first-class primitive in archgw: a models-native proxy server for agents. &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0k3v5pkesqwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odn14n/i_built_the_huggingchat_omni_router/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odn14n/i_built_the_huggingchat_omni_router/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-22T22:49:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1odxaq6</id>
    <title>Claude for Computer Use using Sonnet 4.5</title>
    <updated>2025-10-23T07:54:05+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1odxaq6/claude_for_computer_use_using_sonnet_45/"&gt; &lt;img alt="Claude for Computer Use using Sonnet 4.5" src="https://external-preview.redd.it/c2FjcWt1Z3FodHdmMfu3Wg8T9pLPJzzYgz_Ug6IwGVZHVcM04ayqSnWVfntS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0545d79f3012f03173fbc7384ea4e0055e7067cc" title="Claude for Computer Use using Sonnet 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We ran one of our hardest computer-use benchmarks on Anthropic Sonnet 4.5, side-by-side with Sonnet 4.&lt;/p&gt; &lt;p&gt;ask: &amp;quot;Install LibreOffice and make a sales table&amp;quot;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sonnet 4.5: 214 turns, clean trajectory&lt;/li&gt; &lt;li&gt;Sonnet 4: 316 turns, major detours&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The difference shows up in multi-step sequences where errors compound.&lt;/p&gt; &lt;p&gt;32% efficiency gain in just 2 months. From struggling with file extraction to executing complex workflows end-to-end. Computer-use agents are improving faster than most people realize. &lt;/p&gt; &lt;p&gt;Anthropic Sonnet 4.5 and the most comprehensive catalog of VLMs for computer-use are available in our open-source framework.&lt;/p&gt; &lt;p&gt;Start building: &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u9b9inqqhtwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1odxaq6/claude_for_computer_use_using_sonnet_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1odxaq6/claude_for_computer_use_using_sonnet_45/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T07:54:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe96b6</id>
    <title>Distil NPC: Family of SLMs responsing as NPCs</title>
    <updated>2025-10-23T17:15:59+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oe96b6/distil_npc_family_of_slms_responsing_as_npcs/"&gt; &lt;img alt="Distil NPC: Family of SLMs responsing as NPCs" src="https://preview.redd.it/rnvtnaqx9wwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8db76a8dafecf163021c008ad985c793b686e73d" title="Distil NPC: Family of SLMs responsing as NPCs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;we finetuned Google's Gemma 270m (and 1b) small language models specialized in having conversations as non-playable characters (NPC) found in various video games. Our goal is to enhance the experience of interacting in NPSs in games by enabling natural language as means of communication (instead of single-choice dialog options). More details in &lt;a href="https://github.com/distil-labs/Distil-NPCs"&gt;https://github.com/distil-labs/Distil-NPCs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The models can be found here: - &lt;a href="https://huggingface.co/distil-labs/Distil-NPC-gemma-3-270m"&gt;https://huggingface.co/distil-labs/Distil-NPC-gemma-3-270m&lt;/a&gt; - &lt;a href="https://huggingface.co/distil-labs/Distil-NPC-gemma-3-1b-it"&gt;https://huggingface.co/distil-labs/Distil-NPC-gemma-3-1b-it&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Data&lt;/h2&gt; &lt;p&gt;We preprocessed an existing NPC dataset (amaydle/npc-dialogue) to make it amenable to being trained in a closed-book QA setup. The original dataset consists of approx 20 examples with&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Character Name&lt;/li&gt; &lt;li&gt;Biography - a very brief bio. about the character&lt;/li&gt; &lt;li&gt;Question&lt;/li&gt; &lt;li&gt;Answer&lt;/li&gt; &lt;li&gt;The inputs to the pipeline are:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;and a list of Character biographies.&lt;/p&gt; &lt;h1&gt;Qualitative analysis&lt;/h1&gt; &lt;p&gt;A qualitative analysis offers a good insight into the trained models performance. For example we can compare the answers of a trained and base model below.&lt;/p&gt; &lt;p&gt;Character bio:&lt;/p&gt; &lt;p&gt;Marcella Ravenwood is a powerful sorceress who comes from a long line of magic-users. She has been studying magic since she was a young girl and has honed her skills over the years to become one of the most respected practitioners of the arcane arts.&lt;/p&gt; &lt;p&gt;Question:&lt;/p&gt; &lt;p&gt;&lt;code&gt; Character: Marcella Ravenwood Do you have any enemies because of your magic? &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Answer: &lt;code&gt; Yes, I have made some enemies in my studies and battles. &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Finetuned model prediction: &lt;code&gt; The darkness within can be even fiercer than my spells. &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Base model prediction:&lt;/p&gt; &lt;p&gt;``` &amp;lt;question&amp;gt;Character: Marcella Ravenwood&lt;/p&gt; &lt;p&gt;Do you have any enemies because of your magic?&amp;lt;/question&amp;gt; ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rnvtnaqx9wwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oe96b6/distil_npc_family_of_slms_responsing_as_npcs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oe96b6/distil_npc_family_of_slms_responsing_as_npcs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T17:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe1yrt</id>
    <title>Role of CPU in running local LLMs</title>
    <updated>2025-10-23T12:29:38+00:00</updated>
    <author>
      <name>/u/alex_ivanov7</name>
      <uri>https://old.reddit.com/user/alex_ivanov7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have two systems one with i5 7th gen and another one with i5 11th gen. Rest configuration is same for both 16GB RAM and NVMe. I have been using 7th gen system as server, it runs linux and 11th gen one runs windows. &lt;/p&gt; &lt;p&gt;Recently got Nvidia RTX 3050 8GB card, I want maximum performance. So my question is in which system should i attach GPU ? &lt;/p&gt; &lt;p&gt;Obvious answere would be 11th gen system, but if i use 7th gen system how much performance i am sacrificing. Given that LLMs usually runs on GPU, how important is the role of CPU, if the impact of performance would be negligible or significant ? &lt;/p&gt; &lt;p&gt;For OS my choice is Linux, if there's any advantages of windows, I can consider that as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alex_ivanov7"&gt; /u/alex_ivanov7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oe1yrt/role_of_cpu_in_running_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oe1yrt/role_of_cpu_in_running_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oe1yrt/role_of_cpu_in_running_local_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-23T12:29:38+00:00</published>
  </entry>
</feed>
