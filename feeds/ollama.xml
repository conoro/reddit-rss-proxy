<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-19T16:08:53+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1oxtmn9</id>
    <title>Some questions for a new comer</title>
    <updated>2025-11-15T14:52:38+00:00</updated>
    <author>
      <name>/u/R0B0t1C_Cucumber</name>
      <uri>https://old.reddit.com/user/R0B0t1C_Cucumber</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Folks,&lt;br /&gt; I'm new to the whole AI scene but not newbie in any means when it comes to technology (spent the last 15 years building data centers and acting as devops for support folks).&lt;/p&gt; &lt;p&gt;Over the last few weeks i've played with&lt;br /&gt; - github co-pilot pro and I loved what it could produce, although it felt a bit clunky when it leveraged certain things like simplebrowser in vscode which caused some issues with corruption in the files it was writing writing to. But over all the experience was good.&lt;/p&gt; &lt;p&gt;- Claude CLI this was awesome. It did exactly what I wanted it to do right from my terminal.&lt;/p&gt; &lt;p&gt;Which got me starting to play with Ollama and other tools that could leverage it to make a similar experience. It's not lost on me that I'm bottlenecked by hardware constraints when it comes to local models, and I intend to keep my subscriptions for the other services but the linux hobbiest in me wants to get one running locally just to tinker with and try different models.&lt;/p&gt; &lt;p&gt;The compute setup:&lt;/p&gt; &lt;p&gt;CPU: i9 14900KF&lt;br /&gt; GPU: 4070Ti 12GB&lt;br /&gt; Memory: 64GB&lt;br /&gt; Disk: 2tb nvme&lt;br /&gt; OS: Ubuntu 24.04&lt;/p&gt; &lt;p&gt;So, what is the communities recommendations for a clean setup using ollama to act either like github copilot pro or claude CLI? The use case is code generation and it should be able to do pretty much everything on it's own. The way i'm using AI right now a prompt would look like this: &amp;quot;create a test application in directory /x/y/z using react and serve it on port 8080, send me the link when it's functional for me to test&amp;quot;&lt;/p&gt; &lt;p&gt;I've tried Cline and &lt;a href="http://continue.dev"&gt;continue.dev&lt;/a&gt; plugins in vscode and nanocoder for CLI , all were pretty cool but felt clunky leading me to make this post. I must be pulling the wrong llm's , setting the wrong context lengths, or maybe i'm entirely missing something. Sorry for the long rambling post. Any help pointing me to the next rabbit hole is much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R0B0t1C_Cucumber"&gt; /u/R0B0t1C_Cucumber &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxtmn9/some_questions_for_a_new_comer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxtmn9/some_questions_for_a_new_comer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oxtmn9/some_questions_for_a_new_comer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-15T14:52:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oycim4</id>
    <title>Ollama and LLama3 to write files to a directory</title>
    <updated>2025-11-16T04:35:13+00:00</updated>
    <author>
      <name>/u/InformationPuzzled44</name>
      <uri>https://old.reddit.com/user/InformationPuzzled44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, Im a complete ollama noob. Ive been using Claude Code to write documents to a Ubuntu Server. So i tried this with installed Ollama and using Llama3 and openai-web but when i ask llama3 to write a file, it says its cloud based and can't. So im confused am i doing something wrong, i was under impression that running ollama locally means that i could have AI write files and code etc, locally. &lt;/p&gt; &lt;p&gt;Thanks for any assistance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InformationPuzzled44"&gt; /u/InformationPuzzled44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oycim4/ollama_and_llama3_to_write_files_to_a_directory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oycim4/ollama_and_llama3_to_write_files_to_a_directory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oycim4/ollama_and_llama3_to_write_files_to_a_directory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-16T04:35:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyi7fv</id>
    <title>Working on a Local LLM Device</title>
    <updated>2025-11-16T10:16:18+00:00</updated>
    <author>
      <name>/u/Lonely-Marzipan-9473</name>
      <uri>https://old.reddit.com/user/Lonely-Marzipan-9473</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lonely-Marzipan-9473"&gt; /u/Lonely-Marzipan-9473 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1oyi6xv/working_on_a_local_llm_device/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oyi7fv/working_on_a_local_llm_device/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oyi7fv/working_on_a_local_llm_device/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-16T10:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1oye74t</id>
    <title>Claude randomly added Chinese text. Anyone else seen this?</title>
    <updated>2025-11-16T06:09:03+00:00</updated>
    <author>
      <name>/u/Separate_Bonus_2234</name>
      <uri>https://old.reddit.com/user/Separate_Bonus_2234</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oye74t/claude_randomly_added_chinese_text_anyone_else/"&gt; &lt;img alt="Claude randomly added Chinese text. Anyone else seen this?" src="https://b.thumbs.redditmedia.com/hSCYpdmrFNdqVoJrKjNH5GmSduN_hbBsWPQYa962FMY.jpg" title="Claude randomly added Chinese text. Anyone else seen this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude suddenly inserted a Chinese character in an English sentence:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z65k01nw8k1g1.png?width=988&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05108772771c1b7ec6b7ac81675ac3f153a0e7cf"&gt;https://preview.redd.it/z65k01nw8k1g1.png?width=988&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05108772771c1b7ec6b7ac81675ac3f153a0e7cf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Separate_Bonus_2234"&gt; /u/Separate_Bonus_2234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oye74t/claude_randomly_added_chinese_text_anyone_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oye74t/claude_randomly_added_chinese_text_anyone_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oye74t/claude_randomly_added_chinese_text_anyone_else/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-16T06:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxw4ir</id>
    <title>Ryzen AI MAX+ 395 - LLM metrics</title>
    <updated>2025-11-15T16:33:39+00:00</updated>
    <author>
      <name>/u/Armageddon_80</name>
      <uri>https://old.reddit.com/user/Armageddon_80</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MACHINE:&lt;/strong&gt; AMD Ryzen AI MAX+ 395 &amp;quot;Strix Halo&amp;quot; (Radeon 8060s) 128GB Ram &lt;/p&gt; &lt;p&gt;&lt;strong&gt;OS:&lt;/strong&gt; &lt;strong&gt;Windows 11 pro&lt;/strong&gt; 25H2 build 26200.7171 (15/11/25)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;INFERENCE ENGINES:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lemonade V9.0.2&lt;/li&gt; &lt;li&gt;LMstudio 0.3.31 (build7)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TLDR;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm gonna start saying that i thought I was tech savvy, until i tried to setup this pc with Linux... I felt like my GF when i try to explain her about AI...&lt;/p&gt; &lt;p&gt;If you want to be up and running in no time, stick with Window, download AMD Adrenaline and let it install all drivers needed. That's it, your system is set up.&lt;br /&gt; Then install whatever inference engine and models you want to run. &lt;/p&gt; &lt;p&gt;I would reccomend Lemonade (supported by AMD) but the python API is the generic OpenAI style while LMstudio Python API is more friendly. Up to you.&lt;/p&gt; &lt;p&gt;Here i attached results from different models to give an idea:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LMstudio Metrics:&lt;/strong&gt; &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Rocm engine&lt;/th&gt; &lt;th align="left"&gt;Vulkan engine&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenAI gpt-oss-20b MXFP4 (RAM 11.7gb)&lt;/td&gt; &lt;td align="left"&gt;66 TPS (0.05sec TTFT)&lt;/td&gt; &lt;td align="left"&gt;65 TPS (0.1 TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-30b-a3b-2507 GGUF Q4_K_M (RAM 17.64gb)&lt;/td&gt; &lt;td align="left"&gt;66 TPS (0.06sec TTFT)&lt;/td&gt; &lt;td align="left"&gt;78 TPS (0.1 TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 3 12b GGUF Q4_K_M (RAM 7.19GB)&lt;/td&gt; &lt;td align="left"&gt;23 TPS (0.07 sec TTFT)&lt;/td&gt; &lt;td align="left"&gt;26 TPS (0.1 TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Granite -4-h-small 32B GGUF Q4_K_M (RAM 19.3GB)&lt;/td&gt; &lt;td align="left"&gt;28 TPS (0.1 sec TTFT)&lt;/td&gt; &lt;td align="left"&gt;30 TPS (0.2 TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Granite -4-h-Tiny 7B GGUF Q4_K_M (RAM 4.2 GB)&lt;/td&gt; &lt;td align="left"&gt;97 TPS (0.06 TTFT)&lt;/td&gt; &lt;td align="left"&gt;97 TPS (0.07 TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Vl-4b GGUF Q4_K_M (RAM2.71 GB)&lt;/td&gt; &lt;td align="left"&gt;57 TPS (0.05sec TTFT)&lt;/td&gt; &lt;td align="left"&gt;65 TPS (0.05 TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Lemonade Metrics:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Running on&lt;/th&gt; &lt;th align="left"&gt;Token Per Second&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;LLama-3.2-1B-FLM&lt;/td&gt; &lt;td align="left"&gt;NPU&lt;/td&gt; &lt;td align="left"&gt;42 TPS (0.4sec TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-4B-Instruct-2507-FLM&lt;/td&gt; &lt;td align="left"&gt;NPU&lt;/td&gt; &lt;td align="left"&gt;14.5 TPS (0.9sec TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-4b-Instruct-2507-GGUF&lt;/td&gt; &lt;td align="left"&gt;GPU&lt;/td&gt; &lt;td align="left"&gt;72 TPS (0.04sec TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-30B-A3B-instruct GGUF&lt;/td&gt; &lt;td align="left"&gt;GPU&lt;/td&gt; &lt;td align="left"&gt;74 TPS (0.1sec TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen-2.5-7B-Instruct-Hybrid&lt;/td&gt; &lt;td align="left"&gt;NPU+GPU&lt;/td&gt; &lt;td align="left"&gt;39 TPS (0.6sec TTFT)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;LMstudio (No NPU) is faster with Vulkan llama.cpp engine rather than Rocm llama.cpp engine (bad bad AMD). &lt;/li&gt; &lt;li&gt;Lemonade when using GGUF model perform the same as LMS with Vulkan.&lt;/li&gt; &lt;li&gt;Lemonade offers also &lt;em&gt;NPU only mode&lt;/em&gt; (very power efficient but at 20% of GPU speed) perfect for overnight activities, and &lt;em&gt;Hybrid mode (NPU+GPU)&lt;/em&gt; useful for large context/complex prompts. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ryzen AI MAX+ APU really shines with MOE models, by leveraging the capability to load any size of model while balancing the memory bandwith's &amp;quot;limit&amp;quot; with activation of smaller experts (3B experts @ 70 TPS).&lt;br /&gt; A nice surprise is the new Granite 4 hybrid model series (mamba-2 architecture) where the 7B tiny runs at almost 100TPS and the 32B &lt;a href="mailto:small@28TPS"&gt;small@28TPS&lt;/a&gt;.&lt;br /&gt; With dense models TPS slows down proportionally to size, on different scales depending on model but generally 12B@23TPS , 7B@40TPS, 4B@&amp;gt;70TPS.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;END OF TLDR.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lemonade V9.0.2&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Lemonade Server is a server interface that uses the standard Open AI API allowing applications to integrate with local LLMs that run on your own PC's NPU and GPU.&lt;/p&gt; &lt;p&gt;So far is the only program that can easily switch between:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;1) only GPU:&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;uses the classic&lt;/em&gt; &amp;quot;GGUF&amp;quot; models that runs on iGPU/GPU. On my hardware the model runs on the Radeon 8060s. It can run basically anything, since i can allocate as much RAM i want for the gpu)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;2) GPU + NPU:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;uses niche &amp;quot;OGA&amp;quot; models (ONNXRuntime GenAI).&lt;br /&gt; This is an Hybrid mode that split the inference in 2 steps:&lt;/p&gt; &lt;p&gt;- 1st step uses NPU for the prefill phase (prompt and context ingestion) improving TTFT (time to first token)&lt;/p&gt; &lt;p&gt;- 2nd step uses GPU to handle the decode phase (generation), where high memory bandwidth is critical improving TPS (Tokens Per Second)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;3) only NPU:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Uses &amp;quot;OGA&amp;quot; models or &amp;quot;FLM&amp;quot; models (FastFlowLM).&lt;br /&gt; All inference is executed by the NPU. It's slower than GPU (TPS), but is extremely power efficient compared to GPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LMstudio 0.3.31 (build7)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;LMstudio doesnt need any presentation. Without going too exotic, you can run only GGUF models(GPU). Ollama can also be used with no problem at cost of some performance losses. The big advantage of LMstudio compared to Ollama is that LMS allows you to choose the Runtime to use for inference, improving TPS (speed). We have 2 options:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;1) Rocm llama.cpp v1.56.0&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Rocm is a software stack developed by AMD for GPU-accelerated high-performance computing (HPC). Like CUDA for Nvidia. So this is a llama.cpp version optimized for AMD gpus.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;2) Vulkan llama.cpp v.156.0&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Vulkan is a cross-platform and open standard for 3D graphics and computing API that optimizes performances for GPU workloads. So this is a llama.cpp version optimized for gpus in general via Vulkan.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Whatever option you choose, remember the engine only apply to GGUF files (basically dont apply to OpenAI GTP-oss MXPF4)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results with LMstudio&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;(see table above)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Well, clearly Vulkan Engine is equal or faster than Rocm engine.&lt;/p&gt; &lt;p&gt;Honestly it's difficult to see any difference in this kind of chit-chat with the LLM, but difference could become noticeable if your are processing batch of documents or in any multistep agent pipeline, where time is adding up at every step.&lt;/p&gt; &lt;p&gt;It's funny how Rocm from AMD (the manufacturer of my Halo Strix) is neither faster or energy efficient compared to the more generic Vulkan. The good thing is that while AMD keep improving drivers and software, eventually the situation will flip and we can expect even faster performances. Nonetheless, I'm not complaining about current performances at all :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results with Lemonade&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;(see table above)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've downloaded other models (I know i know) but models are massive and with these kind of machines, the bottleneck is the internet speed connection (and my patience). Also notice that Lemonade doesnt provide as many models as LMstudio.&lt;/p&gt; &lt;p&gt;Also notice that AMD Adrenaline doesnt return any metrics about the NPU. &lt;em&gt;Only think i can say, is that during inference with NPU the cooling fan dont even start, no matter how many tokens are generated. Meaning the power used must be really, really small.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Personal thoughts&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The advantage of having an Hybrid model is only in the prefilling part of the inference, Windows shows clearly a burst (short and high peak) on the NPU usage at the beginning of inference, the rest of generations is off loaded to the GPU as any GGUF model.&lt;/p&gt; &lt;p&gt;Completely different story with only NPU models, that's perfect for overnight works, where speed is not necessary but energy efficiency is, ie: on battery powered devices.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If electric power is not a constrain (at home/office use), then the power usage of NPU needs to be measured before to claim the miracle:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;the NPU speed is 20% compared to GPU meaning it will take X5 more time&lt;/strong&gt; &lt;strong&gt;to do the same job of the GPU.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;thus NPU power usage must be at least 5 times less than GPU otherwise it doesn't really make sense at home. Again different story is for battery powered devices.&lt;/p&gt; &lt;p&gt;In my observations GPU runs around 110W at full inference, so NPU should consume less than 20W which is possible since fan never started.&lt;br /&gt; NPU are very promising, but power consumption should be measured.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;I hope this was helpful (after 4 hours of tests and writing!) and can clarify wether this Ryzen AI max is adapt to you.&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;It is definitely for me, it runs everything you throw at it; with this beast I even replaced my Xbox series X to play BF6.&lt;/em&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Armageddon_80"&gt; /u/Armageddon_80 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxw4ir/ryzen_ai_max_395_llm_metrics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxw4ir/ryzen_ai_max_395_llm_metrics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oxw4ir/ryzen_ai_max_395_llm_metrics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-15T16:33:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyku8j</id>
    <title>Integrate AI agent with Zed or Vscode</title>
    <updated>2025-11-16T12:45:20+00:00</updated>
    <author>
      <name>/u/AirportAcceptable522</name>
      <uri>https://old.reddit.com/user/AirportAcceptable522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Cursor pro, but I wanted some way to use an ollama model in any editor like vscode or zed. Which model would be suitable and how to integrate?&lt;/p&gt; &lt;p&gt;I wanted to do the same thing I do with the cursor agent, but with a more specific and isolated model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AirportAcceptable522"&gt; /u/AirportAcceptable522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oyku8j/integrate_ai_agent_with_zed_or_vscode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oyku8j/integrate_ai_agent_with_zed_or_vscode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oyku8j/integrate_ai_agent_with_zed_or_vscode/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-16T12:45:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz7cgs</id>
    <title>TikTok ¬∑ Rawdog Reverend</title>
    <updated>2025-11-17T04:48:04+00:00</updated>
    <author>
      <name>/u/New_Pomegranate_1060</name>
      <uri>https://old.reddit.com/user/New_Pomegranate_1060</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oz7cgs/tiktok_rawdog_reverend/"&gt; &lt;img alt="TikTok ¬∑ Rawdog Reverend" src="https://external-preview.redd.it/p5ngHChl-SkIYYghfi1mS5ltuLNLSd7YMvD1F27R5fk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c57e024bb7247cb99111f24125a2866c7c66ac39" title="TikTok ¬∑ Rawdog Reverend" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Pomegranate_1060"&gt; /u/New_Pomegranate_1060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tiktok.com/t/ZTMKg2AtW/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oz7cgs/tiktok_rawdog_reverend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oz7cgs/tiktok_rawdog_reverend/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-17T04:48:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozkeey</id>
    <title>üî• Perplexity AI PRO - 1-Year Plan - Limited Time SUPER PROMO! 90% OFF!</title>
    <updated>2025-11-17T16:13:32+00:00</updated>
    <author>
      <name>/u/Verza-</name>
      <uri>https://old.reddit.com/user/Verza-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ozkeey/perplexity_ai_pro_1year_plan_limited_time_super/"&gt; &lt;img alt="üî• Perplexity AI PRO - 1-Year Plan - Limited Time SUPER PROMO! 90% OFF!" src="https://preview.redd.it/sen0jy3pdu1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=420c632da047f12bceae539295d4422e6d29c1f9" title="üî• Perplexity AI PRO - 1-Year Plan - Limited Time SUPER PROMO! 90% OFF!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get Perplexity AI PRO (1-Year) ‚Äì at 90% OFF!&lt;/p&gt; &lt;p&gt;Order here: &lt;a href="https://cheapgpts.store/Perplexity"&gt;CHEAPGPT.STORE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Plan: 12 Months&lt;/p&gt; &lt;p&gt;üí≥ Pay with: PayPal or Revolut&lt;/p&gt; &lt;p&gt;Reddit reviews: &lt;a href="https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu"&gt;FEEDBACK POST&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TrustPilot: &lt;a href="https://www.trustpilot.com/review/cheapgpt.store"&gt;TrustPilot FEEDBACK&lt;/a&gt;&lt;br /&gt; Bonus: Apply code PROMO5 for $5 OFF your order! &lt;/p&gt; &lt;p&gt;BONUS!: Enjoy the AI Powered automated web browser. (Presented by Perplexity) included!&lt;/p&gt; &lt;p&gt;Trusted and the cheapest!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Verza-"&gt; /u/Verza- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sen0jy3pdu1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozkeey/perplexity_ai_pro_1year_plan_limited_time_super/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ozkeey/perplexity_ai_pro_1year_plan_limited_time_super/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-17T16:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozafjk</id>
    <title>Lm playground with open api connections/lmstudio/ollama</title>
    <updated>2025-11-17T07:50:42+00:00</updated>
    <author>
      <name>/u/AceCustom1</name>
      <uri>https://old.reddit.com/user/AceCustom1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ozafjk/lm_playground_with_open_api/"&gt; &lt;img alt="Lm playground with open api connections/lmstudio/ollama" src="https://preview.redd.it/n5xlzljyrr1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2c4d2aa45ab97b5da83f29d35f08bdf1fb80c24" title="Lm playground with open api connections/lmstudio/ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AceCustom1"&gt; /u/AceCustom1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n5xlzljyrr1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozafjk/lm_playground_with_open_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ozafjk/lm_playground_with_open_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-17T07:50:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyzc7l</id>
    <title>Looking for a truly open source web ui for using with my LLMs</title>
    <updated>2025-11-16T22:34:05+00:00</updated>
    <author>
      <name>/u/sadism_popsicle</name>
      <uri>https://old.reddit.com/user/sadism_popsicle</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sadism_popsicle"&gt; /u/sadism_popsicle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1oyv36m/looking_for_a_truly_open_source_web_ui_for_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oyzc7l/looking_for_a_truly_open_source_web_ui_for_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oyzc7l/looking_for_a_truly_open_source_web_ui_for_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-16T22:34:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyye2y</id>
    <title>The best local commercial hardware for coding with LLMs?</title>
    <updated>2025-11-16T21:55:24+00:00</updated>
    <author>
      <name>/u/Grouchy_Key6227</name>
      <uri>https://old.reddit.com/user/Grouchy_Key6227</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Is the Mac Studio M3 Ultra the best ‚Äúlocal rack‚Äù for coding and LLM inference?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;br /&gt; This is my first post on Reddit; I‚Äôve never written anything here before, so I really appreciate any advice or opinions from the community.&lt;/p&gt; &lt;p&gt;I‚Äôve been programming for about 10 years. Like many people, I started relying on tools like vibe-coding, PRD generators, agents, and lately Claude 3.5 with the 200-max subscription. I also use Codex (the 20 USD plan), and on my NAS I run some small local models.&lt;/p&gt; &lt;p&gt;But the problem is always the same: cloud LLM services start cheap and then become arbitrary. Prices go up, limits change, usage rules get stricter. At this point, my ‚Äúweek‚Äù of Claude Max doesn‚Äôt even last me 4 days with heavy use. And I‚Äôm stuck with windows, quotas, schedules, and restrictions that I can‚Äôt control.&lt;/p&gt; &lt;p&gt;So I started thinking: &lt;em&gt;‚ÄúShould I just build my own rack?‚Äù&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I began researching VRAM, compute power, bandwidth, power consumption, pricing, and clustering.&lt;/p&gt; &lt;p&gt;Here are my personal conclusions:&lt;/p&gt; &lt;h1&gt;1. There is no perfect machine&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;High VRAM + high compute power = 10,000 USD or more.&lt;/li&gt; &lt;li&gt;High VRAM but low compute = systems like AIM 395+, which are affordable but choke on heavy models.&lt;/li&gt; &lt;li&gt;Clustering several cheaper GPUs = the bottleneck becomes networking, synchronization, or constant maintenance.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. High-end GPUs are hard to get in Mexico&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;RTX 4090s disappear fast because most people buy ‚Äúone generation behind.‚Äù&lt;/li&gt; &lt;li&gt;RTX 5090s are over 3,000 USD each.&lt;/li&gt; &lt;li&gt;To get ~120 GB VRAM I‚Äôd need 3 GPUs = ~9,000 USD, not including PSU, rack, motherboard, cooling, etc.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. The middle ground I found: Mac Studio M3 Ultra&lt;/h1&gt; &lt;p&gt;My use case is NOT training or heavy finetuning.&lt;br /&gt; I only want:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;My own ‚Äúpersonal rack‚Äù to replace Claude, Cursor, and similar services.&lt;/li&gt; &lt;li&gt;Local inference with no limits, no queues, and no usage windows.&lt;/li&gt; &lt;li&gt;Big models (70B‚Äì90B), and eventually reduced versions of ~400B models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What convinced me about the M3 Ultra:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A lot of unified memory.&lt;/li&gt; &lt;li&gt;Good tokens-per-second performance for large models optimized for Apple Silicon.&lt;/li&gt; &lt;li&gt;Far less noise, lower energy consumption, and zero maintenance compared to running 3 GPUs in a big workstation.&lt;/li&gt; &lt;li&gt;No dealing with drivers, giant PSUs, RPC clustering issues, heat, or random hardware failures.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Cost&lt;/h1&gt; &lt;p&gt;A high-end Mac Studio M3 Ultra setup ends up costing around 10,000 USD‚Äîroughly the same as building a 3√ó5090 cluster, but with far fewer headaches.&lt;/p&gt; &lt;h1&gt;My question&lt;/h1&gt; &lt;p&gt;For my use case (inference only, multi-agent workflows, RAG, analysis, ‚ÄúClaude/Cursor replacement‚Äù),&lt;br /&gt; &lt;strong&gt;do you think the Mac Studio M3 Ultra is a good choice?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Or is there a better option with a more balanced price‚Äìperformance‚ÄìVRAM‚Äìmaintenance ratio, especially considering how inflated GPU prices are in Mexico?&lt;/p&gt; &lt;p&gt;I‚Äôd really appreciate any technical insights or personal experiences with Apple Silicon, AI Max/AIM 395, DGX Spark, or multi-GPU setups with 4090/5090.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grouchy_Key6227"&gt; /u/Grouchy_Key6227 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oyye2y/the_best_local_commercial_hardware_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oyye2y/the_best_local_commercial_hardware_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oyye2y/the_best_local_commercial_hardware_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-16T21:55:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozswfh</id>
    <title>VSC Format Won't Cut Your AI Costs by 71% (Despite the Hype)</title>
    <updated>2025-11-17T21:26:04+00:00</updated>
    <author>
      <name>/u/Particular-Room8732</name>
      <uri>https://old.reddit.com/user/Particular-Room8732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did the math on VSC format (the CSV-without-headers that's trending) and found: &lt;/p&gt; &lt;p&gt;- Yes, it saves 71% on the data portion&lt;br /&gt; - But only 3% on your total prompt (2,125 ‚Üí 2,036 tokens)&lt;br /&gt; - Actual savings: $0.89 per 10,000 API calls Meanwhile, debugging&lt;br /&gt; &amp;quot;Iphone,1999.90&amp;quot; instead of&lt;br /&gt; {&amp;quot;name&amp;quot;:&amp;quot;Iphone&amp;quot;,&amp;quot;price&amp;quot;:1999.90} is a nightmare. &lt;/p&gt; &lt;p&gt;For those who've tried it: Did you stick with it? Was the token savings worth the loss of readability?&lt;/p&gt; &lt;p&gt;Full analysis: &lt;a href="https://www.superfox.ai/blog/vsc-format-ai-token-costs-truth"&gt;https://www.superfox.ai/blog/vsc-format-ai-token-costs-truth&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Particular-Room8732"&gt; /u/Particular-Room8732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozswfh/vsc_format_wont_cut_your_ai_costs_by_71_despite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozswfh/vsc_format_wont_cut_your_ai_costs_by_71_despite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ozswfh/vsc_format_wont_cut_your_ai_costs_by_71_despite/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-17T21:26:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozvjcz</id>
    <title>DSPy on a Pi: Cheap Prompt Optimization with GEPA and Qwen3</title>
    <updated>2025-11-17T23:08:38+00:00</updated>
    <author>
      <name>/u/parenthethethe</name>
      <uri>https://old.reddit.com/user/parenthethethe</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/parenthethethe"&gt; /u/parenthethethe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://leebutterman.com/2025/11/01/prompt-optimization-on-a-raspberry-pi.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozvjcz/dspy_on_a_pi_cheap_prompt_optimization_with_gepa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ozvjcz/dspy_on_a_pi_cheap_prompt_optimization_with_gepa/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-17T23:08:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozkg1m</id>
    <title>JSON input prompts</title>
    <updated>2025-11-17T16:15:12+00:00</updated>
    <author>
      <name>/u/CommunityBrave822</name>
      <uri>https://old.reddit.com/user/CommunityBrave822</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen a lot of posts and comments about JSON structured output. But little information on using JSON format for prompt (string) as well.&lt;/p&gt; &lt;p&gt;What is your experience using JSON as prompt input versus plain text (or Markdown).&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;p&gt;{&amp;quot;request&amp;quot;: &amp;quot;write a review of Toyota Camry 2005. Use online reviews&amp;quot;, &amp;quot;answer_length&amp;quot;: &amp;quot;300-500 words&amp;quot;, &amp;quot;online reviews&amp;quot;: [&amp;quot;...&amp;quot;, &amp;quot;...&amp;quot;, &amp;quot;...&amp;quot;, ...]}&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommunityBrave822"&gt; /u/CommunityBrave822 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozkg1m/json_input_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozkg1m/json_input_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ozkg1m/json_input_prompts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-17T16:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0f88e</id>
    <title>Gemini 3 is here</title>
    <updated>2025-11-18T15:42:16+00:00</updated>
    <author>
      <name>/u/Embarrassed-Way-1350</name>
      <uri>https://old.reddit.com/user/Embarrassed-Way-1350</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Access in AI studio now. Works pretty well an visual understanding and reasoning and coding. Beats every midel on the planet rn for visual design. Amazing just amazing!!!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Way-1350"&gt; /u/Embarrassed-Way-1350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0f88e/gemini_3_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0f88e/gemini_3_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p0f88e/gemini_3_is_here/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-18T15:42:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozli8v</id>
    <title>Reactive Agents: AI agents that self-optimize after every interaction</title>
    <updated>2025-11-17T16:54:26+00:00</updated>
    <author>
      <name>/u/No_Heart_159</name>
      <uri>https://old.reddit.com/user/No_Heart_159</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Heart_159"&gt; /u/No_Heart_159 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ozjz15"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ozli8v/reactive_agents_ai_agents_that_selfoptimize_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ozli8v/reactive_agents_ai_agents_that_selfoptimize_after/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-17T16:54:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0apgz</id>
    <title>Help me</title>
    <updated>2025-11-18T12:32:17+00:00</updated>
    <author>
      <name>/u/AggressiveSuccess105</name>
      <uri>https://old.reddit.com/user/AggressiveSuccess105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p0apgz/help_me/"&gt; &lt;img alt="Help me" src="https://b.thumbs.redditmedia.com/vYjvpZvzm927az1KKAVBUtAH733MUO8byZQv6Dloi1k.jpg" title="Help me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Error&lt;/h1&gt; &lt;p&gt;500 Internal Server Error: fireworks_tp: 500: Internal Server Error &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tzngzfv3f02g1.png?width=458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=317a0a8335a74c0ec6d32651ec92e68eaf58432b"&gt;https://preview.redd.it/tzngzfv3f02g1.png?width=458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=317a0a8335a74c0ec6d32651ec92e68eaf58432b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggressiveSuccess105"&gt; /u/AggressiveSuccess105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0apgz/help_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0apgz/help_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p0apgz/help_me/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-18T12:32:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0ijg7</id>
    <title>Local Kaggle Companion Experiment Using Ollama + MCP</title>
    <updated>2025-11-18T17:44:29+00:00</updated>
    <author>
      <name>/u/Impossible_Grand_552</name>
      <uri>https://old.reddit.com/user/Impossible_Grand_552</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1p0ijg7/video/v2t9f8t3y12g1/player"&gt;https://reddit.com/link/1p0ijg7/video/v2t9f8t3y12g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;br /&gt; I originally started this project just to learn MCP, but it ended up becoming something I actually enjoy working on, so I‚Äôm continuing to build it out. I‚Äôm calling it &lt;strong&gt;Pocket Data Scientist&lt;/strong&gt;. It runs entirely locally using Streamlit, Ollama, and the Model Context Protocol, and the idea is to have a conversational interface that can actually &lt;em&gt;use tools&lt;/em&gt; to explore and analyze CSV datasets.&lt;/p&gt; &lt;p&gt;One thing that pushed me forward is a pretty noticeable gap in current LLMs: they‚Äôre great at conversation, but they‚Äôre still inconsistent when it comes to selecting or sequencing the right analysis steps. I wanted to experiment with giving the model a proper toolset and letting MCP drive the interaction. It works with any Ollama model, but I stuck with &lt;strong&gt;Llama3 8B&lt;/strong&gt; because that‚Äôs what my hardware can handle without melting.&lt;/p&gt; &lt;p&gt;The bigger vision is to eventually turn this into something like a Kaggle companion: an assistant that can study a dataset, point out issues, suggest preprocessing, and guide you toward the right modeling approach for a given competition. I‚Äôm not there yet, but that‚Äôs the direction I‚Äôd love to take it.&lt;br /&gt; The full code is here if you want to try it or look around:&lt;br /&gt; &lt;a href="https://github.com/Real4LA/pocket-data-scientist"&gt;&lt;strong&gt;https://github.com/Real4LA/pocket-data-scientist&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd really appreciate any feedback. Let me know what features would make this genuinely useful, what it would need to move toward a real Kaggle-style companion, and which analysis tools you think are missing. I‚Äôm also looking for advice on improving tool-use reliability with Ollama and any tips to reduce inference latency, which is the main bottleneck right now (the vid has cuts BTW).&lt;/p&gt; &lt;p&gt;Thanks for checking it out, and I‚Äôm open to any ideas or critiques. üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impossible_Grand_552"&gt; /u/Impossible_Grand_552 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0ijg7/local_kaggle_companion_experiment_using_ollama_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0ijg7/local_kaggle_companion_experiment_using_ollama_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p0ijg7/local_kaggle_companion_experiment_using_ollama_mcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-18T17:44:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0ust1</id>
    <title>M.I.M.I.R - Multi-agent orchestration - drag and drop UI</title>
    <updated>2025-11-19T01:52:44+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1p0usmc/mimir_multiagent_orchestration_drag_and_drop_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0ust1/mimir_multiagent_orchestration_drag_and_drop_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p0ust1/mimir_multiagent_orchestration_drag_and_drop_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T01:52:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0p1pm</id>
    <title>Web front end for Ollama? Is llama.cpp what I'm looking for?</title>
    <updated>2025-11-18T21:48:36+00:00</updated>
    <author>
      <name>/u/JortsKitty</name>
      <uri>https://old.reddit.com/user/JortsKitty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm confused about the relationship between these two things. Is llama.cpp just a web front end for Ollama? I'd like to use ollama, but would like a UI that's better than just a command line.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JortsKitty"&gt; /u/JortsKitty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0p1pm/web_front_end_for_ollama_is_llamacpp_what_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0p1pm/web_front_end_for_ollama_is_llamacpp_what_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p0p1pm/web_front_end_for_ollama_is_llamacpp_what_im/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-18T21:48:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p110ju</id>
    <title>LLM Benchmark Tool</title>
    <updated>2025-11-19T07:17:56+00:00</updated>
    <author>
      <name>/u/Dry-Bandicoot9512</name>
      <uri>https://old.reddit.com/user/Dry-Bandicoot9512</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p110ju/llm_benchmark_tool/"&gt; &lt;img alt="LLM Benchmark Tool" src="https://b.thumbs.redditmedia.com/-h-QkdwM_g1OryC3KMKkNlsBx5R0FUI-4iZO_UtJOvY.jpg" title="LLM Benchmark Tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/CordatusAI/ollama-benchmark"&gt;https://github.com/CordatusAI/ollama-benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A lightweight, real‚Äëtime Streamlit application for benchmarking large language models (LLMs) that are hosted with &lt;strong&gt;Ollama&lt;/strong&gt;. The tool automatically detects your GPU, filters models by VRAM requirements, pulls missing models, runs a prompt‚Äëbased benchmark, and visualises the results with interactive Plotly charts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bkc9m3wwz52g1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=292a30629f7704806393ee4f499cf60a65a8832a"&gt;https://preview.redd.it/bkc9m3wwz52g1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=292a30629f7704806393ee4f499cf60a65a8832a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry-Bandicoot9512"&gt; /u/Dry-Bandicoot9512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p110ju/llm_benchmark_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p110ju/llm_benchmark_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p110ju/llm_benchmark_tool/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T07:17:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0zwio</id>
    <title>Is there an slm that supports Function calling on slm</title>
    <updated>2025-11-19T06:11:37+00:00</updated>
    <author>
      <name>/u/Due_Ad3126</name>
      <uri>https://old.reddit.com/user/Due_Ad3126</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tired of trying different models. Want to know if there is a model or a finetuned model I can run on pc that does very accurate function calling&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Ad3126"&gt; /u/Due_Ad3126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0zwio/is_there_an_slm_that_supports_function_calling_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p0zwio/is_there_an_slm_that_supports_function_calling_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p0zwio/is_there_an_slm_that_supports_function_calling_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T06:11:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1157m</id>
    <title>Modest but reliably accurate LLM</title>
    <updated>2025-11-19T07:26:02+00:00</updated>
    <author>
      <name>/u/SignificanceFlat1460</name>
      <uri>https://old.reddit.com/user/SignificanceFlat1460</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I want to run LLM on my hardware which is a &lt;em&gt;bit&lt;/em&gt; old. My Laptop is a FX505DU&lt;/p&gt; &lt;p&gt;GTX 1660 Ti 6GB Ryzen 7 3750H 16 GB RAM&lt;/p&gt; &lt;p&gt;OK it's a bit more than just a bit old haha. But I wanted to run an LLM that can accurately answer questions related to my CV when applying for jobs. I know some will recommend readily available solutions like gpt-4/5 or Gemini but I want to do this for my own project to see if I can actually do it. Any help would be great. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SignificanceFlat1460"&gt; /u/SignificanceFlat1460 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1157m/modest_but_reliably_accurate_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1157m/modest_but_reliably_accurate_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1157m/modest_but_reliably_accurate_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T07:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1aurp</id>
    <title>An update to Nanocoder üî•</title>
    <updated>2025-11-19T15:35:49+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p1aurp/an_update_to_nanocoder/"&gt; &lt;img alt="An update to Nanocoder üî•" src="https://b.thumbs.redditmedia.com/o-J_tGC-BVWwO-0BEfEwLQLLmDRKaHlx0Lt83jP6NzI.jpg" title="An update to Nanocoder üî•" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/r4j2v8emc82g1.png?width=1734&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e09117983a322fd00410d747da2c1cff7cdda800"&gt;https://preview.redd.it/r4j2v8emc82g1.png?width=1734&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e09117983a322fd00410d747da2c1cff7cdda800&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;Just a quick update on Nanocoder - the open-source, open-community coding CLI that's built with privacy + local-first in mind. You may have seen posts on here before with updates!&lt;/p&gt; &lt;p&gt;One of the first comments on the last post was about starting a dedicated sub-reddit for those interested enough. We've now created this and will slowly phase to use it as an additional channel to provide updates and interact with the AI community over other sub-reddits.&lt;/p&gt; &lt;p&gt;We can't thank everyone enough though that has engaged so positively with the project on sub-reddits like &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;. It means a lot and the community we're building as grown hugely since we started in August.&lt;/p&gt; &lt;p&gt;If you want to join our sub-reddit, you can find it here: &lt;a href="/r/nanocoder"&gt;r/nanocoder&lt;/a&gt; - again, we'll breathe more life into this page as time goes along!&lt;/p&gt; &lt;p&gt;As for what's happening in the world of Nanocoder:&lt;/p&gt; &lt;p&gt;- We're almost at 1K stars!!!&lt;/p&gt; &lt;p&gt;- We've fully switched to use AI SDK now over LangGraph. This has been a fantastic change and one that allows us to expand capabilities of the agent.&lt;/p&gt; &lt;p&gt;- You can now tag files into context with `@`.&lt;/p&gt; &lt;p&gt;- You can no track context usage with the `/usage` command.&lt;/p&gt; &lt;p&gt;- One of our main goals is to make Nanocoder work well and reliably with smaller and smaller models. To do this, we've continued to work on everything from fine-tuned models to better tool orchestration and context management. &lt;/p&gt; &lt;p&gt;We're now at a point where models like `gpt-oss:20b` are reliably working well within the CLI for smaller coding tasks. This is ongoing but we're improving every week. The end vision is to be able to code using Nanocoder totally locally with no need for APIs if you don't want them!&lt;/p&gt; &lt;p&gt;- Continued work to build a small language model into &lt;a href="https://github.com/Nano-Collective/get-md"&gt;get-md&lt;/a&gt; for more accurate and context aware markdown generation for LLMs.&lt;/p&gt; &lt;p&gt;If you're interested in the project, we're a completely open collective building privacy-focused AI. We actively invite all contributions to help build a tool for the community by the community! I'd love for you to get involved :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;em&gt;GitHub Repo&lt;/em&gt;: &lt;a href="https://github.com/Nano-Collective/nanocoder"&gt;https://github.com/Nano-Collective/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Discord&lt;/em&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1aurp/an_update_to_nanocoder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1aurp/an_update_to_nanocoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1aurp/an_update_to_nanocoder/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T15:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1836v</id>
    <title>An Open-Source Agent Foundation Model with Interactive ScalingÔºÅMiroThinker V1.0 just launched!</title>
    <updated>2025-11-19T13:45:54+00:00</updated>
    <author>
      <name>/u/wuqiao</name>
      <uri>https://old.reddit.com/user/wuqiao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p1836v/an_opensource_agent_foundation_model_with/"&gt; &lt;img alt="An Open-Source Agent Foundation Model with Interactive ScalingÔºÅMiroThinker V1.0 just launched!" src="https://external-preview.redd.it/Jeli8vyNHpi1OW6VpCLC7sqFTicW7HMwR1zgB4aSLV4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c40f788dfb4b80413245504088417de6a745393" title="An Open-Source Agent Foundation Model with Interactive ScalingÔºÅMiroThinker V1.0 just launched!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiroThinker v1.0 just launched recently! We're back with a MASSIVE update that's gonna blow your mind!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Code&lt;/strong&gt;Ôºö&lt;a href="https://github.com/MiroMindAI/MiroThinker"&gt;https://github.com/MiroMindAI/MiroThinker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Paper&lt;/strong&gt;Ôºö&lt;a href="https://huggingface.co/papers/2511.11793"&gt;https://huggingface.co/papers/2511.11793&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're introducing the &amp;quot;Interactive Scaling&amp;quot; - a completely new dimension for AI scaling! Instead of just throwing more data/params at models, we let agents learn through deep environmental interaction. The more they practice &amp;amp; reflect, the smarter they get! &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;256K Context + 600-Turn Tool Interaction&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance That Slaps:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;BrowseComp: 47.1% accuracy (nearly matches OpenAI DeepResearch at 51.5%)&lt;/li&gt; &lt;li&gt;Chinese tasks (BrowseComp-ZH): 7.7pp better than DeepSeek-v3.2&lt;/li&gt; &lt;li&gt;First-tier performance across HLE, GAIA, xBench-DeepSearch, SEAL-0&lt;/li&gt; &lt;li&gt;Competing head-to-head with GPT, Grok, Claude&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% Open Source&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Full model weights ‚úÖ &lt;/li&gt; &lt;li&gt;Complete toolchains ‚úÖ &lt;/li&gt; &lt;li&gt;Interaction frameworks ‚úÖ&lt;/li&gt; &lt;li&gt;Because transparency &amp;gt; black boxes&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about the Interactive Scaling approach or benchmarks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wuqiao"&gt; /u/wuqiao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-72B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p1836v/an_opensource_agent_foundation_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p1836v/an_opensource_agent_foundation_model_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-19T13:45:54+00:00</published>
  </entry>
</feed>
