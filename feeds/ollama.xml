<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-10-12T02:16:59+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1o0s6we</id>
    <title>MemoryLLM - A conversational AI system using Ollama with persistent memory capabilities.</title>
    <updated>2025-10-07T21:41:16+00:00</updated>
    <author>
      <name>/u/maranone5</name>
      <uri>https://old.reddit.com/user/maranone5</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o0s6we/memoryllm_a_conversational_ai_system_using_ollama/"&gt; &lt;img alt="MemoryLLM - A conversational AI system using Ollama with persistent memory capabilities." src="https://external-preview.redd.it/2KXnTfjRF7fYN7gSWUGWKW5ZuPplML-5c1Y2NAD-nBA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f38a9d6da1ab27070cb74725e72327b05b0f22f9" title="MemoryLLM - A conversational AI system using Ollama with persistent memory capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1o0s6we/video/t56td2xqertf1/player"&gt;https://reddit.com/link/1o0s6we/video/t56td2xqertf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi this is an experiment I've wanted to do for quite a while, It's been done again and again here but this is just a simple implementation.&lt;/p&gt; &lt;p&gt;The idea behind it is to have memory extracted during &amp;quot;idle&amp;quot; time between responses so there's both the memories for rag and the conversation history.&lt;/p&gt; &lt;p&gt;Then just a simple tweak of the system prompt to try and have the model adapt to your tone/mood nothing fancy just prompt.&lt;/p&gt; &lt;p&gt;Anyways it is an attempt of having a conversational local llm that with time get's to know you, with gemma3:4b not so much though :)&lt;/p&gt; &lt;p&gt;Then there is a very bad result of trying to have the llm have a perception of time elapsed between sessions (model gets timestamps and also current date-time)&lt;/p&gt; &lt;p&gt;Anyways here is the repo if someone wants to use or have a blueprint &lt;a href="https://github.com/maranone/MemoryLLM"&gt;MemoryLLM - Github&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maranone5"&gt; /u/maranone5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0s6we/memoryllm_a_conversational_ai_system_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0s6we/memoryllm_a_conversational_ai_system_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0s6we/memoryllm_a_conversational_ai_system_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T21:41:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0ivqz</id>
    <title>Hi folks, sorry for the self‚Äëpromo. I‚Äôve built an open‚Äësource project that could be useful to some of you</title>
    <updated>2025-10-07T16:02:24+00:00</updated>
    <author>
      <name>/u/panos_s_</name>
      <uri>https://old.reddit.com/user/panos_s_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o0ivqz/hi_folks_sorry_for_the_selfpromo_ive_built_an/"&gt; &lt;img alt="Hi folks, sorry for the self‚Äëpromo. I‚Äôve built an open‚Äësource project that could be useful to some of you" src="https://preview.redd.it/61oh2s9aqptf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a22f4bd151e1659d6c522d89b6519dd0237f6d1" title="Hi folks, sorry for the self‚Äëpromo. I‚Äôve built an open‚Äësource project that could be useful to some of you" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Web dashboard for NVIDIA GPUs with 30+ real-time metrics (utilisation, memory, temps, clocks, power, processes). Live charts over WebSockets, multi‚ÄëGPU support, and one‚Äëcommand Docker deployment. No agents, minimal setup.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/psalias2006/gpu-hot"&gt;https://github.com/psalias2006/gpu-hot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I built it&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Wanted simple, real‚Äëtime visibility without standing up a full metrics stack.&lt;/li&gt; &lt;li&gt;Needed clear insight into temps, throttling, clocks, and active processes during GPU work.&lt;/li&gt; &lt;li&gt;A lightweight dashboard that‚Äôs easy to run at home or on a workstation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Polls nvidia-smi and streams 30+ metrics every ~2s via WebSockets.&lt;/li&gt; &lt;li&gt;Tracks per‚ÄëGPU utilization, memory (used/free/total), temps, power draw/limits, fan, clocks, PCIe, P‚ÄëState, encoder/decoder stats, driver/VBIOS, throttle status.&lt;/li&gt; &lt;li&gt;Shows active GPU processes with PIDs and memory usage.&lt;/li&gt; &lt;li&gt;Clean, responsive UI with live historical charts and basic stats (min/max/avg).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup (Docker)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/psalias2006/gpu-hot cd gpu-hot docker-compose up --build # open http://localhost:1312 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Looking for feedback&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panos_s_"&gt; /u/panos_s_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/61oh2s9aqptf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0ivqz/hi_folks_sorry_for_the_selfpromo_ive_built_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0ivqz/hi_folks_sorry_for_the_selfpromo_ive_built_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T16:02:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1o11o3u</id>
    <title>Meer CLI ‚Äî an open-source Claude Code Alternative</title>
    <updated>2025-10-08T05:06:02+00:00</updated>
    <author>
      <name>/u/msaifeldeen</name>
      <uri>https://old.reddit.com/user/msaifeldeen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ I built Meer CLI ‚Äî an open-source AI command-line tool that talks to any model (Ollama, OpenAI, Claude, etc.)&lt;/p&gt; &lt;p&gt;Hey folks üëã I‚Äôve been working on a developer-first CLI called Meer AI, now live at meerai.dev.&lt;/p&gt; &lt;p&gt;It‚Äôs designed for builders who love the terminal and want to use AI locally or remotely without switching between dashboards or UIs.&lt;/p&gt; &lt;p&gt;üß† What it does ‚Ä¢ üîó Model-agnostic ‚Äî works with Ollama, OpenAI, Claude, Gemini, etc. ‚Ä¢ üß∞ Plug-and-play CLI ‚Äî run prompts, analyze code, or run agents directly from your terminal ‚Ä¢ üíæ Local memory ‚Äî remembers your context across sessions ‚Ä¢ ‚öôÔ∏è Configurable providers ‚Äî choose or self-host your backend (e.g., Ollama on your own server) ‚Ä¢ üåä ‚ÄúMeer‚Äù = Sea ‚Äî themed around ocean intelligence üåä&lt;/p&gt; &lt;p&gt;üí° Why I built it&lt;/p&gt; &lt;p&gt;I wanted a simple way to unify my self-hosted models and APIs without constant context loss or UI juggling. The goal is to make AI interaction feel native to the command line.&lt;/p&gt; &lt;p&gt;üê≥ Try it&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://meerai.dev"&gt;https://meerai.dev&lt;/a&gt; It‚Äôs early but functional ‚Äî you can chat with models, run commands, and customize providers.&lt;/p&gt; &lt;p&gt;Would love feedback, ideas, or contributors who want to shape the future of CLI-based AI tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/msaifeldeen"&gt; /u/msaifeldeen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o11o3u/meer_cli_an_opensource_claude_code_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o11o3u/meer_cli_an_opensource_claude_code_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o11o3u/meer_cli_an_opensource_claude_code_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-08T05:06:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o260nw</id>
    <title>llama3.1:8b-text-q8_0 allucinating</title>
    <updated>2025-10-09T13:31:29+00:00</updated>
    <author>
      <name>/u/doleron</name>
      <uri>https://old.reddit.com/user/doleron</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm just testing the model. However, in my first attempts, the model just allucinated. This is an example:&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;sander@sanderbox:~$ ollama run llama3.1:8b-text-q8_0&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; Ol√°, voc√™ pode falar portugu√™s brasileiro?&lt;/p&gt; &lt;p&gt;If you can speak Portuguese, please leave a comment. This is the first ever YouTube video I‚Äôve created in Portuguese and my first attempt at learning it! Thanks for watching!&lt;/p&gt; &lt;p&gt;P.S. My friend who is also learning the language told me that the expression ‚Äúyou‚Äôre welcome‚Äù in Brazilian Portuguese is literally translated as ‚Äúmy pleasure.‚Äù So, if you want to say &lt;/p&gt; &lt;p&gt;thank you after someone has thanked you, you would reply with ‚Äúmy pleasure,‚Äù not ‚Äúyou‚Äôre welcome.‚Äù&lt;/p&gt; &lt;p&gt;Tagged brazil, brasil, brazilian, brasilianos, capoeira, capoeirista, capoeiristas, daniel, fala, foreign, learning, language, learn, lingua, new, olympics, portuguese, rio, spanish&lt;br /&gt; Previous postHow I Learned to Read Faster and Remember What I Read (Part 2)&lt;br /&gt; Next postMy Favorite Books in 2016&lt;/p&gt; &lt;h1&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/h1&gt; &lt;p&gt;The answer I got from llama3.1 is way better:&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;sander@sanderbox:~$ ollama run llama3.1&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; Ol√°, voc√™ pode falar portugu√™s brasileiro?&lt;/p&gt; &lt;p&gt;Sim, posso falar em portugu√™s brasileiro. Como posso ajudar?&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;Can someone spot what I'm doing wrong here? Is not llama3.1:8b-text-q8_0 more likely to provide better responses than llama3.1?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doleron"&gt; /u/doleron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o260nw/llama318btextq8_0_allucinating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o260nw/llama318btextq8_0_allucinating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o260nw/llama318btextq8_0_allucinating/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-09T13:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1qbwr</id>
    <title>Web Research Tool For Ollama - Does this exist?</title>
    <updated>2025-10-08T23:10:50+00:00</updated>
    <author>
      <name>/u/Comfortable_Ad_8117</name>
      <uri>https://old.reddit.com/user/Comfortable_Ad_8117</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an Etsy and EBay business where I sell antiques and I make a number of new listings a week for items that sometimes I don‚Äôt have much information on. Currently I use the Ollama with Open WebUi to help make my listings. &lt;/p&gt; &lt;p&gt;I know a lot of folks in this community have great open source projects and i was hoping one might be able to help me. &lt;/p&gt; &lt;p&gt;Ideally - I would like to feed an Ollama model a prompt with some basics about an item, maybe even an image or images and have it do the following - analyze the image - understand my brief description - web search (research) for similar items and lean about the item - produce a listing with: A brief description and history of the item - Features of the item (in bullet points) Size, color, material, condition, etc. - Keywords / tags - It‚Äôs guess on value (just for fun)&lt;/p&gt; &lt;p&gt;Right now Open Webui is OK at doing this, it struggles with the research, but usually generates a halfway decent listing&lt;/p&gt; &lt;p&gt;The biggest model I can run with acceptable performance is mistral-small3.2: 24B Oh and My Ollama is on its own (Windows) hardware so whatever app would have to be able to connect to it.&lt;/p&gt; &lt;p&gt;Any ideas would be appreciated. Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Ad_8117"&gt; /u/Comfortable_Ad_8117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o1qbwr/web_research_tool_for_ollama_does_this_exist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o1qbwr/web_research_tool_for_ollama_does_this_exist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o1qbwr/web_research_tool_for_ollama_does_this_exist/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-08T23:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o23zby</id>
    <title>Large RAM Macs</title>
    <updated>2025-10-09T11:59:29+00:00</updated>
    <author>
      <name>/u/Dependent_Price_1306</name>
      <uri>https://old.reddit.com/user/Dependent_Price_1306</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking at the possibility of getting a m4 pro mac mini with 64GB of unified ram (or wait for m5). I can reasonably run the Gemma 12:b on my m4 mba (well it generates text as fast as I read. Has anyone got any experience on a similar setup to what I envision?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dependent_Price_1306"&gt; /u/Dependent_Price_1306 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o23zby/large_ram_macs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o23zby/large_ram_macs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o23zby/large_ram_macs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-09T11:59:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2cywq</id>
    <title>I uploaded a Qwen3 distilled model to Ollama with the &lt;think&gt; tag working properly.</title>
    <updated>2025-10-09T17:57:05+00:00</updated>
    <author>
      <name>/u/East-Engineering-653</name>
      <uri>https://old.reddit.com/user/East-Engineering-653</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-FP32-i1-GGUF"&gt;https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-FP32-i1-GGUF&lt;/a&gt;&lt;br /&gt; Using the same &lt;strong&gt;Modelfile&lt;/strong&gt; as the &lt;code&gt;qwen3:30b&lt;/code&gt; model, I fixed an issue where the &lt;strong&gt;&amp;lt;think&amp;gt; tag was not displayed&lt;/strong&gt; when running the model from the link above.&lt;br /&gt; The &lt;strong&gt;model itself is identical&lt;/strong&gt; ‚Äî only the &lt;strong&gt;Modelfile&lt;/strong&gt; has been modified.&lt;br /&gt; &lt;a href="https://ollama.com/uaysk0327/qwen3-30b-distilled"&gt;https://ollama.com/uaysk0327/qwen3-30b-distilled&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/East-Engineering-653"&gt; /u/East-Engineering-653 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2cywq/i_uploaded_a_qwen3_distilled_model_to_ollama_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2cywq/i_uploaded_a_qwen3_distilled_model_to_ollama_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o2cywq/i_uploaded_a_qwen3_distilled_model_to_ollama_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-09T17:57:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2b3g4</id>
    <title>Moondream3 and Salesforce GTA-1 for UI grounding in computer-use agents</title>
    <updated>2025-10-09T16:46:40+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o2b3g4/moondream3_and_salesforce_gta1_for_ui_grounding/"&gt; &lt;img alt="Moondream3 and Salesforce GTA-1 for UI grounding in computer-use agents" src="https://external-preview.redd.it/NGg4OXdqcDA4NHVmMeWLBg1mAXwAigO5PzLkc4zOrANcV84YU3-0ubuxy7cx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76e8c14970d13001a77fa285bc7ac5697606974e" title="Moondream3 and Salesforce GTA-1 for UI grounding in computer-use agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Moondream3 and Salesforce GTA-1 for UI grounding in computer-use agents&lt;/p&gt; &lt;p&gt;The numbers on ScreenSpot-v2 benchmark:&lt;/p&gt; &lt;p&gt;GTA-1 leads in accuracy (96% vs 84%), but Moondream3 is 2x faster (1.04s vs 1.97s avg).&lt;/p&gt; &lt;p&gt;The median time gap is even bigger: 0.78s vs 1.96s - that's a 2.5x speedup.&lt;/p&gt; &lt;p&gt;Both models are open-weight, self-hostable and work out-of-the-box with Cua: &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Run the benchmark yourself: &lt;a href="https://docs.trycua.com/docs/agent-sdk/benchmarks/screenspot-v2"&gt;https://docs.trycua.com/docs/agent-sdk/benchmarks/screenspot-v2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b99iyk6184uf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2b3g4/moondream3_and_salesforce_gta1_for_ui_grounding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o2b3g4/moondream3_and_salesforce_gta1_for_ui_grounding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-09T16:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2x3ql</id>
    <title>Devstral made unusable (nvim/Avante)</title>
    <updated>2025-10-10T10:21:42+00:00</updated>
    <author>
      <name>/u/Mordimer86</name>
      <uri>https://old.reddit.com/user/Mordimer86</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since one of the recent updates that brought tool calling, I cannot use devstral model because each time it gets stuck on some tool calling. Anyone knows if there is a way to make it work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mordimer86"&gt; /u/Mordimer86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2x3ql/devstral_made_unusable_nvimavante/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2x3ql/devstral_made_unusable_nvimavante/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o2x3ql/devstral_made_unusable_nvimavante/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-10T10:21:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2xq6n</id>
    <title>quadro m5000 cuda support?</title>
    <updated>2025-10-10T10:57:18+00:00</updated>
    <author>
      <name>/u/idecidelater</name>
      <uri>https://old.reddit.com/user/idecidelater</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o2xq6n/quadro_m5000_cuda_support/"&gt; &lt;img alt="quadro m5000 cuda support?" src="https://b.thumbs.redditmedia.com/rwkTPSZtbtEXcS4hdgDedgHp79zNhUo60tcjR853XRk.jpg" title="quadro m5000 cuda support?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;which version cuda toolkit should i install for quadro m5000 card?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zu9ez6omm9uf1.png?width=1099&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7d2232163dd6374dbd0ad9cf5946feccdea60dd3"&gt;https://preview.redd.it/zu9ez6omm9uf1.png?width=1099&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7d2232163dd6374dbd0ad9cf5946feccdea60dd3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/idecidelater"&gt; /u/idecidelater &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2xq6n/quadro_m5000_cuda_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2xq6n/quadro_m5000_cuda_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o2xq6n/quadro_m5000_cuda_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-10T10:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o302lf</id>
    <title>Easy way to auto route to different models?</title>
    <updated>2025-10-10T12:53:08+00:00</updated>
    <author>
      <name>/u/EarEquivalent3929</name>
      <uri>https://old.reddit.com/user/EarEquivalent3929</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an ollama instance that I use with homeassistant, n8n as well as a few custom scripts. I only use one model to prevent delay when loading it in to memory. Right now I'm using llama3.2, however if I change this model I also have to update everything that uses my ollama instance to select the proper model. Is there a way for me to just specify the model name as &amp;quot;main&amp;quot; or something in my clients, and have ollama send the request to whatever model is loaded in memory?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EarEquivalent3929"&gt; /u/EarEquivalent3929 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o302lf/easy_way_to_auto_route_to_different_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o302lf/easy_way_to_auto_route_to_different_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o302lf/easy_way_to_auto_route_to_different_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-10T12:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2a5i5</id>
    <title>Nanocoder Continues to Grow - A Small Update</title>
    <updated>2025-10-09T16:11:30+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o2a5i5/nanocoder_continues_to_grow_a_small_update/"&gt; &lt;img alt="Nanocoder Continues to Grow - A Small Update" src="https://external-preview.redd.it/bnZiODl4bGx6M3VmMUcr7w_XMPgkCFzq8MGyFqa-tX1dbJAve2bfR2mbBlv_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cf796ece074ccffbbde5b5a24880c1ead16996b" title="Nanocoder Continues to Grow - A Small Update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I just wanted to share an update post on Nanocoder, the open-source, open-community coding CLI.&lt;/p&gt; &lt;p&gt;Since the last post a couple weeks ago we've surpassed 500 GitHub stars which is epic and I can't thank everyone enough - I know it's still small but we're growing everyday!&lt;/p&gt; &lt;p&gt;The community, the amount of contributors and ideas flowing has also been beyond amazing as we aim to build a coding tool that truly takes advantage of local-first technology and is built for the community.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Here are some highlights of what the last couple of weeks has entailed:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;- Nanocoder has been moved to be under the Nano Collective org on GitHub. This is a new collective which I hope will continue to foster people wanting to build and grow local-first and open-source AI tools for the community whether that be Nanocoder or other packages and software.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Highlight of Features Added:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- A models database, run &lt;code&gt;/recommendations&lt;/code&gt; to let Nanocoder scan your system and make recommendations on models to have the best experience with.&lt;/p&gt; &lt;p&gt;- New agent tools: &lt;code&gt;web_search&lt;/code&gt;, &lt;code&gt;fetch_url&lt;/code&gt; and &lt;code&gt;search_files&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;- Modes, run Nanocoder on normal, auto-accept or planning mode.&lt;/p&gt; &lt;p&gt;- &lt;code&gt;/init&lt;/code&gt; to generate an &lt;code&gt;AGENTS.md&lt;/code&gt; file for your project.&lt;/p&gt; &lt;p&gt;- Lots more.&lt;/p&gt; &lt;p&gt;We've also been making a lot of progress in agent frameworks to offset tasks to tiny models to keep things local and private as much as possible. More on this soon.&lt;/p&gt; &lt;p&gt;Thank you to everyone that is getting involved and supporting the project. It continues to be very early days but we're rapidly taking on feedback and trying to improve the software üòä&lt;/p&gt; &lt;p&gt;That being said, any help within any domain is appreciated and welcomed.&lt;/p&gt; &lt;p&gt;If you want to get involved the links are below.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Link&lt;/strong&gt;: &lt;a href="https://github.com/Nano-Collective/nanocoder"&gt;https://github.com/Nano-Collective/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord Link&lt;/strong&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n0huywllz3uf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2a5i5/nanocoder_continues_to_grow_a_small_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o2a5i5/nanocoder_continues_to_grow_a_small_update/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-09T16:11:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2usji</id>
    <title>4600 Stars- the story about our open source Agent!</title>
    <updated>2025-10-10T07:53:55+00:00</updated>
    <author>
      <name>/u/Petesneaknex</name>
      <uri>https://old.reddit.com/user/Petesneaknex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys üëã&lt;/p&gt; &lt;p&gt;I wanted to share the journey behind a wild couple of days building &lt;a href="http://www.droidrun.ai"&gt;Droidrun&lt;/a&gt;, our open-source agent framework for automating real Android apps.&lt;/p&gt; &lt;p&gt;We started building Droidrun because we were frustrated: everything in automation and agent tech seemed stuck in the browser. But people live on their phones and apps are walled gardens. So we built an agent that could actually tap, scroll, and interact inside real mobile apps, like a human.&lt;/p&gt; &lt;p&gt;A few weeks ago, we posted a short demo no pitch, just an agent running a real Android UI. Within 48 hours:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We hit &lt;a href="https://github.com/droidrun/droidrun"&gt;4600+ GitHub Stars&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Got devs joining our Discord&lt;/li&gt; &lt;li&gt;Landed on the radar of investors&lt;/li&gt; &lt;li&gt;And closed a &lt;strong&gt;$2M+ funding round&lt;/strong&gt; shortly after&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What worked for us:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;We led with a real demo&lt;/strong&gt;, not a roadmap&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Posted in the right communities&lt;/strong&gt;, not product forums&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Asked for feedback&lt;/strong&gt;, not attention&lt;/li&gt; &lt;li&gt;And open-sourced from day one, which gave us credibility + momentum&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôre still in the early days, and there‚Äôs a ton to figure out. But the biggest lesson so far:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Don‚Äôt wait to polish. Ship the weird, broken, raw thing if the core is strong, people will get it.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you‚Äôre working on something agentic, mobile, or just bold than I‚Äôd love to hear what you‚Äôre building too.&lt;/p&gt; &lt;p&gt;AMA if helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Petesneaknex"&gt; /u/Petesneaknex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2usji/4600_stars_the_story_about_our_open_source_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2usji/4600_stars_the_story_about_our_open_source_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o2usji/4600_stars_the_story_about_our_open_source_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-10T07:53:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2myew</id>
    <title>ollama based aps</title>
    <updated>2025-10-10T00:47:29+00:00</updated>
    <author>
      <name>/u/Ok-Function-7101</name>
      <uri>https://old.reddit.com/user/Ok-Function-7101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o2myew/ollama_based_aps/"&gt; &lt;img alt="ollama based aps" src="https://external-preview.redd.it/Ho-rXoU8Y5Bh6-ydPlkNdZdTo3bYsGoeNMIqnIZW_-k.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6834453fe5eeb55d1b7fcd6c7af70140834c2371" title="ollama based aps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;ü§Ø I Built a Massive Collection of Ollama-Powered Desktop Apps (From Private Chatbots to Mind Maps)&lt;/h1&gt; &lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I've been spending a ton of time building open-source desktop applications that are fully powered by &lt;strong&gt;Ollama&lt;/strong&gt; and local Large Language Models (LLMs). My goal is to showcase the power of local AI by creating a suite of tools that are &lt;strong&gt;private, secure, and run right on your machine.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I wanted to share my work with the Ollama community‚Äîmaybe some of these will inspire your next project or become your new favorite tool! Everything is open source, mostly built with Python/PySide6, and designed to make local LLMs genuinely useful for everyday tasks.&lt;/p&gt; &lt;h1&gt;üõ†Ô∏è Core Ollama-Powered Applications&lt;/h1&gt; &lt;p&gt;These are the projects I think will be most relevant and exciting to the local LLM community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cortex&lt;/strong&gt;: Your self-hosted, personal desktop chatbot. A private, secure, and highly responsive &lt;strong&gt;AI assistant&lt;/strong&gt; for seamless interaction with local LLMs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/udn3ew0znauf1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=42143b7041cb309d20546f79207efd24d3eed5b8"&gt;https://preview.redd.it/udn3ew0znauf1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=42143b7041cb309d20546f79207efd24d3eed5b8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/micc4x0znauf1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=828372685a2df8c871bef779f3223ee90a08f063"&gt;https://preview.redd.it/micc4x0znauf1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=828372685a2df8c871bef779f3223ee90a08f063&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other notable aps-&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Autonomous-AI-Web-Search-Assistant&lt;/strong&gt;: An advanced AI research assistant that provides trustworthy, &lt;strong&gt;real-time answers from the web&lt;/strong&gt;. It uses local models to intelligently break down, search, and validate online sources.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fine-Tuned&lt;/strong&gt;: A desktop application designed to bridge the gap between &lt;strong&gt;model fine-tuning and a user-friendly graphical interface&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tree-Graph-MindMap&lt;/strong&gt;: Transforms raw, unstructured text into an &lt;strong&gt;interactive mind map&lt;/strong&gt;. It uses Ollama to intelligently structure the information.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ITT-Qwen&lt;/strong&gt;: A sleek desktop app for &lt;strong&gt;image-to-text analysis&lt;/strong&gt; powered by the Qwen Vision Language Model via Ollama, featuring custom UI and region selection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;File2MD&lt;/strong&gt;: A sleek desktop app that converts text to &lt;strong&gt;Markdown using private, local AI&lt;/strong&gt; with a live rendered preview. Your data stays yours!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;genisis-mini&lt;/strong&gt;: A powerful tool for &lt;strong&gt;generating structured data&lt;/strong&gt; (e.g., synthetic data) for educational purposes or fine-tuning smaller models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;clarity&lt;/strong&gt;: A sophisticated desktop application designed for &lt;strong&gt;in-depth text analysis&lt;/strong&gt; (summaries, structural breakdowns) leveraging LLMs via Ollama.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local-Deepseek-R1&lt;/strong&gt;: A modern desktop interface for local language models through Ollama, featuring persistent chat history and real-time model switching.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üëâ Where to find them&lt;/h1&gt; &lt;p&gt;You can check out all the repos on my GitHub profile: Link - &lt;a href="https://github.com/dovvnloading"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think! Which one are you trying first? Sorry if this comes off as self promo, I'm new to putting my work out there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Function-7101"&gt; /u/Ok-Function-7101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2myew/ollama_based_aps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2myew/ollama_based_aps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o2myew/ollama_based_aps/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-10T00:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2v8dr</id>
    <title>üö® Meta's AI chats are now used for targeted ads... privacy is vanishing fast. Local AI might be our only way forward.</title>
    <updated>2025-10-10T08:23:08+00:00</updated>
    <author>
      <name>/u/Code-Forge-Temple</name>
      <uri>https://old.reddit.com/user/Code-Forge-Temple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta recently announced that AI chat interactions on Facebook and Instagram will be used for ad targeting.&lt;br /&gt; This means that everything you type can now shape how you are profiled, a stark reminder that cloud AI often means zero privacy. &lt;/p&gt; &lt;p&gt;AI is becoming part of our daily tools, but do we really want every thought or query ending up on someone else's server? &lt;/p&gt; &lt;p&gt;Local-first AI puts you in control. Models run entirely on your own device, keeping your data private and giving you full ownership over the process and results. &lt;/p&gt; &lt;p&gt;Here are some of my projects exploring this approach: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://agentic-signal.com"&gt;&lt;strong&gt;Agentic Signal&lt;/strong&gt;&lt;/a&gt;: privacy-first workflows and browser AI agent framework. Open-source for personal use, with commercial licensing available for businesses and SaaS. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://github.com/code-forge-temple/scribe-pal"&gt;ScribePal&lt;/a&gt;: a local browser AI assistant that summarizes and interacts with content without sending data to external servers. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://github.com/code-forge-temple/local-llm-npc"&gt;Local LLM NPC&lt;/a&gt;: an educational Godot game powered by Gemma 3n via Ollama, with offline-first NPCs teaching sustainable farming and botany through interactive dialogue. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Local AI isn‚Äôt just a technical preference - it‚Äôs essential for privacy, autonomy, and transparency in AI. &lt;/p&gt; &lt;p&gt;Will privacy-first local AI become the norm, or will convenience keep most users in the cloud? &lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://www.cnbc.com/2025/10/01/meta-facebook-instagram-ads-ai-chat.html"&gt;https://www.cnbc.com/2025/10/01/meta-facebook-instagram-ads-ai-chat.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Code-Forge-Temple"&gt; /u/Code-Forge-Temple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2v8dr/metas_ai_chats_are_now_used_for_targeted_ads/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2v8dr/metas_ai_chats_are_now_used_for_targeted_ads/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o2v8dr/metas_ai_chats_are_now_used_for_targeted_ads/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-10T08:23:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2y1sb</id>
    <title>I built an app that lets you use your Ollama models remotely (without port forwarding) + AES encryption</title>
    <updated>2025-10-10T11:14:40+00:00</updated>
    <author>
      <name>/u/EntropyMagnets</name>
      <uri>https://old.reddit.com/user/EntropyMagnets</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o2y1sb/i_built_an_app_that_lets_you_use_your_ollama/"&gt; &lt;img alt="I built an app that lets you use your Ollama models remotely (without port forwarding) + AES encryption" src="https://b.thumbs.redditmedia.com/5azRUrZvsfCKxMkXlLnkR_avsa6SA4BaagV-Cj1wWoY.jpg" title="I built an app that lets you use your Ollama models remotely (without port forwarding) + AES encryption" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you want to check out the source code or try the app, you can find it here: &lt;a href="https://github.com/Belluxx/TransFire"&gt;https://github.com/Belluxx/TransFire&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You will need to create a free Firebase instance that will act as a proxy between the app and the local Ollama server, check the README for detailed instructions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntropyMagnets"&gt; /u/EntropyMagnets &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o2y1sb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2y1sb/i_built_an_app_that_lets_you_use_your_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o2y1sb/i_built_an_app_that_lets_you_use_your_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-10T11:14:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3ommb</id>
    <title>/no_think stopped working in Qwen3 after the update v0.12.5</title>
    <updated>2025-10-11T06:42:11+00:00</updated>
    <author>
      <name>/u/Space__Whiskey</name>
      <uri>https://old.reddit.com/user/Space__Whiskey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems to now add /think to every prompt, so even if you type /no_think, it will still add a /think after that, which ignores /no_think.&lt;/p&gt; &lt;p&gt;This only happened after the update, which has broken all of my agents that use /no_think in the prompts. For example, n8n agents rely on the think tags in the prompt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Space__Whiskey"&gt; /u/Space__Whiskey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3ommb/no_think_stopped_working_in_qwen3_after_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3ommb/no_think_stopped_working_in_qwen3_after_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o3ommb/no_think_stopped_working_in_qwen3_after_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-11T06:42:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3obzg</id>
    <title>Get LLM to Query SQL Database</title>
    <updated>2025-10-11T06:23:56+00:00</updated>
    <author>
      <name>/u/thinktank99</name>
      <uri>https://old.reddit.com/user/thinktank99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I want an LLM to parse some XMLs and generate a summary. There are data elememnts in the xml which have description stored in database tables. The tables have about 50k rows so I cant just extract them and attach it to the prompt for the LLM to refer. &lt;/p&gt; &lt;p&gt;How do I get the LLM to query the database table if needs to get the description for data elements?&lt;/p&gt; &lt;p&gt;I am using a python script to read the XMLs and call OLLAMA API to generate a summary.&lt;/p&gt; &lt;p&gt;Any help would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thinktank99"&gt; /u/thinktank99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3obzg/get_llm_to_query_sql_database/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3obzg/get_llm_to_query_sql_database/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o3obzg/get_llm_to_query_sql_database/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-11T06:23:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3u0ui</id>
    <title>GPU Choices for linux</title>
    <updated>2025-10-11T12:11:02+00:00</updated>
    <author>
      <name>/u/Technical-Ant-2866</name>
      <uri>https://old.reddit.com/user/Technical-Ant-2866</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not new to Ollama, but have lazily been running it on a ryzen apu for a while with varying results. I run Linux and don't plan to change that. Most of my activities are working with video (transcoding old movies and such) and occasional gaming (rare). &lt;/p&gt; &lt;p&gt;I've been researching gpus that might work decently with a Minisforum bd750i (7945hx) and had my eyes on a 5060 ti 16gb. I know that this can handle a lot of models in ollama, but has anyone got this working with Linux smoothly? I'm reading varying reports that there's hoops you have to jump through depending on your distro.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Ant-2866"&gt; /u/Technical-Ant-2866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3u0ui/gpu_choices_for_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3u0ui/gpu_choices_for_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o3u0ui/gpu_choices_for_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-11T12:11:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3zzvv</id>
    <title>how to disable thinking qwen3?</title>
    <updated>2025-10-11T16:28:42+00:00</updated>
    <author>
      <name>/u/RadianceTower</name>
      <uri>https://old.reddit.com/user/RadianceTower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;/no_think /nothink /set nothink /set_no_think&lt;/p&gt; &lt;p&gt;None of these work, the model is now thinking what those mean lol.&lt;/p&gt; &lt;p&gt;Neither does disabling thinking in open webui options.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RadianceTower"&gt; /u/RadianceTower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3zzvv/how_to_disable_thinking_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3zzvv/how_to_disable_thinking_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o3zzvv/how_to_disable_thinking_qwen3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-11T16:28:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3w4mk</id>
    <title>Split GGUF Files</title>
    <updated>2025-10-11T13:49:28+00:00</updated>
    <author>
      <name>/u/crossijinn</name>
      <uri>https://old.reddit.com/user/crossijinn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it now possible to import split gguf files into Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crossijinn"&gt; /u/crossijinn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3w4mk/split_gguf_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3w4mk/split_gguf_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o3w4mk/split_gguf_files/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-11T13:49:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3snb3</id>
    <title>Large Language Models for GNU Octave</title>
    <updated>2025-10-11T10:56:04+00:00</updated>
    <author>
      <name>/u/pr0m1th3as</name>
      <uri>https://old.reddit.com/user/pr0m1th3as</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o3snb3/large_language_models_for_gnu_octave/"&gt; &lt;img alt="Large Language Models for GNU Octave" src="https://external-preview.redd.it/fLUft0qowDrg3EmU8ewqahGa16BWmVIOGX-IxNPGObU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb7a0a187a8289012bb41dcb31e91232c8370204" title="Large Language Models for GNU Octave" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pr0m1th3as"&gt; /u/pr0m1th3as &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pr0m1th3as/octave-llms"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3snb3/large_language_models_for_gnu_octave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o3snb3/large_language_models_for_gnu_octave/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-11T10:56:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3vahv</id>
    <title>Fighting Email Spam on Your Mail Server with LLMs ‚Äî Privately</title>
    <updated>2025-10-11T13:11:49+00:00</updated>
    <author>
      <name>/u/unixf0x</name>
      <uri>https://old.reddit.com/user/unixf0x</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unixf0x"&gt; /u/unixf0x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cybercarnet.eu/posts/email-spam-llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3vahv/fighting_email_spam_on_your_mail_server_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o3vahv/fighting_email_spam_on_your_mail_server_with_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-11T13:11:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o42jnm</id>
    <title>Built an MCP tool that connects Qwen, Claude Code, Codex, Gemini to a shared memory database.</title>
    <updated>2025-10-11T18:11:11+00:00</updated>
    <author>
      <name>/u/Alone-Biscotti6145</name>
      <uri>https://old.reddit.com/user/Alone-Biscotti6145</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1o42jnm/video/roi91aiwviuf1/player"&gt;https://reddit.com/link/1o42jnm/video/roi91aiwviuf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(TL:DR) I know the mcp tool I built is useful and can help a ton of people with their workflow and I'm looking for marketing/promoting advice.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Repo - &lt;a href="https://github.com/Lyellr88/MARM-System"&gt;https://github.com/Lyellr88/MARM-System&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Stats - 176 stars, 32 forks, 398 docker pulls and 2026 pip installs&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;What is MARM?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MARM is a production-ready Universal MCP Server that gives AI agents persistent, cross-platform memory. It's built on SQLite with vector embeddings for semantic search, meaning your AI can find information by meaning, not just keywords.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Universal memory layer - Works with Claude, Gemini, Qwen, any MCP client&lt;/li&gt; &lt;li&gt;Persistent cross-session - Memories survive restarts and container rebuilds&lt;/li&gt; &lt;li&gt;Semantic search - Sentence transformers (all-MiniLM-L6-v2) for intelligent recall&lt;/li&gt; &lt;li&gt;Cross-platform - One database, multiple AIs can read/write&lt;/li&gt; &lt;li&gt;Production architecture - WAL mode, connection pooling, rate limiting&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;5-Table Schema:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;memories - Core memory storage with vector embeddings&lt;/li&gt; &lt;li&gt;sessions - Session management and MARM activation state&lt;/li&gt; &lt;li&gt;Log_entries - Structured session logs with auto-dating&lt;/li&gt; &lt;li&gt;notebook_entries - Reusable instructions with semantic search&lt;/li&gt; &lt;li&gt;User_settings - Configuration and preferences &lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;18 complete MCP tools. FastAPI backend. Docker-ready. 8 months of building.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;What Users Are Saying:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;&lt;em&gt;MARM successfully handles our industrial automation workflows in production. Validated session management, persistent logging, and smart recall across container restarts. Reliably tracks complex technical decisions through deployment cycles&lt;/em&gt;.&amp;quot; ‚Äî &lt;a href="/u/Ophy21"&gt;u/Ophy21&lt;/a&gt; (Industrial Automation Engineer)&lt;/p&gt; &lt;p&gt;&amp;quot;&lt;em&gt;100% memory accuracy across 46 services. Semantic search and automated session logs made solving async and infrastructure issues far easier. Value Rating: 9.5/10 - indispensable for enterprise-grade memory&lt;/em&gt;.&amp;quot; ‚Äî &lt;a href="/u/joe_nyc"&gt;u/joe_nyc&lt;/a&gt; (DevOps/Infrastructure Engineer)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Problem:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm a builder, not an experienced marketer. I've spent 6 months building MARM into production-ready infrastructure (2,500+ lines, Pip and Docker deployment, semantic search working), but I have no idea how to get users.&lt;/p&gt; &lt;p&gt;I've tried:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Reddit posts&lt;/li&gt; &lt;li&gt;Twitter&lt;/li&gt; &lt;li&gt;Waiting for organic discovery (doesn't happen)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How do you market technical tools without it feeling spammy?&lt;/p&gt; &lt;p&gt;Also i am open to finding a marketing co-founder who can help take this to the next level. I can build, but this project deserves better visibility than I can give it alone.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker pull lyellr88/marm-mcp-server:latest docker run -d -p 8001:8001 -v ~/.marm:/home/marm/.marm lyellr88/marm-mcp-server:latest claude mcp add --transport http marm-memory http://localhost:8001/mcp pip install marm-mcp-server==2.2.6 marm-mcp-server claude mcp add --transport http marm-memory http://localhost:8001/mcp &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Biscotti6145"&gt; /u/Alone-Biscotti6145 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o42jnm/built_an_mcp_tool_that_connects_qwen_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o42jnm/built_an_mcp_tool_that_connects_qwen_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o42jnm/built_an_mcp_tool_that_connects_qwen_claude_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-11T18:11:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3waak</id>
    <title>Anyone here building Agentic AI into their office workflow? How‚Äôs it going so far?</title>
    <updated>2025-10-11T13:56:32+00:00</updated>
    <author>
      <name>/u/Savings-Internal-297</name>
      <uri>https://old.reddit.com/user/Savings-Internal-297</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, is anyone here integrating Agentic AI into their office workflow or internal operations? If yes, how successful has it been so far?&lt;/p&gt; &lt;p&gt;Would like to hear what kind of use cases you are focusing on (automation, document handling, task management,) and what challenges or success you have seen.&lt;/p&gt; &lt;p&gt;Trying to get some real world insights before we start experimenting with it in our company.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savings-Internal-297"&gt; /u/Savings-Internal-297 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3waak/anyone_here_building_agentic_ai_into_their_office/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o3waak/anyone_here_building_agentic_ai_into_their_office/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o3waak/anyone_here_building_agentic_ai_into_their_office/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-11T13:56:32+00:00</published>
  </entry>
</feed>
