<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-21T11:27:18+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qepvki</id>
    <title>Do you actually need prompt engineering to get value from AI?</title>
    <updated>2026-01-16T19:36:53+00:00</updated>
    <author>
      <name>/u/Xthebuilder</name>
      <uri>https://old.reddit.com/user/Xthebuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been using AI daily for about 6 months while building a local AI inferencing app, and one thing that surprised me is how little prompt engineering mattered compared to other factors.&lt;/p&gt; &lt;p&gt;What ended up making the biggest difference for me was:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;giving the model enough &lt;strong&gt;context&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;iterating on ideas &lt;em&gt;with&lt;/em&gt; the model before writing real code&lt;/li&gt; &lt;li&gt;choosing models that are actually good at the specific task&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Because LLMs have some randomness, I found they’re most useful early on, when you’re still figuring things out. Iterating with the model helped surface bad assumptions before I committed to an approach. They’re especially good at starting broad and narrowing down if you keep the conversation going so context builds up.&lt;/p&gt; &lt;p&gt;When I add new features now, I don’t explain my app’s architecture anymore. I just link the relevant GitHub repos so the model can see how things are structured. That alone cut feature dev time from weeks to about a day in one case.&lt;/p&gt; &lt;p&gt;I’m not saying prompt engineering is useless, just that for most practical work, context, iteration, and model choice mattered more for me.&lt;/p&gt; &lt;p&gt;Curious how others here approach this. Has prompt engineering been critical for you, or have you seen similar results?&lt;/p&gt; &lt;p&gt;(I wrote up the full experience here if anyone wants more detail: &lt;a href="https://xthebuilder.github.io"&gt;https://xthebuilder.github.io&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xthebuilder"&gt; /u/Xthebuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qepvki/do_you_actually_need_prompt_engineering_to_get/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-16T19:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfauq9</id>
    <title>Help a noob figure out how to achieve something in a game engine with Ollama</title>
    <updated>2026-01-17T11:26:10+00:00</updated>
    <author>
      <name>/u/MountainPlantation</name>
      <uri>https://old.reddit.com/user/MountainPlantation</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I want to use Ollama to integrate it with a game engine. It's already in the engine and working, but I have some questions on what model I should use, and any tips in general for the experiments I want to do. I understand most LLMs running locally will take a while to think and generate a response, but for now let's ignore that.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;NPC Chat with commands: I know most people have tried doing NPC chatbots in engines, but I was thinking I could try to spice that up by integrating commands on it. Like the LLM would have a list of commands, given by me, that it could use contextually, like /laugh /cry /givePlayer(item), things like that. And I can make a system that parses the string and extracts/executes the commands. I attempted this one time, not in engine, just by using regular chat GPT and it would eventually come up with its own commands that were not stipulated by me. How to avoid that? Is there a model I should use for that?&lt;/li&gt; &lt;li&gt;NPC consistency in character. I also tried one time to keep chat GPT in character, a peasant from the medieval ages, but I would ask about modern events like COVID and it would eventually break and talk about it as if he knew what it was.&lt;/li&gt; &lt;li&gt;NPC Memory. What if I wanted to have NPCs remember things they have witnessed? I imagine I should make a log system that keeps every action done to that npc (NPC was hit by Player. NPC killed bandit. NPC found 1 gold etc) and then adding it to the beggining of the prompt as a little memory. Is that enough?&lt;/li&gt; &lt;li&gt;Can I reliably limit the response length or is it finicky? Like, setting a limit of how many words per response &lt;/li&gt; &lt;li&gt;Is there a way to guarantee responses are always in character? Because sometimes some of the LLMs will say &amp;quot;I cannot answer to things related to that&amp;quot; and that would be a big immersion breaker &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And another general question, is there a way to train certain models to get them used to a certani context? like i said, using commands I create in my game, or training them to act like a specific type of character etc.&lt;/p&gt; &lt;p&gt;Again, other than my experiments with just the chat GPT window, I am pretty new to this. If you have advice on what models to use or best practices, I'm listening.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MountainPlantation"&gt; /u/MountainPlantation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qfauq9/help_a_noob_figure_out_how_to_achieve_something/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-17T11:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfzbd4</id>
    <title>Ollama not detecting intel arc graphics</title>
    <updated>2026-01-18T04:52:04+00:00</updated>
    <author>
      <name>/u/Titanlucifer18</name>
      <uri>https://old.reddit.com/user/Titanlucifer18</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Thinkpad E14 G7, with intel core ultra 5 225H processor, running Fedora 43. I tried to install Ollama, but it did not detect any GPU. I tried to search docs but couldn’t find anything, or maybe I weren’t looking at the right place.&lt;/p&gt; &lt;p&gt;If anyone can guide me would be helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Titanlucifer18"&gt; /u/Titanlucifer18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfzbd4/ollama_not_detecting_intel_arc_graphics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfzbd4/ollama_not_detecting_intel_arc_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qfzbd4/ollama_not_detecting_intel_arc_graphics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T04:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfzb7g</id>
    <title>Ollama not detecting intel arc graphics</title>
    <updated>2026-01-18T04:51:50+00:00</updated>
    <author>
      <name>/u/Titanlucifer18</name>
      <uri>https://old.reddit.com/user/Titanlucifer18</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Thinkpad E14 G7, with intel core ultra 5 225H processor, running Fedora 43. I tried to install Ollama, but it did not detect any GPU. I tried to search docs but couldn’t find anything, or maybe I weren’t looking at the right place.&lt;/p&gt; &lt;p&gt;If anyone can guide me would be helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Titanlucifer18"&gt; /u/Titanlucifer18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfzb7g/ollama_not_detecting_intel_arc_graphics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfzb7g/ollama_not_detecting_intel_arc_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qfzb7g/ollama_not_detecting_intel_arc_graphics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T04:51:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qg9pek</id>
    <title>[D] Validate Production GenAI Challenges - Seeking Feedback</title>
    <updated>2026-01-18T14:17:25+00:00</updated>
    <author>
      <name>/u/No_Barracuda_415</name>
      <uri>https://old.reddit.com/user/No_Barracuda_415</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Guys,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Quick Backstory:&lt;/strong&gt; While working on LLMOps in past 2 years, I felt chaos with massive LLM workflows where costs exploded without clear attribution(which agent/prompt/retries?), silent sensitive data leakage and compliance had no replayable audit trails. Peers in other teams and externally felt the same: fragmented tools (metrics but not LLM aware), no real-time controls and growing risks with scaling. We felt the major need was &lt;strong&gt;control over costs, security and auditability without overhauling with multiple stacks/tools or adding latency&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problems we're seeing:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Unexplained LLM Spend:&lt;/strong&gt; Total bill known, but no breakdown by model/agent/workflow/team/tenant. Inefficient prompts/retries hide waste.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Silent Security Risks:&lt;/strong&gt; PII/PHI/PCI, API keys, prompt injections/jailbreaks slip through without real-time detection/enforcement.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No Audit Trail:&lt;/strong&gt; Hard to explain AI decisions (prompts, tools, responses, routing, policies) to Security/Finance/Compliance.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Does this resonate with anyone running GenAI workflows/multi-agents?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Few open questions I am having:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is this problem space worth pursuing in production GenAI?&lt;/li&gt; &lt;li&gt;Biggest challenges in cost/security observability to prioritize?&lt;/li&gt; &lt;li&gt;Are there other big pains in observability/governance I'm missing?&lt;/li&gt; &lt;li&gt;How do you currently hack around these (custom scripts, LangSmith, manual reviews)?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Barracuda_415"&gt; /u/No_Barracuda_415 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qg9pek/d_validate_production_genai_challenges_seeking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qg9pek/d_validate_production_genai_challenges_seeking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qg9pek/d_validate_production_genai_challenges_seeking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T14:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfufg3</id>
    <title>Claude Code with Anthropic API compatibility</title>
    <updated>2026-01-18T01:01:12+00:00</updated>
    <author>
      <name>/u/GhettoFob</name>
      <uri>https://old.reddit.com/user/GhettoFob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qfufg3/claude_code_with_anthropic_api_compatibility/"&gt; &lt;img alt="Claude Code with Anthropic API compatibility" src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="Claude Code with Anthropic API compatibility" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhettoFob"&gt; /u/GhettoFob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/blog/claude"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qfufg3/claude_code_with_anthropic_api_compatibility/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qfufg3/claude_code_with_anthropic_api_compatibility/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T01:01:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgbl6g</id>
    <title>(linux) i'm interested in historical roleplay (1600s)/early modern period), what would be your setup ?</title>
    <updated>2026-01-18T15:33:57+00:00</updated>
    <author>
      <name>/u/Mid-Pri6170</name>
      <uri>https://old.reddit.com/user/Mid-Pri6170</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;my longer term goal is to use gemini or other ai to make a little isometric world in Godot i can explore.&lt;/p&gt; &lt;p&gt;yesterday gemini had me instal olama and lama3 on my pc. &lt;/p&gt; &lt;p&gt;i only ran it in the terminal, but i am interested in what other things to consider to make it emersive.... considering cgpt etc are nerf'd&lt;/p&gt; &lt;p&gt;Gemini suggest Dolphin, Qwen and Nemo models too. however i was wondering if these models have a lot obscure trivia, knowledge of the period, language etc in them like the big llms do, otherwise they will quickly sound stale.&lt;/p&gt; &lt;p&gt;i was thinking there might be a specially trained model on period language/literature?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mid-Pri6170"&gt; /u/Mid-Pri6170 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgbl6g/linux_im_interested_in_historical_roleplay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgbl6g/linux_im_interested_in_historical_roleplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qgbl6g/linux_im_interested_in_historical_roleplay/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-18T15:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgythz</id>
    <title>Handle files with ollama SDK</title>
    <updated>2026-01-19T08:35:24+00:00</updated>
    <author>
      <name>/u/mr_dattebayo</name>
      <uri>https://old.reddit.com/user/mr_dattebayo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I have a question regarding file attachments in the new ollama desktop app.&lt;br /&gt; I have been evaluating different models via the app for an inferrence task on a large JSON file, which gave me good results.&lt;/p&gt; &lt;p&gt;But I actually need to use the ollama sdk to prompt the models and neither SDK nor the REST api offer the option to pass files. Directly appending the file content in the prompt is producing far worse results. So I am looking for a way to get the same results as with the desktop app.&lt;/p&gt; &lt;p&gt;Does anyone know how ollama handles file attachments in the desktop app or can point me into the right direction on how to get the same outcome when using the SDK?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_dattebayo"&gt; /u/mr_dattebayo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgythz/handle_files_with_ollama_sdk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgythz/handle_files_with_ollama_sdk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qgythz/handle_files_with_ollama_sdk/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T08:35:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgtc9i</id>
    <title>Summary and Tagging</title>
    <updated>2026-01-19T03:39:53+00:00</updated>
    <author>
      <name>/u/FlibblesHexEyes</name>
      <uri>https://old.reddit.com/user/FlibblesHexEyes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all; I don't usually use LLM's so thought I'd ask here if I'm doing this correctly - or if there is a better way to do it.&lt;/p&gt; &lt;p&gt;I run the Hasheous project - the idea is that if you supply an MD5/SHA1 hash, Hasheous can respond with mappings to video game metadata suppliers such as IGDB and others.&lt;/p&gt; &lt;p&gt;Just as a &amp;quot;I thought this might be a cool addition&amp;quot; feature, I wanted to add descriptions and tags to each record generated from the mapped metadata sources so that I could provide data to ROM management apps to provide similar games and the basis of a game recommendation engine.&lt;/p&gt; &lt;p&gt;I don't have the budget for offloading this to commercial AI providers (this is a free open source project), so I'm going with a distributed model where anyone could download an agent to use their own installation of Ollama to generate the description and tags.&lt;/p&gt; &lt;p&gt;With the help of Copilot, I came up with the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;pull the description for each mapped data source (IGDB, GiantBomb, Wikipedia, etc) and add them as embedded content. Copilot recommended the nomic-embed-text model to generate the vectors.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;run a CosineSimularity over the response to extract the top &lt;code&gt;x&lt;/code&gt; results (I won't pretend to understand how this function works!)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;run this through a prompt generator which generates a string with the top &lt;code&gt;x&lt;/code&gt; embeddings under the heading &amp;quot;Context:&amp;quot;, and the the prompts below under the heading &amp;quot;Instructions:&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;call the &lt;code&gt;/generate&lt;/code&gt; endpoint with the RAG prompt generator to create the response&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For the description model, I'm using Gemma3:12b, with the prompt: ``` Generate a detailed description/synopsis for the game &amp;lt;DATA_OBJECT_NAME&amp;gt; for &amp;lt;DATA_OBJECT_PLATFORM&amp;gt;.&lt;/p&gt; &lt;p&gt;If present; use the Wikipedia source as context for the other provided sources.&lt;/p&gt; &lt;p&gt;You MUST respond only with the description/synopsis. Do not acknowledge you've received this request.&lt;/p&gt; &lt;p&gt;The description should be engaging and informative, highlighting plot, key features, and gameplay. Keep the description concise, ideally between 150 to 200 words, but no more than 250 words. The output should be in markdown format. ```&lt;/p&gt; &lt;p&gt;For the tags model, I'm using qwen3:8b, with the prompt: ``` You are an expert whose responsibility is to help with automatic tagging for a game recommendation engine.&lt;/p&gt; &lt;p&gt;Generate detailed tags for the game &amp;lt;DATA_OBJECT_NAME&amp;gt; for &amp;lt;DATA_OBJECT_PLATFORM&amp;gt;.&lt;/p&gt; &lt;p&gt;If present; use the Wikipedia source as context for the other provided sources.&lt;/p&gt; &lt;p&gt;The tags should accurately represent the game. Only generate tags in the following categories: Genre, Gameplay, Features, Theme, Perspective, and Art Style.&lt;/p&gt; &lt;p&gt;Each tag should be no more than three words long. Ensure each tag is specific and commonly used within the gaming community, but avoid overly broad or generic terms. If you are unable to generate tags relevant to the category, leave it empty.&lt;/p&gt; &lt;p&gt;Generate a minimum of three tags and a maximum of ten tags per category.&lt;/p&gt; &lt;p&gt;Format the output as a raw JSON object, containing an array of tags for each category.&lt;/p&gt; &lt;p&gt;Make sure the JSON is properly structured and valid.&lt;/p&gt; &lt;p&gt;Example output: { &amp;quot;Genre&amp;quot;: [ &amp;quot;Action&amp;quot;, &amp;quot;Adventure&amp;quot; ], &amp;quot;Gameplay&amp;quot;: [ &amp;quot;Open World&amp;quot;, &amp;quot;Multiplayer&amp;quot; ], &amp;quot;Features&amp;quot;: [ &amp;quot;Crafting&amp;quot;, &amp;quot;Character Customization&amp;quot; ], &amp;quot;Theme&amp;quot;: [ &amp;quot;Sci-Fi&amp;quot;, &amp;quot;Fantasy&amp;quot; ], &amp;quot;Perspective&amp;quot;: [ &amp;quot;First-Person&amp;quot;, &amp;quot;Third-Person&amp;quot; ], &amp;quot;Art Style&amp;quot;: [ &amp;quot;Realistic&amp;quot;, &amp;quot;Pixel Art&amp;quot; ] }&lt;/p&gt; &lt;p&gt;Do not include any additional text or content outside of the JSON object. ```&lt;/p&gt; &lt;p&gt;Example output here: &lt;a href="https://beta.hasheous.org/index.html?page=dataobjectdetail&amp;amp;type=game&amp;amp;id=109"&gt;https://beta.hasheous.org/index.html?page=dataobjectdetail&amp;amp;type=game&amp;amp;id=109&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was wondering if anyone had any advice or suggestions to make this process faster or more accurate - or just better :)&lt;/p&gt; &lt;p&gt;Currently takes about 3 minutes per game on my GTX970 (my best GPU sadly) to generate the description and tags, so performance improvements would also be appreciated.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlibblesHexEyes"&gt; /u/FlibblesHexEyes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgtc9i/summary_and_tagging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgtc9i/summary_and_tagging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qgtc9i/summary_and_tagging/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T03:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh1k45</id>
    <title>LocalCopilot</title>
    <updated>2026-01-19T11:18:37+00:00</updated>
    <author>
      <name>/u/Huzaifa_Tech</name>
      <uri>https://old.reddit.com/user/Huzaifa_Tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using Copilot with the Sonnet-4 agent. It works very fast and performs coding tasks well while understanding context, but it is expensive for day-to-day coding and development.&lt;/p&gt; &lt;p&gt;What should I do if I want to run LLMs locally that work similarly to Sonnet-4 and can also understand context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Huzaifa_Tech"&gt; /u/Huzaifa_Tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qh1k45/localcopilot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qh1k45/localcopilot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qh1k45/localcopilot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T11:18:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhirii</id>
    <title>Electricity saving</title>
    <updated>2026-01-19T22:21:59+00:00</updated>
    <author>
      <name>/u/Original-Chapter-112</name>
      <uri>https://old.reddit.com/user/Original-Chapter-112</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Original-Chapter-112"&gt; /u/Original-Chapter-112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/homeassistant/comments/1qhibzi/electricity_saving/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhirii/electricity_saving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qhirii/electricity_saving/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T22:21:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhj3vw</id>
    <title>Has anyone got Ollama to work on an Arc Pro B50 in a proxmox VM?</title>
    <updated>2026-01-19T22:35:11+00:00</updated>
    <author>
      <name>/u/gregusmeus</name>
      <uri>https://old.reddit.com/user/gregusmeus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve tried a dozen ways to try and get ollama to see the GPU but it’s refusing. Any help gratefully received. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gregusmeus"&gt; /u/gregusmeus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhj3vw/has_anyone_got_ollama_to_work_on_an_arc_pro_b50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhj3vw/has_anyone_got_ollama_to_work_on_an_arc_pro_b50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qhj3vw/has_anyone_got_ollama_to_work_on_an_arc_pro_b50/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T22:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhjxf7</id>
    <title>M5 Metal compilation error</title>
    <updated>2026-01-19T23:07:09+00:00</updated>
    <author>
      <name>/u/sidanos</name>
      <uri>https://old.reddit.com/user/sidanos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I’m running into a reproducible crash with Ollama on macOS after updating to macOS 26.2 (Build 25C56) on an Apple M5 machine.&lt;/p&gt; &lt;p&gt;Everything worked fine yesterday. Today, any attempt to run Llama 3.1 with GPU (Metal) fails during Metal library initialization.&lt;/p&gt; &lt;p&gt;Environment&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• macOS: 26.2 (25C56) • Hardware: Apple M5 • Ollama: 0.14.x (Homebrew) • Model: llama3.1:latest, llama3.1:8b, llama3.1:8b- &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;instruct (all fail the same way)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• Xcode Command Line Tools: updated • Rebooted: yes &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Has anyone countered this? And maybe has workaround?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sidanos"&gt; /u/sidanos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhjxf7/m5_metal_compilation_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhjxf7/m5_metal_compilation_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qhjxf7/m5_metal_compilation_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T23:07:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgxulm</id>
    <title>Would Anthropic Block Ollama?</title>
    <updated>2026-01-19T07:36:07+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Few hours ago, Ollama announced following:&lt;/p&gt; &lt;p&gt;Ollama now has Anthropic API compatibility. This enables tools like Claude Code to be used with open-source models.&lt;/p&gt; &lt;p&gt;Ollama Blog: &lt;a href="https://ollama.com/blog/claude"&gt;Claude Code with Anthropic API compatibility · Ollama Blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hands-on Guide: &lt;a href="https://youtu.be/Pbsn-6JEE2s?si=7pdAv5LU9GiBx7aN"&gt;https://youtu.be/Pbsn-6JEE2s?si=7pdAv5LU9GiBx7aN&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For now it's working but for how long?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgxulm/would_anthropic_block_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qgxulm/would_anthropic_block_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qgxulm/would_anthropic_block_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T07:36:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh10xr</id>
    <title>Demo: On-device browser agent (Qwen) running locally in Chrome</title>
    <updated>2026-01-19T10:48:47+00:00</updated>
    <author>
      <name>/u/thecoder12322</name>
      <uri>https://old.reddit.com/user/thecoder12322</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qh10xr/demo_ondevice_browser_agent_qwen_running_locally/"&gt; &lt;img alt="Demo: On-device browser agent (Qwen) running locally in Chrome" src="https://external-preview.redd.it/2j5zq51thj98y10lUAVh-0Pz732bAfFjUfnIlXTUBeM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b649fa820188efe6eb47208c536eb1bdfe9933d2" title="Demo: On-device browser agent (Qwen) running locally in Chrome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecoder12322"&gt; /u/thecoder12322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ljp6zwzfcaeg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qh10xr/demo_ondevice_browser_agent_qwen_running_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qh10xr/demo_ondevice_browser_agent_qwen_running_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T10:48:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhvtnp</id>
    <title>timed out waiting for llama runner to start: context canceled</title>
    <updated>2026-01-20T08:41:50+00:00</updated>
    <author>
      <name>/u/DerZwirbel</name>
      <uri>https://old.reddit.com/user/DerZwirbel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; I’m seeing intermittent model load failures with &lt;strong&gt;Ollama 0.13.4&lt;/strong&gt; running in &lt;strong&gt;Docker&lt;/strong&gt; when loading &lt;strong&gt;phi4&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Error excerpt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;time=2026-01-20T08:17:55.413Z level=INFO source=sched.go:470 msg=&amp;quot;Load failed&amp;quot; model=/data/ollama/blobs/sha256-fd7b6731c33c57f61767612f56517460ec2d1e2e5a3f0163e0eb3d8d8cb5df20 error=&amp;quot;timed out waiting for llama runner to start: context canceled&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This typically happens during container startup or under load.&lt;br /&gt; CUDA is available, but the failure is non-deterministic (cold start related?).&lt;/p&gt; &lt;p&gt;Has anyone seen this with phi4 recently, or has guidance on? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DerZwirbel"&gt; /u/DerZwirbel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhvtnp/timed_out_waiting_for_llama_runner_to_start/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhvtnp/timed_out_waiting_for_llama_runner_to_start/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qhvtnp/timed_out_waiting_for_llama_runner_to_start/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-20T08:41:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qift89</id>
    <title>This was created by my autonomous enhanced programmer, it is no longer for sale.</title>
    <updated>2026-01-20T22:31:04+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qift89/this_was_created_by_my_autonomous_enhanced/"&gt; &lt;img alt="This was created by my autonomous enhanced programmer, it is no longer for sale." src="https://preview.redd.it/43lapgdkykeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=795d987a6ceb4c7d009870904f10a54410329127" title="This was created by my autonomous enhanced programmer, it is no longer for sale." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NeuralNet – Your Intelligent Communication Assistant**&lt;/p&gt; &lt;p&gt;Imagine having your own intelligent assistant that understands your speech, translates languages, and gives you instant access to information. That’s exactly what NeuralNet offers you! This powerful application, created in LM Studio, acts as a flexible server that allows you to communicate with AI that is constantly learning and adapting.&lt;/p&gt; &lt;p&gt;**Here’s what NeuralNet can do for you:**&lt;/p&gt; &lt;p&gt;* **Seamless Text Communication:** Just type your questions or instructions – NeuralNet responds in natural language.&lt;/p&gt; &lt;p&gt;* **Diverse and Intensive Internet Search:** NeuralNet actively searches the Internet to provide you with up-to-date information and answers without the need for links.&lt;/p&gt; &lt;p&gt;* **Multi-Language Support:** Simply set your preferred language (including English!) for optimal communication and translations.&lt;/p&gt; &lt;p&gt;* **Off-Pc Usage:** Thanks to APIs like Engrok, you can also use NeuralNet on your mobile! When your computer is on, NeuralNet is also available offline. You can use the local model directly installed on your device.&lt;/p&gt; &lt;p&gt;* **Creative Translations and Contextual Understanding:** From slang terms to more complex phrases, NeuralNet can translate accurately and with nuance.&lt;/p&gt; &lt;p&gt;**Key Features:**&lt;/p&gt; &lt;p&gt;* **Local Server Operation (LM Studio):** NeuralNet can run locally for maximum privacy and control.&lt;/p&gt; &lt;p&gt;* **API Integration:** Seamless access to external services, like Engrok, for remote use.&lt;/p&gt; &lt;p&gt;* **Continuous Learning:** NeuralNet is constantly improving its understanding based on your interactions.&lt;/p&gt; &lt;p&gt;**Ready to experience the future of communication? Start chatting with NeuralNet today!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/43lapgdkykeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qift89/this_was_created_by_my_autonomous_enhanced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qift89/this_was_created_by_my_autonomous_enhanced/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-20T22:31:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhr198</id>
    <title>GLM 4.7 is apparently almost ready on Ollama</title>
    <updated>2026-01-20T04:18:14+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/"&gt; &lt;img alt="GLM 4.7 is apparently almost ready on Ollama" src="https://a.thumbs.redditmedia.com/1aab6UqlDDpJ6gz379kpHuTSrReU-J_gAGirTkMBCr8.jpg" title="GLM 4.7 is apparently almost ready on Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's listed, just not downloadable yet. Trying in WebOllama, and in CLI gives weird excuses&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/96ly2bgckfeg1.png?width=1723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d8bfc2386dd789ef4f28ff0516de4893bf5c6772"&gt;https://preview.redd.it/96ly2bgckfeg1.png?width=1723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d8bfc2386dd789ef4f28ff0516de4893bf5c6772&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qhr198/glm_47_is_apparently_almost_ready_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-20T04:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh5z0j</id>
    <title>I built a voice-first AI mirror that runs fully on Ollama.</title>
    <updated>2026-01-19T14:43:25+00:00</updated>
    <author>
      <name>/u/DirectorChance4012</name>
      <uri>https://old.reddit.com/user/DirectorChance4012</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qh5z0j/i_built_a_voicefirst_ai_mirror_that_runs_fully_on/"&gt; &lt;img alt="I built a voice-first AI mirror that runs fully on Ollama." src="https://external-preview.redd.it/Y3p4ZmcwM3FpYmVnMXwaGz6cYTO_1FXYid56crf85mAMdgg0ECh85UXNOmp7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a490bc51d40c3d188f9b4edee608ffa0b05365bf" title="I built a voice-first AI mirror that runs fully on Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a voice-first AI mirror that runs fully on Ollama.&lt;/p&gt; &lt;p&gt;The idea was to explore what a “voice-native” interface looks like&lt;/p&gt; &lt;p&gt;when it’s ambient and always there — not a chat window.&lt;/p&gt; &lt;p&gt;Everything runs locally (LLM via Ollama), no cloud dependency.&lt;/p&gt; &lt;p&gt;Still very experimental, but surprisingly usable.&lt;/p&gt; &lt;p&gt;Blog (how it works + design decisions):&lt;/p&gt; &lt;p&gt;&lt;a href="https://noted.lol/mirrormate/"&gt;https://noted.lol/mirrormate/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub (WIP, self-hostable):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/orangekame3/mirrormate"&gt;https://github.com/orangekame3/mirrormate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DirectorChance4012"&gt; /u/DirectorChance4012 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bjeyts2qibeg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qh5z0j/i_built_a_voicefirst_ai_mirror_that_runs_fully_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qh5z0j/i_built_a_voicefirst_ai_mirror_that_runs_fully_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-19T14:43:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi6ngs</id>
    <title>Model choice for big (huge) text-based data search and analysis</title>
    <updated>2026-01-20T17:01:24+00:00</updated>
    <author>
      <name>/u/qball2kb</name>
      <uri>https://old.reddit.com/user/qball2kb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I’m looking at setting up an Ollama model to assist non-technical folks with searching through some massive (potentially greater than terabyte-sized), text-based datasets (stored locally in TSV/CSV, SQLite and similar formats). It would ideally be run completely offline. Is there a particular model that does this sort of thing well? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qball2kb"&gt; /u/qball2kb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qi6ngs/model_choice_for_big_huge_textbased_data_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qi6ngs/model_choice_for_big_huge_textbased_data_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qi6ngs/model_choice_for_big_huge_textbased_data_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-20T17:01:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi8ouc</id>
    <title>Plano 0.4.3 ⭐️ Filter Chains via MCP and OpenRouter Integration</title>
    <updated>2026-01-20T18:12:47+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qi8ouc/plano_043_filter_chains_via_mcp_and_openrouter/"&gt; &lt;img alt="Plano 0.4.3 ⭐️ Filter Chains via MCP and OpenRouter Integration" src="https://preview.redd.it/o590ks78pjeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4bc3a071d59ce6ea4b05664dd2470be1e05764a" title="Plano 0.4.3 ⭐️ Filter Chains via MCP and OpenRouter Integration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey peeps - excited to ship &lt;a href="https://github.com/katanemo/plano"&gt;Plano&lt;/a&gt; 0.4.3. Two critical updates that I think could be helpful for developers.&lt;/p&gt; &lt;p&gt;1/Filter Chains&lt;/p&gt; &lt;p&gt;Filter chains are Plano’s way of capturing &lt;strong&gt;reusable workflow steps&lt;/strong&gt; in the data plane, without duplication and coupling logic into application code. A filter chain is an ordered list of &lt;strong&gt;mutations&lt;/strong&gt; that a request flows through before reaching its final destination —such as an agent, an LLM, or a tool backend. Each filter is a network-addressable service/path that can:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Inspect the incoming prompt, metadata, and conversation state.&lt;/li&gt; &lt;li&gt;Mutate or enrich the request (for example, rewrite queries or build context).&lt;/li&gt; &lt;li&gt;Short-circuit the flow and return a response early (for example, block a request on a compliance failure).&lt;/li&gt; &lt;li&gt;Emit structured logs and traces so you can debug and continuously improve your agents.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In other words, filter chains provide a lightweight programming model over HTTP for building reusable steps in your agent architectures.&lt;/p&gt; &lt;p&gt;2/ Passthrough Client Bearer Auth&lt;/p&gt; &lt;p&gt;When deploying Plano in front of LLM proxy services that manage their own API key validation (such as LiteLLM, OpenRouter, or custom gateways), users currently have to configure a static access_key. However, in many cases, it's desirable to forward the client's original Authorization header instead. This allows the upstream service to handle per-user authentication, rate limiting, and virtual keys.&lt;/p&gt; &lt;p&gt;0.4.3 introduces a passthrough_auth option iWhen set to true, Plano will forward the client's Authorization header to the upstream instead of using the configured access_key.&lt;/p&gt; &lt;p&gt;Use Cases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;OpenRouter: Forward requests to OpenRouter with per-user API keys.&lt;/li&gt; &lt;li&gt;Multi-tenant Deployments: Allow different clients to use their own credentials via Plano.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Hope you all enjoy these updates&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o590ks78pjeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qi8ouc/plano_043_filter_chains_via_mcp_and_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qi8ouc/plano_043_filter_chains_via_mcp_and_openrouter/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-20T18:12:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qidbdr</id>
    <title>Local LLM (16GBRAM + 8VRAM) for gamedev</title>
    <updated>2026-01-20T20:58:38+00:00</updated>
    <author>
      <name>/u/RagingBass2020</name>
      <uri>https://old.reddit.com/user/RagingBass2020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a developer that has been doing gamedev for 2 years but I used to be a backend developer for almost 10 years and a CS researcher before that.&lt;/p&gt; &lt;p&gt;I use mostly Unity and Jetbrains Rider. &lt;/p&gt; &lt;p&gt;Although I have a computer with more RAM at home, I need something that runs on a 16+8 GB laptop.&lt;/p&gt; &lt;p&gt;I don't want to use it to develop full systems. I want something that is decent enough to create boilerplate code and help with some scripts and maybe some stuff I'm less used to (getting ready for the global game jam).&lt;/p&gt; &lt;p&gt;It needs to run offline with no access to the internet. I'm using ollama but I also have ComfyUI for some uni classes I was taking last semester.&lt;/p&gt; &lt;p&gt;If anyone could give me recommendations, I'd appreciate it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RagingBass2020"&gt; /u/RagingBass2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qidbdr/local_llm_16gbram_8vram_for_gamedev/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qidbdr/local_llm_16gbram_8vram_for_gamedev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qidbdr/local_llm_16gbram_8vram_for_gamedev/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-20T20:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qijhth</id>
    <title>Weekend Project: An Open-Source Claude Cowork That Can Handle Skills</title>
    <updated>2026-01-21T00:59:50+00:00</updated>
    <author>
      <name>/u/Frequent_Cash2598</name>
      <uri>https://old.reddit.com/user/Frequent_Cash2598</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent last weekend building something I had been thinking about for a while. Claude Cowork is great, but I wanted an open-source, lightweight version that could run with any model, so I created Open Cowork.&lt;/p&gt; &lt;p&gt;It's written entirely in Rust, which I had never used before. Starting from scratch meant no heavy dependencies, no Python bloat, and no reliance on existing agent SDKs. Just a tiny, fast binary that works anywhere.&lt;/p&gt; &lt;p&gt;Security was a big concern since the agents can execute code. Open Cowork handles this by running tasks inside temporary Docker containers. Everything stays isolated, but you can still experiment freely.&lt;/p&gt; &lt;p&gt;You can plug in any model you want. OpenAI, Anthropic, or even fully offline LLMs through Ollama are all supported. You keep full control over your API keys and your data.&lt;/p&gt; &lt;p&gt;It already comes with built-in skills for handling documents like PDFs and Excel files. I was surprised by how useful it became right away.&lt;/p&gt; &lt;p&gt;The development experience was wild. An AI agent helped me build a secure, open-source version of itself, and I learned Rust along the way. It was one of those projects where everything just clicked together in a weekend.&lt;/p&gt; &lt;p&gt;The code is live on GitHub: &lt;a href="https://github.com/kuse-ai/kuse_cowork"&gt;https://github.com/kuse-ai/kuse_cowork&lt;/a&gt; . It's still early, but I'd love to hear feedback from anyone who wants to try it out or contribute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frequent_Cash2598"&gt; /u/Frequent_Cash2598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qijhth/weekend_project_an_opensource_claude_cowork_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-21T00:59:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiv7v8</id>
    <title>I built a CLI tool using Ollama (nomic-embed-text) to replace grep with Semantic Code Search</title>
    <updated>2026-01-21T11:16:14+00:00</updated>
    <author>
      <name>/u/Technical_Meeting_81</name>
      <uri>https://old.reddit.com/user/Technical_Meeting_81</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I've been working on an open-source tool called &lt;strong&gt;GrepAI&lt;/strong&gt;, and I wanted to share it here because it relies heavily on &lt;strong&gt;Ollama&lt;/strong&gt; to function.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is it?&lt;/strong&gt; GrepAI is a CLI tool (written in Go) designed to help AI agents (like Claude Code, Cursor, or local agents) understand your codebase better.&lt;/p&gt; &lt;p&gt;Instead of using standard regex &lt;code&gt;grep&lt;/code&gt; to find code—which often misses the context—GrepAI uses &lt;strong&gt;Ollama&lt;/strong&gt; to generate local embeddings of your code. This allows you to perform &lt;strong&gt;semantic searches&lt;/strong&gt; directly from the terminal.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Core:&lt;/strong&gt; Written in Go.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Embeddings:&lt;/strong&gt; Connects to your local Ollama instance (defaults to &lt;code&gt;nomic-embed-text&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector Store:&lt;/strong&gt; In-memory / Local (fast and private).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why use Ollama for this?&lt;/strong&gt; I wanted a solution that respects privacy and doesn't cost a fortune in API credits just to index a repo. By using Ollama locally, GrepAI builds an index of your project (respecting &lt;code&gt;.gitignore&lt;/code&gt;) without your code leaving your machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-world Impact (Benchmark)&lt;/strong&gt; I tested this setup by using GrepAI as a filter for Claude Code (instead of the default grep). The idea was to let Ollama decide what files were relevant before sending them to the cloud. The results were huge:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;-97% Input Tokens&lt;/strong&gt; sent to the LLM (because Ollama filtered the noise).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;-27.5% Cost reduction&lt;/strong&gt; on the task.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even if you don't use Claude, this demonstrates how effective local embeddings (via Ollama) are at retrieving the right context for RAG applications.&lt;/p&gt; &lt;p&gt;👉 &lt;strong&gt;Benchmark details:&lt;/strong&gt;&lt;a href="https://yoanbernabeu.github.io/grepai/blog/benchmark-grepai-vs-grep-claude-code/"&gt;https://yoanbernabeu.github.io/grepai/blog/benchmark-grepai-vs-grep-claude-code/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;📦 &lt;strong&gt;GitHub:&lt;/strong&gt;&lt;a href="https://github.com/yoanbernabeu/grepai"&gt;https://github.com/yoanbernabeu/grepai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;📚 &lt;strong&gt;Docs:&lt;/strong&gt;&lt;a href="https://yoanbernabeu.github.io/grepai/"&gt;https://yoanbernabeu.github.io/grepai/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to know what other embedding models you guys are running with Ollama. Currently, &lt;code&gt;nomic-embed-text&lt;/code&gt; gives me the best results for code, but I'm open to suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical_Meeting_81"&gt; /u/Technical_Meeting_81 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qiv7v8/i_built_a_cli_tool_using_ollama_nomicembedtext_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qiv7v8/i_built_a_cli_tool_using_ollama_nomicembedtext_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qiv7v8/i_built_a_cli_tool_using_ollama_nomicembedtext_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-21T11:16:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiug46</id>
    <title>Fine-tuned Qwen3 0.6B for Text2SQL using a claude skill. The result tiny model matches a Deepseek 3.1 and runs locally on CPU.</title>
    <updated>2026-01-21T10:30:24+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qiug46/finetuned_qwen3_06b_for_text2sql_using_a_claude/"&gt; &lt;img alt="Fine-tuned Qwen3 0.6B for Text2SQL using a claude skill. The result tiny model matches a Deepseek 3.1 and runs locally on CPU." src="https://preview.redd.it/fy13421qjoeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5f9926ff9e7f48668957408f4d674a40d2079b2" title="Fine-tuned Qwen3 0.6B for Text2SQL using a claude skill. The result tiny model matches a Deepseek 3.1 and runs locally on CPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing a workflow for training custom models and deploying them to Ollama.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Base small models aren't great at specialized tasks. I needed Text2SQL and Qwen3 0.6B out of the box gave me things like:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sql -- Question: &amp;quot;Which artists have total album sales over 1 million?&amp;quot; SELECT artists.name FROM artists WHERE artists.genre IS NULL OR artists.country IS NULL; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Completely ignores the question. Fine-tuning is the obvious answer, but usually means setting up training infrastructure, formatting datasets, debugging CUDA errors...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The workflow I used:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;distil-cli with a Claude skill that handles the training setup, to get started I installed &lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;Setup&lt;/h1&gt; &lt;p&gt;curl -fsSL &lt;a href="https://cli-assets.distillabs.ai/install.sh"&gt;https://cli-assets.distillabs.ai/install.sh&lt;/a&gt; | sh distil login&lt;/p&gt; &lt;h1&gt;In Claude Code — add the skill&lt;/h1&gt; &lt;p&gt;/plugin marketplace add &lt;a href="https://github.com/distil-labs/distil-cli-skill"&gt;https://github.com/distil-labs/distil-cli-skill&lt;/a&gt; /plugin install distil-cli@distil-cli-skill ```&lt;/p&gt; &lt;p&gt;And then, Claude guides me through the training workflow:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash 1. Create a model (`distil model create`) 2. Pick a task type (QA, classification, tool calling, or RAG) 3. Prepare data files (job description, config, train/test sets) 4. Upload data 5. Run teacher evaluation 6. Train the model 7. Download and deploy &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What training produces:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt; downloaded-model/ ├── model.gguf (2.2 GB) — quantized, Ollama-ready ├── Modelfile (system prompt baked in) ├── model_client.py (Python wrapper) ├── model/ (full HF format) └── model-adapter/ (LoRA weights if you want to merge yourself) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deploying to Ollama:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash ollama create my-text2sql -f Modelfile ollama run my-text2sql &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Custom fine-tuned model, running locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;LLM-as-a-Judge&lt;/th&gt; &lt;th&gt;ROUGE&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Base Qwen3 0.6B&lt;/td&gt; &lt;td&gt;36%&lt;/td&gt; &lt;td&gt;69.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek-V3 (teacher)&lt;/td&gt; &lt;td&gt;80%&lt;/td&gt; &lt;td&gt;88.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Fine-tuned 0.6B&lt;/td&gt; &lt;td&gt;74%&lt;/td&gt; &lt;td&gt;88.5%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Started at 36%, ended at 74% — nearly matching the teacher at a fraction of the size.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Before/after:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Question: &amp;quot;How many applicants applied for each position?&amp;quot;&lt;/p&gt; &lt;p&gt;Base: &lt;code&gt;sql SELECT COUNT(DISTINCT position) AS num_applicants FROM applicants; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Fine-tuned: &lt;code&gt;sql SELECT position, COUNT(*) AS applicant_count FROM applicants GROUP BY position; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo app:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Built a quick script that loads CSVs into SQLite and queries via the model:&lt;/p&gt; &lt;p&gt;```bash python app.py --csv employees.csv \ --question &amp;quot;What is the average salary per department?&amp;quot; --show-sql&lt;/p&gt; &lt;h1&gt;Generated SQL: SELECT department, AVG(salary) FROM employees GROUP BY department;&lt;/h1&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;All local.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fy13421qjoeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qiug46/finetuned_qwen3_06b_for_text2sql_using_a_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qiug46/finetuned_qwen3_06b_for_text2sql_using_a_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-21T10:30:24+00:00</published>
  </entry>
</feed>
