<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-11T17:36:41+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1q6ntug</id>
    <title>which small model can i use to read this gauge?</title>
    <updated>2026-01-07T18:45:38+00:00</updated>
    <author>
      <name>/u/Curious_Party_4683</name>
      <uri>https://old.reddit.com/user/Curious_Party_4683</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q6ntug/which_small_model_can_i_use_to_read_this_gauge/"&gt; &lt;img alt="which small model can i use to read this gauge?" src="https://b.thumbs.redditmedia.com/DLXVJ2GHyhFrw2D5yc48Nuqyw-zou_eamL2TJaVde3o.jpg" title="which small model can i use to read this gauge?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i tried &amp;quot;granite4:latest&amp;quot; on my i7 (7th gen intel) and the output i got was 5 in Home Assistant.&lt;/p&gt; &lt;p&gt;Google Gemini was spot on at &amp;quot;88&amp;quot; &lt;/p&gt; &lt;p&gt;is there a small model good for reading photos of gauges? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Curious_Party_4683"&gt; /u/Curious_Party_4683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q6ntug"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q6ntug/which_small_model_can_i_use_to_read_this_gauge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q6ntug/which_small_model_can_i_use_to_read_this_gauge/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-07T18:45:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1q762hz</id>
    <title>Practical checklist: approvals + audit logs for MCP tool-calling agents (GitHub/Jira/Slack)</title>
    <updated>2026-01-08T08:03:35+00:00</updated>
    <author>
      <name>/u/NoAdministration6906</name>
      <uri>https://old.reddit.com/user/NoAdministration6906</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;I‚Äôve been seeing more teams let agents call tools directly (GitHub/Jira/Slack). The failure mode is usually not ‚Äòagent had access‚Äô, it‚Äôs ‚Äòagent executed the wrong parameters‚Äô without a gate.&lt;/li&gt; &lt;li&gt;Here‚Äôs a practical checklist that reduces blast radius:&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;Separate &lt;strong&gt;agent identity&lt;/strong&gt; from &lt;strong&gt;tool credentials&lt;/strong&gt; (never hand PATs to agents)&lt;/li&gt; &lt;li&gt;Classify actions: &lt;strong&gt;Read / Write / Destructive&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Require &lt;strong&gt;payload-bound approvals&lt;/strong&gt; for Write/Destructive (approve exact params)&lt;/li&gt; &lt;li&gt;Store immutable &lt;strong&gt;audit trail&lt;/strong&gt; (request ‚Üí approval ‚Üí execution ‚Üí result)&lt;/li&gt; &lt;li&gt;Add &lt;strong&gt;rate limits&lt;/strong&gt; per user/workspace/tool&lt;/li&gt; &lt;li&gt;Redact secrets in logs; block suspicious tokens&lt;/li&gt; &lt;li&gt;Add policy defaults: PR create, Jira issue update, Slack channel changes = approval&lt;/li&gt; &lt;li&gt;Export logs for compliance (CSV is enough early).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;all this can be handled in &lt;a href="http://mcptoolgate.com"&gt;mcptoolgate.com&lt;/a&gt; mcp server.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Example policy: ‚Äúgithub.create_pr requires approval; github.search_issues does not.‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoAdministration6906"&gt; /u/NoAdministration6906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q762hz/practical_checklist_approvals_audit_logs_for_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q762hz/practical_checklist_approvals_audit_logs_for_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q762hz/practical_checklist_approvals_audit_logs_for_mcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-08T08:03:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7bd4q</id>
    <title>Need advice on packaging my app that uses two LLM's</title>
    <updated>2026-01-08T13:09:31+00:00</updated>
    <author>
      <name>/u/7_Taha</name>
      <uri>https://old.reddit.com/user/7_Taha</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/7_Taha"&gt; /u/7_Taha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LLMDevs/comments/1q7bcqv/need_advice_on_packaging_my_app_that_uses_two_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q7bd4q/need_advice_on_packaging_my_app_that_uses_two_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q7bd4q/need_advice_on_packaging_my_app_that_uses_two_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-08T13:09:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7c8rr</id>
    <title>What are people using for evals right now?</title>
    <updated>2026-01-08T13:48:29+00:00</updated>
    <author>
      <name>/u/FlimsyProperty8544</name>
      <uri>https://old.reddit.com/user/FlimsyProperty8544</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q7c8rr/what_are_people_using_for_evals_right_now/"&gt; &lt;img alt="What are people using for evals right now?" src="https://b.thumbs.redditmedia.com/K1LlcPiTGwRf2DtCGaY5j5i6MsUQndRAd9L82cjV9tw.jpg" title="What are people using for evals right now?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlimsyProperty8544"&gt; /u/FlimsyProperty8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AIEval/comments/1q7c7ss/what_are_people_using_for_evals_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q7c8rr/what_are_people_using_for_evals_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q7c8rr/what_are_people_using_for_evals_right_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-08T13:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7ihlb</id>
    <title>I built a Gmail AI extension that uses your own LLMs (Ollama, OpenRouter, n8n) to cut writing time by 75%. Is this something you‚Äôd use?</title>
    <updated>2026-01-08T17:47:03+00:00</updated>
    <author>
      <name>/u/smyoss</name>
      <uri>https://old.reddit.com/user/smyoss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q7ihlb/i_built_a_gmail_ai_extension_that_uses_your_own/"&gt; &lt;img alt="I built a Gmail AI extension that uses your own LLMs (Ollama, OpenRouter, n8n) to cut writing time by 75%. Is this something you‚Äôd use?" src="https://b.thumbs.redditmedia.com/hCPfYtSVIokfaMJ0sSFqbM1oeLB-LxbBGkYFbsGlaFo.jpg" title="I built a Gmail AI extension that uses your own LLMs (Ollama, OpenRouter, n8n) to cut writing time by 75%. Is this something you‚Äôd use?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smyoss"&gt; /u/smyoss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/selfhosted/comments/1q7ig8n/i_built_a_gmail_ai_extension_that_uses_your_own/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q7ihlb/i_built_a_gmail_ai_extension_that_uses_your_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q7ihlb/i_built_a_gmail_ai_extension_that_uses_your_own/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-08T17:47:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q79f82</id>
    <title>Rethinking RAG: How Agents Learn to Operate</title>
    <updated>2026-01-08T11:30:17+00:00</updated>
    <author>
      <name>/u/frank_brsrk</name>
      <uri>https://old.reddit.com/user/frank_brsrk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q79f82/rethinking_rag_how_agents_learn_to_operate/"&gt; &lt;img alt="Rethinking RAG: How Agents Learn to Operate" src="https://preview.redd.it/f6gc8q8k24cg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a7cca2c4c33c619434a13dd203060c2067a67f5" title="Rethinking RAG: How Agents Learn to Operate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Runtime Evolution, From Static to Dynamic Agents, Through Retrieval&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey reddit builders, &lt;/p&gt; &lt;p&gt;You have an agent. You add documents. You retrieve text. You paste it into context. And that‚Äôs supposed to make the agent better. It does help, but only in a narrow way. It adds facts. It doesn‚Äôt change how the agent actually operates.&lt;/p&gt; &lt;p&gt;What I eventually realized is that many of the failures we blame on models aren‚Äôt model problems at all. They‚Äôre architectural ones. Agents don‚Äôt fail because they lack intelligence. They fail because we force everything into the same flat space.&lt;/p&gt; &lt;p&gt;Knowledge, reasoning, behavior, safety, instructions, all blended together as if they play the same role. They don‚Äôt. The mistake we keep repeating In most systems today, retrieval is treated as one thing. Facts, examples, reasoning hints, safety rules, instructions. All retrieved the same way. Injected the same way. Given the same authority.&lt;/p&gt; &lt;p&gt;The result is agents that feel brittle. They overfit to prompts. They swing between being verbose and being rigid. They break the moment the situation changes. Not because the model is weak, but because we never taught the agent how to distinguish what is real from how to think and from what must be enforced.&lt;/p&gt; &lt;p&gt;Humans don‚Äôt reason this way. Agents shouldn‚Äôt either.&lt;/p&gt; &lt;p&gt;&lt;em&gt;put yourself in the pants of the agent&lt;/em&gt;&lt;/p&gt; &lt;p&gt;From content to structure At some point, I stopped asking ‚Äúwhat should I retrieve?‚Äù and started asking something else. What role does this information play in cognition?&lt;/p&gt; &lt;p&gt;That shift changes everything. Because not all information exists to do the same job. Some describes reality. Some shapes how we approach a problem. Some exists only to draw hard boundaries. What matters here isn‚Äôt any specific technique.&lt;/p&gt; &lt;p&gt;It‚Äôs the shift from treating retrieval as content to treating it as structure. Once you see that, everything else follows naturally. RAG stops being storage and starts becoming part of how thinking happens at runtime. Knowledge grounds, it doesn‚Äôt decide Knowledge answers one question: what is true. Facts, constraints, definitions, limits. All essential. None of them decide anything on their own.&lt;/p&gt; &lt;p&gt;When an agent hallucinates, it‚Äôs usually because knowledge is missing. When an agent reasons badly, it‚Äôs often because knowledge is being asked to do too much. Knowledge should ground the agent, not steer it.&lt;/p&gt; &lt;p&gt;When you keep knowledge factual and clean, it stops interfering with reasoning and starts stabilizing it. The agent doesn‚Äôt suddenly behave differently. It just stops guessing. This is the move from speculative to anchored.&lt;/p&gt; &lt;p&gt;Reasoning should be situational Most agents hard-code reasoning into the system prompt. That‚Äôs fragile by design. In reality, reasoning is situational. An agent shouldn‚Äôt always think analytically. Or experimentally. Or emotionally. It should choose how to approach a problem based on what‚Äôs happening.&lt;/p&gt; &lt;p&gt;This is where RAG becomes powerful in a deeper sense. Not as memory, but as recall of ways of thinking. You don‚Äôt retrieve answers. You retrieve approaches. These approaches don‚Äôt force behavior. They shape judgment. The agent still has discretion. It can adapt as context shifts. This is where intelligence actually emerges. The move from informed to intentional.&lt;/p&gt; &lt;p&gt;Control is not intelligence There are moments where freedom is dangerous. High stakes. Safety. Compliance. Evaluation. Sometimes behavior must be enforced. But control doesn‚Äôt create insight. It guarantees outcomes. When control is separated from reasoning, agents become more flexible by default, and enforcement becomes precise when it‚Äôs actually needed.&lt;/p&gt; &lt;p&gt;The agent still understands the situation. Its freedom is just temporarily narrowed. This doesn‚Äôt make the agent smarter. It makes it reliable under pressure. That‚Äôs the move from intentional to guaranteed.&lt;/p&gt; &lt;p&gt;How agents evolve Seen this way, an agent evolves in three moments. First, knowledge enters. The agent understands what is real. Then, reasoning enters. The agent knows how to approach the situation. Only if necessary, control enters. The agent must operate within limits. Each layer changes something different inside the agent.&lt;/p&gt; &lt;p&gt;Without grounding, the agent guesses. Without reasoning, it rambles. Without control, it can‚Äôt be trusted when it matters.&lt;/p&gt; &lt;p&gt;When they arrive in the right order, the agent doesn‚Äôt feel scripted or rigid. It feels grounded, thoughtful, dependable when it needs to be. That‚Äôs the difference between an agent that talks and one that operates.&lt;/p&gt; &lt;p&gt;Thin agents, real capability One consequence of this approach is that agents themselves become simple. They don‚Äôt need to contain everything. They don‚Äôt need all the knowledge, all the reasoning styles, all the rules. They become thin interfaces that orchestrate capabilities at runtime. This means intelligence can evolve without rewriting agents. Reasoning can be reused. Control can be applied without killing adaptability. Agents stop being products. They become configurations.&lt;/p&gt; &lt;p&gt;That‚Äôs the direction agent architecture needs to go.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I am building some categorized datasets that prove my thought, very soon i will be pubblishing some open source modules that act as passive &amp;amp; active factual knowledge, followed by intelligence simulations datasets, and runtime ability injectors activated by context assembly.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks a lot for the reading, I've been working on this hard to arrive to a conclusion and test it and find failures behind. &lt;/p&gt; &lt;p&gt;Cheers frank &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frank_brsrk"&gt; /u/frank_brsrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f6gc8q8k24cg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q79f82/rethinking_rag_how_agents_learn_to_operate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q79f82/rethinking_rag_how_agents_learn_to_operate/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-08T11:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1q874d1</id>
    <title>I benchmarked GraphRAG on Groq vs Ollama. Groq is 90x faster.</title>
    <updated>2026-01-09T12:35:12+00:00</updated>
    <author>
      <name>/u/BitterHouse8234</name>
      <uri>https://old.reddit.com/user/BitterHouse8234</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q874d1/i_benchmarked_graphrag_on_groq_vs_ollama_groq_is/"&gt; &lt;img alt="I benchmarked GraphRAG on Groq vs Ollama. Groq is 90x faster." src="https://preview.redd.it/zoa4sb80jbcg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d15db0482269f4f0a5fb9d0468cb4a6e80de58f4" title="I benchmarked GraphRAG on Groq vs Ollama. Groq is 90x faster." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Comparison:&lt;/p&gt; &lt;p&gt;Ollama (Local CPU): $0 cost, 45 mins time. (Positioning: Free but slow)&lt;/p&gt; &lt;p&gt;OpenAI (GPT-4o): $5 cost, 5 mins time. (Positioning: Premium standard)&lt;/p&gt; &lt;p&gt;Groq (Llama-3-70b): $0.10 cost, 30 seconds time. (Positioning: The &amp;quot;Holy Grail&amp;quot;)&lt;/p&gt; &lt;p&gt;Live Demo:&lt;a href="https://bibinprathap.github.io/VeritasGraph/demo/"&gt;https://bibinprathap.github.io/VeritasGraph/demo/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bibinprathap/VeritasGraph"&gt;https://github.com/bibinprathap/VeritasGraph&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BitterHouse8234"&gt; /u/BitterHouse8234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zoa4sb80jbcg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q874d1/i_benchmarked_graphrag_on_groq_vs_ollama_groq_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q874d1/i_benchmarked_graphrag_on_groq_vs_ollama_groq_is/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T12:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7ppq8</id>
    <title>Are the servers down?</title>
    <updated>2026-01-08T22:08:44+00:00</updated>
    <author>
      <name>/u/Antyto2021</name>
      <uri>https://old.reddit.com/user/Antyto2021</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q7ppq8/are_the_servers_down/"&gt; &lt;img alt="Are the servers down?" src="https://preview.redd.it/b91fc02787cg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3be2b07769a1c7643226a236add1ca712fb7af4a" title="Are the servers down?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to know if anyone else is experiencing this, or if it's known whether they're undergoing maintenance or if it's something else. It's not just Ollama that's down; other websites are also failing, and I thought it might be something to do with a large server.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Antyto2021"&gt; /u/Antyto2021 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b91fc02787cg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q7ppq8/are_the_servers_down/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q7ppq8/are_the_servers_down/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-08T22:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q844l0</id>
    <title>JL engine, could use a hand as ive hit a roadblock with my local ollama personality/persona orchestrator/engine project.</title>
    <updated>2026-01-09T09:45:09+00:00</updated>
    <author>
      <name>/u/Upbeat_Reporter8244</name>
      <uri>https://old.reddit.com/user/Upbeat_Reporter8244</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Upbeat_Reporter8244"&gt; /u/Upbeat_Reporter8244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ArtificialInteligence/comments/1q6awgr/jl_engine_could_use_a_hand_as_ive_hit_a_roadblock/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q844l0/jl_engine_could_use_a_hand_as_ive_hit_a_roadblock/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q844l0/jl_engine_could_use_a_hand_as_ive_hit_a_roadblock/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T09:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7qy2i</id>
    <title>Happy New Year! üéâ Nanocoder 1.20.0 Release: A Fresh Start to 2026 with Major Improvements</title>
    <updated>2026-01-08T22:56:35+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q7qy2i/happy_new_year_nanocoder_1200_release_a_fresh/"&gt; &lt;img alt="Happy New Year! üéâ Nanocoder 1.20.0 Release: A Fresh Start to 2026 with Major Improvements" src="https://external-preview.redd.it/oG13rWVL82PyrP_PsvPyqK6d2Zl1lCpwv0c0dzfkAUQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3de7247660b7c7896f2e77b433318f89eb3dd75" title="Happy New Year! üéâ Nanocoder 1.20.0 Release: A Fresh Start to 2026 with Major Improvements" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/nanocoder/comments/1q7qxqq/happy_new_year_nanocoder_1200_release_a_fresh/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q7qy2i/happy_new_year_nanocoder_1200_release_a_fresh/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q7qy2i/happy_new_year_nanocoder_1200_release_a_fresh/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-08T22:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1q81rvx</id>
    <title>Make an AI continue mid-sentence?</title>
    <updated>2026-01-09T07:18:41+00:00</updated>
    <author>
      <name>/u/poobumfartwee</name>
      <uri>https://old.reddit.com/user/poobumfartwee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know a little how AI works, it just predicts the next word in a sentence. However, when I ask ollama `1 + 1 = ` then it answers `Yes, 1 + 1 is 2`.&lt;/p&gt; &lt;p&gt;How do I make it simply continue a sentence of my choosing as if it was the one that said it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/poobumfartwee"&gt; /u/poobumfartwee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q81rvx/make_an_ai_continue_midsentence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q81rvx/make_an_ai_continue_midsentence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q81rvx/make_an_ai_continue_midsentence/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T07:18:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7xej4</id>
    <title>I built Plano - a framework-friendly data plane with orchestration for agents</title>
    <updated>2026-01-09T03:32:09+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q7xej4/i_built_plano_a_frameworkfriendly_data_plane_with/"&gt; &lt;img alt="I built Plano - a framework-friendly data plane with orchestration for agents" src="https://preview.redd.it/hn2gtphwt8cg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ecdde49cb7c4b04ae32b067c6ade544a6dd2179" title="I built Plano - a framework-friendly data plane with orchestration for agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thrilled to be launching &lt;a href="https://github.com/katanemo/plano"&gt;Plano&lt;/a&gt; today - delivery infrastructure for agentic apps: An edge and service proxy server with orchestration for AI agents. Plano's core purpose is to offload all the plumbing work required to deliver agents to production so that developers can stay focused on core product logic.&lt;/p&gt; &lt;p&gt;Plano runs alongside your app servers (cloud, on-prem, or local dev) deployed as a side-car, and leaves GPUs where your models are hosted.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On the ground AI practitioners will tell you that calling an LLM is not the hard part. The really hard part is delivering agentic applications to production quickly and reliably, then iterating without rewriting system code every time. In practice, teams keep rebuilding the same concerns that sit outside any single agent‚Äôs core logic:&lt;/p&gt; &lt;p&gt;This includes model agility - the ability to pull from a large set of LLMs and swap providers without refactoring prompts or streaming handlers. Developers need to learn from production by collecting signals and traces that tell them what to fix. They also need consistent policy enforcement for moderation and jailbreak protection, rather than sprinkling hooks across codebases. And they need multi-agent patterns to improve performance and latency without turning their app into orchestration glue.&lt;/p&gt; &lt;p&gt;These concerns get rebuilt and maintained inside fast-changing frameworks and application code, coupling product logic to infrastructure decisions. It‚Äôs brittle, and pulls teams away from core product work into plumbing they shouldn‚Äôt have to own.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Plano does&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Plano moves core delivery concerns out of process into a modular proxy and dataplane designed for agents. It supports inbound listeners (agent orchestration, safety and moderation hooks), outbound listeners (hosted or API-based LLM routing), or both together. Plano provides the following capabilities via a unified dataplane:&lt;/p&gt; &lt;p&gt;- Orchestration: Low-latency routing and handoff between agents. Add or change agents without modifying app code, and evolve strategies centrally instead of duplicating logic across services.&lt;/p&gt; &lt;p&gt;- Guardrails &amp;amp; Memory Hooks: Apply jailbreak protection, content policies, and context workflows (rewriting, retrieval, redaction) once via filter chains. This centralizes governance and ensures consistent behavior across your stack.&lt;/p&gt; &lt;p&gt;- Model Agility: Route by model name, semantic alias, or preference-based policies. Swap or add models without refactoring prompts, tool calls, or streaming handlers.&lt;/p&gt; &lt;p&gt;- Agentic Signals‚Ñ¢: Zero-code capture of behavior signals, traces, and metrics across every agent, surfacing traces, token usage, and learning signals in one place.&lt;/p&gt; &lt;p&gt;The goal is to keep application code focused on product logic while Plano owns delivery mechanics.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;More on Architecture&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Plano has two main parts:&lt;/p&gt; &lt;p&gt;Envoy-based data plane. Uses Envoy‚Äôs HTTP connection management to talk to model APIs, services, and tool backends. We didn‚Äôt build a separate model server‚ÄîEnvoy already handles streaming, retries, timeouts, and connection pooling. Some of us are core Envoy contributors at Katanemo.&lt;/p&gt; &lt;p&gt;Brightstaff, a lightweight controller and state machine written in Rust. It inspects prompts and conversation state, decides which agents to call and in what order, and coordinates routing and fallback. It uses small LLMs (1‚Äì4B parameters) trained for constrained routing and orchestration. These models do not generate responses and fall back to static policies on failure. The models are open sourced here: &lt;a href="https://huggingface.co/katanemo"&gt;https://huggingface.co/katanemo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hn2gtphwt8cg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q7xej4/i_built_plano_a_frameworkfriendly_data_plane_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q7xej4/i_built_plano_a_frameworkfriendly_data_plane_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T03:32:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1q88mrg</id>
    <title>Trying to get mistral-small running on arch linux</title>
    <updated>2026-01-09T13:45:28+00:00</updated>
    <author>
      <name>/u/keldrin_</name>
      <uri>https://old.reddit.com/user/keldrin_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I am currently trying to get mistral-small running on my PC.&lt;/p&gt; &lt;p&gt;Hardware: CPU: AMD Ryzen 5 4600G, GPU: Nvidia GeForce RTX 4060&lt;/p&gt; &lt;p&gt;I have arch linux installed and the desktop running on the internal AMD Graphics card, the nvidia-dkms drivers are installed and ollama-cuda. The ollama server is running (via systemd) and as user i already downloaded the mistral-small llm.&lt;/p&gt; &lt;p&gt;Now, when I run &lt;code&gt;ollama run mistral-small&lt;/code&gt; i can see in nvtop that GPU memory jumps up to around 75% as expected and after a couple of seconds I get my ollama prompt &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;But then, things don't run like I think they should be. I enter my message (&amp;quot;Hello, who are you?&amp;quot;) and then I wait... quite some time.&lt;/p&gt; &lt;p&gt;In nvtop I see CPU usage going up to 80-120% (for the ollama process), GPU is stuck at 0%. Sometimes it also says N/A. Every 10-20 seconds it spits out 4-6 letters and I see a very little spike in GPU usage (maybe 5% for a split second)&lt;/p&gt; &lt;p&gt;Something is clearly going wrong but I don't even know where to start troubleshooting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keldrin_"&gt; /u/keldrin_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q88mrg/trying_to_get_mistralsmall_running_on_arch_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q88mrg/trying_to_get_mistralsmall_running_on_arch_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q88mrg/trying_to_get_mistralsmall_running_on_arch_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T13:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8bz5a</id>
    <title>I learnt about LLM Evals the hard way ‚Äì here's what actually matters</title>
    <updated>2026-01-09T15:56:19+00:00</updated>
    <author>
      <name>/u/sunglasses-guy</name>
      <uri>https://old.reddit.com/user/sunglasses-guy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunglasses-guy"&gt; /u/sunglasses-guy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AIEval/comments/1q8buxb/i_learnt_about_llm_evals_the_hard_way_heres_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q8bz5a/i_learnt_about_llm_evals_the_hard_way_heres_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q8bz5a/i_learnt_about_llm_evals_the_hard_way_heres_what/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T15:56:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q81go3</id>
    <title>Fine-tune SLMs 2x faster, with TuneKit!</title>
    <updated>2026-01-09T07:00:11+00:00</updated>
    <author>
      <name>/u/Consistent_One7493</name>
      <uri>https://old.reddit.com/user/Consistent_One7493</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q81go3/finetune_slms_2x_faster_with_tunekit/"&gt; &lt;img alt="Fine-tune SLMs 2x faster, with TuneKit!" src="https://external-preview.redd.it/am5iNndqbDZ2OWNnMbT87PqvDogJwy2Ycy4k1LQ7g2Ze7paTRMjPgpbX3pAb.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99bb2501df81b364dce9e80f8492ae8f18d19e39" title="Fine-tune SLMs 2x faster, with TuneKit!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Fine-tuning SLMs the way I wish it worked!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Same model. Same prompt. Completely different results. That's what fine-tuning does (when you can actually get it running).&lt;/p&gt; &lt;p&gt;I got tired of the setup nightmare. So I built:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TuneKit&lt;/strong&gt;: Upload your data. Get a notebook. Train free on Colab (2x faster with Unsloth AI). &lt;/p&gt; &lt;p&gt;&lt;strong&gt;No GPUs to rent. No scripts to write. No cost. Just results!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Üí GitHub: &lt;a href="https://github.com/riyanshibohra/TuneKit"&gt;&lt;strong&gt;https://github.com/riyanshibohra/TuneKit&lt;/strong&gt;&lt;/a&gt; (please star the repo if you find it interesting!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_One7493"&gt; /u/Consistent_One7493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9586ijk6v9cg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q81go3/finetune_slms_2x_faster_with_tunekit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q81go3/finetune_slms_2x_faster_with_tunekit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T07:00:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8a8uy</id>
    <title>RAGLight Framework Update : Reranking, Memory, VLM PDF Parser &amp; More!</title>
    <updated>2026-01-09T14:50:41+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Quick update on &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;RAGLight&lt;/a&gt;, my framework for building RAG pipelines in a few lines of code. Try it easily using your favorite Ollama model üéâ&lt;/p&gt; &lt;h1&gt;Better Reranking&lt;/h1&gt; &lt;p&gt;Classic RAG now retrieves more docs and reranks them for higher-quality answers.&lt;/p&gt; &lt;h1&gt;Memory Support&lt;/h1&gt; &lt;p&gt;RAG now includes memory for multi-turn conversations.&lt;/p&gt; &lt;h1&gt;New PDF Parser (with VLM)&lt;/h1&gt; &lt;p&gt;A new PDF parser based on a vision-language model can extract content from images, diagrams, and charts inside PDFs.&lt;/p&gt; &lt;h1&gt;Agentic RAG Refactor&lt;/h1&gt; &lt;p&gt;Agentic RAG has been rewritten using &lt;strong&gt;LangChain&lt;/strong&gt; for better tools, compatibility, and reliability.&lt;/p&gt; &lt;h1&gt;Dependency Updates&lt;/h1&gt; &lt;p&gt;All dependencies refreshed to fix vulnerabilities and improve stability.&lt;/p&gt; &lt;p&gt;üëâ Repo: &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;https://github.com/Bessouat40/RAGLight&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üëâ Documentation : &lt;a href="https://raglight.mintlify.app/"&gt;https://raglight.mintlify.app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to get feedback or questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q8a8uy/raglight_framework_update_reranking_memory_vlm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q8a8uy/raglight_framework_update_reranking_memory_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q8a8uy/raglight_framework_update_reranking_memory_vlm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T14:50:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1q84177</id>
    <title>Create specialized Ollama models in 30 seconds</title>
    <updated>2026-01-09T09:39:13+00:00</updated>
    <author>
      <name>/u/ComfyTightwad</name>
      <uri>https://old.reddit.com/user/ComfyTightwad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q84177/create_specialized_ollama_models_in_30_seconds/"&gt; &lt;img alt="Create specialized Ollama models in 30 seconds" src="https://external-preview.redd.it/ZTc4MXF5aGJuYWNnMS7UCVRbAZLkQIWiFFfUkEkFXIGpC2B-Hyedu35-gMRt.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8033c7563f557c495097d3cb70ed2e912bbf268d" title="Create specialized Ollama models in 30 seconds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just released a new update for OllaMan(Ollama Manager), and it includes a Model Factory to make local agent creation effortless.&lt;/p&gt; &lt;p&gt;Pick a base model (Llama 3, Mistral, etc.).&lt;/p&gt; &lt;p&gt;Set your System Prompt (or use one of the built-in presets).&lt;/p&gt; &lt;p&gt;Tweak Parameters visually (Temp, TopP, TopK).&lt;/p&gt; &lt;p&gt;Click Create.&lt;/p&gt; &lt;p&gt;Boom. You have a custom, specialized model ready to use throughout the app (and via the Ollama CLI).&lt;/p&gt; &lt;p&gt;It's Free and runs locally on your machine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComfyTightwad"&gt; /u/ComfyTightwad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rqpjr0gbnacg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q84177/create_specialized_ollama_models_in_30_seconds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q84177/create_specialized_ollama_models_in_30_seconds/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T09:39:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8zuli</id>
    <title>Method to run 30B Parameter Model</title>
    <updated>2026-01-10T09:29:21+00:00</updated>
    <author>
      <name>/u/Constant_Record_9691</name>
      <uri>https://old.reddit.com/user/Constant_Record_9691</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a decent laptop (3050ti) but nowhere near enough VRAM to runt the model I have in mind. Any free online options?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Constant_Record_9691"&gt; /u/Constant_Record_9691 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q8zuli/method_to_run_30b_parameter_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q8zuli/method_to_run_30b_parameter_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q8zuli/method_to_run_30b_parameter_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T09:29:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8li8b</id>
    <title>I built an Ollama LLM client for Mac OS9. Because why not.</title>
    <updated>2026-01-09T21:52:06+00:00</updated>
    <author>
      <name>/u/MishyJari</name>
      <uri>https://old.reddit.com/user/MishyJari</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q8li8b/i_built_an_ollama_llm_client_for_mac_os9_because/"&gt; &lt;img alt="I built an Ollama LLM client for Mac OS9. Because why not." src="https://external-preview.redd.it/9y2VEONxbCzpJuXVrahQ402wEWsxlCGaL_u9A1Jv6-0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea922d28cb55f27614a0b2f9d3c73af1344cefd9" title="I built an Ollama LLM client for Mac OS9. Because why not." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MishyJari"&gt; /u/MishyJari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bq29memb5ecg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q8li8b/i_built_an_ollama_llm_client_for_mac_os9_because/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q8li8b/i_built_an_ollama_llm_client_for_mac_os9_because/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-09T21:52:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q99hsz</id>
    <title>Nvidia Quadro P400 2GB GDDR5 card good enough?</title>
    <updated>2026-01-10T17:09:37+00:00</updated>
    <author>
      <name>/u/Curious_Party_4683</name>
      <uri>https://old.reddit.com/user/Curious_Party_4683</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q99hsz/nvidia_quadro_p400_2gb_gddr5_card_good_enough/"&gt; &lt;img alt="Nvidia Quadro P400 2GB GDDR5 card good enough?" src="https://b.thumbs.redditmedia.com/Ir0lGRSqX5OLg3aYUy2LQyL0XiK7SGfDKqBhqb0hngM.jpg" title="Nvidia Quadro P400 2GB GDDR5 card good enough?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;qwen3-vl:8b refuses to work on my i7, 7th gen, windows machine. &lt;/p&gt; &lt;p&gt;will this cheap nvidia work? or what's the bare minimum card?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lj263g5g0kcg1.png?width=1639&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ba459647259dba77ccefd933da88fdfc646e3fa"&gt;https://preview.redd.it/lj263g5g0kcg1.png?width=1639&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ba459647259dba77ccefd933da88fdfc646e3fa&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Curious_Party_4683"&gt; /u/Curious_Party_4683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q99hsz/nvidia_quadro_p400_2gb_gddr5_card_good_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q99hsz/nvidia_quadro_p400_2gb_gddr5_card_good_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q99hsz/nvidia_quadro_p400_2gb_gddr5_card_good_enough/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T17:09:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9bdj0</id>
    <title>STT and TTS compatible with ROCm</title>
    <updated>2026-01-10T18:22:15+00:00</updated>
    <author>
      <name>/u/EnvironmentalToe3130</name>
      <uri>https://old.reddit.com/user/EnvironmentalToe3130</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnvironmentalToe3130"&gt; /u/EnvironmentalToe3130 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1q9bd1v/stt_and_tts_compatible_with_rocm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9bdj0/stt_and_tts_compatible_with_rocm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q9bdj0/stt_and_tts_compatible_with_rocm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T18:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9ff6k</id>
    <title>what AI models can I run locally on my PC with Ollama?</title>
    <updated>2026-01-10T20:57:13+00:00</updated>
    <author>
      <name>/u/Kitchen-Patience8176</name>
      <uri>https://old.reddit.com/user/Kitchen-Patience8176</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I‚Äôm pretty new to local AI and still learning, so sorry if this is a basic question.&lt;/p&gt; &lt;p&gt;I can‚Äôt afford a ChatGPT subscription anymore due to financial reasons, so I‚Äôm trying to use &lt;strong&gt;local models&lt;/strong&gt; instead. I‚Äôve installed &lt;strong&gt;Ollama&lt;/strong&gt;, and it works, but I don‚Äôt really know which models I should be using or what my PC can realistically handle.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ryzen 9 5900X&lt;/li&gt; &lt;li&gt;RTX 3080 (10GB VRAM)&lt;/li&gt; &lt;li&gt;32GB RAM&lt;/li&gt; &lt;li&gt;2TB NVMe SSD&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm mainly curious about:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Which models run well on this setup&lt;/li&gt; &lt;li&gt;What I &lt;em&gt;can‚Äôt&lt;/em&gt; run&lt;/li&gt; &lt;li&gt;How close local models can get to ChatGPT&lt;/li&gt; &lt;li&gt;If things like web search, fact-checking, or up-to-date info are possible locally (or any workarounds)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any beginner advice or model recommendations would really help.&lt;/p&gt; &lt;p&gt;Thanks üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kitchen-Patience8176"&gt; /u/Kitchen-Patience8176 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9ff6k/what_ai_models_can_i_run_locally_on_my_pc_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9ff6k/what_ai_models_can_i_run_locally_on_my_pc_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q9ff6k/what_ai_models_can_i_run_locally_on_my_pc_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T20:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q95z91</id>
    <title>Built a Local Research Agent with Ollama - No API Keys, Just Citations</title>
    <updated>2026-01-10T14:51:06+00:00</updated>
    <author>
      <name>/u/Xthebuilder</name>
      <uri>https://old.reddit.com/user/Xthebuilder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q95z91/built_a_local_research_agent_with_ollama_no_api/"&gt; &lt;img alt="Built a Local Research Agent with Ollama - No API Keys, Just Citations" src="https://b.thumbs.redditmedia.com/f-UABymhEJedVxkN3owagmqQGsW0QyXoIv6GlbP6OBc.jpg" title="Built a Local Research Agent with Ollama - No API Keys, Just Citations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a research agent that runs entirely locally using Ollama. Give it a topic, get back a markdown report with proper citations. Simple as that.&lt;/p&gt; &lt;p&gt;What It Does&lt;/p&gt; &lt;p&gt;The agent handles the full research workflow:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô Gathers sources asynchronously ‚àô Uses semantic embeddings to filter for relevance ‚àô Generates structured reports with citations ‚àô Everything stays on your machine &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Why I Built This&lt;/p&gt; &lt;p&gt;I wanted deep research capabilities without depending on cloud services or burning through API credits. With Ollama making local LLMs practical, it seemed like the obvious foundation.&lt;/p&gt; &lt;p&gt;How It Works&lt;/p&gt; &lt;p&gt;python research_agent.py &amp;quot;quantum computing applications&amp;quot;&lt;/p&gt; &lt;p&gt;The agent:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. Pulls sources from DuckDuckGo 2. Extracts and evaluates content using sentence-transformers 3. Runs quality checks on similarity scores 4. Generates a markdown report with references &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;All processing happens locally. No external APIs.&lt;/p&gt; &lt;p&gt;Design Choices (Explicit By Design)&lt;/p&gt; &lt;p&gt;Local-first: Works with any Ollama model - llama2, mistral, whatever you have running&lt;/p&gt; &lt;p&gt;Quality thresholds: Configurable similarity scores ensure sources are actually relevant&lt;/p&gt; &lt;p&gt;Async operations: Fast source gathering without blocking&lt;/p&gt; &lt;p&gt;Structured output: Clean markdown reports you can actually use&lt;/p&gt; &lt;p&gt;Tradeoffs&lt;/p&gt; &lt;p&gt;I optimized for:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô Privacy and offline workflows ‚àô Explicit configuration over automation ‚àô Simple setup (just Python + Ollama) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This means it‚Äôs not:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô A cloud-scale solution ‚àô Zero-configuration ‚àô Designed for multi-source integrations (yet) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What‚Äôs Next&lt;/p&gt; &lt;p&gt;Considering:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô PDF source support improvements ‚àô Local caching to avoid re-fetching ‚àô Better semantic chunking for long sources &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Code‚Äôs on GitHub: &lt;a href="https://github.com/Xthebuilder/Research_Agent"&gt;https://github.com/Xthebuilder/Research_Agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xthebuilder"&gt; /u/Xthebuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q95z91"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q95z91/built_a_local_research_agent_with_ollama_no_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q95z91/built_a_local_research_agent_with_ollama_no_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T14:51:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9jdo8</id>
    <title>Is it possible to see AI Request and Response in Realtime on Llama</title>
    <updated>2026-01-10T23:36:56+00:00</updated>
    <author>
      <name>/u/Professional-Fun7765</name>
      <uri>https://old.reddit.com/user/Professional-Fun7765</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone&lt;/p&gt; &lt;p&gt;I am new in the world of Ollama so pardon me if this may sound like a stupid question. I am transitioning from GPT4All where when I make a request via API I can see in real time On the desktop app (on the server chat tab) The incoming request, the model thinking and the model generating a response. This was so great in debugging but GPT4All was slow for me so a colleague suggested Ollama and I can see much improvements in speed. I am currently integrating a Laravel App with Ollama and sending various request to the model and I wish I can be able to see the request and response in real time in Ollama just like I did in GPT4All Desktop App, so my question is whether or not this is possible? If it is then how can I go about configuring it?&lt;/p&gt; &lt;p&gt;if it helps I am on Windows and this is for my local development.&lt;/p&gt; &lt;p&gt;Thank you in advance, your input will be highly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Fun7765"&gt; /u/Professional-Fun7765 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9jdo8/is_it_possible_to_see_ai_request_and_response_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9jdo8/is_it_possible_to_see_ai_request_and_response_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q9jdo8/is_it_possible_to_see_ai_request_and_response_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-10T23:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9svp5</id>
    <title>Looking for open source contributers | LocalAgent</title>
    <updated>2026-01-11T07:11:25+00:00</updated>
    <author>
      <name>/u/FriendshipCreepy8045</name>
      <uri>https://old.reddit.com/user/FriendshipCreepy8045</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;br /&gt; Hope you're all doing well.&lt;/p&gt; &lt;p&gt;So little background: I'm a frontend/performance engineer working as an IT consultant for the past year or so.&lt;br /&gt; Recently made a goal to learn and code more in python and basically entering the field of AI Applied engineering.&lt;br /&gt; I'm still learning concepts but with a little knowledge and claude, I made a researcher assistent that runs entirly on laptop(if you have a descent one using Ollama) or just use the default cloud.&lt;/p&gt; &lt;p&gt;I understand langchain quite a bit and might be worth checking out langraph to somehow migrate it into more controlled research assistent(controlling tools,tokens used etc.).&lt;br /&gt; So I need your help, I would really appretiate if you guys go ahead and check &amp;quot;&lt;a href="https://github.com/vedas-dixit/LocalAgent"&gt;https://github.com/vedas-dixit/LocalAgent&lt;/a&gt;&amp;quot; and let me know:&lt;/p&gt; &lt;p&gt;Your thoughts | Potential Improvements | Guidance *what i did right/wrong&lt;/p&gt; &lt;p&gt;or if i may ask, just some meaningful contribution to the project if you have time ;).&lt;/p&gt; &lt;p&gt;I posted about this like idk a month ago and got 100+ stars in a week so might have some potential but idk.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FriendshipCreepy8045"&gt; /u/FriendshipCreepy8045 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9svp5/looking_for_open_source_contributers_localagent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q9svp5/looking_for_open_source_contributers_localagent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q9svp5/looking_for_open_source_contributers_localagent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-11T07:11:25+00:00</published>
  </entry>
</feed>
