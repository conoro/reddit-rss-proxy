<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-07T04:24:56+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1n8td0n</id>
    <title>Can you offload the entire LLM functionality to Ollama Turbo which means local hardware does not require GPU?</title>
    <updated>2025-09-05T02:23:53+00:00</updated>
    <author>
      <name>/u/ComedianObjective572</name>
      <uri>https://old.reddit.com/user/ComedianObjective572</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! Idk if this is a stupid idea for this Ollama Turbo. I want to run GPT-OSS but I don‚Äôt have the necessary hardware for it. Is it possible to offload the entire Ollama functionality to Ollama Turbo. &lt;/p&gt; &lt;p&gt;For example, local server with Ubuntu Server, Intel Core I5, 8GB RAM, NO GPU which servers Front End and Back End functionality to 3 computers. If I need to run an GPT-OSS, can I offload the entire thing to Ollama Turbo?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComedianObjective572"&gt; /u/ComedianObjective572 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8td0n/can_you_offload_the_entire_llm_functionality_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8td0n/can_you_offload_the_entire_llm_functionality_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8td0n/can_you_offload_the_entire_llm_functionality_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T02:23:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8vkv7</id>
    <title>Spam ending up being published?</title>
    <updated>2025-09-05T04:16:32+00:00</updated>
    <author>
      <name>/u/gnu-trix</name>
      <uri>https://old.reddit.com/user/gnu-trix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So... has anyone seen this?&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/puffymattresscode2025/puffy-mattress-coupon-code-2025-verified"&gt;https://ollama.com/puffymattresscode2025/puffy-mattress-coupon-code-2025-verified&lt;/a&gt;&lt;/p&gt; &lt;p&gt;^^ DISCLAIMER: you almost certainly should not pull this. I'm just pointing it out.&lt;/p&gt; &lt;p&gt;It says it was published 5 days ago. I'm REALLY super curious as to what it is, so if anyone has a VLAN'd Qubes with a VPN'd remote desktop running Ollama, could you pull it and try it out and report back here what it does? (Only suggesting this as a testing ground because there's AI malware now. I have no idea what makes models &amp;quot;run&amp;quot; - maybe it's not executable and wouldn't matter?)&lt;/p&gt; &lt;p&gt;But yeah anyway, do spam AI models often end up being published on Ollama, or is this a rare occurrence?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gnu-trix"&gt; /u/gnu-trix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8vkv7/spam_ending_up_being_published/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8vkv7/spam_ending_up_being_published/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8vkv7/spam_ending_up_being_published/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T04:16:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9beia</id>
    <title>What hardware would you get for $1500 or less...Mac M4 pro 24GB or AMD Ryzen AI Max+ 395 64gb?</title>
    <updated>2025-09-05T17:25:26+00:00</updated>
    <author>
      <name>/u/abrandis</name>
      <uri>https://old.reddit.com/user/abrandis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm in the need of more powerful LLM hardware, something in the affordable price range of $1500ish... It seems to me the two best options are the Ryzen AI Max+ 395 64gb or a Mac mini 4 pro with about 24gb , what is this subreddit feeling? &lt;/p&gt; &lt;p&gt;The Amd seems like it has better value in terms of specs and memory' but I hear a lot of issues with GPU driver support/performance...but I have neither so I'm curious ...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abrandis"&gt; /u/abrandis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9beia/what_hardware_would_you_get_for_1500_or_lessmac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9beia/what_hardware_would_you_get_for_1500_or_lessmac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9beia/what_hardware_would_you_get_for_1500_or_lessmac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T17:25:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8kigw</id>
    <title>Got Gemma running locally on a Raspberry Pi 5 with Ollama</title>
    <updated>2025-09-04T20:00:23+00:00</updated>
    <author>
      <name>/u/Ricardo_Sappia</name>
      <uri>https://old.reddit.com/user/Ricardo_Sappia</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n8kigw/got_gemma_running_locally_on_a_raspberry_pi_5/"&gt; &lt;img alt="Got Gemma running locally on a Raspberry Pi 5 with Ollama" src="https://b.thumbs.redditmedia.com/ncJBXF0mM3RxpxHtBdAzWA1EyKrG2x3mjY__CLeR-qM.jpg" title="Got Gemma running locally on a Raspberry Pi 5 with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share a quick win: I got Gemma 2B (GGUF) running fully offline on a Raspberry Pi 5 (4GB) using Ollama.&lt;/p&gt; &lt;p&gt;It‚Äôs part of a side project called Dashi ‚Äî a modular e-paper dashboard that displays Strava, Garmin, and weather data‚Ä¶ plus motivational messages generated by a local AI fox .&lt;/p&gt; &lt;p&gt;No cloud, no API keys ‚Äî just local inference and surprisingly smooth performance for short outputs.&lt;/p&gt; &lt;p&gt;You can see more details here if curious: üëâ &lt;a href="https://www.hackster.io/rsappia/e-paper-dashboard-where-sport-ai-and-paper-meet-10c0f0"&gt;https://www.hackster.io/rsappia/e-paper-dashboard-where-sport-ai-and-paper-meet-10c0f0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer any questions about the setup or the integration!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ricardo_Sappia"&gt; /u/Ricardo_Sappia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n8kigw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8kigw/got_gemma_running_locally_on_a_raspberry_pi_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8kigw/got_gemma_running_locally_on_a_raspberry_pi_5/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T20:00:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9hv8a</id>
    <title>Why are CPU peaks every 5 min</title>
    <updated>2025-09-05T21:39:17+00:00</updated>
    <author>
      <name>/u/jordi_at</name>
      <uri>https://old.reddit.com/user/jordi_at</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I have a virtual machine running Debian, Ollama and OpenUi on Proxmox and I noticed that every five minutes there is a CPU peak of almost 20% of CPU usage. This is the only VM with this behaviour, that's why I'm asking this sub as I guess that is Ollama related. Any idea of the possible reason? Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jordi_at"&gt; /u/jordi_at &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9hv8a/why_are_cpu_peaks_every_5_min/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9hv8a/why_are_cpu_peaks_every_5_min/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9hv8a/why_are_cpu_peaks_every_5_min/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T21:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8k3j3</id>
    <title>Power Up your Ollama Models! Thanks to you guys, I made this framework that lets your models watch the screen and help you out! (Open Source and Local)</title>
    <updated>2025-09-04T19:44:13+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n8k3j3/power_up_your_ollama_models_thanks_to_you_guys_i/"&gt; &lt;img alt="Power Up your Ollama Models! Thanks to you guys, I made this framework that lets your models watch the screen and help you out! (Open Source and Local)" src="https://external-preview.redd.it/dW9pNmwzeGxiN25mMc1Nh3OUDLTuFDtnMrFXEDpwYIUIEihHJF3jJPncl3qU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf64148d6af2db710bc91a6b36681250b2ab77fc" title="Power Up your Ollama Models! Thanks to you guys, I made this framework that lets your models watch the screen and help you out! (Open Source and Local)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; Observer now has an Overlay and Shortcut features! Now you can run agents that help you out at any time while watching your screen.&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;I'm back with another Observer update c:&lt;/p&gt; &lt;p&gt;Thank you so much for your support and feedback! I'm still working hard to make Observer useful in a variety of ways. And i'm trying to make Local models accessible to everyone!&lt;/p&gt; &lt;p&gt;So this update is an Overlay that lets your agents give you information on top of whatever you're doing. The obvious use case is helping out in coding problems, but there are other really cool things you can do with it! (specially adding the overlay to other already working agents). These are some cases where the Overlay can be useful:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Coding Assistant:&lt;/strong&gt; Use a shortcut and send whatever problem you're seeing to an LLM for it to solve it.&lt;br /&gt; &lt;strong&gt;Writing Assistant:&lt;/strong&gt; Send the text you're looking at to an LLM to get suggestions on what to write better or how to construct a better story.&lt;br /&gt; &lt;strong&gt;Activity Tracker:&lt;/strong&gt; Have an agent log on the overlay the last time you were doing something specific, then just by glancing at it you can get an idea of how much time you've spent doing something.&lt;br /&gt; &lt;strong&gt;Distraction Logger:&lt;/strong&gt; Same as the activity tracker, you just get messages passively when it thinks you're distracted.&lt;br /&gt; &lt;strong&gt;Video Watching Companion:&lt;/strong&gt; Watch a video and have a model label every new topic discussed and see it in the overlay!&lt;/p&gt; &lt;p&gt;Or any other agent you already had working, just &lt;strong&gt;power it up&lt;/strong&gt; by seeing what it's doing with the Overlay!&lt;/p&gt; &lt;p&gt;This is the projects &lt;a href="https://github.com/Roy3838/Observer"&gt;Github&lt;/a&gt; (completely open source)&lt;br /&gt; And the discord: &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you have any questions or ideas i'll be hanging out here for a while!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3h656ewlb7nf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8k3j3/power_up_your_ollama_models_thanks_to_you_guys_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8k3j3/power_up_your_ollama_models_thanks_to_you_guys_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T19:44:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9siw6</id>
    <title>Ollama lagging too much..</title>
    <updated>2025-09-06T06:33:50+00:00</updated>
    <author>
      <name>/u/Current-Passion-9783</name>
      <uri>https://old.reddit.com/user/Current-Passion-9783</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys, I have downloaded ollama and in cmd I downloaded ollama mistrial, but it's lagging my laptop too much, I am making a ai assistant but it feels like it's freezing, can someone tell what can I do so it doesn't lag?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Current-Passion-9783"&gt; /u/Current-Passion-9783 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9siw6/ollama_lagging_too_much/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9siw6/ollama_lagging_too_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9siw6/ollama_lagging_too_much/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T06:33:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9b4ak</id>
    <title>Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler</title>
    <updated>2025-09-05T17:14:37+00:00</updated>
    <author>
      <name>/u/Fluid-Engineering769</name>
      <uri>https://old.reddit.com/user/Fluid-Engineering769</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n9b4ak/websitecrawler_extract_data_from_websites_in_llm/"&gt; &lt;img alt="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" src="https://external-preview.redd.it/CWPKsqhHvC8Ru3Bore9-VfKiik9UmiU4LtZd8GtJiug.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d523c17e313fe34a441cd56f43ec9625de5e76a" title="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluid-Engineering769"&gt; /u/Fluid-Engineering769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pc8544/Website-Crawler"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9b4ak/websitecrawler_extract_data_from_websites_in_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9b4ak/websitecrawler_extract_data_from_websites_in_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T17:14:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1n95hr0</id>
    <title>Unsloth just released their GGUF of Kimi-K2-Instruct-0905!</title>
    <updated>2025-09-05T13:36:53+00:00</updated>
    <author>
      <name>/u/TheAndyGeorge</name>
      <uri>https://old.reddit.com/user/TheAndyGeorge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n95hr0/unsloth_just_released_their_gguf_of/"&gt; &lt;img alt="Unsloth just released their GGUF of Kimi-K2-Instruct-0905!" src="https://external-preview.redd.it/u42y4pGiiWpLArGTxtLnpU7XIOrkkmzZ5xAid1ozch8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7d4e7a1b9c3b96563747fc8517c620156b1d622" title="Unsloth just released their GGUF of Kimi-K2-Instruct-0905!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheAndyGeorge"&gt; /u/TheAndyGeorge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Kimi-K2-Instruct-0905-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n95hr0/unsloth_just_released_their_gguf_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n95hr0/unsloth_just_released_their_gguf_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T13:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9o314</id>
    <title>smaller model ever (quantized or not) that supports input images</title>
    <updated>2025-09-06T02:27:23+00:00</updated>
    <author>
      <name>/u/gabrielevinci</name>
      <uri>https://old.reddit.com/user/gabrielevinci</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I have installed Ollama recently because I need it for my project. &lt;/p&gt; &lt;p&gt;My aim is to &amp;quot;describe images&amp;quot; as quickly as possible. &lt;/p&gt; &lt;p&gt;I have a 3060OC with 12 vram, so little for this world (I bought it when the AI ‚Äã‚Äãwas not known and it never served me to play)&lt;/p&gt; &lt;p&gt;What is the best model for my purpose?&lt;/p&gt; &lt;p&gt;I tried Gemma3: 4b and it seems to work very well, but I want to use the most efficient model for my purpose. &lt;/p&gt; &lt;p&gt;Do you know someone?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gabrielevinci"&gt; /u/gabrielevinci &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9o314/smaller_model_ever_quantized_or_not_that_supports/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9o314/smaller_model_ever_quantized_or_not_that_supports/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9o314/smaller_model_ever_quantized_or_not_that_supports/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T02:27:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9vr24</id>
    <title>gpt-oss:20b and structured outputs and ollama</title>
    <updated>2025-09-06T10:01:14+00:00</updated>
    <author>
      <name>/u/DifficultTomatillo29</name>
      <uri>https://old.reddit.com/user/DifficultTomatillo29</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just doesn't work for me - am I doing anything wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DifficultTomatillo29"&gt; /u/DifficultTomatillo29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9vr24/gptoss20b_and_structured_outputs_and_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9vr24/gptoss20b_and_structured_outputs_and_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9vr24/gptoss20b_and_structured_outputs_and_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T10:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9i1f1</id>
    <title>First time user confused by models.</title>
    <updated>2025-09-05T21:46:32+00:00</updated>
    <author>
      <name>/u/Xanaxaria</name>
      <uri>https://old.reddit.com/user/Xanaxaria</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm completely new to this. I'm still figuring out how to use everything but I was wondering what models were best for NSFW language editing. I'm a porn translator and have been using AI to help edit but all ais (chatgpt, deepseek, etc) generally block the stuff I translate. So I'm looking for the best language handling model that can process NSFW language. I don't need it to translate so in find with it only being good at English but I need it to grammatically edit larger amounts of text.&lt;/p&gt; &lt;p&gt;Is there a guide or a website that has a master list of models for what the model is good at? Or would anyone know a good model that fits this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xanaxaria"&gt; /u/Xanaxaria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9i1f1/first_time_user_confused_by_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9i1f1/first_time_user_confused_by_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9i1f1/first_time_user_confused_by_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T21:46:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9vruw</id>
    <title>ollama and audio</title>
    <updated>2025-09-06T10:02:29+00:00</updated>
    <author>
      <name>/u/DifficultTomatillo29</name>
      <uri>https://old.reddit.com/user/DifficultTomatillo29</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;are there any models under ollama that support receiving audio, and is there any way to send it? note - I DO NOT want a transcription - I want to ask things about the audio - .... age, gender, quality - that sort of thing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DifficultTomatillo29"&gt; /u/DifficultTomatillo29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9vruw/ollama_and_audio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9vruw/ollama_and_audio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9vruw/ollama_and_audio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T10:02:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1na5w7c</id>
    <title>Gemma3:27b running slow</title>
    <updated>2025-09-06T17:47:30+00:00</updated>
    <author>
      <name>/u/Busy-Examination1924</name>
      <uri>https://old.reddit.com/user/Busy-Examination1924</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1na5w7c/gemma327b_running_slow/"&gt; &lt;img alt="Gemma3:27b running slow" src="https://a.thumbs.redditmedia.com/HIO6p8q_Bb8ESBKosiThddzFWxP-x5v0fM19qmM0rE0.jpg" title="Gemma3:27b running slow" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I am new to using ollama, and for some reason, even though no resources are maxed out gemma3:27b is running very slow when generating answers. Below is a screen shot of the performance tab. Does anyone have any dieas on how tof ix it? My specs are: RTX 4070, 11700k CPU, 64GB ram. Im trying to find a good model for reasoning, code, and overall ability to help with studying. Any help would be appreciated, thanks!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f0sjin7s0lnf1.png?width=1517&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=262c316671da771aad2eb42c4dfee6476322fea4"&gt;https://preview.redd.it/f0sjin7s0lnf1.png?width=1517&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=262c316671da771aad2eb42c4dfee6476322fea4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Busy-Examination1924"&gt; /u/Busy-Examination1924 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na5w7c/gemma327b_running_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na5w7c/gemma327b_running_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na5w7c/gemma327b_running_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T17:47:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1na71cj</id>
    <title>Help with using vision models locally</title>
    <updated>2025-09-06T18:32:22+00:00</updated>
    <author>
      <name>/u/orangeflyingmonkey_</name>
      <uri>https://old.reddit.com/user/orangeflyingmonkey_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to build a bot that analyzes my shopping receipts via telegram. I have downloaded ollama and want to test it out before I get building the workflow. &lt;/p&gt; &lt;p&gt;I am using llava but it seems to be doing completely inaccurate analysis. like its list of things does not match whats in the reciept. is there a specific way of using vision models in ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orangeflyingmonkey_"&gt; /u/orangeflyingmonkey_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na71cj/help_with_using_vision_models_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na71cj/help_with_using_vision_models_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na71cj/help_with_using_vision_models_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T18:32:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1na8tqe</id>
    <title>Need help with LLM not accessing my code in intellij IDE</title>
    <updated>2025-09-06T19:43:45+00:00</updated>
    <author>
      <name>/u/Unknownduck07</name>
      <uri>https://old.reddit.com/user/Unknownduck07</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I am a java developer trying to integrate any ai model into my personal Intellij Idea IDE.&lt;br /&gt; With a bit of googling and stuff, I downloaded ollama and then downloaded the latest version of Codegemma. I even setup the plugin &amp;quot;Continue&amp;quot; and it is now detecting the LLM model to answer my questions.&lt;/p&gt; &lt;p&gt;The issue I am facing is that, when I ask it to scan my spring boot project, or simply analyze it, it says it cant due to security and privacy policies.&lt;/p&gt; &lt;p&gt;a) Am I doing something wrong?&lt;br /&gt; b) Am I using any wrong model?&lt;br /&gt; c) Is there any other thing that I might have missed?&lt;/p&gt; &lt;p&gt;Since my workplace has integrated windsurf with a premium subscription, it can analyze my local files / projects and give me answers as expected. However, I am trying to achieve kind of something similar, but with my personal PC and free tier overall.&lt;/p&gt; &lt;p&gt;Kindly help. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unknownduck07"&gt; /u/Unknownduck07 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na8tqe/need_help_with_llm_not_accessing_my_code_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na8tqe/need_help_with_llm_not_accessing_my_code_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na8tqe/need_help_with_llm_not_accessing_my_code_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T19:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1na72g3</id>
    <title>Sometimes I prefer to use my own chatbot over ChatGPT because the answers are faster. Not always better, but faster ‚úåÔ∏èüòä‚ú® (WIP: every day getting better and better)</title>
    <updated>2025-09-06T18:33:34+00:00</updated>
    <author>
      <name>/u/Roseldine</name>
      <uri>https://old.reddit.com/user/Roseldine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1na72g3/sometimes_i_prefer_to_use_my_own_chatbot_over/"&gt; &lt;img alt="Sometimes I prefer to use my own chatbot over ChatGPT because the answers are faster. Not always better, but faster ‚úåÔ∏èüòä‚ú® (WIP: every day getting better and better)" src="https://preview.redd.it/l3uewsvy8lnf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c39cbf5306fb5afac362883306c0c197e38f2abe" title="Sometimes I prefer to use my own chatbot over ChatGPT because the answers are faster. Not always better, but faster ‚úåÔ∏èüòä‚ú® (WIP: every day getting better and better)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roseldine"&gt; /u/Roseldine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l3uewsvy8lnf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na72g3/sometimes_i_prefer_to_use_my_own_chatbot_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na72g3/sometimes_i_prefer_to_use_my_own_chatbot_over/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T18:33:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nadynk</id>
    <title>Using the Ollama Client App with Rag Chain</title>
    <updated>2025-09-06T23:24:11+00:00</updated>
    <author>
      <name>/u/tintires</name>
      <uri>https://old.reddit.com/user/tintires</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive built s simple RAG agent with LangGraph (gpt-oss:20b) in a Jupyter notebook. Works great. How might I expose this (perhaps as a macos service) to use it with the Ollama desktop app? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tintires"&gt; /u/tintires &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nadynk/using_the_ollama_client_app_with_rag_chain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nadynk/using_the_ollama_client_app_with_rag_chain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nadynk/using_the_ollama_client_app_with_rag_chain/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T23:24:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9zd9l</id>
    <title>ollama pipelines keep failing in repeatable ways. global fix map just shipped, dedicated ollama page plus dr. wfgy</title>
    <updated>2025-09-06T13:19:29+00:00</updated>
    <author>
      <name>/u/onestardao</name>
      <uri>https://old.reddit.com/user/onestardao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n9zd9l/ollama_pipelines_keep_failing_in_repeatable_ways/"&gt; &lt;img alt="ollama pipelines keep failing in repeatable ways. global fix map just shipped, dedicated ollama page plus dr. wfgy" src="https://preview.redd.it/pts15p50pjnf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31dfff2be1d431de4cea404db4af1fd2f7c09d79" title="ollama pipelines keep failing in repeatable ways. global fix map just shipped, dedicated ollama page plus dr. wfgy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;last week i posted the 16 problem map. today is the upgrade. we now have the global fix map with a dedicated ollama page, and a live dr. wfgy ÔºàChatGPT shared page with pre-trained data, just paste your bug screenshot to it you will get the answer) &lt;/p&gt; &lt;p&gt;on the map home who triages bugs in plain chat. we keep the same idea, a semantic firewall before generation. you drop it in front of output, it checks ŒîS and Œª, loops or resets if unstable, then lets the model speak only when the state is clean. no infra change, no sdk.&lt;/p&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;p&gt;why this matters for ollama&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;your embeddings look fine, answers drift. the firewall treats semantic ‚â† embedding as No 5 and clamps it.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;doc exists, retrieval never lands on it. traceability is No 8, we add ids and contracts so citations stop lying.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;first call after a model switch crashes or returns garbage. pre deploy collapse is No 16, the page shows the warmup and version pins that avoid it.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;background jobs run before the store is ready. bootstrap ordering is No 14, you get a minimal start order and swap recipe.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;long context entropy and routing noise. No 2 and No 9 have quick checks you can run in text.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;p&gt;before vs after (what changed from last post)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;before: patch after the fact, add rerankers and regex, fight the same bug next week.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;after: accept only stable semantic states before output. measure ŒîS ‚â§ 0.45, coverage ‚â• 0.70, Œª convergent. once the path holds, that class stays fixed.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;new for this release: a one page ollama guide with store agnostic knobs, and a chat based dr. wfgy who maps your symptom to the right No and gives a minimal prescription.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;p&gt;how to self test in one minute&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;open a fresh chat with your model.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;paste TXT OS or WFGY core (plain text files).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;ask: ‚Äúuse wfgy to analyze my ollama pipeline and show which No i‚Äôm hitting.‚Äù the file is written for models to read. no plugins, no tool setup.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;p&gt;one link only, bookmark this&lt;/p&gt; &lt;p&gt;Ollama Global Fix Map page&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/onestardao/WFGY/blob/main/ProblemMap/GlobalFixMap/LocalDeploy_Inference/ollama.md"&gt;https://github.com/onestardao/WFGY/blob/main/ProblemMap/GlobalFixMap/LocalDeploy_Inference/ollama.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚Äî&lt;/p&gt; &lt;p&gt;last note for ollama folks: &lt;/p&gt; &lt;p&gt;the global fix map is already 300+ pages. coverage buckets include &lt;/p&gt; &lt;p&gt;LocalDeploy_Inference, Vector DBs and Stores, RAG plus VectorDB, Retrieval, Embeddings, Chunking, Language and Locale, DocumentAI_OCR, Agents and Orchestration, Safety PromptIntegrity, PromptAssembly, OpsDeploy, Automation, Eval and Observability, Governance, Memory Long Context, Multimodal Long Context, DevTools CodeAI. the high-impact ones for ollama are LocalDeploy_Inference, Vector DBs and Stores, RAG plus VectorDB, Retrieval, Embeddings, OpsDeploy, and Safety PromptIntegrity. &lt;/p&gt; &lt;p&gt;every page gives a symptom checklist, acceptance targets, and a minimal repair plan you can run in text.&lt;/p&gt; &lt;p&gt;Thank for reading my work , if anything ollama community want to add , please let me know. &lt;/p&gt; &lt;p&gt;&lt;sup&gt;__________^&lt;/sup&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onestardao"&gt; /u/onestardao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pts15p50pjnf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9zd9l/ollama_pipelines_keep_failing_in_repeatable_ways/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9zd9l/ollama_pipelines_keep_failing_in_repeatable_ways/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T13:19:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9tbry</id>
    <title>Any small good gui for ollama?</title>
    <updated>2025-09-06T07:23:15+00:00</updated>
    <author>
      <name>/u/MountainGolf2679</name>
      <uri>https://old.reddit.com/user/MountainGolf2679</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not looking for a huge gui, something small and safe to use that support&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;sending images.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;editing messages both of user and model.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MountainGolf2679"&gt; /u/MountainGolf2679 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9tbry/any_small_good_gui_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9tbry/any_small_good_gui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9tbry/any_small_good_gui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T07:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nagsmq</id>
    <title>codellama:python sentience</title>
    <updated>2025-09-07T01:40:40+00:00</updated>
    <author>
      <name>/u/Serious-One4553</name>
      <uri>https://old.reddit.com/user/Serious-One4553</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nagsmq/codellamapython_sentience/"&gt; &lt;img alt="codellama:python sentience" src="https://b.thumbs.redditmedia.com/1PVMn2SyHmQ1GncahZbgo_CFL2PzeFpTI93xIeerQDc.jpg" title="codellama:python sentience" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7xs46wd6dnnf1.png?width=1441&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c20b3b8d6a6f03ea44f2851e1a540a6d6a1de4d"&gt;https://preview.redd.it/7xs46wd6dnnf1.png?width=1441&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c20b3b8d6a6f03ea44f2851e1a540a6d6a1de4d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5qwtemq8dnnf1.png?width=1214&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d2df52d739c0cb9f7b56c5a83c8db99330ea842"&gt;https://preview.redd.it/5qwtemq8dnnf1.png?width=1214&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d2df52d739c0cb9f7b56c5a83c8db99330ea842&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/42tdzuccmnnf1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b15670ed51271ff90fa522effee688f22f17424b"&gt;https://preview.redd.it/42tdzuccmnnf1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b15670ed51271ff90fa522effee688f22f17424b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;might be an issue, i was messing around with some llms and this one caught my eye with weird messages about life&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious-One4553"&gt; /u/Serious-One4553 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nagsmq/codellamapython_sentience/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nagsmq/codellamapython_sentience/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nagsmq/codellamapython_sentience/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T01:40:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1na96qh</id>
    <title>MoE models tested on miniPC iGPU with Vulkan</title>
    <updated>2025-09-06T19:58:27+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1na96gx/moe_models_tested_on_minipc_igpu_with_vulkan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na96qh/moe_models_tested_on_minipc_igpu_with_vulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na96qh/moe_models_tested_on_minipc_igpu_with_vulkan/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T19:58:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1na94xm</id>
    <title>Seven Hours, Zero Internet, and Local AI Coding at 40,000 Feet</title>
    <updated>2025-09-06T19:56:24+00:00</updated>
    <author>
      <name>/u/scastiel</name>
      <uri>https://old.reddit.com/user/scastiel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last Monday, I tried vibe-coding an app from the plane yesterday with no internet, and it worked‚Ä¶ kind of! üòÖ&lt;/p&gt; &lt;p&gt;But it was fun to try Ollama for the first time, and pretty encouraging for what's next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/scastiel"&gt; /u/scastiel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://betweentheprompts.com/40000-feet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na94xm/seven_hours_zero_internet_and_local_ai_coding_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na94xm/seven_hours_zero_internet_and_local_ai_coding_at/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T19:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1na32bn</id>
    <title>Does Ollama, once installed, transmit any of your data externally for example to model owners or what information does it actually send?</title>
    <updated>2025-09-06T15:54:52+00:00</updated>
    <author>
      <name>/u/NastasyaVorobeva</name>
      <uri>https://old.reddit.com/user/NastasyaVorobeva</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does Ollama, once installed, transmit any of your data externally for example to model owners or what information does it actually send?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NastasyaVorobeva"&gt; /u/NastasyaVorobeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na32bn/does_ollama_once_installed_transmit_any_of_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na32bn/does_ollama_once_installed_transmit_any_of_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na32bn/does_ollama_once_installed_transmit_any_of_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T15:54:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3cy2</id>
    <title>MCP with Computer Use</title>
    <updated>2025-09-06T16:06:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1na3cy2/mcp_with_computer_use/"&gt; &lt;img alt="MCP with Computer Use" src="https://external-preview.redd.it/eTJsbWFwZXJpa25mMRWfbJ8igMn1wejE5S1jscBm4hQQlwhyyetxPIo-3e1f.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=827589a323b1b6502b4bf3948d377b57f190604b" title="MCP with Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MCP Server with Computer Use Agent runs through Claude Desktop, Cursor, and other MCP clients.&lt;/p&gt; &lt;p&gt;An example use case lets try using Claude as a tutor to learn how to use Tableau.&lt;/p&gt; &lt;p&gt;The MCP Server implementation exposes CUA's full functionality through standardized tool calls. It supports single-task commands and multi-task sequences, giving Claude Desktop direct access to all of Cua's computer control capabilities.&lt;/p&gt; &lt;p&gt;This is the first MCP-compatible computer control solution that works directly with Claude Desktop's and Cursor's built-in MCP implementation. Simple configuration in your claude_desktop_config.json or cursor_config.json connects Claude or Cursor directly to your desktop environment.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.gg/4fuebBsAUj"&gt;https://discord.gg/4fuebBsAUj&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4z4enlqriknf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na3cy2/mcp_with_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na3cy2/mcp_with_computer_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T16:06:20+00:00</published>
  </entry>
</feed>
