<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-25T10:08:35+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1p4eoje</id>
    <title>Will an RTX 6000 run the gpt-oss:120b model?</title>
    <updated>2025-11-23T05:31:08+00:00</updated>
    <author>
      <name>/u/pawnstew</name>
      <uri>https://old.reddit.com/user/pawnstew</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sizing a GPU for work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pawnstew"&gt; /u/pawnstew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4eoje/will_an_rtx_6000_run_the_gptoss120b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4eoje/will_an_rtx_6000_run_the_gptoss120b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4eoje/will_an_rtx_6000_run_the_gptoss120b_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T05:31:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4f8iv</id>
    <title>GPT-OSS 20B: Capacit√©s et Tests</title>
    <updated>2025-11-23T06:02:53+00:00</updated>
    <author>
      <name>/u/Desperate_Yellow_541</name>
      <uri>https://old.reddit.com/user/Desperate_Yellow_541</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Le mythe de la taille : comment un 20B bien r√©gl√© peut rivaliser avec les ‚Äúg√©ants‚Äù sur des t√¢ches d‚Äôextraction r√©elles&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Mod√®le : &lt;strong&gt;GPT-OSS 20B&lt;/strong&gt; (local)&lt;/li&gt; &lt;li&gt;Contexte : &lt;strong&gt;65k tokens&lt;/strong&gt; (menu PDF OCR bien sale)&lt;/li&gt; &lt;li&gt;T√¢che : extraire des &lt;strong&gt;prix corrects&lt;/strong&gt; d‚Äôun document bruit√©&lt;/li&gt; &lt;li&gt;Cl√© du succ√®s : &lt;strong&gt;Mirostat v2 + Repeat Penalty √† 1.2&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Id√©e centrale : le probl√®me n‚Äôest pas ‚Äúle mod√®le est trop petit‚Äù, mais &lt;strong&gt;comment on le contraint&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Contexte : ce qu‚Äôon essaie vraiment de faire&lt;/h1&gt; &lt;p&gt;L‚Äôindustrie de l‚ÄôIA est obs√©d√©e par la taille : 70B, 405B, 1T de param√®tres‚Ä¶&lt;br /&gt; Mais sur du &lt;strong&gt;local&lt;/strong&gt;, avec un 20B, je me suis rendu compte que :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Cas d‚Äôusage concret :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Un &lt;strong&gt;menu de restaurant&lt;/strong&gt; en PDF,&lt;/li&gt; &lt;li&gt;Pass√© par un &lt;strong&gt;OCR imparfait&lt;/strong&gt; (chiffres coll√©s, espaces bizarres, accents foutus),&lt;/li&gt; &lt;li&gt;Inject√© dans une fen√™tre de contexte de &lt;strong&gt;65 536 tokens&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Objectif : extraire un tableau propre &lt;em&gt;plat ‚Üí prix&lt;/em&gt;, &lt;strong&gt;sans halluciner&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Spoiler : au d√©part, le mod√®le a fait n‚Äôimporte quoi‚Ä¶ mais pas pour les raisons qu‚Äôon croit.&lt;/p&gt; &lt;h1&gt;I. L‚Äôillusion de la ‚Äúparesse cognitive‚Äù des LLM locaux&lt;/h1&gt; &lt;p&gt;Quand on joue avec des LLM locaux, on voit souvent :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄúD√©sol√©, ce texte est trop long‚Äù ‚Üí refus implicite&lt;/li&gt; &lt;li&gt;Boucles de texte ‚Üí le mod√®le r√©p√®te la m√™me phrase&lt;/li&gt; &lt;li&gt;Hallucinations de chiffres ou de lignes enti√®res&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;R√©flexe classique :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Sauf que dans mon cas, sur ce menu OCR :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Le mod√®le &lt;strong&gt;comprenait&lt;/strong&gt; globalement la structure,&lt;/li&gt; &lt;li&gt;Mais il avait un comportement ‚Äúparesseux‚Äù : &lt;ul&gt; &lt;li&gt;Il attribuait le &lt;strong&gt;m√™me prix&lt;/strong&gt; √† plusieurs plats,&lt;/li&gt; &lt;li&gt;Ou il ‚Äúinventait‚Äù des prix plausibles visuellement, mais &lt;strong&gt;faux&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ce n‚Äô√©tait pas un probl√®me de compr√©hension linguistique.&lt;br /&gt; C‚Äô√©tait une &lt;strong&gt;optimisation probabiliste&lt;/strong&gt; :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Le mod√®le se cale dans une orni√®re : ‚Äúj‚Äôai d√©j√† √©crit 150, √ßa marche bien, je vais remettre 150‚Äù.&lt;br /&gt; Notre job n‚Äôa pas √©t√© de ‚Äúlui expliquer mieux‚Äù le menu,&lt;br /&gt; mais de &lt;strong&gt;rendre cette orni√®re tr√®s co√ªteuse&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;II. Le ‚ÄúSettings Engineering‚Äù : trianguler les hyperparam√®tres&lt;/h1&gt; &lt;p&gt;Pour transformer ce g√©n√©rateur de texte probabiliste en quelque chose qui ressemble √† un &lt;strong&gt;agent de raisonnement fiable&lt;/strong&gt;, j‚Äôai fini par traiter les r√©glages comme un vrai espace de recherche.&lt;/p&gt; &lt;p&gt;J‚Äôai identifi√© trois leviers qui, ensemble, changent radicalement le comportement :&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Levier de stabilit√©&lt;/strong&gt; : Mirostat vs Top_K / Top_P&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Levier de v√©rit√©&lt;/strong&gt; : la p√©nalit√© de r√©p√©tition (Repeat Penalty)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Levier de charge cognitive&lt;/strong&gt; : ‚Äúforcer‚Äù le mod√®le √† r√©fl√©chir (System 2-like)&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;1. Mirostat v2 : stabiliser le chaos sur 65k tokens&lt;/h1&gt; &lt;p&gt;Les r√©glages classiques (Top_K, Top_P) sont des &lt;strong&gt;filtres statiques&lt;/strong&gt; :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ils coupent la queue de distribution,&lt;/li&gt; &lt;li&gt;Mais ils ne s‚Äôadaptent pas √† la &lt;strong&gt;difficult√© locale&lt;/strong&gt; du passage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sur un contexte de &lt;strong&gt;65k tokens&lt;/strong&gt;, tu as des zones :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Faciles&lt;/strong&gt; : traduction simple, texte tr√®s clair,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ambigu√´s&lt;/strong&gt; : chiffres flous, OCR parasit√©, colonnes bancales.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Avec Top_K / Top_P seul, le mod√®le n‚Äôa aucune conscience que ‚Äúl√†, c‚Äôest chaud‚Äù, ou ‚Äúl√†, c‚Äôest trivial‚Äù.&lt;/p&gt; &lt;p&gt;Avec &lt;strong&gt;Mirostat v2&lt;/strong&gt;, le comportement change :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;L‚Äôalgorithme surveille une forme de &lt;strong&gt;perplexit√©&lt;/strong&gt; (surprise du mod√®le) &lt;em&gt;en temps r√©el&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Intuition (simplifi√©e) : &lt;ul&gt; &lt;li&gt;Si le mod√®le &lt;strong&gt;boucle&lt;/strong&gt; ‚Üí la perplexit√© chute (trop pr√©visible) ‚Üí ‚Üí Mirostat &lt;strong&gt;injecte du chaos&lt;/strong&gt;, oblige le mod√®le √† sortir de la boucle.&lt;/li&gt; &lt;li&gt;Si le mod√®le &lt;strong&gt;part en d√©lire&lt;/strong&gt; ‚Üí perplexit√© explose ‚Üí ‚Üí Mirostat &lt;strong&gt;resserre&lt;/strong&gt; la g√©n√©ration vers quelque chose de plus coh√©rent.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;R√©sultat empirique :&lt;br /&gt; Sur ce menu OCR, Mirostat a rendu les r√©ponses :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Moins sujettes aux &lt;strong&gt;boucles&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Et plus robustes sur des zones ‚Äúcass√©es‚Äù du texte source.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. La p√©nalit√© de r√©p√©tition 1.2 : casser la voie facile&lt;/h1&gt; &lt;p&gt;La vraie bascule, pour l‚Äôextraction de &lt;strong&gt;prix&lt;/strong&gt;, est venue de la &lt;strong&gt;Repeat Penalty&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Ce qu‚Äôon lit partout :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄúMettez 1.05 ou 1.1, au-del√† vous tuez la qualit√© du texte.‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ce que j‚Äôai observ√© :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pour de la &lt;strong&gt;donn√©e chiffr√©e&lt;/strong&gt; dans un contexte bruit√©, le &lt;strong&gt;seuil critique&lt;/strong&gt; n‚Äôest pas 1.1, mais plut√¥t &lt;strong&gt;1.2&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Intuition :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;√Ä 1.1 : &lt;ul&gt; &lt;li&gt;Le ‚Äúco√ªt‚Äù de r√©√©crire &lt;code&gt;150&lt;/code&gt; est encore &lt;strong&gt;faible&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Le ‚Äúco√ªt‚Äù d‚Äôaller chercher &lt;code&gt;190&lt;/code&gt; dans le contexte OCR est &lt;strong&gt;plus √©lev√©&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Le mod√®le choisit : &lt;em&gt;‚ÄúJe recycle 150, c‚Äôest plus confortable.‚Äù&lt;/em&gt; ‚Üí &lt;strong&gt;Hallucination par r√©p√©tition.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;√Ä 1.2 : &lt;ul&gt; &lt;li&gt;La r√©p√©tition de &lt;code&gt;150&lt;/code&gt; devient &lt;strong&gt;prohibitive&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;Le mod√®le se retrouve &lt;strong&gt;bloqu√©&lt;/strong&gt; :‚ÄúJe veux √©crire un prix, mais je ne peux pas r√©√©crire 150.‚Äù&lt;/li&gt; &lt;li&gt;Son seul chemin ‚Äúpeu co√ªteux‚Äù devient alors : ‚Üí &lt;strong&gt;replonger dans le contexte&lt;/strong&gt;, chercher un autre motif valide (&lt;code&gt;190&lt;/code&gt;, &lt;code&gt;135&lt;/code&gt;, etc.).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ce n‚Äôest √©videmment pas une ‚Äúv√©rification logique‚Äù au sens humain, mais :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;En pratique, la mont√©e √† &lt;strong&gt;1.2&lt;/strong&gt; a fait dispara√Ætre un grand nombre de &lt;strong&gt;lignes dupliqu√©es&lt;/strong&gt; ou ‚Äúprix coll√©s partout‚Äù.&lt;/p&gt; &lt;h1&gt;III. Quand le mod√®le commence √† ‚Äúcompter‚Äù : √©mergence d‚Äôun pseudo System 2&lt;/h1&gt; &lt;p&gt;Une fois Mirostat v2 activ√© + Repeat Penalty √† 1.2, un truc int√©ressant est arriv√© dans les logs de r√©flexion (Chain-of-Thought).&lt;/p&gt; &lt;p&gt;Le mod√®le a g√©n√©r√© un raisonnement du type :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;C‚Äôest exactement le genre de comportement que j‚Äôattends d‚Äôun mod√®le ‚Äúplus gros‚Äù sur ce type de t√¢che :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Aligner&lt;/strong&gt; une liste de plats avec une liste de prix,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compter&lt;/strong&gt; les √©l√©ments des deux c√¥t√©s,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Recomposer&lt;/strong&gt; un prix cass√© par l‚ÄôOCR (&lt;code&gt;1 9 0&lt;/code&gt; ‚Üí &lt;code&gt;190&lt;/code&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ce qui est int√©ressant ici :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ce n‚Äôest pas ‚Äúmagiquement‚Äù apparu gr√¢ce au nombre de param√®tres,&lt;/li&gt; &lt;li&gt;C‚Äôest apparu &lt;strong&gt;apr√®s&lt;/strong&gt; qu‚Äôon ait &lt;strong&gt;ferm√© les voies faciles&lt;/strong&gt; : &lt;ul&gt; &lt;li&gt;refus,&lt;/li&gt; &lt;li&gt;r√©p√©tition,&lt;/li&gt; &lt;li&gt;hallucination d√©corative.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;On a, en quelque sorte, forc√© le mod√®le √† adopter un comportement plus proche d‚Äôun &lt;strong&gt;System 2&lt;/strong&gt; (au sens Kahneman) :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Et √ßa, avec &lt;strong&gt;un 20B local&lt;/strong&gt;, sur une machine perso, pas un datacenter.&lt;/p&gt; &lt;h1&gt;Conclusion : on n‚Äôa pas (toujours) besoin de mod√®les plus gros, on a besoin de meilleures contraintes&lt;/h1&gt; &lt;p&gt;Ce que cette exp√©rience sugg√®re pour les &lt;strong&gt;LLM locaux&lt;/strong&gt; :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pour beaucoup de t√¢ches &lt;strong&gt;administratives / d‚Äôextraction documentaire&lt;/strong&gt; : &lt;ul&gt; &lt;li&gt;factures, menus, tableaux OCR, listes, etc.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Le vrai levier n‚Äôest &lt;strong&gt;pas forc√©ment&lt;/strong&gt; : &lt;ul&gt; &lt;li&gt;‚Äúmonter en taille‚Äù,&lt;/li&gt; &lt;li&gt;‚Äúpasser au closed-source cloud‚Äù.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Mais plut√¥t :&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;En pratique, √ßa veut dire :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jouer s√©rieusement avec : &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Temp√©rature&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mirostat / Top_P&lt;/strong&gt;,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Repeat Penalty&lt;/strong&gt;,&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Et accepter que pour des t√¢ches &lt;strong&gt;critiques sur les chiffres&lt;/strong&gt;, un mod√®le ‚Äúmoins fluide‚Äù mais &lt;strong&gt;plus contraint&lt;/strong&gt; est &lt;strong&gt;pr√©f√©rable&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Annexe : le ‚ÄúPreset‚Äù qui a d√©clench√© le changement&lt;/h1&gt; &lt;p&gt;Pour ceux qui veulent le preset brut, voici la config qui a march√© &lt;strong&gt;dans mon cas&lt;/strong&gt;&lt;br /&gt; (GPT-OSS 20B via Ollama / OpenWebUI, extraction de prix sur menu OCR, contexte 65k) :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Mod√®le : GPT-OSS 20B Contexte : 65 536 tokens T√¢che : extraction de prix depuis un PDF OCR bruit√© [Architecture cognitive] Mirostat : ON (Mode 2) Mirostat Tau : 5.0 Mirostat Eta : 0.1 Reasoning Effort : High [Param√®tres de contrainte] Temp√©rature : 0.60 Top_p : 0.90 Repeat Penalty : 1.20 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Je ne pr√©tends pas que ce preset est universel, mais :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sur ce cas d‚Äôusage, il a fait la diff√©rence entre : &lt;ul&gt; &lt;li&gt;un mod√®le qui ‚Äúfait joli mais faux‚Äù,&lt;/li&gt; &lt;li&gt;et un mod√®le qui &lt;strong&gt;compte&lt;/strong&gt; et &lt;strong&gt;recroise&lt;/strong&gt; les donn√©es du PDF.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Si √ßa int√©resse du monde, je peux partager dans un autre post :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Un exemple de prompt complet (avec consignes d‚Äôextraction),&lt;/li&gt; &lt;li&gt;La structure de validation que j‚Äôutilise derri√®re (checks simples type ‚Äúnombre de lignes / coh√©rence des prix‚Äù),&lt;/li&gt; &lt;li&gt;Et pourquoi &lt;strong&gt;je teste toujours d‚Äôabord des changements de settings&lt;/strong&gt; avant de conclure qu‚Äôun mod√®le est ‚Äútrop petit‚Äù.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Yellow_541"&gt; /u/Desperate_Yellow_541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4f8iv/gptoss_20b_capacit√©s_et_tests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4f8iv/gptoss_20b_capacit√©s_et_tests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4f8iv/gptoss_20b_capacit√©s_et_tests/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T06:02:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4f9j0</id>
    <title>GPT-OSS 20B: Capacit√©s et Tests</title>
    <updated>2025-11-23T06:04:36+00:00</updated>
    <author>
      <name>/u/Desperate_Yellow_541</name>
      <uri>https://old.reddit.com/user/Desperate_Yellow_541</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Yellow_541"&gt; /u/Desperate_Yellow_541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ollama/comments/1p4f8iv/gptoss_20b_capacit√©s_et_tests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4f9j0/gptoss_20b_capacit√©s_et_tests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4f9j0/gptoss_20b_capacit√©s_et_tests/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T06:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4qhpr</id>
    <title>Manus</title>
    <updated>2025-11-23T16:09:16+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://manus.im/invitation/BHKBHIT0WJFORVO"&gt;https://manus.im/invitation/BHKBHIT0WJFORVO&lt;/a&gt; login with gmail or apple, or microsoft&lt;/p&gt; &lt;p&gt;Find redeem section in invite friends and use this code to get 1000 credits: njexode&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4qhpr/manus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4qhpr/manus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4qhpr/manus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T16:09:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ie4a</id>
    <title>Help with a prompt?</title>
    <updated>2025-11-23T09:19:57+00:00</updated>
    <author>
      <name>/u/SystemAromatic</name>
      <uri>https://old.reddit.com/user/SystemAromatic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, i'm having a problem. There is this program that gives me a question and 4 Answers but i cant copypaste and i'm pretty lazy to rewrite it all. I've Tried a couple of prompt but i'm pretty new to this and i don't really know if there even is a proper way to do what i want to do, any suggestion? &lt;/p&gt; &lt;p&gt;Tl;dr: Prompt for ollama to read my screen (?) And answer without copypaste &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SystemAromatic"&gt; /u/SystemAromatic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4ie4a/help_with_a_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4ie4a/help_with_a_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4ie4a/help_with_a_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T09:19:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4jen4</id>
    <title>ToolNeuron ‚Äî Privacy-first AI hub on Android (offline + online models, plugin support)</title>
    <updated>2025-11-23T10:23:14+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p4jen4/toolneuron_privacyfirst_ai_hub_on_android_offline/"&gt; &lt;img alt="ToolNeuron ‚Äî Privacy-first AI hub on Android (offline + online models, plugin support)" src="https://external-preview.redd.it/AuuQ6Jw6VH9e6ZU098Q0G_x_fI41xouhvcXLl_FeCi8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a50a03c0422760aefd8861c44779d24f9c6b02ab" title="ToolNeuron ‚Äî Privacy-first AI hub on Android (offline + online models, plugin support)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://tool-neuron.vercel.app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4jen4/toolneuron_privacyfirst_ai_hub_on_android_offline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4jen4/toolneuron_privacyfirst_ai_hub_on_android_offline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T10:23:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4lxnr</id>
    <title>Best Local Coding Agent Model for 64GB RAM and 12GB VRAM?</title>
    <updated>2025-11-23T12:49:55+00:00</updated>
    <author>
      <name>/u/fallen0523</name>
      <uri>https://old.reddit.com/user/fallen0523</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallen0523"&gt; /u/fallen0523 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1p4lwyc/best_local_coding_agent_model_for_64gb_ram_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4lxnr/best_local_coding_agent_model_for_64gb_ram_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4lxnr/best_local_coding_agent_model_for_64gb_ram_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T12:49:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4od24</id>
    <title>Summarize and manage local files?</title>
    <updated>2025-11-23T14:42:32+00:00</updated>
    <author>
      <name>/u/cl326</name>
      <uri>https://old.reddit.com/user/cl326</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a huge number of files on my laptop. They are not well organized or named. I‚Äôve removed all duplicates that I can by comparing hashes and names. Now I‚Äôd like to use Ollama to summarize each file in a folder so ai can get an idea of what the file is about if I can‚Äôt tell by the name. The files are mostly MS Office documents, PDFs, and images. Is there a model that you‚Äôd suggest? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cl326"&gt; /u/cl326 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4od24/summarize_and_manage_local_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4od24/summarize_and_manage_local_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4od24/summarize_and_manage_local_files/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T14:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3vpni</id>
    <title>Your local Ollama agents can be just as good as closed-source models - I open-sourced Stanford's ACE framework that makes agents learn from mistakes</title>
    <updated>2025-11-22T15:17:51+00:00</updated>
    <author>
      <name>/u/cheetguy</name>
      <uri>https://old.reddit.com/user/cheetguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I implemented Stanford's &lt;a href="https://arxiv.org/abs/2510.04618"&gt;Agentic Context Engineering paper&lt;/a&gt; for Ollama. The framework makes agents learn from their own execution feedback through in-context learning instead of fine-tuning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; Agent runs task ‚Üí reflects on what worked/failed ‚Üí curates strategies into playbook ‚Üí uses playbook on next run&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Improvement:&lt;/strong&gt; Paper shows +17.1pp accuracy improvement vs base LLM (‚âà+40% relative improvement) on agent benchmarks (DeepSeek-V3.1 non-thinking mode), helping close the gap with closed-source models. All through in-context learning (no fine-tuning needed).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Open-Source Implementation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop into existing agents in ~10 lines of code&lt;/li&gt; &lt;li&gt;Works with any Ollama model (Llama, Qwen, Mistral, DeepSeek, etc.)&lt;/li&gt; &lt;li&gt;Real-world test on browser automation agent: &lt;ul&gt; &lt;li&gt;30% ‚Üí 100% success rate&lt;/li&gt; &lt;li&gt;82% fewer steps&lt;/li&gt; &lt;li&gt;65% decrease in token cost&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;https://github.com/kayba-ai/agentic-context-engine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ollama Starter Template: &lt;a href="https://github.com/kayba-ai/agentic-context-engine/blob/main/examples/ollama/ollama_starter_template.py"&gt;https://github.com/kayba-ai/agentic-context-engine/blob/main/examples/ollama/ollama_starter_template.py&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear if anyone tries this with Ollama! Especially curious how it performs with different Ollama models.&lt;/p&gt; &lt;p&gt;I'm currently actively improving this based on feedback - &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;‚≠ê the repo&lt;/a&gt; so you can stay updated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cheetguy"&gt; /u/cheetguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-22T15:17:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p52b8z</id>
    <title>RAG follow-ups not working ‚Äî Qwen2.5 ignores previous context and gives unrelated answers</title>
    <updated>2025-11-24T00:10:24+00:00</updated>
    <author>
      <name>/u/NoBlackberry3264</name>
      <uri>https://old.reddit.com/user/NoBlackberry3264</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm building a &lt;strong&gt;RAG-based chat system&lt;/strong&gt; using FastAPI + &lt;strong&gt;Qwen/Qwen2.5-7B-Instruct&lt;/strong&gt;, and I‚Äôm running into an issue with follow-up queries.&lt;/p&gt; &lt;p&gt;The first query works fine, retrieving relevant documents from my knowledge base. But when the user asks a follow-up question, the model completely ignores previous context and fetches unrelated information.&lt;/p&gt; &lt;h1&gt;Example Payload (Client Request)&lt;/h1&gt; &lt;p&gt;Here‚Äôs the structure of the payload my client sends:&lt;br /&gt; {&lt;/p&gt; &lt;p&gt;&amp;quot;system_persona&amp;quot;: &amp;quot;KB&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;system_prompt&amp;quot;: { ... }, &lt;/p&gt; &lt;p&gt;&amp;quot;context&amp;quot;: [&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;pageUrl&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;sourceUrl&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;pageUrl&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;sourceUrl&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;],&lt;/p&gt; &lt;p&gt;&amp;quot;chat_history&amp;quot;: [&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;query&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;response&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;query&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;response&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;],&lt;/p&gt; &lt;p&gt;&amp;quot;query&amp;quot;: &amp;quot;nabil bank ko baryama bhana?&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Issue:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Follow-ups are not linked to previous conversation.&lt;/li&gt; &lt;li&gt;Chat history is sent but not effectively used.&lt;/li&gt; &lt;li&gt;Retrieval is based only on the latest query.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoBlackberry3264"&gt; /u/NoBlackberry3264 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p52b8z/rag_followups_not_working_qwen25_ignores_previous/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p52b8z/rag_followups_not_working_qwen25_ignores_previous/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p52b8z/rag_followups_not_working_qwen25_ignores_previous/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T00:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4wq3l</id>
    <title>ollama troubles</title>
    <updated>2025-11-23T20:15:31+00:00</updated>
    <author>
      <name>/u/Remote-Ad8602</name>
      <uri>https://old.reddit.com/user/Remote-Ad8602</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi people&lt;br /&gt; I recently heard about ollama and tought of giving it a try after installing the app i tried to download some existing models like gpt and qwen when try to give a prompt for the model the screen keeps loading for long time and so on, one time i had to wait 44 mins just to see an error message so i tried removing the app and reinstall again even after multiple tries I still couldn't figure out what's wrong is it my computer or some problems with the app, I use a Macbook Air M4 chip &lt;/p&gt; &lt;p&gt;has anyone faced the same issue, please let me know is there any remedy to get it working normally let me know guys....&lt;/p&gt; &lt;p&gt;cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remote-Ad8602"&gt; /u/Remote-Ad8602 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4wq3l/ollama_troubles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4wq3l/ollama_troubles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4wq3l/ollama_troubles/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T20:15:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4z140</id>
    <title>Need some honest opinions on GPU Ai in a box</title>
    <updated>2025-11-23T21:49:26+00:00</updated>
    <author>
      <name>/u/Whyme-__-</name>
      <uri>https://old.reddit.com/user/Whyme-__-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is related to Nvidia Spark and its competitors. I have a use case where I have to deliver my Ai software to deploy on customer infrastructure. I have 3 8b models fine tuned for each use case. I want to know if using a Nvidia spark or similar GPU in a box is worth the investment for privacy, speed and economics. &lt;/p&gt; &lt;p&gt;For my use case my models and software burn about $2000 per month if I rent a pod using runpod and I have to be extra careful due to rate limits. I want to consider running my models using llamacpp or ollama or offering direct inference for customer using their own on prem machine shipped by me. &lt;/p&gt; &lt;p&gt;Here are my 2 concern:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Are these machines stable enough to deploy in production environments? I know they run Linux and my software stack is dockerized so won‚Äôt affect much. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Cost: I know as we enter the new year GPU cost might go down but should that be something to wait it out or get 1 DGX spark box and test things out to see the functionality and ease of deployment? I can always repurpose the box for my startup instead of relying on Runpod GPU. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This community has helped me a lot in the past, I‚Äôm hoping to get some answers from the community regarding these issues. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whyme-__-"&gt; /u/Whyme-__- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4z140/need_some_honest_opinions_on_gpu_ai_in_a_box/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4z140/need_some_honest_opinions_on_gpu_ai_in_a_box/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4z140/need_some_honest_opinions_on_gpu_ai_in_a_box/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T21:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4o31k</id>
    <title>Nanocoder VS Code Plugin is Coming Along!</title>
    <updated>2025-11-23T14:30:09+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p4o31k/nanocoder_vs_code_plugin_is_coming_along/"&gt; &lt;img alt="Nanocoder VS Code Plugin is Coming Along!" src="https://preview.redd.it/0bdpxyrvm03g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a452110a43bda75376e5ffac7e29e49df77ae63" title="Nanocoder VS Code Plugin is Coming Along!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0bdpxyrvm03g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4o31k/nanocoder_vs_code_plugin_is_coming_along/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4o31k/nanocoder_vs_code_plugin_is_coming_along/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T14:30:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5b6pd</id>
    <title>Win 7 days of unlimited API access on GLM-4.6! 7 winners</title>
    <updated>2025-11-24T07:53:03+00:00</updated>
    <author>
      <name>/u/cobra91310</name>
      <uri>https://old.reddit.com/user/cobra91310</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cobra91310"&gt; /u/cobra91310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ZaiGLM/comments/1p5b6cw/win_7_days_of_unlimited_api_access_on_glm46_7/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5b6pd/win_7_days_of_unlimited_api_access_on_glm46_7/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5b6pd/win_7_days_of_unlimited_api_access_on_glm46_7/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T07:53:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p56i8v</id>
    <title>M.I.M.I.R - Now with visual intelligence built in for embeddings - MIT licensed - use with local ollama or llama.cpp for full control over your data</title>
    <updated>2025-11-24T03:29:23+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p56i8v/mimir_now_with_visual_intelligence_built_in_for/"&gt; &lt;img alt="M.I.M.I.R - Now with visual intelligence built in for embeddings - MIT licensed - use with local ollama or llama.cpp for full control over your data" src="https://preview.redd.it/icw2e1nbj43g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6af6e5db3bde853b33115fc981027e4bea84b11" title="M.I.M.I.R - Now with visual intelligence built in for embeddings - MIT licensed - use with local ollama or llama.cpp for full control over your data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/icw2e1nbj43g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p56i8v/mimir_now_with_visual_intelligence_built_in_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p56i8v/mimir_now_with_visual_intelligence_built_in_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T03:29:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5lwse</id>
    <title>Neural Network?</title>
    <updated>2025-11-24T16:40:04+00:00</updated>
    <author>
      <name>/u/spreader123</name>
      <uri>https://old.reddit.com/user/spreader123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p5lwse/neural_network/"&gt; &lt;img alt="Neural Network?" src="https://b.thumbs.redditmedia.com/p3QvsuKFfOO8YLriXQxxpJVaY4BrKd-MO8_AO9SEQSg.jpg" title="Neural Network?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spreader123"&gt; /u/spreader123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/neuralnetworks/comments/1p5jjig/neural_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5lwse/neural_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5lwse/neural_network/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T16:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5hoqh</id>
    <title>Mistral-Small3.2:latest - Broken after a recent Ollama update?</title>
    <updated>2025-11-24T13:56:05+00:00</updated>
    <author>
      <name>/u/Comfortable_Ad_8117</name>
      <uri>https://old.reddit.com/user/Comfortable_Ad_8117</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p5hoqh/mistralsmall32latest_broken_after_a_recent_ollama/"&gt; &lt;img alt="Mistral-Small3.2:latest - Broken after a recent Ollama update?" src="https://b.thumbs.redditmedia.com/Rw0CGw1zj8CmJZE93FbqE6s_PBI7HNwH1lT2EDsSJss.jpg" title="Mistral-Small3.2:latest - Broken after a recent Ollama update?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone else seen this behavior? Repetitive characters when interacting with the 24b Mistral model? This was working fine until a recent update of Ollama. Any suggestions for an alternative model around the same size that has similar vision capability and instruction following skills. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dzdq4ybhn73g1.png?width=1030&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30993c9fa83fb47de7923593a007e0a3fc7677fa"&gt;https://preview.redd.it/dzdq4ybhn73g1.png?width=1030&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30993c9fa83fb47de7923593a007e0a3fc7677fa&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Ad_8117"&gt; /u/Comfortable_Ad_8117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5hoqh/mistralsmall32latest_broken_after_a_recent_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5hoqh/mistralsmall32latest_broken_after_a_recent_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5hoqh/mistralsmall32latest_broken_after_a_recent_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T13:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5jy43</id>
    <title>archgw (0.3.20) - removed all (500mb) python deps in the request path. Ollama and Rust-first now</title>
    <updated>2025-11-24T15:27:32+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/katanemo/archgw"&gt;archgw&lt;/a&gt; (a models-native sidecar proxy for AI agents) offered two capabilities that required loading small LLMs in memory: guardrails to prevent jailbreak attempts, and function-calling for routing requests to the right downstream tool or agent. These built-in features required the project running a thread-safe python process that used libs like transformers, torch, safetensors, etc. 500M in dependencies, not to mention all the security vulnerabilities in the dep tree. Not hating on python, but our GH project was flagged with all sorts of issues.&lt;/p&gt; &lt;p&gt;Those models are loaded as a separate out-of-process server via ollama/lama.cpp which you all know are built in C++/Go. Lighter, faster and safer. And ONLY if the developer uses these features of the product. This meant 9000 lines of less code, a total start time of &amp;lt;2 seconds (vs 30+ seconds), etc.&lt;/p&gt; &lt;p&gt;Why archgw? So that you can build AI agents in any language or framework and offload the plumbing work in AI (like agent routing/hand-off, guardrails, zero-code logs and traces, and a unified API for all LLMs) to a durable piece of infrastructure, deployed as a sidecar.&lt;/p&gt; &lt;p&gt;Proud of this release, so sharing üôè&lt;/p&gt; &lt;p&gt;P.S Sample demos, the CLI and some tests still use python. But we'll move those over to Rust in the coming months. We are punting convenience for robustness.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5jy43/archgw_0320_removed_all_500mb_python_deps_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5jy43/archgw_0320_removed_all_500mb_python_deps_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5jy43/archgw_0320_removed_all_500mb_python_deps_in_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T15:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5lmcw</id>
    <title>Need clarifications or advice with coding and ollama.</title>
    <updated>2025-11-24T16:29:37+00:00</updated>
    <author>
      <name>/u/Lotus-006</name>
      <uri>https://old.reddit.com/user/Lotus-006</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, i'm not sure if tbe models can works like in vscode for coding with ai like claude sonnet or gpt with agent mode.&lt;/p&gt; &lt;p&gt;i tried some days before but the speed to react was slow even with my hardware i9-13900k and 96gb ddr5 at 6800mhz and RTX Msi 5070ti oc 16gb gen 5&lt;/p&gt; &lt;p&gt;i tried with ollama and also on docker desktop but not very usefull as in vscode compared to claude or gpt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lotus-006"&gt; /u/Lotus-006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5lmcw/need_clarifications_or_advice_with_coding_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5lmcw/need_clarifications_or_advice_with_coding_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5lmcw/need_clarifications_or_advice_with_coding_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T16:29:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1p66utd</id>
    <title>M.I.M.I.R - drag and drop graph task UI + lambdas - MIT License - use ollama completely local for offline task orchestration.</title>
    <updated>2025-11-25T08:03:55+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p66tku"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p66utd/mimir_drag_and_drop_graph_task_ui_lambdas_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p66utd/mimir_drag_and_drop_graph_task_ui_lambdas_mit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-25T08:03:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p62zxz</id>
    <title>formatting_func issue (unsloth)</title>
    <updated>2025-11-25T04:18:27+00:00</updated>
    <author>
      <name>/u/toavepa</name>
      <uri>https://old.reddit.com/user/toavepa</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toavepa"&gt; /u/toavepa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/unsloth/comments/1p62znc/formatting_func_issue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p62zxz/formatting_func_issue_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p62zxz/formatting_func_issue_unsloth/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-25T04:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6345t</id>
    <title>Anyone doing an import of AI2's open-source Olmo3 model to Ollama?</title>
    <updated>2025-11-25T04:24:46+00:00</updated>
    <author>
      <name>/u/Conser-ai</name>
      <uri>https://old.reddit.com/user/Conser-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Subject line says it all. We use Ollama and small LMs for AI research, running locally for reproducibility. The Olmo series is particularly attractive since one can also know what it is trained on, thereby eliminating potential for data contamination. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conser-ai"&gt; /u/Conser-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p6345t/anyone_doing_an_import_of_ai2s_opensource_olmo3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p6345t/anyone_doing_an_import_of_ai2s_opensource_olmo3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p6345t/anyone_doing_an_import_of_ai2s_opensource_olmo3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-25T04:24:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1p61c5w</id>
    <title>Computer Use with Claude Opus 4.5</title>
    <updated>2025-11-25T02:56:09+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p61c5w/computer_use_with_claude_opus_45/"&gt; &lt;img alt="Computer Use with Claude Opus 4.5" src="https://external-preview.redd.it/cmp6aDE1OHBpYjNnMQ_ZQVTLVe7PyWJ0CuMi4sbpJgazjwD6JSk5JzpvEusC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f15feb81f8963c61b65ccb94fb57dde4fdc42069" title="Computer Use with Claude Opus 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Computer Use with cua playground.&lt;/p&gt; &lt;p&gt;Claude Opus 4.5 is 80.9% on SWE Bench. Pretty good for agentic and computer use tasks.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try yourself : &lt;a href="https://cua.ai/"&gt;https://cua.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6hq5mthpib3g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p61c5w/computer_use_with_claude_opus_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p61c5w/computer_use_with_claude_opus_45/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-25T02:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p64o5b</id>
    <title>Askimo: Open source of Ollama native desktop client</title>
    <updated>2025-11-25T05:50:03+00:00</updated>
    <author>
      <name>/u/Revolutionary-Judge9</name>
      <uri>https://old.reddit.com/user/Revolutionary-Judge9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building a desktop client called Askimo, and I use it with Ollama and other AI providers every day. I know there are already a lot of Ollama GUIs out there, but I kept running into the same problems: browser tabs slowing down, long chats eating memory, and losing good prompts I wanted to reuse.&lt;/p&gt; &lt;p&gt;This app actually started as a CLI tool I wrote for automation at work. After running into slowdowns and crashes in browser-based chats, I wanted something more solid, so I built a desktop client too. I treated myself as the first customer and added the features I kept wishing other apps had but never did, so I ended up creating the one I wanted to use.&lt;/p&gt; &lt;p&gt;I‚Äôve attached a short demo video if you want to see how it works.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p64o5b/video/du0zq92pbc3g1/player"&gt;Askimo Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also wrote a quick overview of the desktop client‚Äôs features on my blog. You can find everything here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Installation page: &lt;a href="https://askimo.chat/docs/desktop/installation/"&gt;https://askimo.chat/docs/desktop/installation/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub links: &lt;a href="https://github.com/haiphucnguyen/askimo"&gt;https://github.com/haiphucnguyen/askimo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Askimo desktop features: &lt;a href="https://askimo.chat/blog/askimo-with-ollama-the-best-desktop-for-local-ai/"&gt;https://askimo.chat/blog/askimo-with-ollama-the-best-desktop-for-local-ai/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm planning to keep adding more features, so any feedback from the community is definitely welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Revolutionary-Judge9"&gt; /u/Revolutionary-Judge9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p64o5b/askimo_open_source_of_ollama_native_desktop_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p64o5b/askimo_open_source_of_ollama_native_desktop_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p64o5b/askimo_open_source_of_ollama_native_desktop_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-25T05:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p65zvj</id>
    <title>Worth using both qwen 3 and llama3.2 for Linux system engineering?</title>
    <updated>2025-11-25T07:09:15+00:00</updated>
    <author>
      <name>/u/Zecside</name>
      <uri>https://old.reddit.com/user/Zecside</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got interested in deploying an Llm model locally to help me in my daily tasks as Linux sysadmin and I wonder if it would be useful. Leaving confidentiality issues solved , For example Can it help in debugging and log analysis? In the sense , are its reponses relevant? Thanks ! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zecside"&gt; /u/Zecside &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p65zvj/worth_using_both_qwen_3_and_llama32_for_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p65zvj/worth_using_both_qwen_3_and_llama32_for_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p65zvj/worth_using_both_qwen_3_and_llama32_for_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-25T07:09:15+00:00</published>
  </entry>
</feed>
