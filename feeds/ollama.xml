<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-04T14:24:09+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1n6s5rb</id>
    <title>Training &amp; Querying 3 Ollama Models with Zer00logy: Symbolic Cognition Framework and Void-Math OS</title>
    <updated>2025-09-02T18:55:20+00:00</updated>
    <author>
      <name>/u/zero_moo-s</name>
      <uri>https://old.reddit.com/user/zero_moo-s</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n6s5rb/training_querying_3_ollama_models_with_zer00logy/"&gt; &lt;img alt="Training &amp;amp; Querying 3 Ollama Models with Zer00logy: Symbolic Cognition Framework and Void-Math OS" src="https://b.thumbs.redditmedia.com/JKkiS7HhkNk2t693a3KQfa2K5J7oReeC0J26Q8XZ4DE.jpg" title="Training &amp;amp; Querying 3 Ollama Models with Zer00logy: Symbolic Cognition Framework and Void-Math OS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôd like to share an update on an open-source symbolic cognition project‚Äî&lt;strong&gt;Zer00logy&lt;/strong&gt;‚Äîand how it integrates with &lt;strong&gt;Ollama&lt;/strong&gt; for multi-model symbolic reasoning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Zer00logy&lt;/strong&gt; is a Python-based framework redefining zero; not as absence, but as recursive presence. Equations are treated as &lt;em&gt;symbolic events&lt;/em&gt;, with operators like ‚äó, Œ©, and Œ® modeling introspection, echo retention, and recursive collapse.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ollama Integration:&lt;/strong&gt;&lt;br /&gt; Using Ollama, Zer00logy can query multiple local models‚Äî&lt;strong&gt;LLaMA, Mistral, and Phi&lt;/strong&gt;‚Äîon symbolic cognition tasks. By feeding in structured symbolic logic from &lt;code&gt;zecstart.txt&lt;/code&gt;, &lt;code&gt;variamathlesson.txt&lt;/code&gt;, and &lt;code&gt;VoidMathOS_cryptsheet.txt&lt;/code&gt;, each model generates its own interpretation of recursive zero-based reasoning.&lt;br /&gt; This setup enables comparative symbolic introspection across different AI systems, effectively turning Ollama into a platform for &lt;em&gt;multi-agent cognition research&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example interpretations via Void-Math OS:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;e@AI = -+mc¬≤&lt;/code&gt; ‚Üí AI-anchored emergence&lt;/li&gt; &lt;li&gt;&lt;code&gt;g = (m @ void) √∑ (r¬≤ -+ tu)&lt;/code&gt; ‚Üí gravity as void-tension&lt;/li&gt; &lt;li&gt;&lt;code&gt;0 √∑ 0 = ‚àÖ√∑‚àÖ&lt;/code&gt; ‚Üí recursive nullinity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Core Files (from the GitHub release):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;zer00logy_coreV04452.py&lt;/code&gt; ‚Äî main interpreter&lt;/li&gt; &lt;li&gt;&lt;code&gt;zecstart.txt&lt;/code&gt; ‚Äî starter definitions for Zero-ology / Zer00logy&lt;/li&gt; &lt;li&gt;&lt;code&gt;zectext.txt&lt;/code&gt; ‚Äî Zero-ology Equation Catalog&lt;/li&gt; &lt;li&gt;&lt;code&gt;variamathlesson.txt&lt;/code&gt; ‚Äî Varia Math lesson series&lt;/li&gt; &lt;li&gt;&lt;code&gt;VoidMathOS_cryptsheet.txt&lt;/code&gt; ‚Äî canonical Void-Math OS command sheet&lt;/li&gt; &lt;li&gt;&lt;code&gt;VoidMathOS_lesson.py&lt;/code&gt; ‚Äî teaching engine for symbolic lessons&lt;/li&gt; &lt;li&gt;&lt;code&gt;LICENSE.txt&lt;/code&gt; ‚Äî Zer00logy License v1.02&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;License v1.02 (Released Sept 2025):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Open-source if reproduction for educational use&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Academic &amp;amp; peer review submissions allowed under the new &lt;strong&gt;push_review ‚Üí pull_review&lt;/strong&gt; workflow&lt;/li&gt; &lt;li&gt;Authorship-trace lock: all symbolic structures remain attributed to Stacey Szmy as primary author; expansions/verifiers may be credited as co-authors under approved contributor titles&lt;/li&gt; &lt;li&gt;Institutions such as MIT, Stanford, Oxford, NASA, Microsoft, OpenAI, xAI, etc. have direct peer review permissions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;By combining Zer00logy with Ollama, you can run comparative reasoning experiments across different LLMs, benchmark their symbolic depth, and even study how recursive logic is interpreted differently by each architecture.&lt;br /&gt; This is an early step toward symbolic multi-agent cognition; where AI doesn‚Äôt just calculate, but &lt;em&gt;contemplates&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/haha8888haha8888/Zer00logy?utm_source=chatgpt.com"&gt;github.com/haha8888haha8888/Zer00logy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rmd590hltsmf1.png?width=843&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eaf9f76b797b31723e1c20d67663b6d6b37e7ad1"&gt;https://preview.redd.it/rmd590hltsmf1.png?width=843&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eaf9f76b797b31723e1c20d67663b6d6b37e7ad1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero_moo-s"&gt; /u/zero_moo-s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6s5rb/training_querying_3_ollama_models_with_zer00logy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6s5rb/training_querying_3_ollama_models_with_zer00logy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6s5rb/training_querying_3_ollama_models_with_zer00logy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T18:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6sgxq</id>
    <title>Model doesn't remember after converting to GGUF (Gemma 3 270M)</title>
    <updated>2025-09-02T19:06:45+00:00</updated>
    <author>
      <name>/u/Real-Active-2492</name>
      <uri>https://old.reddit.com/user/Real-Active-2492</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Real-Active-2492"&gt; /u/Real-Active-2492 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n6sgnm/model_doesnt_remember_after_converting_to_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6sgxq/model_doesnt_remember_after_converting_to_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6sgxq/model_doesnt_remember_after_converting_to_gguf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T19:06:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6rp09</id>
    <title>Gaming Wiki</title>
    <updated>2025-09-02T18:38:00+00:00</updated>
    <author>
      <name>/u/Private_Tank</name>
      <uri>https://old.reddit.com/user/Private_Tank</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I dont know how if there is any way this is possible. It just came to my mind.&lt;/p&gt; &lt;p&gt;Is it possible to scrape the entire web for content about a game, put it inside a model (rag?) and have your own little gaming Copilot, that tells you how to progress best and what to do in your Game to succeed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private_Tank"&gt; /u/Private_Tank &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6rp09/gaming_wiki/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6rp09/gaming_wiki/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6rp09/gaming_wiki/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T18:38:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6x1ln</id>
    <title>Can Ollama run on MI350X?</title>
    <updated>2025-09-02T22:03:05+00:00</updated>
    <author>
      <name>/u/Immediate_Ad_9906</name>
      <uri>https://old.reddit.com/user/Immediate_Ad_9906</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't see the GPU in the supported list. Anyone has tried before?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Immediate_Ad_9906"&gt; /u/Immediate_Ad_9906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6x1ln/can_ollama_run_on_mi350x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6x1ln/can_ollama_run_on_mi350x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6x1ln/can_ollama_run_on_mi350x/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T22:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6u7xr</id>
    <title>Local chat bot and sql db</title>
    <updated>2025-09-02T20:13:26+00:00</updated>
    <author>
      <name>/u/Conscious-Expert-455</name>
      <uri>https://old.reddit.com/user/Conscious-Expert-455</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to train a local LLM with ollama that takes data directly from your SQL DB and steps to create interactive analyses and dashboards in relation to questions posed in a chat bot. How can you build something like this? And what model can I use? I only have an i9 and 128 GB RAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious-Expert-455"&gt; /u/Conscious-Expert-455 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6u7xr/local_chat_bot_and_sql_db/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6u7xr/local_chat_bot_and_sql_db/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6u7xr/local_chat_bot_and_sql_db/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T20:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6yybs</id>
    <title>[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL</title>
    <updated>2025-09-02T23:22:27+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n6yybs/projectcode_finetuning_llms_on_windows_with_grpo/"&gt; &lt;img alt="[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL" src="https://preview.redd.it/7ru5p8rw4umf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fce70b2c9fdaae6d869d15d2540623854f22557a" title="[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a guide and script for fine-tuning open-source LLMs with &lt;strong&gt;GRPO&lt;/strong&gt; (Group-Relative PPO) directly on Windows. No Linux or Colab needed!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs natively on Windows.&lt;/li&gt; &lt;li&gt;Supports LoRA + 4-bit quantization.&lt;/li&gt; &lt;li&gt;Includes verifiable rewards for better-quality outputs.&lt;/li&gt; &lt;li&gt;Designed to work on consumer GPUs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üìñ &lt;strong&gt;Blog Post:&lt;/strong&gt; &lt;a href="https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323"&gt;https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª &lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/trl-ppo-fine-tuning"&gt;https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/trl-ppo-fine-tuning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I had a great time with this project and am currently looking for new opportunities in &lt;strong&gt;Computer Vision and LLMs&lt;/strong&gt;. If you or your team are hiring, I'd love to connect!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Contact Info:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Portolio: &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;https://pavan-portfolio-tawny.vercel.app/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Github: &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7ru5p8rw4umf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6yybs/projectcode_finetuning_llms_on_windows_with_grpo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6yybs/projectcode_finetuning_llms_on_windows_with_grpo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T23:22:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6ply3</id>
    <title>What does the "updated" date actually mean?</title>
    <updated>2025-09-02T17:20:32+00:00</updated>
    <author>
      <name>/u/XdtTransform</name>
      <uri>https://old.reddit.com/user/XdtTransform</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking through the models, I noticed that Gemma3 was &lt;a href="https://imgur.com/tLaswfx"&gt;updated&lt;/a&gt; 2 weeks ago. &lt;/p&gt; &lt;p&gt;I am pretty sure Gemma came out about 4-5 months ago. So what exactly was &amp;quot;updated&amp;quot;?&lt;/p&gt; &lt;p&gt;I downloaded one of the model variants - same one that I normally use and the files appear to be identical. &lt;/p&gt; &lt;p&gt;So what is this update referring to?&lt;/p&gt; &lt;p&gt;P.S. The &lt;a href="https://ollama.com/library/gemma3"&gt;readme&lt;/a&gt; on the model page doesn't provide any information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XdtTransform"&gt; /u/XdtTransform &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6ply3/what_does_the_updated_date_actually_mean/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6ply3/what_does_the_updated_date_actually_mean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6ply3/what_does_the_updated_date_actually_mean/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T17:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6icod</id>
    <title>Running LLM Locally with Ollama + RAG</title>
    <updated>2025-09-02T12:39:32+00:00</updated>
    <author>
      <name>/u/Tough_Wrangler_6075</name>
      <uri>https://old.reddit.com/user/Tough_Wrangler_6075</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n6icod/running_llm_locally_with_ollama_rag/"&gt; &lt;img alt="Running LLM Locally with Ollama + RAG" src="https://external-preview.redd.it/BPsfK6tF48FEZfYsejUp1jtQVo-8HzMuWSqGwZUflzY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b27330825317d976070fbec281feea47b604b58a" title="Running LLM Locally with Ollama + RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tough_Wrangler_6075"&gt; /u/Tough_Wrangler_6075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@zackydzacky/running-llm-locally-with-ollama-rag-cb68ff31e838"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6icod/running_llm_locally_with_ollama_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6icod/running_llm_locally_with_ollama_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T12:39:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7mdy4</id>
    <title>Best current local NSFW TTS model?</title>
    <updated>2025-09-03T18:12:42+00:00</updated>
    <author>
      <name>/u/Stock-Fault5734</name>
      <uri>https://old.reddit.com/user/Stock-Fault5734</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stock-Fault5734"&gt; /u/Stock-Fault5734 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7mdy4/best_current_local_nsfw_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7mdy4/best_current_local_nsfw_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7mdy4/best_current_local_nsfw_tts_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T18:12:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7iizk</id>
    <title>How to use a Hugging Face embedding model in Ollama</title>
    <updated>2025-09-03T15:51:43+00:00</updated>
    <author>
      <name>/u/StringIntelligent763</name>
      <uri>https://old.reddit.com/user/StringIntelligent763</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StringIntelligent763"&gt; /u/StringIntelligent763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7iizk/how_to_use_a_hugging_face_embedding_model_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7iizk/how_to_use_a_hugging_face_embedding_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7iizk/how_to_use_a_hugging_face_embedding_model_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T15:51:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7t0bu</id>
    <title>Microsoft with their sketchy data collection techniques as always</title>
    <updated>2025-09-03T22:27:14+00:00</updated>
    <author>
      <name>/u/Formal_Jeweler_488</name>
      <uri>https://old.reddit.com/user/Formal_Jeweler_488</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n7t0bu/microsoft_with_their_sketchy_data_collection/"&gt; &lt;img alt="Microsoft with their sketchy data collection techniques as always" src="https://external-preview.redd.it/eHQ1aGFraHl6MG5mMSKEsllT2BaBkbUqwvk0riQfqTI-3zznlfwJiR2mpLoX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ded832e0e00248c95a04eb99e4391e0262e136a6" title="Microsoft with their sketchy data collection techniques as always" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guy please pause and check my first chat where he reponds the exact same thing i called it out and, it started gaslighting me into thinking i left the memory on.&lt;/p&gt; &lt;p&gt;Things I discussed with Co Pilot (Mentions after deleting)&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Kohya_ss (To train my face with Loras)&lt;/li&gt; &lt;li&gt;JuggernautXLv9 (Have recommended people on reddit previously)&lt;/li&gt; &lt;li&gt;Continue.dev for BYOK in VS code (you can read the first chat in video he mentions it then as well)&lt;/li&gt; &lt;li&gt;Mafia 3 (Was trying to find best cars and get some help in missions, too lazy to visit youtube.com) &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Irony is I am using Swift Keyboard, Gonna change&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Formal_Jeweler_488"&gt; /u/Formal_Jeweler_488 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x5rqqqcyz0nf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7t0bu/microsoft_with_their_sketchy_data_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7t0bu/microsoft_with_their_sketchy_data_collection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T22:27:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7auub</id>
    <title>Unsloth gpt-oss gguf in Ollama</title>
    <updated>2025-09-03T10:11:45+00:00</updated>
    <author>
      <name>/u/Tema_Art_7777</name>
      <uri>https://old.reddit.com/user/Tema_Art_7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama pull certainly works as advertized however when I download the huggingface unsloth gpt-oss-20b or 120b models, I get gibberish output (I am guessing due to template required?). Has anyone gotten it to work with ollama create -f Modelfile? Many thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tema_Art_7777"&gt; /u/Tema_Art_7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7auub/unsloth_gptoss_gguf_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7auub/unsloth_gptoss_gguf_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7auub/unsloth_gptoss_gguf_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T10:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n71bil</id>
    <title>Any actual downside to 4 x 3090 ($2400 total) vs RTX pro 6000 ($9000) other than power?</title>
    <updated>2025-09-03T01:09:30+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n71bil/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n71bil/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T01:09:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7vnwy</id>
    <title>Hate AI frameworks? I may have something for you...</title>
    <updated>2025-09-04T00:22:55+00:00</updated>
    <author>
      <name>/u/BeautifulQuote6295</name>
      <uri>https://old.reddit.com/user/BeautifulQuote6295</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you're building with AI you may have found yourself grappling with one of the mainstream frameworks. Since I never really liked no having granular control over what's happening, last year I built a lib called `grafo` for easily AI workflows. It's rules are simple:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nodes contain coroutines to be run&lt;/li&gt; &lt;li&gt;A node only starts executing once all it's parent's have finished running&lt;/li&gt; &lt;li&gt;State is not passed around automatically, but you can do it manually&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These rules come together to make building AI-driven workflows generally easy. However, building around AI has more than DAGs: we need prompt building and mode calling - in comes `grafo ai tools`.&lt;/p&gt; &lt;p&gt;`Grafo AI Tools` is basically a wrapper lib where I've added some very simple prompt managing &amp;amp; model calling, coupled with `grafo`. It's built around the big guys, like `jinja2` and `instructor`.&lt;/p&gt; &lt;p&gt;My goal here is not to create a framework or any set of abstractions that take away from our control of the program as developers - I just wanted to bundle a toolkit which I found useful. In any case, here's the URL: &lt;a href="https://github.com/paulomtts/Grafo-AI-Tools"&gt;https://github.com/paulomtts/Grafo-AI-Tools&lt;/a&gt; . Let me know if you find this interesting at all. I'll be updating it going forward.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeautifulQuote6295"&gt; /u/BeautifulQuote6295 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7vnwy/hate_ai_frameworks_i_may_have_something_for_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7vnwy/hate_ai_frameworks_i_may_have_something_for_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7vnwy/hate_ai_frameworks_i_may_have_something_for_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T00:22:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7kz78</id>
    <title>Conseils IA pour 3 use cases (email, briefing, chatbot) sur serveur local modeste</title>
    <updated>2025-09-03T17:20:56+00:00</updated>
    <author>
      <name>/u/Maleficent-Hotel8207</name>
      <uri>https://old.reddit.com/user/Maleficent-Hotel8207</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Salut, Je cherche des id√©es d‚ÄôIA √† faire tourner en local sur ma config : ‚Ä¢ GTX 1050 low profile (2 Go VRAM) ‚Ä¢ i3-3400 ‚Ä¢ 16 Go de RAM&lt;/p&gt; &lt;p&gt;J‚Äôai 3 besoins : ‚Ä¢ IA pour g√©n√©rer des emails : environ 500 tokens en entr√©e, 30 tokens en sortie. R√©ponse en moins de 5 minutes. ‚Ä¢ IA pour faire un briefing du matin : environ 3000 tokens en entr√©e, 100 tokens en sortie. R√©sum√© clair et rapide. ‚Ä¢ Chatbot ultra-rapide : environ 20 tokens en entr√©e, 20 tokens en sortie. R√©ponse en moins de 5 secondes.&lt;/p&gt; &lt;p&gt;Je cherche des mod√®les l√©gers (quantifi√©s, optimis√©s, open-source si possible) pour que √ßa tourne sur cette config limit√©e. Si vous avez des id√©es de mod√®les, de frameworks ou de tips pour que √ßa passe, je suis preneur !&lt;/p&gt; &lt;p&gt;Merci d‚Äôavance !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Hotel8207"&gt; /u/Maleficent-Hotel8207 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T17:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7m86q</id>
    <title>Build a Visual Document Index from multiple formats all at once - PDFs, Images, Slides - with ColPali without OCR</title>
    <updated>2025-09-03T18:06:40+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would love to share my latest project that builds visual document index from multiple formats in the same flow for PDFs, images using Colpali without OCR. Incremental processing out-of-box and can connect to google drive, s3, azure blob store.&lt;/p&gt; &lt;p&gt;- Detailed write up: &lt;a href="https://cocoindex.io/blogs/multi-format-indexing"&gt;https://cocoindex.io/blogs/multi-format-indexing&lt;/a&gt;&lt;br /&gt; - Fully open sourced: &lt;a href="https://github.com/cocoindex-io/cocoindex/tree/main/examples/multi_format_indexing"&gt;https://github.com/cocoindex-io/cocoindex/tree/main/examples/multi_format_indexing&lt;/a&gt;&lt;br /&gt; (70 lines python on index path)&lt;/p&gt; &lt;p&gt;Looking forward to your suggestions&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T18:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1n739d3</id>
    <title>ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization</title>
    <updated>2025-09-03T02:40:16+00:00</updated>
    <author>
      <name>/u/cornucopea</name>
      <uri>https://old.reddit.com/user/cornucopea</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt; &lt;img alt="ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization" src="https://b.thumbs.redditmedia.com/xqOJbGWme_Lnii3nAdQLvJli58h2TtNMtqIsZum6xqs.jpg" title="ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;This refactors the main run loop of the ollama runner to perform the main GPU intensive tasks (Compute+Floats) in a go routine so we can prepare the next batch in parallel to reduce the amount of time the GPU stalls waiting for the next batch of work.&lt;/p&gt; &lt;p&gt;On metal, I see a 2-3% speedup in token rate. On a single RTX 4090 I see a ~7% speedup.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cs98ja944vmf1.jpg?width=650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=01fd1804e5580b7cc7e85287b110a5cece68865d"&gt;https://preview.redd.it/cs98ja944vmf1.jpg?width=650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=01fd1804e5580b7cc7e85287b110a5cece68865d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.phoronix.com/news/ollama-0.11.9-More-Performance"&gt;https://www.phoronix.com/news/ollama-0.11.9-More-Performance&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cornucopea"&gt; /u/cornucopea &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T02:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n831ad</id>
    <title>my ranking and I am not sure whether it is your ranking</title>
    <updated>2025-09-04T06:48:32+00:00</updated>
    <author>
      <name>/u/Zestyclose-Duty3239</name>
      <uri>https://old.reddit.com/user/Zestyclose-Duty3239</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;best level: claude (for programming)&lt;/p&gt; &lt;p&gt;good level: gpt o3 &amp;gt; grok (almost uncensored) &amp;gt; gpt 4o (before $200/month plan) &amp;gt; copliot&lt;/p&gt; &lt;p&gt;normal level: deepseek r1 (no multi-modal) &amp;gt; gpt 4o (after $200/month plan) &amp;gt; gemini (for colab) &amp;gt; gpt 5&lt;/p&gt; &lt;p&gt;shitty level: gpt o4-mini &amp;gt; gpt 4.5&lt;/p&gt; &lt;p&gt;I am using a mac pro 2019 with gv100. it is very difficult on running local ollama. I have to use online model.&lt;/p&gt; &lt;p&gt;I believe no company is actually earning money from their ai competition. $30-$300/month subscription is still much lower than the actual cost of the llm model and their gpu base. microsoft, google, meta, amazon, and openai are just wasting money for the market share. they will eventually let us use the weaker model in the next 2-3 years.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zestyclose-Duty3239"&gt; /u/Zestyclose-Duty3239 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n831ad/my_ranking_and_i_am_not_sure_whether_it_is_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n831ad/my_ranking_and_i_am_not_sure_whether_it_is_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n831ad/my_ranking_and_i_am_not_sure_whether_it_is_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T06:48:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7h8ox</id>
    <title>Anyone else frustrated with AI assistants forgetting context?</title>
    <updated>2025-09-03T15:03:43+00:00</updated>
    <author>
      <name>/u/PrestigiousBet9342</name>
      <uri>https://old.reddit.com/user/PrestigiousBet9342</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep bouncing between ChatGPT, Claude, and Perplexity depending on the task. The problem is every new session feels like starting over‚ÄîI have to re-explain everything.&lt;/p&gt; &lt;p&gt;Just yesterday I wasted 10+ minutes walking perplexity through my project direction again just to get related search if not it is just useless. This morning, ChatGPT didn‚Äôt remember anything about my client‚Äôs requirements.&lt;/p&gt; &lt;p&gt;The result? I lose a couple of hours each week just re-establishing context. It also makes it hard to keep project discussions consistent across tools. Switching platforms means resetting, and there‚Äôs no way to keep a running history of decisions or knowledge.&lt;/p&gt; &lt;p&gt;I‚Äôve tried copy-pasting old chats (messy and unreliable), keeping manual notes (which defeats the point of using AI), and sticking to just one tool (but each has its strengths).&lt;/p&gt; &lt;p&gt;Has anyone actually found a fix for this? I‚Äôm especially interested in something that works across different platforms, not just one. On my end, I‚Äôve started tinkering with a solution and would love to hear what features people would find most useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrestigiousBet9342"&gt; /u/PrestigiousBet9342 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T15:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n868ux</id>
    <title>Local Code Analyser</title>
    <updated>2025-09-04T10:15:26+00:00</updated>
    <author>
      <name>/u/r00tdr1v3</name>
      <uri>https://old.reddit.com/user/r00tdr1v3</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/r00tdr1v3"&gt; /u/r00tdr1v3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n868dj/local_code_analyser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n868ux/local_code_analyser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n868ux/local_code_analyser/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T10:15:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7ypzr</id>
    <title>MoE models benchmarked on AMD iGPU</title>
    <updated>2025-09-04T02:47:16+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n7ypio/moe_models_benchmarked_on_igpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7ypzr/moe_models_benchmarked_on_amd_igpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7ypzr/moe_models_benchmarked_on_amd_igpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T02:47:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n87ngf</id>
    <title>Can I use Ollama + OpenWebUI through Docker Engine (In Terminal) or only through Desktop version?</title>
    <updated>2025-09-04T11:33:50+00:00</updated>
    <author>
      <name>/u/PracticalAd6966</name>
      <uri>https://old.reddit.com/user/PracticalAd6966</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently on Linux PC and I really need to use Docker Engine and as I understand they have conflicting files so I can use only one of them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PracticalAd6966"&gt; /u/PracticalAd6966 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n87ngf/can_i_use_ollama_openwebui_through_docker_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n87ngf/can_i_use_ollama_openwebui_through_docker_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n87ngf/can_i_use_ollama_openwebui_through_docker_engine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T11:33:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7q8cx</id>
    <title>Ollama model most similar to GPT-4o?</title>
    <updated>2025-09-03T20:36:55+00:00</updated>
    <author>
      <name>/u/amstlicht</name>
      <uri>https://old.reddit.com/user/amstlicht</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been researching AI models and am looking for models similar to 4o in terms of personality, mostly. I remember 4o would often suggest interesting paths when I used it for research, it would remember the context and relate it to previous ideas. Does anyone have a recommendation of something similar for Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amstlicht"&gt; /u/amstlicht &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7q8cx/ollama_model_most_similar_to_gpt4o/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7q8cx/ollama_model_most_similar_to_gpt4o/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7q8cx/ollama_model_most_similar_to_gpt4o/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T20:36:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n89sa4</id>
    <title>Most affordable AI computer with GPU (‚ÄúGPUter‚Äù) you can build in 2025?</title>
    <updated>2025-09-04T13:13:36+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n89sa4/most_affordable_ai_computer_with_gpu_gputer_you/"&gt; &lt;img alt="Most affordable AI computer with GPU (‚ÄúGPUter‚Äù) you can build in 2025?" src="https://preview.redd.it/bk6tf5l2e5nf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8da7afc16f4d8ff260c98ad24de5cc8adc50a222" title="Most affordable AI computer with GPU (‚ÄúGPUter‚Äù) you can build in 2025?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bk6tf5l2e5nf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n89sa4/most_affordable_ai_computer_with_gpu_gputer_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n89sa4/most_affordable_ai_computer_with_gpu_gputer_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T13:13:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7uhkv</id>
    <title>Hows your experience running Ollama on Apple Sillicon M1, M2, M3 or M4</title>
    <updated>2025-09-03T23:29:58+00:00</updated>
    <author>
      <name>/u/Cultural-You-7096</name>
      <uri>https://old.reddit.com/user/Cultural-You-7096</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How's the experience, Does it run welll like web versions or is it slow. I'm concerned becuase I want to get a Macbook Pro just to run models .&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cultural-You-7096"&gt; /u/Cultural-You-7096 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T23:29:58+00:00</published>
  </entry>
</feed>
