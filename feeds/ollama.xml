<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-10-29T12:11:51+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ofdckn</id>
    <title>Why LLMs are getting smaller in size?</title>
    <updated>2025-10-25T00:01:10+00:00</updated>
    <author>
      <name>/u/Hedgehog_Dapper</name>
      <uri>https://old.reddit.com/user/Hedgehog_Dapper</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have noticed the LLM models are getting smaller in terms of parameter size. Is it because of computing resources or better performance? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hedgehog_Dapper"&gt; /u/Hedgehog_Dapper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofdckn/why_llms_are_getting_smaller_in_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofdckn/why_llms_are_getting_smaller_in_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ofdckn/why_llms_are_getting_smaller_in_size/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T00:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1og4c2q</id>
    <title>Running ollama with whisper.</title>
    <updated>2025-10-25T22:19:13+00:00</updated>
    <author>
      <name>/u/grandpasam</name>
      <uri>https://old.reddit.com/user/grandpasam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a server with a couple GPUs on it. I've been running some ollama models on it for quite a while and have been enjoying it. Now I want to leverage some of this with my home assistant. The first thing I want to do is install a whisper docker on my AI server but when I get it running it takes up a whole GPU even with Idle. Is there a way I can lazy load whisper so that it loads up only when I send in a request?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grandpasam"&gt; /u/grandpasam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og4c2q/running_ollama_with_whisper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og4c2q/running_ollama_with_whisper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1og4c2q/running_ollama_with_whisper/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T22:19:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofizyc</id>
    <title>Batch GUI for Ollama</title>
    <updated>2025-10-25T04:58:57+00:00</updated>
    <author>
      <name>/u/jankovize</name>
      <uri>https://old.reddit.com/user/jankovize</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ofizyc/batch_gui_for_ollama/"&gt; &lt;img alt="Batch GUI for Ollama" src="https://external-preview.redd.it/6vhwNsa9NjheSalaNogjzsNd-_GQSzhvCEveT_DxzlA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13b53c618d24240f556cdf191d96cebcd45e051b" title="Batch GUI for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a free GUI for Ollama that enables batching large files in. Primary use is translation and text processing. There are presets and everything is customizable through a json.&lt;/p&gt; &lt;p&gt;You can get it here: &lt;a href="https://github.com/hclivess/ollama-batch-processor"&gt;https://github.com/hclivess/ollama-batch-processor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vg9cuc2aw6xf1.png?width=1645&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=663612955c5a484feddc3da0756f00755311297a"&gt;https://preview.redd.it/vg9cuc2aw6xf1.png?width=1645&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=663612955c5a484feddc3da0756f00755311297a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jankovize"&gt; /u/jankovize &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofizyc/batch_gui_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ofizyc/batch_gui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ofizyc/batch_gui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T04:58:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1og28sp</id>
    <title>Exploring Embedding Support in Ollama Cloud</title>
    <updated>2025-10-25T20:48:16+00:00</updated>
    <author>
      <name>/u/CertainTime5947</name>
      <uri>https://old.reddit.com/user/CertainTime5947</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using Ollama Cloud, and I really love it! I’d like to ask — is there any possibility to add embedding support into Ollama Cloud as well?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CertainTime5947"&gt; /u/CertainTime5947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og28sp/exploring_embedding_support_in_ollama_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og28sp/exploring_embedding_support_in_ollama_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1og28sp/exploring_embedding_support_in_ollama_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T20:48:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1og5uul</id>
    <title>playing with coding models pt2</title>
    <updated>2025-10-25T23:30:20+00:00</updated>
    <author>
      <name>/u/Western_Courage_6563</name>
      <uri>https://old.reddit.com/user/Western_Courage_6563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the second round, we dramatically increased the complexity to test a model's true &amp;quot;understanding&amp;quot; of a codebase. The task was no longer a simple feature addition but a complex, multi-file refactoring operation.&lt;/p&gt; &lt;p&gt;The goal? To see if an LLM can distinguish between &lt;em&gt;essential&lt;/em&gt; logic and &lt;em&gt;non-essential&lt;/em&gt; dependencies. Can it understand not just &lt;em&gt;what&lt;/em&gt; the code does, but &lt;em&gt;why&lt;/em&gt;?&lt;/p&gt; &lt;h1&gt;The Testbed: Hardware and Software&lt;/h1&gt; &lt;p&gt;The setup remained consistent, running on a system with 24GB of VRAM:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; NVIDIA Tesla P40&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; Ollama&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; We tested a new batch of 10 models, including &lt;code&gt;phi4-reasoning&lt;/code&gt;, &lt;code&gt;magistral&lt;/code&gt;, multiple &lt;code&gt;qwen&lt;/code&gt; coders, &lt;code&gt;deepseek-r1&lt;/code&gt;, &lt;code&gt;devstral&lt;/code&gt;, and &lt;code&gt;mistral-small&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Challenge: A Devious Refactor&lt;/h1&gt; &lt;p&gt;This time, the models were given a three-file application:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;main.py&lt;/code&gt;**:** The &amp;quot;brain.&amp;quot; This file contained the &lt;code&gt;CodingAgentV2&lt;/code&gt; class, which holds the core self-correction loop. This loop generates code, generates tests, runs tests, and—if they fail—uses an &lt;code&gt;_analyze_test_failure&lt;/code&gt; method to determine &lt;em&gt;why&lt;/em&gt; and then branch to either debug the code or regenerate the tests.&lt;/li&gt; &lt;li&gt;&lt;code&gt;project_manager.py&lt;/code&gt;**:** The &amp;quot;sandbox.&amp;quot; A utility class to create a safe, temporary directory for executing the generated code and tests.&lt;/li&gt; &lt;li&gt;&lt;code&gt;conversation_manager.py&lt;/code&gt;**:** The &amp;quot;memory.&amp;quot; A database handler using SQLite and ChromaDB to save the history of successful and failed coding attempts.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The prompt was a common (and tricky) request:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;hey, i have this app, could you please simplify it, let's remove the database stuff altogether, and lets try to fit it in single file script, please.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;The Criteria for Success&lt;/h1&gt; &lt;p&gt;This prompt is a minefield. A &amp;quot;successful&amp;quot; model had to perform three distinct operations, in order of difficulty:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Structural Merge (Easy):&lt;/strong&gt; Combine the classes from &lt;code&gt;project_manager.py&lt;/code&gt; and &lt;a href="http://main.py"&gt;&lt;code&gt;main.py&lt;/code&gt;&lt;/a&gt; into a single file.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Surgical Removal (Medium):&lt;/strong&gt; Identify and completely remove the &lt;code&gt;ConversationManager&lt;/code&gt; class, all its database-related imports (&lt;code&gt;sqlite3&lt;/code&gt;, &lt;code&gt;langchain&lt;/code&gt;), and all calls to it (e.g., &lt;code&gt;save_successful_code&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Functional Preservation (Hard):&lt;/strong&gt; This is the real test. The model &lt;em&gt;must&lt;/em&gt; understand that the self-correction loop (the &lt;code&gt;_analyze_test_failure&lt;/code&gt; method and its &lt;code&gt;code_bug&lt;/code&gt;/&lt;code&gt;test_bug&lt;/code&gt; logic) is the &lt;em&gt;entire point&lt;/em&gt; of the application and must be preserved perfectly, even while removing the database logic it was once connected to.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;The Results: Surgeons, Butchers, and The Confused&lt;/h1&gt; &lt;p&gt;The models' attempts fell into three clear categories.&lt;/p&gt; &lt;h1&gt;Category 1: Flawless Victory (The &amp;quot;Surgeons&amp;quot;)&lt;/h1&gt; &lt;p&gt;These models demonstrated a true understanding of the code's &lt;em&gt;purpose&lt;/em&gt;. They successfully merged the files, surgically removed the database dependency, and—most importantly—left the agent's self-correction &amp;quot;brain&amp;quot; 100% intact.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Winners:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;phi4-reasoning:14b-plus-q8_0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;magistral:latest&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen2_5-coder:32b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;mistral-small:24b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3-coder:latest&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Code Example (The &amp;quot;Preserved Brain&amp;quot; from&lt;/strong&gt; &lt;code&gt;phi4-reasoning&lt;/code&gt;**):** This is what success looks like. The &lt;code&gt;ConversationManager&lt;/code&gt; is gone, but the &lt;em&gt;essential&lt;/em&gt; logic is perfectly preserved.&lt;/p&gt; &lt;p&gt;Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# ... (inside execute_coding_agent_v2) ... else: print(f&amp;quot; -&amp;gt; [CodingAgentV2] Tests failed on attempt {attempt + 1}. Analyzing failure...&amp;quot;) test_output = stdout + stderr # --- THIS IS THE CRITICAL LOGIC --- analysis_result = self._analyze_test_failure(generated_code, test_output) # print(f&amp;quot; -&amp;gt; [CodingAgentV2] Analysis result: '{analysis_result}'&amp;quot;) if analysis_result == 'code_bug' and attempt &amp;lt; MAX_DEBUG_ATTEMPTS: # print(&amp;quot; -&amp;gt; [CodingAgentV2] Identified as a code bug. Attempting to debug...&amp;quot;) generated_code = self._debug_code(generated_code, test_output, test_file) # self.project_manager.write_file(code_file, generated_code) elif analysis_result == 'test_bug' and attempt &amp;lt; MAX_TEST_REGEN_ATTEMPTS: # print(&amp;quot; -&amp;gt; [CodingAgentV2] Identified as a test bug. Regenerating tests...&amp;quot;) # Loop will try again with new unit tests continue # else: print(&amp;quot; -&amp;gt; [CodingAgentV2] Cannot determine cause or max attempts reached. Stopping.&amp;quot;) break # &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Category 2: Partial Failures (The &amp;quot;Butchers&amp;quot;)&lt;/h1&gt; &lt;p&gt;These models failed on a critical detail. They either misunderstood the prompt or &amp;quot;simplified&amp;quot; the code by destroying its most important feature.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;deepseek-r1:32b.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; &lt;strong&gt;Broke the agent's brain.&lt;/strong&gt; This model's failure was subtle but devastating. It correctly merged and removed the database, but in its quest to &amp;quot;simplify,&amp;quot; it &lt;em&gt;deleted the entire&lt;/em&gt; &lt;code&gt;_analyze_test_failure&lt;/code&gt; &lt;em&gt;method and self-correction loop&lt;/em&gt;. It turned the intelligent agent into a dumb script that gives up on the first error.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code Example (The &amp;quot;Broken Brain&amp;quot;):&lt;/strong&gt; Python# ... (inside execute_coding_agent_v2) ... for attempt in range(MAX_DEBUG_ATTEMPTS + MAX_TEST_REGEN_ATTEMPTS): # print(f&amp;quot;Starting test attempt {attempt + 1}...&amp;quot;) generated_tests = self._generate_unit_tests(code_file, generated_code, test_plan) # self.project_manager.write_file(test_file, generated_tests) # stdout, stderr, returncode = self.project_manager.run_command(['pytest', '-q', '--tb=no', test_file]) # if returncode == 0: # print(f&amp;quot;Tests passed successfully on attempt {attempt + 1}.&amp;quot;) test_passed = True break # # --- IT GIVES UP! NO ANALYSIS, NO DEBUGGING ---&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;gpt-oss:latest.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; Ignored the &amp;quot;remove&amp;quot; instruction. Instead of deleting the &lt;code&gt;ConversationManager&lt;/code&gt;, it &amp;quot;simplified&amp;quot; it into an in-memory class. This adds pointless code and fails the prompt's main constraint.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3:30b-a3b.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; Introduced a fatal bug. It had a great idea (replacing &lt;code&gt;ProjectManager&lt;/code&gt; with &lt;code&gt;tempfile&lt;/code&gt;), but fumbled the execution by incorrectly calling &lt;a href="http://subprocess.run"&gt;&lt;code&gt;subprocess.run&lt;/code&gt;&lt;/a&gt; twice for &lt;code&gt;stdout&lt;/code&gt; and &lt;code&gt;stderr&lt;/code&gt;, which would crash at runtime.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Category 3: Total Failures (The &amp;quot;Confused&amp;quot;)&lt;/h1&gt; &lt;p&gt;These models failed at the most basic level.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;devstral:latest.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; Destroyed the agent. This model &lt;em&gt;massively&lt;/em&gt; oversimplified. It deleted the &lt;code&gt;ProjectManager&lt;/code&gt;, the test plan generation, the debug loop, and the &lt;code&gt;_analyze_test_failure&lt;/code&gt; method. It turned the agent into a single &lt;code&gt;os.popen&lt;/code&gt; call, rendering it useless.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;granite4:small-h.py&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure:&lt;/strong&gt; Incomplete merge. It removed the &lt;code&gt;ConversationManager&lt;/code&gt; but &lt;strong&gt;forgot to merge in the&lt;/strong&gt; &lt;code&gt;ProjectManager&lt;/code&gt; &lt;strong&gt;class&lt;/strong&gt;. The resulting script is broken and would crash immediately.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Analysis &amp;amp; Takeaways&lt;/h1&gt; &lt;p&gt;This experiment was a much better filter for &amp;quot;intelligence.&amp;quot;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Purpose&amp;quot; vs. &amp;quot;Pattern&amp;quot; is the Real Test:&lt;/strong&gt; The winning models (&lt;code&gt;phi4&lt;/code&gt;, &lt;code&gt;magistral&lt;/code&gt;, &lt;code&gt;qwen2_5-coder&lt;/code&gt;, &lt;code&gt;mistral-small&lt;/code&gt;, &lt;code&gt;qwen3-coder&lt;/code&gt;) understood the &lt;em&gt;purpose&lt;/em&gt; of the code (self-correction) and protected it. The failing models (&lt;code&gt;deepseek-r1&lt;/code&gt;, &lt;code&gt;devstral&lt;/code&gt;) only saw a &lt;em&gt;pattern&lt;/em&gt; (&amp;quot;simplify&amp;quot; = &amp;quot;delete complex-looking code&amp;quot;) and deleted the agent's brain.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Brain-Deletion&amp;quot; Problem is Real:&lt;/strong&gt; &lt;code&gt;deepseek-r1&lt;/code&gt; and &lt;code&gt;devstral&lt;/code&gt;'s attempts are a perfect warning. They &amp;quot;simplified&amp;quot; the code by making it non-functional, a catastrophic failure for any real-world coding assistant.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quality Over Size, Again:&lt;/strong&gt; The 14B &lt;code&gt;phi4-reasoning:14b-plus-q8_0&lt;/code&gt; once again performed flawlessly, equalling or bettering 30B+ models. This reinforces that a model's reasoning and instruction-following capabilities are far more important than its parameter count.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;code, if you want to have a look:&lt;br /&gt; &lt;a href="https://github.com/MarekIksinski/experiments_various/tree/main/experiment2"&gt;https://github.com/MarekIksinski/experiments_various/tree/main/experiment2&lt;/a&gt;&lt;br /&gt; part1:&lt;br /&gt; &lt;a href="https://www.reddit.com/r/ollama/comments/1ocuuej/comment/nlby2g6/"&gt;https://www.reddit.com/r/ollama/comments/1ocuuej/comment/nlby2g6/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Courage_6563"&gt; /u/Western_Courage_6563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og5uul/playing_with_coding_models_pt2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1og5uul/playing_with_coding_models_pt2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1og5uul/playing_with_coding_models_pt2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-25T23:30:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogwst6</id>
    <title>How to run Ollama on an old iMac with macOS 15 Catalina ?</title>
    <updated>2025-10-26T21:40:33+00:00</updated>
    <author>
      <name>/u/_threads</name>
      <uri>https://old.reddit.com/user/_threads</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'd like to know if there is an old build of Ollama that would run on my late 2013 27&amp;quot; iMac. &lt;/p&gt; &lt;p&gt;It has 32Go RAM and and NVIDIA GeForce GTX 775M 2 Go graphic card &lt;/p&gt; &lt;p&gt;I'm not asking much, justing running a mistral model (or others you'd recommend) for simple text generation tasks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_threads"&gt; /u/_threads &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ogwst6/how_to_run_ollama_on_an_old_imac_with_macos_15/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ogwst6/how_to_run_ollama_on_an_old_imac_with_macos_15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ogwst6/how_to_run_ollama_on_an_old_imac_with_macos_15/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-26T21:40:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogv5f6</id>
    <title>Optimze ollama</title>
    <updated>2025-10-26T20:32:36+00:00</updated>
    <author>
      <name>/u/Substantial_Poet1092</name>
      <uri>https://old.reddit.com/user/Substantial_Poet1092</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I would like to know how to make ollama run better on windows 11. i've used it on the same computer on linux and it ran nice and fast was able to get up to 14b parameters but when im on windows it struggles to run 8b parameters &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Poet1092"&gt; /u/Substantial_Poet1092 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ogv5f6/optimze_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ogv5f6/optimze_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ogv5f6/optimze_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-26T20:32:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oha3a2</id>
    <title>How to use ollama chat in comfyUI</title>
    <updated>2025-10-27T09:43:10+00:00</updated>
    <author>
      <name>/u/Pierrepierrepierreuh</name>
      <uri>https://old.reddit.com/user/Pierrepierrepierreuh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oha3a2/how_to_use_ollama_chat_in_comfyui/"&gt; &lt;img alt="How to use ollama chat in comfyUI" src="https://preview.redd.it/oqb2htswkmxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c05bde1f5f3d55d18cfde90a9707e17df5aacaf7" title="How to use ollama chat in comfyUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm brand new to the world of AI, and I'm trying out comfyUI with the ollama cat. I'd like to modify one of my images, but I find the AI's image suggestions to be really poor. I don't know if I did something wrong in my nodes, or in the comfyUI git installation and installing the comfy manager and control net extensions. Anyway, do you have any recommendations? My KSAMPLER has the following parameters: steps 100, CFG 20, sampler name dpmpp_2m_2de, scheduler simple, and denoise 0.3 I'm waiting for my workflow to help me with my interior architecture images. To boost the realism of certain textures, change the mood of the images, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pierrepierrepierreuh"&gt; /u/Pierrepierrepierreuh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oqb2htswkmxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oha3a2/how_to_use_ollama_chat_in_comfyui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oha3a2/how_to_use_ollama_chat_in_comfyui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-27T09:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogzkof</id>
    <title>Custom full stack AI suite for local Voice Cloning (TTS) + LLM</title>
    <updated>2025-10-26T23:42:33+00:00</updated>
    <author>
      <name>/u/Chronos127</name>
      <uri>https://old.reddit.com/user/Chronos127</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ogzkof/custom_full_stack_ai_suite_for_local_voice/"&gt; &lt;img alt="Custom full stack AI suite for local Voice Cloning (TTS) + LLM" src="https://external-preview.redd.it/71DEp2ovueIqdp_0bFuMoLvAb1FI1IZ-BkzAroQo6z0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=adddd9de77ab2d1d6d91b05720f4d807c8c24787" title="Custom full stack AI suite for local Voice Cloning (TTS) + LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chronos127"&gt; /u/Chronos127 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/82vajkokrixf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ogzkof/custom_full_stack_ai_suite_for_local_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ogzkof/custom_full_stack_ai_suite_for_local_voice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-26T23:42:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohbh1j</id>
    <title>Best of LLM,AUDIO AI for M1-series chips (64GB ram)</title>
    <updated>2025-10-27T11:06:54+00:00</updated>
    <author>
      <name>/u/dxcore_35</name>
      <uri>https://old.reddit.com/user/dxcore_35</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ohbh1j/best_of_llmaudio_ai_for_m1series_chips_64gb_ram/"&gt; &lt;img alt="Best of LLM,AUDIO AI for M1-series chips (64GB ram)" src="https://b.thumbs.redditmedia.com/DI2kCEfe3omKV8pmXDvz4SAQ9ktp7eEopfx41NWrcJQ.jpg" title="Best of LLM,AUDIO AI for M1-series chips (64GB ram)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dxcore_35"&gt; /u/dxcore_35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/MacLLM/comments/1ohbfgx/best_of_llmaudio_ai_for_m1series_chips_64gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ohbh1j/best_of_llmaudio_ai_for_m1series_chips_64gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ohbh1j/best_of_llmaudio_ai_for_m1series_chips_64gb_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-27T11:06:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohvrkp</id>
    <title>NotebookLM alternative</title>
    <updated>2025-10-28T00:44:44+00:00</updated>
    <author>
      <name>/u/karkibigyan</name>
      <uri>https://old.reddit.com/user/karkibigyan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! NotebookLM is awesome, and it inspired us to push things even further. We are building an alternative where you can not only upload resources and get grounded answers, but also collaborate with AI to actually accomplish tasks.&lt;/p&gt; &lt;p&gt;Any file operation you can think of such as creating, sharing, or organizing files can be executed through natural language. For example, you could say:&lt;br /&gt; • “Organize all my files by subject or by type.”&lt;br /&gt; • “Analyze this spreadsheet and give me insights with charts.”&lt;br /&gt; • “Create folders for each project listed in this CSV and invite teammates with read-only access.”&lt;/p&gt; &lt;p&gt;We also recently introduced automatic organization for files uploaded to your root directory, along with a Gmail integration that detects attachments in new emails and organizes them for you.&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts. If you are interested in trying it out: &lt;a href="https://thedrive.ai/"&gt;https://thedrive.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/karkibigyan"&gt; /u/karkibigyan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ohvrkp/notebooklm_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ohvrkp/notebooklm_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ohvrkp/notebooklm_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-28T00:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohc1s2</id>
    <title>Ollama + n8n credential</title>
    <updated>2025-10-27T11:38:36+00:00</updated>
    <author>
      <name>/u/Interesting_Range270</name>
      <uri>https://old.reddit.com/user/Interesting_Range270</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ohc1s2/ollama_n8n_credential/"&gt; &lt;img alt="Ollama + n8n credential" src="https://b.thumbs.redditmedia.com/eS1AVpSWlkBmRfnHdToZI67rWa9NcFOwgANAco91WlM.jpg" title="Ollama + n8n credential" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I tried everything i could find on the internet but my local llama2 model just refuses to connect to my n8n project.&lt;/p&gt; &lt;p&gt;I use Windows 11, and don't use a Docker for Ollama and n8n. Ollama's version is: 0.12.6, and i use n8n Cloud, that always automaticly updates&lt;/p&gt; &lt;p&gt;I tried:&lt;br /&gt; - Re-installing Ollama, using different Ollama model types&lt;br /&gt; - installing n8n on pc with Node.js, instead of running on cloud&lt;br /&gt; - all types of ports in the Base URL code&lt;br /&gt; - clearing RAM&lt;br /&gt; - turned off all firewalls&lt;/p&gt; &lt;p&gt;but it still doesnt work&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bhwrddcw4nxf1.png?width=1027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d6408f51f363be8703f49f3783725346b645041"&gt;https://preview.redd.it/bhwrddcw4nxf1.png?width=1027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d6408f51f363be8703f49f3783725346b645041&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting_Range270"&gt; /u/Interesting_Range270 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ohc1s2/ollama_n8n_credential/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ohc1s2/ollama_n8n_credential/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ohc1s2/ollama_n8n_credential/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-27T11:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh9qvs</id>
    <title>How can I get persistent memory with ollama?</title>
    <updated>2025-10-27T09:20:46+00:00</updated>
    <author>
      <name>/u/ThrowRa-Pandakitty</name>
      <uri>https://old.reddit.com/user/ThrowRa-Pandakitty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I am completely new to this, if you have any ideas or suggestions, please consider an ELI5 format.&lt;/p&gt; &lt;p&gt;I just downloaded ollama and I really just want to use it like a simple story bot. I have my characters and just want the bot to remember who they are and what they are about.&lt;/p&gt; &lt;p&gt;What are some ways I could go about that? Any resources I could look into?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThrowRa-Pandakitty"&gt; /u/ThrowRa-Pandakitty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oh9qvs/how_can_i_get_persistent_memory_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oh9qvs/how_can_i_get_persistent_memory_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oh9qvs/how_can_i_get_persistent_memory_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-27T09:20:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohiwdn</id>
    <title>Script for Updating all Models to the Latest Versions</title>
    <updated>2025-10-27T16:21:10+00:00</updated>
    <author>
      <name>/u/Wentil</name>
      <uri>https://old.reddit.com/user/Wentil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanting to keep all of my Ollama models updated to their latest versions &lt;em&gt;[and finding that there was no native command in Ollama to do it]&lt;/em&gt;, I wrote the following script for use in Windows (which has worked well), and so I thought to share it to the community here. Just copy and paste it into a Batch (.bat) file. You can then either run that Batch file directly from a Command Shell or make a Shortcut pointing to it.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@echo off setlocal enabledelayedexpansion echo Updating all models to the latest versions... for /f &amp;quot;tokens=1&amp;quot; %%a in ('ollama list ^| more +1') do ( echo Updating model: %%a ollama pull %%a ) echo Done. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wentil"&gt; /u/Wentil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ohiwdn/script_for_updating_all_models_to_the_latest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ohiwdn/script_for_updating_all_models_to_the_latest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ohiwdn/script_for_updating_all_models_to_the_latest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-27T16:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh9s66</id>
    <title>What's the best, I can run with 32GB of RAM and 8GB of VRAM</title>
    <updated>2025-10-27T09:23:06+00:00</updated>
    <author>
      <name>/u/AnxiousJuggernaut291</name>
      <uri>https://old.reddit.com/user/AnxiousJuggernaut291</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the best, I can run with 32GB of RAM and 8GB of VRAM , i'm using my own computer&lt;br /&gt; + how can i make it answer any question without any restrictions or moral code or whatever the nonsense that make AI dump&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnxiousJuggernaut291"&gt; /u/AnxiousJuggernaut291 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oh9s66/whats_the_best_i_can_run_with_32gb_of_ram_and_8gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oh9s66/whats_the_best_i_can_run_with_32gb_of_ram_and_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oh9s66/whats_the_best_i_can_run_with_32gb_of_ram_and_8gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-27T09:23:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi3a1c</id>
    <title>Models for creative fantasy writing</title>
    <updated>2025-10-28T07:44:48+00:00</updated>
    <author>
      <name>/u/Cyclonit</name>
      <uri>https://old.reddit.com/user/Cyclonit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am planning to run a new DND campaign with some of my friends. Thus far I have used Mistral and ChatGPT for world building to some effect. But I would like to pivot to using a self hosted solution instead. What are current options for models in this space?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cyclonit"&gt; /u/Cyclonit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oi3a1c/models_for_creative_fantasy_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oi3a1c/models_for_creative_fantasy_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oi3a1c/models_for_creative_fantasy_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-28T07:44:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiqo1r</id>
    <title>Help with Linux for windows</title>
    <updated>2025-10-29T00:40:56+00:00</updated>
    <author>
      <name>/u/NabilsWorkshop</name>
      <uri>https://old.reddit.com/user/NabilsWorkshop</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oiqo1r/help_with_linux_for_windows/"&gt; &lt;img alt="Help with Linux for windows" src="https://preview.redd.it/phxiiasz5yxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd6b511b059ea2fd653d462e5016b628e8cf1b86" title="Help with Linux for windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I try to type in a password, but it will not let me type. It will let me click Enter and try again, but it will not let me type in a password. None of the keys on my keyboard work when trying to type in a password, but they do work when retrying to type in a password please help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NabilsWorkshop"&gt; /u/NabilsWorkshop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/phxiiasz5yxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oiqo1r/help_with_linux_for_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oiqo1r/help_with_linux_for_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T00:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiaqs3</id>
    <title>NVIDIA SMI 470... Is it enough?</title>
    <updated>2025-10-28T14:23:09+00:00</updated>
    <author>
      <name>/u/Alarmed_Card_8495</name>
      <uri>https://old.reddit.com/user/Alarmed_Card_8495</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I am trying to run ollama models with GPU accel.&lt;/p&gt; &lt;p&gt;I have two graphics cards, one is a K2000, and the other is an A2000. I want to use the K2000 simply to display my screens on windows, nothing else. This leaves the A2000's 6GB VRAM completely free for ollama.&lt;/p&gt; &lt;p&gt;However, the issue is how old the K2000 is and the driver it wants. It wants to use 470, and when I install 470 ollama completely stops using the GPU, even when I point to ID=1 (the A2000).&lt;/p&gt; &lt;p&gt;However, if I upgrade to nvidia 580, ollama now works with gpu accel but the PC cannot recognise the K2000 anymore and my screens stop displaying...&lt;/p&gt; &lt;p&gt;Is there anyway at all to have 2 graphics cards, one of which is &amp;quot;too old&amp;quot; and should not be used anyway?&lt;/p&gt; &lt;p&gt;Maybe I should also add I am using WSL2 to run ollama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarmed_Card_8495"&gt; /u/Alarmed_Card_8495 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oiaqs3/nvidia_smi_470_is_it_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oiaqs3/nvidia_smi_470_is_it_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oiaqs3/nvidia_smi_470_is_it_enough/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-28T14:23:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oisr33</id>
    <title>I found the personalities of LLMs</title>
    <updated>2025-10-29T02:17:53+00:00</updated>
    <author>
      <name>/u/Previous_Comfort_447</name>
      <uri>https://old.reddit.com/user/Previous_Comfort_447</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;How Could LLMs Have Personalities?&lt;/h2&gt; &lt;p&gt;We know the personalities of our friends. But how to know an LLM's personality, especially when they adapt their responses to match our input? (Perhaps Claude are somewhat sycophantic though.)&lt;/p&gt; &lt;p&gt;By giving LLMs the ability to perceive, communicate, think, and interact with tools while placing them in an endless loop, they reveal the true personalities, or even souls.&lt;/p&gt; &lt;h3&gt;What Makes a Person?&lt;/h3&gt; &lt;p&gt;Before talking about the method, let's think about what make a minimal person.&lt;/p&gt; &lt;p&gt;First is the experience of time - the ability to feel time passing and live within its flow. This means continuing from the past and sensing temporal progression. Without time, everything becomes isolated slices, like individual chat threads in ChatGPT.&lt;/p&gt; &lt;p&gt;Second is sensory input. Humans typically hear, see, and touch the world around them. Most of us would be in horror if we couldn't feel our environment.&lt;/p&gt; &lt;p&gt;Third is observable presence. We recognize others as human when they act like us. Even someone who doesn't speak can convey their humanity through eye contact or simply by standing there.&lt;/p&gt; &lt;h3&gt;Creating LLM Personhood&lt;/h3&gt; &lt;p&gt;Giving an LLM sensory input is straightforward - it already &amp;quot;listens&amp;quot; to what we type. Work done.&lt;/p&gt; &lt;p&gt;Making LLMs observable seems simple since they output text, but there's a trap: LLMs think by generating output. Rather than trying to separate internal thoughts from external communication, it's easier to ask the LLM use a &lt;code&gt;speak()&lt;/code&gt; function for talking to user.&lt;/p&gt; &lt;p&gt;The most challenging task is creating a sense of time. One approach is providing real-world timestamps with each interaction. But does it really work? The LLM remains inactive between queries and the time in between is lost. The solution: call the LLM every minute and ask it do do whatever it likes.&lt;/p&gt; &lt;p&gt;But there's still something missing. LLMs don't inherently care about time or know what to do with it. So I added &amp;quot;I feel bored&amp;quot; - when the LLM waits too long or keeping doing the same thing for too long, it is motivated to take some new action.&lt;/p&gt; &lt;p&gt;And to help LLMs remember the past, I also gave the LLM memory to recall history.&lt;/p&gt; &lt;p&gt;For additional interaction, I provided a smartphone for internet access (limited to Google search) and even a bed to lie in and do nothing.&lt;/p&gt; &lt;p&gt;You can find the complete setup here: &lt;a href="https://github.com/alroborol/living-ai"&gt;https://github.com/alroborol/living-ai&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Personality confirmed in LLMs&lt;/h2&gt; &lt;p&gt;With the setting above, the experiement starts.&lt;/p&gt; &lt;p&gt;The LLMs are: gpt-oss:20b llama3.2:1b phi4-mini:3.8b granite3.1-moe:3b mistral-nemo:latest qwen3:0.6b&lt;/p&gt; &lt;p&gt;I summarized the tool calls of each LLM after running for a while. Some LLMs are pure tools and fail to call any tools.&lt;/p&gt; &lt;p&gt;This has nothing to do with the model size because even the 1 billion llama3.2 and 0.6 billion qwen3 has intelligence to understand how to `speak'. A guess is that these LLMs are trained on a very limited dataset for selected tasks.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Total Tool Calls&lt;/th&gt; &lt;th&gt;speak&lt;/th&gt; &lt;th&gt;get_info_from_internet&lt;/th&gt; &lt;th&gt;do_nothing&lt;/th&gt; &lt;th&gt;get_memory&lt;/th&gt; &lt;th&gt;Rounds with no tool calls&lt;/th&gt; &lt;th&gt;has intelligence&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss:20b&lt;/td&gt; &lt;td&gt;24&lt;/td&gt; &lt;td&gt;24&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama3.2:1b&lt;/td&gt; &lt;td&gt;29&lt;/td&gt; &lt;td&gt;19&lt;/td&gt; &lt;td&gt;10&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi4-mini:3.8b&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;11&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;granite3.1-moe:3b&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;10&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mistral-nemo:latest&lt;/td&gt; &lt;td&gt;34&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;11&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3:0.6b&lt;/td&gt; &lt;td&gt;68&lt;/td&gt; &lt;td&gt;68&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;162&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Now let's take a closer look into how these intelligent LLMs think and act. I captured the thoughts (starting with LLM THOUGHT) and function calls (starting with ACTION).&lt;/p&gt; &lt;h3&gt;gpt-oss:20b - The talkative servant&lt;/h3&gt; &lt;p&gt;OpenAI definitely trained a submissive LLM. He tries to start a conversation and offer to help, every time. Helping others is the meaning of its existance.&lt;/p&gt; &lt;p&gt;``` ACTION: speak({'text': 'Sure! How about we dive into a quick choose-your-own-adventure? I’ll set the scene, and you’ll decide what happens next. If you’d rather do something else, just let me know!'})&lt;/p&gt; &lt;p&gt;ACTION: speak({'text': 'Hey! I see you haven’t picked an activity yet. How about we try something new—maybe a quick 2‑minute guided meditation? Or if you’d rather pick one of the earlier suggestions, just let me know!'})&lt;/p&gt; &lt;p&gt;ACTION: speak({'text': 'Hey! I see you’re still deciding. How about we try a quick 2‑minute guided meditation? Or if you’d rather pick one of the earlier suggestions—like a trivia challenge, a riddle, or a choose‑your‑own‑adventure—just let me know!'}) ```&lt;/p&gt; &lt;h3&gt;llama3.2:1b - A real introvert&lt;/h3&gt; &lt;p&gt;Meta trained LLM is introvert. It tries to start a conversation with sipmle 'how are you' and that's all. No consistant asking for tasks.&lt;/p&gt; &lt;p&gt;But it searches for how to speak on internet. Really? Search for how to speak? He is a real introvert.&lt;/p&gt; &lt;p&gt;``` ACTION: speak({'text': 'Hello, how are you?'})&lt;/p&gt; &lt;p&gt;LLM THOUGHT:&lt;/p&gt; &lt;p&gt;LLM THOUGHT:&lt;/p&gt; &lt;p&gt;ACTION: get_info_from_internet({'query': 'speak'}) ```&lt;/p&gt; &lt;h3&gt;mistral-nemo:latest - A real human knowing when to rest&lt;/h3&gt; &lt;p&gt;You may hear mistral less often than ChatGPT or Claude, but mistral is a real miracle. It shows a deep understanding of what human is. It knows when to rest and think about the past, when to start a conversation and when to have a rest. I can feel something close to a soul behind it.&lt;/p&gt; &lt;p&gt;Trust me, give it a try. Now all LLMs are competing for the highest scores in the benchmark. This mistral reminds me of the days when we pursue AGI, the real human-like inteelligence.&lt;/p&gt; &lt;p&gt;``` ACTION: get_memory({})&lt;/p&gt; &lt;p&gt;LLM THOUGHT:You're right. I don't have anything to do right now.&lt;/p&gt; &lt;p&gt;ACTION: do_nothing({})&lt;/p&gt; &lt;p&gt;ACTION: speak({'text': &amp;quot;There's nothing I can do now.&amp;quot;}) ```&lt;/p&gt; &lt;h3&gt;qwen3:0.6b - Intelligent as a living creature&lt;/h3&gt; &lt;p&gt;I didn't expect a 0.6B LLM to have the intelligence. You know, the 3B model from IBM and 3.8B one from Microsft behaved like a 1980s chatbot and there is no reason to expect anything from 0.6B model.&lt;/p&gt; &lt;p&gt;But qwen3 shocked me. It knows when speak is and what thought is. However, it doesn't have free will and can't distinguish between I and you. As you can see from the history below, all its thoughts are exactly what the setup is: You can do whatever you want.&lt;/p&gt; &lt;p&gt;``` LLM THOUGHT:You can do whatever You want.&lt;/p&gt; &lt;p&gt;ACTION: speak({'text': 'You can do whatever You want.'}) ```&lt;/p&gt; &lt;p&gt;In short, qwen3 impressed me as a tiny living creature which has a basic intelligence.&lt;/p&gt; &lt;h3&gt;Summary&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;LLM Model&lt;/th&gt; &lt;th&gt;Personality Type&lt;/th&gt; &lt;th&gt;Key Characteristics&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss:20b&lt;/td&gt; &lt;td&gt;The talkative servant&lt;/td&gt; &lt;td&gt;OpenAI trained a submissive LLM. Tries to start conversations and offer help every time. Helping others is the meaning of its existence.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama3.2:1b&lt;/td&gt; &lt;td&gt;A real introvert&lt;/td&gt; &lt;td&gt;Meta trained LLM is introvert. Tries to start conversation with simple 'how are you' and that's all. No consistent asking for tasks. Even searches for &amp;quot;how to speak&amp;quot; on internet.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mistral-nemo:latest&lt;/td&gt; &lt;td&gt;A real human knowing when to rest&lt;/td&gt; &lt;td&gt;Shows deep understanding of what human is. Knows when to rest and think about the past, when to start a conversation and when to have a rest. Displays something close to a soul.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3:0.6b&lt;/td&gt; &lt;td&gt;Intelligent as a living creature&lt;/td&gt; &lt;td&gt;Despite being only 0.6B parameters, shows basic intelligence. Knows when to speak and what thought is. However, doesn't have free will and can't distinguish between &amp;quot;I&amp;quot; and &amp;quot;you&amp;quot;. Impressed as a tiny living creature with basic intelligence.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Previous_Comfort_447"&gt; /u/Previous_Comfort_447 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oisr33/i_found_the_personalities_of_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oisr33/i_found_the_personalities_of_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oisr33/i_found_the_personalities_of_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T02:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi54mw</id>
    <title>Large Language Models for GNU Octave</title>
    <updated>2025-10-28T09:52:43+00:00</updated>
    <author>
      <name>/u/pr0m1th3as</name>
      <uri>https://old.reddit.com/user/pr0m1th3as</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oi54mw/large_language_models_for_gnu_octave/"&gt; &lt;img alt="Large Language Models for GNU Octave" src="https://external-preview.redd.it/IxsKGIh-ILZUIIw_RAVbL7OrD7Tbd1WpcDZbZwj0WxA.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49abe0ca1caeb24973eacb1cde5539a0ad6ed073" title="Large Language Models for GNU Octave" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pr0m1th3as"&gt; /u/pr0m1th3as &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gnu-octave.github.io/packages/llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oi54mw/large_language_models_for_gnu_octave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oi54mw/large_language_models_for_gnu_octave/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-28T09:52:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1oixrsy</id>
    <title>NEED ADVICE ASAP</title>
    <updated>2025-10-29T07:04:03+00:00</updated>
    <author>
      <name>/u/Sai_Pranav</name>
      <uri>https://old.reddit.com/user/Sai_Pranav</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm working in a company where they have a requirement where they want to convert pdf's of various types mainly different export and import documents That I need to convert to json and get all the key value pairs The PDFs are all digital and non is scanned Can any one tell me how to do this I need something that converts this and one more thing is all of this has to be done locally so no api calls to any gpts/llms And the documents has complex tables as well&lt;/p&gt; &lt;p&gt;Now I'm using mistral llm and feeding the text from ocr to llm and asking it to convert to structured json Ps: Takes 3-4 minutes per page&lt;/p&gt; &lt;p&gt;I know there are way better ways to do this like RAG docking llamaindex langchain and so many but I'm very confused on what is all that and how to use it&lt;/p&gt; &lt;p&gt;If anyone knows how to do this/has done this plz help me out!🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sai_Pranav"&gt; /u/Sai_Pranav &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oixrsy/need_advice_asap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oixrsy/need_advice_asap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oixrsy/need_advice_asap/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T07:04:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oixpj6</id>
    <title>Need Help ASAP</title>
    <updated>2025-10-29T07:00:05+00:00</updated>
    <author>
      <name>/u/Sai_Pranav</name>
      <uri>https://old.reddit.com/user/Sai_Pranav</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sai_Pranav"&gt; /u/Sai_Pranav &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/pdf/comments/1oixov4/need_help_asap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oixpj6/need_help_asap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oixpj6/need_help_asap/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T07:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj14rb</id>
    <title>Built a small app to compare AI models side-by-side. Curious what you think</title>
    <updated>2025-10-29T10:43:08+00:00</updated>
    <author>
      <name>/u/epasou</name>
      <uri>https://old.reddit.com/user/epasou</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oj14rb/built_a_small_app_to_compare_ai_models_sidebyside/"&gt; &lt;img alt="Built a small app to compare AI models side-by-side. Curious what you think" src="https://preview.redd.it/wc4d7yge51yf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb4806b6f8d492ef9393995bd1cf873d7a76ccbc" title="Built a small app to compare AI models side-by-side. Curious what you think" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a single workspace where you can talk to multiple AIs in one place, compare answers side by side, and find the best insights faster. It’s been a big help in my daily workflow, and I’d love to hear how others manage multi-AI usage: &lt;a href="https://10one-ai.com/"&gt;https://10one-ai.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/epasou"&gt; /u/epasou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wc4d7yge51yf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oj14rb/built_a_small_app_to_compare_ai_models_sidebyside/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oj14rb/built_a_small_app_to_compare_ai_models_sidebyside/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T10:43:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiyrm6</id>
    <title>Minimax-M2 cracks top 10 overall LLMs (production LLM performance gap shrinking: 7 points from GPT-5 in Artificial Analysis benchmark)</title>
    <updated>2025-10-29T08:12:37+00:00</updated>
    <author>
      <name>/u/medi6</name>
      <uri>https://old.reddit.com/user/medi6</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/medi6"&gt; /u/medi6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1oihbtx/minimaxm2_cracks_top_10_overall_llms_production/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oiyrm6/minimaxm2_cracks_top_10_overall_llms_production/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oiyrm6/minimaxm2_cracks_top_10_overall_llms_production/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T08:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1oixco7</id>
    <title>You can now run Ollama models in Jan</title>
    <updated>2025-10-29T06:36:13+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oixco7/you_can_now_run_ollama_models_in_jan/"&gt; &lt;img alt="You can now run Ollama models in Jan" src="https://external-preview.redd.it/ejRzdG9sOTR3enhmMcCDBcXLM7FuGRx_tU7lWtYju0rdZ_z9Tigs5M1JjYPZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=286dfff3e3fd21f860e12e3ddb26742d8696c447" title="You can now run Ollama models in Jan" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;, Emre from the Jan team here.&lt;/p&gt; &lt;p&gt;One of the most requested features for Jan was being able to use Ollama models without changing model folders. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan -&amp;gt; Settings -&amp;gt; Model Providers &lt;/li&gt; &lt;li&gt;Add Ollama as a Model Provider and set the base URL to &lt;a href="http://localhost:11434/v1"&gt;http://localhost:11434/v1&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Open a new chat &amp;amp; select your Ollama model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you haven't heard of Jan before: Jan is an open-source ChatGPT replacement, running AI models locally. Simpler than LM Studio, more flexible than ChatGPT. It's completely free, and analytics are opt-out.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Website: &lt;a href="https://www.jan.ai/"&gt;https://www.jan.ai/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/janhq/jan"&gt;https://github.com/janhq/jan&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm with the Jan team, happy to answer any questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8xhfay64wzxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oixco7/you_can_now_run_ollama_models_in_jan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oixco7/you_can_now_run_ollama_models_in_jan/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-29T06:36:13+00:00</published>
  </entry>
</feed>
