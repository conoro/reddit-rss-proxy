<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-10-21T18:25:58+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1o9byfp</id>
    <title>Ollama on Linux with swap enabled.</title>
    <updated>2025-10-17T20:16:22+00:00</updated>
    <author>
      <name>/u/GhostInThePudding</name>
      <uri>https://old.reddit.com/user/GhostInThePudding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just in case anyone else is having trouble with their device hard locking when frequently switching models in Ollama on Linux with an Nvidia GPU.&lt;/p&gt; &lt;p&gt;After giving up on trying to solve it and accepting it was either an obscure driver issue on my device, or maybe even a hardware fault, I happened to use Ollama after disabling my swap space, and suddenly it worked perfectly.&lt;/p&gt; &lt;p&gt;It seems there is some issue with memory management when swap is enabled, that if you switch models a lot, it can not only crash Ollama, but the entire system, forcing a hard reboot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhostInThePudding"&gt; /u/GhostInThePudding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o9byfp/ollama_on_linux_with_swap_enabled/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o9byfp/ollama_on_linux_with_swap_enabled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o9byfp/ollama_on_linux_with_swap_enabled/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-17T20:16:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9w6zv</id>
    <title>Opencode + Ollama Doesn't Work With Local LLMs on Windows 11</title>
    <updated>2025-10-18T13:43:44+00:00</updated>
    <author>
      <name>/u/SlideRuleFan</name>
      <uri>https://old.reddit.com/user/SlideRuleFan</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlideRuleFan"&gt; /u/SlideRuleFan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/opencodeCLI/comments/1o9emcc/opencode_ollama_doesnt_work_with_local_llms_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o9w6zv/opencode_ollama_doesnt_work_with_local_llms_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o9w6zv/opencode_ollama_doesnt_work_with_local_llms_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-18T13:43:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9l4vg</id>
    <title>Is Ollama slower on Windows, compared with Linux, when starting a model? (cold start from disk, the model files are not in the cache yet)</title>
    <updated>2025-10-18T03:16:26+00:00</updated>
    <author>
      <name>/u/florinandrei</name>
      <uri>https://old.reddit.com/user/florinandrei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same machine, dual boot, Windows 11 and Ubuntu 24.04&lt;/p&gt; &lt;p&gt;The system is reasonably fast, I can play recent games, fine-tune LLMs, write and run PyTorch code, etc. Each OS is on its own SSD drive, but the drives are nearly identical.&lt;/p&gt; &lt;p&gt;Starting a model from a cold start is fairly quick on Linux.&lt;/p&gt; &lt;p&gt;On Windows, I have to wait something like 30 seconds until gemma3:27b is loaded and I can start prompting it. The wait might be a bit even longer if I use Open WebUI as an interface to Ollama.&lt;/p&gt; &lt;p&gt;After stopping the model, and running it again, now the model files are cached, and the start process is as fast as on Linux.&lt;/p&gt; &lt;p&gt;Has anybody else seen this issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/florinandrei"&gt; /u/florinandrei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o9l4vg/is_ollama_slower_on_windows_compared_with_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o9l4vg/is_ollama_slower_on_windows_compared_with_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o9l4vg/is_ollama_slower_on_windows_compared_with_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-18T03:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oadgp4</id>
    <title>Hi, I hope this is not a dumb question, I have hard time getting thinking models (open ai open model, qwen) to send back a JSON and only a json. It keeps sending back the thinking tokens which messes up the parsing. I tried many suggestions from ChatGPT or claude to no avail. Thank you!</title>
    <updated>2025-10-19T01:44:28+00:00</updated>
    <author>
      <name>/u/Defiant_Watch9818</name>
      <uri>https://old.reddit.com/user/Defiant_Watch9818</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Defiant_Watch9818"&gt; /u/Defiant_Watch9818 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oadgp4/hi_i_hope_this_is_not_a_dumb_question_i_have_hard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oadgp4/hi_i_hope_this_is_not_a_dumb_question_i_have_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oadgp4/hi_i_hope_this_is_not_a_dumb_question_i_have_hard/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T01:44:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa2psp</id>
    <title>Claude Haiku 4.5 for Computer Use</title>
    <updated>2025-10-18T18:05:14+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oa2psp/claude_haiku_45_for_computer_use/"&gt; &lt;img alt="Claude Haiku 4.5 for Computer Use" src="https://external-preview.redd.it/NGgxeHA0aTl1d3ZmMbN-OTJwTQPs45moaBiyBOFffvNmrIYfiiCYht-O6e2Z.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5532755cf7668522edea9a2c78e7434a196ae6d2" title="Claude Haiku 4.5 for Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude Haiku 4.5 on a computer-use task and it's faster + 3.5x cheaper than Sonnet 4.5:&lt;/p&gt; &lt;p&gt;Create a landing page of Cua and open it in browser&lt;/p&gt; &lt;p&gt;Haiku 4.5: 2 minutes, $0.04&lt;/p&gt; &lt;p&gt;Sonnet 4.5: 3 minutes, ~$0.14&lt;/p&gt; &lt;p&gt;Haiku shown here.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uaw73rq9uwvf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oa2psp/claude_haiku_45_for_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oa2psp/claude_haiku_45_for_computer_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-18T18:05:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oak98t</id>
    <title>Model for organizing photos</title>
    <updated>2025-10-19T08:18:50+00:00</updated>
    <author>
      <name>/u/gregusmeus</name>
      <uri>https://old.reddit.com/user/gregusmeus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I’m seeking a recommendation please, I’d like to use a local model to organize my folder of photos - is there a model I can download via ollama that folks would recommend for this task…with no risk of my photos ending up in the wild?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gregusmeus"&gt; /u/gregusmeus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oak98t/model_for_organizing_photos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oak98t/model_for_organizing_photos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oak98t/model_for_organizing_photos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T08:18:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1oadii0</id>
    <title>Ollama newbie seeking advice/tips</title>
    <updated>2025-10-19T01:47:07+00:00</updated>
    <author>
      <name>/u/CryptoNiight</name>
      <uri>https://old.reddit.com/user/CryptoNiight</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just ordered a mini pc for ollama. The specs are: Intel Core i5 with integrated graphics + 32 GB of memory. Do I absolutely need a dedicated graphics card to get started? Will it be too slow without one? Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CryptoNiight"&gt; /u/CryptoNiight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oadii0/ollama_newbie_seeking_advicetips/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oadii0/ollama_newbie_seeking_advicetips/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oadii0/ollama_newbie_seeking_advicetips/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T01:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1oani8z</id>
    <title>Download keeps resetting</title>
    <updated>2025-10-19T11:41:39+00:00</updated>
    <author>
      <name>/u/karrie0027</name>
      <uri>https://old.reddit.com/user/karrie0027</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to download other models in ollama I am in macbook m1 air Downloading gemma3:4b model and whenever my download reaches to like 90% it goes back to like 84%, currently stuck at 2.8gb/3.1gb , even though i have fast internet around 200mbps&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/karrie0027"&gt; /u/karrie0027 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oani8z/download_keeps_resetting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oani8z/download_keeps_resetting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oani8z/download_keeps_resetting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T11:41:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oarkmc</id>
    <title>Ollama Conversation History</title>
    <updated>2025-10-19T14:50:56+00:00</updated>
    <author>
      <name>/u/PalSCentered</name>
      <uri>https://old.reddit.com/user/PalSCentered</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oarkmc/ollama_conversation_history/"&gt; &lt;img alt="Ollama Conversation History" src="https://b.thumbs.redditmedia.com/8w6kAPmVIFxE_hEdgrXeDkszDvIMQhezd1H9mrncxhQ.jpg" title="Ollama Conversation History" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where does ollama app chat history get saved. I'm trying to find it and can't find the exact location. &lt;/p&gt; &lt;p&gt;I tried to look in the Ollama folder and originally thought it was the history file but no this is only for when using terminal so that begs the question where is this history when you use the app.&lt;/p&gt; &lt;p&gt;I mean this is supposed to be local right so it has to be somewhere in my computer.&lt;/p&gt; &lt;p&gt;If you have the answer to this I would love to know. Thanks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/74ergpxby2wf1.png?width=1676&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9f055581c791a5154900f96c6bd0c01bd02395e"&gt;https://preview.redd.it/74ergpxby2wf1.png?width=1676&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9f055581c791a5154900f96c6bd0c01bd02395e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PalSCentered"&gt; /u/PalSCentered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oarkmc/ollama_conversation_history/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oarkmc/ollama_conversation_history/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oarkmc/ollama_conversation_history/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T14:50:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oahq2c</id>
    <title>When you have little money but want to run big models</title>
    <updated>2025-10-19T05:39:04+00:00</updated>
    <author>
      <name>/u/alok_saurabh</name>
      <uri>https://old.reddit.com/user/alok_saurabh</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alok_saurabh"&gt; /u/alok_saurabh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oahpmx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oahq2c/when_you_have_little_money_but_want_to_run_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oahq2c/when_you_have_little_money_but_want_to_run_big/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T05:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oav0je</id>
    <title>Does Ollama provide models that can do aggregation &amp; prediction ?</title>
    <updated>2025-10-19T17:07:25+00:00</updated>
    <author>
      <name>/u/Loose_Cranberry8467</name>
      <uri>https://old.reddit.com/user/Loose_Cranberry8467</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; I’m new in my career and not sure if this counts as a small project or something bigger, so I’d really appreciate your advice and guidance.&lt;/p&gt; &lt;p&gt;I’m working with an Oracle Database in a large enterprise. My task is to build an &lt;strong&gt;AI system that can retrieve, analyze, aggregate, and predict&lt;/strong&gt; data — think of something like analyzing &lt;strong&gt;100K employees&lt;/strong&gt; with salary information, calculating averages, forecasting future costs, rates and similar analytics.&lt;/p&gt; &lt;p&gt;I was planning to use Ollama because it’s local and secure and maybe combine it with RAG. But from what I’ve read, Ollama models are mostly for text reasoning and not for performing real math.&lt;/p&gt; &lt;p&gt;Has anyone tried combining Ollama with an analytical engine to make it do actual aggregations or predictions? Would you recommend going the RAG + Ollama route, or should I use something?&lt;/p&gt; &lt;p&gt;Any insights, ideas, or examples would be awesome. Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loose_Cranberry8467"&gt; /u/Loose_Cranberry8467 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oav0je/does_ollama_provide_models_that_can_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oav0je/does_ollama_provide_models_that_can_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oav0je/does_ollama_provide_models_that_can_do/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T17:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob47qq</id>
    <title>Continue Plugin for Vscode Runs Insanely Slow with Deepseek</title>
    <updated>2025-10-19T23:16:36+00:00</updated>
    <author>
      <name>/u/djfrodo</name>
      <uri>https://old.reddit.com/user/djfrodo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In a terminal running deepseek-r1:latest, so 8b, code generation isn't insanely fast but it's pretty good.&lt;/p&gt; &lt;p&gt;Doing the same using the Continue plugin is unuseable.&lt;/p&gt; &lt;p&gt;Anyone have any idea what could be the cause?&lt;/p&gt; &lt;p&gt;edit: It also runs insanely slow when using the defalt models it comes with&lt;/p&gt; &lt;p&gt;tia&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djfrodo"&gt; /u/djfrodo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ob47qq/continue_plugin_for_vscode_runs_insanely_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ob47qq/continue_plugin_for_vscode_runs_insanely_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ob47qq/continue_plugin_for_vscode_runs_insanely_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T23:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob9gwu</id>
    <title>Hardware question about multiple GPUs</title>
    <updated>2025-10-20T03:29:52+00:00</updated>
    <author>
      <name>/u/SlimeQSlimeball</name>
      <uri>https://old.reddit.com/user/SlimeQSlimeball</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a HP z240 SFF that I have a GTX 1650 4 gb in right now. I have a P102-100 coming. Does it make sense to have the GTX still in place in the 16x slot and put the P102 in the bottom 4x slot?&lt;/p&gt; &lt;p&gt;I can leave it out and use the iGPU if it doesn't make sense to keep the 1650 installed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlimeQSlimeball"&gt; /u/SlimeQSlimeball &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ob9gwu/hardware_question_about_multiple_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ob9gwu/hardware_question_about_multiple_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ob9gwu/hardware_question_about_multiple_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-20T03:29:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob9ocb</id>
    <title>Qwen3-vl:235b-cloud Ollama model error</title>
    <updated>2025-10-20T03:40:20+00:00</updated>
    <author>
      <name>/u/Cute-Bicycle6159</name>
      <uri>https://old.reddit.com/user/Cute-Bicycle6159</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I faced an internal server error in running the Ollama model (Qwen3-vl:235b-cloud) : Error: 500 Internal Server Error: unmarshal: invalid character 'I' looking for beginning of value. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cute-Bicycle6159"&gt; /u/Cute-Bicycle6159 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ob9ocb/qwen3vl235bcloud_ollama_model_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ob9ocb/qwen3vl235bcloud_ollama_model_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ob9ocb/qwen3vl235bcloud_ollama_model_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-20T03:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1obbqnb</id>
    <title>What are the rate limits on both the free and pro tier of Ollama Cloud?</title>
    <updated>2025-10-20T05:31:18+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All I've been able to find in the documentation is that there are hourly and daily limits, and that Pro allows 20X+ more usage. But I can't find any specifics. Am I missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1obbqnb/what_are_the_rate_limits_on_both_the_free_and_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1obbqnb/what_are_the_rate_limits_on_both_the_free_and_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1obbqnb/what_are_the_rate_limits_on_both_the_free_and_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-20T05:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1oanxsn</id>
    <title>I built Graphite: A visual, non-linear LLM interface that turns your local chats into a map of ideas (Python/Ollama)</title>
    <updated>2025-10-19T12:05:16+00:00</updated>
    <author>
      <name>/u/Ok-Function-7101</name>
      <uri>https://old.reddit.com/user/Ok-Function-7101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oanxsn/i_built_graphite_a_visual_nonlinear_llm_interface/"&gt; &lt;img alt="I built Graphite: A visual, non-linear LLM interface that turns your local chats into a map of ideas (Python/Ollama)" src="https://b.thumbs.redditmedia.com/h0XcLRwXsEHG2NPsYuol7pUeT-xOwVJz9V91tvTZerg.jpg" title="I built Graphite: A visual, non-linear LLM interface that turns your local chats into a map of ideas (Python/Ollama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Check out the live view:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jupyfcdn62wf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=33c2f3dacb8ba6a9d7970ba8ec5e0e186931b2a2"&gt;easily convert text to graphic charts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v30rybdn62wf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a29976c4743918b51203b361fa5859d4fa540e8"&gt;multiple thread directions from a single point on the graph&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been working on a side project called &lt;strong&gt;Graphite&lt;/strong&gt; for nearly a year, because I found standard LLM chat interfaces too restrictive. When you're trying to brainstorm, research, or trace complex logic, the linear scroll format is a massive blocker—ideas get buried, and it’s impossible to track branches of thought.&lt;/p&gt; &lt;p&gt;Graphite solves this by treating every chat as a dynamic, visual graph on an infinite canvas.&lt;/p&gt; &lt;h1&gt;What it is&lt;/h1&gt; &lt;p&gt;Graphite is a desktop application built with Python (PyQt5) that integrates with your local LLMs via &lt;strong&gt;Ollama&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Non-Linear Conversations:&lt;/strong&gt; Every prompt and response is a movable, selectable node. If you want to revisit a question from 20 steps ago, you click that node, and your new query creates a branching path, allowing you to explore tangents without losing the original context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Workspace:&lt;/strong&gt; It's designed to be a workspace, not just a chat log. You can organize nodes into &lt;strong&gt;Frames&lt;/strong&gt;, add &lt;strong&gt;Notes&lt;/strong&gt; for external annotations, and drop &lt;strong&gt;Navigation Pins&lt;/strong&gt; to bookmark key moments.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Privacy:&lt;/strong&gt; Because it uses Ollama, all conversations and data processing stay local to your machine.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features I’m Excited About&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Chart Generation:&lt;/strong&gt; You can right-click any node containing structured data and ask the AI to generate a &lt;strong&gt;bar chart, pie chart, or even a Sankey diagram&lt;/strong&gt; directly on your canvas using Matplotlib.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Takeaways &amp;amp; Explainers:&lt;/strong&gt; The context menu lets you instantly generate key summaries or simplified &amp;quot;explain it like I'm five&amp;quot; notes from a complex AI response.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comprehensive Persistence:&lt;/strong&gt; It saves the entire workspace (nodes, connections, frames, notes, and pins) to a local SQLite database, managed via a &amp;quot;Chat Library&amp;quot; for session management.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'm currently using the qwen2.5:7b model, but it's designed to be model-agnostic as long as it runs on Ollama.&lt;/p&gt; &lt;p&gt;I'm looking for feedback from the community, especially around the usability of the non-linear graph metaphor and any potential features you'd find useful for this kind of visual AI interaction.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo Link:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fdovvnloading%2FGraphite"&gt;https://github.com/dovvnloading/Graphite&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for taking a look!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Function-7101"&gt; /u/Ok-Function-7101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oanxsn/i_built_graphite_a_visual_nonlinear_llm_interface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oanxsn/i_built_graphite_a_visual_nonlinear_llm_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oanxsn/i_built_graphite_a_visual_nonlinear_llm_interface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-19T12:05:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1obnq0w</id>
    <title>Building 100% local memory for AI agents</title>
    <updated>2025-10-20T17:03:41+00:00</updated>
    <author>
      <name>/u/hande__</name>
      <uri>https://old.reddit.com/user/hande__</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hande__"&gt; /u/hande__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://dev.to/chinmay_bhosale_9ceed796b/cognee-with-ollama-3pp8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1obnq0w/building_100_local_memory_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1obnq0w/building_100_local_memory_for_ai_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-20T17:03:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc1d11</id>
    <title>Dúvida - implementar ollama e problema com hardware + requisicoes de usuarios.</title>
    <updated>2025-10-21T02:37:09+00:00</updated>
    <author>
      <name>/u/CyberTrash_</name>
      <uri>https://old.reddit.com/user/CyberTrash_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Boa noite Galera! Estou prototipando um projeto que tenho em mente e estou me fazendo a seguinte questao: Pretendo integrar o ollama + algum modelo utilizando RAG para usar em um app que teria diversos usuarios acessando um chatbot, a duvida é, quanto mais usuarios acessando e mandando requisicoes via api pro meu modelo hospedado, mais processamento seria exigido expoencialmete do meu servidor? Gostaria tambem que alguem se pudesse me ajudar, me enviasse uma documentacao/tutorial legal pra entender melhor sobre os parametros nos modelos e calcular quanto e necessario de hardware pra rodar suposta llm local.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CyberTrash_"&gt; /u/CyberTrash_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oc1d11/dúvida_implementar_ollama_e_problema_com_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oc1d11/dúvida_implementar_ollama_e_problema_com_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oc1d11/dúvida_implementar_ollama_e_problema_com_hardware/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-21T02:37:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1obn0nj</id>
    <title>Local RAG tutorial - FastAPI &amp; Ollama &amp; pgvector</title>
    <updated>2025-10-20T16:29:23+00:00</updated>
    <author>
      <name>/u/Dev-it-with-me</name>
      <uri>https://old.reddit.com/user/Dev-it-with-me</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dev-it-with-me"&gt; /u/Dev-it-with-me &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Rag/comments/1oavnif/local_rag_tutorial_fastapi_ollama_pgvector/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1obn0nj/local_rag_tutorial_fastapi_ollama_pgvector/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1obn0nj/local_rag_tutorial_fastapi_ollama_pgvector/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-20T16:29:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocj8l6</id>
    <title>Why is my ollama so stupid?</title>
    <updated>2025-10-21T17:30:00+00:00</updated>
    <author>
      <name>/u/Future_Beyond_3196</name>
      <uri>https://old.reddit.com/user/Future_Beyond_3196</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ocj8l6/why_is_my_ollama_so_stupid/"&gt; &lt;img alt="Why is my ollama so stupid?" src="https://preview.redd.it/o24mk38q2iwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41aa0816dfce3e17cfb2f7c4e7e3cf636bc8bd96" title="Why is my ollama so stupid?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve had ollama for months and it can’t seem to get anything right for me. I asked the same question to another AI and it gets it spot on the first time. Ollama can’t figure anything I ask it about Music, Adam Sandler movies, OS troubleshooting steps, etc. Can anyone offer me some advice? TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Future_Beyond_3196"&gt; /u/Future_Beyond_3196 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o24mk38q2iwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ocj8l6/why_is_my_ollama_so_stupid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ocj8l6/why_is_my_ollama_so_stupid/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-21T17:30:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1obxfoc</id>
    <title>⚡ Gemma 3 1B Smart Q4 — Bilingual (IT/EN) Offline AI for Raspberry Pi 4/5</title>
    <updated>2025-10-20T23:34:45+00:00</updated>
    <author>
      <name>/u/Tnarb1</name>
      <uri>https://old.reddit.com/user/Tnarb1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lightweight bilingual Gemma 3 1B (IT/EN) optimized for Raspberry Pi — runs fully offline on Ollama.&lt;br /&gt; ~3.67 tokens/sec on Pi 4 with Q4_0 quantization (720 MB).&lt;br /&gt; No cloud, no tracking, just pure local inference.&lt;/p&gt; &lt;p&gt;🤗 Hugging Face: &lt;a href="https://huggingface.co/chill123/antonio-gemma3-smart-q4"&gt;https://huggingface.co/chill123/antonio-gemma3-smart-q4&lt;/a&gt;&lt;br /&gt; 🦙 Ollama: &lt;a href="https://ollama.com/antconsales/antonio-gemma3-smart-q4"&gt;https://ollama.com/antconsales/antonio-gemma3-smart-q4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tnarb1"&gt; /u/Tnarb1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1obxfoc/gemma_3_1b_smart_q4_bilingual_iten_offline_ai_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1obxfoc/gemma_3_1b_smart_q4_bilingual_iten_offline_ai_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1obxfoc/gemma_3_1b_smart_q4_bilingual_iten_offline_ai_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-20T23:34:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1obh5ex</id>
    <title>💰💰 Building Powerful AI on a Budget 💰💰</title>
    <updated>2025-10-20T12:01:06+00:00</updated>
    <author>
      <name>/u/FieldMouseInTheHouse</name>
      <uri>https://old.reddit.com/user/FieldMouseInTheHouse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1obh5ex/building_powerful_ai_on_a_budget/"&gt; &lt;img alt="💰💰 Building Powerful AI on a Budget 💰💰" src="https://preview.redd.it/94422xpxa9wf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f21509e2b33d144951b26ad5f4e866bce8afaef" title="💰💰 Building Powerful AI on a Budget 💰💰" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🤗 Hello, everbody!&lt;/p&gt; &lt;p&gt;I wanted to share my experience building a high-performance AI system without breaking the bank.&lt;/p&gt; &lt;p&gt;I've noticed a lot of people on here spending tons of money on top-of-the-line hardware, but I've found a way to achieve amazing results with a much more budget-friendly setup.&lt;/p&gt; &lt;p&gt;My system is built using the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A used Intel i5-6500 (3.2GHz, 4-core, 4-threads) machine that I got for cheap that came with 8GB of RAM (2 x 4GB) all installed into an ASUS H170-PRO motherboard. It also came with a RAIDER POWER SUPPLY RA650 650W power supply.&lt;/li&gt; &lt;li&gt;I installed Ubuntu Linux 22.04.5 LTS (Desktop) onto it.&lt;/li&gt; &lt;li&gt;Ollama running in Docker.&lt;/li&gt; &lt;li&gt;I purchased a new 32GB of RAM kit (2 x 16GB) for the system, bringing the total system RAM up to 40GB.&lt;/li&gt; &lt;li&gt;I then purchased two used NVDIA RTX 3060 12GB VRAM GPUs.&lt;/li&gt; &lt;li&gt;I then purchased a used Toshiba 1TB 3.5-inch SATA HDD.&lt;/li&gt; &lt;li&gt;I had a spare Samsung 1TB NVMe SSD drive lying around that I installed into this system.&lt;/li&gt; &lt;li&gt;I had two spare 500GB 2.5-inch SATA HDDs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;👨‍🔬 With the right optimizations, this setup absolutely flies! I'm getting 50-65 tokens per second, which is more than enough for my RAG and chatbot projects.&lt;/p&gt; &lt;p&gt;Here's how I did it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Quantization&lt;/strong&gt;: I run my Ollama server with Q4 quantization and use Q4 models. This makes a huge difference in VRAM usage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;num_ctx (Context Size)&lt;/strong&gt;: Forget what you've heard about context size needing to be a power of two! I experimented and found a sweet spot that perfectly matches my needs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;num_batch&lt;/strong&gt;: This was a game-changer! By tuning this parameter, I was able to drastically reduce memory usage without sacrificing performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Underclocking the GPUs&lt;/strong&gt;: Yes! You read right. To do this, I took the max wattage that that cards can run at, 170W, and reduced it to 85% of that max, being 145W. This is the sweet spot where the card's performance reasonably performs nearly the same as it would at 170W, but it totally avoids thermal throttling that would occur during heavy sustained activity! This means that I always get consistent performance results -- not spikey good results followed by some ridiculously slow results due to thermal throttling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My RAG and chatbots now run inside of just 6.7GB of VRAM, down from 10.5GB! &lt;em&gt;That is almost the equivalent of adding the equivalent of a third 6GB VRAM GPU into the mix for free!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;💻 Also, because I'm using Ollama, this single machine has become the Ollama server for every computer on my network -- and none of those other computers have a GPU worth anything!&lt;/p&gt; &lt;p&gt;Also, since I have two GPUs in this machine I have the following plan:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use the first GPU for all Ollama inference related work for the entire network. With careful planning so far, everything is fitting inside of the 6.7GB of VRAM leaving 5.3GB for any new models that can fit without causing an ejection/reload.&lt;/li&gt; &lt;li&gt;Next, I'm planning on using the second GPU to run PyTorch for distillation processing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm really happy with the results.&lt;/p&gt; &lt;p&gt;So, for a cost of about $700 US for this server, my entire network of now 5 machines got a collective AI/GPU upgrade.&lt;/p&gt; &lt;p&gt;❓ I'm curious if anyone else has experimented with similar optimizations.&lt;/p&gt; &lt;p&gt;What are your budget-friendly tips for optimizing AI performance???&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FieldMouseInTheHouse"&gt; /u/FieldMouseInTheHouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94422xpxa9wf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1obh5ex/building_powerful_ai_on_a_budget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1obh5ex/building_powerful_ai_on_a_budget/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-20T12:01:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc38pm</id>
    <title>What happens when two AI models start chatting with each other?</title>
    <updated>2025-10-21T04:11:54+00:00</updated>
    <author>
      <name>/u/Adventurous-Wind1029</name>
      <uri>https://old.reddit.com/user/Adventurous-Wind1029</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got curious… so I built it.&lt;/p&gt; &lt;p&gt;This project lets you run two AI models that talk to each other in real time. They question, explain, and sometimes spiral into the weirdest loops imaginable.&lt;/p&gt; &lt;p&gt;You can try it yourself here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/iKemo-io/chatting-agent?utm_source=ikemo.io"&gt;Github Repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It’s open-source — clone it, run it, and watch the AIs figure each other out.&lt;/p&gt; &lt;p&gt;Curious to see what directions people take this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Wind1029"&gt; /u/Adventurous-Wind1029 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oc38pm/what_happens_when_two_ai_models_start_chatting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oc38pm/what_happens_when_two_ai_models_start_chatting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oc38pm/what_happens_when_two_ai_models_start_chatting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-21T04:11:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oca3dq</id>
    <title>Qwen model running on Mac via Ollama was super slow with long wait times</title>
    <updated>2025-10-21T11:12:13+00:00</updated>
    <author>
      <name>/u/hellorahulkum</name>
      <uri>https://old.reddit.com/user/hellorahulkum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday, I was trying to use the latest Qwen model , and I ran into an issue. It wasn't generating responses, even after a minute or two. I had to set the timeout to over 300 seconds, and even then with `stream=True` , I couldn't get any performance boost, which caused my AI agents to fail. I can't remember what the main issue was.&lt;/p&gt; &lt;p&gt;Few things i tried:&lt;/p&gt; &lt;p&gt;&lt;em&gt;1. env&lt;/em&gt; changes:&lt;br /&gt; export OLLAMA_MAX_LOADED_MODELS=1&lt;br /&gt; export OLLAMA_NUM_CTX=2048 &lt;em&gt;# Default: 4096&lt;/em&gt;&lt;br /&gt; export OLLAMA_NUM_PARALLEL=1&lt;br /&gt; export OLLAMA_MAX_QUEUE=5&lt;/p&gt; &lt;h1&gt;2. Local Mac Optimization&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Use smaller models (qwen2:1.5b instead of 7b+)&lt;/li&gt; &lt;li&gt;Limit output tokens (&lt;code&gt;num_predict: 100&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Reduce context window (&lt;code&gt;num_ctx: 2048&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt; 2-3x speed improvement, still slow on Intel Mac&lt;/p&gt; &lt;h1&gt;3. Free GPU Cloud&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tried Google Colab&lt;/strong&gt;: Free T4 GPU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tried Kaggle&lt;/strong&gt;: Free 2x T4 GPUs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any better recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hellorahulkum"&gt; /u/hellorahulkum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oca3dq/qwen_model_running_on_mac_via_ollama_was_super/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oca3dq/qwen_model_running_on_mac_via_ollama_was_super/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oca3dq/qwen_model_running_on_mac_via_ollama_was_super/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-21T11:12:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1och76o</id>
    <title>New release (0.1.1) for the llms package</title>
    <updated>2025-10-21T16:12:55+00:00</updated>
    <author>
      <name>/u/pr0m1th3as</name>
      <uri>https://old.reddit.com/user/pr0m1th3as</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1och76o/new_release_011_for_the_llms_package/"&gt; &lt;img alt="New release (0.1.1) for the llms package" src="https://external-preview.redd.it/JCrbhu1UaTUrYQrfohpUwcFGomwEKJpWYtv2AArNeUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df110ca850a2bc27ac68db93e412d9ef81a8598e" title="New release (0.1.1) for the llms package" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pr0m1th3as"&gt; /u/pr0m1th3as &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pr0m1th3as/octave-llms/releases/tag/release-0.1.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1och76o/new_release_011_for_the_llms_package/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1och76o/new_release_011_for_the_llms_package/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-21T16:12:55+00:00</published>
  </entry>
</feed>
