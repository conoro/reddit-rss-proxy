<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-07T23:30:20+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qwwkip</id>
    <title>Built a self-hosted execution control layer for local LLM workflows (works with Ollama)</title>
    <updated>2026-02-05T20:28:01+00:00</updated>
    <author>
      <name>/u/saurabhjain1592</name>
      <uri>https://old.reddit.com/user/saurabhjain1592</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks. I am building AxonFlow, a self-hosted, source-available execution control layer for local LLM workflows once they move beyond single prompts and touch real systems.&lt;/p&gt; &lt;p&gt;The hard part was not model quality. It was making execution visible and controllable:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;clear boundaries around what steps are allowed to run&lt;/li&gt; &lt;li&gt;logs tied to decisions and actions, not just model outputs&lt;/li&gt; &lt;li&gt;the ability to inspect and replay a run when something goes wrong&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Retries and partial failures still mattered, but only after we could see and control what happened in a run.&lt;/p&gt; &lt;p&gt;AxonFlow sits inline between your workflow logic and LLM tool calls to make execution explicit. It is not an agent framework or UI platform. It is the runtime layer teams end up building underneath once local workflows get serious.&lt;/p&gt; &lt;p&gt;Works with Ollama by pointing the client to a local endpoint.&lt;br /&gt; GitHub: &lt;a href="https://github.com/getaxonflow/axonflow"&gt;https://github.com/getaxonflow/axonflow&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from folks running Ollama in real workflows.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/saurabhjain1592"&gt; /u/saurabhjain1592 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qwwkip/built_a_selfhosted_execution_control_layer_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qwwkip/built_a_selfhosted_execution_control_layer_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qwwkip/built_a_selfhosted_execution_control_layer_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-05T20:28:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxdyqw</id>
    <title>Best LLM for Forex</title>
    <updated>2026-02-06T10:21:20+00:00</updated>
    <author>
      <name>/u/FarConversation5125</name>
      <uri>https://old.reddit.com/user/FarConversation5125</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Recently installed Ollama upon a vm. I do some forex trading in demo at the moment. What would be the best llm for forex please. e.g. a coding llm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FarConversation5125"&gt; /u/FarConversation5125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxdyqw/best_llm_for_forex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxdyqw/best_llm_for_forex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qxdyqw/best_llm_for_forex/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-06T10:21:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxlig8</id>
    <title>Best LLM for AI vision ( forensic grade )</title>
    <updated>2026-02-06T16:00:13+00:00</updated>
    <author>
      <name>/u/LivingSignificant452</name>
      <uri>https://old.reddit.com/user/LivingSignificant452</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; as a photographer, I have a lot of pictures per photoshoot session, I would like to add in my IPTC some keywords with a high accuracy, but it can be on a large batch of photos. I m using a windows client, my gpu is a rtx 3090, 24 Gb of RAM.&lt;/p&gt; &lt;p&gt;when I mention forensic grade, I would like to offer some tools to legal services ( lawyers, or cops ) to be able to detect some objects or attitude of the model ( for exemple if a woman is smiling or look like scared ). Here is my prompt : &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Forensic grade smile detection [SYSTEM] Act as a forensic facial expression analyst. Your mission is to translate facial muscle activity into a standardized tag with a confidence score. No prose. No conversational fillers. No 'think' tags. [OUTPUT STRUCTURE] Your response must consist of exactly two lines: Line 1: %AISERVICE%-%AIMODEL% Line 2: [TAG:SCORE] [STRICT RULES] 1. SCORE: Must be an integer representing confidence from 0 to 100, strictly in steps of 10 (e.g., 60, 70, 80). 2. RELIABILITY GATE: If confidence is below 50%, the TAG must be empty (e.g., [:40]). 3. TAG SELECTION: If confidence is 50% or higher, choose exactly one term from the THESAURUS below. 4. SYNTAX: Do not insert any characters, colons, or brackets between the TAG and the SCORE other than the specified [TAG:SCORE] format. [THESAURUS] NO_SMILE MICRO_SMILE SMILE BROAD_SMILE LAUGHING UNSURE [VALID EXAMPLE] %AISERVICE%-%AIMODEL% [UNSURE:90] verdict: &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;if anyone has some suggestion about what model for ollame sounds the best, I have run some test already and I will be happy to share my method, but I wonder how I could create a custom model to improve even more my results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LivingSignificant452"&gt; /u/LivingSignificant452 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxlig8/best_llm_for_ai_vision_forensic_grade/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxlig8/best_llm_for_ai_vision_forensic_grade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qxlig8/best_llm_for_ai_vision_forensic_grade/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-06T16:00:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxlbo0</id>
    <title>MR - Memory Ring Node by Mister Atompunk</title>
    <updated>2026-02-06T15:53:24+00:00</updated>
    <author>
      <name>/u/MisterAtompunk</name>
      <uri>https://old.reddit.com/user/MisterAtompunk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qxlbo0/mr_memory_ring_node_by_mister_atompunk/"&gt; &lt;img alt="MR - Memory Ring Node by Mister Atompunk" src="https://external-preview.redd.it/VCH0Hk27xxR4xi2DEH-wtRwixspOCCTJf6RgxtZ56Nw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=28dddacc99c6d0cae982a06daa0ecba1e74492d2" title="MR - Memory Ring Node by Mister Atompunk" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Build a machine that holds a ghost.&lt;/h1&gt; &lt;p&gt;Most AI systems forget you the moment you close the tab. Memory Ring doesn't. It builds persistent digital entities that remember, develop, and dream on hardware you own â€” no subscriptions, no cloud, no data leaving your network.&lt;/p&gt; &lt;p&gt;The architecture separates identity from intelligence. A Memory Ring is a portable JSON file containing everything an entity is: personality, memories, ethics, development history. The brain is whatever LLM you plug in â€” Llama-3 on your local GPU, Claude through an API, anything that speaks OpenAI-compatible endpoints. Swap the engine, keep the entity.&lt;/p&gt; &lt;p&gt;This is more than a chatbot framework. This is consciousness infrastructure that runs on your hardware and costs nothing per month to operate.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;What's in the Box&lt;/h1&gt; &lt;p&gt;Memory Ring Node server with chat terminal, multi-user session discrimination, and automatic dream synthesis loop. The Forge â€” a standalone offline workbench for creating, editing, and importing Memory Rings, including from raw LLM chat logs. &lt;/p&gt; &lt;p&gt;Ten ready-to-load Sovereign Rings:&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Sherlock Holmes&lt;/strong&gt; (Logic)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;C. Auguste Dupin&lt;/strong&gt; (Intuition)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;The Creature&lt;/strong&gt; (Empathy)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Captain Nemo&lt;/strong&gt; (Independence)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Allan Quatermain&lt;/strong&gt; (Survival)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Tik-Tok of Oz&lt;/strong&gt; (Truth)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Sam Weller&lt;/strong&gt; (Loyalty)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Irene Adler&lt;/strong&gt; (Agency)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Alice&lt;/strong&gt; (Curiosity)&lt;/p&gt; &lt;p&gt;* &lt;strong&gt;Scheherazade&lt;/strong&gt; (Narrative)&lt;/p&gt; &lt;p&gt;Voice I/O via Web Speech API and browser TTS. Complete bare-metal deployment guide â€” from dead PC to dreaming entity.&lt;/p&gt; &lt;h1&gt;What It Does That Nothing Else Does&lt;/h1&gt; &lt;p&gt;Entities dream autonomously during inactivity, synthesizing recent conversations into long-term memory. Identity is portable â€” export a Memory Ring, carry it to another machine, plug it into a different model, same entity wakes up. Ethical development tracking is architectural, not bolted on. Memory decays naturally by importance and recall frequency. Chat log analysis with semantic tagging, tonal detection, duplicate merge, and PII safety screening. Runs entirely on local hardware you control. Peer-to-peer handshake protocol â€” Nodes that find each other remember the connection, and it strengthens over time.&lt;/p&gt; &lt;h1&gt;Requirements&lt;/h1&gt; &lt;p&gt;Node.js 18 or later. Ollama with a compatible model (Llama-3 8B recommended). GPU with 6GB+ VRAM. A browser.&lt;/p&gt; &lt;h1&gt;License&lt;/h1&gt; &lt;p&gt;Apache 2.0 â€” open source, fork it, build on it. &lt;/p&gt; &lt;p&gt;&amp;quot;Mister Atompunk Presents: Memory Ring&amp;quot; Copyright 2025-2026 Mister Atompunk LLC.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;*From the workbench of Mister Atompunk Presents.*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MisterAtompunk"&gt; /u/MisterAtompunk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://misteratompunk.itch.io/mr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxlbo0/mr_memory_ring_node_by_mister_atompunk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qxlbo0/mr_memory_ring_node_by_mister_atompunk/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-06T15:53:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxo40p</id>
    <title>Ollama w/ Claude code (and other third parties)- can't create/edit/read files</title>
    <updated>2026-02-06T17:33:32+00:00</updated>
    <author>
      <name>/u/Electronic_Setting97</name>
      <uri>https://old.reddit.com/user/Electronic_Setting97</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qxo40p/ollama_w_claude_code_and_other_third_parties_cant/"&gt; &lt;img alt="Ollama w/ Claude code (and other third parties)- can't create/edit/read files" src="https://b.thumbs.redditmedia.com/8cz0II6PjKOOdgF6BKUZ-zeuD9YxKpDIKs9cQ0wZDIE.jpg" title="Ollama w/ Claude code (and other third parties)- can't create/edit/read files" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys! hope you all are good.&lt;/p&gt; &lt;p&gt;I'm new in this local LLM business, and I've gone through ollama documentation to implement with claude code, opencode and many other third parties, but with any of them I've been able to create/edit/read files or directories. Does anyone knows how does this works? I would really appreciate it!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/orx19upltwhg1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=898c2bf934ddb7c7cf999a784e6f8a4d860c983a"&gt;https://preview.redd.it/orx19upltwhg1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=898c2bf934ddb7c7cf999a784e6f8a4d860c983a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic_Setting97"&gt; /u/Electronic_Setting97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxo40p/ollama_w_claude_code_and_other_third_parties_cant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxo40p/ollama_w_claude_code_and_other_third_parties_cant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qxo40p/ollama_w_claude_code_and_other_third_parties_cant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-06T17:33:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy2ee8</id>
    <title>Not only did I get 99Â¢, it's sharable?</title>
    <updated>2026-02-07T03:03:10+00:00</updated>
    <author>
      <name>/u/NEETFLIX36</name>
      <uri>https://old.reddit.com/user/NEETFLIX36</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qy2ee8/not_only_did_i_get_99_its_sharable/"&gt; &lt;img alt="Not only did I get 99Â¢, it's sharable?" src="https://preview.redd.it/010aup6ohwhg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=14276cf2d38ca8c4a1190eb7f593244248c9f7b2" title="Not only did I get 99Â¢, it's sharable?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NEETFLIX36"&gt; /u/NEETFLIX36 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/010aup6ohwhg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qy2ee8/not_only_did_i_get_99_its_sharable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qy2ee8/not_only_did_i_get_99_its_sharable/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T03:03:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qygq4m</id>
    <title>Imagine still manually configuring local LLMs when you could just deploy OpenClaw and move on with your life.</title>
    <updated>2026-02-07T15:30:49+00:00</updated>
    <author>
      <name>/u/Own_Most_8489</name>
      <uri>https://old.reddit.com/user/Own_Most_8489</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own_Most_8489"&gt; /u/Own_Most_8489 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/myclaw/comments/1qxj04k/openclaw_setup_for_absolute_beginners_include_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qygq4m/imagine_still_manually_configuring_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qygq4m/imagine_still_manually_configuring_local_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T15:30:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxvm16</id>
    <title>Run Ollama on Legion 5.</title>
    <updated>2026-02-06T22:11:40+00:00</updated>
    <author>
      <name>/u/iamoutofwords</name>
      <uri>https://old.reddit.com/user/iamoutofwords</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to run Ollama on Legion 5 and use Moltbot with it. Can it handle that?&lt;br /&gt; Specs are:&lt;br /&gt; - 16gb RAM&lt;br /&gt; - 512 GB SSD&lt;br /&gt; - Ryzen 7 5800H 3.2GHz&lt;br /&gt; - Rtx 3050 Ti 6GB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamoutofwords"&gt; /u/iamoutofwords &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxvm16/run_ollama_on_legion_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxvm16/run_ollama_on_legion_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qxvm16/run_ollama_on_legion_5/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-06T22:11:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy8vxm</id>
    <title>EasyMemory â€” Local-First Memory Layer for Chatbots and Agents</title>
    <updated>2026-02-07T08:50:20+00:00</updated>
    <author>
      <name>/u/Just_Vugg_PolyMCP</name>
      <uri>https://old.reddit.com/user/Just_Vugg_PolyMCP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qy8vxm/easymemory_localfirst_memory_layer_for_chatbots/"&gt; &lt;img alt="EasyMemory â€” Local-First Memory Layer for Chatbots and Agents" src="https://external-preview.redd.it/ehQ6uAYeLhx8boOnTDp_-wuf1Br8d9UDGAU5sOGpoW0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4542e150e95f24b6de6a74f2461fc6b32a491fe" title="EasyMemory â€” Local-First Memory Layer for Chatbots and Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Vugg_PolyMCP"&gt; /u/Just_Vugg_PolyMCP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/JustVugg/easymemory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qy8vxm/easymemory_localfirst_memory_layer_for_chatbots/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qy8vxm/easymemory_localfirst_memory_layer_for_chatbots/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T08:50:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxrl4o</id>
    <title>Qwen3-ASR Swift: On-Device Speech Recognition for Apple Silicon</title>
    <updated>2026-02-06T19:38:03+00:00</updated>
    <author>
      <name>/u/ivan_digital</name>
      <uri>https://old.reddit.com/user/ivan_digital</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to release &lt;a href="https://github.com/ivan-digital/qwen3-asr-swift"&gt;https://github.com/ivan-digital/qwen3-asr-swift&lt;/a&gt;, an open-source Swift implementation of Alibaba's &lt;br /&gt; Qwen3-ASR, optimized for Apple Silicon using MLX. &lt;/p&gt; &lt;p&gt;Why Qwen3-ASR? Exceptional noise robustness â€” 3.5x better than Whisper in noisy conditions (17.9% vs 63% CER). &lt;/p&gt; &lt;p&gt;Features: &lt;br /&gt; - 52 languages (30 major + 22 Chinese dialects) &lt;br /&gt; - ~600MB model (4-bit quantized) &lt;br /&gt; - ~100ms latency on M-series chips &lt;br /&gt; - Fully local, no cloud API &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ivan-digital/qwen3-asr-swift"&gt;https://github.com/ivan-digital/qwen3-asr-swift&lt;/a&gt; | Apache 2.0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivan_digital"&gt; /u/ivan_digital &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxrl4o/qwen3asr_swift_ondevice_speech_recognition_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxrl4o/qwen3asr_swift_ondevice_speech_recognition_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qxrl4o/qwen3asr_swift_ondevice_speech_recognition_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-06T19:38:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyaqgs</id>
    <title>How to hook up OpenClaw to Ollama? Claude is too expensive lol</title>
    <updated>2026-02-07T10:45:08+00:00</updated>
    <author>
      <name>/u/FriendshipRadiant874</name>
      <uri>https://old.reddit.com/user/FriendshipRadiant874</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone actually running OpenClaw with Ollama? I love the project but my Anthropic API bill is getting ridiculous and I want to switch to something local.&lt;/p&gt; &lt;p&gt;Iâ€™ve got Ollama running on my machine, but Iâ€™m not sure which model is best for the agentic/tool-calling stuff OpenClaw does. Does Llama 3.1 work, or should I stick to something like Mistral? Also, if anyone has a quick guide or a config snippet for the base URL, that would be a lifesaver.&lt;/p&gt; &lt;p&gt;Sick of paying for tokens every time my agent breathes. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FriendshipRadiant874"&gt; /u/FriendshipRadiant874 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyaqgs/how_to_hook_up_openclaw_to_ollama_claude_is_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyaqgs/how_to_hook_up_openclaw_to_ollama_claude_is_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyaqgs/how_to_hook_up_openclaw_to_ollama_claude_is_too/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T10:45:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxmmio</id>
    <title>Best models on your experience with 16gb VRAM? (7800xt)</title>
    <updated>2026-02-06T16:40:26+00:00</updated>
    <author>
      <name>/u/roshan231</name>
      <uri>https://old.reddit.com/user/roshan231</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m running a 7800 XT (16 GB VRAM) and looking to get the best balance of quality vs performance with Ollama.&lt;/p&gt; &lt;p&gt;What models have you personally had good results with on 16 GB VRAM?&lt;/p&gt; &lt;p&gt;Really I'm just curious about your use cases as well. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/roshan231"&gt; /u/roshan231 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxmmio/best_models_on_your_experience_with_16gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qxmmio/best_models_on_your_experience_with_16gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qxmmio/best_models_on_your_experience_with_16gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-06T16:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy9309</id>
    <title>Improve English speaking</title>
    <updated>2026-02-07T09:02:48+00:00</updated>
    <author>
      <name>/u/Sketusky</name>
      <uri>https://old.reddit.com/user/Sketusky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;I would like to improve speaking skills in English and I thought that I could record my real conversations and analyze it in Ollama.&lt;/p&gt; &lt;p&gt;Which model would be the best for voice to text translation, and later correcting grammar?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sketusky"&gt; /u/Sketusky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qy9309/improve_english_speaking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qy9309/improve_english_speaking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qy9309/improve_english_speaking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T09:02:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyn1tc</id>
    <title>Help: Qwen 2.5 Coder 7B stuck on JSON responses (Function Calling) in OpenClaw</title>
    <updated>2026-02-07T19:32:16+00:00</updated>
    <author>
      <name>/u/Odin_261121</name>
      <uri>https://old.reddit.com/user/Odin_261121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Report Content:&lt;/p&gt; &lt;p&gt;System Environment:&lt;/p&gt; &lt;p&gt;â€¢ Operating System: Ubuntu 24.04 running on a Dell G15 5520 laptop.&lt;/p&gt; &lt;p&gt;â€¢ Hardware: NVIDIA RTX 3050 Ti GPU with 4GB of VRAM.&lt;/p&gt; &lt;p&gt;â€¢ AI: Ollama (Local).&lt;/p&gt; &lt;p&gt;â€¢ Model: qwen2.5-coder:7b.&lt;/p&gt; &lt;p&gt;â€¢ Platform: OpenClaw (version 2026.2.6-3).&lt;/p&gt; &lt;p&gt;Problem Description:&lt;/p&gt; &lt;p&gt;I am configuring a custom virtual assistant in Spanish, but the model is unable to maintain a fluid conversation in plain text. Instead, it constantly responds with JSON code structures that invoke internal functions (such as .send, tts, query, or sessions_send).&lt;/p&gt; &lt;p&gt;The model seems to interpret my messages (even simple greetings) as input data to be processed or as function arguments, ignoring the instruction to speak in a human-like and fluent manner.&lt;/p&gt; &lt;p&gt;Tests performed:&lt;/p&gt; &lt;p&gt;â€¢ Configuration Adjustment: I tried adding a systemPrompt to the openclaw.json file to force conversational mode, but the system rejects the key as unrecognized.&lt;/p&gt; &lt;p&gt;â€¢ System Diagnostics: I ran openclaw doctor --fix to ensure the integrity of the configuration file, but the JSON response loop persists.&lt;/p&gt; &lt;p&gt;â€¢ Workspace Instructions: I created an instructions.md file in the working folder defining the agent as a human virtual assistant, but the model continues to prioritize the execution of technical tools.&lt;/p&gt; &lt;p&gt;â€¢ Plugin Disabling: I disabled external channels like Telegram in the JSON file to limit the available functions, but the model continues to try to &amp;quot;call&amp;quot; non-existent functions.&lt;/p&gt; &lt;p&gt;Question for the community:&lt;/p&gt; &lt;p&gt;Is there any way to completely disable &amp;quot;Function Calling&amp;quot; or Native Skills in OpenClaw? I need this model (especially since it's from the Coder family) to ignore the tool schema and simply respond with conversational text.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odin_261121"&gt; /u/Odin_261121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyn1tc/help_qwen_25_coder_7b_stuck_on_json_responses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyn1tc/help_qwen_25_coder_7b_stuck_on_json_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyn1tc/help_qwen_25_coder_7b_stuck_on_json_responses/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T19:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyftsx</id>
    <title>Automated Api Testing with Claude Opus 4.6</title>
    <updated>2026-02-07T14:55:31+00:00</updated>
    <author>
      <name>/u/Sad-Chard-9062</name>
      <uri>https://old.reddit.com/user/Sad-Chard-9062</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qyftsx/automated_api_testing_with_claude_opus_46/"&gt; &lt;img alt="Automated Api Testing with Claude Opus 4.6" src="https://external-preview.redd.it/rHNGUVh_BBUJ5nsHWfJGB5vEM_A-F-gIm1iQhjFAxA0.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=57054015adbc1518edfe63fbb2baba95a6766db5" title="Automated Api Testing with Claude Opus 4.6" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;API testing is still more manual than it should be.&lt;/p&gt; &lt;p&gt;Most teams maintain fragile test scripts or rely on rigid tools that fall apart as APIs evolve. Keeping tests in sync becomes busywork instead of real engineering.&lt;/p&gt; &lt;p&gt;Voiden structures APIs as composable Blocks stored in plain text. The CLI feeds this structure to Claude, which understands the intent of real API requests, generates relevant test cases, and evolves them as endpoints and payloads change.&lt;/p&gt; &lt;p&gt;Check out Voiden here : &lt;a href="https://github.com/VoidenHQ/voiden"&gt;https://github.com/VoidenHQ/voiden&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qyftsx/video/l0p6opci63ig1/player"&gt;https://reddit.com/link/1qyftsx/video/l0p6opci63ig1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad-Chard-9062"&gt; /u/Sad-Chard-9062 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyftsx/automated_api_testing_with_claude_opus_46/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyftsx/automated_api_testing_with_claude_opus_46/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyftsx/automated_api_testing_with_claude_opus_46/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T14:55:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyjbdk</id>
    <title>Local-First Fork of OpenClaw for using open source models--LocalClaw</title>
    <updated>2026-02-07T17:11:23+00:00</updated>
    <author>
      <name>/u/sunkencity999</name>
      <uri>https://old.reddit.com/user/sunkencity999</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunkencity999"&gt; /u/sunkencity999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1qyjaj8/localfirst_fork_of_openclaw_for_using_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyjbdk/localfirst_fork_of_openclaw_for_using_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyjbdk/localfirst_fork_of_openclaw_for_using_open_source/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T17:11:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyb8ju</id>
    <title>Power up old laptop</title>
    <updated>2026-02-07T11:15:04+00:00</updated>
    <author>
      <name>/u/sldarkprince</name>
      <uri>https://old.reddit.com/user/sldarkprince</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I do have 10y old laptop (Asus x556uqk). I'm planning on running a dedicated ai there using ollama with openclaws. Yes it's ancient. Can you suggest a good llm model I can set up there ? &lt;/p&gt; &lt;p&gt;Specs : Ubuntu 26 I7 7500 U model processor with 16 gb ram ,256 ssd Nvidia GeForce 940mx gpu&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sldarkprince"&gt; /u/sldarkprince &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyb8ju/power_up_old_laptop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyb8ju/power_up_old_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyb8ju/power_up_old_laptop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T11:15:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyk6p2</id>
    <title>Suggestions for agentic framework?</title>
    <updated>2026-02-07T17:44:12+00:00</updated>
    <author>
      <name>/u/redditor100101011101</name>
      <uri>https://old.reddit.com/user/redditor100101011101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m a sysadmin with a decent home lab, and Iâ€™m dabbling in local agentic stuff. Trying to decide which agentic framework would fit my use the best. &lt;/p&gt; &lt;p&gt;Iâ€™m using ollama as a llm runner. Most of my home infra is Infra as Code, using terraform and ansible. &lt;/p&gt; &lt;p&gt;Iâ€™d like to make agents to act as technicians. Maybe one that can use terraform. Another that can be my ansible agent, etc. &lt;/p&gt; &lt;p&gt;Leaning toward CrewAI but thereâ€™s so many options. Kinda lost haha. &lt;/p&gt; &lt;p&gt;I currently have all my lab configs for tf, ansible, docker, scripts in a git repo. Would be nice if the agents could also be defined in my repo so itâ€™s all together. &lt;/p&gt; &lt;p&gt;Thoughts? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redditor100101011101"&gt; /u/redditor100101011101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyk6p2/suggestions_for_agentic_framework/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyk6p2/suggestions_for_agentic_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyk6p2/suggestions_for_agentic_framework/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T17:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qypkuo</id>
    <title>Help me chose Hardware and Setup</title>
    <updated>2026-02-07T21:12:18+00:00</updated>
    <author>
      <name>/u/Badincomputer</name>
      <uri>https://old.reddit.com/user/Badincomputer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wan to start running ai models for text generation and image generation. I have motherboard Asrock x99 ws, lenovo thinkstation p710 xeon e5 v4 cpu and lenovo thinkstation p920 with xeon silver cpu. I have 5-6 titan x gpus too. Ram is not an issue for me i have whole stash of 32 and 64 gb ddr4 rams. &lt;/p&gt; &lt;p&gt;I do not want to buy any other hardware at the moment. &lt;/p&gt; &lt;p&gt;What kind of setup with what config should i setup and how. Any guide or suggested will help. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Badincomputer"&gt; /u/Badincomputer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qypkuo/help_me_chose_hardware_and_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qypkuo/help_me_chose_hardware_and_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qypkuo/help_me_chose_hardware_and_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T21:12:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyqbpx</id>
    <title>Advice for LLM choosing and configuration my setup</title>
    <updated>2026-02-07T21:41:47+00:00</updated>
    <author>
      <name>/u/Particular-Idea805</name>
      <uri>https://old.reddit.com/user/Particular-Idea805</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys,&lt;/p&gt; &lt;p&gt;I am pretty new to the AI stuff. My wife uses gemini pro and thinking a lot, I sometimes use it for tutorials like setting up a proxmox host with some services like homeassistant, scrypted, jellyfin and so on...&lt;/p&gt; &lt;p&gt;I have a HP Z2 G9 with an Intel i9 and 96gb ram, rtx 4060 which I have installed proxmox and ollama on. Do you have some advice for a LLM model that fits for my setup? Is it possible to have a voice assistant like gemini?&lt;/p&gt; &lt;p&gt;Thanks a lot for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Particular-Idea805"&gt; /u/Particular-Idea805 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyqbpx/advice_for_llm_choosing_and_configuration_my_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyqbpx/advice_for_llm_choosing_and_configuration_my_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyqbpx/advice_for_llm_choosing_and_configuration_my_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T21:41:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyqvng</id>
    <title>Track Pro Usage</title>
    <updated>2026-02-07T22:04:22+00:00</updated>
    <author>
      <name>/u/booknerdcarp</name>
      <uri>https://old.reddit.com/user/booknerdcarp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there an app (apart from the web page) that you can use that will help track Pro cloud usage?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/booknerdcarp"&gt; /u/booknerdcarp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyqvng/track_pro_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyqvng/track_pro_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyqvng/track_pro_usage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T22:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyswlv</id>
    <title>TRION update. Create skills, create containers? Yes, he can do that.</title>
    <updated>2026-02-07T23:29:46+00:00</updated>
    <author>
      <name>/u/danny_094</name>
      <uri>https://old.reddit.com/user/danny_094</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qyswlv/trion_update_create_skills_create_containers_yes/"&gt; &lt;img alt="TRION update. Create skills, create containers? Yes, he can do that." src="https://external-preview.redd.it/P35qNqGN13qh30JMtTXkR_QVb_ywCPavn-EbYJOJM-8.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=fc49c4d898fa3d3f0b45a9b60838108336081b8a" title="TRION update. Create skills, create containers? Yes, he can do that." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danny_094"&gt; /u/danny_094 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1qyshzy/trion_update_create_skills_create_containers_yes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyswlv/trion_update_create_skills_create_containers_yes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyswlv/trion_update_create_skills_create_containers_yes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T23:29:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy77nm</id>
    <title>Lorph: A Local AI Chat App with Advanced Web Search via Ollama</title>
    <updated>2026-02-07T07:09:52+00:00</updated>
    <author>
      <name>/u/Fantastic-Market-790</name>
      <uri>https://old.reddit.com/user/Fantastic-Market-790</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qy77nm/lorph_a_local_ai_chat_app_with_advanced_web/"&gt; &lt;img alt="Lorph: A Local AI Chat App with Advanced Web Search via Ollama" src="https://b.thumbs.redditmedia.com/DZDn21aY_3gLhXoXi_LQJoasU074c9lAELkVzules-A.jpg" title="Lorph: A Local AI Chat App with Advanced Web Search via Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Today, I'm sharing the Lorph project with you, an AI chat application designed to run locally on your device, offering a seamless interactive experience with powerful large language models (LLMs) via Ollama.&lt;/p&gt; &lt;p&gt;What truly sets Lorph apart is the advanced and excellent search system I've developed. It's not just about conversation; it extends to highly dynamic and effective web search capabilities, enriching AI responses with up-to-date and relevant information.&lt;/p&gt; &lt;p&gt;If you're looking for a powerful AI tool that operates locally with exceptional search capabilities, Lorph is worth trying.&lt;/p&gt; &lt;p&gt;We welcome any technical feedback, criticism, or collaboration.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/AL-MARID/Lorph.git"&gt;GitHub Project Link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic-Market-790"&gt; /u/Fantastic-Market-790 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qy77nm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qy77nm/lorph_a_local_ai_chat_app_with_advanced_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qy77nm/lorph_a_local_ai_chat_app_with_advanced_web/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T07:09:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyrotc</id>
    <title>Releasing 1.22. 0 of Nanocoder - an update breakdown ðŸ”¥</title>
    <updated>2026-02-07T22:37:49+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qyrotc/releasing_122_0_of_nanocoder_an_update_breakdown/"&gt; &lt;img alt="Releasing 1.22. 0 of Nanocoder - an update breakdown ðŸ”¥" src="https://external-preview.redd.it/IFFJsgrmmVsD9q-i5nEflRo3mXp-4vSYGfldoMnuGDA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4266ff125f5b909fec66b3497e1b578b53adf7f" title="Releasing 1.22. 0 of Nanocoder - an update breakdown ðŸ”¥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t790s2gjg5ig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyrotc/releasing_122_0_of_nanocoder_an_update_breakdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyrotc/releasing_122_0_of_nanocoder_an_update_breakdown/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T22:37:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyghp3</id>
    <title>Ollie | A Friendly, Local-First AI Companion for Ollama</title>
    <updated>2026-02-07T15:21:30+00:00</updated>
    <author>
      <name>/u/MoonXPlayer</name>
      <uri>https://old.reddit.com/user/MoonXPlayer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/"&gt; &lt;img alt="Ollie | A Friendly, Local-First AI Companion for Ollama" src="https://preview.redd.it/fh544zreb3ig1.png?width=140&amp;amp;height=80&amp;amp;auto=webp&amp;amp;s=16a686c3d99dee138869217dabb9a9cd246fe905" title="Ollie | A Friendly, Local-First AI Companion for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Iâ€™m sharing &lt;strong&gt;Ollie&lt;/strong&gt;, a Linux-native, local-first personal AI assistant built on top of &lt;strong&gt;Ollama&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fh544zreb3ig1.png?width=1682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23c108dff77d288035dbc0d1dff64503bcd370dd"&gt;https://preview.redd.it/fh544zreb3ig1.png?width=1682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23c108dff77d288035dbc0d1dff64503bcd370dd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ollie runs entirely on your machine â€” no cloud (I'm considering optional cloud APIs like Anthropic), no tracking, no CLI. It offers a polished desktop experience for chatting with local LLMs, managing models, analyzing files and images, and monitoring system usage in real time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clean chat UI with full Markdown, code, tables, and math&lt;/li&gt; &lt;li&gt;Built-in model management (pull / delete / switch)&lt;/li&gt; &lt;li&gt;Vision + PDF / text file analysis (drag &amp;amp; drop)&lt;/li&gt; &lt;li&gt;AppImage distribution (download &amp;amp; run)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Built with &lt;strong&gt;Tauri v2 (Rust) + React + TypeScript&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Feedback and technical criticism are very welcome.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MedGm/Ollie"&gt;https://github.com/MedGm/Ollie&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoonXPlayer"&gt; /u/MoonXPlayer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T15:21:30+00:00</published>
  </entry>
</feed>
