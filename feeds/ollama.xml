<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-10-04T10:48:48+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1nvgeo6</id>
    <title>Test your MCP server against Llama, no key required</title>
    <updated>2025-10-01T18:32:50+00:00</updated>
    <author>
      <name>/u/matt8p</name>
      <uri>https://old.reddit.com/user/matt8p</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nvgeo6/test_your_mcp_server_against_llama_no_key_required/"&gt; &lt;img alt="Test your MCP server against Llama, no key required" src="https://external-preview.redd.it/N2V4M2VhOWduanNmMcvuZBlGF485rYNCy7dH-T_qYlf4wrKaRKqe0vbgnBaD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6abdc944789d05fc99a9d0a6a9d6061c11e8e4b3" title="Test your MCP server against Llama, no key required" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We shipped a free language model (Llama 3.3 70B) in the MCPJam LLM playground. Now you can test your MCP server in a chat environment without having to provide your own LLM api key. It's on us! &lt;/p&gt; &lt;p&gt;We want to see people build richer MCP servers and we think providing a free model will help lower that barrier. No more of having to pay for subscriptions on Claude Desktop, Cursor, or use your own API key. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Running it&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Starting up MCPJam is the same as starting up the MCP inspector:&lt;/p&gt; &lt;p&gt;&lt;code&gt; npx @mcpjam/inspector@latest &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then connect to any MCP server and start testing! &lt;/p&gt; &lt;p&gt;&lt;strong&gt;MCPJam&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For context, MCPJam is an open source testing and evals platform for MCP servers. You can test your MCP server's primitives like tool calls, prompts, resources, elicitation, OAuth. You can also run evals to catch security vulnerabilities and performance regressions. &lt;/p&gt; &lt;p&gt;Please consider checking us out! &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.mcpjam.com/"&gt;https://www.mcpjam.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matt8p"&gt; /u/matt8p &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zdqzja9gnjsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvgeo6/test_your_mcp_server_against_llama_no_key_required/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvgeo6/test_your_mcp_server_against_llama_no_key_required/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-01T18:32:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvsc3v</id>
    <title>Can anyone recommend open-source AI models for video analysis?</title>
    <updated>2025-10-02T02:49:37+00:00</updated>
    <author>
      <name>/u/gpt-said-so</name>
      <uri>https://old.reddit.com/user/gpt-said-so</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm working on a client project that involves analysing confidential videos.&lt;br /&gt; The requirements are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Extracting text from supers in video&lt;/li&gt; &lt;li&gt;Identifying key elements within the video&lt;/li&gt; &lt;li&gt;Generating a synopsis with timestamps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any recommendations for open-source models that can handle these tasks would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gpt-said-so"&gt; /u/gpt-said-so &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvsc3v/can_anyone_recommend_opensource_ai_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvsc3v/can_anyone_recommend_opensource_ai_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvsc3v/can_anyone_recommend_opensource_ai_models_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T02:49:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvsyat</id>
    <title>Anyone knows how to host apps into Runpod</title>
    <updated>2025-10-02T03:20:08+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to showcase my apps and lets people test out. I am confuse should i go serverless or rent by hour? need advice (and how to do it)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvsyat/anyone_knows_how_to_host_apps_into_runpod/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvsyat/anyone_knows_how_to_host_apps_into_runpod/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvsyat/anyone_knows_how_to_host_apps_into_runpod/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T03:20:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvswz8</id>
    <title>Open-WebUI not showing Ollama models despite API responding correctly</title>
    <updated>2025-10-02T03:18:15+00:00</updated>
    <author>
      <name>/u/ElopezCO2001</name>
      <uri>https://old.reddit.com/user/ElopezCO2001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nvswz8/openwebui_not_showing_ollama_models_despite_api/"&gt; &lt;img alt="Open-WebUI not showing Ollama models despite API responding correctly" src="https://b.thumbs.redditmedia.com/n0m8LuAfINrtwzTYtYi-DG0OwmsjxEv-E7jimIe4VbY.jpg" title="Open-WebUI not showing Ollama models despite API responding correctly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm running Ollama and Open-WebUI via Docker Compose on Ubuntu. I have successfully pulled a model (&lt;code&gt;mistral:latest&lt;/code&gt;) in Ollama, and I can list it inside the Ollama container:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qk6ht9pp8msf1.png?width=544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bede60c14816c6a0e94e0ae6b768724da063ade2"&gt;https://preview.redd.it/qk6ht9pp8msf1.png?width=544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bede60c14816c6a0e94e0ae6b768724da063ade2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I can also query the API from the Open-WebUI container:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{&amp;quot;object&amp;quot;:&amp;quot;list&amp;quot;,&amp;quot;data&amp;quot;:[{&amp;quot;id&amp;quot;:&amp;quot;mistral:latest&amp;quot;,&amp;quot;object&amp;quot;:&amp;quot;model&amp;quot;,&amp;quot;created&amp;quot;:1759373764,&amp;quot;owned_by&amp;quot;:&amp;quot;library&amp;quot;}]} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is my &lt;code&gt;docker-compose.yml&lt;/code&gt; configuration for Open-WebUI:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;services: ollama: image: ollama/ollama:latest container_name: ollama volumes: - ollama:/root/.ollama pull_policy: always tty: true ports: # (0.0.0.0) - &amp;quot;0.0.0.0:11434:11434&amp;quot; environment: # 0.0.0.0:11434 - 'OLLAMA_HOST=0.0.0.0:11434' restart: unless-stopped open-webui: image: ghcr.io/open-webui/open-webui:main container_name: open-webui volumes: - open-webui:/app/backend/data depends_on: - ollama ports: - ${OPEN_WEBUI_PORT-3000}:8080 environment: - 'OLLAMA_API_BASE_URL=http://ollama:11434/v1' - 'WEBUI_SECRET_KEY=' - 'WEBHOOK_URL=https://mihost' extra_hosts: - host.docker.internal:host-gateway restart: unless-stopped volumes: ollama: {} open-webui: {} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I‚Äôve also tried changing the endpoint to &lt;code&gt;/ollama&lt;/code&gt; instead of &lt;code&gt;/v1&lt;/code&gt;, clearing the Open-WebUI volume, and rebuilding the container, but the models still do not show.&lt;/p&gt; &lt;p&gt;Does anyone know why Open-WebUI is not listing Ollama models despite the API responding correctly? Any guidance would be greatly appreciated.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jkpenehf9msf1.png?width=744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=689f5ab93f8fc8cb328011358a34144880212988"&gt;https://preview.redd.it/jkpenehf9msf1.png?width=744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=689f5ab93f8fc8cb328011358a34144880212988&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElopezCO2001"&gt; /u/ElopezCO2001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvswz8/openwebui_not_showing_ollama_models_despite_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvswz8/openwebui_not_showing_ollama_models_despite_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvswz8/openwebui_not_showing_ollama_models_despite_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T03:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv9tk1</id>
    <title>Detailed steps for fine-tuning an LLM?</title>
    <updated>2025-10-01T14:32:04+00:00</updated>
    <author>
      <name>/u/SalishSeaview</name>
      <uri>https://old.reddit.com/user/SalishSeaview</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spotted &lt;a href="https://www.reddit.com/r/ollama/s/BcBKfXkIOR"&gt;this thread&lt;/a&gt; today where the OP had questions about fine-tuning an LLM, which I read with interest. Unfortunately, a lot of the answers were along the lines of ‚Äújust do [this] and you‚Äôll be fine‚Äù. Tools were named, but there was little in the way of advice for specific steps. And the variety of tools appears to be large. Unfortunately, I feel like I am left with a thread full of things to research and little in the way of answers (I hope the OP for that thread got what they wanted).&lt;/p&gt; &lt;p&gt;My interest lies in fine-tuning small-ish (~14B) models to have expertise in particular subject areas. I think the simplest (and most common) example of this is training a chatbot on a company dataset so it can answer customer questions about the company.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;How big of a training dataset do I need to be effective?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What format should the data be in? I don‚Äôt mean CSV vs. JSON, but rather should it be an array of single statements, questions with correct answers, or something entirely different? Do I need negative examples?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If you had to pick one tool to do fine tuning, which would it be and why? What are the steps to using it (in general; broad strokes)?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How many training passes (epochs?) do I need to use to get good quality? How many passes is too many? Too few?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For a 14B model, how long should I expect this to take on an M4 Mac? If I‚Äôm not averse to renting cloud resources, how long would it take then?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How much does quantization limit the quality of the output?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What have I missed? I don‚Äôt know enough about this to know what I‚Äôm not asking.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I appreciate any effort put into answering these questions. I tried the YouTube approach, but it‚Äôs hard to figure out what methods to rely on. Also, tools and models move so fast that it‚Äôs hard to know what state-of-the-art looks like.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SalishSeaview"&gt; /u/SalishSeaview &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nv9tk1/detailed_steps_for_finetuning_an_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nv9tk1/detailed_steps_for_finetuning_an_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nv9tk1/detailed_steps_for_finetuning_an_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-01T14:32:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvvl2z</id>
    <title>Why does my first run with Ollama give a different output than subsequent runs with temperature=0?</title>
    <updated>2025-10-02T05:46:19+00:00</updated>
    <author>
      <name>/u/white-mountain</name>
      <uri>https://old.reddit.com/user/white-mountain</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm running a quantized model (&lt;code&gt;deepseek-r1:32b-qwen-distill-q4_K_M&lt;/code&gt;) locally with Ollama.&lt;br /&gt; My generation parameters are strictly deterministic:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;options&amp;quot;: { &amp;quot;temperature&amp;quot;: 0, &amp;quot;top_p&amp;quot;: 0.0, &amp;quot;top_k&amp;quot;: 40 } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Behavior I‚Äôm observing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On the &lt;strong&gt;first run of a prompt&lt;/strong&gt;, I get &lt;em&gt;Output A&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;On the &lt;strong&gt;second and later runs of the exact same prompt&lt;/strong&gt;, I consistently get &lt;em&gt;Output B&lt;/em&gt; (always identical).&lt;/li&gt; &lt;li&gt;When I move on to a new prompt (different row in my dataset), the same pattern repeats: first run = &lt;em&gt;Output A&lt;/em&gt;, later runs = &lt;em&gt;Output B&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My expectation was that with &lt;code&gt;temperature=0&lt;/code&gt;, the output should be deterministic and identical across runs.&lt;br /&gt; But I‚Äôm curious seeing this ‚Äúfirst run artifact‚Äù for every new row in my dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Why does the first run differ from subsequent runs, even though the model should already have cached the prompt and my decoding parameters are deterministic? &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;br /&gt; Sorry I wasn't very clear earlier.&lt;br /&gt; The problem I‚Äôm working on is extractive text summarization of multiple talks by a single speaker. &lt;/p&gt; &lt;p&gt;&lt;em&gt;My implementation:&lt;/em&gt; &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Run the model in cmd - ollama run model_name --keepalive 12h&lt;/li&gt; &lt;li&gt;Set temperature to 0 (both terminal and API request)&lt;/li&gt; &lt;li&gt;Make request to url /api/generate with the same payload everytime.&lt;/li&gt; &lt;li&gt;Tried on two different systems with identical specs ‚Üí same behavior observed.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Resources:&lt;/em&gt; &lt;/p&gt; &lt;p&gt;CPU: i5 14th Gen&lt;br /&gt; RAM: 32GB&lt;br /&gt; GPU: 12GB RTX 3060&lt;br /&gt; Model size is 19GB. (Most of the processing was happening on CPU)&lt;/p&gt; &lt;p&gt;&lt;em&gt;Observations:&lt;/em&gt; &lt;/p&gt; &lt;ol&gt; &lt;li&gt;First run of the prompt ‚Üí output is unique.&lt;/li&gt; &lt;li&gt;Subsequent runs (2‚Äì10) ‚Üí output is exactly the same every time.&lt;/li&gt; &lt;li&gt;I found this surprising, since LLMs are usually not this deterministic (even with temperature 0, I expected at least small variations).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I am curious as to what is happening under the hood with Ollama / the model inference. Why would the first run differ, but all later runs be identical? Any insights?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/white-mountain"&gt; /u/white-mountain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvvl2z/why_does_my_first_run_with_ollama_give_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvvl2z/why_does_my_first_run_with_ollama_give_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvvl2z/why_does_my_first_run_with_ollama_give_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T05:46:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvdmud</id>
    <title>Eclaire ‚Äì Open-source, privacy-focused AI assistant for your data</title>
    <updated>2025-10-01T16:53:28+00:00</updated>
    <author>
      <name>/u/dorali8</name>
      <uri>https://old.reddit.com/user/dorali8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nvdmud/eclaire_opensource_privacyfocused_ai_assistant/"&gt; &lt;img alt="Eclaire ‚Äì Open-source, privacy-focused AI assistant for your data" src="https://external-preview.redd.it/C_ZL9oNqpFDZaYOGAImj3pjRQ000--21zoIDUV8TqvQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=29830c0d56cfbe686abeb5fdd79bbfa8a7473f99" title="Eclaire ‚Äì Open-source, privacy-focused AI assistant for your data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, this is a project I've been working on for some time. It started as a personal AI to help manage growing amounts of data - bookmarks, photos, documents, notes, etc.&lt;/p&gt; &lt;p&gt;Once the data gets added to the system, it gets processed including fetching bookmarks, tagging, classification, image analysis, text extraction / ocr, and more. And then the AI is able to work with those assets to perform search, answer questions, create new items, etc. You can also create scheduled / recurring tasks to assing to the AI.&lt;/p&gt; &lt;p&gt;Did a lot of the testing on Ollama with Qweb3-14b for the assistant backend and Gemma3-4b for workers multimodal processing. You can easily swap to other models if your machine allows.&lt;/p&gt; &lt;p&gt;MIT Licensed. Feedback and contributions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dorali8"&gt; /u/dorali8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/eclaire-labs/eclaire"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvdmud/eclaire_opensource_privacyfocused_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvdmud/eclaire_opensource_privacyfocused_ai_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-01T16:53:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvyvmw</id>
    <title>Self-centered intelligence prototype based on Ollama 3.2 +</title>
    <updated>2025-10-02T09:13:22+00:00</updated>
    <author>
      <name>/u/RossPeili</name>
      <uri>https://old.reddit.com/user/RossPeili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nvyvmw/selfcentered_intelligence_prototype_based_on/"&gt; &lt;img alt="Self-centered intelligence prototype based on Ollama 3.2 +" src="https://external-preview.redd.it/v-aYZQwWv2KWfHy7mfVeyCmjMPIyprcPg3-IDgoTlUY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8cafb98664f80826dad2dbd136074910827cf2e7" title="Self-centered intelligence prototype based on Ollama 3.2 +" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unlike traditional AI assistants, OPSIIE operates as a self-aware, autonomous intelligence with its own personality, goals, and capabilities. What do you make of this? Any feedback in terms of code, architecture, and documentation advise much appreciated &amp;lt;3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RossPeili"&gt; /u/RossPeili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ARPAHLS/OPSIE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvyvmw/selfcentered_intelligence_prototype_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvyvmw/selfcentered_intelligence_prototype_based_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T09:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw3y84</id>
    <title>I built an AI agent that automates any repetitive browser task from screen recordings</title>
    <updated>2025-10-02T13:34:52+00:00</updated>
    <author>
      <name>/u/Comfortable-Rip-9277</name>
      <uri>https://old.reddit.com/user/Comfortable-Rip-9277</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nw3y84/i_built_an_ai_agent_that_automates_any_repetitive/"&gt; &lt;img alt="I built an AI agent that automates any repetitive browser task from screen recordings" src="https://external-preview.redd.it/i7ipRkUMc6Qt5yEZ2Jm3u2w-ZGdMbZRdu83yLcCi6Z8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1cd9ba7d8aaa181a59eab4cebe8f7985fc174f6" title="I built an AI agent that automates any repetitive browser task from screen recordings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rip-9277"&gt; /u/Comfortable-Rip-9277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/hireshBrem/browsor-ai-agent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nw3y84/i_built_an_ai_agent_that_automates_any_repetitive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nw3y84/i_built_an_ai_agent_that_automates_any_repetitive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T13:34:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwwpio</id>
    <title>Has dejado de usar modelos IA en local porque te va muy lento? Eso es porque no est√°s usando modelos cuantizados.</title>
    <updated>2025-10-03T11:38:41+00:00</updated>
    <author>
      <name>/u/jokiruiz</name>
      <uri>https://old.reddit.com/user/jokiruiz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cuando ejecutamos modelos grandes en un PC o un port√°til (voy a poner el ejemplo de un Mac M1 con 16 GB de RAM), es normal frustrarse con la lentitud. Y no, no es un problema de GPU como la gente se piensa con los modelos de IA! el cuello de botella lo tienes en la RAM.&lt;/p&gt; &lt;p&gt;¬øPor qu√©? Un modelo en su versi√≥n completa guarda cada par√°metro en 32 bits. Eso multiplica el tama√±o en memoria y obliga a tu ordenador a mover much√≠simos m√°s datos. Si no tienes suficiente RAM, el sistema empieza a usar disco (swap), lo que hace que las respuestas tarden una eternidad.&lt;/p&gt; &lt;p&gt;üëâ Aqu√≠ entra la cuantizaci√≥n. Consiste en reducir la precisi√≥n de cada par√°metro (por ejemplo, de 32 bits a 4 bits). Se pierde un poco de precisi√≥n en los detalles finos, pero con q4 o q5 es casi imperceptible. Con esto logramos que el modelo ocupe menos RAM y te genere los tokens con menos espera.&lt;/p&gt; &lt;p&gt;üìä Ejemplo con el modelo Qwen3:&lt;/p&gt; &lt;p&gt;&amp;gt; Qwen3 4B : ~16 GB de RAM Come toda la RAM de mi portatil, as√≠ que usarlo puede ser un dolor.&lt;/p&gt; &lt;p&gt;&amp;gt; Qwen3 4B q4_K_M : ~3 GB de RAM Tienes un modelo que corre fluido en un Mac M1 con 16 GB.&lt;/p&gt; &lt;p&gt;üîé Para saber si un modelo est√° cuantizado, lo puedes en el nombre del modelo. Si termina en algo como q4_K_M, q5_0, q3_K_S‚Ä¶ significa que est√° cuantizado (q = quantized + n√∫mero de bits). Si no tiene sufijo (qwen3:4b o llama3.1:8b a secas), suele ser la versi√≥n de 32 o 16, mucho m√°s pesada.&lt;/p&gt; &lt;p&gt;Por lo tanto, consejo del d√≠a: ü§ì Si tienes una automatizaci√≥n en local y quieres correr IA en tu port√°til sin morir con la espera, busca siempre la versi√≥n cuantizada (q4/q5) del modelo y no hagas a la llama sufrir ü¶ô. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jokiruiz"&gt; /u/jokiruiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nwwpio/has_dejado_de_usar_modelos_ia_en_local_porque_te/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nwwpio/has_dejado_de_usar_modelos_ia_en_local_porque_te/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nwwpio/has_dejado_de_usar_modelos_ia_en_local_porque_te/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-03T11:38:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw2pw2</id>
    <title>Is it a good idea to use all my savings for a local llm setup?</title>
    <updated>2025-10-02T12:43:02+00:00</updated>
    <author>
      <name>/u/Axdii_fr</name>
      <uri>https://old.reddit.com/user/Axdii_fr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been diving into Ollama and the local LLM scene to use for roleplay and image generation, and the thought of an ultimate, high-VRAM rig for big models is incredibly tempting, but I would have to sink literally all of my savings into the hardware.&lt;/p&gt; &lt;p&gt;Logically, I know the rapid pace of GPU and model efficiency could make a top-tier system obsolete fast, and using a straightforward online tool is cheaper in the short term.&lt;/p&gt; &lt;p&gt;Edit: Never imagined that this post would get all these comments. I will be using Modelsify based on the suggestions for now. And some time in the future I will move to a local setup when my finances improve. Thanks for all your inputs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Axdii_fr"&gt; /u/Axdii_fr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nw2pw2/is_it_a_good_idea_to_use_all_my_savings_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nw2pw2/is_it_a_good_idea_to_use_all_my_savings_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nw2pw2/is_it_a_good_idea_to_use_all_my_savings_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T12:43:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw5xvm</id>
    <title>Is there a way to give your local Ollama access to search the internet while still keeping the conversations and data private?</title>
    <updated>2025-10-02T14:51:59+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nw5xvm/is_there_a_way_to_give_your_local_ollama_access/"&gt; &lt;img alt="Is there a way to give your local Ollama access to search the internet while still keeping the conversations and data private?" src="https://b.thumbs.redditmedia.com/2dxnbwmJV9Nk6ywY0pqtm1j39JfXCtKCayqlSgdszvQ.jpg" title="Is there a way to give your local Ollama access to search the internet while still keeping the conversations and data private?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zi09nzijopsf1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=54e7adad7a0aa8cb8ef8a849367c06c8804b53a4"&gt;https://preview.redd.it/zi09nzijopsf1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=54e7adad7a0aa8cb8ef8a849367c06c8804b53a4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is Llama3.1:8b.&lt;/p&gt; &lt;p&gt;I know, I am going to hear that as soon as you give anything locally hosted access to the internet, you forfeit privacy.&lt;/p&gt; &lt;p&gt;ChatGPT, Gemini, etc all store data about you. They're hosted by large tech companies and by that fact alone are inherently NOT private. Is there a way to host your own private LLM and simply give it the ability to search the internet for current data so that the knowledge cutoff is irrelevant?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nw5xvm/is_there_a_way_to_give_your_local_ollama_access/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nw5xvm/is_there_a_way_to_give_your_local_ollama_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nw5xvm/is_there_a_way_to_give_your_local_ollama_access/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T14:51:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nws533</id>
    <title>Is there a very small ollama model?</title>
    <updated>2025-10-03T07:01:09+00:00</updated>
    <author>
      <name>/u/Serious_Trade5646</name>
      <uri>https://old.reddit.com/user/Serious_Trade5646</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It must &lt;strong&gt;&lt;em&gt;NOT&lt;/em&gt;&lt;/strong&gt; exceed 500mb or 600mb and my goal is to get a very small 2 digit mb model &lt;em&gt;‚Ññ‚Ññmb&lt;/em&gt; not &lt;em&gt;‚Ññ‚Ññ‚Ññmb&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious_Trade5646"&gt; /u/Serious_Trade5646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nws533/is_there_a_very_small_ollama_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nws533/is_there_a_very_small_ollama_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nws533/is_there_a_very_small_ollama_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-03T07:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwgnce</id>
    <title>Unsure which ollama model to use? Here's a tool I built to help</title>
    <updated>2025-10-02T21:29:10+00:00</updated>
    <author>
      <name>/u/h3xzur7</name>
      <uri>https://old.reddit.com/user/h3xzur7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm fairly new to working with local LLMs, and like many, I wondered which model(s) I should use. To help answer that, I put together a tool that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Automates running multiple models on custom prompts&lt;/li&gt; &lt;li&gt;Outputs everything into a clean, easy-to-read HTML report&lt;/li&gt; &lt;li&gt;Lets you quickly compare results side by side&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;While there might be similar tools out there, I wanted something lightweight and straightforward for my own workflow. I figured I‚Äôd share in case others find it useful too.&lt;/p&gt; &lt;p&gt;I‚Äôd love any constructive feedback‚Äîwhether you think this fills a gap, how it could be improved, or if you know of alternatives I should check out.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Spectral-Knight-Ops/local-llm-evaluator"&gt;https://github.com/Spectral-Knight-Ops/local-llm-evaluator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/h3xzur7"&gt; /u/h3xzur7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nwgnce/unsure_which_ollama_model_to_use_heres_a_tool_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nwgnce/unsure_which_ollama_model_to_use_heres_a_tool_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nwgnce/unsure_which_ollama_model_to_use_heres_a_tool_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T21:29:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwdnea</id>
    <title>I visualized embeddings walking across the latent space as you type! :)</title>
    <updated>2025-10-02T19:37:46+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nwdnea/i_visualized_embeddings_walking_across_the_latent/"&gt; &lt;img alt="I visualized embeddings walking across the latent space as you type! :)" src="https://external-preview.redd.it/NWphdDhwNTY0cnNmMcfpx6_IdDgYBGvf-fwH7xFuI_ot2ErqijE3fUPasYhL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88e9c777c7ada4a09dec47d31a332256767a3b39" title="I visualized embeddings walking across the latent space as you type! :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1axflp564rsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nwdnea/i_visualized_embeddings_walking_across_the_latent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nwdnea/i_visualized_embeddings_walking_across_the_latent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T19:37:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwxx0g</id>
    <title>AI- Invoice/ Bill Parser (Ocr &amp; DocAI proj)</title>
    <updated>2025-10-03T12:35:39+00:00</updated>
    <author>
      <name>/u/Putrid-Use-4955</name>
      <uri>https://old.reddit.com/user/Putrid-Use-4955</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good Evening Everyone!&lt;/p&gt; &lt;p&gt;Has anyone worked on OCR / Invoice/ bill parser project? I needed advice.&lt;/p&gt; &lt;p&gt;I have got a project where I have to extract data from the uploaded bill whether it's png or pdf to json format. It should not be Closed AI api calling. I am working on some but no break through... Can ollama be helpful here ?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Putrid-Use-4955"&gt; /u/Putrid-Use-4955 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nwxx0g/ai_invoice_bill_parser_ocr_docai_proj/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nwxx0g/ai_invoice_bill_parser_ocr_docai_proj/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nwxx0g/ai_invoice_bill_parser_ocr_docai_proj/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-03T12:35:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwv30h</id>
    <title>what is a best mobile client?</title>
    <updated>2025-10-03T10:10:00+00:00</updated>
    <author>
      <name>/u/Steus_au</name>
      <uri>https://old.reddit.com/user/Steus_au</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying the ollama cloud and wondering what would be the best client from iOS (iphone/ipad) to use? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Steus_au"&gt; /u/Steus_au &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nwv30h/what_is_a_best_mobile_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nwv30h/what_is_a_best_mobile_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nwv30h/what_is_a_best_mobile_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-03T10:10:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx812m</id>
    <title>Would it make financial and logistical sense to run an instance of Ollama in the cloud until one can afford reasonable hardware for a decent LLM model?</title>
    <updated>2025-10-03T19:02:45+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this will depend on use-case. But right now I'm just talking experimentation with Open WebUI, N8N, and maybe some eventual Home Assistant experimentation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nx812m/would_it_make_financial_and_logistical_sense_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nx812m/would_it_make_financial_and_logistical_sense_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nx812m/would_it_make_financial_and_logistical_sense_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-03T19:02:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwsw9h</id>
    <title>Uncensored ollama llms</title>
    <updated>2025-10-03T07:50:35+00:00</updated>
    <author>
      <name>/u/realsplrk</name>
      <uri>https://old.reddit.com/user/realsplrk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anybody know of any half decent completely uncensored and unrestricted llms available on ollama, rest assured I am not a terrorist I just want a model that I can put my own guidelines into &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realsplrk"&gt; /u/realsplrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nwsw9h/uncensored_ollama_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nwsw9h/uncensored_ollama_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nwsw9h/uncensored_ollama_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-03T07:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxo3ql</id>
    <title>Best local model for open code?</title>
    <updated>2025-10-04T08:00:53+00:00</updated>
    <author>
      <name>/u/LastCulture3768</name>
      <uri>https://old.reddit.com/user/LastCulture3768</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LastCulture3768"&gt; /u/LastCulture3768 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nxo3ao/best_local_model_for_open_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxo3ql/best_local_model_for_open_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nxo3ql/best_local_model_for_open_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-04T08:00:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxp1db</id>
    <title>Service manual llm</title>
    <updated>2025-10-04T09:00:43+00:00</updated>
    <author>
      <name>/u/MyNameIsFifty</name>
      <uri>https://old.reddit.com/user/MyNameIsFifty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, me and friends are servicing Japanese cars in spare time and we have bunch of pdf service manuals (around 3000 pages each). I setteed up ollama and anythingllm on Linux server. We currently have GTX 1080 will upgrade on some 12gb rtx soon. What current models would you recommend for llm and for embedding with what settings. Purpose of this is to help us find answers to technical questions from the documents. Citation with reference would be the best answers. Thanks in advance for any answers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MyNameIsFifty"&gt; /u/MyNameIsFifty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxp1db/service_manual_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxp1db/service_manual_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nxp1db/service_manual_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-04T09:00:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxlfor</id>
    <title>model recommendation for coding/networking/linux questions</title>
    <updated>2025-10-04T05:18:50+00:00</updated>
    <author>
      <name>/u/Dear-Resident-6488</name>
      <uri>https://old.reddit.com/user/Dear-Resident-6488</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im interested in trying ollama to add inside my terminal multiplexer workflow but i got overwhelmed when i saw how many models were availiable. right now im using claude in the browser mainly and its really good. if there is a model atleast somewhat similar thats avaliable locally that would be awesome.&lt;/p&gt; &lt;p&gt;16gb ram ~200gb free storage&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Resident-6488"&gt; /u/Dear-Resident-6488 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxlfor/model_recommendation_for_codingnetworkinglinux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxlfor/model_recommendation_for_codingnetworkinglinux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nxlfor/model_recommendation_for_codingnetworkinglinux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-04T05:18:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxptsq</id>
    <title>Modelfile. Do I need these tags PER prompt?</title>
    <updated>2025-10-04T09:51:00+00:00</updated>
    <author>
      <name>/u/peyton_montana</name>
      <uri>https://old.reddit.com/user/peyton_montana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm confused, been on ollama docs and github for a while.&lt;/p&gt; &lt;p&gt;If I'm not going to run the modelfile as a GGUF, then do I need the following tags per prompt, or only one time?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;&amp;quot;&amp;quot;[INST] {{ .System }} {{ .Prompt }} [/INST]‚Äù&amp;quot;‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For example: if I have 5 different prompts in the modelfile, do I need that ^ code parsed 5 times, aka for each individual prompt?&lt;/p&gt; &lt;p&gt;I think I'm confused because the examples I've looked are mostly Llama and/or only contain a single prompt.&lt;/p&gt; &lt;p&gt;Sidenote: had Openai and Grok run sample modelfiles based on Ollama documentation (&lt;a href="https://ollama.readthedocs.io/en/modelfile/"&gt;https://ollama.readthedocs.io/en/modelfile/&lt;/a&gt;) and they both included the above code - per prompt. But, I think they were somehow assuming I was going to convert the txt to GGUF. So, yeah, confused.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peyton_montana"&gt; /u/peyton_montana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxptsq/modelfile_do_i_need_these_tags_per_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxptsq/modelfile_do_i_need_these_tags_per_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nxptsq/modelfile_do_i_need_these_tags_per_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-04T09:51:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxqaiv</id>
    <title>[Tool Release] ollama_server_manager: A Simple Web UI to Manage Models Across Multiple Local Ollama Servers</title>
    <updated>2025-10-04T10:19:49+00:00</updated>
    <author>
      <name>/u/GennadiiM</name>
      <uri>https://old.reddit.com/user/GennadiiM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was struggling to keep track of models across my &lt;strong&gt;three local Ollama servers&lt;/strong&gt; using only the command line. It got tedious!&lt;/p&gt; &lt;p&gt;To solve this, I created &lt;code&gt;ollama_server_manager&lt;/code&gt;- a simple tool that provides a &lt;strong&gt;web-based dashboard&lt;/strong&gt; to overview which models are present on which server.&lt;/p&gt; &lt;p&gt;Since I only use this on my &lt;strong&gt;private, trusted network&lt;/strong&gt;, I kept it intentionally &lt;strong&gt;simple with no authentication&lt;/strong&gt; required.&lt;/p&gt; &lt;p&gt;Hope others find this useful for managing their local setups!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Project Link:&lt;/strong&gt; &lt;a href="https://github.com/GhennadiiMir/ollama_server_manager"&gt;&lt;code&gt;https://github.com/GhennadiiMir/ollama_server_manager&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GennadiiM"&gt; /u/GennadiiM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxqaiv/tool_release_ollama_server_manager_a_simple_web/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxqaiv/tool_release_ollama_server_manager_a_simple_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nxqaiv/tool_release_ollama_server_manager_a_simple_web/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-04T10:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxms96</id>
    <title>Pardus AI: Open source AI Assistant thanks for the help with Ollama</title>
    <updated>2025-10-04T06:38:42+00:00</updated>
    <author>
      <name>/u/jasonhon2013</name>
      <uri>https://old.reddit.com/user/jasonhon2013</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nxms96/pardus_ai_open_source_ai_assistant_thanks_for_the/"&gt; &lt;img alt="Pardus AI: Open source AI Assistant thanks for the help with Ollama" src="https://external-preview.redd.it/Ake-kZcpHuhp7eDMuefYBJn8h_Eek4tI7hJD6Fg5Ly0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cababb145e25a96f1e5dfbe2653ec717bfc7a70" title="Pardus AI: Open source AI Assistant thanks for the help with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys. I always love open source. Our team decided to open source the Pardus AI Assistant &lt;a href="https://github.com/PardusAI/PardusAI"&gt;https://github.com/PardusAI/PardusAI&lt;/a&gt;, which basically is an AI assistant that memorizes what you have done, and you can ask it about your personal information, like what you have to do later, or ask it about the information you have just visited. The underlying relies on Ollama to do the embedding about you; actually, you can change everything locally, not even needing to use OpenRouter! Please give us a little star :) (begging star right now, loll) Thanks to the Ollama community, and I always love this community so much!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1nxms96/video/whyexd2ai1tf1/player"&gt;https://reddit.com/link/1nxms96/video/whyexd2ai1tf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonhon2013"&gt; /u/jasonhon2013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxms96/pardus_ai_open_source_ai_assistant_thanks_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nxms96/pardus_ai_open_source_ai_assistant_thanks_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nxms96/pardus_ai_open_source_ai_assistant_thanks_for_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-04T06:38:42+00:00</published>
  </entry>
</feed>
