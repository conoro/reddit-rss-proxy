<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-09T18:08:31+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pfo6zh</id>
    <title>Vllama: CLI based framework to run vision models in local or remote gpus(inspired from Ollama)</title>
    <updated>2025-12-06T12:30:17+00:00</updated>
    <author>
      <name>/u/Weekly_Layer_9315</name>
      <uri>https://old.reddit.com/user/Weekly_Layer_9315</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, this is my first post. I have built a simple CLI tool, which can help all to run the llms, vision models like image and video gen, models in the local system and if the system doesn't have the gpu or sufficient ram, they can also run it using kaggle's gpu(which is 30 hrs free for a week).&lt;/p&gt; &lt;p&gt;This is inspired from Ollama, which made downloading llms easy and interacting with it much easy, so I thought of why can't this be made for vision models, so I tried this first on my system, basic image generation is working but not that good, then I thought, why can't we use the Kaggle's GPU to generate videos and images and that can happen directly from the terminal with a single step, so that everyone can use this, so I built this VLLAMA.&lt;/p&gt; &lt;p&gt;In this, currently there are many features, like image, video generation in local and kaggles gpu session; download llms and make it run and also interact with it from anywhere (inspired by ollama) also improved it further by creating a vs code extension VLLAMA, using which you can chat directly from the vs code's chat section, users can chat with the local running llm with just adding &amp;quot;@vllama&amp;quot; at the start of the message and this doesn't use any usage cost and can be used as much as anyone wants, you can check this out at in the vscode extensions.&lt;/p&gt; &lt;p&gt;I want to implement this further so that the companies or anyone with gpu access can download the best llms for their usage and initialize it in their gpu servers, and can directly interact with it from the vscode's chat section and also in further versions, I am planning to implement agentic features so that users can use the local llm to use for code editing, in line suggestions, so that they don't have to pay for premiums and many more.&lt;/p&gt; &lt;p&gt;Currently it also has simple Text-to-Speech, and Speech-to-Text, which I am planning to include in the further versions, using open source audio models and also in further, implement 3D generation models, so that everyone can leverage the use of the open models directly from their terminal, and making the complex process of the using open models easy with just a single command in the terminal.&lt;/p&gt; &lt;p&gt;I have also implemented simple functionalities which can help, like listing the downloaded models and their sizes. Other things available are, basic dataset preprocessing, and training ML models directly with just two commands by just providing it the dataset. This is a basic implementation and want to further improve this so that users with just a dataset can clean and pre-process the data, train the models in their local or using the kaggle's or any free gpu providing services or their own gpus or cloud provided gpus, and can directly deploy the models and can use it any cases.&lt;/p&gt; &lt;p&gt;Currently this are the things it is doing and I want to improve such that everyone can use this for any case of the AI and leveraging the use of open models.&lt;/p&gt; &lt;p&gt;Please checkout the work at: &lt;a href="https://github.com/ManvithGopu13/Vllama"&gt;https://github.com/ManvithGopu13/Vllama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Published version at: &lt;a href="https://pypi.org/project/vllama/"&gt;https://pypi.org/project/vllama/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also the extension: &lt;a href="https://marketplace.visualstudio.com/items?itemName=ManvithGopu.vllama"&gt;https://marketplace.visualstudio.com/items?itemName=ManvithGopu.vllama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would appreciate your time for reading and thankful for everyone who want to contribute and spread a word of it.&lt;/p&gt; &lt;p&gt;Please leave your requests for improvements and any suggestions, ideas, and even roasts or anything in the comments or in the issues, this is well taken and appreciated. Thanks in advance. If you find the project useful, kindly contribute and can star it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weekly_Layer_9315"&gt; /u/Weekly_Layer_9315 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T12:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg213d</id>
    <title>Usable models and Performance of RTX 2000 Ada 16GB or RTX 4000 20GB?</title>
    <updated>2025-12-06T22:34:06+00:00</updated>
    <author>
      <name>/u/Temporary_Sir</name>
      <uri>https://old.reddit.com/user/Temporary_Sir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm considering picking up an RTX2000 or RTX Pro 4000 card to add to a server. (Ideally directly powered via pci)&lt;/p&gt; &lt;p&gt;Any insights on what performance would be for general usage as well as a bit of coding? &lt;/p&gt; &lt;p&gt;What models would be recommended? Would those even be useful?&lt;/p&gt; &lt;p&gt;Looking forward to your replies&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Sir"&gt; /u/Temporary_Sir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg213d/usable_models_and_performance_of_rtx_2000_ada/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg213d/usable_models_and_performance_of_rtx_2000_ada/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pg213d/usable_models_and_performance_of_rtx_2000_ada/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T22:34:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfiedc</id>
    <title>If it weren't for the Chinese we wouldn't have local AI</title>
    <updated>2025-12-06T06:28:03+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GRACIAS A ELON MUSK!! ESTOY SEGURO!!! Elon el mas trabajador e inteligente del planeta , lo quisieron fastidiar y ahora OpenAi corre riesgo de ir a la quiebra por traicionarlo...todos lo que lo traicionaron por envidia y le ocultaron los resultados de la IA (google Y microsoft) Ahora veremos lo que agutanta OpenAI sin el dinero de musk...&lt;/p&gt; &lt;p&gt;Mientras GROK Y LOS CHINOS avanzan con las cuentas saneadas, con las empresas sin riesgo..bien apalancadas financiadas con seguridad , OPENAI esta temblando de miedo de ir a la quiebra aajjajajajajajaja&lt;/p&gt; &lt;p&gt;Todos sabemos el desastre que pas√≥ cuando un ingeniero chino rob√≥ tecnolog√≠a de Open AI (O eso nos han contado) con todo el l√≠o que caus√≥ cuando Deepseek sac√≥ su modelo de IA al mundo Opensource,&lt;/p&gt; &lt;p&gt;Todos sabemos que Open AI se neg√≥ a sacar lo mejor de su tecnolog√≠a...luego cuando MOE technology sali√≥ con Qwen e hizo que la bolsa americana cayera por trillones de d√≥lares d√°ndose cuenta que los chips Nvidia quiz√°s no fueran necesarios para la IA.&lt;/p&gt; &lt;p&gt;Sacudi√≥ los cimientos de los mercados de valores americanos. No sabemos exactamente qu√© pas√≥... si Elon Musk tuvo algo que ver con todo esto al irse de la empresa y se aprovecharon de su financiaci√≥n al principio... Quiz√°s le ocultaron a Elon Musk los verdaderos resultados que estaba dando la IA para que siguiera financi√°ndolos y √©l se veng√≥ cuando vio que no hab√≠a resultados cuando se fue de Open AI y justo despu√©s de que Elon Musk se fuera, Open AI dio el gran salto.&lt;/p&gt; &lt;p&gt;Fue demasiada coincidencia...Quiz√°s quien realmente controlaba Open AI (Microsoft), ya que Elon Musk estaba muy ocupado con sus problemas en sus empresas, aprovecharon la oportunidad para apostar y beneficiarse ya que ahora son el accionista mayoritario...ya es cosa tuya saber si Elon Musk estaba sobornando a ingenieros para que le dieran la tecnolog√≠a MOE y como se negaron hizo un pacto con ellos para d√°rsela a los chinos...y por ah√≠ openai mientras sacaban Grok al mercado...nunca sabremos que pas√≥ en esa telenovela...toda una historia de conspiraciones e hip√≥tesis extra√±as...&lt;/p&gt; &lt;p&gt;Lo extra√±o es que Qwen parece haber adquirido la tecnolog√≠a que todos creen que est√° en los modelos de Open AI...y nunca lo sabremos porque los modelos son cerrados y no sabemos qu√© ingenier√≠a hay realmente detr√°s. Todo esto pensando... y si no hubiera pasado toda esta historia de robo de tecnolog√≠a... conspiraciones... etc... ca√≠da de la bolsa por el miedo de los inversores a que la burbuja de la IA estallara... fin... si todo eso no hubiera pasado...&lt;/p&gt; &lt;p&gt;Yo creo que la comunidad Opensource no estar√≠a disfrutando de los modelos MOE que tenemos hoy en d√≠a... ya que eso conllev√≥ a una feroz competencia y a la salida de modelos para perjudicarse unos a otros... lo que tenemos que ver es que si no hubiera pasado lo de Open AI... creo que no hubieran sacado el GPT-OSS y tambi√©n creo que nunca volver√°n a sacar un modelo similar...&lt;/p&gt; &lt;p&gt;Estas empresas son muy avariciosas y quieren todo para ellas y si no fuera por la presi√≥n de los chinos, la comunidad Opensource tendr√≠a muchos menos modelos capaces y tenemos que estar agradecidos de que QWEN Y DEEPSEEK y otros metieran presi√≥n sacando sus modelos...porque los occidentales...si por ellos fuera...se quedar√≠an todo para ellos como hace Google con su buscador...facebook o Amazon con sus algoritmos o Apple Con sus tecnolog√≠as...&lt;/p&gt; &lt;p&gt;Normalmente los chinos siempre han sido muy cerrados...pero la competencia por ver qui√©n es ahora la superpotencia que domina el mundo ha tra√≠do esos golpes que han sido maravillosos para la comunidad Opensource...&lt;/p&gt; &lt;p&gt;El miedo que tengo es que no se repita algo as√≠ y los modelos que saquen de ahora en adelante sean calderilla... las migajas... y lo realmente bueno y las grandes cosas se las queden ellos y lo protejan todo con patentes... y mucha seguridad... ya viste la ca√≠da que OpenAi tuvo que ser fundada sin √°nimo de lucro para el beneficio de la Humanidad...&lt;/p&gt; &lt;p&gt;Se ha convertido en un negocio que mueve billones con una deuda muy apalancada...y juegan funambulistas colgados de un hilo...C√≥mo termina todo esto...si todo explota...o siguen sacando modelos...porque los inversores no invierten en el pasado...invierten en el futuro...y si no hay novedades...los occidentales van a estar en problemas...algo que los chinos se financian ellos mismos y son cautos y buscaron la forma de hacer el negocio m√°s rentable requiriendo menos potencia de c√°lculo...&lt;/p&gt; &lt;p&gt;En resumen, los chinos y asi√°ticos siempre han destacado en muchas √°reas como la electr√≥nica... pero ahora China se est√° convirtiendo en una Superpotencia mayor que EEUU en muchas √°reas y creo que no debemos subestimarlos ni menospreciarlos... OPEN AI va a quebrar completamente creo... y los chinos van a ganar esta batalla... creo que ser√° el principio de la hegemon√≠a de China sobre el mundo...&lt;/p&gt; &lt;p&gt;Son gente trabajadora, eficiente y pac√≠fica...as√≠ que realmente apoyo sus modelos y creo que gracias a ellos esta competencia por ver qui√©n gana est√° beneficiando a la comunidad Opensource de una manera inimaginable que nunca pens√© que podr√≠amos tener modelos tan capaces y √∫tiles en nuestras manos para trabajar completamente offline...y gran parte es gracias a los chinos!!!!! Quiz√°s no es gracias a los chinos... sino... a ELON MUSK. Si me equivoco en mis suposiciones OPENAI estar√≠a evolucionando por detr√°s de todos los dem√°s competidores...ya que ser√≠an los mejores...aunque sin la financiaci√≥n de Elon Musk...todo cambia.&lt;/p&gt; &lt;p&gt;Pronto veremos que ha pasado aqu√≠ y que est√° pasando...porque Elon Musk es una persona brillante y valiente...y una buena persona...y las buenas personas se enfr√≠an...creo que Microsoft pens√≥ que su jugada contra Musk iba a salir bien...pero pronto veremos si OPEN AI quiebra...&lt;/p&gt; &lt;p&gt;Si mis suposiciones son ciertas, Elon Musk nos habr√≠a dado a todos la tecnolog√≠a MOE completamente gratis para que la IA china diera un golpe fuerte a OPENAI para frenarlos y entrar en el top 3 con GROK. Si eso fuera cierto deber√≠amos agradecerle... pero no se pudo hacer porque nadie sabr√≠a que todos los modelos de alta calidad que disfrutamos offline son gracias a √âL. Al hombre m√°s rico del planeta que nos dio esta tecnolog√≠a porque algunas personas quer√≠an enga√±arlo!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T06:28:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg5kui</id>
    <title>create model name without the ending :latest</title>
    <updated>2025-12-07T01:18:48+00:00</updated>
    <author>
      <name>/u/Frosty_Chest8025</name>
      <uri>https://old.reddit.com/user/Frosty_Chest8025</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI,&lt;br /&gt; how can i create a copy of gemma3-27b without it having a name gemma3:latest. I need it to be named as gemma3 in the ollama ps list.&lt;/p&gt; &lt;p&gt;ollama create gemma3 -f ollamaproduction&lt;br /&gt; creates model named gemma3:latest. &lt;/p&gt; &lt;p&gt;litellm needs that name without :latest. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frosty_Chest8025"&gt; /u/Frosty_Chest8025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg5kui/create_model_name_without_the_ending_latest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg5kui/create_model_name_without_the_ending_latest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pg5kui/create_model_name_without_the_ending_latest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T01:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgdlw6</id>
    <title>n8n not connecting to Ollama - Locally Hosted</title>
    <updated>2025-12-07T08:43:54+00:00</updated>
    <author>
      <name>/u/zohan_796</name>
      <uri>https://old.reddit.com/user/zohan_796</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I've been using NetworkChuck's videos to get locally hosted AI models and n8n through OpenWebUI.&lt;/p&gt; &lt;p&gt;However, I'm currently having issues with getting my Ollama account on n8n to connect to Ollama. Both are locally hosted using OpenWebUI as per Chuck's videos. I've got the Base URL as &lt;a href="http://localhost:11434"&gt;http://localhost:11434&lt;/a&gt;, which doesn't seem to connect. What do I need to do to allow n8n to link to Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zohan_796"&gt; /u/zohan_796 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgdlw6/n8n_not_connecting_to_ollama_locally_hosted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgdlw6/n8n_not_connecting_to_ollama_locally_hosted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgdlw6/n8n_not_connecting_to_ollama_locally_hosted/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T08:43:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgnjbo</id>
    <title>Aquif 3.5 Max 1205 (42B-A3B)</title>
    <updated>2025-12-07T17:08:31+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pgnjbo/aquif_35_max_1205_42ba3b/"&gt; &lt;img alt="Aquif 3.5 Max 1205 (42B-A3B)" src="https://b.thumbs.redditmedia.com/NEG8P3hqBPVNg2ugj-hAZZUETF03fHVZR0b3FQKZD7g.jpg" title="Aquif 3.5 Max 1205 (42B-A3B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pgnj1q/aquif_35_max_1205_42ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgnjbo/aquif_35_max_1205_42ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgnjbo/aquif_35_max_1205_42ba3b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T17:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph2sem</id>
    <title>Ollama is the next big thing to slice bread.</title>
    <updated>2025-12-08T04:07:38+00:00</updated>
    <author>
      <name>/u/newbietofx</name>
      <uri>https://old.reddit.com/user/newbietofx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing and setting up my own ui for pulling response from the ollama api server and fronting it. The docker approach is amazing. How do I put claude code prompt into tinyllama? It still spouts nonsense. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newbietofx"&gt; /u/newbietofx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ph2sem/ollama_is_the_next_big_thing_to_slice_bread/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ph2sem/ollama_is_the_next_big_thing_to_slice_bread/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ph2sem/ollama_is_the_next_big_thing_to_slice_bread/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T04:07:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgmbna</id>
    <title>Need opinion/help on my Memory System for LLM</title>
    <updated>2025-12-07T16:19:38+00:00</updated>
    <author>
      <name>/u/I_DiMooo</name>
      <uri>https://old.reddit.com/user/I_DiMooo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! I've been slowly learning and developing a LLM based on the character &lt;strong&gt;Cyn&lt;/strong&gt; from the series &amp;quot;Murder Drones&amp;quot;. My goal is to bring that silly robot to life someday but right now I'm developing her software controlled by an LLM.&lt;/p&gt; &lt;p&gt;I'm currently trying to figure out the &lt;em&gt;(hopefully)&lt;/em&gt; ideal &lt;strong&gt;memory system&lt;/strong&gt; for her. I've been developing this whole project with the help from ChatGPT, we've been brainstorming and we landed on an idea but I want to get some experienced peoples opinions before implementing it.&lt;/p&gt; &lt;p&gt;Cyn currently receives something I call &lt;strong&gt;&amp;quot;State Calls&amp;quot;&lt;/strong&gt; containing various world data and she responds with an array of &lt;strong&gt;&amp;quot;Executable Functions&amp;quot;&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example: {&amp;quot;finalized_speech&amp;quot;: &amp;quot;hi cyn&amp;quot;, &amp;quot;battery&amp;quot;: 80} ---&amp;gt; [&amp;quot;name&amp;quot;: &amp;quot;speak&amp;quot;, &amp;quot;params&amp;quot;: {&amp;quot;text&amp;quot;: &amp;quot;Hello&amp;quot;}]&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So the idea for the &lt;strong&gt;Memory System&lt;/strong&gt; is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;State Calls and Executable Functions are converted into easily readable information (finalized_speech would be: &amp;quot;User said smth&amp;quot;), this gets embedded and stored in recent_memories.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Every State Call will be analyzed and with embedding we will return some memories in &amp;quot;memory&amp;quot; variable within state call.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Every Minute/Hour/etc. a seperate summarizer model will make a minute/hour/etc. summary of the memories. These summary memories will simulate memory decays. We could store them as long-term memories after some point.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That is the base for the system. I am also thinking about making memory types and some memory storing system like cataloging the people she meets and other stuff like that, but right now I just want to land on a base that will make conversations with her have actual continuity, context and meaning.&lt;/p&gt; &lt;p&gt;I'd really appreciate the opinions and possible help with enhancing the idea for the system to make it as stable and lively as possible. If someone wants to help and needs some clarifications I'm happy to answer them!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_DiMooo"&gt; /u/I_DiMooo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgmbna/need_opinionhelp_on_my_memory_system_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgmbna/need_opinionhelp_on_my_memory_system_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgmbna/need_opinionhelp_on_my_memory_system_for_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T16:19:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgio6a</id>
    <title>GPU acceleration on Ryzen AI 9 HX 370</title>
    <updated>2025-12-07T13:43:34+00:00</updated>
    <author>
      <name>/u/GarauGarau</name>
      <uri>https://old.reddit.com/user/GarauGarau</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm running a machine with the new Ryzen AI 9 HX 370 and 96GB of RAM on Windows 11.&lt;/p&gt; &lt;p&gt;I have a large dataset to process (~320k rows) and I'm trying to determine if it's possible for Ollama to utilize the Radeon 890M iGPU.&lt;/p&gt; &lt;p&gt;Current Status:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: qwen2.5:7b&lt;/li&gt; &lt;li&gt;Setup: Python script sending requests to Ollama with 2 parallel workers.&lt;/li&gt; &lt;li&gt;Performance: I am averaging 8.68s/it (seconds per iteration/row).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been trying to get it to work, but I'm curious if anyone else with this specific chip has actually succeeded in enabling hardware acceleration on Windows, or if we are currently limited to CPU-only inference due to driver/software maturity issues.&lt;/p&gt; &lt;p&gt;I attempted to force GPU usage via PowerShell environment variables using Gemini's advice, but the logs always show &lt;code&gt;inference compute id=cpu&lt;/code&gt; and &lt;code&gt;entering low vram mode&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I have tried the following configurations (restarting the server each time):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Standard Parallelism: $env:OLLAMA_NUM_PARALLEL = &amp;quot;4&amp;quot;&lt;/li&gt; &lt;li&gt;Forcing Vulkan: $env:OLLAMA_VULKAN = &amp;quot;1&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you got it working, determine if it's possible to get Ollama to utilize the Radeon 890M iGPU at this time with stable or preview drivers?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GarauGarau"&gt; /u/GarauGarau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgio6a/gpu_acceleration_on_ryzen_ai_9_hx_370/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgio6a/gpu_acceleration_on_ryzen_ai_9_hx_370/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgio6a/gpu_acceleration_on_ryzen_ai_9_hx_370/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T13:43:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgl21p</id>
    <title>DevCrew agent swarm for accelerating your software development</title>
    <updated>2025-12-07T15:28:24+00:00</updated>
    <author>
      <name>/u/The_Research_Ninja</name>
      <uri>https://old.reddit.com/user/The_Research_Ninja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Fam. A new version of DevCrew_s1 with 20 agents is available at &lt;a href="https://github.com/GSA-TTS/devCrew_s1"&gt;https://github.com/GSA-TTS/devCrew_s1&lt;/a&gt; . Key agents are: BluePrint writer, ADR/ASR writers, Backend Engineer, Frontend Engineer, Code Reviewer, QA tester, Security Auditor, System Architect, UX/UI designer. DevCrew allows your organization to at least bootstrap any new full-stack software project with design documents, codes, tests, and more. &lt;/p&gt; &lt;p&gt;Imagine giving DevCrew one good software program design document in plain language and the crew gives you back a well documented executable full-stack software program. üöÄ DevCrew_s1 means &amp;quot;Specification 1 of DevCrew&amp;quot; where workflows are rigidly structured while &amp;quot;Specification 2&amp;quot; workflows are more flexible. You may implement and deploy Specification 1 with Claude Code, Amazon Strands, or Crew AI. It would be lovely to see some of us implement DevCrew_s1 locally with Ollama. &lt;/p&gt; &lt;p&gt;My upcoming book about production-grade AI Agent Systems will give you more practical guidance on how to build your own production-grade AI agent teams. I'm also seeking reviewers for the beta version of the book. Any experts from Ollama, nVidia, AWS, etc are welcomed - please DM me for more details. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_Research_Ninja"&gt; /u/The_Research_Ninja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgl21p/devcrew_agent_swarm_for_accelerating_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgl21p/devcrew_agent_swarm_for_accelerating_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgl21p/devcrew_agent_swarm_for_accelerating_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T15:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgxojv</id>
    <title>How to create a local developer only agent</title>
    <updated>2025-12-08T00:00:45+00:00</updated>
    <author>
      <name>/u/xtremeLinux</name>
      <uri>https://old.reddit.com/user/xtremeLinux</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently I am working on 4 projects from scratch. Up to now I have avoided AI like the plague for these projects but would want to use it now since several great ideas that I have had over the years are now possible with the technological changes in php, python and web servers.&lt;/p&gt; &lt;p&gt;My current scenario is that I want to (and I do not know if this is possible), show the ful code to the ai and explain to it what it does. Then explain the things I want to add to it, but the flow should be in such a way that the updates the AI gives me, it should apply them itself to the code, save it, test ot and do a live feeding of the output of it to learn about any mistakes it made, then circle back to code the new interation of it.&lt;/p&gt; &lt;p&gt;This cycle would repeat until certain tasks are satisfied.&lt;/p&gt; &lt;p&gt;My current setup hardware for this is Ubuntu, 128gb ddr5, rtx 5090,a 14900k cpu and 8tb of nvme. Don't know if I need anything else.&lt;/p&gt; &lt;p&gt;1 am just throwing this idea out there to see, mainly, if AI would speed up the process. I do not trust it completely since even with gemini 3, grok 4.1 or latest openai they really suck at coding when things get complex.&lt;/p&gt; &lt;p&gt;So maybe limiting the area of expertise (again I am guessing here) to only coding and only to a specific amount of languages and areas of interest like web, sockets, webworkers, http protocol, etc could help in speeding up and producing better results.&lt;/p&gt; &lt;p&gt;Can someone guide me if this iterating process is possible where the AI has read/write access to a folder and sub folder, it has access to a local web page and can thereby process in a loop certain criteria to iterate over it until i t accomplish certain points.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xtremeLinux"&gt; /u/xtremeLinux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgxojv/how_to_create_a_local_developer_only_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgxojv/how_to_create_a_local_developer_only_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgxojv/how_to_create_a_local_developer_only_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T00:00:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1phmg5z</id>
    <title>Either fix the pull, or add the jinja.... GGUF Struggles</title>
    <updated>2025-12-08T19:55:32+00:00</updated>
    <author>
      <name>/u/DriftTony</name>
      <uri>https://old.reddit.com/user/DriftTony</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok, currently I have a crappy internet connection, but when I use wget, everything works quite well, I do have the occasional restart, but it starts right from where I left off....&lt;/p&gt; &lt;p&gt;Now I thought to just download this GGUF file and create a ModelFile and use it like that, but because it is a VL model I thought I take the safe/certain route and use the Ollama Pull Method, because I could not for the life of me figure out how to convert the *.jinja file into the modelfile, nor could I find any documentation (nor could the yelled-at AI) for what Ollama was actually expecting and what from or even if any of the jinja-file is in the GGUF I can download from HF. And does the Model-file also need the link to the MMPROJ?&lt;/p&gt; &lt;p&gt;Then this horror just scoups up the little bandwidth I have (I do remember like a year ago, the Ollama Pull was horrible... but still??)&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 6% ‚ñï‚ñà ‚ñè 1.3 GB/ 21 GB&lt;/p&gt; &lt;p&gt;Error: max retries exceeded: EOF&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 15% ‚ñï‚ñà‚ñà ‚ñè 3.4 GB/ 21 GB&lt;/p&gt; &lt;p&gt;Error: max retries exceeded: EOF&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 36% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 7.9 GB/ 21 GB 649 KB/s 5h55m&lt;/p&gt; &lt;p&gt;Error: max retries exceeded: EOF&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 24% ‚ñï‚ñà‚ñà‚ñà‚ñà ‚ñè 5.2 GB/ 21 GB&lt;/p&gt; &lt;p&gt;Error: max retries exceeded: EOF&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 26% ‚ñï‚ñà‚ñà‚ñà‚ñà ‚ñè 5.8 GB/ 21 GB&lt;/p&gt; &lt;p&gt;Error: max retries exceeded: EOF&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 28% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 6.2 GB/ 21 GB&lt;/p&gt; &lt;p&gt;Error: max retries exceeded: EOF&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 33% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 7.1 GB/ 21 GB 971 KB/s 4h10m^Ccontext canceled&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 36% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 7.9 GB/ 21 GB 199 KB/s 19h19m^Ccontext canceled&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 39% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 8.4 GB/ 21 GB 988 KB/s 3h44m^Ccontext canceled&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 40% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 8.8 GB/ 21 GB 814 KB/s 4h24m^Ccontext canceled&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 41% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 8.9 GB/ 21 GB 781 KB/s 4h34m&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 48% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 10 GB/ 21 GB 934 KB/s 3h20m^Ccontext canceled&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 50% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 10 GB/ 21 GB 608 KB/s 4h56m^Ccontext canceled&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 53% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 11 GB/ 21 GB 504 KB/s 5h39m&lt;/p&gt; &lt;p&gt;Error: max retries exceeded: EOF&lt;/p&gt; &lt;p&gt;admin@Legion:/mnt/c/AI$ sudo docker exec ollama ollama pull &lt;a href="http://hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M"&gt;hf.co/mradermacher/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated-i1-GGUF:Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;pulling 52f2c7495a61: 30% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñè 6.5 GB/ 21 GB 539 KB/s 7h50m&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;As you can see the only resolution is to use regularly CTRL-C because then it at least resumes where it ends.... but really, is there anyone that could either implement the jinja-files or solve the pull process? This is becoming rather.... 'frustrated' as the AI would say....&lt;/p&gt; &lt;p&gt;And to be honest, I've had many GGUF models in the past (with using pull) that acted wrong or corrupt, so how 'solid' is the automatic pull and Model-file implementation from Ollama? Is there anyone that can shed a light? Did any of you encounter something similar, or perhaps I'm approaching this totally wrong?&lt;/p&gt; &lt;p&gt;Edit: OMG it just jumped from 53% back to 30%.... Sigh&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DriftTony"&gt; /u/DriftTony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phmg5z/either_fix_the_pull_or_add_the_jinja_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phmg5z/either_fix_the_pull_or_add_the_jinja_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phmg5z/either_fix_the_pull_or_add_the_jinja_gguf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T19:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1phi3ln</id>
    <title>Confused about ROCm hardware support</title>
    <updated>2025-12-08T17:13:55+00:00</updated>
    <author>
      <name>/u/NE556</name>
      <uri>https://old.reddit.com/user/NE556</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm looking at what hardware might be compatible to get some reasonably decent performance (looking to put it in my NAS to integrate with Home-Assistant, something like what this guy did &lt;a href="https://www.youtube.com/watch?v=j7yumDPWAEA"&gt;https://www.youtube.com/watch?v=j7yumDPWAEA&lt;/a&gt;), but I'm confused by the AMD documentation on what is compatible. I'm aiming for a 16GB GPU to allow me to run one of the better models, but given it's going to just be sitting there, I'm hoping one of the less powerful (and thus less power hungry) GPUs will work from a previous generation to save some money. &lt;a href="https://rocm.docs.amd.com/en/latest/compatibility/compatibility-matrix.html"&gt;https://rocm.docs.amd.com/en/latest/compatibility/compatibility-matrix.html&lt;/a&gt; seems to imply anything CDNA onwards or RDNA2 onwards is supported. But &lt;a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html"&gt;https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html&lt;/a&gt; starts with RX 7700 XT/RDNA3 as the oldest/lowest hardware that is supported. Anyone able to help me figure this out? I'm aiming to stick with AMD, since the Linux GPU driver support is much better these days in my experience. I've also looked into NPU hardware, but seen that Ollama NPU support is pretty early days, it seems, at the moment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NE556"&gt; /u/NE556 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phi3ln/confused_about_rocm_hardware_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phi3ln/confused_about_rocm_hardware_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phi3ln/confused_about_rocm_hardware_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T17:13:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1phf6qv</id>
    <title>DataKit: your all in browser data studio is open source now</title>
    <updated>2025-12-08T15:24:39+00:00</updated>
    <author>
      <name>/u/Sea-Assignment6371</name>
      <uri>https://old.reddit.com/user/Sea-Assignment6371</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1phf6qv/datakit_your_all_in_browser_data_studio_is_open/"&gt; &lt;img alt="DataKit: your all in browser data studio is open source now" src="https://external-preview.redd.it/0HAU6Wdhz0KvBSjuRcJDgbxcR0Y5tImb_WCXq9uJsF8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=529a415eb7ee57d542327e222db133faed945255" title="DataKit: your all in browser data studio is open source now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Assignment6371"&gt; /u/Sea-Assignment6371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0sm263x4iz5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phf6qv/datakit_your_all_in_browser_data_studio_is_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phf6qv/datakit_your_all_in_browser_data_studio_is_open/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T15:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1phbdte</id>
    <title>data privacy policy of ollama cloud models?</title>
    <updated>2025-12-08T12:40:21+00:00</updated>
    <author>
      <name>/u/EatTFM</name>
      <uri>https://old.reddit.com/user/EatTFM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;we are processing scans containing highly sensitive personal information. Where can I find the details about the data privacy policy when using ollama cloud models?&lt;/p&gt; &lt;p&gt;I have heard that there is something on the roadmap to encrypt data when using cloud models - where to find more info about that?&lt;/p&gt; &lt;p&gt;thank you&lt;/p&gt; &lt;p&gt;Update: Until now, we are processing these data just using the local ollama models. This is not clear from the OP&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EatTFM"&gt; /u/EatTFM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phbdte/data_privacy_policy_of_ollama_cloud_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phbdte/data_privacy_policy_of_ollama_cloud_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phbdte/data_privacy_policy_of_ollama_cloud_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T12:40:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1phbooy</id>
    <title>Ollama vision models can't see images via console</title>
    <updated>2025-12-08T12:55:22+00:00</updated>
    <author>
      <name>/u/jozzo402</name>
      <uri>https://old.reddit.com/user/jozzo402</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SOLVED: My ollama server is remote and I was using a path to a local image lol. FTPing the image to the ollama server, then providing it the path from there works fine!&lt;/p&gt; &lt;p&gt;Original post:&lt;/p&gt; &lt;p&gt;According to Ollama's post here, &lt;a href="https://ollama.com/blog/qwen3-vl"&gt;https://ollama.com/blog/qwen3-vl&lt;/a&gt;, you can provide image paths to vision models when prompting via commandline. The /help command also confirms this.&lt;/p&gt; &lt;p&gt;But when I try to do it, the LLM responses indicate that it doesn't actually see the image, and it just responds as though the image path is literal text, eg. qwen3-vl:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;What do you see in this image? /home/user/Pictures/Screenshot_20250628_223552.png Thinking... Okay, the user is asking what I see in an image located at /home/user/Pictures/Screenshot_20250628_223552.png. Let me break this down.&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;First, I need to check the path. The path starts with /home/user, which is a standard user directory on Linux or Unix systems. The directory name &amp;quot;Pictures&amp;quot; is where images are typically stored. The filename is &amp;quot;Screenshot_20250628_223552.png&amp;quot;. &lt;/p&gt; &lt;p&gt;Wait, the date in the filename is 2025-06-28, which is in the future. That's a bit odd because the current year is 2023. Maybe it's a typo, or maybe it's a future date. But regardless, it's a screenshot filename with the date and time.&lt;/p&gt; &lt;p&gt;The user is asking me to describe what's in this image. But I don't have the actual image. The path provided is just a location, not the content. So I can't see the image. I should inform them that I can't view the image and maybe ask if they want to describe it or use another method&lt;sup&gt;C&lt;/sup&gt; ```&lt;/p&gt; &lt;p&gt;the same for llama3.2-vision:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;what is this image /home/user/Pictures/Screenshot_20250628_223552.png Unfortunately, I'm a large language model, I don't have the ability to directly access or view external files, including images. However, I can try to help you identify what the image might be.&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;If you're asking about the image file &lt;code&gt;/home/user/Pictures/Screenshot_20250628_223552.png&lt;/code&gt;, here are a few possibilities: ... ```&lt;/p&gt; &lt;p&gt;I have tried restarting ollama, updating ollama, restarting the PC it runs on, reinstalling the models. None have helped.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jozzo402"&gt; /u/jozzo402 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phbooy/ollama_vision_models_cant_see_images_via_console/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phbooy/ollama_vision_models_cant_see_images_via_console/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phbooy/ollama_vision_models_cant_see_images_via_console/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T12:55:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pho3g7</id>
    <title>Ollama + OpenVINO</title>
    <updated>2025-12-08T20:57:07+00:00</updated>
    <author>
      <name>/u/G4rp</name>
      <uri>https://old.reddit.com/user/G4rp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a notebook with an Intel NPU, I would like to use Ollama with it. Seems there is an initial support of Ollama with OpenVINO as a backend not much information about.&lt;/p&gt; &lt;p&gt;Anyone has experience with them and docker?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/G4rp"&gt; /u/G4rp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pho3g7/ollama_openvino/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pho3g7/ollama_openvino/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pho3g7/ollama_openvino/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T20:57:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1phzbsa</id>
    <title>Is there any hope one day we could buy Radxa AICore AX-M1?</title>
    <updated>2025-12-09T05:26:08+00:00</updated>
    <author>
      <name>/u/theodiousolivetree</name>
      <uri>https://old.reddit.com/user/theodiousolivetree</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theodiousolivetree"&gt; /u/theodiousolivetree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/SBCs/comments/1phzbfz/is_there_any_hope_one_day_we_could_buy_radxa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phzbsa/is_there_any_hope_one_day_we_could_buy_radxa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phzbsa/is_there_any_hope_one_day_we_could_buy_radxa/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T05:26:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1phprx0</id>
    <title>Nanocoder 1.18.0 - Multi-step tool calls, debugging mode, and searchable model database</title>
    <updated>2025-12-08T22:01:50+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1phprx0/nanocoder_1180_multistep_tool_calls_debugging/"&gt; &lt;img alt="Nanocoder 1.18.0 - Multi-step tool calls, debugging mode, and searchable model database" src="https://external-preview.redd.it/6NB3vTkt_CTvZIVvpoaPWkTNJd0mSRDP_VXtKMbUZkI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=226a675fb4732b1d83b0bfb2fd7f29a4d888dec1" title="Nanocoder 1.18.0 - Multi-step tool calls, debugging mode, and searchable model database" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/nanocoder/comments/1phpris/nanocoder_1180_multistep_tool_calls_debugging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phprx0/nanocoder_1180_multistep_tool_calls_debugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phprx0/nanocoder_1180_multistep_tool_calls_debugging/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T22:01:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1phcz5j</id>
    <title>Newbie: How to "teach" ollama with 150MB PDF</title>
    <updated>2025-12-08T13:54:53+00:00</updated>
    <author>
      <name>/u/GabesVirtualWorld</name>
      <uri>https://old.reddit.com/user/GabesVirtualWorld</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want my local Ollama have the knowledge that is in a 150MB PDF and then ask it questions about that pdf. Am I right in trying to upload this? But I'm hitting the 20MB upload limit, is there a way to change that limit?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GabesVirtualWorld"&gt; /u/GabesVirtualWorld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phcz5j/newbie_how_to_teach_ollama_with_150mb_pdf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1phcz5j/newbie_how_to_teach_ollama_with_150mb_pdf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1phcz5j/newbie_how_to_teach_ollama_with_150mb_pdf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-08T13:54:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi6n2c</id>
    <title>Models that has the least collapse when ctx length grows. Especially using it with tools.</title>
    <updated>2025-12-09T12:52:55+00:00</updated>
    <author>
      <name>/u/Express_Quail_1493</name>
      <uri>https://old.reddit.com/user/Express_Quail_1493</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Quail_1493"&gt; /u/Express_Quail_1493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pi6hbx/models_that_has_the_least_collapse_when_ctx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi6n2c/models_that_has_the_least_collapse_when_ctx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pi6n2c/models_that_has_the_least_collapse_when_ctx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T12:52:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi6ww5</id>
    <title>META AI LLM llama3.2 TERMUX</title>
    <updated>2025-12-09T13:05:50+00:00</updated>
    <author>
      <name>/u/PlayOnAndroid</name>
      <uri>https://old.reddit.com/user/PlayOnAndroid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pi6ww5/meta_ai_llm_llama32_termux/"&gt; &lt;img alt="META AI LLM llama3.2 TERMUX" src="https://preview.redd.it/zmeur9p5zw5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c69f4efc2eddb61badb6a791be033627bb1d4f0" title="META AI LLM llama3.2 TERMUX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PlayOnAndroid"&gt; /u/PlayOnAndroid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zmeur9p5zw5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi6ww5/meta_ai_llm_llama32_termux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pi6ww5/meta_ai_llm_llama32_termux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T13:05:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi99uh</id>
    <title>Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found</title>
    <updated>2025-12-09T14:47:41+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pi99uh/which_small_model_is_best_for_finetuning_we/"&gt; &lt;img alt="Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found" src="https://preview.redd.it/kwjv946ey66g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f6c270a4751620f162cbf8017b5321a7a5b5017" title="Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; We fine-tuned 12 small models to find which ones are most tunable and perform best after fine-tuning. Surprise finding: Llama-3.2-1B showed the biggest improvement (most tunable), while Qwen3-4B delivered the best final performance - matching a 120B teacher on 7/8 tasks and outperforming by 19 points on the SQuAD 2.0 dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;12 models total - Qwen3 (8B, 4B, 1.7B, 0.6B), Llama (3.1-8B, 3.2-3B, 3.2-1B), SmolLM2 (1.7B, 135M), Gemma (1B, 270M), and Granite 8B.&lt;/p&gt; &lt;p&gt;Used GPT-OSS 120B as teacher to generate 10k synthetic training examples per task. Fine-tuned everything with identical settings: LoRA rank 64, 4 epochs, 5e-5 learning rate.&lt;/p&gt; &lt;p&gt;Tested on 8 benchmarks: classification tasks (TREC, Banking77, Ecommerce, Mental Health), document extraction, and QA (HotpotQA, Roman Empire, SQuAD 2.0).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding #1: Tunability (which models improve most)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The smallest models showed the biggest gains from fine-tuning. Llama-3.2-1B ranked #1 for tunability, followed by Llama-3.2-3B and Qwen3-0.6B.&lt;/p&gt; &lt;p&gt;This pattern makes sense - smaller models start weaker but have more room to grow. Fine-tuning closed the gap hard. The 8B models ranked lowest for tunability not because they're bad, but because they started strong and had less room to improve.&lt;/p&gt; &lt;p&gt;If you're stuck with small models due to hardware constraints, this is good news. Fine-tuning can make a 1B model competitive with much larger models on specific tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding #2: Best fine-tuned performance (can student match teacher?)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3-4B-Instruct-2507 came out on top for final performance. After fine-tuning, it matched or exceeded the 120B teacher on 7 out of 8 benchmarks.&lt;/p&gt; &lt;p&gt;Breakdown: TREC (+3 points), Docs (+2), Ecommerce (+3), HotpotQA (tied), Mental Health (+1), Roman Empire (+5). Only fell short on Banking77 by 3 points.&lt;/p&gt; &lt;p&gt;SQuAD 2.0 was wild - the 4B student scored 0.71 vs teacher's 0.52. That's a 19 point gap favoring the smaller model. A model 30x smaller outperforming the one that trained it.&lt;/p&gt; &lt;p&gt;Before fine-tuning, the 8B models dominated everything. After fine-tuning, model size mattered way less.&lt;/p&gt; &lt;p&gt;If you're running stuff on your own hardware, you can get frontier-level performance from a 4B model on a single consumer GPU. No expensive cloud instances. No API rate limits.&lt;/p&gt; &lt;p&gt;Let us know if there's a specific model you want benchmarked.&lt;/p&gt; &lt;p&gt;Full write-up: &lt;a href="https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning"&gt;https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kwjv946ey66g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi99uh/which_small_model_is_best_for_finetuning_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pi99uh/which_small_model_is_best_for_finetuning_we/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T14:47:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi5fr6</id>
    <title>Qwen3-Next here!</title>
    <updated>2025-12-09T11:48:29+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ollama.com/library/qwen3-next"&gt;https://ollama.com/library/qwen3-next&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen3-Next-80B-A3B is the first installment in the Qwen3-Next series and features the following key enhancements:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Hybrid Attention: Replaces standard attention with the combination of Gated DeltaNet and Gated Attention, enabling efficient context modeling for ultra-long context length. High-Sparsity Mixture-of-Experts (MoE): Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity. Stability Optimizations: Includes techniques such as zero-centered and weight-decayed layernorm, and other stabilizing enhancements for robust pre-training and post-training. Multi-Token Prediction (MTP): Boosts pretraining model performance and accelerates inference. &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;p&gt;requires ollama 0.13.2 &lt;a href="https://github.com/ollama/ollama/releases/tag/v0.13.2"&gt;https://github.com/ollama/ollama/releases/tag/v0.13.2&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Surprizy good for local model on my benchmark 50k tokens, read whole book &amp;quot;Alice in wonders&amp;quot; and ask all heroes Alice met&lt;/p&gt; &lt;ul&gt; &lt;li&gt;almost consistent inference speed regardless of context size&lt;/li&gt; &lt;li&gt;~40 t/s inference on w7900 48gb&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Upd: llama.cpp gives 40 t/s, ollama only 10 t/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi5fr6/qwen3next_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi5fr6/qwen3next_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pi5fr6/qwen3next_here/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T11:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi492e</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-12-09T10:37:50+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Notion Like Document Editing experience&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agentic chat&lt;/li&gt; &lt;li&gt;Note Management (Like Notion)&lt;/li&gt; &lt;li&gt;Multi Collaborative Chats.&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Installation (Self-Host)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Linux/macOS:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 \ -v surfsense-data:/data \ --name surfsense \ --restart unless-stopped \ ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Windows (PowerShell):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 ` -v surfsense-data:/data ` --name surfsense ` --restart unless-stopped ` ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi492e/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi492e/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pi492e/open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T10:37:50+00:00</published>
  </entry>
</feed>
