<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-12T17:37:53+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pi99uh</id>
    <title>Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found</title>
    <updated>2025-12-09T14:47:41+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pi99uh/which_small_model_is_best_for_finetuning_we/"&gt; &lt;img alt="Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found" src="https://preview.redd.it/kwjv946ey66g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f6c270a4751620f162cbf8017b5321a7a5b5017" title="Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; We fine-tuned 12 small models to find which ones are most tunable and perform best after fine-tuning. Surprise finding: Llama-3.2-1B showed the biggest improvement (most tunable), while Qwen3-4B delivered the best final performance - matching a 120B teacher on 7/8 tasks and outperforming by 19 points on the SQuAD 2.0 dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;12 models total - Qwen3 (8B, 4B, 1.7B, 0.6B), Llama (3.1-8B, 3.2-3B, 3.2-1B), SmolLM2 (1.7B, 135M), Gemma (1B, 270M), and Granite 8B.&lt;/p&gt; &lt;p&gt;Used GPT-OSS 120B as teacher to generate 10k synthetic training examples per task. Fine-tuned everything with identical settings: LoRA rank 64, 4 epochs, 5e-5 learning rate.&lt;/p&gt; &lt;p&gt;Tested on 8 benchmarks: classification tasks (TREC, Banking77, Ecommerce, Mental Health), document extraction, and QA (HotpotQA, Roman Empire, SQuAD 2.0).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding #1: Tunability (which models improve most)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The smallest models showed the biggest gains from fine-tuning. Llama-3.2-1B ranked #1 for tunability, followed by Llama-3.2-3B and Qwen3-0.6B.&lt;/p&gt; &lt;p&gt;This pattern makes sense - smaller models start weaker but have more room to grow. Fine-tuning closed the gap hard. The 8B models ranked lowest for tunability not because they're bad, but because they started strong and had less room to improve.&lt;/p&gt; &lt;p&gt;If you're stuck with small models due to hardware constraints, this is good news. Fine-tuning can make a 1B model competitive with much larger models on specific tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding #2: Best fine-tuned performance (can student match teacher?)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3-4B-Instruct-2507 came out on top for final performance. After fine-tuning, it matched or exceeded the 120B teacher on 7 out of 8 benchmarks.&lt;/p&gt; &lt;p&gt;Breakdown: TREC (+3 points), Docs (+2), Ecommerce (+3), HotpotQA (tied), Mental Health (+1), Roman Empire (+5). Only fell short on Banking77 by 3 points.&lt;/p&gt; &lt;p&gt;SQuAD 2.0 was wild - the 4B student scored 0.71 vs teacher's 0.52. That's a 19 point gap favoring the smaller model. A model 30x smaller outperforming the one that trained it.&lt;/p&gt; &lt;p&gt;Before fine-tuning, the 8B models dominated everything. After fine-tuning, model size mattered way less.&lt;/p&gt; &lt;p&gt;If you're running stuff on your own hardware, you can get frontier-level performance from a 4B model on a single consumer GPU. No expensive cloud instances. No API rate limits.&lt;/p&gt; &lt;p&gt;Let us know if there's a specific model you want benchmarked.&lt;/p&gt; &lt;p&gt;Full write-up: &lt;a href="https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning"&gt;https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kwjv946ey66g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi99uh/which_small_model_is_best_for_finetuning_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pi99uh/which_small_model_is_best_for_finetuning_we/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T14:47:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1piis2t</id>
    <title>Best LLM for writing text/summaries/tables under 30B</title>
    <updated>2025-12-09T20:46:40+00:00</updated>
    <author>
      <name>/u/tombino104</name>
      <uri>https://old.reddit.com/user/tombino104</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Could you recommend me an LLM that is able to make good quality texts, even summaries but also well-structured tables?&lt;/p&gt; &lt;p&gt;If it exists, but I doubt it, one of them allows you to create a sort of concept map even if not from a complete graphic point of view.&lt;/p&gt; &lt;p&gt;I‚Äôm looking for a preferably maximum 30B, I use LLMStudio, but I‚Äôm looking for Ollama models to use.&lt;/p&gt; &lt;p&gt;Thank you! üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tombino104"&gt; /u/tombino104 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1piis2t/best_llm_for_writing_textsummariestables_under_30b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1piis2t/best_llm_for_writing_textsummariestables_under_30b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1piis2t/best_llm_for_writing_textsummariestables_under_30b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T20:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi492e</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-12-09T10:37:50+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Notion Like Document Editing experience&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agentic chat&lt;/li&gt; &lt;li&gt;Note Management (Like Notion)&lt;/li&gt; &lt;li&gt;Multi Collaborative Chats.&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Installation (Self-Host)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Linux/macOS:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 \ -v surfsense-data:/data \ --name surfsense \ --restart unless-stopped \ ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Windows (PowerShell):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 ` -v surfsense-data:/data ` --name surfsense ` --restart unless-stopped ` ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi492e/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi492e/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pi492e/open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T10:37:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1piuw64</id>
    <title>Ollama remote client?</title>
    <updated>2025-12-10T05:52:31+00:00</updated>
    <author>
      <name>/u/answerencr</name>
      <uri>https://old.reddit.com/user/answerencr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm running ollama locally on my server in a VM and I'm accessing it via Windows app.&lt;/p&gt; &lt;p&gt;How can I access same ollama instance (so I can see previous chats and generate new ones that will be stored) remotely? On both PC and Android? thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/answerencr"&gt; /u/answerencr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1piuw64/ollama_remote_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1piuw64/ollama_remote_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1piuw64/ollama_remote_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-10T05:52:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi5fr6</id>
    <title>Qwen3-Next here!</title>
    <updated>2025-12-09T11:48:29+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ollama.com/library/qwen3-next"&gt;https://ollama.com/library/qwen3-next&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen3-Next-80B-A3B is the first installment in the Qwen3-Next series and features the following key enhancements:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Hybrid Attention: Replaces standard attention with the combination of Gated DeltaNet and Gated Attention, enabling efficient context modeling for ultra-long context length. High-Sparsity Mixture-of-Experts (MoE): Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity. Stability Optimizations: Includes techniques such as zero-centered and weight-decayed layernorm, and other stabilizing enhancements for robust pre-training and post-training. Multi-Token Prediction (MTP): Boosts pretraining model performance and accelerates inference. &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;p&gt;requires ollama 0.13.2 &lt;a href="https://github.com/ollama/ollama/releases/tag/v0.13.2"&gt;https://github.com/ollama/ollama/releases/tag/v0.13.2&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Surprizy good for local model on my benchmark 50k tokens, read whole book &amp;quot;Alice in wonders&amp;quot; and ask all heroes Alice met&lt;/p&gt; &lt;ul&gt; &lt;li&gt;almost consistent inference speed regardless of context size&lt;/li&gt; &lt;li&gt;~40 t/s inference on w7900 48gb&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Upd: llama.cpp gives 40 t/s, ollama only 10 t/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi5fr6/qwen3next_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pi5fr6/qwen3next_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pi5fr6/qwen3next_here/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T11:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pif88t</id>
    <title>OSS 120 GPT vs ChatGPT 5.1</title>
    <updated>2025-12-09T18:34:31+00:00</updated>
    <author>
      <name>/u/FX2021</name>
      <uri>https://old.reddit.com/user/FX2021</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In real world performance &amp;quot;intelligence&amp;quot; how close or how far apart is OSS 120 compared to GPT 5.1? in the field of STEM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FX2021"&gt; /u/FX2021 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pif88t/oss_120_gpt_vs_chatgpt_51/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pif88t/oss_120_gpt_vs_chatgpt_51/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pif88t/oss_120_gpt_vs_chatgpt_51/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-09T18:34:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj9og4</id>
    <title>Need a headless macOS Ollama binary for CI (CircleCI macOS M1/M2/M3 runners)</title>
    <updated>2025-12-10T18:04:30+00:00</updated>
    <author>
      <name>/u/Fabulous_Classroom22</name>
      <uri>https://old.reddit.com/user/Fabulous_Classroom22</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm integrating Ollama into an automated test framework.&lt;br /&gt; The Linux jobs work perfectly because the headless server runs fine inside Docker.&lt;/p&gt; &lt;p&gt;But for iOS automation we must use &lt;strong&gt;macOS CI runners&lt;/strong&gt; (CircleCI macOS M-series machines), and that‚Äôs where Ollama breaks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;curl -fsSL&lt;/code&gt; &lt;a href="https://ollama.com/install.sh"&gt;&lt;code&gt;https://ollama.com/install.sh&lt;/code&gt;&lt;/a&gt; &lt;code&gt;| sh&lt;/code&gt; ‚Üí exits with &lt;em&gt;‚ÄúThis script is intended to run on Linux only.‚Äù&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;brew install --cask ollama&lt;/code&gt; ‚Üí installs the &lt;strong&gt;GUI .app&lt;/strong&gt; ‚Üí tries to request macOS authorization ‚Üí hangs CI forever&lt;/li&gt; &lt;li&gt;No headless macOS CLI binary seems to exist that works in CI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;I need a pure macOS CLI/server binary (like the Linux one) that runs headless with no GUI, no dialogs, no user session.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Is this available?&lt;br /&gt; If not, is it planned?&lt;br /&gt; This is blocking CI pipelines for anyone running iOS automation + Ollama inside the same workflow.&lt;/p&gt; &lt;p&gt;Any official guidance or community workarounds would be appreciated. #help #dev-support #headless-server #macos&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Classroom22"&gt; /u/Fabulous_Classroom22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pj9og4/need_a_headless_macos_ollama_binary_for_ci/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pj9og4/need_a_headless_macos_ollama_binary_for_ci/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pj9og4/need_a_headless_macos_ollama_binary_for_ci/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-10T18:04:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pisiq2</id>
    <title>Ollama now supports the rnj-1 model</title>
    <updated>2025-12-10T03:47:03+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pisiq2/ollama_now_supports_the_rnj1_model/"&gt; &lt;img alt="Ollama now supports the rnj-1 model" src="https://preview.redd.it/kqymea6bta6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4211dbfa7f03d832d8370b4d36bca83dec365ed7" title="Ollama now supports the rnj-1 model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;rnj-1 is the best Open-Source 8B-Parameter LLM Built in the USA and it is optimized for code and STEM with capabilities on par with SOTA open-weight models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; These models require the pre-release version of Ollama &lt;strong&gt;v0.13.3&lt;/strong&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kqymea6bta6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pisiq2/ollama_now_supports_the_rnj1_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pisiq2/ollama_now_supports_the_rnj1_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-10T03:47:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pje3ij</id>
    <title>Best encoding model below 40B</title>
    <updated>2025-12-10T20:48:47+00:00</updated>
    <author>
      <name>/u/tombino104</name>
      <uri>https://old.reddit.com/user/tombino104</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tombino104"&gt; /u/tombino104 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pje2tb/best_coding_model_under_40b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pje3ij/best_encoding_model_below_40b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pje3ij/best_encoding_model_below_40b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-10T20:48:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjbf6w</id>
    <title>ClaraVerse</title>
    <updated>2025-12-10T19:07:30+00:00</updated>
    <author>
      <name>/u/Scary_Salamander_114</name>
      <uri>https://old.reddit.com/user/Scary_Salamander_114</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone using the local hosted ClaraVerse (currently in 0.3x) . How has your experience been. I have other local-hosted LLM set-ups, but I am really intrigued by ClaraVerse's focus on privacy. I know that it is a single-DEV project, so not expecting rapid upgrades. But..if you you have used it-what are your feelings about it's potential. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Scary_Salamander_114"&gt; /u/Scary_Salamander_114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pjbf6w/claraverse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pjbf6w/claraverse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pjbf6w/claraverse/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-10T19:07:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj29u1</id>
    <title>Letting a local Ollama model judge my AI agents and it‚Äôs surprisingly usable</title>
    <updated>2025-12-10T13:14:57+00:00</updated>
    <author>
      <name>/u/hidai25</name>
      <uri>https://old.reddit.com/user/hidai25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been hacking on a little testing framework for AI agents, and I just wired it up to &lt;strong&gt;Ollama&lt;/strong&gt; so you can use a &lt;em&gt;local&lt;/em&gt; model as the judge instead of always hitting cloud APIs.&lt;/p&gt; &lt;p&gt;Basic idea: you write test cases for your agent, the tool runs them, and a model checks ‚Äúdid this response look right / use the right tools?‚Äù. Until now I was only using OpenAI; now you can point it at whatever you‚Äôve pulled in Ollama.&lt;/p&gt; &lt;p&gt;Setup is pretty simple:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;brew install ollama # or curl install for Linux ollama serve ollama pull llama3.2 pip install evalview evalview run --judge-provider ollama --judge-model llama3.2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Why I bothered doing this: I was sick of burning API credits just to tweak prompts and tools. Local judge means I can iterate tests all day without caring about tokens, my test data never leaves the machine, and it still works offline. For serious / prod evals you can still swap back to cloud models if you want.&lt;/p&gt; &lt;p&gt;Example of a test (YAML):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;name: &amp;quot;Weather agent test&amp;quot; input: query: &amp;quot;What's the weather in NYC?&amp;quot; expected: tools: - get_weather thresholds: min_score: 80 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Repo is here if you want to poke at it:&lt;br /&gt; &lt;a href="https://github.com/hidai25/eval-view"&gt;https://github.com/hidai25/eval-view&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious what people here use as a &lt;em&gt;judge&lt;/em&gt; model in Ollama. I‚Äôve been playing with &lt;code&gt;llama3.2&lt;/code&gt;, but if you‚Äôve found something that works better for grading agent outputs, I‚Äôd love to hear about your setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hidai25"&gt; /u/hidai25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pj29u1/letting_a_local_ollama_model_judge_my_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pj29u1/letting_a_local_ollama_model_judge_my_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pj29u1/letting_a_local_ollama_model_judge_my_ai_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-10T13:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjbggq</id>
    <title>Ubuntu Linux, ollama service uses CPU instead of GPU "seemingly randomly"</title>
    <updated>2025-12-10T19:08:49+00:00</updated>
    <author>
      <name>/u/BloodyIron</name>
      <uri>https://old.reddit.com/user/BloodyIron</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm still teh newb to ollama so please don't hit me with too many trouts...&lt;/p&gt; &lt;p&gt;My workstation is pretty beefy, Ryzen 9600x (with on-die GPU naturally) and RX 9070 XT.&lt;/p&gt; &lt;p&gt;I'm on Ubuntu Desktop, 25.04. Rocking ollama, and I think I have ROCm active.&lt;/p&gt; &lt;p&gt;I'm generally just using a deepseek model via CLI.&lt;/p&gt; &lt;p&gt;Seemingly at random (I haven't identified a pattern) ollama will just use my CPU instead of my GPU, until I restart the ollama service.&lt;/p&gt; &lt;p&gt;Anyone have any advice on what I can do about this? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BloodyIron"&gt; /u/BloodyIron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pjbggq/ubuntu_linux_ollama_service_uses_cpu_instead_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pjbggq/ubuntu_linux_ollama_service_uses_cpu_instead_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pjbggq/ubuntu_linux_ollama_service_uses_cpu_instead_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-10T19:08:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjxz8a</id>
    <title>Anthropic claims to have solved the AI Memory problem for Agents</title>
    <updated>2025-12-11T13:49:55+00:00</updated>
    <author>
      <name>/u/Far-Photo4379</name>
      <uri>https://old.reddit.com/user/Far-Photo4379</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pjxz8a/anthropic_claims_to_have_solved_the_ai_memory/"&gt; &lt;img alt="Anthropic claims to have solved the AI Memory problem for Agents" src="https://external-preview.redd.it/5UymBFDYHaFKRZHHNo_0E-ClIRE2tai814BUmhhXnlQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=316fa4774949d65d4f2ba50b1d837047c8b98c80" title="Anthropic claims to have solved the AI Memory problem for Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Photo4379"&gt; /u/Far-Photo4379 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pjxz8a/anthropic_claims_to_have_solved_the_ai_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pjxz8a/anthropic_claims_to_have_solved_the_ai_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T13:49:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk0h7a</id>
    <title>Ollama connection abortedd</title>
    <updated>2025-12-11T15:34:47+00:00</updated>
    <author>
      <name>/u/SantiagoEtcheberrito</name>
      <uri>https://old.reddit.com/user/SantiagoEtcheberrito</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a server with a powerful video card dedicated to AI.&lt;/p&gt; &lt;p&gt;I am making connections with n8n, but when I run the flows, it keeps thinking and thinking for long minutes until I get this error: The connection was aborted, perhaps the server is offline [item 0].&lt;/p&gt; &lt;p&gt;I'm trying to run Qwen3:14b models, which are models that should support my 32GB VRAM. Does anyone have any idea what might be happening?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SantiagoEtcheberrito"&gt; /u/SantiagoEtcheberrito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pk0h7a/ollama_connection_abortedd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pk0h7a/ollama_connection_abortedd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pk0h7a/ollama_connection_abortedd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T15:34:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjpbg5</id>
    <title>Darkmode website please.</title>
    <updated>2025-12-11T05:14:39+00:00</updated>
    <author>
      <name>/u/Maltz42</name>
      <uri>https://old.reddit.com/user/Maltz42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That is all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maltz42"&gt; /u/Maltz42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pjpbg5/darkmode_website_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pjpbg5/darkmode_website_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pjpbg5/darkmode_website_please/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T05:14:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk4fcw</id>
    <title>Llm locally</title>
    <updated>2025-12-11T18:06:48+00:00</updated>
    <author>
      <name>/u/Lucky-Divide-2633</name>
      <uri>https://old.reddit.com/user/Lucky-Divide-2633</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Better to run llm locally on two mac mini m4 16gb each or one mac mini m4 pro with 24 gb ram? Any tips? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lucky-Divide-2633"&gt; /u/Lucky-Divide-2633 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pk4fcw/llm_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pk4fcw/llm_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pk4fcw/llm_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T18:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkbtqh</id>
    <title>Local project</title>
    <updated>2025-12-11T23:03:29+00:00</updated>
    <author>
      <name>/u/BackUpBiii</name>
      <uri>https://old.reddit.com/user/BackUpBiii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please check out my GitHub it features a full ml ide that uses custom made local models and normal local models hugging face models and gguf!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ItsMehRAWRXD/RawrXD/tree/production-lazy-init"&gt;https://github.com/ItsMehRAWRXD/RawrXD/tree/production-lazy-init&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I need as much feedback as possible! Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BackUpBiii"&gt; /u/BackUpBiii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkbtqh/local_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkbtqh/local_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkbtqh/local_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T23:03:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkbvjs</id>
    <title>How do you eject a model in the Ollama GUI?</title>
    <updated>2025-12-11T23:05:44+00:00</updated>
    <author>
      <name>/u/Lacooooo</name>
      <uri>https://old.reddit.com/user/Lacooooo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When using Ollama with the GUI, how can you unload or stop a model‚Äîsimilar to running &lt;code&gt;ollama stop &amp;lt;model-name&amp;gt;&lt;/code&gt; in the terminal‚Äîwithout using the terminal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lacooooo"&gt; /u/Lacooooo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkbvjs/how_do_you_eject_a_model_in_the_ollama_gui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkbvjs/how_do_you_eject_a_model_in_the_ollama_gui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkbvjs/how_do_you_eject_a_model_in_the_ollama_gui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T23:05:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk7chx</id>
    <title>Same Hardware, but Linux 5√ó Slower Than Windows? What's Going On?</title>
    <updated>2025-12-11T20:00:23+00:00</updated>
    <author>
      <name>/u/Al1x-ai</name>
      <uri>https://old.reddit.com/user/Al1x-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm working on an open-source speech‚Äëto‚Äëtext project called Murmure. It includes a new feature that uses Ollama to refine or transform the transcription produced by an ASR model.&lt;/p&gt; &lt;p&gt;To do this, I call Ollama‚Äôs API with models like ministral‚Äë3 or Qwen‚Äë3, and while running tests on the software, I noticed something surprising.&lt;/p&gt; &lt;p&gt;On Windows, the model response time is very fast (under 1-2 seconds), but on Linux Mint, using the exact same hardware (i5‚Äë13600KF and an Nvidia GeForce RTX 4070), the same operation easily takes 6-7 seconds on the same short audio.&lt;/p&gt; &lt;p&gt;It doesn‚Äôt seem to be a model‚Äëloading issue (I‚Äôm warming up the models in both cases, so the slowdown isn‚Äôt related to the initial load.), and the drivers look fine (inxi -G):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Device-1: NVIDIA AD104 [GeForce RTX 4070] driver: nvidia v: 580.95.05 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Ollama is also definitely using the GPU:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ministral-3:latest a5e54193fd34 16 GB 32%/68% CPU/GPU 4096 3 minutes from now &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm not sure what's causing this difference. Are any other Linux users experiencing the same slowdown compared to Windows? And if so, is there a known way to fix it or at least understand where the bottleneck comes from?&lt;/p&gt; &lt;p&gt;EDIT 1:&lt;br /&gt; On Windows:&lt;/p&gt; &lt;p&gt;&lt;code&gt; ministral-3:latest a5e54193fd34 7.5 GB 100% GPU 4096 4 minutes from now &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Same model, same hardware, but on Windows &lt;strong&gt;it runs 100% on GPU&lt;/strong&gt;, unlike on Linux and size is not the same at all.&lt;/p&gt; &lt;p&gt;EDIT 2 : Updating Ollama from 0.13.1 to 0.13.3, the models now show the correct sizes &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Al1x-ai"&gt; /u/Al1x-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pk7chx/same_hardware_but_linux_5_slower_than_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pk7chx/same_hardware_but_linux_5_slower_than_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pk7chx/same_hardware_but_linux_5_slower_than_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T20:00:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk1l6w</id>
    <title>Introducing TreeThinkerAgent: A Lightweight Autonomous Reasoning Agent for LLMs</title>
    <updated>2025-12-11T16:17:59+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pk1l6w/introducing_treethinkeragent_a_lightweight/"&gt; &lt;img alt="Introducing TreeThinkerAgent: A Lightweight Autonomous Reasoning Agent for LLMs" src="https://external-preview.redd.it/eHI2cjY1YzdvbDZnMbDWimRkn0jd8-nhfiwI9sUY7kTZ1aYtNE_bmUfANVp8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5a78127f6b07fdaf0ec1e187c532661354ded20" title="Introducing TreeThinkerAgent: A Lightweight Autonomous Reasoning Agent for LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone ! I‚Äôm excited to share my latest project: &lt;strong&gt;TreeThinkerAgent&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It‚Äôs an open-source orchestration layer that &lt;strong&gt;turns any Large Language Model into an autonomous, multi-step reasoning agent&lt;/strong&gt;, built entirely from scratch &lt;strong&gt;without any framework&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Try it locally using your favourite Ollama model.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Bessouat40/TreeThinkerAgent?utm_source=chatgpt.com"&gt;https://github.com/Bessouat40/TreeThinkerAgent&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What it does&lt;/h1&gt; &lt;p&gt;TreeThinkerAgent helps you:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Build a reasoning tree&lt;/strong&gt; so that every decision is structured and traceable&lt;br /&gt; - Turn an LLM into a &lt;strong&gt;multi-step planner and executor&lt;/strong&gt;&lt;br /&gt; - Perform &lt;strong&gt;step-by-step reasoning&lt;/strong&gt; with tool support&lt;br /&gt; - Execute complex tasks by planning and following through independently&lt;/p&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;p&gt;Most LLM interactions are ‚Äúone shot‚Äù: you ask a question and get an answer.&lt;/p&gt; &lt;p&gt;But many real-world problems require higher-level thinking: planning, decomposing into steps, and using tools like web search. TreeThinkerAgent tackles exactly that by making the reasoning process explicit and autonomous.&lt;/p&gt; &lt;p&gt;Check it out and let me know what you think. Your feedback, feature ideas, or improvements are more than welcome.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Bessouat40/TreeThinkerAgent?utm_source=chatgpt.com"&gt;https://github.com/Bessouat40/TreeThinkerAgent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h4k3w3b7ol6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pk1l6w/introducing_treethinkeragent_a_lightweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pk1l6w/introducing_treethinkeragent_a_lightweight/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T16:17:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkfv0i</id>
    <title>In OllaMan, using the Qwen3-Next model</title>
    <updated>2025-12-12T02:08:46+00:00</updated>
    <author>
      <name>/u/ComfyTightwad</name>
      <uri>https://old.reddit.com/user/ComfyTightwad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pkfv0i/in_ollaman_using_the_qwen3next_model/"&gt; &lt;img alt="In OllaMan, using the Qwen3-Next model" src="https://external-preview.redd.it/emRicXR0ZG9sbzZnMdW2ajc4pd0IkwIxB2RsxHZaHcmU2fwx0RdQoBvo_mLe.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb5c8330f13bf10d85a8aafb9df92bf8714eddbc" title="In OllaMan, using the Qwen3-Next model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComfyTightwad"&gt; /u/ComfyTightwad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cflbcgdolo6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkfv0i/in_ollaman_using_the_qwen3next_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkfv0i/in_ollaman_using_the_qwen3next_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T02:08:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkn6bl</id>
    <title>How to get rid of rendering glitches in browser</title>
    <updated>2025-12-12T08:52:09+00:00</updated>
    <author>
      <name>/u/Hot-Finger3903</name>
      <uri>https://old.reddit.com/user/Hot-Finger3903</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I recently installed ollama upon my device which has Intel iris xe ... And I noticed some issues concerned with mouse click and rendering any way to overcome this!?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hot-Finger3903"&gt; /u/Hot-Finger3903 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkn6bl/how_to_get_rid_of_rendering_glitches_in_browser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkn6bl/how_to_get_rid_of_rendering_glitches_in_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkn6bl/how_to_get_rid_of_rendering_glitches_in_browser/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T08:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkwzon</id>
    <title>Help me outt</title>
    <updated>2025-12-12T16:58:17+00:00</updated>
    <author>
      <name>/u/Ok_Cap3333</name>
      <uri>https://old.reddit.com/user/Ok_Cap3333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys i am new in here, I need ai model that is uncensored and unrestricted, help me out how do I find one?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Cap3333"&gt; /u/Ok_Cap3333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkwzon/help_me_outt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkwzon/help_me_outt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkwzon/help_me_outt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T16:58:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkvopv</id>
    <title>AI Agent from scratch: Django + Ollama + Pydantic AI - A Step-by-Step Guide</title>
    <updated>2025-12-12T16:07:00+00:00</updated>
    <author>
      <name>/u/tom-mart</name>
      <uri>https://old.reddit.com/user/tom-mart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone, &lt;/p&gt; &lt;p&gt;I just published Part 2 of the article series, which dives deep into creating a multi-layered memory system.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The agent has:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Short-term memory&lt;/strong&gt; for the current chat (with auto-pruning).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-term memory&lt;/strong&gt; using &lt;code&gt;pgvector&lt;/code&gt; to find relevant info from past conversations (RAG).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Summarization&lt;/strong&gt; to create condensed memories of old chats.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structured Memory&lt;/strong&gt; using tools to save/retrieve data from a Django model (I used a fitness tracker as an example).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Django &amp;amp; Django Ninja&lt;/li&gt; &lt;li&gt;Ollama (to run models like Llama 3 or Gemma locally)&lt;/li&gt; &lt;li&gt;Pydantic AI (for agent logic and tools)&lt;/li&gt; &lt;li&gt;PostgreSQL + &lt;code&gt;pgvector&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's a step-by-step guide meant to be easy to follow. I tried to explain the &amp;quot;why&amp;quot; behind the design, not just the &amp;quot;how.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;You can read the full article here:&lt;/strong&gt; &lt;a href="https://medium.com/@tom.mart/build-self-hosted-ai-agent-with-ollama-pydantic-ai-and-django-ninja-65214a3afb35"&gt;https://medium.com/@tom.mart/build-self-hosted-ai-agent-with-ollama-pydantic-ai-and-django-ninja-65214a3afb35&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The full code is on GitHub if you just want to browse. Happy to answer any questions!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/tom-mart/ai-agent"&gt;https://github.com/tom-mart/ai-agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tom-mart"&gt; /u/tom-mart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkvopv/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkvopv/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkvopv/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T16:07:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkn0j5</id>
    <title>Europe's devstral-small-2, available in the ollama library, looks promising</title>
    <updated>2025-12-12T08:40:23+00:00</updated>
    <author>
      <name>/u/New_Cranberry_6451</name>
      <uri>https://old.reddit.com/user/New_Cranberry_6451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share with you that yesterday I tested the devstral-small-2 model and it surprised me for good. With 24B params, it runs at 2,5 tokens per sec on my 8Gb VRAM laptop on Windows. (GPT-OSS, with 20B, runs at 20 tokens per sec, don't know how they do it...).&lt;/p&gt; &lt;p&gt;Despite this substantial performance difference, the quality of the answers is very high in my opinion, obtaining great results with simple prompts, and working great on instruction processing and system prompt following.&lt;/p&gt; &lt;p&gt;I am very happy, give it a try and tell me what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Cranberry_6451"&gt; /u/New_Cranberry_6451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkn0j5/europes_devstralsmall2_available_in_the_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkn0j5/europes_devstralsmall2_available_in_the_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkn0j5/europes_devstralsmall2_available_in_the_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T08:40:23+00:00</published>
  </entry>
</feed>
