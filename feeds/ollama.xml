<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-16T09:11:36+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ly7bnc</id>
    <title>whats the best model for my use case?</title>
    <updated>2025-07-12T18:22:22+00:00</updated>
    <author>
      <name>/u/Witty_Mycologist_995</name>
      <uri>https://old.reddit.com/user/Witty_Mycologist_995</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;whats the fastest local ollama model, that has tool support.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Witty_Mycologist_995"&gt; /u/Witty_Mycologist_995 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ly7bnc/whats_the_best_model_for_my_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ly7bnc/whats_the_best_model_for_my_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ly7bnc/whats_the_best_model_for_my_use_case/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T18:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxzg14</id>
    <title>Github copilot with Ollama - need to sign in?</title>
    <updated>2025-07-12T12:38:01+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, now that Github copilot for Visual Studio Code supports Ollama, i consider using it instead of Continue. However, it seems like you can only get to the model switcher dialogue when you are signed into github?&lt;/p&gt; &lt;p&gt;Of course, i don't want to sign in to anything, that's why i want to use my local ollama instance in the 1st place!&lt;/p&gt; &lt;p&gt;Has anyone found a workaround to use Ollama with copilot without having to sign in?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxzg14/github_copilot_with_ollama_need_to_sign_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxzg14/github_copilot_with_ollama_need_to_sign_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxzg14/github_copilot_with_ollama_need_to_sign_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T12:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyhl3h</id>
    <title>Socio especialista en N8N - Buscamos Socio</title>
    <updated>2025-07-13T02:19:08+00:00</updated>
    <author>
      <name>/u/BBCC37</name>
      <uri>https://old.reddit.com/user/BBCC37</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Somos un Grupo de 2 estudiantes de negocios (Socio 1 : Econom√≠a y Negocios Internacionales , Socio 2 Estudiante de Administraci√≥n y Marketing)&lt;/p&gt; &lt;p&gt;Tenemos experiencia en impulsar negocios ya que tenemos un negocio de venta de autom√≥viles en Per√∫, pero queremos incursionar en La creaci√≥n de automatizaciones para empresas y hacer escalable el negocio, ya que es un nicho en crecimiento y creemos que es posible que con nuestra experiencia podamos hacer crecer la Startup que queremos crear para utilizar agentes de IA. &lt;/p&gt; &lt;p&gt;Buscamos un socio especialista en N8N en de su entorno para poder hacer escalable el negocio desde lo t√©cnico ya que nosotros no encargaremos del desarrollo empresarial de la Startup con la b√∫squeda de financiamiento, planificaci√≥n financiera y b√∫squeda de clientes a trav√©s de Marketing. &lt;/p&gt; &lt;p&gt;Lima - Per√∫ &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BBCC37"&gt; /u/BBCC37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyhl3h/socio_especialista_en_n8n_buscamos_socio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyhl3h/socio_especialista_en_n8n_buscamos_socio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyhl3h/socio_especialista_en_n8n_buscamos_socio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T02:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxd4j0</id>
    <title>Two guys on a bus</title>
    <updated>2025-07-11T17:35:12+00:00</updated>
    <author>
      <name>/u/TodoLoQueCompartimos</name>
      <uri>https://old.reddit.com/user/TodoLoQueCompartimos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lxd4j0/two_guys_on_a_bus/"&gt; &lt;img alt="Two guys on a bus" src="https://preview.redd.it/kiuj2t0o6acf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258cace7d27d200a281a0ce5f08e1ed7c163739d" title="Two guys on a bus" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TodoLoQueCompartimos"&gt; /u/TodoLoQueCompartimos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kiuj2t0o6acf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxd4j0/two_guys_on_a_bus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxd4j0/two_guys_on_a_bus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T17:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxvdhu</id>
    <title>Henceforth ‚Ä¶</title>
    <updated>2025-07-12T08:22:04+00:00</updated>
    <author>
      <name>/u/But-I-Am-a-Robot</name>
      <uri>https://old.reddit.com/user/But-I-Am-a-Robot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Overly joyous posters in this group shall be referred to as Ollama Lama Ding Dongs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/But-I-Am-a-Robot"&gt; /u/But-I-Am-a-Robot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxvdhu/henceforth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxvdhu/henceforth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxvdhu/henceforth/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T08:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyh7p2</id>
    <title>newbie on Ollama some issues with searxng</title>
    <updated>2025-07-13T01:59:15+00:00</updated>
    <author>
      <name>/u/ElTamales</name>
      <uri>https://old.reddit.com/user/ElTamales</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks!&lt;/p&gt; &lt;p&gt;I have a 4090 and I wanted to give a try to set some models to summarize news from time to time. &lt;/p&gt; &lt;p&gt;So I decided the safest way was to download the dockerized version of ollama + openwebui.&lt;/p&gt; &lt;p&gt;All was good on the first installation.&lt;/p&gt; &lt;p&gt;Problem? I was silly and forgot that all the models were downloaded into my main drive, which was a kinda small 1TB NVME which was already 90% full.&lt;/p&gt; &lt;p&gt;During this moment, the models were working fine.&lt;/p&gt; &lt;p&gt;So I decided to switch the storage to a much bigger place. Which started to give me some issues.&lt;/p&gt; &lt;p&gt;Since I did not want to make things complicated. I simply removed the images instead of packing them to Tar and then move them to the new disk.&lt;/p&gt; &lt;p&gt;So after making the changes. I redownloaded everything. Then I started to have problems.&lt;/p&gt; &lt;p&gt;The models (phi4) and others, seem to work fine using searxng hosted in a docker on my NAS.&lt;/p&gt; &lt;p&gt;Until I try to search sports content. (Ie soccer).&lt;/p&gt; &lt;p&gt;Upon doing this search, I suddenly will get a &lt;em&gt;&amp;quot;I'm sorry, but I don't have access to real-time data or events beyond my training cut-off in October 2023.&amp;quot;&lt;/em&gt; response over and over in different sports and stuff.&lt;/p&gt; &lt;p&gt;over the subsequent queries, it will repeat this similarly and starting to output incorrect data.&lt;/p&gt; &lt;p&gt;Yet it seems to have searched and found many correct websites where the content is.. and then inviting you to check the links instead of summarizing the data.&lt;/p&gt; &lt;p&gt;Am I doing something wrong?&lt;/p&gt; &lt;p&gt;The Specs:&lt;/p&gt; &lt;p&gt;Searxng : UNRAID Docker container in a NAS.&lt;/p&gt; &lt;p&gt;Running computer: 14900k 4090, 64GB of RAM 3HDDS, 3 NVMEs, 1 SSD.&lt;/p&gt; &lt;p&gt;software: Nobara42 (Fedora 42 core), Podman 1x ollama 1x openwui.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElTamales"&gt; /u/ElTamales &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyh7p2/newbie_on_ollama_some_issues_with_searxng/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyh7p2/newbie_on_ollama_some_issues_with_searxng/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyh7p2/newbie_on_ollama_some_issues_with_searxng/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T01:59:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyp2kk</id>
    <title>Ollama can't start - exit status 2</title>
    <updated>2025-07-13T09:55:24+00:00</updated>
    <author>
      <name>/u/Antoni_Nabzdyk</name>
      <uri>https://old.reddit.com/user/Antoni_Nabzdyk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys,&lt;/p&gt; &lt;p&gt;I'm a prrammer, and have used Ollama for some time now. Now, out of nowhere, my Ollama local installation on my VPS stopped working altogheter. Each respoinse was rejected with the 500 error. I didn't know what to do. I use Google's AIStudio for the fix, but fater 3 hours, I have enough. The AIis telling me that I might have hardware-compatibility issues, and that my hardware can't run those models. That's impossible! I used it for a few months. I did clean installs, but then my AI said that the real clue was buried deep in the journalctl -u ollama.service logs:&lt;/p&gt; &lt;p&gt;SIGILL: illegal instruction&lt;/p&gt; &lt;p&gt;This is my journal as of right now:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Jul 13 09:36:53 srv670432 ollama[490754]: time=2025-07-13T09:36:53.992Z level=ERROR source=sched.go:489 msg=&amp;quot;error loading llama server&amp;quot; error=&amp;quot;llama runner process has terminated: exit status 2&amp;quot; Jul 13 09:36:53 srv670432 ollama[490754]: [GIN] 2025/07/13 - 09:36:53 | 500 | 339.406703ms | 127.0.0.1 | POST &amp;quot;/api/generate&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: [GIN] 2025/07/13 - 09:40:08 | 200 | 38.231¬µs | 127.0.0.1 | HEAD &amp;quot;/&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: [GIN] 2025/07/13 - 09:40:08 | 200 | 22.95465ms | 127.0.0.1 | POST &amp;quot;/api/show&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.678Z level=INFO source=server.go:135 msg=&amp;quot;system memory&amp;quot; total=&amp;quot;7.8 GiB&amp;quot; free=&amp;quot;6.9 GiB&amp;quot; free_swap=&amp;quot;4.4 GiB&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.678Z level=WARN source=server.go:145 msg=&amp;quot;requested context size too large for model&amp;quot; num_ctx=8192 num_parallel=2 n_ctx_train=2048 Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.678Z level=INFO source=server.go:175 msg=offload library=cpu layers.requested=-1 layers.model=23 layers.offload=0 layers.split=&amp;quot;&amp;quot; memory.available=&amp;quot;[6.9 GiB]&amp;quot; memory.gpu_overhead=&amp;quot;0 B&amp;quot; memory.required.full=&amp;quot;967.0 MiB&amp;quot; memory.required.partial=&amp;quot;0 B&amp;quot; memory.required.kv=&amp;quot;88.0 MiB&amp;quot; memory.required.allocations=&amp;quot;[967.0 MiB]&amp;quot; memory.weights.total=&amp;quot;571.4 MiB&amp;quot; memory.weights.repeating=&amp;quot;520.1 MiB&amp;quot; memory.weights.nonrepeating=&amp;quot;51.3 MiB&amp;quot; memory.graph.full=&amp;quot;280.0 MiB&amp;quot; memory.graph.partial=&amp;quot;278.3 MiB&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 (version GGUF V3 (latest)) Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output. Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 0: general.architecture str = llama Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 1: general.name str = TinyLlama Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 2: llama.context_length u32 = 2048 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 3: llama.embedding_length u32 = 2048 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 4: llama.block_count u32 = 22 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 5: llama.feed_forward_length u32 = 5632 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 6: llama.rope.dimension_count u32 = 64 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 7: llama.attention.head_count u32 = 32 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 8: llama.attention.head_count_kv u32 = 4 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 9: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 10: llama.rope.freq_base f32 = 10000.000000 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 11: general.file_type u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 12: tokenizer.ggml.model str = llama Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 13: tokenizer.ggml.tokens arr[str,32000] = [&amp;quot;&amp;lt;unk&amp;gt;&amp;quot;, &amp;quot;&amp;lt;s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;/s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;0x00&amp;gt;&amp;quot;, &amp;quot;&amp;lt;... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 14: tokenizer.ggml.scores arr[f32,32000] = [0.000000, 0.000000, 0.000000, 0.0000... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 15: tokenizer.ggml.token_type arr[i32,32000] = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 16: tokenizer.ggml.merges arr[str,61249] = [&amp;quot;‚ñÅ t&amp;quot;, &amp;quot;e r&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;‚ñÅ a&amp;quot;, &amp;quot;e n... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 17: tokenizer.ggml.bos_token_id u32 = 1 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 18: tokenizer.ggml.eos_token_id u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 19: tokenizer.ggml.unknown_token_id u32 = 0 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 20: tokenizer.ggml.padding_token_id u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 21: tokenizer.chat_template str = {% for message in messages %}\n{% if m... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 22: general.quantization_version u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - type f32: 45 tensors Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - type q4_0: 155 tensors Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - type q6_K: 1 tensors Jul 13 09:40:08 srv670432 ollama[490754]: print_info: file format = GGUF V3 (latest) Jul 13 09:40:08 srv670432 ollama[490754]: print_info: file type = Q4_0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: file size = 606.53 MiB (4.63 BPW) Jul 13 09:40:08 srv670432 ollama[490754]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect Jul 13 09:40:08 srv670432 ollama[490754]: load: special tokens cache size = 3 Jul 13 09:40:08 srv670432 ollama[490754]: load: token to piece cache size = 0.1684 MB Jul 13 09:40:08 srv670432 ollama[490754]: print_info: arch = llama Jul 13 09:40:08 srv670432 ollama[490754]: print_info: vocab_only = 1 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: model type = ?B Jul 13 09:40:08 srv670432 ollama[490754]: print_info: model params = 1.10 B Jul 13 09:40:08 srv670432 ollama[490754]: print_info: general.name = TinyLlama Jul 13 09:40:08 srv670432 ollama[490754]: print_info: vocab type = SPM Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_vocab = 32000 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_merges = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: BOS token = 1 '&amp;lt;s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: EOS token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: UNK token = 0 '&amp;lt;unk&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: PAD token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: LF token = 13 '&amp;lt;0x0A&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: EOG token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: max token length = 48 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_load: vocab only - skipping tensors Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.733Z level=INFO source=server.go:438 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 --ctx-size 4096 --batch-size 512 --threads 2 --no-mmap --parallel 2 --port 33555&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.734Z level=INFO source=sched.go:483 msg=&amp;quot;loaded runners&amp;quot; count=1 Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.734Z level=INFO source=server.go:598 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.735Z level=INFO source=server.go:632 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server not responding&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.758Z level=INFO source=runner.go:815 msg=&amp;quot;starting go runner&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.766Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc) Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.766Z level=INFO source=runner.go:874 msg=&amp;quot;Server listening on 127.0.0.1:33555&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 (version GGUF V3 (latest)) Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output. Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 0: general.architecture str = llama Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 1: general.name str = TinyLlama Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 2: llama.context_length u32 = 2048 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 3: llama.embedding_length u32 = 2048 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 4: llama.block_count u32 = 22 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 5: llama.feed_forward_length u32 = 5632 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 6: llama.rope.dimension_count u32 = 64 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 7: llama.attention.head_count u32 = 32 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 8: llama.attention.head_count_kv u32 = 4 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 9: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 10: llama.rope.freq_base f32 = 10000.000000 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 11: general.file_type u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 12: tokenizer.ggml.model str = llama Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 13: tokenizer.ggml.tokens arr[str,32000] = [&amp;quot;&amp;lt;unk&amp;gt;&amp;quot;, &amp;quot;&amp;lt;s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;/s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;0x00&amp;gt;&amp;quot;, &amp;quot;&amp;lt;... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 14: tokenizer.ggml.scores arr[f32,32000] = [0.000000, 0.000000, 0.000000, 0.0000... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 15: tokenizer.ggml.token_type arr[i32,32000] = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 16: tokenizer.ggml.merges arr[str,61249] = [&amp;quot;‚ñÅ t&amp;quot;, &amp;quot;e r&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;‚ñÅ a&amp;quot;, &amp;quot;e n... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 17: tokenizer.ggml.bos_token_id u32 = 1 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 18: tokenizer.ggml.eos_token_id u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 19: tokenizer.ggml.unknown_token_id u32 = 0 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 20: tokenizer.ggml.padding_token_id u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 21: tokenizer.chat_template str = {% for message in messages %}\n{% if m... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 22: general.quantization_version u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - type f32: 45 tensors Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - type q4_0: 155 tensors Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - type q6_K: 1 tensors Jul 13 09:40:08 srv670432 ollama[490754]: print_info: file format = GGUF V3 (latest) Jul 13 09:40:08 srv670432 ollama[490754]: print_info: file type = Q4_0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: file size = 606.53 MiB (4.63 BPW) Jul 13 09:40:08 srv670432 ollama[490754]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect Jul 13 09:40:08 srv670432 ollama[490754]: load: special tokens cache size = 3 Jul 13 09:40:08 srv670432 ollama[490754]: load: token to piece cache size = 0.1684 MB Jul 13 09:40:08 srv670432 ollama[490754]: print_info: arch = llama Jul 13 09:40:08 srv670432 ollama[490754]: print_info: vocab_only = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_ctx_train = 2048 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_embd = 2048 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_layer = 22 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_head = 32 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_head_kv = 4 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_rot = 64 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_swa = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_swa_pattern = 1 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_embd_head_k = 64 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_embd_head_v = 64 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_gqa = 8 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_embd_k_gqa = 256 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_embd_v_gqa = 256 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: f_norm_eps = 0.0e+00 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: f_norm_rms_eps = 1.0e-05 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: f_clamp_kqv = 0.0e+00 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: f_max_alibi_bias = 0.0e+00 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: f_logit_scale = 0.0e+00 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: f_attn_scale = 0.0e+00 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_ff = 5632 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_expert = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_expert_used = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: causal attn = 1 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: pooling type = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: rope type = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: rope scaling = linear Jul 13 09:40:08 srv670432 ollama[490754]: print_info: freq_base_train = 10000.0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: freq_scale_train = 1 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_ctx_orig_yarn = 2048 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: rope_finetuned = unknown Jul 13 09:40:08 srv670432 ollama[490754]: print_info: ssm_d_conv = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: ssm_d_inner = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: ssm_d_state = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: ssm_dt_rank = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: ssm_dt_b_c_rms = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: model type = 1B Jul 13 09:40:08 srv670432 ollama[490754]: print_info: model params = 1.10 B Jul 13 09:40:08 srv670432 ollama[490754]: print_info: general.name = TinyLlama Jul 13 09:40:08 srv670432 ollama[490754]: print_info: vocab type = SPM Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_vocab = 32000 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_merges = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: BOS token = 1 '&amp;lt;s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: EOS token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: UNK token = 0 '&amp;lt;unk&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: PAD token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: LF token = 13 '&amp;lt;0x0A&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: EOG token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: max token length = 48 Jul 13 09:40:08 srv670432 ollama[490754]: load_tensors: loading model tensors, this can take a while... (mmap = false) Jul 13 09:40:08 srv670432 ollama[490754]: SIGILL: illegal instruction Jul 13 09:40:08 srv670432 ollama[490754]: PC=0x7f7803f1c5aa m=0 sigcode=2 Jul 13 09:40:08 srv670432 ollama[490754]: signal arrived during cgo execution &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have no idea what to do next? My VPS has 8GB of RAM. After running this: root@srv670432:~# ollama run tinyllama &amp;quot;Hello, what's 2+2?&amp;quot;&lt;/p&gt; &lt;p&gt;Error: llama runner process has terminated: exit status 2&lt;/p&gt; &lt;p&gt;root@srv670432:~# &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Jul 13 09:50:55 srv670432 ollama[490754]: [GIN] 2025/07/13 - 09:50:55 | 200 | 39.52¬µs | 127.0.0.1 | HEAD &amp;quot;/&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: [GIN] 2025/07/13 - 09:50:55 | 200 | 39.553332ms | 127.0.0.1 | POST &amp;quot;/api/show&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.154Z level=INFO source=server.go:135 msg=&amp;quot;system memory&amp;quot; total=&amp;quot;7.8 GiB&amp;quot; free=&amp;quot;5.9 GiB&amp;quot; free_swap=&amp;quot;4.4 GiB&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.154Z level=WARN source=server.go:145 msg=&amp;quot;requested context size too large for model&amp;quot; num_ctx=8192 num_parallel=2 n_ctx_train=2048 Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.155Z level=INFO source=server.go:175 msg=offload library=cpu layers.requested=-1 layers.model=23 layers.offload=0 layers.split=&amp;quot;&amp;quot; memory.available=&amp;quot;[5.9 GiB]&amp;quot; memory.gpu_overhead=&amp;quot;0 B&amp;quot; memory.required.full=&amp;quot;967.0 MiB&amp;quot; memory.required.partial=&amp;quot;0 B&amp;quot; memory.required.kv=&amp;quot;88.0 MiB&amp;quot; memory.required.allocations=&amp;quot;[967.0 MiB]&amp;quot; memory.weights.total=&amp;quot;571.4 MiB&amp;quot; memory.weights.repeating=&amp;quot;520.1 MiB&amp;quot; memory.weights.nonrepeating=&amp;quot;51.3 MiB&amp;quot; memory.graph.full=&amp;quot;280.0 MiB&amp;quot; memory.graph.partial=&amp;quot;278.3 MiB&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 (version GGUF V3 (latest)) Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output. Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 0: general.architecture str = llama Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 1: general.name str = TinyLlama Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 2: llama.context_length u32 = 2048 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 3: llama.embedding_length u32 = 2048 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 4: llama.block_count u32 = 22 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 5: llama.feed_forward_length u32 = 5632 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 6: llama.rope.dimension_count u32 = 64 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 7: llama.attention.head_count u32 = 32 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 8: llama.attention.head_count_kv u32 = 4 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 9: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 10: llama.rope.freq_base f32 = 10000.000000 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 11: general.file_type u32 = 2 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 12: tokenizer.ggml.model str = llama Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 13: tokenizer.ggml.tokens arr[str,32000] = [&amp;quot;&amp;lt;unk&amp;gt;&amp;quot;, &amp;quot;&amp;lt;s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;/s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;0x00&amp;gt;&amp;quot;, &amp;quot;&amp;lt;... Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 14: tokenizer.ggml.scores arr[f32,32000] = [0.000000, 0.000000, 0.000000, 0.0000... Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 15: tokenizer.ggml.token_type arr[i32,32000] = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ... Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 16: tokenizer.ggml.merges arr[str,61249] = [&amp;quot;‚ñÅ t&amp;quot;, &amp;quot;e r&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;‚ñÅ a&amp;quot;, &amp;quot;e n... Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 17: tokenizer.ggml.bos_token_id u32 = 1 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 18: tokenizer.ggml.eos_token_id u32 = 2 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 19: tokenizer.ggml.unknown_token_id u32 = 0 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 20: tokenizer.ggml.padding_token_id u32 = 2 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 21: tokenizer.chat_template str = {% for message in messages %}\n{% if m... Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 22: general.quantization_version u32 = 2 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - type f32: 45 tensors Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - type q4_0: 155 tensors Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - type q6_K: 1 tensors Jul 13 09:50:55 srv670432 ollama[490754]: print_info: file format = GGUF V3 (latest) Jul 13 09:50:55 srv670432 ollama[490754]: print_info: file type = Q4_0 Jul 13 09:50:55 srv670432 ollama[490754]: print_info: file size = 606.53 MiB (4.63 BPW) Jul 13 09:50:55 srv670432 ollama[490754]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect Jul 13 09:50:55 srv670432 ollama[490754]: load: special tokens cache size = 3 Jul 13 09:50:55 srv670432 ollama[490754]: load: token to piece cache size = 0.1684 MB Jul 13 09:50:55 srv670432 ollama[490754]: print_info: arch = llama Jul 13 09:50:55 srv670432 ollama[490754]: print_info: vocab_only = 1 Jul 13 09:50:55 srv670432 ollama[490754]: print_info: model type = ?B Jul 13 09:50:55 srv670432 ollama[490754]: print_info: model params = 1.10 B Jul 13 09:50:55 srv670432 ollama[490754]: print_info: general.name = TinyLlama Jul 13 09:50:55 srv670432 ollama[490754]: print_info: vocab type = SPM Jul 13 09:50:55 srv670432 ollama[490754]: print_info: n_vocab = 32000 Jul 13 09:50:55 srv670432 ollama[490754]: print_info: n_merges = 0 Jul 13 09:50:55 srv670432 ollama[490754]: print_info: BOS token = 1 '&amp;lt;s&amp;gt;' Jul 13 09:50:55 srv670432 ollama[490754]: print_info: EOS token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:50:55 srv670432 ollama[490754]: print_info: UNK token = 0 '&amp;lt;unk&amp;gt;' Jul 13 09:50:55 srv670432 ollama[490754]: print_info: PAD token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:50:55 srv670432 ollama[490754]: print_info: LF token = 13 '&amp;lt;0x0A&amp;gt;' Jul 13 09:50:55 srv670432 ollama[490754]: print_info: EOG token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:50:55 srv670432 ollama[490754]: print_info: max token length = 48 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_load: vocab only - skipping tensors Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.214Z level=INFO source=server.go:438 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 --ctx-size 4096 --batch-size 512 --threads 2 --no-mmap --parallel 2 --port 35479&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.214Z level=INFO source=sched.go:483 msg=&amp;quot;loaded runners&amp;quot; count=1 Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.214Z level=INFO source=server.go:598 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.215Z level=INFO source=server.go:632 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server not responding&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.243Z level=INFO source=runner.go:815 msg=&amp;quot;starting go runner&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.267Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc) Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.268Z level=INFO source=runner.go:874 msg=&amp;quot;Server listening on 127.0.0.1:35479&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 (version GGUF V3 (latest)) // de;ete some to keep post shorter Jul 13 09:50:55 srv670432 ollama[490754]: SIGILL: illegal instruction Jul 13 09:50:55 srv670432 ollama[490754]: PC=0x7f68f2ceb5aa m=3 sigcode=2 Jul 13 09:50:55 srv670432 ollama[490754]: signal arrived during cgo execution Jul 13 09:50:55 srv670432 ollama[490754]: instruction bytes: 0x62 0xf2 0xfd 0x8 0x7c 0xc0 0xc5 0xfa 0x7f 0x43 0x18 0x48 0x83 0xc4 0x8 0x5b Jul 13 09:50:55 srv670432 ollama[490754]: goroutine 5 gp=0xc000002000 m=3 mp=0xc000067008 [syscall]: Jul 13 09:50:55 srv670432 ollama[490754]: runtime.cgocall(0x55d03641b7c0, 0xc000070bb0) Jul 13 09:50:55 srv670432 ollama[490754]: runtime/cgocall.go:167 +0x4b fp=0xc000070b88 sp=0xc000070b50 pc=0x55d0357598cb Jul 13 09:50:55 srv670432 ollama[490754]: github.com/ollama/ollama/llama._Cfunc_llama_model_load_from_file(0x7f68ec000b70, {0x0, 0x0, 0x0, 0x1, 0x0, 0x0, 0x55d03641b030, 0xc000519890, 0x0, ...}) Jul 13 09:50:55 srv670432 ollama[490754]: _cgo_gotypes.go:815 // delete some lines here level=ERROR source=sched.go:489 msg=&amp;quot;error loading llama server&amp;quot; error=&amp;quot;llama runner process has terminated: exit status 2&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: [GIN] 2025/07/13 - 09:50:55 | 500 | 370.079219ms | 127.0.0.1 | POST &amp;quot;/api/generate&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have no idea what to do, guys. Sorry if this post is very long, but I have no clue as to what is happening - any help will be welcome!&lt;/p&gt; &lt;p&gt;Thanks,&lt;/p&gt; &lt;p&gt;Antoni&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Antoni_Nabzdyk"&gt; /u/Antoni_Nabzdyk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyp2kk/ollama_cant_start_exit_status_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyp2kk/ollama_cant_start_exit_status_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyp2kk/ollama_cant_start_exit_status_2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T09:55:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxmtc1</id>
    <title>Thank you Ollama team! Observer AI launches tonight! üöÄ I built the local open-source screen-watching tool you guys asked for.</title>
    <updated>2025-07-12T00:21:19+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lxmtc1/thank_you_ollama_team_observer_ai_launches/"&gt; &lt;img alt="Thank you Ollama team! Observer AI launches tonight! üöÄ I built the local open-source screen-watching tool you guys asked for." src="https://external-preview.redd.it/OGRoeWJxNG82Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76ef79c27e133311336216d3d1ce535a146b2fa5" title="Thank you Ollama team! Observer AI launches tonight! üöÄ I built the local open-source screen-watching tool you guys asked for." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; The open-source tool that lets local LLMs watch your screen launches tonight! Thanks to your feedback, it now has a &lt;strong&gt;1-command install (completely offline no certs to accept)&lt;/strong&gt;, supports &lt;strong&gt;any OpenAI-compatible API&lt;/strong&gt;, and has &lt;strong&gt;mobile support&lt;/strong&gt;. I'd love your feedback!&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;You guys are so amazing! After all the feedback from my last post, I'm very happy to announce that Observer AI is almost officially launched! I want to thank everyone for their encouragement and ideas.&lt;/p&gt; &lt;p&gt;For those who are new, Observer AI is a privacy-first, open-source tool to build your own micro-agents that watch your screen (or camera) and trigger simple actions, all running 100% locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New in the last few days(Directly from your feedback!):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;‚úÖ 1-Command 100% Local Install:&lt;/strong&gt; I made it super simple. Just run docker compose up --build and the entire stack runs locally. No certs to accept or &amp;quot;online activation&amp;quot; needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚úÖ Universal Model Support:&lt;/strong&gt; You're no longer limited to Ollama! You can now connect to &lt;strong&gt;any endpoint that uses the OpenAI v1/chat standard&lt;/strong&gt;. This includes local servers like LM Studio, Llama.cpp, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚úÖ Mobile Support:&lt;/strong&gt; You can now use the app on your phone, using its camera and microphone as sensors. (Note: Mobile browsers don't support screen sharing).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Roadmap:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I hope that I'm just getting started. Here's what I will focus on next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Standalone Desktop App:&lt;/strong&gt; A 1-click installer for a native app experience. (With inference and everything!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Telegram Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Slack Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Sharing:&lt;/strong&gt; Easily share your creations with others via a simple link.&lt;/li&gt; &lt;li&gt;And much more!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Let's Build Together:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is a tool built for tinkerers, builders, and privacy advocates like you. Your feedback is crucial.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub (Please Star if you find it cool!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;App Link (Try it in your browser no install!):&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord (Join the community):&lt;/strong&gt; &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out in the comments all day. Let me know what you think and what you'd like to see next. Thank you again!&lt;/p&gt; &lt;p&gt;PS. Sorry to everyone who &lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ksu3dt4o6ccf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxmtc1/thank_you_ollama_team_observer_ai_launches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxmtc1/thank_you_ollama_team_observer_ai_launches/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T00:21:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly8ys0</id>
    <title>Anyone run Ollama on a gaming pc?</title>
    <updated>2025-07-12T19:31:50+00:00</updated>
    <author>
      <name>/u/pdawg17</name>
      <uri>https://old.reddit.com/user/pdawg17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know it's not ideal, but I just got a 5070ti and want to see how it does compared to my Mac Mini M4 with Ollama. The challenge is that I like having keep_alive at -1 (I use Ollama for Home Assistant so I ask it questions a lot), but that means when I play a game it cannot grab enough vram to run well.&lt;/p&gt; &lt;p&gt;Anyone use this setup and happy enough with it? Do you just shut down Ollama when playing then reload when done? Other options?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pdawg17"&gt; /u/pdawg17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ly8ys0/anyone_run_ollama_on_a_gaming_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ly8ys0/anyone_run_ollama_on_a_gaming_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ly8ys0/anyone_run_ollama_on_a_gaming_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T19:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lz0lv5</id>
    <title>Any front ends/GUIs that works in windows?</title>
    <updated>2025-07-13T18:53:01+00:00</updated>
    <author>
      <name>/u/Ancient-Asparagus837</name>
      <uri>https://old.reddit.com/user/Ancient-Asparagus837</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Any front ends/GUIs that works in windows natively?&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ancient-Asparagus837"&gt; /u/Ancient-Asparagus837 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lz0lv5/any_front_endsguis_that_works_in_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lz0lv5/any_front_endsguis_that_works_in_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lz0lv5/any_front_endsguis_that_works_in_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T18:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxzo4w</id>
    <title>I built a little CLI tool to do Ollama powered "deep" research from your terminal</title>
    <updated>2025-07-12T12:49:15+00:00</updated>
    <author>
      <name>/u/LightIn_</name>
      <uri>https://old.reddit.com/user/LightIn_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lxzo4w/i_built_a_little_cli_tool_to_do_ollama_powered/"&gt; &lt;img alt="I built a little CLI tool to do Ollama powered &amp;quot;deep&amp;quot; research from your terminal" src="https://preview.redd.it/aryz1e0hwfcf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70c17e0dfdca28b999b4bd63dd9bf81a030be45d" title="I built a little CLI tool to do Ollama powered &amp;quot;deep&amp;quot; research from your terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;I‚Äôve been messing around with local LLMs lately (with Ollama) and‚Ä¶ well, I ended up making a tiny CLI tool that tries to do ‚Äúdeep‚Äù research from your terminal.&lt;/p&gt; &lt;p&gt;It‚Äôs called &lt;strong&gt;deepsearch&lt;/strong&gt;. Basically you give it a question, and it tries to break it down into smaller sub-questions, search stuff on Wikipedia and DuckDuckGo, filter what seems relevant, summarize it all, and give you a final answer. Like‚Ä¶ what a human would do, I guess.&lt;/p&gt; &lt;p&gt;Here‚Äôs the repo if you‚Äôre curious:&lt;br /&gt; &lt;a href="https://github.com/LightInn/deepsearch"&gt;https://github.com/LightInn/deepsearch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don‚Äôt really know if this is &lt;em&gt;good&lt;/em&gt; (and even less if it's somewhat usefull :c ), just trying to glue something like this together. Honestly, it‚Äôs probably pretty rough, and I‚Äôm sure there are better ways to do what it does. But I thought it was a fun experiment and figured someone else might find it interesting too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LightIn_"&gt; /u/LightIn_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aryz1e0hwfcf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxzo4w/i_built_a_little_cli_tool_to_do_ollama_powered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxzo4w/i_built_a_little_cli_tool_to_do_ollama_powered/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T12:49:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyp75q</id>
    <title>Has any rolled their own ollama farm? What is your hardware/software setup for your remote personal ollama server?</title>
    <updated>2025-07-13T10:03:24+00:00</updated>
    <author>
      <name>/u/RyanBThiesant</name>
      <uri>https://old.reddit.com/user/RyanBThiesant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am interested in reusing old tech to make a ollama server. I like the idea of buying a bunch of ps2s, mineral oil, fish tanks, batteries and solar panels.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RyanBThiesant"&gt; /u/RyanBThiesant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyp75q/has_any_rolled_their_own_ollama_farm_what_is_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyp75q/has_any_rolled_their_own_ollama_farm_what_is_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyp75q/has_any_rolled_their_own_ollama_farm_what_is_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T10:03:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lywmxq</id>
    <title>Customization</title>
    <updated>2025-07-13T16:11:54+00:00</updated>
    <author>
      <name>/u/BikeDazzling8818</name>
      <uri>https://old.reddit.com/user/BikeDazzling8818</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BikeDazzling8818"&gt; /u/BikeDazzling8818 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenWebUI/comments/1lywmex/customization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lywmxq/customization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lywmxq/customization/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T16:11:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyu3pn</id>
    <title>Is there a good model for generating working mechanical designs?</title>
    <updated>2025-07-13T14:26:25+00:00</updated>
    <author>
      <name>/u/spookyclever</name>
      <uri>https://old.reddit.com/user/spookyclever</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm trying to design a gear system and it would be helpful if I could get a model that could translate my basic ideas to working systems that I could improve on in blender or solid works. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spookyclever"&gt; /u/spookyclever &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyu3pn/is_there_a_good_model_for_generating_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyu3pn/is_there_a_good_model_for_generating_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyu3pn/is_there_a_good_model_for_generating_working/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T14:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lz2mos</id>
    <title>AMD GPU</title>
    <updated>2025-07-13T20:14:48+00:00</updated>
    <author>
      <name>/u/neofita_</name>
      <uri>https://old.reddit.com/user/neofita_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys I made a mistake and bought GPU based on AMD‚Ä¶is there a lot of work to make different framework than Ollama work with my GPU? Or is there any way to make it work with AMD? Or O should just sell and buy Nvidia? üôà&lt;/p&gt; &lt;p&gt;EDIT: you were all right. It took me 10minutes including downloading everything to make it work with AMD GPU&lt;/p&gt; &lt;p&gt;THANKS ALL! üí™üèøüí™üèø&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neofita_"&gt; /u/neofita_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lz2mos/amd_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lz2mos/amd_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lz2mos/amd_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T20:14:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyx1xt</id>
    <title>Trying to get my Ollama model to run faster, is my solution a good one?</title>
    <updated>2025-07-13T16:29:03+00:00</updated>
    <author>
      <name>/u/Convillious</name>
      <uri>https://old.reddit.com/user/Convillious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm a bit confused on how memory storage within the LLM works but from what I‚Äôve seen so far, it is common to pass in a system prompt with the user prompt for every chat that is sent to the LLM.&lt;/p&gt; &lt;p&gt;I have a slow computer and I need this to speed up so I had an idea. My project is a server hosting an LLM which a user can access with an API and receive a response.&lt;/p&gt; &lt;p&gt;Instead of sending a system prompt every time, would it speed things up if on server initialization, I send a system prompt that instructed the LLM on what it‚Äôs supposed to do. And then I stored this information using LangGraphs long term memory, and then whenever a user prompts my LLM it simply derives from its memory when answering?&lt;/p&gt; &lt;p&gt;Sorry if that sounds convoluted but I just figured cutting down on the total number of input tokens would speed things up. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Convillious"&gt; /u/Convillious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyx1xt/trying_to_get_my_ollama_model_to_run_faster_is_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyx1xt/trying_to_get_my_ollama_model_to_run_faster_is_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyx1xt/trying_to_get_my_ollama_model_to_run_faster_is_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T16:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyv7ge</id>
    <title>How I use Gemma 3 to help me reply my texts</title>
    <updated>2025-07-13T15:13:03+00:00</updated>
    <author>
      <name>/u/sean01-eth</name>
      <uri>https://old.reddit.com/user/sean01-eth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lyv7ge/how_i_use_gemma_3_to_help_me_reply_my_texts/"&gt; &lt;img alt="How I use Gemma 3 to help me reply my texts" src="https://external-preview.redd.it/rVejZdVoLwcawbSc5Q7BMTfnBvVftpV8Jx64l7lRtUY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c583a73ddcbd440fd51b76ffac1e785cc2ae281b" title="How I use Gemma 3 to help me reply my texts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sean01-eth"&gt; /u/sean01-eth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/48w6qb1mincf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyv7ge/how_i_use_gemma_3_to_help_me_reply_my_texts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyv7ge/how_i_use_gemma_3_to_help_me_reply_my_texts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T15:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyv9vm</id>
    <title>Ollama helping me study</title>
    <updated>2025-07-13T15:15:50+00:00</updated>
    <author>
      <name>/u/Economy_Cucumber_702</name>
      <uri>https://old.reddit.com/user/Economy_Cucumber_702</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lyv9vm/ollama_helping_me_study/"&gt; &lt;img alt="Ollama helping me study" src="https://b.thumbs.redditmedia.com/5mWC0Sa6yLocvhKCpyWdyHMzOonYz7tV1G0Gxwp49Bs.jpg" title="Ollama helping me study" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy_Cucumber_702"&gt; /u/Economy_Cucumber_702 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lyv9vm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyv9vm/ollama_helping_me_study/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyv9vm/ollama_helping_me_study/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T15:15:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyodnc</id>
    <title>Podcast generation app -- works with Ollama</title>
    <updated>2025-07-13T09:07:51+00:00</updated>
    <author>
      <name>/u/lfnovo</name>
      <uri>https://old.reddit.com/user/lfnovo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I've built a podcast generation app for people that use Notebook LM for this purpose and would lke some extra capabilities like Ollama support, 1-4 speakers, multiple generation profiles, other voice provider support, and enhanced control on the generation. It also handles extracting content from any file or URL to use in the casts.&lt;/p&gt; &lt;p&gt;It comes with all you need to run, plus a UI for you to create and manage your podcasts.&lt;/p&gt; &lt;p&gt;Community feedback is very welcome. I plan to maintain this actively as its used on another big project of ours.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lfnovo/podcast-creator"&gt;https://github.com/lfnovo/podcast-creator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are some examples of a [4 person debate](&lt;a href="https://soundcloud.com/lfnovo/situational-awareness-podcast"&gt;https://soundcloud.com/lfnovo/situational-awareness-podcast&lt;/a&gt;) and [single speaker lesson](&lt;a href="https://soundcloud.com/lfnovo/single-speaker-podcast-on-situational-awareness"&gt;https://soundcloud.com/lfnovo/single-speaker-podcast-on-situational-awareness&lt;/a&gt;) on the Situational Awareness paper. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lfnovo"&gt; /u/lfnovo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyodnc/podcast_generation_app_works_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyodnc/podcast_generation_app_works_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyodnc/podcast_generation_app_works_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T09:07:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzjle1</id>
    <title>Ollama retaining history?</title>
    <updated>2025-07-14T11:11:38+00:00</updated>
    <author>
      <name>/u/DimensionEnergy</name>
      <uri>https://old.reddit.com/user/DimensionEnergy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so ive hosted ollama locally on my system on &lt;a href="http://localhost:11434/api/generate"&gt;http://localhost:11434/api/generate&lt;/a&gt; and was testing it out a bit and it seems that between separate fetch calls, ollama seems to be retaining some memory. &lt;/p&gt; &lt;p&gt;i don't understand why this would happen because as much as i have seen modern llms, they don't change their weights during inference. &lt;/p&gt; &lt;p&gt;Scenario:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;makes a query to ollama for topic 1 with a very specific keyword that i have created&lt;/li&gt; &lt;li&gt;makes another query to ollama for a topic that is similar to topic 1 but has a new keyword. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Turns out that the first keyword shows up in the second response aswell. Not always, but this shouldn't happen at all as much as i know&lt;/p&gt; &lt;p&gt;Is there something that i am missing?&lt;br /&gt; I checked the ollama/history file and it only contained prompts that i have made from the terminal using ollama run &amp;lt;model\_name&amp;gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DimensionEnergy"&gt; /u/DimensionEnergy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzjle1/ollama_retaining_history/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzjle1/ollama_retaining_history/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lzjle1/ollama_retaining_history/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-14T11:11:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzm1px</id>
    <title>Is it possible to generate images in open-webui about the generated text?</title>
    <updated>2025-07-14T13:12:45+00:00</updated>
    <author>
      <name>/u/assmaycsgoass</name>
      <uri>https://old.reddit.com/user/assmaycsgoass</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For ex. I ask the AI to write an intro for a story about a small village near a river, describing how it looks etc.&lt;/p&gt; &lt;p&gt;AI generates the text, and the image generation model uses that as a prompt and generates an image right below the paragraph in the window.&lt;/p&gt; &lt;p&gt;Is doing something like this possible? I use comfyui a lot but am a beginner here and was wondering if something like this can be done.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/assmaycsgoass"&gt; /u/assmaycsgoass &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzm1px/is_it_possible_to_generate_images_in_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzm1px/is_it_possible_to_generate_images_in_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lzm1px/is_it_possible_to_generate_images_in_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-14T13:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzmpnn</id>
    <title>With ROCm 7 expanding hardware compatibility and offering Windows support, will my 6700xt finally work natively on Windows?</title>
    <updated>2025-07-14T13:41:39+00:00</updated>
    <author>
      <name>/u/toast___ghost</name>
      <uri>https://old.reddit.com/user/toast___ghost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Struggling to find a GPU compatibility list. Any one know or have a prediction?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toast___ghost"&gt; /u/toast___ghost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzmpnn/with_rocm_7_expanding_hardware_compatibility_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzmpnn/with_rocm_7_expanding_hardware_compatibility_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lzmpnn/with_rocm_7_expanding_hardware_compatibility_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-14T13:41:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzxmfy</id>
    <title>How do I setup a research mode with ollama?</title>
    <updated>2025-07-14T20:29:56+00:00</updated>
    <author>
      <name>/u/MineDrumPE</name>
      <uri>https://old.reddit.com/user/MineDrumPE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want my local ai models to be able to search the web, is this possible locally? I've searched and haven't found any tutorials.&lt;/p&gt; &lt;p&gt;I want to be able to give ollama research access when I am accessing through webui and through n8n which will probably be 2 different setups I'm assuming?&lt;/p&gt; &lt;p&gt;Thanks for any help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MineDrumPE"&gt; /u/MineDrumPE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzxmfy/how_do_i_setup_a_research_mode_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzxmfy/how_do_i_setup_a_research_mode_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lzxmfy/how_do_i_setup_a_research_mode_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-14T20:29:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1m12wm4</id>
    <title>üö® Docker container stuck on ‚ÄúWaiting for application startup‚Äù ‚Äî Open WebUI won‚Äôt load in browser</title>
    <updated>2025-07-16T04:02:04+00:00</updated>
    <author>
      <name>/u/0nlyAxeman</name>
      <uri>https://old.reddit.com/user/0nlyAxeman</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0nlyAxeman"&gt; /u/0nlyAxeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1m12ij7/docker_container_stuck_on_waiting_for_application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m12wm4/docker_container_stuck_on_waiting_for_application/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m12wm4/docker_container_stuck_on_waiting_for_application/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T04:02:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0gzg2</id>
    <title>We built Explainable AI with pinpointed citations &amp; reasoning ‚Äî works across PDFs, Excel, CSV, Docs &amp; more</title>
    <updated>2025-07-15T12:53:56+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just added explainability to our RAG pipeline ‚Äî the AI now shows &lt;strong&gt;pinpointed citations&lt;/strong&gt; down to the &lt;strong&gt;exact paragraph, table row, or cell&lt;/strong&gt; it used to generate its answer.&lt;/p&gt; &lt;p&gt;It doesn‚Äôt just name the source file but also &lt;strong&gt;highlights the exact text&lt;/strong&gt; and lets you &lt;strong&gt;jump directly to that part of the document&lt;/strong&gt;. This works across formats: PDFs, Excel, CSV, Word, PowerPoint, Markdown, and more.&lt;/p&gt; &lt;p&gt;It makes AI answers easy to &lt;strong&gt;trust and verify&lt;/strong&gt;, especially in messy or lengthy enterprise files. You also get insight into the &lt;strong&gt;reasoning&lt;/strong&gt; behind the answer.&lt;/p&gt; &lt;p&gt;It‚Äôs fully open-source: &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;br /&gt; Would love to hear your thoughts or feedback!&lt;/p&gt; &lt;p&gt;üìπ Demo: &lt;a href="https://youtu.be/QWY_jtjRcCM"&gt;https://youtu.be/QWY_jtjRcCM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m0gzg2/we_built_explainable_ai_with_pinpointed_citations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m0gzg2/we_built_explainable_ai_with_pinpointed_citations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m0gzg2/we_built_explainable_ai_with_pinpointed_citations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-15T12:53:56+00:00</published>
  </entry>
</feed>
