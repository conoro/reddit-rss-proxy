<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-26T16:25:13+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1no2b7d</id>
    <title>Most Dangerous Ollama Agent? Demo + Repo</title>
    <updated>2025-09-22T23:45:15+00:00</updated>
    <author>
      <name>/u/New_Pomegranate_1060</name>
      <uri>https://old.reddit.com/user/New_Pomegranate_1060</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1no2b7d/most_dangerous_ollama_agent_demo_repo/"&gt; &lt;img alt="Most Dangerous Ollama Agent? Demo + Repo" src="https://external-preview.redd.it/YWp4Y29jaDB6c3FmMXt94OWDuAWHQAvniQ96yay80ZGLpddsHnI1O_WJEL_6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=653779aec1642f70f934b334caddfdb4f0fe5ee2" title="Most Dangerous Ollama Agent? Demo + Repo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on an ollama agent I’m calling TermNet and it’s honestly kind of nuts. In the demo video I show it doing a bunch of stuff most agents probably shouldn’t be trusted with. It’s got full terminal access so it can run commands directly on my machine.&lt;/p&gt; &lt;p&gt;It doesn’t stop there. It pulls system info, makes directories and files, writes and executes programs (can do gui) browses the web, and scans my local network. None of it is scripted or staged either. The agent strings everything together on its own and gives me the results in plain language. It’s a strange mix of useful and dangerous, which is why I figured I’d share it here.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/RawdodReverend/TermNet"&gt;https://github.com/RawdodReverend/TermNet&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TikTok: &lt;a href="https://www.tiktok.com/@rawdogreverend"&gt;https://www.tiktok.com/@rawdogreverend&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone decides to try it, I’d highly recommend running it in a VM or sandbox. It has full access to the system, so don’t point it at anything you care about.&lt;/p&gt; &lt;p&gt;Not trying to make this into some big “AI safety” post, just showing off what I’ve been playing with. But after seeing it chain commands and spin up code on the fly, I think it might be one of the more dangerous ollama agents out there right now. Curious what people here think and if anyone else has pushed agents this far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Pomegranate_1060"&gt; /u/New_Pomegranate_1060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ff0w6fm0zsqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1no2b7d/most_dangerous_ollama_agent_demo_repo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1no2b7d/most_dangerous_ollama_agent_demo_repo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-22T23:45:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1np1ubc</id>
    <title>Revolutionary</title>
    <updated>2025-09-24T03:29:08+00:00</updated>
    <author>
      <name>/u/ReviewDazzling9105</name>
      <uri>https://old.reddit.com/user/ReviewDazzling9105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1np1ubc/revolutionary/"&gt; &lt;img alt="Revolutionary" src="https://a.thumbs.redditmedia.com/1j7-hIlhJWAdUYS64t1WL9gfmDUd1jRdbRlRo8H6o18.jpg" title="Revolutionary" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gr2azbhy71rf1.png?width=1648&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c3447be84ceb7ed757b6d0638b485694588f81e"&gt;https://preview.redd.it/gr2azbhy71rf1.png?width=1648&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c3447be84ceb7ed757b6d0638b485694588f81e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Running ollama using openwebui on a pop-os workstation with RTXA2000 I7-7700 with 32gb of ram&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReviewDazzling9105"&gt; /u/ReviewDazzling9105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1np1ubc/revolutionary/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1np1ubc/revolutionary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1np1ubc/revolutionary/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-24T03:29:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1npbruw</id>
    <title>iPhone app for voice recording and AI processing</title>
    <updated>2025-09-24T13:12:18+00:00</updated>
    <author>
      <name>/u/Altruistic_Call_3023</name>
      <uri>https://old.reddit.com/user/Altruistic_Call_3023</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic_Call_3023"&gt; /u/Altruistic_Call_3023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1npbrcw/iphone_app_for_voice_recording_and_ai_processing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npbruw/iphone_app_for_voice_recording_and_ai_processing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1npbruw/iphone_app_for_voice_recording_and_ai_processing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-24T13:12:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nokzx7</id>
    <title>Computer Use on Windows Sandbox</title>
    <updated>2025-09-23T15:47:59+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nokzx7/computer_use_on_windows_sandbox/"&gt; &lt;img alt="Computer Use on Windows Sandbox" src="https://external-preview.redd.it/NGVuamVvaXpxeHFmMYar2P-d3EU8x2ju_uKYrB4yrb0aAUxLp4mH5szJsZ9M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db4b778804fd2edc20bde5e8cf8b999696f4b392" title="Computer Use on Windows Sandbox" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing Windows Sandbox support - run computer-use agents on Windows business apps without VMs or cloud costs.&lt;/p&gt; &lt;p&gt;Your enterprise software runs on Windows, but testing agents required expensive cloud instances. Windows Sandbox changes this - it's Microsoft's built-in lightweight virtualization sitting on every Windows 10/11 machine, ready for instant agent development.&lt;/p&gt; &lt;p&gt;Enterprise customers kept asking for AutoCAD automation, SAP integration, and legacy Windows software support. Traditional VM testing was slow and resource-heavy. Windows Sandbox solves this with disposable, seconds-to-boot Windows environments for safe agent testing.&lt;/p&gt; &lt;p&gt;What you can build: AutoCAD drawing automation, SAP workflow processing, Bloomberg terminal trading bots, manufacturing execution system integration, or any Windows-only enterprise software automation - all tested safely in disposable sandbox environments.&lt;/p&gt; &lt;p&gt;Free with Windows 10/11, boots in seconds, completely disposable. Perfect for development and testing before deploying to Windows cloud instances (coming later this month).&lt;/p&gt; &lt;p&gt;Check out the github here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/windows-sandbox"&gt;https://www.trycua.com/blog/windows-sandbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ibs8dytzqxqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nokzx7/computer_use_on_windows_sandbox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nokzx7/computer_use_on_windows_sandbox/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-23T15:47:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1npq4kb</id>
    <title>Any recommended small and snappy (but not dumb) models for a budget GPU?</title>
    <updated>2025-09-24T22:31:10+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got an unRAID server with an Intel Arc A380 GPU. So, in order to be able to use my non-NVIDIA GPU, I'm running Intel’s IPEX‑LLM Ollama container and accessing the models through Open WebUI.&lt;/p&gt; &lt;p&gt;I want to know what small and snappy, but not stupid, models you'd recommend for simple tasks? Right now I'm just experimenting, but we'll see how I'd like to expand in the future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npq4kb/any_recommended_small_and_snappy_but_not_dumb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npq4kb/any_recommended_small_and_snappy_but_not_dumb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1npq4kb/any_recommended_small_and_snappy_but_not_dumb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-24T22:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1npqhis</id>
    <title>Qwen3-embedding, how to set dimensionality?</title>
    <updated>2025-09-24T22:46:53+00:00</updated>
    <author>
      <name>/u/vredditt</name>
      <uri>https://old.reddit.com/user/vredditt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All 3 qwen3-embedding models seem to work great. However, I would very much like to compare results with different dimensions other than their respective maximum (1k, 2k, 4k dim respectively for 0.6b, 4b and 8b). &lt;/p&gt; &lt;p&gt;Did anyone succeed in finding the right parameter for that? &amp;quot;dimentions&amp;quot;: 512, as well as &amp;quot;dim&amp;quot;, &amp;quot;emd_dim&amp;quot; or options -&amp;gt; &amp;quot;dimentions&amp;quot; etc. do nothing. I didn't find anything in both, the ollama API reference and the model's description except a textual reference to the fact that setting users dimension is supported (from 32 dim to max).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vredditt"&gt; /u/vredditt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npqhis/qwen3embedding_how_to_set_dimensionality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npqhis/qwen3embedding_how_to_set_dimensionality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1npqhis/qwen3embedding_how_to_set_dimensionality/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-24T22:46:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq12jg</id>
    <title>Is there an additional fee if I use ollama cloud?</title>
    <updated>2025-09-25T08:14:30+00:00</updated>
    <author>
      <name>/u/Successful-Agent7030</name>
      <uri>https://old.reddit.com/user/Successful-Agent7030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to analyze a lot of data using ollama cloud. &lt;/p&gt; &lt;p&gt;I'm the only one user, but I have a lot of data. &lt;/p&gt; &lt;p&gt;Can I continue this for $20 a month? forever? &lt;/p&gt; &lt;p&gt;If I use it, I will use the gpt-oss:120b model.&lt;/p&gt; &lt;p&gt;* this post was translated with papago&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Successful-Agent7030"&gt; /u/Successful-Agent7030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nq12jg/is_there_an_additional_fee_if_i_use_ollama_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nq12jg/is_there_an_additional_fee_if_i_use_ollama_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nq12jg/is_there_an_additional_fee_if_i_use_ollama_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T08:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1npx76n</id>
    <title>analyze a pdf for content and structure/design</title>
    <updated>2025-09-25T04:12:45+00:00</updated>
    <author>
      <name>/u/fttklr</name>
      <uri>https://old.reddit.com/user/fttklr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if it is better to use a LLM with vision capacities or something else like ConfyUI, so I thought to ask here.&lt;/p&gt; &lt;p&gt;I would like to extract from documents (mostly PDF or word); the content of each page. The problem is that I want to get the images and the text, and get the way in which the text is arranged with the images (so the design/structure of each page basically).&lt;/p&gt; &lt;p&gt;The final result is to restore some old documents without actually scan them all and use OCR and then re-create the existing layout and text. So anything that can help me with this task would be really appreciated&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fttklr"&gt; /u/fttklr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npx76n/analyze_a_pdf_for_content_and_structuredesign/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npx76n/analyze_a_pdf_for_content_and_structuredesign/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1npx76n/analyze_a_pdf_for_content_and_structuredesign/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T04:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1npo4jh</id>
    <title>Orchestrate multiple Ollama models to do complex stuff with the automatic Multi-Agent Builder using Observer! (Free and Open Source)</title>
    <updated>2025-09-24T21:08:53+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1npo4jh/orchestrate_multiple_ollama_models_to_do_complex/"&gt; &lt;img alt="Orchestrate multiple Ollama models to do complex stuff with the automatic Multi-Agent Builder using Observer! (Free and Open Source)" src="https://external-preview.redd.it/56SPQExPI827GpA98kpipwMJSEu04uBeqWtJrHoG4nc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5567fa8755a2e8bb9fdc634d1c9e2f0436b0e05a" title="Orchestrate multiple Ollama models to do complex stuff with the automatic Multi-Agent Builder using Observer! (Free and Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR; This new Automatic Multi-Agent creator and editor makes Observer super super powerful. You can create multiple agents automatically and iterate System Prompts to get your local agents working really fast!&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;, &lt;/p&gt; &lt;p&gt;Ever since i started using Ollama i've thought about this exact use case for local models. Using vision + reasoning models to do more advanced things, like guiding you while creating a Google account! &lt;/p&gt; &lt;p&gt;Last time i showed you guys how to create them manually using Observer to solve LeetCode problems on screen, but now the Agent Builder can create them automatically!! And better yet, if a model is hallucinating or not triggering your notifications correctly, you just click one button and the Agent Builder can fix it for you. &lt;/p&gt; &lt;p&gt;This lets you have some agents that do the following: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Monitor &amp;amp; Document&lt;/strong&gt; - One agent describes your screen, another keeps a document of the process.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extract &amp;amp; Solve&lt;/strong&gt; - One agent extracts problems from the screen, another solves them.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Watch &amp;amp; Guide&lt;/strong&gt; - One agent lists out possible buttons or actions, another provides step-by-step guidance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Of course you can still have simple one-agent configs to get notifications when downloads finish, renders complete, something happens on a video game etc. etc. Everything using your local Ollama models! &lt;/p&gt; &lt;p&gt;You can download the app and look at the code right here: &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Or try it out without any install (non-local but easy): &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Thanks to the Ollama team for making this type of App possible! I hope this App makes more people interested in local models and their possible uses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=6zJh8NmCXYw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npo4jh/orchestrate_multiple_ollama_models_to_do_complex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1npo4jh/orchestrate_multiple_ollama_models_to_do_complex/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-24T21:08:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1npwqw0</id>
    <title>local computer vision on webcam</title>
    <updated>2025-09-25T03:48:43+00:00</updated>
    <author>
      <name>/u/faflappy</name>
      <uri>https://old.reddit.com/user/faflappy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1npwqw0/local_computer_vision_on_webcam/"&gt; &lt;img alt="local computer vision on webcam" src="https://external-preview.redd.it/Ypd2kQ-k8wRi8W0sQ8OMojLcTwuapPuL7em0lGxtBK4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64d37f8896fb78027d65fe7720961fbc08448972" title="local computer vision on webcam" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i made a local object detection and identification script that uses yolo, sam, and ollama vlm models. it runs on the webcam with ~30fps on my laptop. &lt;/p&gt; &lt;p&gt;two versions:&lt;br /&gt; 1. YOLO/SAM object detection and tracking with vlm object tagging&lt;/p&gt; &lt;ol&gt; &lt;li&gt;motion detection with vlm descriptions of the entire frame&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;still new to computer vision systems so very open to feedback and advice&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/faflappy"&gt; /u/faflappy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kazumah1/local-detection"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1npwqw0/local_computer_vision_on_webcam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1npwqw0/local_computer_vision_on_webcam/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T03:48:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nph8mq</id>
    <title>using ollama&amp;gemini with comfyui</title>
    <updated>2025-09-24T16:44:40+00:00</updated>
    <author>
      <name>/u/Far-Entertainer6755</name>
      <uri>https://old.reddit.com/user/Far-Entertainer6755</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nph8mq/using_ollamagemini_with_comfyui/"&gt; &lt;img alt="using ollama&amp;amp;gemini with comfyui" src="https://external-preview.redd.it/YTdiczBwOTA2NXJmMSzTzzAWBhaeIDAUXYuWZ8SFYObdBmY6Opua2srAH4iY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7eac2af0758ab1ebbccbe4247c0b7b73152d0740" title="using ollama&amp;amp;gemini with comfyui" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;📌 ComfyUI-OllamaGemini – Run Ollama inside ComfyUI&lt;/h1&gt; &lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I’ve put together a &lt;strong&gt;ComfyUI custom node&lt;/strong&gt; that integrates directly with &lt;strong&gt;Ollama&lt;/strong&gt; so you can use your local LLMs inside ComfyUI workflows.&lt;/p&gt; &lt;p&gt;👉 GitHub: &lt;a href="https://github.com/al-swaiti/ComfyUI-OllamaGemini"&gt;ComfyUI-OllamaGemini&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;🔹 Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Use any Ollama model (Llama 3, Mistral, Gemma, etc.) inside ComfyUI&lt;/li&gt; &lt;li&gt;Combine text generation with image and video workflows&lt;/li&gt; &lt;li&gt;Build multimodal pipelines (reasoning → prompts → visuals)&lt;/li&gt; &lt;li&gt;Keep everything &lt;strong&gt;local and private&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔹 Installation&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;cd ComfyUI/custom_nodes git clone https://github.com/al-swaiti/ComfyUI-OllamaGemini.git &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Entertainer6755"&gt; /u/Far-Entertainer6755 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/27ulvq9065rf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nph8mq/using_ollamagemini_with_comfyui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nph8mq/using_ollamagemini_with_comfyui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-24T16:44:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqymvc</id>
    <title>How to delete this malware?</title>
    <updated>2025-09-26T11:17:37+00:00</updated>
    <author>
      <name>/u/Far_Buyer_7281</name>
      <uri>https://old.reddit.com/user/Far_Buyer_7281</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do I delete ollama, and why is it made this hard?&lt;br /&gt; why does it need to UPDATE at every BOOT?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Buyer_7281"&gt; /u/Far_Buyer_7281 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqymvc/how_to_delete_this_malware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqymvc/how_to_delete_this_malware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqymvc/how_to_delete_this_malware/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T11:17:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq6myv</id>
    <title>Service for Efficient Vector Embeddings</title>
    <updated>2025-09-25T13:22:28+00:00</updated>
    <author>
      <name>/u/mrdabbler</name>
      <uri>https://old.reddit.com/user/mrdabbler</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sometimes I need to use a vector database and do semantic search.&lt;br /&gt; Generating text embeddings via the ML model is the main bottleneck, especially when working with large amounts of data.&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;Vectrain&lt;/strong&gt;, a service that helps speed up this process and might be useful to others. I’m guessing some of you might be facing the same kind of problems.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What the service does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Receives messages for embedding from Kafka or via its own REST API.&lt;/li&gt; &lt;li&gt;Spins up multiple embedder instances working in parallel to speed up embedding generation (currently only Ollama is supported).&lt;/li&gt; &lt;li&gt;Stores the resulting embeddings in a vector database (currently only Qdrant is supported).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’d love to hear your feedback, tips, and, of course, stars on GitHub.&lt;/p&gt; &lt;p&gt;The service is fully functional, and I plan to keep developing it gradually. I’d also love to know how relevant it is—maybe it’s worth investing more effort and pushing it much more actively.&lt;/p&gt; &lt;p&gt;Vectrain repo: &lt;a href="https://github.com/torys877/vectrain"&gt;https://github.com/torys877/vectrain&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrdabbler"&gt; /u/mrdabbler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nq6myv/service_for_efficient_vector_embeddings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nq6myv/service_for_efficient_vector_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nq6myv/service_for_efficient_vector_embeddings/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T13:22:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqki3x</id>
    <title>Me and my friends connected an Humanoid Robot to Local Large Language Models</title>
    <updated>2025-09-25T22:23:04+00:00</updated>
    <author>
      <name>/u/lightofshadow_</name>
      <uri>https://old.reddit.com/user/lightofshadow_</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lightofshadow_"&gt; /u/lightofshadow_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nqe2ll/me_and_my_friends_connected_an_humanoid_robot_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqki3x/me_and_my_friends_connected_an_humanoid_robot_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqki3x/me_and_my_friends_connected_an_humanoid_robot_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T22:23:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqb2ij</id>
    <title>How can I minimize cold start time?</title>
    <updated>2025-09-25T16:15:39+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My server is relatively low-power. Here are some of the main specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 5 3400G (Quad-core)&lt;/li&gt; &lt;li&gt;32 GB DDR4&lt;/li&gt; &lt;li&gt;Intel Arc A380 (6GB GDDR6)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have Ollama up and running through my Intel Arc. Specifically, I have Intel’s IPEX‑LLM Ollama container and accessing the models through Open WebUI.&lt;/p&gt; &lt;p&gt;Given my lower powered specs, I'm sticking with, at highest, 8B models. Once I'm past the first chat, responses come somewhere between instantaneous to maybe 2 seconds of waiting. However, the first chat I send in a while generally takes between 30 - 45 seconds for a response, depending on the model.&lt;/p&gt; &lt;p&gt;I've gathered that this slow start is &amp;quot;warm-up time,&amp;quot; as the model is loading in. I have my appdata on an NVME drive, so there shouldn't be any slowness there. How can I minimize this loading time?&lt;/p&gt; &lt;p&gt;I realize this end-goal may not be able to work as intended with my current hardware, but I do intend to eventually replace Alexa with a self-hosted assistant, powered by Ollama. 45 seconds of wait time seems very excessive for testing, especially since I've found that waiting only about 5 minutes between chats is enough for the model to need that 45 seconds to warm up again..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqb2ij/how_can_i_minimize_cold_start_time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqb2ij/how_can_i_minimize_cold_start_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqb2ij/how_can_i_minimize_cold_start_time/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T16:15:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqknap</id>
    <title>How to change design of 3500 images fast,easy and extremely accurate?</title>
    <updated>2025-09-25T22:29:08+00:00</updated>
    <author>
      <name>/u/Real_Investment_3726</name>
      <uri>https://old.reddit.com/user/Real_Investment_3726</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nqknap/how_to_change_design_of_3500_images_fasteasy_and/"&gt; &lt;img alt="How to change design of 3500 images fast,easy and extremely accurate?" src="https://b.thumbs.redditmedia.com/90gmZt2Rvlu4nBkCtORFJFssImuOYj2OTFVEdj5_7RY.jpg" title="How to change design of 3500 images fast,easy and extremely accurate?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to change the design of 3500 football training exercise images, fast, easily, and extremely accurately? It's not necessary to be 3500 at once; 50 by 50 is totally fine as well, but only if it's extremely accurate.&lt;/p&gt; &lt;p&gt;I was thinking of using the OpenAI API in my custom project and with a prompt to modify a large number of exercises at once (from .png to create a new .png with the Image creator), but the problem is that ChatGPT 5's vision capabilities and image generation were not accurate enough. It was always missing some of the balls, lines, and arrows; some of the arrows were not accurate enough. For example, when I ask ChatGPT to explain how many balls there are in an exercise image and to make it in JSON, instead of hitting the correct number, 22, it hits 5-10 instead, which is pretty terrible if I want perfect or almost perfect results. Seems like it's bad at counting.&lt;/p&gt; &lt;p&gt;Guys how to change design of 3500 images fast,easy and extremely accurate?&lt;/p&gt; &lt;p&gt;That's what OpenAI image generator generated. On the left side is the generated image and on the right side is the original:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9qlzma6e0erf1.png?width=2351&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd33b97691d9e207edfbaaffcfb489ec4195e19b"&gt;https://preview.redd.it/9qlzma6e0erf1.png?width=2351&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd33b97691d9e207edfbaaffcfb489ec4195e19b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Real_Investment_3726"&gt; /u/Real_Investment_3726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqknap/how_to_change_design_of_3500_images_fasteasy_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqknap/how_to_change_design_of_3500_images_fasteasy_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqknap/how_to_change_design_of_3500_images_fasteasy_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T22:29:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq0fgx</id>
    <title>Dead-simple example code for Ollama function calling.</title>
    <updated>2025-09-25T07:31:16+00:00</updated>
    <author>
      <name>/u/kirill_saidov</name>
      <uri>https://old.reddit.com/user/kirill_saidov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nq0fgx/deadsimple_example_code_for_ollama_function/"&gt; &lt;img alt="Dead-simple example code for Ollama function calling." src="https://external-preview.redd.it/nwRZM-Te2VIqEnwZwvslkKw4wJFzzEeW_-DOTY7mWJs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efa43a58d310b84e2cd256d70fe4206bb5f499ca" title="Dead-simple example code for Ollama function calling." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This shows how to use function calling + how to get a coherent response from LLM, not just raw results returned by functions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kirill_saidov"&gt; /u/kirill_saidov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kirillsaidov/ollama-function-calling"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nq0fgx/deadsimple_example_code_for_ollama_function/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nq0fgx/deadsimple_example_code_for_ollama_function/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T07:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqf0t2</id>
    <title>I trained a 4B model to be good at reasoning. Wasn’t expecting this!</title>
    <updated>2025-09-25T18:46:01+00:00</updated>
    <author>
      <name>/u/adeelahmadch</name>
      <uri>https://old.reddit.com/user/adeelahmadch</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adeelahmadch"&gt; /u/adeelahmadch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nqf049/i_trained_a_4b_model_to_be_good_at_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqf0t2/i_trained_a_4b_model_to_be_good_at_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqf0t2/i_trained_a_4b_model_to_be_good_at_reasoning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T18:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqfnff</id>
    <title>First time using granite-code too 😂</title>
    <updated>2025-09-25T19:09:51+00:00</updated>
    <author>
      <name>/u/___-___---</name>
      <uri>https://old.reddit.com/user/___-___---</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nqfnff/first_time_using_granitecode_too/"&gt; &lt;img alt="First time using granite-code too 😂" src="https://b.thumbs.redditmedia.com/Spf6bS4wWycIgh5zgI8v2hKI8H95KDpO56xcj6oiu-A.jpg" title="First time using granite-code too 😂" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/oknmefvj0drf1.png?width=1047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=179ba86a5c4c2b9578a27a3783cc755cc740a0b0"&gt;https://preview.redd.it/oknmefvj0drf1.png?width=1047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=179ba86a5c4c2b9578a27a3783cc755cc740a0b0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/___-___---"&gt; /u/___-___--- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqfnff/first_time_using_granitecode_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqfnff/first_time_using_granitecode_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqfnff/first_time_using_granitecode_too/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T19:09:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqwp6i</id>
    <title>The Evolution of Search - A Brief History of Information Retrieval</title>
    <updated>2025-09-26T09:21:19+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nqwp6i/the_evolution_of_search_a_brief_history_of/"&gt; &lt;img alt="The Evolution of Search - A Brief History of Information Retrieval" src="https://external-preview.redd.it/rdiqDR_pScizHcdiYSt4CYhHHCgIfLWKQPCyQEs-E3k.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9980443776d6f25f3145283b6475431f5b3cff07" title="The Evolution of Search - A Brief History of Information Retrieval" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/ghE4gQkx2b4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqwp6i/the_evolution_of_search_a_brief_history_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqwp6i/the_evolution_of_search_a_brief_history_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T09:21:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqhnrc</id>
    <title>Do you give your models a system prompt? If so, can I get some examples?</title>
    <updated>2025-09-25T20:27:38+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqhnrc/do_you_give_your_models_a_system_prompt_if_so_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqhnrc/do_you_give_your_models_a_system_prompt_if_so_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqhnrc/do_you_give_your_models_a_system_prompt_if_so_can/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-25T20:27:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqxvsn</id>
    <title>Ollama hangs after a while</title>
    <updated>2025-09-26T10:34:35+00:00</updated>
    <author>
      <name>/u/Aggravating_Pin_8922</name>
      <uri>https://old.reddit.com/user/Aggravating_Pin_8922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using ollama to run models on prem, in order to call them from my code using langchain.&lt;/p&gt; &lt;p&gt;I've noticed that everytime I run ollama for a long time, it starts hangging and I have to reboot it otherwise it doesn't work.&lt;/p&gt; &lt;p&gt;I've also tried to do &amp;quot;ollama run &amp;lt;model&amp;gt;&amp;quot; using the terminal and it also freezes when I do it.&lt;/p&gt; &lt;p&gt;Has anyone had similar problems? How did you overcome them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggravating_Pin_8922"&gt; /u/Aggravating_Pin_8922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqxvsn/ollama_hangs_after_a_while/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqxvsn/ollama_hangs_after_a_while/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqxvsn/ollama_hangs_after_a_while/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T10:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr0x0l</id>
    <title>Ollama API: 405 Method Not Allowed on POST requests</title>
    <updated>2025-09-26T13:08:30+00:00</updated>
    <author>
      <name>/u/max1302</name>
      <uri>https://old.reddit.com/user/max1302</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Self-hosted Ollama on VPS at &lt;code&gt;https://ollama.mydomain.com&lt;/code&gt;. Web UI works fine, can GET &lt;code&gt;api/models&lt;/code&gt; with API key, but POST to &lt;code&gt;api/generate&lt;/code&gt; and &lt;code&gt;api/chat&lt;/code&gt; returns &lt;strong&gt;Error 405: Method Not Allowed&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Anyone know what might be causing this? Thinking it could be a reverse proxy config issue.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/max1302"&gt; /u/max1302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nr0x0l/ollama_api_405_method_not_allowed_on_post_requests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nr0x0l/ollama_api_405_method_not_allowed_on_post_requests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nr0x0l/ollama_api_405_method_not_allowed_on_post_requests/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T13:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqutzy</id>
    <title>Paiperwork 1.0.2 released, new functionality: SlideForge</title>
    <updated>2025-09-26T07:18:29+00:00</updated>
    <author>
      <name>/u/Infinitai-cn</name>
      <uri>https://old.reddit.com/user/Infinitai-cn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everybody! &lt;/p&gt; &lt;p&gt;We just released an update to our Paiperwork software with a new function: SlideForge, and many bug fixes.&lt;/p&gt; &lt;p&gt;Find us at: &lt;a href="https://infinitai-cn.github.io/paiperwork/"&gt;https://infinitai-cn.github.io/paiperwork/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A shootout to the Presenton team and their gorgeous &lt;a href="https://github.com/presenton/presenton"&gt;AI Presentation software&lt;/a&gt;!, we truly love the style.&lt;/p&gt; &lt;p&gt;Latest update:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Added new presentation tab functionality: SlideForge.&lt;/li&gt; &lt;li&gt;Now when selecting a model in any model selector in the APP, previously loaded models are unloaded to save Vram/ram (Ollama's behavior is to let small models coexist if enough memory, but unused models are not unloaded on demand).&lt;/li&gt; &lt;li&gt;Gpt -oss UI update (thinking level buttons).&lt;/li&gt; &lt;li&gt;Fixed missing translations for paperwork generation.&lt;/li&gt; &lt;li&gt;Fixed meeting minutes generator line spacing.&lt;/li&gt; &lt;li&gt;Web search improved.&lt;/li&gt; &lt;li&gt;Added web search to global document RAG.&lt;/li&gt; &lt;li&gt;Added edit thinking models list to models tab.&lt;/li&gt; &lt;li&gt;Portuguese translations revised and cleaned.&lt;/li&gt; &lt;li&gt;In models tab now you can expand the lists of new thinking and visual models manually.&lt;/li&gt; &lt;li&gt;Added Portuguese to online help.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1nqutzy/video/hvl14pammgrf1/player"&gt;https://reddit.com/link/1nqutzy/video/hvl14pammgrf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our previous introduction post &lt;a href="https://www.reddit.com/r/ollama/comments/1lbpz7w/introducing_paiperwork_a_privacyfirst_ai/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infinitai-cn"&gt; /u/Infinitai-cn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqutzy/paiperwork_102_released_new_functionality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqutzy/paiperwork_102_released_new_functionality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqutzy/paiperwork_102_released_new_functionality/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T07:18:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqpz41</id>
    <title>First steps toward local AI Agents with Ollama (browser extension)</title>
    <updated>2025-09-26T02:39:49+00:00</updated>
    <author>
      <name>/u/InfiniteJX</name>
      <uri>https://old.reddit.com/user/InfiniteJX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We’ve been experimenting with Ollama and recently built &lt;a href="https://nativemind.app/"&gt;a browser extension&lt;/a&gt; that turns a local model into an Agent. The idea is to run everything locally—no cloud APIs—while leztting the model interact directly with web pages.&lt;/p&gt; &lt;p&gt;Our extension already supported features like multi-tab conversations, Chat with PDF/images/screenshots, Gmail assistant, and a writing helper. Recently, we upgraded the Chat capability, taking our first significant step toward local AI agents.&lt;/p&gt; &lt;p&gt;We wrote up some details here if you’re curious: &lt;a href="https://nativemind.app/blog/ai-agent/"&gt;https://nativemind.app/blog/ai-agent/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few highlights of what the Agent can currently do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Read and summarize Webpages/PDFs directly in the browser&lt;/li&gt; &lt;li&gt;Extract and interpret information from multiple web pages&lt;/li&gt; &lt;li&gt;Perform searches and navigate through resultsb&lt;/li&gt; &lt;li&gt;Click buttons and interact with elements on a page (basic browser-use actions)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;One of the biggest challenges we’ve run into is the &lt;strong&gt;limited context window of local models&lt;/strong&gt;, which restricts how capable the Agent can be when dealing with larger documents or more complex workflows.&lt;/p&gt; &lt;p&gt;Still, even with this limitation, it already feels useful for lightweight automation and research tasks.&lt;/p&gt; &lt;p&gt;Curious—has anyone else been exploring similar directions with Ollama? Would love to hear your thoughts or feedback.&lt;/p&gt; &lt;p&gt;If you’re interested in our project, it’s open-source — feel free to check it out or support us here: &lt;a href="https://github.com/NativeMindBrowser/NativeMindExtension"&gt;https://github.com/NativeMindBrowser/NativeMindExtension&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InfiniteJX"&gt; /u/InfiniteJX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqpz41/first_steps_toward_local_ai_agents_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nqpz41/first_steps_toward_local_ai_agents_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nqpz41/first_steps_toward_local_ai_agents_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-26T02:39:49+00:00</published>
  </entry>
</feed>
