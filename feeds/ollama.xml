<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-24T22:14:44+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1rc3fkr</id>
    <title>Model requires more system memory (eventhough I have enough)</title>
    <updated>2026-02-23T01:03:23+00:00</updated>
    <author>
      <name>/u/gfejer</name>
      <uri>https://old.reddit.com/user/gfejer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have two Tesla P40s passed through as vGPU profiles to a Ubuntu 24.04 VM. As an example I can run gpt-oss just fine and the GPUs get recognized by the system, but when it comes to running Llama 3.3 which is a 43GB model (my 2*24GB VRAM should be enough right?) gives me an error that I don‚Äôt have enough system memory.&lt;/p&gt; &lt;p&gt;I am guessing that for some reason it tries to run the model on the CPU, but I don‚Äôt understand why‚Ä¶ Are there any possible fixes for this issue?&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gfejer"&gt; /u/gfejer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc3fkr/model_requires_more_system_memory_eventhough_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:03:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbq3dy</id>
    <title>Ollama for Dummies</title>
    <updated>2026-02-22T16:24:37+00:00</updated>
    <author>
      <name>/u/catbutchie</name>
      <uri>https://old.reddit.com/user/catbutchie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone needs to write a book. Right now my mentor is ChatGPT. There are so many parameters i just tell it my issue and it tells me what to change. Some small tweaks are significant performance adjustment. I‚Äôm new obviously. I‚Äôd like to know why I‚Äôm doing what I‚Äôm doing so I can be more in control. I‚Äôm using silly tavern and ollama for self hosted chat. Suggestions needed. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/catbutchie"&gt; /u/catbutchie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbq3dy/ollama_for_dummies/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T16:24:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc7fd1</id>
    <title>AMD 5550xt Macbook Pro 2019</title>
    <updated>2026-02-23T04:11:48+00:00</updated>
    <author>
      <name>/u/tubaraodogroove</name>
      <uri>https://old.reddit.com/user/tubaraodogroove</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm planning to purchase this MacBook second-hand just to run some models that my 2017 laptop can't handle.&lt;/p&gt; &lt;p&gt;However, based on what I found during my research, Ollama doesn‚Äôt recognize this graphics card as a usable GPU. Can someone explain whether this is a good deal for running Ollama, or if it simply won‚Äôt work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tubaraodogroove"&gt; /u/tubaraodogroove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc7fd1/amd_5550xt_macbook_pro_2019/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc7fd1/amd_5550xt_macbook_pro_2019/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc7fd1/amd_5550xt_macbook_pro_2019/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T04:11:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rccbn8</id>
    <title>Built a honeypot token library for AI agents ‚Äî detects prompt injection the moment it succeeds</title>
    <updated>2026-02-23T08:49:56+00:00</updated>
    <author>
      <name>/u/Responsible-Yak-9657</name>
      <uri>https://old.reddit.com/user/Responsible-Yak-9657</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rccbn8/built_a_honeypot_token_library_for_ai_agents/"&gt; &lt;img alt="Built a honeypot token library for AI agents ‚Äî detects prompt injection the moment it succeeds" src="https://external-preview.redd.it/y-WYMevcIN_5-ZHiVkm0emVpTgZqx82u0L69rZGk6FM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18cf100549c9b843e934f38bccc2bb7dc338f724" title="Built a honeypot token library for AI agents ‚Äî detects prompt injection the moment it succeeds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible-Yak-9657"&gt; /u/Responsible-Yak-9657 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/aiagents/comments/1rby96z/built_a_honeypot_token_library_for_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rccbn8/built_a_honeypot_token_library_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rccbn8/built_a_honeypot_token_library_for_ai_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T08:49:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcgy0h</id>
    <title>Help Setting up - nanobot?</title>
    <updated>2026-02-23T13:07:26+00:00</updated>
    <author>
      <name>/u/lawfulcrispy</name>
      <uri>https://old.reddit.com/user/lawfulcrispy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lawfulcrispy"&gt; /u/lawfulcrispy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1rcgxpv/help_setting_up_nanobot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcgy0h/help_setting_up_nanobot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcgy0h/help_setting_up_nanobot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T13:07:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc4l9j</id>
    <title>16GB VRAM for mode agent</title>
    <updated>2026-02-23T01:56:50+00:00</updated>
    <author>
      <name>/u/ColdTransition5828</name>
      <uri>https://old.reddit.com/user/ColdTransition5828</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was hoping to enjoy something similar to Cursor on my PC. I even bought what was supposed to be a mid-range card. But the results are disappointing.&lt;/p&gt; &lt;p&gt;After studying it, I realized I'm missing the core agent and a better Ollama model that accepts tools. But honestly, I'm bored. What do you recommend I do to get the most out of local models with my 16GB of VRAM?&lt;/p&gt; &lt;p&gt;I mostly do full-track coding and Java.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ColdTransition5828"&gt; /u/ColdTransition5828 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc4l9j/16gb_vram_for_mode_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:56:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcaxo7</id>
    <title>Qwen3VL:30b-Q6-Thinking</title>
    <updated>2026-02-23T07:23:02+00:00</updated>
    <author>
      <name>/u/CheekyMonkeee</name>
      <uri>https://old.reddit.com/user/CheekyMonkeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been trying for hours to make a Modelfile that will actually make this work in Ollama. I have the ggufs for both the base model and the mmproj together in a folder and the create command finishes successfully. I get the directory and the blobs and the chat accepts images, but I get a 500 error when I prompt. Downloaded both ggufs from huggingface.&lt;/p&gt; &lt;p&gt;At this point, I don‚Äôt even care if it‚Äôs the best model for my use case anymore. I‚Äôm fairly sure it‚Äôs not a memory issue as I‚Äôm on a 5090 with 64GB of system RAM, and it will run the Q4 32b non-thinking model that I downloaded straight from Ollama (even though that spills over into RAM).&lt;/p&gt; &lt;p&gt;I‚Äôve just never had to do the modelfile thing with a download from huggingface before (still fairly new to this) and I don‚Äôt want to give up. I have to sleep, but any advice would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CheekyMonkeee"&gt; /u/CheekyMonkeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcaxo7/qwen3vl30bq6thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcaxo7/qwen3vl30bq6thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcaxo7/qwen3vl30bq6thinking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T07:23:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcv5by</id>
    <title>I turned MCP tools into standard CLI commands to solve context pollution</title>
    <updated>2026-02-23T21:51:03+00:00</updated>
    <author>
      <name>/u/_pdp_</name>
      <uri>https://old.reddit.com/user/_pdp_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;One of the biggest issues with MCP is context pollution. Loading a single service might be fine, but when you have 10 or 100 of them, you're spending most of your valuable context on tool definitions.&lt;/p&gt; &lt;p&gt;The usual solution is to use an MCP gateway that exposes a single generic function. Unfortunately, this doesn't work well because with a single function, the context of how and when to use each tool is completely lost.&lt;/p&gt; &lt;p&gt;What I've found is that the best approach is usually the Unix way. So what if, instead of loading MCP tools into the context, you make them available as standard CLIs? Now you can write your own SKILL.md and be happy.&lt;/p&gt; &lt;p&gt;This is what MCPShim does. It starts a background daemon that keeps all your MCPs nicely organized. It supports all authentication types (including OAuth, even when you don't have a publicly exposed HTTP server). Most importantly, you can now call into any of these MCPs and tools like standard commands. MCPShim even automatically generates bashrc aliases and bash completion to make things super easy.&lt;/p&gt; &lt;p&gt;As an added benefit, if you develop your own agents you no longer need to bolt on an MCP library and handle everything manually. You can focus on building a lean, high-quality AI agent and leave the MCP work to system processes.&lt;/p&gt; &lt;p&gt;The link to the open-source project is in the comments below.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_pdp_"&gt; /u/_pdp_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcv5by/i_turned_mcp_tools_into_standard_cli_commands_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcv5by/i_turned_mcp_tools_into_standard_cli_commands_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcv5by/i_turned_mcp_tools_into_standard_cli_commands_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T21:51:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcurz4</id>
    <title>Building a service and PWA for Ollama (and other models) with SQLite RAG and artifacts. Is this project interesting to the community?</title>
    <updated>2026-02-23T21:37:21+00:00</updated>
    <author>
      <name>/u/pokemondodo</name>
      <uri>https://old.reddit.com/user/pokemondodo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! For almost a year, I‚Äôve been working on a project that serves as a smart, functional, and secure UI for LLM models. There are many ready-made solutions, but most often they require complex Docker setups or writing configurations. Projects with a simpler launch but similar functionality are usually paid.&lt;/p&gt; &lt;p&gt;My solution works in the browser or via a PWA application. Absolutely all computations happen on the user's device. The project has no server at all; hosting with SSL is only needed to organize the PWA application.&lt;/p&gt; &lt;p&gt;In general, the project will work even without internet if the models are deployed locally and the PWA application is already downloaded.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical points I focused on:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Client-side data storage&lt;/strong&gt; ‚Äî a SQLite database is used. Once created, you can place it anywhere; the browser will only ask for permission to write to the file. Everything is stored in the database: chats, messages, embeddings, system settings, artifacts. You can change databases whenever you want.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic module&lt;/strong&gt; ‚Äî through triggers, the application extracts any important facts about the user. Name, other people, allergies, city of residence, favorite games ‚Äî anything. Everything is stored in the selected database as a text fact and an embedding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Heuristic module&lt;/strong&gt; ‚Äî the project has a mascot that displays its emotions in the form of stickers under each message. This can be turned off in the settings. Besides this, the assistant has its own mood. Through a mathematical expression, its final behavior is calculated based on variables: general mood, level of sarcasm, level of humor. This doesn't affect the quality of the answer or tasks, but it affects human perception ‚Äî answers can be dry, restrained, or sarcastic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Artifacts&lt;/strong&gt; ‚Äî the project has a library of documents and an application. There are 4 types of artifacts in total: games, applications, documents, analytical documents. You can ask to generate a document by prompt or by feeding information through a file attachment.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Working with files&lt;/strong&gt; ‚Äî PDF, DOCX, XLSX, TXT, and any files that can be interpreted as text or code are accepted. Nothing goes anywhere beyond your device; text is extracted at the moment of the request by the application or the browser tab.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt; ‚Äî the database itself you work in is not encrypted, only the password. This is done so that you don't lose access to your documents and chats even without using the project. But in the project, connecting any database or entering settings happens only through a password.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Operating modes&lt;/strong&gt; ‚Äî there are three modes: Kids, Teens, and Adult. I should probably write a whole separate post for this. Briefly: the kids' mode is protected from adult and dangerous topics, and the assistant will not give answers to homework, only tell and help with what and how to solve.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compatibility&lt;/strong&gt; ‚Äî it will work not only with local models but also through cloud APIs. It's not as secure, of course, but you can buy tokens in Gemini or a plan in OpenRouter ‚Äî everything will work perfectly. There are three connection providers in total: Gemini, OpenRouter, Custom (Ollama, etc.). You can change providers on the fly in any chat; the assistant's behavior will only change due to the power of the model itself.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why I‚Äôm writing this:&lt;/strong&gt; This is a completely free project. It started as a tool for personal needs to check semantics in another project.&lt;/p&gt; &lt;p&gt;The project has no ads, subscriptions, price plans, or anywhere you need to enter your card details.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would the community be interested in a full technical review with a demonstration of functionality?&lt;/strong&gt; Or is this too niche a topic for such a sub?&lt;/p&gt; &lt;p&gt;If a demonstration is needed, won't I get banned for advertising or promotion? And what aspects do you want me to reveal in more detail ‚Äî general demonstration or a deep dive into code and architecture?&lt;/p&gt; &lt;p&gt;I'll be glad to hear your opinion and am ready to answer any questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pokemondodo"&gt; /u/pokemondodo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcurz4/building_a_service_and_pwa_for_ollama_and_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcurz4/building_a_service_and_pwa_for_ollama_and_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcurz4/building_a_service_and_pwa_for_ollama_and_other/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T21:37:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc8xvy</id>
    <title>spend more time downloading models than actually using them</title>
    <updated>2026-02-23T05:29:41+00:00</updated>
    <author>
      <name>/u/Sharp-Mouse9049</name>
      <uri>https://old.reddit.com/user/Sharp-Mouse9049</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;qwen 2.5 dropped and suddenly mixtral is dead to me. downloaded the 72b. ran it once. went back to 7b cause i dont actually need 72b for anything i do&lt;/p&gt; &lt;p&gt;got like 200gb of models sitting on my drive. couldnt tell you the difference between half of them without checking the folder names&lt;/p&gt; &lt;p&gt;every week theres a new one thats supposedly better and i gotta have it. run some vibes check. wow this one feels smarter. back to doing the same three things i always do&lt;/p&gt; &lt;p&gt;its like im collecting pokemon but the pokemon just sit there&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sharp-Mouse9049"&gt; /u/Sharp-Mouse9049 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc8xvy/spend_more_time_downloading_models_than_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc8xvy/spend_more_time_downloading_models_than_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc8xvy/spend_more_time_downloading_models_than_actually/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T05:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rddkjr</id>
    <title>Just Dropped My First Ollama + AI Video ‚Äî What Should I Build Next?</title>
    <updated>2026-02-24T11:21:02+00:00</updated>
    <author>
      <name>/u/SnooApples5040</name>
      <uri>https://old.reddit.com/user/SnooApples5040</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just dropped my first AI video, and it's helping my friends and others around me. Ollama is so easy to use, and I've seen it as the base in #Ethical #Hacking #AI videos. &lt;/p&gt; &lt;p&gt;Here is the video: &lt;a href="https://youtu.be/snDTuCuxfzE"&gt;https://youtu.be/snDTuCuxfzE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want to keep helping my friends. So I'd love your input, &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;What is something about Ollama or AI you wish someone told you?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What would be the next logical step?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Do you have specific resources you find better than others?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;p&gt;Christy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooApples5040"&gt; /u/SnooApples5040 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rddkjr/just_dropped_my_first_ollama_ai_video_what_should/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rddkjr/just_dropped_my_first_ollama_ai_video_what_should/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rddkjr/just_dropped_my_first_ollama_ai_video_what_should/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T11:21:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc3srb</id>
    <title>I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review</title>
    <updated>2026-02-23T01:19:58+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"&gt; &lt;img alt="I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review" src="https://external-preview.redd.it/anBvOHE4ZWxiNWxnMYmmpi9UU3yP9yrC87ePDCyv5Mn4iZk_AUHCQZq2TOQ_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=452fe5fc9cf576221ea71aff1d15b07c8fa36f35" title="I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all ‚Äî been experimenting with running a local AI agent using ClawBot + Ollama and wanted to share what actually happened.&lt;/p&gt; &lt;p&gt;Link to full tutorial: &lt;a href="https://www.youtube.com/watch?v=FxyQkj95VXs"&gt;https://www.youtube.com/watch?v=FxyQkj95VXs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yes, ClawBot + Ollama works on Mac. Does it work well? Depends on what you mean by &amp;quot;work&amp;quot;&lt;/li&gt; &lt;li&gt;With an 8B model, agentic tasks are limited. Basic Q&amp;amp;A? Fine. Anything complex? It'll humble you real quick&lt;/li&gt; &lt;li&gt;Should you expect ChatGPT-level speed? Absolutely not. Go make a coffee while you wait üòÖ&lt;/li&gt; &lt;li&gt;Is it worth it for learning the stack and experimenting locally for free? Honestly yes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What worked&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Setup is cleaner than expected - VS Code, JSON config, localhost dashboard, done. I have no luck setting up ollama using their onboarding. So...I went straight to config file.&lt;/li&gt; &lt;li&gt;Ollama model switching is straightforward once you understand the config structure&lt;/li&gt; &lt;li&gt;Great for understanding how local AI agent setups actually work under the hood&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What didn't&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Speed is rough on anything under 32GB RAM&lt;/li&gt; &lt;li&gt;8B models hit their ceiling fast on multi-step reasoning and real agentic workflows. Keep context window low.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1ee663elb5lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcf94q</id>
    <title>What GPU do you use?</title>
    <updated>2026-02-23T11:43:10+00:00</updated>
    <author>
      <name>/u/Keensworth</name>
      <uri>https://old.reddit.com/user/Keensworth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've recently started using Ollama with an old GPU I had laying around.&lt;/p&gt; &lt;p&gt;Problem is that my GTX 1660S only got 6Gb VRAM and you can't do much with that.&lt;/p&gt; &lt;p&gt;I can run Mistral 7B Instruct but he sucks.&lt;/p&gt; &lt;p&gt;What hardware are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Keensworth"&gt; /u/Keensworth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T11:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd9l5i</id>
    <title>Best practices for running local LLMs for ~70‚Äì150 developers (agentic coding use case)</title>
    <updated>2026-02-24T07:16:14+00:00</updated>
    <author>
      <name>/u/Resident_Potential97</name>
      <uri>https://old.reddit.com/user/Resident_Potential97</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Resident_Potential97"&gt; /u/Resident_Potential97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1rd9kpk/best_practices_for_running_local_llms_for_70150/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd9l5i/best_practices_for_running_local_llms_for_70150/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rd9l5i/best_practices_for_running_local_llms_for_70150/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T07:16:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcxt3m</id>
    <title>How are you monitoring your Ollama calls/usage?</title>
    <updated>2026-02-23T23:32:55+00:00</updated>
    <author>
      <name>/u/gkarthi280</name>
      <uri>https://old.reddit.com/user/gkarthi280</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"&gt; &lt;img alt="How are you monitoring your Ollama calls/usage?" src="https://preview.redd.it/b8gxcch9xblg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=ced5ee353224c286bd1ab54c4819b57ff238fad4" title="How are you monitoring your Ollama calls/usage?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Ollama in my LLM applications and wanted some feedback on what type of metrics people here would find useful to track in an app that eventually would go into prod. I used OpenTelemetry to instrument my app by following this&lt;a href="https://signoz.io/docs/ollama-monitoring/"&gt; Ollama observability guide&lt;/a&gt; and was able to create this dashboard.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b8gxcch9xblg1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc90458a61e2e80c8ed5e283edc3e914ccbddcd6"&gt;Ollama dashboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It tracks things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;token usage&lt;/li&gt; &lt;li&gt;error rate&lt;/li&gt; &lt;li&gt;number of requests&lt;/li&gt; &lt;li&gt;latency&lt;/li&gt; &lt;li&gt;LLM provider and model &amp;amp; token distribution&lt;/li&gt; &lt;li&gt;logs and errors&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Are there any important metrics that you would want to keep track of in prod for monitoring your Ollama usage that aren't included here? And have you guys found any other ways to monitor these llm calls made through ollama?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1r8j5ob"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gkarthi280"&gt; /u/gkarthi280 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T23:32:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdjiu7</id>
    <title>How exactly do I go about making AI video?</title>
    <updated>2026-02-24T15:38:35+00:00</updated>
    <author>
      <name>/u/Sol33t303</name>
      <uri>https://old.reddit.com/user/Sol33t303</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm new to running local AI instances. I have an AMD 7800 XT, so 16GB of VRAM, and I use Alpaca in a flatpak for running my stuff, rocm seems to fail for whatever reason so I'm using vulkan which works fine.&lt;/p&gt; &lt;p&gt;What would be the best model for playing around with video generation for my hardware? And could somebody give an example prompt to get it to generate a video? In particular I would be interested in animating an existing image, I'd like to make it seem like it's breathing. &lt;/p&gt; &lt;p&gt;Thank you for the help :), I can't seem to find many resources on this in regards to running locally. I see references to hunyuan but they are a year old and I for some reason can't find it in alpaca.&lt;/p&gt; &lt;p&gt;EDIT: Ok so I think I have just realized alpaca simply doesn't have image generation support.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sol33t303"&gt; /u/Sol33t303 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdjiu7/how_exactly_do_i_go_about_making_ai_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdjiu7/how_exactly_do_i_go_about_making_ai_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdjiu7/how_exactly_do_i_go_about_making_ai_video/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T15:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd5ely</id>
    <title>Full GPU Acceleration for Ollama on Mac Pro 2013 (Dual FirePro D700) - Linux</title>
    <updated>2026-02-24T04:23:21+00:00</updated>
    <author>
      <name>/u/manu7irl</name>
      <uri>https://old.reddit.com/user/manu7irl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;# [Guide] Full GPU Acceleration for Ollama on Mac Pro 2013 (Dual FirePro D700) - Linux&lt;/p&gt; &lt;p&gt;Hey everyone! I finally managed to get full GPU acceleration working for **Ollama** on the legendary **Mac Pro 6.1 (2013 &amp;quot;Trashcan&amp;quot;)** running Nobara Linux (and it should work on other distros too).&lt;/p&gt; &lt;p&gt;The problem with these machines is that they have dual **AMD FirePro D700s (Tahiti XT)**. By default, Linux uses the legacy `radeon` driver for these cards. While `radeon` works for display, it **does not support Vulkan or ROCm**, meaning Ollama defaults to the CPU, which is slow as molasses.&lt;/p&gt; &lt;p&gt;### My Setup:&lt;/p&gt; &lt;p&gt;- **Model:** Mac Pro 6,1 (Late 2013)&lt;/p&gt; &lt;p&gt;- **CPU:** Xeon E5-1680 v2 (8C/16T @ 3.0 GHz)&lt;/p&gt; &lt;p&gt;- **RAM:** 32GB&lt;/p&gt; &lt;p&gt;- **GPU:** Dual AMD FirePro D700 (6GB each, 12GB total VRAM)&lt;/p&gt; &lt;p&gt;- **OS:** Nobara Linux (Fedora 40/41 base)&lt;/p&gt; &lt;p&gt;### The Solution:&lt;/p&gt; &lt;p&gt;We need to force the `amdgpu` driver for the Southern Islands (SI) architecture. Once `amdgpu` is active, Vulkan is enabled, and Ollama picks up both GPUs automatically!&lt;/p&gt; &lt;p&gt;### Performance (The Proof):&lt;/p&gt; &lt;p&gt;I'm currently testing **`qwen2.5-coder:14b`** (9GB model). &lt;/p&gt; &lt;p&gt;- **GPU Offload:** 100% (49/49 layers)&lt;/p&gt; &lt;p&gt;- **VRAM Split:** Perfectly balanced across both D700s (~4GB each)&lt;/p&gt; &lt;p&gt;- **Speed:** **~11.5 tokens/second** üöÄ&lt;/p&gt; &lt;p&gt;- **Total Response Time:** ~13.8 seconds for a standard coding prompt.&lt;/p&gt; &lt;p&gt;On CPU alone, this model was barely usable at &amp;lt;2 tokens/sec. This fix makes the Trashcan a viable local LLM workstation in 2026!&lt;/p&gt; &lt;p&gt;### How to do it:&lt;/p&gt; &lt;p&gt;**1. Update Kernel Parameters**&lt;/p&gt; &lt;p&gt;Add these to your GRUB configuration:&lt;/p&gt; &lt;p&gt;`radeon.si_support=0 amdgpu.si_support=1`&lt;/p&gt; &lt;p&gt;On Fedora/Nobara:&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;sudo sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;/GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;radeon.si_support=0 amdgpu.si_support=1 /' /etc/default/grub&lt;/p&gt; &lt;p&gt;sudo grub2-mkconfig -o /boot/grub2/grub.cfg&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;**2. Reboot**&lt;/p&gt; &lt;p&gt;`sudo reboot`&lt;/p&gt; &lt;p&gt;**3. Container Config (Crucial!)**&lt;/p&gt; &lt;p&gt;If you're running Ollama in a container (Podman or Docker), you MUST:&lt;/p&gt; &lt;p&gt;- Pass `/dev/dri` to the container.&lt;/p&gt; &lt;p&gt;- Set `OLLAMA_VULKAN=1`.&lt;/p&gt; &lt;p&gt;- Disable security labels (SecurityLabel=disable in Quadlet).&lt;/p&gt; &lt;p&gt;**Result:**&lt;/p&gt; &lt;p&gt;My D700s are now identified as **Vulkan0** and **Vulkan1** in Ollama logs, and they split the model VRAM perfectly! üöÄ&lt;/p&gt; &lt;p&gt;I've put together a GitHub-ready folder with scripts and configs here: [Link to your repo]&lt;/p&gt; &lt;p&gt;Hope this helps any fellow Trashcan owners out there trying to run local LLMs!&lt;/p&gt; &lt;p&gt;#MacPro #Linux #Ollama #SelfHosted #AMD #FireProD700&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/manu7irl"&gt; /u/manu7irl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd5ely/full_gpu_acceleration_for_ollama_on_mac_pro_2013/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd5ely/full_gpu_acceleration_for_ollama_on_mac_pro_2013/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rd5ely/full_gpu_acceleration_for_ollama_on_mac_pro_2013/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T04:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdjoo6</id>
    <title>Ollama Cloud Model Free Limit</title>
    <updated>2026-02-24T15:44:30+00:00</updated>
    <author>
      <name>/u/madmanari</name>
      <uri>https://old.reddit.com/user/madmanari</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rdjoo6/ollama_cloud_model_free_limit/"&gt; &lt;img alt="Ollama Cloud Model Free Limit" src="https://preview.redd.it/ese9rgoqqglg1.jpg?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=4ad9e01ae9b4659db3410adbd2cd5785fdb0c65e" title="Ollama Cloud Model Free Limit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama Free Limit around 3M per Day / 6M per Week..&lt;/p&gt; &lt;p&gt;#ollama&lt;/p&gt; &lt;p&gt;#openclaw&lt;/p&gt; &lt;p&gt;#ollamatokenprice&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madmanari"&gt; /u/madmanari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rdjoo6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdjoo6/ollama_cloud_model_free_limit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdjoo6/ollama_cloud_model_free_limit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T15:44:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdmak0</id>
    <title>PersonaPlex-7B on Apple Silicon: full-duplex speech-to-speech in native Swift (MLX)</title>
    <updated>2026-02-24T17:17:39+00:00</updated>
    <author>
      <name>/u/ivan_digital</name>
      <uri>https://old.reddit.com/user/ivan_digital</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivan_digital"&gt; /u/ivan_digital &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1rd94lb/personaplex7b_on_apple_silicon_fullduplex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdmak0/personaplex7b_on_apple_silicon_fullduplex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdmak0/personaplex7b_on_apple_silicon_fullduplex/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T17:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd8cu5</id>
    <title>Built an open-source Ollama/MLX/OpenAI benchmark and leaderboard site with in-app submissions. Trying to test and collect more data.</title>
    <updated>2026-02-24T06:07:36+00:00</updated>
    <author>
      <name>/u/peppaz</name>
      <uri>https://old.reddit.com/user/peppaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rd8cu5/built_an_opensource_ollamamlxopenai_benchmark_and/"&gt; &lt;img alt="Built an open-source Ollama/MLX/OpenAI benchmark and leaderboard site with in-app submissions. Trying to test and collect more data." src="https://preview.redd.it/tcn61r39rdlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=386c5ea0626fa055a8b52140758687ce7f84b8c9" title="Built an open-source Ollama/MLX/OpenAI benchmark and leaderboard site with in-app submissions. Trying to test and collect more data." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peppaz"&gt; /u/peppaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tcn61r39rdlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd8cu5/built_an_opensource_ollamamlxopenai_benchmark_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rd8cu5/built_an_opensource_ollamamlxopenai_benchmark_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T06:07:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd920r</id>
    <title>Built an app that connects Ollama to your clipboard with ‚å•C (macOS, open source)</title>
    <updated>2026-02-24T06:44:32+00:00</updated>
    <author>
      <name>/u/morning-cereals</name>
      <uri>https://old.reddit.com/user/morning-cereals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rd920r/built_an_app_that_connects_ollama_to_your/"&gt; &lt;img alt="Built an app that connects Ollama to your clipboard with ‚å•C (macOS, open source)" src="https://external-preview.redd.it/b3I0ajYxMzQxZWxnMRYe2Ofy5EHN92E2iHf3x_Xw6DIJymXP2cpGHUmRCdgH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eee47683fdc493bda6a62a1bc30963579bd0d86" title="Built an app that connects Ollama to your clipboard with ‚å•C (macOS, open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to skip the &lt;em&gt;copy ‚Üí switch window ‚Üí prompt + paste ‚Üí copy result ‚Üí paste&lt;/em&gt; loop and just have Ollama be &lt;em&gt;there&lt;/em&gt; whenever I copy something.&lt;/p&gt; &lt;p&gt;So I built Cai. Press Option+C on any text, it detects what you copied and shows actions powered by your Ollama models. It talks to your local server, so whatever model you're running just works.&lt;/p&gt; &lt;p&gt;Instead of sending everything to the LLM, it detects content type first:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üìç Addresses (Open in Maps)&lt;/li&gt; &lt;li&gt;üóìÔ∏è Meetings (Create Calendar Event)&lt;/li&gt; &lt;li&gt;üìù Short Text (Define, Reply, Explain)&lt;/li&gt; &lt;li&gt;üåç Long Text (Summarize, Translate)&lt;/li&gt; &lt;li&gt;üíª Code/JSON (Beautify, Explain)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can also trigger custom prompts on-the-fly, and save ones you reuse as shortcuts :)&lt;/p&gt; &lt;p&gt;It should automatically detect the Ollama endpoint after installing, otherwise you can manually set it via settings.&lt;br /&gt; This is free and open source project: &lt;a href="https://getcai.app/"&gt;https://getcai.app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been using Ministral 3B on my Macbook Air M2 16GB RAM, curious what works best for you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/morning-cereals"&gt; /u/morning-cereals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hhklml241elg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd920r/built_an_app_that_connects_ollama_to_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rd920r/built_an_app_that_connects_ollama_to_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T06:44:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdg1a8</id>
    <title>Stop treating every bug as ‚Äòhallucination‚Äô: a 16-problem atlas for Ollama + RAG</title>
    <updated>2026-02-24T13:19:24+00:00</updated>
    <author>
      <name>/u/StarThinker2025</name>
      <uri>https://old.reddit.com/user/StarThinker2025</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rdg1a8/stop_treating_every_bug_as_hallucination_a/"&gt; &lt;img alt="Stop treating every bug as ‚Äòhallucination‚Äô: a 16-problem atlas for Ollama + RAG" src="https://preview.redd.it/qasweqos0glg1.png?width=140&amp;amp;height=107&amp;amp;auto=webp&amp;amp;s=36eaf9cdbcd91b5c71f76376cdd00ab7fc9b2621" title="Stop treating every bug as ‚Äòhallucination‚Äô: a 16-problem atlas for Ollama + RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, i‚Äôve been building local RAG / agent stacks on top of Ollama for a while, and at some point I got tired of describing every bug as ‚Äúhallucination‚Äù or ‚ÄúRAG is trash‚Äù. so I did something slightly obsessive: I turned all the weird failure patterns into a fixed 16-slot ‚ÄúProblem Map‚Äù that I now use to debug my pipelines.&lt;/p&gt; &lt;p&gt;this map is now referenced in a few places (including the LlamaIndex RAG troubleshooting docs), but this post is not about ‚Äúlook at my repo‚Äù. it‚Äôs about: how to use a 16-problem checklist to debug your &lt;strong&gt;Ollama-based&lt;/strong&gt; RAG stack in a systematic way.&lt;/p&gt; &lt;p&gt;you don‚Äôt need to adopt any framework to use it. it is just text.&lt;/p&gt; &lt;h1&gt;0. link first, so you can skim while reading&lt;/h1&gt; &lt;p&gt;the map lives here as a single README:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;WFGY ProblemMap ‚Äì 16 real RAG / LLM failure modes with fixes&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md"&gt;https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;MIT-licensed, text only, no telemetry, no API.&lt;br /&gt; you can read it like a long blog post, or you can literally paste it into a model running under &lt;code&gt;ollama&lt;/code&gt; and ask it to reason with the map.&lt;/p&gt; &lt;h1&gt;1. what this ‚ÄúProblem Map‚Äù is (and is not)&lt;/h1&gt; &lt;p&gt;most tools we use for RAG focus on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;vector stores, indexes, retrievers&lt;/li&gt; &lt;li&gt;orchestration frameworks, agents, graphs&lt;/li&gt; &lt;li&gt;prompt templates and guardrails&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;this map is none of those.&lt;/p&gt; &lt;p&gt;it is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a &lt;strong&gt;16-slot failure catalog&lt;/strong&gt; for RAG / agent pipelines&lt;/li&gt; &lt;li&gt;each slot has: &lt;ul&gt; &lt;li&gt;a stable number (No.1 ‚Ä¶ No.16) that never changes&lt;/li&gt; &lt;li&gt;a short name you can say out loud&lt;/li&gt; &lt;li&gt;how it looks from the outside (user complaints, logs, traces)&lt;/li&gt; &lt;li&gt;which layer to inspect first (ingestion, embeddings, retriever, routing, reasoning)&lt;/li&gt; &lt;li&gt;a minimal structural fix that tends to stay fixed&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the basic idea is:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;instead of saying ‚Äúthe model hallucinated again‚Äù,&lt;br /&gt; you say ‚Äúthis is Problem No.3 plus a bit of No.7‚Äù,&lt;br /&gt; and you know roughly where to look.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;it works well with Ollama for a simple reason: a lot of us are already running everything locally, with multiple small components glued together. the map gives you a consistent vocabulary for the glue.&lt;/p&gt; &lt;h1&gt;2. where the ‚Äúsemantic firewall‚Äù sits: after vs before&lt;/h1&gt; &lt;p&gt;most of the ecosystem is about &lt;strong&gt;patching after the fact&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM gives a bad answer&lt;/li&gt; &lt;li&gt;you add a reranker, one more retrieval hop, another guardrail, maybe a second LLM call&lt;/li&gt; &lt;li&gt;your pipeline slowly becomes a jungle of patches&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the ProblemMap is designed as a &lt;strong&gt;semantic firewall before generation&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;you look at what the pipeline is about to do: &lt;ul&gt; &lt;li&gt;what got retrieved&lt;/li&gt; &lt;li&gt;how it was chunked and routed&lt;/li&gt; &lt;li&gt;how much of the user‚Äôs intent is actually covered&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;you classify the situation into one or two of the 16 problems&lt;/li&gt; &lt;li&gt;if the semantic state looks unstable, you loop, reset, or refuse to answer&lt;/li&gt; &lt;li&gt;only if things look healthy, you let the model speak&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;in other words:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;most people add more logic &lt;strong&gt;after&lt;/strong&gt; the model talks&lt;/li&gt; &lt;li&gt;this map helps you decide when the model should probably &lt;strong&gt;not&lt;/strong&gt; talk yet&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;you can still use whatever framework you like on top of Ollama (LlamaIndex, LangChain, custom Python, plain HTTP). the firewall is a way of thinking, not a competing framework.&lt;/p&gt; &lt;h1&gt;3. illusions vs reality in Ollama-based stacks&lt;/h1&gt; &lt;p&gt;here are a few patterns that keep coming up when people build local RAG / agents on top of Ollama.&lt;/p&gt; &lt;h1&gt;3.1 ‚Äúthe model is too small, it keeps hallucinating‚Äù&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;what it looks like&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you run a local 7B / 8B model through Ollama&lt;/li&gt; &lt;li&gt;user asks a detailed question about your docs&lt;/li&gt; &lt;li&gt;answer sounds fluent but misses critical constraints, or confidently mixes up two similar products&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;easy narrative:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;‚Äúthis tiny model is just dumb, I should upgrade.‚Äù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;what‚Äôs actually broken, very often:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;chunking ignores document structure&lt;/li&gt; &lt;li&gt;sections that belong together are split across different chunks&lt;/li&gt; &lt;li&gt;your retriever pulls top-k chunks that each have ‚Äúa bit of truth‚Äù, but never the full picture in one place&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;in the ProblemMap this is one of the ‚Äúsemantic chunking / segmentation‚Äù problems:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;from the outside it looks like hallucination&lt;/li&gt; &lt;li&gt;from the inside the retrieval was ‚Äúformally correct but semantically broken‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;practical fix directions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;switch from naive fixed-token chunking to structure-aware chunking (headings, sections, bullet groups)&lt;/li&gt; &lt;li&gt;ensure that logically indivisible blocks (like ‚Äúdefinition + exceptions‚Äù) live in the same chunk&lt;/li&gt; &lt;li&gt;for critical flows, add a pre-answer check that forces the model to list which clauses / IDs it sees in context, and refuse to answer if key ones are missing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the point is: this is a &lt;strong&gt;structural fix&lt;/strong&gt;, not a ‚Äúchange model‚Äù fix.&lt;/p&gt; &lt;h1&gt;3.2 ‚ÄúRAG is trash, my vector DB keeps pulling the wrong file‚Äù&lt;/h1&gt; &lt;p&gt;common setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you use Ollama as the LLM layer&lt;/li&gt; &lt;li&gt;embeddings come from somewhere (local or remote)&lt;/li&gt; &lt;li&gt;you toss them into qdrant / chroma / milvus / pgvector / faiss&lt;/li&gt; &lt;li&gt;your code is something like ‚Äútake top-k, join into a context, send to &lt;code&gt;ollama&lt;/code&gt;‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;symptoms:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;tiny wording changes cause completely different retrieved docs&lt;/li&gt; &lt;li&gt;sometimes an obviously relevant doc is ranked way down or missing&lt;/li&gt; &lt;li&gt;adding new files degrades older use cases&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;easy narrative:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;‚Äúthis vector DB sucks, I should switch, or embeddings are bad.‚Äù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;ProblemMap view:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;metric choice and normalization do not match the embedding family&lt;/li&gt; &lt;li&gt;blending multiple sources created index skew&lt;/li&gt; &lt;li&gt;query transformation step is pushing queries too far away from the actual semantics&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;in the map, this cluster lives under ‚Äúmetric / normalization mismatch‚Äù and ‚Äúindex &amp;amp; refresh skew‚Äù:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;it‚Äôs not that ‚Äúvector DB is low quality‚Äù&lt;/li&gt; &lt;li&gt;it‚Äôs that the geometry you assumed does not match what is actually stored&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;fix directions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;make sure metric (cosine / L2 / dot) matches the embedding‚Äôs training assumptions&lt;/li&gt; &lt;li&gt;normalize vectors consistently (or explicitly choose not to) across all indexes&lt;/li&gt; &lt;li&gt;monitor a small, fixed set of ‚Äúprobe queries‚Äù every time you change the corpus or indexing code, and compare their nearest neighbors over time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;again, the firewall idea is to name this as ‚Äúa metric / index problem‚Äù, not ‚ÄúRAG is trash‚Äù.&lt;/p&gt; &lt;h1&gt;3.3 ‚Äúmy agent sometimes just goes crazy‚Äù&lt;/h1&gt; &lt;p&gt;setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you run Ollama behind some orchestration: &lt;ul&gt; &lt;li&gt;small Python agent you wrote&lt;/li&gt; &lt;li&gt;or a graph in LlamaIndex / LangChain&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;tools include: &lt;ul&gt; &lt;li&gt;a docs index&lt;/li&gt; &lt;li&gt;an internal notes index&lt;/li&gt; &lt;li&gt;maybe a web search or code interpreter&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;sometimes it picks a beautiful chain, sometimes it does something that looks insane&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;easy narrative:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;‚Äúagents are chaotic toys, this is just the LLM being random.‚Äù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;ProblemMap view:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;tool / index routing spec is underspecified&lt;/li&gt; &lt;li&gt;a few edge cases are left completely to the LLM‚Äôs guess&lt;/li&gt; &lt;li&gt;there is no explicit ‚Äúresource policy‚Äù that says which sources are allowed for which intents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;symptoms:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;for a certain slice of queries, traces show the agent picking the same wrong tool over and over&lt;/li&gt; &lt;li&gt;retries bounce between totally different tool chains&lt;/li&gt; &lt;li&gt;prod and dev behave differently because some tools are wired slightly differently&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;this maps onto ‚Äúrouting contract mismatch‚Äù and ‚Äúsafety boundary leaks‚Äù in the ProblemMap.&lt;/p&gt; &lt;p&gt;fix directions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;explicitly classify queries into a small intent set first (even with a cheap classification model)&lt;/li&gt; &lt;li&gt;attach a strict ‚Äúallowed tools / indexes per intent‚Äù map, and apply hard filters before the LLM plans a chain&lt;/li&gt; &lt;li&gt;add a cheap pre-flight step that inspects the planned tool chain and vetoes it if it violates your policies (for example, ‚Äúnever use internal notes for external users‚Äù)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the key is: you fix the &lt;strong&gt;tool graph and routing spec&lt;/strong&gt;, not the answer wording.&lt;/p&gt; &lt;h1&gt;3.4 ‚Äúi fixed this last week, why is it broken again in a new place‚Äù&lt;/h1&gt; &lt;p&gt;this one is everywhere.&lt;/p&gt; &lt;p&gt;symptoms:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you fix a bug by adding a special case in a prompt or post-processor&lt;/li&gt; &lt;li&gt;users are happy for a week&lt;/li&gt; &lt;li&gt;then a new endpoint, new document set, or new type of question shows exactly the same failure pattern in a fresh coat of paint&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ProblemMap view:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you never gave the failure a stable name and number&lt;/li&gt; &lt;li&gt;you only treated it as ‚Äúthat one weird ticket from last week‚Äù&lt;/li&gt; &lt;li&gt;so when it reappears elsewhere, everyone treats it as brand new&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the 16-problem map forces you to say things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚Äúthis is yet another instance of No.4‚Äù&lt;/li&gt; &lt;li&gt;‚Äúthis incident smells like No.2 plus a bit of No.9‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;that may sound like a small naming trick, but over time it changes how teams debug.&lt;/p&gt; &lt;h1&gt;4. a concrete Ollama RAG example, step by step&lt;/h1&gt; &lt;p&gt;let‚Äôs make this less abstract.&lt;/p&gt; &lt;p&gt;say you have a simple local docs QA stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;documents in a &lt;code&gt;./docs&lt;/code&gt; folder (PDFs, Markdown, whatever)&lt;/li&gt; &lt;li&gt;you embed them and store vectors in your favorite DB&lt;/li&gt; &lt;li&gt;you wrap it with a small API that sends prompts to an Ollama model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;in pseudocode, the flow is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;user asks a question about a product, policy, or contract&lt;/li&gt; &lt;li&gt;you search the vector DB and fetch top-k chunks&lt;/li&gt; &lt;li&gt;you build a prompt like:&lt;/li&gt; &lt;li&gt;you call &lt;code&gt;ollama&lt;/code&gt; with that prompt&lt;/li&gt; &lt;li&gt;you stream back the answer&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;now imagine this incident:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;user asks: ‚Äúsummarize the warranty exclusions for product X‚Äù&lt;/li&gt; &lt;li&gt;you expect: a careful list of exclusions that match the PDF&lt;/li&gt; &lt;li&gt;you get: a very confident paragraph that sounds legal-ish, but: &lt;ul&gt; &lt;li&gt;misses a few critical clauses&lt;/li&gt; &lt;li&gt;mixes in a condition that belongs to product Y&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;how to run this through the 16-problem map:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;look at the retrieved chunks&lt;/strong&gt;, not just the answer &lt;ul&gt; &lt;li&gt;do we see all relevant sections for product X?&lt;/li&gt; &lt;li&gt;are chunks from product X and Y mixed?&lt;/li&gt; &lt;li&gt;are ‚Äúunless / except‚Äù clauses cut off at chunk boundaries?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;if chunks look bad: &lt;ul&gt; &lt;li&gt;this is mostly a chunking / segmentation issue (one of the early ProblemMap numbers)&lt;/li&gt; &lt;li&gt;maybe also an index organization issue (different product families not separated)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;fix in the pipeline: &lt;ul&gt; &lt;li&gt;change ingestion to split by headings or semantic units&lt;/li&gt; &lt;li&gt;create separate indexes per product family and add a very simple router&lt;/li&gt; &lt;li&gt;reduce &lt;code&gt;k&lt;/code&gt; once retrieval is more precise, to avoid mixing product lines&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;optionally, add a tiny semantic firewall check: &lt;ul&gt; &lt;li&gt;before answering, ask the model: &lt;ul&gt; &lt;li&gt;‚Äúlist all product names and IDs you see in the retrieved context‚Äù&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;if the list is more than one product family, refuse to answer and log it as a routing issue&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;once you‚Äôve done this once, you can label the incident as ‚ÄúProblemMap No.X‚Äù in your notes. the next time something similar happens on a different dataset, you‚Äôre not starting from zero.&lt;/p&gt; &lt;h1&gt;5. how to actually use the map with Ollama in practice&lt;/h1&gt; &lt;p&gt;you don‚Äôt need to implement a big framework. here are three simple modes that work well in a local setting.&lt;/p&gt; &lt;h1&gt;5.1 as a mental model + incident vocabulary&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;read the ProblemMap README once&lt;/li&gt; &lt;li&gt;when something weird happens, write a short incident note: &lt;ul&gt; &lt;li&gt;what the user asked&lt;/li&gt; &lt;li&gt;what you expected&lt;/li&gt; &lt;li&gt;what actually happened&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;skim the 16 problems and assign: &lt;ul&gt; &lt;li&gt;‚Äúthis feels like mostly No.A‚Äù&lt;/li&gt; &lt;li&gt;‚Äúthis smells like No.B + a bit of No.C‚Äù&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;put that in your commit message or issue title&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;over time you will see patterns:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚Äúwe hit No.3 and No.7 all the time‚Äù&lt;/li&gt; &lt;li&gt;‚Äúwe almost never see No.12‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;that is already useful.&lt;/p&gt; &lt;h1&gt;5.2 using an LLM (maybe through Ollama) as a triage helper&lt;/h1&gt; &lt;p&gt;if you are willing to involve an LLM in the debugging loop:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;take the ProblemMap README and load it into a strong model &lt;ul&gt; &lt;li&gt;can be local or remote&lt;/li&gt; &lt;li&gt;you can split it into chunks and keep it in a ‚Äúsystem memory‚Äù&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;for each tricky incident, feed: &lt;ul&gt; &lt;li&gt;the user query&lt;/li&gt; &lt;li&gt;the retrieved context&lt;/li&gt; &lt;li&gt;the answer&lt;/li&gt; &lt;li&gt;a short description of why it‚Äôs wrong&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;ask something like:&lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;‚ÄúUsing the 16-problem WFGY ProblemMap as ground truth, which problem numbers best explain this failure in my Ollama-based RAG pipeline, and what should I inspect first?‚Äù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;this is how many teams end up discovering that they have 2‚Äì3 ‚Äúfavorite failure modes‚Äù that dominate all their bug reports.&lt;/p&gt; &lt;h1&gt;5.3 turning it into a lightweight semantic firewall&lt;/h1&gt; &lt;p&gt;if you want to go a bit further, you can bake a small semantic firewall into your stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;add a pre-answer step: &lt;ul&gt; &lt;li&gt;inspect retrieved context and planned tool calls&lt;/li&gt; &lt;li&gt;optionally ask a model: ‚Äúdoes this look like any of the high-risk ProblemMap modes?‚Äù&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;if the answer is yes for some numbers you care about: &lt;ul&gt; &lt;li&gt;refuse to answer&lt;/li&gt; &lt;li&gt;or re-run retrieval with different parameters&lt;/li&gt; &lt;li&gt;or surface a warning to the caller&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;this does not require extra infra:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;it‚Äôs just another small function in your code&lt;/li&gt; &lt;li&gt;the ‚Äúfirewall spec‚Äù is the text of the ProblemMap itself&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the goal is not perfection. it‚Äôs to catch the worst ‚Äúlandmines‚Äù before they reach users.&lt;/p&gt; &lt;h1&gt;6. why I trust this map enough to share it here&lt;/h1&gt; &lt;p&gt;quick context so this doesn‚Äôt feel like a random checklist.&lt;/p&gt; &lt;p&gt;over time, this 16-problem view has been:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;adopted into the &lt;strong&gt;LlamaIndex RAG troubleshooting docs&lt;/strong&gt; as a structured failure-mode checklist&lt;/li&gt; &lt;li&gt;wrapped as a tool in &lt;strong&gt;Harvard MIMS Lab‚Äôs ToolUniverse&lt;/strong&gt; to triage LLM / RAG incidents by ProblemMap number&lt;/li&gt; &lt;li&gt;used as a failure taxonomy by the &lt;strong&gt;Rankify&lt;/strong&gt; project (University of Innsbruck)&lt;/li&gt; &lt;li&gt;listed as a practical debugging atlas in a &lt;strong&gt;Multimodal RAG survey&lt;/strong&gt; by QCRI‚Äôs LLM Lab&lt;/li&gt; &lt;li&gt;included in several curated ‚Äúawesome-style‚Äù lists under RAG / LLM debugging and reliability&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;all of that happened while keeping it very boring on purpose:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MIT license&lt;/li&gt; &lt;li&gt;text only&lt;/li&gt; &lt;li&gt;no binaries, no hosted API, no data collection&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;it‚Äôs basically a frozen notebook of hard-earned failures, cleaned up into 16 reusable patterns.&lt;/p&gt; &lt;h1&gt;7. what I‚Äôd love to learn from the Ollama community&lt;/h1&gt; &lt;p&gt;since many of the original incidents came from local / self-hosted stacks, it feels natural to bring this back to Ollama and see how it lands here.&lt;/p&gt; &lt;p&gt;if you are running:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;an Ollama-based RAG stack (with any vector DB)&lt;/li&gt; &lt;li&gt;a local agent / tool system&lt;/li&gt; &lt;li&gt;or just a bunch of scripts that glue Ollama to your own data&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôd be very curious about:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;which of the 16 problems you see the most in your own traces&lt;/li&gt; &lt;li&gt;which failure patterns you are hitting that don‚Äôt fit cleanly into any of the 16 slots&lt;/li&gt; &lt;li&gt;whether adding a bit of ‚Äúsemantic firewall before generation‚Äù feels realistic in your environment, or if your constraints make that too heavy&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;again, the entry point is just this README:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md"&gt;https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;if you have a weird incident and you‚Äôre comfortable sharing a redacted version, drop it in the comments and I can try to map it to ProblemMap numbers and suggest where in the stack to look first.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qasweqos0glg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3456b740e00b53b9469e5ed78ba44359e326e07c"&gt;16 Problem Map&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StarThinker2025"&gt; /u/StarThinker2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdg1a8/stop_treating_every_bug_as_hallucination_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdg1a8/stop_treating_every_bug_as_hallucination_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdg1a8/stop_treating_every_bug_as_hallucination_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T13:19:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdeex0</id>
    <title>GPU issue with running Models locally</title>
    <updated>2026-02-24T12:02:07+00:00</updated>
    <author>
      <name>/u/Badincomputer</name>
      <uri>https://old.reddit.com/user/Badincomputer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have setup my rig with 5 Geforce GTX titan X maxwell gpus. &lt;/p&gt; &lt;p&gt;Nvidia driver is 532. 7b models are running fine but when i try to run modes 30b or higher. I just loads on only one gpu amd then just crashed. I have installed 64 gb ram as well but i have having issues running bigger models. I think this is the software or driver issue. Can anyone help me please&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Badincomputer"&gt; /u/Badincomputer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdeex0/gpu_issue_with_running_models_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdeex0/gpu_issue_with_running_models_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdeex0/gpu_issue_with_running_models_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T12:02:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdriz3</id>
    <title>Ho creato un assistente vocale completamente offline per Windows, senza cloud e senza chiavi API</title>
    <updated>2026-02-24T20:23:00+00:00</updated>
    <author>
      <name>/u/Immediate-Ice-9989</name>
      <uri>https://old.reddit.com/user/Immediate-Ice-9989</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rdriz3/ho_creato_un_assistente_vocale_completamente/"&gt; &lt;img alt="Ho creato un assistente vocale completamente offline per Windows, senza cloud e senza chiavi API" src="https://external-preview.redd.it/MVpQDQPKITwW08B1LLAzfF0SGmW9RjJPZbT0m70bl8I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66c300a100fdabfc64a6425e8875a8655dc75d77" title="Ho creato un assistente vocale completamente offline per Windows, senza cloud e senza chiavi API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Immediate-Ice-9989"&gt; /u/Immediate-Ice-9989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/u_Immediate-Ice-9989/comments/1rdrazk/i_built_a_fully_offline_voice_assistant_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdriz3/ho_creato_un_assistente_vocale_completamente/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdriz3/ho_creato_un_assistente_vocale_completamente/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T20:23:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdl1fd</id>
    <title>What's the best model to run on mac m1 pro 16gb?</title>
    <updated>2026-02-24T16:33:37+00:00</updated>
    <author>
      <name>/u/Embarrassed-Baby3964</name>
      <uri>https://old.reddit.com/user/Embarrassed-Baby3964</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an old m1 mac pro with 16gb ram. Was wondering if there are any good performing models in 2026 that I can run on this hardware? And if so, what is the best one in your opinion?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Baby3964"&gt; /u/Embarrassed-Baby3964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdl1fd/whats_the_best_model_to_run_on_mac_m1_pro_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdl1fd/whats_the_best_model_to_run_on_mac_m1_pro_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdl1fd/whats_the_best_model_to_run_on_mac_m1_pro_16gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T16:33:37+00:00</published>
  </entry>
</feed>
