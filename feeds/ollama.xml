<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-03T11:23:05+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1okhoit</id>
    <title>Since 12.3 Ollama doesn‚Äôt work on CPU only, how has this not been fixed yet?</title>
    <updated>2025-10-31T01:24:56+00:00</updated>
    <author>
      <name>/u/RegularPerson2020</name>
      <uri>https://old.reddit.com/user/RegularPerson2020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It‚Äôs amazing to me that forever we have been able to run smollm2, granite3.1-3b, the micro qwens on modest cpu mini pcs, then bam they won‚Äôt load. It‚Äôs been on Reddit, discord, yet nothing. I‚Äôm not asking for a fix just put out an old version of Ollama and call it ‚Äúcpu shmucks‚Äù or something. No one with a 5090 is running Smollm2.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RegularPerson2020"&gt; /u/RegularPerson2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okhoit/since_123_ollama_doesnt_work_on_cpu_only_how_has/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okhoit/since_123_ollama_doesnt_work_on_cpu_only_how_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okhoit/since_123_ollama_doesnt_work_on_cpu_only_how_has/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-31T01:24:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol1u0r</id>
    <title>I'm currently solving a problem I have with ollama and lmstudio.</title>
    <updated>2025-10-31T18:07:33+00:00</updated>
    <author>
      <name>/u/Sileniced</name>
      <uri>https://old.reddit.com/user/Sileniced</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sileniced"&gt; /u/Sileniced &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ol1px9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ol1u0r/im_currently_solving_a_problem_i_have_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ol1u0r/im_currently_solving_a_problem_i_have_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-31T18:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1okw7yq</id>
    <title>Which open-source LLMs support schema?</title>
    <updated>2025-10-31T14:33:20+00:00</updated>
    <author>
      <name>/u/ThingRexCom</name>
      <uri>https://old.reddit.com/user/ThingRexCom</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThingRexCom"&gt; /u/ThingRexCom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1okw7jh/which_opensource_llms_support_schema/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okw7yq/which_opensource_llms_support_schema/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okw7yq/which_opensource_llms_support_schema/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-31T14:33:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1okw3i5</id>
    <title>How to Create a Personalized AI (Free &amp; Easy Guide). I made this English blog post after you told me my Spanish video wasn't accessible. Hope this helps!</title>
    <updated>2025-10-31T14:28:27+00:00</updated>
    <author>
      <name>/u/jokiruiz</name>
      <uri>https://old.reddit.com/user/jokiruiz</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jokiruiz"&gt; /u/jokiruiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1okw32m/how_to_create_a_personalized_ai_free_easy_guide_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okw3i5/how_to_create_a_personalized_ai_free_easy_guide_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okw3i5/how_to_create_a_personalized_ai_free_easy_guide_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-31T14:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1okbuub</id>
    <title>Your Ollama models just got a data analysis superpower - query 10GB files locally with your models</title>
    <updated>2025-10-30T21:11:47+00:00</updated>
    <author>
      <name>/u/Sea-Assignment6371</name>
      <uri>https://old.reddit.com/user/Sea-Assignment6371</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1okbuub/your_ollama_models_just_got_a_data_analysis/"&gt; &lt;img alt="Your Ollama models just got a data analysis superpower - query 10GB files locally with your models" src="https://external-preview.redd.it/dTdtMzhlYzZlYnlmMWlS5XgjX4Ps-WSRdX4z1sYfrnkFwn3pUrSEA6TnxsyB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66f98e16de7e1767f766dd2bf427ddbf17c3e3eb" title="Your Ollama models just got a data analysis superpower - query 10GB files locally with your models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;Built something for the local AI community - DataKit Assistant with native Ollama integration.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The combo:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Your local Ollama models + massive dataset analysis&lt;/p&gt; &lt;p&gt;- Query 10GB+ CSV/Parquet files entirely offline&lt;/p&gt; &lt;p&gt;- SQL + Python notebooks + AI assistance&lt;/p&gt; &lt;p&gt;- Zero cloud dependencies, zero uploads&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Perfect for:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Analyzing sensitive data with your own models&lt;/p&gt; &lt;p&gt;- Learning data analysis with AI guidance (completely private)&lt;/p&gt; &lt;p&gt;- Prototyping without API costs&lt;/p&gt; &lt;p&gt;Works with any Ollama model that handles structured data well.&lt;/p&gt; &lt;p&gt;Try it: &lt;a href="https://datakit.page"&gt;https://datakit.page&lt;/a&gt; and let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Assignment6371"&gt; /u/Sea-Assignment6371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6va4lec6ebyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okbuub/your_ollama_models_just_got_a_data_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okbuub/your_ollama_models_just_got_a_data_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-30T21:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1olk9fm</id>
    <title>what ollama model should i run</title>
    <updated>2025-11-01T10:14:46+00:00</updated>
    <author>
      <name>/u/Efficient_Roll4891</name>
      <uri>https://old.reddit.com/user/Efficient_Roll4891</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what ollama model should i run on my macbook air m1 with 16gb ram&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Efficient_Roll4891"&gt; /u/Efficient_Roll4891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1olk9fm/what_ollama_model_should_i_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1olk9fm/what_ollama_model_should_i_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1olk9fm/what_ollama_model_should_i_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-01T10:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1okw9ts</id>
    <title>TreeThinkerAgent, an open-source reasoning agent using LLMs + tools</title>
    <updated>2025-10-31T14:35:26+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1okw9ts/treethinkeragent_an_opensource_reasoning_agent/"&gt; &lt;img alt="TreeThinkerAgent, an open-source reasoning agent using LLMs + tools" src="https://preview.redd.it/fa8u7btlkgyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3283c979b595bb7dbdc1c65c867dd3721993b884" title="TreeThinkerAgent, an open-source reasoning agent using LLMs + tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone üëã&lt;/p&gt; &lt;p&gt;I‚Äôve just released &lt;a href="https://github.com/Bessouat40/TreeThinkerAgent"&gt;&lt;strong&gt;TreeThinkerAgent&lt;/strong&gt;&lt;/a&gt;, a minimalist app built from scratch without any framework to explore &lt;strong&gt;multi-step reasoning&lt;/strong&gt; with LLMs using different providers including Ollama.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does it do?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This LLM application :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Plans a list of reasoning&lt;/li&gt; &lt;li&gt;Executes any needed tools per step&lt;/li&gt; &lt;li&gt;Builds a full &lt;strong&gt;reasoning tree&lt;/strong&gt; to make each decision traceable&lt;/li&gt; &lt;li&gt;Produces a final, professional summary as output&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I wanted something clean and understandable to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Play with &lt;strong&gt;autonomous agent planning&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Prototype research assistants that don‚Äôt rely on heavy infra&lt;/li&gt; &lt;li&gt;Focus on &lt;strong&gt;agentic logic&lt;/strong&gt;, not on tool integration complexity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Üí &lt;a href="https://github.com/Bessouat40/TreeThinkerAgent"&gt;https://github.com/Bessouat40/TreeThinkerAgent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think : feedback, ideas, improvements all welcome!TreeThinkerAgent, an open-source reasoning agent using LLMs + tools&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fa8u7btlkgyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okw9ts/treethinkeragent_an_opensource_reasoning_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okw9ts/treethinkeragent_an_opensource_reasoning_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-31T14:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1okxpa3</id>
    <title>10x slower Qwen3 and 2.5 VL</title>
    <updated>2025-10-31T15:30:39+00:00</updated>
    <author>
      <name>/u/RIP26770</name>
      <uri>https://old.reddit.com/user/RIP26770</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen 2.5 VL and Qwen 3 VL have become so slow since the last update that they are barely usable!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIP26770"&gt; /u/RIP26770 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okxpa3/10x_slower_qwen3_and_25_vl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okxpa3/10x_slower_qwen3_and_25_vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okxpa3/10x_slower_qwen3_and_25_vl/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-31T15:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oluzux</id>
    <title>What‚Äôs the best uncensored model for Ollama?</title>
    <updated>2025-11-01T18:21:30+00:00</updated>
    <author>
      <name>/u/mihirfriends20</name>
      <uri>https://old.reddit.com/user/mihirfriends20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can anyone suggestion me that which model are full uncensored, i am trying some automation and i want to use this all words: pussy,ass,anal,cum,fucked,boobs, nipple etc. &lt;/p&gt; &lt;p&gt;thanks in advance &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mihirfriends20"&gt; /u/mihirfriends20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oluzux/whats_the_best_uncensored_model_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oluzux/whats_the_best_uncensored_model_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oluzux/whats_the_best_uncensored_model_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-01T18:21:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1okw14t</id>
    <title>A quick update on Nanocoder and the Nano Collective üòÑ</title>
    <updated>2025-10-31T14:25:46+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1okw14t/a_quick_update_on_nanocoder_and_the_nano/"&gt; &lt;img alt="A quick update on Nanocoder and the Nano Collective üòÑ" src="https://preview.redd.it/mxzpa3kk0gyf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=d91fd17cb5daeee2776b5ac5f3057b653d8a07ba" title="A quick update on Nanocoder and the Nano Collective üòÑ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;As is becoming a thing, I just wanted to share an update post on Nanocoder, the open-source, open-community coding CLI as well as the Nano Collective, the community behind building it!&lt;/p&gt; &lt;p&gt;Over the last few weeks we've been steadily growing, continuing to build out our vision for community-led, privacy-first and open source AI. &lt;/p&gt; &lt;p&gt;&lt;em&gt;Here are a couple of highlights:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Nanocoder&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We've just surpassed 750 stars on the GitHub repo with the number growing every day.&lt;/li&gt; &lt;li&gt;We're continuing to refine the software and make it better with several big updates to configuration. One of the common complaints was that configuring Nanocoder was pretty hard so now there's a configuration wizard built right into the CLI to help you set them up easily!&lt;/li&gt; &lt;li&gt;We released a new package called &lt;a href="https://github.com/Nano-Collective/get-md"&gt;get-md&lt;/a&gt; - this takes any website URL or HTML content and processes it into LLM optimized markdown. This is a great package which we'll continue to expand as another step towards privacy-focused AI.&lt;/li&gt; &lt;li&gt;We're about to begin training our own tiny models to offset some of the work within Nanocoder. For example, we're experimenting with a tiny language model that converts questions to bash commands. Hopefully an update soon on this and we'll fully open source it as well. The aim here to keep as much processing on device without having to rely on large models in the cloud.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Nano Collective&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This is all setup now and we have a basic website here: &lt;a href="https://nanocollective.org"&gt;https://nanocollective.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;We want to welcome everyone here to drive discussions and ideas.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you to everyone that is getting involved and supporting the project. As I've said previously, it's early days but direction, improvements and growth is happening every day. The vision has always been to build private, local-first AI for the community and it's amazing to be building one where so many people are getting involved üòä&lt;/p&gt; &lt;p&gt;That being said, any help within any domain is appreciated and welcomed.&lt;/p&gt; &lt;p&gt;If you want to get involved the links are below.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Link&lt;/strong&gt;: &lt;a href="https://github.com/Nano-Collective/nanocoder"&gt;https://github.com/Nano-Collective/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord Link&lt;/strong&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mxzpa3kk0gyf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1okw14t/a_quick_update_on_nanocoder_and_the_nano/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1okw14t/a_quick_update_on_nanocoder_and_the_nano/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-31T14:25:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1olxhbb</id>
    <title>Airplane mode in Ollama on Ubuntu Server?</title>
    <updated>2025-11-01T20:01:49+00:00</updated>
    <author>
      <name>/u/DocSchaub</name>
      <uri>https://old.reddit.com/user/DocSchaub</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;since the new search and Online Options is active, i set my Ollama on airplane mode on windows for privacy reasons. .&lt;br /&gt; Has anyone figured out how to do that on Linux (Specifically Ubuntu) in CLI.&lt;br /&gt; Is it even an issue there?&lt;/p&gt; &lt;p&gt;Thanks for your insights.&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;p&gt;DocSchaub&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocSchaub"&gt; /u/DocSchaub &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1olxhbb/airplane_mode_in_ollama_on_ubuntu_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1olxhbb/airplane_mode_in_ollama_on_ubuntu_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1olxhbb/airplane_mode_in_ollama_on_ubuntu_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-01T20:01:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1omdjpx</id>
    <title>I‚Äôm currently trying to develop a WordPress plugin using Chatgpt Pro</title>
    <updated>2025-11-02T10:03:39+00:00</updated>
    <author>
      <name>/u/mihirfriends20</name>
      <uri>https://old.reddit.com/user/mihirfriends20</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1omdjpx/im_currently_trying_to_develop_a_wordpress_plugin/"&gt; &lt;img alt="I‚Äôm currently trying to develop a WordPress plugin using Chatgpt Pro" src="https://preview.redd.it/36ys95jphtyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78775da824bbaf33891d4de870cb2eab6ac64071" title="I‚Äôm currently trying to develop a WordPress plugin using Chatgpt Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm using the &lt;strong&gt;ChatGPT Pro&lt;/strong&gt; version and currently developing a &lt;strong&gt;WordPress plugin&lt;/strong&gt;. However, it doesn‚Äôt allow me to bypass safety filters or generate adult or explicit content. Could you please suggest what I should do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mihirfriends20"&gt; /u/mihirfriends20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/36ys95jphtyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1omdjpx/im_currently_trying_to_develop_a_wordpress_plugin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1omdjpx/im_currently_trying_to_develop_a_wordpress_plugin/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-02T10:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1om6f2z</id>
    <title>Pro R9700 build</title>
    <updated>2025-11-02T02:47:16+00:00</updated>
    <author>
      <name>/u/Glittering_Ease4630</name>
      <uri>https://old.reddit.com/user/Glittering_Ease4630</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1om6f2z/pro_r9700_build/"&gt; &lt;img alt="Pro R9700 build" src="https://preview.redd.it/nh0m9mq2cryf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca465ecf363962c30fc1c99eb11e40764e1e2814" title="Pro R9700 build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering_Ease4630"&gt; /u/Glittering_Ease4630 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nh0m9mq2cryf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1om6f2z/pro_r9700_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1om6f2z/pro_r9700_build/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-02T02:47:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1olj3m5</id>
    <title>What local models do you use for coding?</title>
    <updated>2025-11-01T08:59:28+00:00</updated>
    <author>
      <name>/u/LaFllamme</name>
      <uri>https://old.reddit.com/user/LaFllamme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I have been playing with AI for a while but right now I am mostly exploring what is actually possible locally in combination with local tools. I want to plug a local model into the editor and see how far I can get without calling an external API or service!&lt;/p&gt; &lt;p&gt;My setup at the moment is a MacBook with M4 and 16 GB RAM&lt;br /&gt; I run stuff either through Ollama or LM Studio like tools.&lt;/p&gt; &lt;p&gt;So far I tried out these models for coding:&lt;br /&gt; Qwen3 VL 8B in 4 bit&lt;br /&gt; Deepseek R1 0528 Qwen3 8B in 4 bit&lt;br /&gt; Qwen3 4B Thinking 2507 in 4 bit &lt;/p&gt; &lt;p&gt;Gemma and Mistral are on the list but I did not test them properly yet&lt;/p&gt; &lt;p&gt;What I would like to know is, which models you are using for local coding on which hardware and if you have some settings that made a difference like context window or temperature.&lt;/p&gt; &lt;p&gt;Im just wondering if anyone experienced a very good usage with a given model in explicit programming context.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LaFllamme"&gt; /u/LaFllamme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1olj3m5/what_local_models_do_you_use_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1olj3m5/what_local_models_do_you_use_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1olj3m5/what_local_models_do_you_use_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-01T08:59:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1olmjry</id>
    <title>Made my first AI Agent Researcher with Python + Langchain + Ollama</title>
    <updated>2025-11-01T12:27:28+00:00</updated>
    <author>
      <name>/u/FriendshipCreepy8045</name>
      <uri>https://old.reddit.com/user/FriendshipCreepy8045</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1olmjry/made_my_first_ai_agent_researcher_with_python/"&gt; &lt;img alt="Made my first AI Agent Researcher with Python + Langchain + Ollama" src="https://preview.redd.it/hv4kpzlwymyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43007a490607e85807ec1ca6a719f6b32c882cde" title="Made my first AI Agent Researcher with Python + Langchain + Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;br /&gt; So I always wondered how AI agent worked and as a Frontend Engineer, I use copilot agent everyday for personalprofessional projects and always wondered &amp;quot;how the hack it decides what files to read, write, what cmd commands to execute, how the hack did it called my terminal and ran (npm run build)&amp;quot;&lt;/p&gt; &lt;p&gt;And in a week i can't complitely learn about how transformers work or embeddings algorithim store and retrive data but i can learn something high level, to code something high level to post something low level ü•≤&lt;/p&gt; &lt;p&gt;So I built a small &lt;strong&gt;local research agent&lt;/strong&gt; with a few simple tools:&lt;br /&gt; it runs entirely offline, uses a local LLM through &lt;strong&gt;Ollama&lt;/strong&gt;, connects tools via &lt;strong&gt;LangChain&lt;/strong&gt;, and stores memory using &lt;strong&gt;ChromaDB&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;Basically, it‚Äôs my attempt to understand how an AI agent &lt;em&gt;thinks, reasons, and remembers.&lt;/em&gt; but built from scratch in my own style.&lt;br /&gt; Do check and let me know what you guys thing, how i can improve this agent in terms of prompt | code structure or anything :)&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/vedas-dixit/LocalAgent"&gt;https://github.com/vedas-dixit/LocalAgent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Documentation: &lt;a href="https://github.com/vedas-dixit/LocalAgent/blob/main/documentation.md"&gt;https://github.com/vedas-dixit/LocalAgent/blob/main/documentation.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FriendshipCreepy8045"&gt; /u/FriendshipCreepy8045 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hv4kpzlwymyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1olmjry/made_my_first_ai_agent_researcher_with_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1olmjry/made_my_first_ai_agent_researcher_with_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-01T12:27:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1om33uq</id>
    <title>LlamaPen now supports custom tool calling</title>
    <updated>2025-11-02T00:03:55+00:00</updated>
    <author>
      <name>/u/DarkTom21</name>
      <uri>https://old.reddit.com/user/DarkTom21</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1om33uq/llamapen_now_supports_custom_tool_calling/"&gt; &lt;img alt="LlamaPen now supports custom tool calling" src="https://external-preview.redd.it/NDd3Mzd2eXpmcXlmMR53Pr_qj2-qFfnGx9gtpVYk0-Q8wdJqE8Pmx-1oUu0J.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=522b87d55e5073f217e851a08b5f31aebe6c4b49" title="LlamaPen now supports custom tool calling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;A while ago I showcased here the first version of LlamaPen, an open-source web interface for Ollama, and since then I have been continuously polishing and adding new features to make it as convenient to use as possible. Recently I've reached a new milestone with the addition of tool calling support, allowing you to add your own tools and integrations into LlamaPen.&lt;/p&gt; &lt;p&gt;Tool calling works by letting you setup a custom URL to send requests to, letting the LLM set the request parameters/body for each request, and optionally letting you format the response before it gets returned back and added into the chat as context.&lt;/p&gt; &lt;p&gt;Ever since I've made my first post here I've been awestruck by the amount of support that has been given in the form of GitHub stars and interaction, and I hope that people continue to find this as useful as I do.&lt;/p&gt; &lt;p&gt;As before, the GitHub repo is available at &lt;a href="https://github.com/ImDarkTom/LlamaPen"&gt;https://github.com/ImDarkTom/LlamaPen&lt;/a&gt;, with the official instance at &lt;a href="https://llamapen.app/"&gt;https://llamapen.app/&lt;/a&gt;, and if you want to setup web search like is showcased in the demo, you can do that so &lt;a href="https://github.com/ImDarkTom/LlamaPen-Search"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Once again, thanks for reading, and I hope you find this useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkTom21"&gt; /u/DarkTom21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xqi8mlyzfqyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1om33uq/llamapen_now_supports_custom_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1om33uq/llamapen_now_supports_custom_tool_calling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-02T00:03:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1omi6o8</id>
    <title>HELP! Ollama Success But Stuck At Loading</title>
    <updated>2025-11-02T14:08:26+00:00</updated>
    <author>
      <name>/u/Han53l</name>
      <uri>https://old.reddit.com/user/Han53l</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1omi6o8/help_ollama_success_but_stuck_at_loading/"&gt; &lt;img alt="HELP! Ollama Success But Stuck At Loading" src="https://external-preview.redd.it/d3U5eGd2dHVvdXlmMY-6CfOXvDjdwzJaILkyg5qlWayHBzagz4j9GQN09tIc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5381ce3f4ea2fc2e16315b3c9f5841956f4d725f" title="HELP! Ollama Success But Stuck At Loading" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use the &amp;quot;ollama run tinyllama&amp;quot;, but it kept getting stuck at the loading after success (other models also does this). &lt;/p&gt; &lt;p&gt;I installed ollama before, and it can run deepseek-coder and phi3:mini just fine. &lt;/p&gt; &lt;p&gt;I recently reset my PC and installed Ollama again but not it doesn't work, can someone tell me how I can fix this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Han53l"&gt; /u/Han53l &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6qrqp8xuouyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1omi6o8/help_ollama_success_but_stuck_at_loading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1omi6o8/help_ollama_success_but_stuck_at_loading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-02T14:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oms58w</id>
    <title>Hardware recommendation please: new device or external solution?</title>
    <updated>2025-11-02T20:36:38+00:00</updated>
    <author>
      <name>/u/cnkrc</name>
      <uri>https://old.reddit.com/user/cnkrc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I have Nuc14 pro Asus for my Home Assistant setup, but it is not enough for voice commands locally.&lt;br /&gt; So, what do you guys recommend good solution run models locally?&lt;br /&gt; 1. I have Mac Mini M4pro with 24GB RAM, this could be an option for some models am I right?&lt;br /&gt; 2. I can buy any external device to atach my Nuc14 pro&lt;br /&gt; 3. I can buy a new mini pc and/or device to run with good result.&lt;br /&gt; Thank you very much.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cnkrc"&gt; /u/cnkrc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oms58w/hardware_recommendation_please_new_device_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oms58w/hardware_recommendation_please_new_device_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oms58w/hardware_recommendation_please_new_device_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-02T20:36:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1omjjbv</id>
    <title>Thread vs. Session based short-term memory</title>
    <updated>2025-11-02T15:02:43+00:00</updated>
    <author>
      <name>/u/Far-Photo4379</name>
      <uri>https://old.reddit.com/user/Far-Photo4379</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been looking into how local agents handle short-term memory and noticed two main approaches: &lt;strong&gt;thread-based&lt;/strong&gt; and &lt;strong&gt;session-based&lt;/strong&gt;. Both aim to preserve context across turns, but their structure and persistence differ which makes me wonder which approach is actually cleaner/better.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thread-based approach&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/MehKh-Analysis/ReAct-Agent-with-Memory"&gt;This agent&lt;/a&gt; is built on the ReAct architecture and integrates Ollama with the Llama 3.2 model for reasoning and tool-based actions. The short-term memory is thread-specific, keeping a rolling buffer of messages within a conversation. Once the thread ends, the memory resets. It‚Äôs simple, lightweight, and well-suited for contained chat sessions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Session-based approach&lt;/strong&gt;&lt;br /&gt; Session-based memory maintains a shared state across the entire session, independent of threads. Instead of relying on a message buffer, it tracks contextual entities and interactions so agents or tools can reuse that state. &lt;a href="https://www.cognee.ai/"&gt;Cognee&lt;/a&gt; is one example where this design enables multiple agents to share a unified context within a session, while long-term semantic memory is managed separately through embeddings and ontological links.&lt;/p&gt; &lt;p&gt;What do you think, would you define short-term memory differently or am I missing something? I feel like session-based is better for multi-agent setups but thread-based is simply faster, easier to implement and more convenient for back-and-forth chatbot applications.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Photo4379"&gt; /u/Far-Photo4379 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1omjjbv/thread_vs_session_based_shortterm_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1omjjbv/thread_vs_session_based_shortterm_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1omjjbv/thread_vs_session_based_shortterm_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-02T15:02:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1omf5ee</id>
    <title>Next evolution of agentic memory</title>
    <updated>2025-11-02T11:42:32+00:00</updated>
    <author>
      <name>/u/Any-Cockroach-3233</name>
      <uri>https://old.reddit.com/user/Any-Cockroach-3233</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every new AI startup says they've &amp;quot;solved memory&amp;quot;&lt;/p&gt; &lt;p&gt;99% of them just dump text into a vector DB&lt;/p&gt; &lt;p&gt;I wrote about why that approach is broken, and how agents can build human-like memory instead&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://manthanguptaa.in/posts/towards_human_like_memory_for_ai_agents/"&gt;https://manthanguptaa.in/posts/towards_human_like_memory_for_ai_agents/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Cockroach-3233"&gt; /u/Any-Cockroach-3233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1omf5ee/next_evolution_of_agentic_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1omf5ee/next_evolution_of_agentic_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1omf5ee/next_evolution_of_agentic_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-02T11:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1on7sq2</id>
    <title>Can i somehow connect the ollama gui to my remote server?</title>
    <updated>2025-11-03T10:00:09+00:00</updated>
    <author>
      <name>/u/Messyextacy</name>
      <uri>https://old.reddit.com/user/Messyextacy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Messyextacy"&gt; /u/Messyextacy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1on7sq2/can_i_somehow_connect_the_ollama_gui_to_my_remote/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1on7sq2/can_i_somehow_connect_the_ollama_gui_to_my_remote/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1on7sq2/can_i_somehow_connect_the_ollama_gui_to_my_remote/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-03T10:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1omxj5g</id>
    <title>What model do you use to transcribe videos?</title>
    <updated>2025-11-03T00:24:23+00:00</updated>
    <author>
      <name>/u/AirportAcceptable522</name>
      <uri>https://old.reddit.com/user/AirportAcceptable522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So guys, how are you?&lt;/p&gt; &lt;p&gt;I'm not sure which model I can use to transcribe videos, which one would you recommend to use on the machine?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AirportAcceptable522"&gt; /u/AirportAcceptable522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1omxj5g/what_model_do_you_use_to_transcribe_videos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1omxj5g/what_model_do_you_use_to_transcribe_videos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1omxj5g/what_model_do_you_use_to_transcribe_videos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-03T00:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ompo4t</id>
    <title>TreeThinkerAgent : UI Update</title>
    <updated>2025-11-02T19:00:17+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ompo4t/treethinkeragent_ui_update/"&gt; &lt;img alt="TreeThinkerAgent : UI Update" src="https://external-preview.redd.it/a2lzeGdtb2k1d3lmMbDWimRkn0jd8-nhfiwI9sUY7kTZ1aYtNE_bmUfANVp8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efe7429c6191f7e198e4792216c472209750141e" title="TreeThinkerAgent : UI Update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone üëã&lt;/p&gt; &lt;p&gt;I‚Äôve just upgraded &lt;a href="https://github.com/Bessouat40/TreeThinkerAgent"&gt;&lt;strong&gt;TreeThinkerAgent&lt;/strong&gt;&lt;/a&gt; UI.&lt;/p&gt; &lt;p&gt;It's a minimalist app built from scratch without any framework to explore &lt;strong&gt;multi-step reasoning&lt;/strong&gt; with LLMs. You can use Ollama as a provider ü•≥&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does it do?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This LLM application :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Plans a list of reasoning&lt;/li&gt; &lt;li&gt;Executes any needed tools per step&lt;/li&gt; &lt;li&gt;Builds a full &lt;strong&gt;reasoning tree&lt;/strong&gt; to make each decision traceable&lt;/li&gt; &lt;li&gt;Produces a final, professional summary as output&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Üí &lt;a href="https://github.com/Bessouat40/TreeThinkerAgent"&gt;https://github.com/Bessouat40/TreeThinkerAgent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think : feedback, ideas, improvements all welcome ! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p7jk7moi5wyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ompo4t/treethinkeragent_ui_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ompo4t/treethinkeragent_ui_update/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-02T19:00:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1omtzuz</id>
    <title>If ram is not the issue what model would you run for coding?</title>
    <updated>2025-11-02T21:50:19+00:00</updated>
    <author>
      <name>/u/wylywade</name>
      <uri>https://old.reddit.com/user/wylywade</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ended up with 2 Rdx 6000 pros with 96gb ram. I am looking at what could I do to make these things cry? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wylywade"&gt; /u/wylywade &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1omtzuz/if_ram_is_not_the_issue_what_model_would_you_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1omtzuz/if_ram_is_not_the_issue_what_model_would_you_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1omtzuz/if_ram_is_not_the_issue_what_model_would_you_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-02T21:50:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1on60b8</id>
    <title>Ollama no longer uses 780M Radeon GPU, now 100% CPU after update models / update ollama</title>
    <updated>2025-11-03T08:01:06+00:00</updated>
    <author>
      <name>/u/patach</name>
      <uri>https://old.reddit.com/user/patach</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running a Beelink SER8 AMD Ryzen‚Ñ¢ 7 8845HS with 96 GB of Ram. I have allocated 16gb to my vram, and my setup was working with ollama quite well with the rocm image through Docker / Linux Mint.&lt;/p&gt; &lt;p&gt;Then a couple of days ago, I was pulling a new model into open webui and saw the little button on there to 'update all models', curiously I clicked it...pulled my model in and tried it... only to have even a 4b inference model (qwen3-vl:4b) take forever. &lt;/p&gt; &lt;p&gt;I started going to all of my models, and all of them (asides from gemma 2b) took forever, or it would just hang and give up. &lt;/p&gt; &lt;p&gt;Inference models could hardly function. What used to be within seconds was now taking 15-20 minutes.&lt;/p&gt; &lt;p&gt;I did some look into it, and found the ollama ps was revealing a 100% CPU usage and no GPU usage at all. Which probably explains why even 4b models were struggling.&lt;/p&gt; &lt;p&gt;Logs also from my interpretation... is not able to find the GPU at all. &lt;/p&gt; &lt;p&gt;Logs:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;time=2025-11-03T07:50:35.745Z level=INFO source=routes.go:1524 msg=&amp;quot;server config&amp;quot; &amp;gt;env=&amp;quot;map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: &amp;gt;HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION:11.0.0 HTTPS_PROXY: HTTP_PROXY: NO_PROXY: &amp;gt;OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:DEBUG OLLAMA_FLASH_ATTENTION:false &amp;gt;OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:&lt;a href="http://0.0.0.0:11434"&gt;http://0.0.0.0:11434&lt;/a&gt; OLLAMA_INTEL_GPU:false &amp;gt;OLLAMA_KEEP_ALIVE:24h0m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: &amp;gt;OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 &amp;gt;OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false &amp;gt;OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* &lt;a href="http://127.0.0.1"&gt;http://127.0.0.1&lt;/a&gt; &lt;a href="https://127.0.0.1"&gt;https://127.0.0.1&lt;/a&gt; &lt;a href="http://127.0.0.1:*"&gt;http://127.0.0.1:*&lt;/a&gt; &lt;a href="https://127.0.0.1:*"&gt;https://127.0.0.1:*&lt;/a&gt; &lt;a href="http://0.0.0.0"&gt;http://0.0.0.0&lt;/a&gt; &lt;a href="https://0.0.0.0"&gt;https://0.0.0.0&lt;/a&gt; &lt;a href="http://0.0.0.0:*"&gt;http://0.0.0.0:*&lt;/a&gt; &lt;a href="https://0.0.0.0:*"&gt;https://0.0.0.0:*&lt;/a&gt; app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-11-03T07:50:35.748Z level=INFO source=images.go:522 msg=&amp;quot;total blobs: 82&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-11-03T07:50:35.749Z level=INFO source=images.go:529 msg=&amp;quot;total unused blobs removed: 0&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;t&amp;gt;ime=2025-11-03T07:50:35.750Z level=INFO source=routes.go:1577 msg=&amp;quot;Listening on [::]:11434 (version 0.12.9)&amp;quot;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;time=2025-11-03T07:50:35.750Z level=DEBUG source=sched.go:120 msg=&amp;quot;starting llm scheduler&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-11-03T07:50:35.750Z level=INFO source=runner.go:76 msg=&amp;quot;discovering available GPUs...&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-11-03T07:50:35.750Z level=INFO source=server.go:400 msg=&amp;quot;starting runner&amp;quot; cmd=&amp;quot;/usr/bin/ollama runner --ollama-engine --port 39943&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-11-03T07:50:35.750Z level=DEBUG source=server.go:401 msg=subprocess &amp;gt;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin OLLAMA_DEBUG=1 OLLAMA_KEEP_ALIVE=24h &amp;gt;HSA_OVERRIDE_GFX_VERSION=&amp;quot;\&amp;quot;11.0.0\&amp;quot;&amp;quot; &amp;gt;LD_LIBRARY_PATH=/usr/lib/ollama:/usr/lib/ollama/rocm:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 &amp;gt;OLLAMA_HOST=0.0.0.0:11434 OLLAMA_LIBRARY_PATH=/usr/lib/ollama:/usr/lib/ollama/rocm&lt;/p&gt; &lt;p&gt;time=2025-11-03T07:50:35.809Z level=DEBUG source=runner.go:471 msg=&amp;quot;bootstrap discovery took&amp;quot; &amp;gt;duration=58.847541ms OLLAMA_LIBRARY_PATH=&amp;quot;[/usr/lib/ollama /usr/lib/ollama/rocm]&amp;quot; extra_envs=map[]&lt;/p&gt; &lt;p&gt;time=2025-11-03T07:50:35.809Z level=DEBUG source=runner.go:120 msg=&amp;quot;evluating which if any devices to filter out&amp;quot; initial_count=0&lt;/p&gt; &lt;p&gt;time=2025-11-03T07:50:35.809Z level=DEBUG source=runner.go:41 msg=&amp;quot;GPU bootstrap discovery took&amp;quot; duration=59.157807ms&lt;/p&gt; &lt;p&gt;time=2025-11-03T07:50:35.809Z level=INFO source=types.go:60 msg=&amp;quot;inference compute&amp;quot; id=cpu library=cpu compute=&amp;quot;&amp;quot; name=cpu description=cpu libdirs=ollama driver=&amp;quot;&amp;quot; pci_id=&amp;quot;&amp;quot; type=&amp;quot;&amp;quot; total=&amp;quot;78.3 GiB&amp;quot; available=&amp;quot;66.1 GiB&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-11-03T07:50:35.809Z level=INFO source=routes.go:1618 msg=&amp;quot;entering low vram mode&amp;quot; &amp;quot;total vram&amp;quot;=&amp;quot;0 B&amp;quot; threshold=&amp;quot;20.0 GiB&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;My docker compose:&lt;/p&gt; &lt;p&gt;&lt;code&gt; ollama: image: ollama/ollama:rocm ports: - 11434:11434/tcp environment: - OLLAMA_DEBUG=1 - OLLAMA_KEEP_ALIVE=24h - HSA_OVERRIDE_GFX_VERSION=&amp;quot;11.0.2&amp;quot; - ENABLE_WEB_SEARCH=&amp;quot;True&amp;quot; volumes: - ./var/opt/data/ollama/ollama:/root/.ollama devices: - /dev/kfd - /dev/dri restart: always &lt;/code&gt;&lt;/p&gt; &lt;p&gt;I reinstalled rocm and the amdgpu drivers for linux to no avail. &lt;/p&gt; &lt;p&gt;Is there something I am missing here?&lt;/p&gt; &lt;p&gt;I have also tried GFX_VERSION 11.0.3 &amp;amp; 11.0.0 as well... but it was working at 11.0.2 until this incident.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/patach"&gt; /u/patach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1on60b8/ollama_no_longer_uses_780m_radeon_gpu_now_100_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1on60b8/ollama_no_longer_uses_780m_radeon_gpu_now_100_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1on60b8/ollama_no_longer_uses_780m_radeon_gpu_now_100_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-03T08:01:06+00:00</published>
  </entry>
</feed>
