<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-23T06:46:10+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1m4igz7</id>
    <title>Gpu support</title>
    <updated>2025-07-20T07:00:21+00:00</updated>
    <author>
      <name>/u/Ok-Band6009</name>
      <uri>https://old.reddit.com/user/Ok-Band6009</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys how long do you think its gonna take for ollama to add support for the new AMD cards, my 10th gen i5 is kinda struggling, my 9060xt 16gb would perform a lot better&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Band6009"&gt; /u/Ok-Band6009 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4igz7/gpu_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4igz7/gpu_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4igz7/gpu_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T07:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4k2t1</id>
    <title>ollama models and Hugging Face models use case</title>
    <updated>2025-07-20T08:45:14+00:00</updated>
    <author>
      <name>/u/cipherninjabyte</name>
      <uri>https://old.reddit.com/user/cipherninjabyte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just curious what would you use ollama models and hugging face models for ? writing articles locally or fine tuning or what else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cipherninjabyte"&gt; /u/cipherninjabyte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4k2t1/ollama_models_and_hugging_face_models_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4k2t1/ollama_models_and_hugging_face_models_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4k2t1/ollama_models_and_hugging_face_models_use_case/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T08:45:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4j5qp</id>
    <title>When is SmolLM3 coming on Ollama?</title>
    <updated>2025-07-20T07:44:40+00:00</updated>
    <author>
      <name>/u/falconHigh13</name>
      <uri>https://old.reddit.com/user/falconHigh13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have tried the new Huggingface Model on different platforms and even hosting locally but its very slow and take a lot of compute. I even tried huggingface Inference API and its not working. So when is this model coming on Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/falconHigh13"&gt; /u/falconHigh13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4j5qp/when_is_smollm3_coming_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4j5qp/when_is_smollm3_coming_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4j5qp/when_is_smollm3_coming_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T07:44:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4624q</id>
    <title>i just managed to run tinyllama1.1b and n8n in a low-end android phone</title>
    <updated>2025-07-19T20:22:28+00:00</updated>
    <author>
      <name>/u/actuallytech</name>
      <uri>https://old.reddit.com/user/actuallytech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m4624q/i_just_managed_to_run_tinyllama11b_and_n8n_in_a/"&gt; &lt;img alt="i just managed to run tinyllama1.1b and n8n in a low-end android phone" src="https://b.thumbs.redditmedia.com/6dh4kc8nJaE5b1uWFo_bUTN5U0tOMrcyUHvZJ6zjJ-U.jpg" title="i just managed to run tinyllama1.1b and n8n in a low-end android phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the phone i used is an samsung m32 6gb ram with a mediatek G80 &lt;/p&gt; &lt;p&gt;i runned in a Debian via proot-distro in Termux (no root) and i can access both locally, It‚Äôs working better than I expected &lt;/p&gt; &lt;p&gt;i dont know is there any way to use its gpu &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/actuallytech"&gt; /u/actuallytech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m4624q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4624q/i_just_managed_to_run_tinyllama11b_and_n8n_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4624q/i_just_managed_to_run_tinyllama11b_and_n8n_in_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T20:22:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4ploe</id>
    <title>mistral-small3.2:latest 15B takes 28GB VRAM?</title>
    <updated>2025-07-20T13:58:54+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;NAME ID SIZE PROCESSOR UNTIL mistral-small3.2:latest 5a408ab55df5 28 GB 38%/62% CPU/GPU 36 minutes from now 7900 XTX 24gb vram ryzen 7900 64GB RAM Question: Mistral size on disk is 15GB. Why it needs 28GB of VRAM and does not fit into 24GB GPU? ollama version is 0.9.6 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4ploe/mistralsmall32latest_15b_takes_28gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4ploe/mistralsmall32latest_15b_takes_28gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4ploe/mistralsmall32latest_15b_takes_28gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T13:58:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4nclb</id>
    <title>ChatGPT-like Voice LLM</title>
    <updated>2025-07-20T12:08:58+00:00</updated>
    <author>
      <name>/u/embracing_athena</name>
      <uri>https://old.reddit.com/user/embracing_athena</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like the ChaGPT voice mode where I was able to converse with the AI with voice but that is limited to 15 minutes or so daily.&lt;/p&gt; &lt;p&gt;My question is, is there an LLM that I can run with Ollama to achieve the same but with no limits? I feel like any LLM can be used but at the same time seems like I'm feeling I'm missing something. Any extra software must be used along with Ollama for this work?&lt;/p&gt; &lt;p&gt;Please excuse me for my bad English.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/embracing_athena"&gt; /u/embracing_athena &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4nclb/chatgptlike_voice_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4nclb/chatgptlike_voice_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4nclb/chatgptlike_voice_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T12:08:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4rl2z</id>
    <title>Anyone else tracking their local LLMs‚Äô performance? I built a tool to make it easier</title>
    <updated>2025-07-20T15:23:18+00:00</updated>
    <author>
      <name>/u/Hades_7658</name>
      <uri>https://old.reddit.com/user/Hades_7658</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I've been running some LLMs locally and was curious how others are keeping tabs on model performance, latency, and token usage. I didn‚Äôt find a lightweight tool that fit my needs, so I started working on one myself.&lt;/p&gt; &lt;p&gt;It‚Äôs a simple dashboard + API setup that helps me monitor and analyze what's going on under the hood mainly for performance tuning and observability. Still early days, but it‚Äôs been surprisingly useful for understanding how my models are behaving over time.&lt;/p&gt; &lt;p&gt;Curious how the rest of you handle observability. Do you use logs, custom scripts, or something else? I‚Äôll drop a link in the comments in case anyone wants to check it out or build on top of it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hades_7658"&gt; /u/Hades_7658 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4rl2z/anyone_else_tracking_their_local_llms_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4rl2z/anyone_else_tracking_their_local_llms_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4rl2z/anyone_else_tracking_their_local_llms_performance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T15:23:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4zl6c</id>
    <title>vision model that can "scape" webpages?</title>
    <updated>2025-07-20T20:44:39+00:00</updated>
    <author>
      <name>/u/larz01larz</name>
      <uri>https://old.reddit.com/user/larz01larz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone aware of a vision model that would be able to take a screenshot of a webpage and create a playwright script to navigate the page based on the screen shot?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/larz01larz"&gt; /u/larz01larz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4zl6c/vision_model_that_can_scape_webpages/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4zl6c/vision_model_that_can_scape_webpages/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4zl6c/vision_model_that_can_scape_webpages/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T20:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5cqfp</id>
    <title>Can I run an embedding model on a dell wyse 3040? If so, How do I set it up for this single purpose?</title>
    <updated>2025-07-21T07:45:49+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use obsidian+smart connections plugin to look up for semantical similarities between the texts of several research papers I have saved in markdown format, I have no clue about how to utilise RAG or LLMs in general for my usecase but what I do is just enough as of yet.&lt;/p&gt; &lt;p&gt;I want to unload some of the embeddings processing to a secondary device I have on me since both my devices are weak hardware wise, how to set up the thin client for this one purpose and what os+model to use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m5cqfp/can_i_run_an_embedding_model_on_a_dell_wyse_3040/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m5cqfp/can_i_run_an_embedding_model_on_a_dell_wyse_3040/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m5cqfp/can_i_run_an_embedding_model_on_a_dell_wyse_3040/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-21T07:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m61ivl</id>
    <title>This started as a prompt snippet manager‚Ä¶</title>
    <updated>2025-07-22T01:49:01+00:00</updated>
    <author>
      <name>/u/TutorialDoctor</name>
      <uri>https://old.reddit.com/user/TutorialDoctor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m61ivl/this_started_as_a_prompt_snippet_manager/"&gt; &lt;img alt="This started as a prompt snippet manager‚Ä¶" src="https://preview.redd.it/1s4x9x9wzbef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbe47b33c0d74023d7751a1e50657eb44cc148e0" title="This started as a prompt snippet manager‚Ä¶" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a snippet manager desktop app with ollama for myself and it quickly became a lot more than that‚Ä¶ &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TutorialDoctor"&gt; /u/TutorialDoctor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1s4x9x9wzbef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m61ivl/this_started_as_a_prompt_snippet_manager/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m61ivl/this_started_as_a_prompt_snippet_manager/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T01:49:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6faor</id>
    <title>It make so much time to downlaod</title>
    <updated>2025-07-22T14:17:25+00:00</updated>
    <author>
      <name>/u/Maleficent_Floor_941</name>
      <uri>https://old.reddit.com/user/Maleficent_Floor_941</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m6faor/it_make_so_much_time_to_downlaod/"&gt; &lt;img alt="It make so much time to downlaod" src="https://preview.redd.it/le6tlowepfef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fda6831476840efa099cfc80c1af6ad4683bd96f" title="It make so much time to downlaod" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm just downloading the Ola model, but there is so many issues in the when my MacBook is suddenly when the screen is inactive, then it is off. After that time, there will be a half and complete, but not such more completed. Will start again from the 0 to downloading around the model, that‚Äôs called a three, that‚Äôs why I am using right now so for such a bug, so please fix it right now, okay&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent_Floor_941"&gt; /u/Maleficent_Floor_941 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/le6tlowepfef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6faor/it_make_so_much_time_to_downlaod/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6faor/it_make_so_much_time_to_downlaod/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T14:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1m67s3f</id>
    <title>Quali sono i passaggi per installare una GPU NVIDIA M40 24GB su un Dell Precision T5820?</title>
    <updated>2025-07-22T07:34:24+00:00</updated>
    <author>
      <name>/u/Jaded_Treat_230</name>
      <uri>https://old.reddit.com/user/Jaded_Treat_230</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sto cercando di installare seconda gpu m40 24gb sul dell t5820. Attualemnte monta una p4000. quando installo m40 pc non si avvia. &lt;/p&gt; &lt;p&gt;Sembra che ci sia problema di incompatibilit√†, ho provato queste soluzioni: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;aggiornamento bios, problrma persiste&lt;/li&gt; &lt;li&gt;Uso nvflash, e impostare m40 in modalita grafica ma come faccio non avendo gpu installata?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;qualcuno a soluzioni?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jaded_Treat_230"&gt; /u/Jaded_Treat_230 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m67s3f/quali_sono_i_passaggi_per_installare_una_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m67s3f/quali_sono_i_passaggi_per_installare_una_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m67s3f/quali_sono_i_passaggi_per_installare_una_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T07:34:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6gkzw</id>
    <title>Monitoring your repo 24/7 using Agents.</title>
    <updated>2025-07-22T15:07:12+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m6gkzw/monitoring_your_repo_247_using_agents/"&gt; &lt;img alt="Monitoring your repo 24/7 using Agents." src="https://external-preview.redd.it/MTZlOWQxZGF5ZmVmMXY1D7FxNTB0y5Y5bDWcUCwmhhELkEtUCpjgYWthIbZh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5504a3b62012fcba7cec25b8b88207b943a226a" title="Monitoring your repo 24/7 using Agents." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever wish you could have someone watching your Github repo 24/7? &lt;/p&gt; &lt;p&gt;We built an agent that monitors your repo, finds who most recently starred it, and autonomously reaches out via email!&lt;/p&gt; &lt;p&gt;Discord : &lt;a href="https://discord.com/invite/ZYN7f7KPjS"&gt;https://discord.com/invite/ZYN7f7KPjS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wcf1sbjayfef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6gkzw/monitoring_your_repo_247_using_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6gkzw/monitoring_your_repo_247_using_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T15:07:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m60gro</id>
    <title>How do I generate an entire book?</title>
    <updated>2025-07-22T00:58:22+00:00</updated>
    <author>
      <name>/u/-ProfitLogical-</name>
      <uri>https://old.reddit.com/user/-ProfitLogical-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like to listen to something while doing things like painting and whatnot. Sometime I have an idea for a story that might be interesting to listen to but doesn't exist. What model and how can I get a book of approximately 80k-120k words to generate from an idea I put in. It seems like they can't generate it all in one window but can it just keep making new windows till its done? Maybe it can then go back and put all those windows in a doc? Most people seem to want an AI to help them write a story while I want it to do the whole thing. I know its not going to be awesome but it might be good enough to listen to while working on something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ProfitLogical-"&gt; /u/-ProfitLogical- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m60gro/how_do_i_generate_an_entire_book/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m60gro/how_do_i_generate_an_entire_book/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m60gro/how_do_i_generate_an_entire_book/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T00:58:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1m638nd</id>
    <title>Realtime codebase indexing for coding agents with ~ 50 lines of Python (open source)</title>
    <updated>2025-07-22T03:11:53+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would love to share my open source project that buildings realtime indexing &amp;amp; context for coding agents ~ 50 lines of Python on the &lt;a href="https://github.com/cocoindex-io/cocoindex/blob/main/examples/code_embedding/main.py#L11-L84"&gt;indexing path&lt;/a&gt;. Full blog and explanation &lt;a href="https://cocoindex.io/blogs/index-code-base-for-rag"&gt;here&lt;/a&gt;. Would love your feedback and appreciate a star on the repo if it is helpful, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m638nd/realtime_codebase_indexing_for_coding_agents_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m638nd/realtime_codebase_indexing_for_coding_agents_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m638nd/realtime_codebase_indexing_for_coding_agents_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T03:11:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6dow3</id>
    <title>Disable ssl check</title>
    <updated>2025-07-22T13:10:46+00:00</updated>
    <author>
      <name>/u/qtm_music</name>
      <uri>https://old.reddit.com/user/qtm_music</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a way to disable ssl check for ollama in docker? I work on windows, my corporate proxy replaces certificates, is there a way to disable the check?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qtm_music"&gt; /u/qtm_music &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6dow3/disable_ssl_check/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6dow3/disable_ssl_check/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6dow3/disable_ssl_check/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T13:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5z1x7</id>
    <title>Is there a simple way to "enhance" a model with the content of a book?</title>
    <updated>2025-07-21T23:52:30+00:00</updated>
    <author>
      <name>/u/RaticateLV99</name>
      <uri>https://old.reddit.com/user/RaticateLV99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run some DnD adventures and I want to teach local models with the content of a book.&lt;/p&gt; &lt;p&gt;But, I also want to add more details about &lt;strong&gt;my adventure from time to time&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Is there a simple way to enhance the model with the content of my adventures and the content of the books?&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaticateLV99"&gt; /u/RaticateLV99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m5z1x7/is_there_a_simple_way_to_enhance_a_model_with_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m5z1x7/is_there_a_simple_way_to_enhance_a_model_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m5z1x7/is_there_a_simple_way_to_enhance_a_model_with_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-21T23:52:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5jqbo</id>
    <title>Use llm to gather insights of market fluctuations</title>
    <updated>2025-07-21T14:02:07+00:00</updated>
    <author>
      <name>/u/m19990328</name>
      <uri>https://old.reddit.com/user/m19990328</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m5jqbo/use_llm_to_gather_insights_of_market_fluctuations/"&gt; &lt;img alt="Use llm to gather insights of market fluctuations" src="https://preview.redd.it/diudkgucf8ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6f52d897baf99be2923a0d6ecfbc861119fda53" title="Use llm to gather insights of market fluctuations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I've recently built a project that explores stock price trends and gathers market insights. Last time I shared it here, some of you showed interest. Now, I've packaged it as a Windows app with a GUI. Feel free to check it out!&lt;/p&gt; &lt;p&gt;Project: &lt;a href="https://github.com/CyrusCKF/stock-gone-wrong"&gt;https://github.com/CyrusCKF/stock-gone-wrong&lt;/a&gt;&lt;br /&gt; Download: &lt;a href="https://github.com/CyrusCKF/stock-gone-wrong/releases/tag/v0.1.0-alpha"&gt;https://github.com/CyrusCKF/stock-gone-wrong/releases/tag/v0.1.0-alpha&lt;/a&gt; (Windows may display a warning)&lt;/p&gt; &lt;p&gt;To use this function, first navigate to the &lt;strong&gt;&amp;quot;Events&amp;quot;&lt;/strong&gt; tab. Enter your ticker, select a date range, and click the button. The stock trends will be split into several &amp;quot;&lt;em&gt;major events&amp;quot;&lt;/em&gt;. Use the slider to select an event you're interested in, then click &lt;strong&gt;&amp;quot;Find News&amp;quot;&lt;/strong&gt;. This will initialize an Ollama agent to scrape and summarize stock news around the timeframe. Note that this process may take several minutes, depending on your machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DISCLAIMER&lt;/strong&gt; This tool is not intended to provide stock-picking recommendations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m19990328"&gt; /u/m19990328 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/diudkgucf8ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m5jqbo/use_llm_to_gather_insights_of_market_fluctuations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m5jqbo/use_llm_to_gather_insights_of_market_fluctuations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-21T14:02:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6j0n9</id>
    <title>"You are a teacher. Teach me about a random topic"</title>
    <updated>2025-07-22T16:38:52+00:00</updated>
    <author>
      <name>/u/TutorialDoctor</name>
      <uri>https://old.reddit.com/user/TutorialDoctor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This prompt doesn't generate random topics for llama 3.2 or Gemma 3 4B. In fact, it often generates the same topic on Bioluminescence or the Science of Color and one other topic.&lt;/p&gt; &lt;p&gt;What does it generate for you? I'm using ollama locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TutorialDoctor"&gt; /u/TutorialDoctor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6j0n9/you_are_a_teacher_teach_me_about_a_random_topic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6j0n9/you_are_a_teacher_teach_me_about_a_random_topic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6j0n9/you_are_a_teacher_teach_me_about_a_random_topic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T16:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6svz1</id>
    <title>Moving 1 big Ollama model to another PC</title>
    <updated>2025-07-22T22:56:56+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I started using GPUStack and got it installed and working on 3 systems with 7 GPUs. Problem is that I exceeded my 1.2 TB internet usage. I wanted to test larger 70B models but needed to wait several days for my ISP to reset the meter. I took the time to figure out how to transfer individual ollama models to other network systems.&lt;/p&gt; &lt;p&gt;First issue is that models are store as:&lt;/p&gt; &lt;p&gt;&lt;em&gt;sha256-f1b16b5d5d524a6de624e11ac48cc7d2a9b5cab399aeab6346bd0600c94cfd12&lt;/em&gt;&lt;/p&gt; &lt;p&gt;We get can needed info like path to model and model sha256 name:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama show --modelfile llava:13b-v1.5-q8_0&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Modelfile generated by &amp;quot;ollama show&amp;quot; # To build a new Modelfile based on this, replace FROM with: # FROM llava:13b-v1.5-q8_0 FROM /usr/share/ollama/.ollama/models/blobs/sha256-f1b16b5d5d524a6de624e11ac48cc7d2a9b5cab399aeab6346bd0600c94cfd12 FROM /usr/share/ollama/.ollama/models/blobs/sha256-0af93a69825fd741ffdc7c002dcd47d045c795dd55f73a3e08afa484aff1bcd3 TEMPLATE &amp;quot;{{ .System }} USER: {{ .Prompt }} ASSSISTANT: &amp;quot; PARAMETER stop USER: PARAMETER stop ASSSISTANT: LICENSE &amp;quot;&amp;quot;&amp;quot;LLAMA 2 COMMUNITY LICENSE AGREEMENT Llama 2 Version Release Date: July 18, 2023 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I used the first listed sha256- file based on the size (13G)&lt;/p&gt; &lt;p&gt;&lt;code&gt;ls -lhS /usr/share/ollama/.ollama/models/blobs/sha256-f1b*&lt;/code&gt;&lt;/p&gt; &lt;p&gt;-rw-r--r-- 1 ollama ollama 13G May 17&lt;/p&gt; &lt;p&gt;From SOURCE PC:&lt;/p&gt; &lt;p&gt;Will be using scp and ssh to remote into destination pc so if necessary just install:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo apt install openssh-server&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This is where we will have model info saved&lt;/p&gt; &lt;p&gt;&lt;code&gt;mkdir ~/models.txt&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Lets find a big model to transfer&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama list | sort -k3&lt;/code&gt;&lt;/p&gt; &lt;p&gt;On my system I'll use llava:13b-v1.5-q8_0&lt;/p&gt; &lt;p&gt;ollama show --modelfile llava:13b-v1.5-q8_0&lt;/p&gt; &lt;p&gt;simpler view&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama show --modelfile llava:13b-v1.5-q8_0 | grep FROM \ | tee -a ~/models.txt; echo &amp;quot;&amp;quot; &amp;gt;&amp;gt; ~/models.txt &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;By appending &amp;gt;&amp;gt; the output to 'models.txt' we have a record \&lt;/p&gt; &lt;p&gt;of data on both PC.&lt;/p&gt; &lt;p&gt;Now add the sha256- model number then scp transfer to local \&lt;/p&gt; &lt;p&gt;remote PC's home directory.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;scp ~/models.txt user3@10.0.0.34:~ &amp;amp;&amp;amp; scp \ /usr/share/ollama/.ollama/models/blobs/sha256-xxx user3@10.0.0.34:~ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is what full command looks like.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;scp ~/models.txt user3@10.0.0.34:~ &amp;amp;&amp;amp; scp \ /usr/share/ollama/.ollama/models/blobs/\ sha256-f1b16b5d5d524a6de624e11ac48cc7d2a9b5cab399aeab6346bd0600c94cfd12 \ user3@10.0.0.34:~ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;About 2 minutes to transfer 12GB over 1 Gigabit Ethernet network (1000Base-T or Gb3 or 1 GigE)&lt;/p&gt; &lt;p&gt;Lets get into remote PC (ssh), change permission (chown) \&lt;/p&gt; &lt;p&gt;of the file and move (mv) file to correct path for ollama.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ssh user3@10.0.0.34 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;view the transferred file.&lt;/p&gt; &lt;p&gt;&lt;code&gt;cat ~/models.txt&lt;/code&gt;&lt;/p&gt; &lt;p&gt;copy sha256- (or just tab auto complete) number and change permission&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo chown ollama:ollama sha256-*&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Move to ollama blobs folder, view in size order and then ready to \&lt;/p&gt; &lt;p&gt;ollama pull&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo mv ~/sha256-* /usr/share/ollama/.ollama/models/blobs/ &amp;amp;&amp;amp; ls -lhS /usr/share/ollama/.ollama/models/blobs/ ; echo &amp;quot;ls -lhS then pull model&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;formatting issues:&lt;/p&gt; &lt;p&gt;sudo mv ~/sha256-* /usr/share/ollama/.ollama/models/blobs/ &amp;amp;&amp;amp; \&lt;/p&gt; &lt;p&gt;ls -lhS /usr/share/ollama/.ollama/models/blobs/ ; \&lt;/p&gt; &lt;p&gt;echo &amp;quot;ls -lhS then pull model&amp;quot;&lt;/p&gt; &lt;p&gt;ollama pull llava:13b-v1.5-q8_0&lt;/p&gt; &lt;p&gt;Ollama will recognize the largest part of the file and only download \&lt;/p&gt; &lt;p&gt;the smaller needed parts. Should be done in a few seconds.&lt;/p&gt; &lt;p&gt;Now I just need to figure out how to get GPUStack to use my already \&lt;/p&gt; &lt;p&gt;download ollama file instead of downloading it all over again.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6svz1/moving_1_big_ollama_model_to_another_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6svz1/moving_1_big_ollama_model_to_another_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6svz1/moving_1_big_ollama_model_to_another_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T22:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6jypu</id>
    <title>Why isn't this already a standard in robotics?</title>
    <updated>2025-07-22T17:13:50+00:00</updated>
    <author>
      <name>/u/PranavVermaa</name>
      <uri>https://old.reddit.com/user/PranavVermaa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I was playing around with Ollama and got this working in under &lt;strong&gt;2 minutes&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;You give it a natural language command like:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Run 10 meters&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It instantly returns:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;action&amp;quot;: &amp;quot;run&amp;quot;, &amp;quot;distance_meters&amp;quot;: 10, &amp;quot;unit&amp;quot;: &amp;quot;meters&amp;quot; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I didn‚Äôt tweak anything. I just used llama3.2:3b and created a straightforward system prompt in a Modelfile. That‚Äôs all. No additional tools. No ROS integration yet. But the main idea is ‚Äî the whole &amp;quot;understand action and structure it&amp;quot; issue is pretty much resolved with a good LLM and some JSON formatting.&lt;/p&gt; &lt;p&gt;Think about what we could achieve if we had:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time voice-to-action systems,&lt;/li&gt; &lt;li&gt;A lightweight LLM operating on-device (or at the edge),&lt;/li&gt; &lt;li&gt;A basic robotic API to process these tokens and carry them out.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I feel like we‚Äôve made robotics interfaces way too complicated for years.&lt;br /&gt; This is so simple now. What are we waiting for?&lt;/p&gt; &lt;p&gt;For Reference, here is my Modelfile that I used: &lt;a href="https://pastebin.com/TaXBQGZK"&gt;https://pastebin.com/TaXBQGZK&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PranavVermaa"&gt; /u/PranavVermaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6jypu/why_isnt_this_already_a_standard_in_robotics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6jypu/why_isnt_this_already_a_standard_in_robotics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6jypu/why_isnt_this_already_a_standard_in_robotics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T17:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6mfml</id>
    <title>Models which perform better as Q8 (int8) over Q4_(X_Y)?</title>
    <updated>2025-07-22T18:44:56+00:00</updated>
    <author>
      <name>/u/RecoJohnson</name>
      <uri>https://old.reddit.com/user/RecoJohnson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tested models that perform more accurately or with more efficiency with Q8 quantization instead of the more common Q4_K_M etc?&lt;/p&gt; &lt;p&gt;AMD's newer consumer video cards improved the performance of int8 and fp16 computation and I want to learn more about it and am curious if Q8 models are going to take over in the long run with attention techniques.&lt;/p&gt; &lt;p&gt;I would love to see some benchmarks if anyone has done their own testing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RecoJohnson"&gt; /u/RecoJohnson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6mfml/models_which_perform_better_as_q8_int8_over_q4_x_y/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6mfml/models_which_perform_better_as_q8_int8_over_q4_x_y/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6mfml/models_which_perform_better_as_q8_int8_over_q4_x_y/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T18:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6mice</id>
    <title>Local Long Term Memory with Ollama?</title>
    <updated>2025-07-22T18:47:39+00:00</updated>
    <author>
      <name>/u/Debug_Mode_On</name>
      <uri>https://old.reddit.com/user/Debug_Mode_On</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For whatever reason I prefer to run everything local. When I search long term memory for my little conversational bot, I see a lot of solutions. Many of them are cloud based. Is there a standard solution to offer my little chat bot long term memory that runs locally with Ollama that I should be looking at? Or a tutorial you would recommend? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Debug_Mode_On"&gt; /u/Debug_Mode_On &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6mice/local_long_term_memory_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6mice/local_long_term_memory_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6mice/local_long_term_memory_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T18:47:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m70f7q</id>
    <title>**üîì I built Hearth-UI ‚Äî A fully-featured desktop app for chatting with local LLMs (Ollama-ready, attachments, themes, markdown, and more)**</title>
    <updated>2025-07-23T04:59:48+00:00</updated>
    <author>
      <name>/u/Vast-Helicopter-3719</name>
      <uri>https://old.reddit.com/user/Vast-Helicopter-3719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m70f7q/i_built_hearthui_a_fullyfeatured_desktop_app_for/"&gt; &lt;img alt="**üîì I built Hearth-UI ‚Äî A fully-featured desktop app for chatting with local LLMs (Ollama-ready, attachments, themes, markdown, and more)**" src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="**üîì I built Hearth-UI ‚Äî A fully-featured desktop app for chatting with local LLMs (Ollama-ready, attachments, themes, markdown, and more)**" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt; &lt;p&gt;I recently put together a desktop AI chat interface called &lt;strong&gt;Hearth-UI&lt;/strong&gt;, made for anyone using &lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt; for local LLMs like LLaMA3, Mistral, Gemma, etc.&lt;/p&gt; &lt;p&gt;It includes everything I wish existed in a typical Ollama UI ‚Äî and it‚Äôs fully offline, customizable, and open-source.&lt;/p&gt; &lt;p&gt;üß† Features:&lt;/p&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Multi-session chat history&lt;/strong&gt; (rename, delete, auto-save)&lt;br /&gt; ‚úÖ &lt;strong&gt;Markdown + syntax highlighting&lt;/strong&gt; (like ChatGPT)&lt;br /&gt; ‚úÖ &lt;strong&gt;Streaming responses&lt;/strong&gt; + prompt &lt;strong&gt;queueing while streaming&lt;/strong&gt;&lt;br /&gt; ‚úÖ &lt;strong&gt;File uploads&lt;/strong&gt; &amp;amp; &lt;strong&gt;drag-and-drop attachments&lt;/strong&gt;&lt;br /&gt; ‚úÖ Beautiful &lt;strong&gt;theme picker&lt;/strong&gt; (Dark/Light/Blue/Green/etc)&lt;br /&gt; ‚úÖ &lt;strong&gt;Cancel response mid-generation&lt;/strong&gt; (Stop button)&lt;br /&gt; ‚úÖ Export chat to &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.json&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;&lt;br /&gt; ‚úÖ &lt;strong&gt;Electron-powered desktop app for Windows&lt;/strong&gt; (macOS/Linux coming)&lt;br /&gt; ‚úÖ Works with your existing &lt;code&gt;ollama serve&lt;/code&gt; ‚Äî no cloud, no signup&lt;/p&gt; &lt;h1&gt;üîß Tech stack:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Ollama (as LLM backend)&lt;/li&gt; &lt;li&gt;HTML/CSS/JS (Vanilla frontend)&lt;/li&gt; &lt;li&gt;Electron for standalone app&lt;/li&gt; &lt;li&gt;Node.js backend (for model list &amp;amp; /chat proxy)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6cjcdgu90kef1.png?width=3790&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51c3f0e53500960e0fe3313f94699dd725d33187"&gt;https://preview.redd.it/6cjcdgu90kef1.png?width=3790&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51c3f0e53500960e0fe3313f94699dd725d33187&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;GitHub link:&lt;/h1&gt; &lt;p&gt;üëâ &lt;a href="https://github.com/Saurabh682/Hearth-UI"&gt;https://github.com/Saurabh682/Hearth-UI&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;üôè I'd love your feedback on:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Other must-have features?&lt;/li&gt; &lt;li&gt;Would a Windows/exe help?&lt;/li&gt; &lt;li&gt;Any bugs or improvement ideas?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking it out. Hope it helps the self-hosted LLM community!&lt;br /&gt; ‚ù§Ô∏è&lt;/p&gt; &lt;h1&gt;üè∑Ô∏è Tags:&lt;/h1&gt; &lt;p&gt;&lt;code&gt;[Electron] [Ollama] [Local LLM] [Desktop AI UI] [Markdown] [Self Hosted]&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast-Helicopter-3719"&gt; /u/Vast-Helicopter-3719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m70f7q/i_built_hearthui_a_fullyfeatured_desktop_app_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m70f7q/i_built_hearthui_a_fullyfeatured_desktop_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m70f7q/i_built_hearthui_a_fullyfeatured_desktop_app_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-23T04:59:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6pxln</id>
    <title>Digital twins that attend meetings for you. Dystopia or soon reality?</title>
    <updated>2025-07-22T20:56:54+00:00</updated>
    <author>
      <name>/u/DerErzfeind61</name>
      <uri>https://old.reddit.com/user/DerErzfeind61</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m6pxln/digital_twins_that_attend_meetings_for_you/"&gt; &lt;img alt="Digital twins that attend meetings for you. Dystopia or soon reality?" src="https://external-preview.redd.it/Mnh0bG1lNW1vaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80201ecaa90deb38fdf6db31774e15c9a1e0c78e" title="Digital twins that attend meetings for you. Dystopia or soon reality?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In more and more meetings these days there are AI notetakers that someone has sent instead of showing up themselves. You can think what you want about these notetakers, but they seem to have become part of our everyday working lives. This raises the question of how long it will be before the next stage of development occurs and we are sitting in meetings with ‚Äúdigital twins‚Äù who are standing in for an absent employee.&lt;/p&gt; &lt;p&gt;To find out, I tried to build such a digital twin and it actually turned out to be very easy to create a meeting agent that can actively interact with other participants, share insights about my work and answer follow-up questions for me. Of course, many of the leading providers of voice clones and personalized LLMs are closed-source, which increases the privacy issue that already exists with AI Notetakers. However, my approach using joinly could also be implemented with Chatterbox and a self-hosted LLM with few-shot prompting, for example.&lt;/p&gt; &lt;p&gt;But there are of course many other critical questions: how exactly can we control what these digital twins disclose or are allowed to decide, ethical concerns about whether my company is allowed to create such a twin for me, how this is compatible with meeting etiquette and of course whether we shouldn't simply plan better meetings instead.&lt;/p&gt; &lt;p&gt;What do you think? Will such digital twins catch on? Would you use one to skip a boring meeting?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DerErzfeind61"&gt; /u/DerErzfeind61 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ynyz3f5mohef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6pxln/digital_twins_that_attend_meetings_for_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6pxln/digital_twins_that_attend_meetings_for_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T20:56:54+00:00</published>
  </entry>
</feed>
