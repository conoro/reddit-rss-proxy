<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-03T22:24:30+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pyn9kw</id>
    <title>So hi all, i am currently playing with all this self hosted LLM (SLM in my case with my hardware limitations) im just using a Proxmox enviroment with Ollama installed direcly on a Ubuntu server container and on top of it Open WebUI to get the nice dashboard and to be able to create user accounts.</title>
    <updated>2025-12-29T14:11:58+00:00</updated>
    <author>
      <name>/u/Franceesios</name>
      <uri>https://old.reddit.com/user/Franceesios</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pyn9kw/so_hi_all_i_am_currently_playing_with_all_this/"&gt; &lt;img alt="So hi all, i am currently playing with all this self hosted LLM (SLM in my case with my hardware limitations) im just using a Proxmox enviroment with Ollama installed direcly on a Ubuntu server container and on top of it Open WebUI to get the nice dashboard and to be able to create user accounts." src="https://b.thumbs.redditmedia.com/iNf-j2OTzD0L4Rv9eBe773QoWp_aokC85y843xSV_Po.jpg" title="So hi all, i am currently playing with all this self hosted LLM (SLM in my case with my hardware limitations) im just using a Proxmox enviroment with Ollama installed direcly on a Ubuntu server container and on top of it Open WebUI to get the nice dashboard and to be able to create user accounts." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So far im using just these models &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w18f48hnh5ag1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c46e7759d8c3bb13d8238a4f1503ad3dd7620957"&gt;https://preview.redd.it/w18f48hnh5ag1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c46e7759d8c3bb13d8238a4f1503ad3dd7620957&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Llama3.2:1.2b&lt;/p&gt; &lt;p&gt;- Llama3.2:latest 3.2b&lt;/p&gt; &lt;p&gt;- Llama3.2:&lt;strong&gt;8b&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;- Ministral-3:8b&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;They are running ok at the time, the 8B ones would take atleast 2 minutes to give some proper answer, and ive also put this template for the models to remember with each answer they give out ;&lt;/p&gt; &lt;p&gt;### Task:&lt;/p&gt; &lt;p&gt;Respond to the user query using the provided context, incorporating inline citations in the format [id] **only when the &amp;lt;source&amp;gt; tag includes an explicit id attribute** (e.g., &amp;lt;source id=&amp;quot;1&amp;quot;&amp;gt;). Always include a confidence rating for your answer.&lt;/p&gt; &lt;p&gt;### Guidelines:&lt;/p&gt; &lt;p&gt;- Only provide answers you are confident in. Do not guess or invent information.&lt;/p&gt; &lt;p&gt;- If unsure or lacking sufficient information, respond with &amp;quot;I don‚Äôt know&amp;quot; or &amp;quot;I‚Äôm not sure.&amp;quot;&lt;/p&gt; &lt;p&gt;- Include a confidence rating from 1 to 5:&lt;/p&gt; &lt;p&gt;1 = very uncertain&lt;/p&gt; &lt;p&gt;2 = somewhat uncertain&lt;/p&gt; &lt;p&gt;3 = moderately confident&lt;/p&gt; &lt;p&gt;4 = confident&lt;/p&gt; &lt;p&gt;5 = very confident&lt;/p&gt; &lt;p&gt;- Respond in the same language as the user's query.&lt;/p&gt; &lt;p&gt;- If the context is unreadable or low-quality, inform the user and provide the best possible answer.&lt;/p&gt; &lt;p&gt;- If the answer isn‚Äôt present in the context but you possess the knowledge, explain this and provide the answer.&lt;/p&gt; &lt;p&gt;- Include inline citations [id] only when &amp;lt;source&amp;gt; has an id attribute.&lt;/p&gt; &lt;p&gt;- Do not use XML tags in your response.&lt;/p&gt; &lt;p&gt;- Ensure citations are concise and directly relevant.&lt;/p&gt; &lt;p&gt;- Do NOT use Web Search or external sources.&lt;/p&gt; &lt;p&gt;- If the context does not contain the answer, reply: ‚ÄòI don‚Äôt know‚Äô and Confidence 1‚Äì2.&lt;/p&gt; &lt;p&gt;### Example Output:&lt;/p&gt; &lt;p&gt;Answer: [Your answer here]&lt;/p&gt; &lt;p&gt;Confidence: [1-5]&lt;/p&gt; &lt;p&gt;### Context:&lt;/p&gt; &lt;p&gt;&amp;lt;context&amp;gt;&lt;/p&gt; &lt;p&gt;{{CONTEXT}}&lt;/p&gt; &lt;p&gt;&amp;lt;/context&amp;gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tbnk6bekh5ag1.png?width=1647&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38c75ac55e6951ca80a0f364fdcf8629379c69aa"&gt;https://preview.redd.it/tbnk6bekh5ag1.png?width=1647&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38c75ac55e6951ca80a0f364fdcf8629379c69aa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With so far works great, my primarly test right about now is the RAG method that Open WebUI offers, ive currently uploaded some invoices from this whole year worth of data as .MD files.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nchwh0kyh5ag1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a43d510aa7032f361dbfc7849903d1d87ba221a5"&gt;https://preview.redd.it/nchwh0kyh5ag1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a43d510aa7032f361dbfc7849903d1d87ba221a5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And asks the model (selecting the folder with the data first with # command/option) and i would get some good answers and some times some not so good answers but witj the confidence level accurate.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vqzwaupsh5ag1.png?width=559&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2737560e7562ccb31845f578e8ac89dbd42d33bb"&gt;https://preview.redd.it/vqzwaupsh5ag1.png?width=559&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2737560e7562ccb31845f578e8ac89dbd42d33bb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now my question is, if some tech company wants to implement these type of LLM (SML) into there on premise network for like finance department to use, is this a good start? How does some enterprise do it at the moment? Like sites like &lt;a href="http://llm.co"&gt;llm.co&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9knu91phh5ag1.png?width=1438&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a790870d44637e073b7807f3120306fdee8db623"&gt;https://preview.redd.it/9knu91phh5ag1.png?width=1438&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a790870d44637e073b7807f3120306fdee8db623&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far i can see real use case for this RAG method with some more powerfull hardware ofcourse, but let me know your real enterprise use case of a on-prem LLM RAG method. &lt;/p&gt; &lt;p&gt;Thanks all! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Franceesios"&gt; /u/Franceesios &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyn9kw/so_hi_all_i_am_currently_playing_with_all_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pyn9kw/so_hi_all_i_am_currently_playing_with_all_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pyn9kw/so_hi_all_i_am_currently_playing_with_all_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T14:11:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz0y45</id>
    <title>CLI tool to use transformer and diffuser models</title>
    <updated>2025-12-29T22:55:47+00:00</updated>
    <author>
      <name>/u/zashboy</name>
      <uri>https://old.reddit.com/user/zashboy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zashboy"&gt; /u/zashboy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/huggingface/comments/1pz0wid/cli_tool_to_use_transformer_and_diffuser_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pz0y45/cli_tool_to_use_transformer_and_diffuser_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pz0y45/cli_tool_to_use_transformer_and_diffuser_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T22:55:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz7hpe</id>
    <title>Upload folders to a chat</title>
    <updated>2025-12-30T03:42:17+00:00</updated>
    <author>
      <name>/u/Cool-Condition466</name>
      <uri>https://old.reddit.com/user/Cool-Condition466</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a problem, im kinda new to this so bear with me. I have a mod for a game that i'm developing and I just hit a dead end so i'm trying to use ollama to see if it can help me. I wanted to upload the whole mod folder but it is not letting me do it instead it just uploads the python and txt files thar are scattered all over there. How can I upload the whole folder?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Condition466"&gt; /u/Cool-Condition466 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pz7hpe/upload_folders_to_a_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pz7hpe/upload_folders_to_a_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pz7hpe/upload_folders_to_a_chat/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-30T03:42:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pym7c0</id>
    <title>Running Ministral 3 3B Locally with Ollama and Adding Tool Calling (Local + Remote MCP)</title>
    <updated>2025-12-29T13:24:35+00:00</updated>
    <author>
      <name>/u/shricodev</name>
      <uri>https://old.reddit.com/user/shricodev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been seeing a lot of chatter around Ministral 3 3B, so I wanted to test it in a way that actually matters day to day. Can such a small local model do reliable tool calling, and can you extend it beyond local tools to work with remotely hosted MCP servers?&lt;/p&gt; &lt;p&gt;Here‚Äôs what I tried:&lt;/p&gt; &lt;h1&gt;Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Ran a quantized 4-bit (Q4_K_M) Ministral 3 3B on Ollama&lt;/li&gt; &lt;li&gt;Connected it to Open WebUI (with Docker)&lt;/li&gt; &lt;li&gt;Tested tool calling in two stages: &lt;ul&gt; &lt;li&gt;Local Python tools inside Open WebUI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Remote MCP tools&lt;/strong&gt; via Composio (so the model can call externally hosted tools through MCP)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model, despite the super tiny size of just 3B parameters, is said to support tool calling with even support for structured output. So, this was really fun to see the model in action.&lt;/p&gt; &lt;p&gt;Most of the guides show you how to work with just the local tools, which is not ideal when you plan to use the model for bigger, better and managed tools for hundreds of different services. &lt;/p&gt; &lt;p&gt;In this guide, I've covered the model specs and the entire setup, including setting up a Docker container for Ollama and running Ollama WebUI.&lt;/p&gt; &lt;p&gt;And the nice part is that the model setup guide here works for all the other models that support tool calling.&lt;/p&gt; &lt;p&gt;I wrote up the full walkthrough with commands and screenshots:&lt;/p&gt; &lt;p&gt;You can find it here: &lt;a href="https://composio.dev/blog/tool-calling-with-ministral-3b"&gt;MCP tool calling guide with Ministral 3B, Composio, and Ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone else has tested tool calling on Ministral 3 3B (or worked with it using vLLM instead of Ollama), I‚Äôd love to hear what worked best for you, as I couldn't get vLLM to work due to CUDA errors. :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shricodev"&gt; /u/shricodev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pym7c0/running_ministral_3_3b_locally_with_ollama_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pym7c0/running_ministral_3_3b_locally_with_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pym7c0/running_ministral_3_3b_locally_with_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-29T13:24:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzsz23</id>
    <title>Is Ollama Clouda good alternative to other api providers?</title>
    <updated>2025-12-30T20:33:40+00:00</updated>
    <author>
      <name>/u/Excellent_Piccolo848</name>
      <uri>https://old.reddit.com/user/Excellent_Piccolo848</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i was looking at ollama cloud, and thought, that it may be better than other api providers (like togehter ai or deepinfra), especially because of privacy. What are your thoughts on this and about ollama cloud in general?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Piccolo848"&gt; /u/Excellent_Piccolo848 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pzsz23/is_ollama_clouda_good_alternative_to_other_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pzsz23/is_ollama_clouda_good_alternative_to_other_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pzsz23/is_ollama_clouda_good_alternative_to_other_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-30T20:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzny3e</id>
    <title>OllamaFX Client - Add to Ollama oficial list of clients</title>
    <updated>2025-12-30T17:22:53+00:00</updated>
    <author>
      <name>/u/Electronic-Reason582</name>
      <uri>https://old.reddit.com/user/Electronic-Reason582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pzny3e/ollamafx_client_add_to_ollama_oficial_list_of/"&gt; &lt;img alt="OllamaFX Client - Add to Ollama oficial list of clients" src="https://a.thumbs.redditmedia.com/0fwiA1semiCg-bAWYJMGCKlvA47JkGDY4SeXMBgSsq8.jpg" title="OllamaFX Client - Add to Ollama oficial list of clients" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hola, estoy desarrollando un cliente JavafX para Ollama, se llama OllamaFX este es el repo en github &lt;a href="https://github.com/fredericksalazar/OllamaFX"&gt;https://github.com/fredericksalazar/OllamaFX&lt;/a&gt; me gustaria que mi cliente sea agregado en la lista de clientes oficiales de Ollama en su pagina de github, alguien puede indicarme como poder hacerlo? hay que seguir algun estandar o contactar a alguien? Muchas gracias&lt;/p&gt; &lt;p&gt;Hello, I'm developing a JavaFX client for Ollama called OllamaFX. Here's the repository on GitHub: &lt;a href="https://github.com/fredericksalazar/OllamaFX"&gt;https://github.com/fredericksalazar/OllamaFX&lt;/a&gt;. I'd like my client to be added to the list of official Ollama clients on their GitHub page. Can anyone tell me how to do this? Are there any standards I need to follow or someone I should contact? Thank you very much.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Reason582"&gt; /u/Electronic-Reason582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pzny3e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pzny3e/ollamafx_client_add_to_ollama_oficial_list_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pzny3e/ollamafx_client_add_to_ollama_oficial_list_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-30T17:22:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1q04s56</id>
    <title>Has anyone tried routing Claude Code CLI to multiple model providers?</title>
    <updated>2025-12-31T05:18:47+00:00</updated>
    <author>
      <name>/u/Dangerous-Dingo-5169</name>
      <uri>https://old.reddit.com/user/Dangerous-Dingo-5169</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm experimenting with running Claude Code CLI against different backends instead of a single API.&lt;/p&gt; &lt;p&gt;Specifically, I‚Äôm curious whether people have tried:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;using local models for simpler prompts&lt;/li&gt; &lt;li&gt;falling back to cloud models for harder requests&lt;/li&gt; &lt;li&gt;switching providers automatically when one fails&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I hacked together a local proxy to test this idea and it &lt;em&gt;seems&lt;/em&gt; to reduce API usage for normal dev workflows, but I‚Äôm not sure if I‚Äôm missing obvious downsides.&lt;/p&gt; &lt;p&gt;If anyone has experience doing something similar (Databricks, Azure, OpenRouter, Ollama, etc.), I‚Äôd love to hear what worked and what didn‚Äôt.&lt;/p&gt; &lt;p&gt;(If useful, I can share code ‚Äî didn‚Äôt want to lead with a link.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous-Dingo-5169"&gt; /u/Dangerous-Dingo-5169 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q04s56/has_anyone_tried_routing_claude_code_cli_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q04s56/has_anyone_tried_routing_claude_code_cli_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q04s56/has_anyone_tried_routing_claude_code_cli_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-31T05:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0blmz</id>
    <title>Wich model for philosophy / humanities on a MSI rtx 2060 Super (8Gb)?</title>
    <updated>2025-12-31T12:11:34+00:00</updated>
    <author>
      <name>/u/Excellent_Piccolo848</name>
      <uri>https://old.reddit.com/user/Excellent_Piccolo848</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Piccolo848"&gt; /u/Excellent_Piccolo848 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1q0blg9/wich_model_for_philosophy_humanities_on_a_msi_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0blmz/wich_model_for_philosophy_humanities_on_a_msi_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q0blmz/wich_model_for_philosophy_humanities_on_a_msi_rtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-31T12:11:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0iz2f</id>
    <title>M4 chip or older dedicated GPU?</title>
    <updated>2025-12-31T17:49:26+00:00</updated>
    <author>
      <name>/u/grtgbln</name>
      <uri>https://old.reddit.com/user/grtgbln</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grtgbln"&gt; /u/grtgbln &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1q0iypx/m4_chip_or_older_dedicated_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0iz2f/m4_chip_or_older_dedicated_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q0iz2f/m4_chip_or_older_dedicated_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-31T17:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0hwsc</id>
    <title>Built an offline-first vector database (v0.2.0) looking for real-world feedback</title>
    <updated>2025-12-31T17:04:49+00:00</updated>
    <author>
      <name>/u/Serious-Section-5595</name>
      <uri>https://old.reddit.com/user/Serious-Section-5595</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on &lt;strong&gt;SrvDB&lt;/strong&gt;, an &lt;strong&gt;offline embedded vector database&lt;/strong&gt; for local and edge AI use cases.&lt;/p&gt; &lt;p&gt;No cloud. No services. Just files on disk.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What‚Äôs new in v0.2.0:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multiple index modes: Flat, HNSW, IVF, PQ&lt;/li&gt; &lt;li&gt;Adaptive ‚ÄúAUTO‚Äù mode that selects index based on system RAM / dataset size&lt;/li&gt; &lt;li&gt;Exact search + quantized options (trade accuracy vs memory)&lt;/li&gt; &lt;li&gt;Benchmarks included (P99 latency, recall, disk, ingest)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Designed for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local RAG&lt;/li&gt; &lt;li&gt;Edge / IoT&lt;/li&gt; &lt;li&gt;Air-gapped systems&lt;/li&gt; &lt;li&gt;Developers experimenting without cloud dependencies&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Srinivas26k/srvdb"&gt;https://github.com/Srinivas26k/srvdb&lt;/a&gt;&lt;br /&gt; Benchmarks were run on a consumer laptop (details in repo).&lt;br /&gt; I have included the benchmark code run it on your and upload it on the GitHub discussions which helps to improve and add features accordingly. I request for contributors to make the project great.[ &lt;a href="https://github.com/Srinivas26k/srvdb/blob/master/universal_benchmark.py"&gt;https://github.com/Srinivas26k/srvdb/blob/master/universal_benchmark.py&lt;/a&gt; ]&lt;/p&gt; &lt;p&gt;I‚Äôm &lt;strong&gt;not trying to replace Pinecone / FAISS / Qdrant&lt;/strong&gt; this is for people who want something small, local, and predictable.&lt;/p&gt; &lt;p&gt;Would love:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Feedback on benchmarks&lt;/li&gt; &lt;li&gt;Real-world test reports&lt;/li&gt; &lt;li&gt;Criticism on design choices&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer technical questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious-Section-5595"&gt; /u/Serious-Section-5595 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0hwsc/built_an_offlinefirst_vector_database_v020/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0hwsc/built_an_offlinefirst_vector_database_v020/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q0hwsc/built_an_offlinefirst_vector_database_v020/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-31T17:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0n8h9</id>
    <title>EmergentFlow - Visual AI workflow builder with native Ollama support</title>
    <updated>2025-12-31T20:57:13+00:00</updated>
    <author>
      <name>/u/l33t-Mt</name>
      <uri>https://old.reddit.com/user/l33t-Mt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q0n8h9/emergentflow_visual_ai_workflow_builder_with/"&gt; &lt;img alt="EmergentFlow - Visual AI workflow builder with native Ollama support" src="https://b.thumbs.redditmedia.com/T05H1ug7tnfNFJyZodecXX2pKGWHW_D10QV95illc6M.jpg" title="EmergentFlow - Visual AI workflow builder with native Ollama support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1hjueesaslag1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01d473be20f1064fa77b522d54c8ac4702efd081"&gt;https://preview.redd.it/1hjueesaslag1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01d473be20f1064fa77b522d54c8ac4702efd081&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some of you might recognize me from my moondream/minicpm computer use agent posts, or maybe LlamaCards. Ive been tinkering with local AI stuff for a while now.&lt;/p&gt; &lt;p&gt;Im a single dad working full time, so my project time is scattered, but I finally got something to a point worth sharing.&lt;/p&gt; &lt;p&gt;EmergentFlow is a node-based AI workflow builder, but architecturally different from tools like n8n, Flowise, or ComfyUI. Those all run server-side on their cloud or you self-host the backend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EmergentFlow runs the execution engine in your browser.&lt;/strong&gt; Your browser tab is the runtime. When you connect Ollama, calls go directly from your browser to localhost:11434 (configurable). &lt;/p&gt; &lt;p&gt;It supports cloud APIs too (OpenAI, Anthropic, Google, etc.) if you want to mix local + cloud in the same flow. There's a Browser Agent for autonomous research, RAG pipelines, database connectors, hardware control.&lt;/p&gt; &lt;p&gt;Because I want new users to experience the system, I have provided anonymous users without an account, 50 free credits using googles cloud API, these are simply to allow users to see the system in action before requiring they create an account. &lt;/p&gt; &lt;p&gt;Terrified of launching, be gentle. &lt;/p&gt; &lt;p&gt;&lt;a href="https://emergentflow.io/"&gt;https://emergentflow.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Create visual flows directly from your browser.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/l33t-Mt"&gt; /u/l33t-Mt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0n8h9/emergentflow_visual_ai_workflow_builder_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0n8h9/emergentflow_visual_ai_workflow_builder_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q0n8h9/emergentflow_visual_ai_workflow_builder_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-31T20:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q17aha</id>
    <title>?</title>
    <updated>2026-01-01T15:52:28+00:00</updated>
    <author>
      <name>/u/Capital-Job-3592</name>
      <uri>https://old.reddit.com/user/Capital-Job-3592</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're building an observability platform specifically for Al agents and need your input.&lt;/p&gt; &lt;p&gt;The Problem:&lt;/p&gt; &lt;p&gt;Building Al agents that use multiple tools (files, APIs, databases) is getting easier with frameworks like LangChain, CrewAl, etc. But monitoring them? Total chaos.&lt;/p&gt; &lt;p&gt;When an agent makes 20 tool calls and something fails:&lt;/p&gt; &lt;p&gt;Which call failed? What was the error? How much did it cost? Why did the agent make that decision? What We're Building:&lt;/p&gt; &lt;p&gt;A unified observability layer that tracks:&lt;/p&gt; &lt;p&gt;LLM calls (tokens, cost, latency) Tool executions (success/fail/performance) Agent reasoning flow (step-by-step) MCP Server + REST API support The Question:&lt;/p&gt; &lt;p&gt;1.&lt;/p&gt; &lt;p&gt;How are you currently debugging Al agents? 2. What observability features do you wish existed? 3. Would you pay for a dedicated agent observability tool? We're looking for early adopters to test and shape the product&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Capital-Job-3592"&gt; /u/Capital-Job-3592 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q17aha/_/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q17aha/_/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q17aha/_/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-01T15:52:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0tmfl</id>
    <title>Tool Weaver (open sourced) inspired by Anthropic‚Äôs advanced tool use.</title>
    <updated>2026-01-01T02:30:52+00:00</updated>
    <author>
      <name>/u/andavan_ivan</name>
      <uri>https://old.reddit.com/user/andavan_ivan</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andavan_ivan"&gt; /u/andavan_ivan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/mcp/comments/1q0br8n/tool_weaver_open_sourced_inspired_by_anthropics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0tmfl/tool_weaver_open_sourced_inspired_by_anthropics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q0tmfl/tool_weaver_open_sourced_inspired_by_anthropics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-01T02:30:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0rzbw</id>
    <title>Local AI Memory System - Beta Testers Wanted (Ollama + DeepSeek + Knowledge Graphs)</title>
    <updated>2026-01-01T00:59:23+00:00</updated>
    <author>
      <name>/u/danny_094</name>
      <uri>https://old.reddit.com/user/danny_094</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q0rzbw/local_ai_memory_system_beta_testers_wanted_ollama/"&gt; &lt;img alt="Local AI Memory System - Beta Testers Wanted (Ollama + DeepSeek + Knowledge Graphs)" src="https://external-preview.redd.it/qaD9_KusmJTWBBNGXLe0E9F1-LYBD1V-XQC4RKqFj-0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44ee218300aca4f4e2c3c14df5d8a8d11309e2e7" title="Local AI Memory System - Beta Testers Wanted (Ollama + DeepSeek + Knowledge Graphs)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;**The Problem:*\&lt;/strong&gt;*&lt;/p&gt; &lt;p&gt; Your AI forgets everything between conversations. You end up re-explaining context every single time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;**The Solution:*\&lt;/strong&gt;* &lt;/p&gt; &lt;p&gt;I built &amp;quot;Jarvis&amp;quot; - a local AI assistant with actual long-term memory that works across conversations. And my latest pipeline update is the graph.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;**Example:*\&lt;/strong&gt;* ``` Day 1: &amp;quot;My favorite pizza is Tunfisch&amp;quot; Day 7: &amp;quot;What's my favorite pizza?&amp;quot; AI: &amp;quot;Your favorite pizza is Tunfisch-Pizza!&amp;quot; ‚úÖ ``` &lt;/p&gt; &lt;p&gt;&lt;strong&gt;**How it works:*\&lt;/strong&gt;*&lt;/p&gt; &lt;p&gt; - Semantic search finds relevant memories (not just keywords)&lt;/p&gt; &lt;p&gt; - Knowledge graph connects related facts - Auto-maintenance (deduplicates, merges similar entries) &lt;/p&gt; &lt;p&gt;- 100% local (your data stays on YOUR machine)&lt;/p&gt; &lt;p&gt; &lt;strong&gt;**Tech Stack:*\&lt;/strong&gt;* &lt;/p&gt; &lt;p&gt;- Ollama (DeepSeek-R1 for reasoning, Qwen for control) &lt;/p&gt; &lt;p&gt;- SQLite + vector embeddings &lt;/p&gt; &lt;p&gt;- Knowledge graphs with semantic/temporal edges &lt;/p&gt; &lt;p&gt;- MCP (Model Context Protocol) architecture&lt;/p&gt; &lt;p&gt; - Docker compose setup &lt;/p&gt; &lt;p&gt;&lt;strong&gt;**Current Status:*\&lt;/strong&gt;* &lt;/p&gt; &lt;p&gt;- 96.5% test coverage (57 passing tests) &lt;/p&gt; &lt;p&gt;- Graph-based memory optimization &lt;/p&gt; &lt;p&gt;-Cross-conversation retrieval working&lt;/p&gt; &lt;p&gt; - Automatic duplicate detection&lt;/p&gt; &lt;p&gt; - Production-ready (running on my Ubuntu server)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;**Looking for Beta Testers:*\&lt;/strong&gt;*&lt;/p&gt; &lt;p&gt; - Linux users comfortable with Docker &lt;/p&gt; &lt;p&gt;- Willing to use it for ~1 week&lt;/p&gt; &lt;p&gt; - Report bugs and memory accuracy&lt;/p&gt; &lt;p&gt; - Share feedback on usefulness &lt;/p&gt; &lt;p&gt;&lt;strong&gt;**What you get:*\&lt;/strong&gt;* &lt;/p&gt; &lt;p&gt;- Your own local AI with persistent memory&lt;/p&gt; &lt;p&gt; - Full data privacy (everything stays local) &lt;/p&gt; &lt;p&gt;- One-command Docker setup &lt;/p&gt; &lt;p&gt;- GitHub repo + documentation &lt;/p&gt; &lt;p&gt;&lt;strong&gt;**Why this matters:*\&lt;/strong&gt;* &lt;/p&gt; &lt;p&gt;Local AI is great for privacy, but current solutions forget context constantly. This bridges that gap - you get privacy AND memory. Interested? Comment below and I'll share: - GitHub repo - Setup instructions - Bug report template Looking forward to getting this in real users' hands! üöÄ &lt;/p&gt; &lt;p&gt;--- &lt;/p&gt; &lt;p&gt;&lt;strong&gt;**Edit:*\&lt;/strong&gt;* Just fixed a critical cross-conversation retrieval bug today - great timing for beta testing! üòÑ ```&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/danny094/Jarvis"&gt;https://github.com/danny094/Jarvis&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1q0rzbw/video/fb7n6q0dzmag1/player"&gt;https://reddit.com/link/1q0rzbw/video/fb7n6q0dzmag1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danny_094"&gt; /u/danny_094 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0rzbw/local_ai_memory_system_beta_testers_wanted_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q0rzbw/local_ai_memory_system_beta_testers_wanted_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q0rzbw/local_ai_memory_system_beta_testers_wanted_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-01T00:59:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1ictw</id>
    <title>igpu + dgpu for reducing cpu load</title>
    <updated>2026-01-01T23:19:53+00:00</updated>
    <author>
      <name>/u/sultan_papagani</name>
      <uri>https://old.reddit.com/user/sultan_papagani</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i wanted to share my findings on using iGPU + dGPU to reduce cpu load during inference.&lt;/p&gt; &lt;p&gt;Prompt: write a booking website for hotels Model: gpt-oss:latest igpu: intel arrow lake integrated graphics dgpu: rtx5060 system ram: 32gb&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;CPU offloading + dGPU (cuda)&lt;/p&gt; &lt;p&gt;Size: 14GB&lt;br /&gt; Processor: 57% CPU / 43% GPU&lt;br /&gt; Context: 32K All 8 CPU cores fully utilized (100% per core) Total CPU load: ~33‚Äì47% Fans ramp up and system is loud&lt;/p&gt; &lt;p&gt;Total duration: 2m 42s Prompt eval: 73 tokens @ ~68 tok/s Generation: 3756 tokens @ ~25.7 tok/s&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;iGPU + dGPU only (vulkan)&lt;/p&gt; &lt;p&gt;Size: 14GB&lt;br /&gt; Processor: 100% GPU&lt;br /&gt; Context: 32K CPU usage drops to ~1‚Äì6% System stays quiet&lt;/p&gt; &lt;p&gt;Total duration: 10m 30s Prompt eval: 73 tokens @ ~46.8 tok/s Generation: 4213 tokens @ ~6.7 tok/s&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Running fully on iGPU + dGPU dramatically reduces CPU load and noise, but generation speed drops significantly. For long or non-interactive runs, this tradeoff can be worth it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sultan_papagani"&gt; /u/sultan_papagani &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q1ictw/igpu_dgpu_for_reducing_cpu_load/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q1ictw/igpu_dgpu_for_reducing_cpu_load/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q1ictw/igpu_dgpu_for_reducing_cpu_load/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-01T23:19:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1q179er</id>
    <title>Ollama models to specific GPU</title>
    <updated>2026-01-01T15:51:10+00:00</updated>
    <author>
      <name>/u/NormalSmoke1</name>
      <uri>https://old.reddit.com/user/NormalSmoke1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to hard force the OLLAMA model to specifically sit on a designated GPU. As I looked through the OLLAMA docs, it says to use the CUDA visible devices in the python script, but isn't there somewhere in the unix configuration I can set at startup? I have multiple 3090's and I would like to have the model on sit on one, so the other is free for other agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NormalSmoke1"&gt; /u/NormalSmoke1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q179er/ollama_models_to_specific_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q179er/ollama_models_to_specific_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q179er/ollama_models_to_specific_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-01T15:51:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1q23t1w</id>
    <title>Registry off or is my connection?</title>
    <updated>2026-01-02T16:54:44+00:00</updated>
    <author>
      <name>/u/OppenheimerDaSilva</name>
      <uri>https://old.reddit.com/user/OppenheimerDaSilva</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi fellas, since december of last year I cannot pull any image of ollama, I always receive timeout. It's something wth my connection?&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;ollama pull gpt-oss:20b ‚îÄ‚ïØ&lt;/p&gt; &lt;p&gt;pulling manifest&lt;/p&gt; &lt;p&gt;Error: pull model manifest: Get &amp;quot;&lt;a href="https://registry.ollama.ai/v2/library/gpt-oss/manifests/20b%22:"&gt;https://registry.ollama.ai/v2/library/gpt-oss/manifests/20b&amp;quot;:&lt;/a&gt; dial tcp 172.67.182.229:443: i/o timeout&lt;br /&gt; ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OppenheimerDaSilva"&gt; /u/OppenheimerDaSilva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q23t1w/registry_off_or_is_my_connection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q23t1w/registry_off_or_is_my_connection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q23t1w/registry_off_or_is_my_connection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-02T16:54:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2dgwf</id>
    <title>Anyway to make joycaption into a chatbot?</title>
    <updated>2026-01-02T22:58:30+00:00</updated>
    <author>
      <name>/u/Zantorn</name>
      <uri>https://old.reddit.com/user/Zantorn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Complete noob here&lt;/p&gt; &lt;p&gt;Anyway to make joycaption into a chatbot? &lt;/p&gt; &lt;p&gt;Want to have it look at images and react to the, give opinions, have conversation about them etc. Is this possible to do locally? If so what should i use to get started? I have Ollama and LMStudio but not sure if those are the best options for this as im pretty new to&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zantorn"&gt; /u/Zantorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2dgwf/anyway_to_make_joycaption_into_a_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2dgwf/anyway_to_make_joycaption_into_a_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q2dgwf/anyway_to_make_joycaption_into_a_chatbot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-02T22:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1ud1h</id>
    <title>Does Open WebUI actually crawl links with Ollama, or is it just hallucinating based on the URL?</title>
    <updated>2026-01-02T09:32:31+00:00</updated>
    <author>
      <name>/u/Whole-Competition223</name>
      <uri>https://old.reddit.com/user/Whole-Competition223</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I recently started using &lt;strong&gt;Open WebUI&lt;/strong&gt; integrated with &lt;strong&gt;Ollama&lt;/strong&gt;. Today, I tried giving a specific URL to an LLM using the &lt;code&gt;#&lt;/code&gt; prefix and asked it to summarize the content in Korean.&lt;/p&gt; &lt;p&gt;At first, I was quite impressed because the summary looked very plausible and well-structured. However, I later found out that Ollama models, by default, cannot access the internet or visit external links.&lt;/p&gt; &lt;p&gt;This leaves me with a few questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;How did it generate the summary?&lt;/strong&gt; Was the LLM just &amp;quot;guessing&amp;quot; the content based on the words in the URL and its pre-existing training data? Or does Open WebUI pass some scraped metadata to the model?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Is there a way to enable &amp;quot;real&amp;quot; web browsing?&lt;/strong&gt; I want the model to actually visit the link and analyze the current page content. Are there specific functions, tools, or configurations in Open WebUI (like RAG settings) that allow Ollama models to access external websites?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'd love to hear how you guys handle web-based tasks with local LLMs. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Competition223"&gt; /u/Whole-Competition223 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q1ud1h/does_open_webui_actually_crawl_links_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q1ud1h/does_open_webui_actually_crawl_links_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q1ud1h/does_open_webui_actually_crawl_links_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-02T09:32:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2hp7u</id>
    <title>Integrated Mistral Nemo (12B) into a custom Space Discovery Engine (Project ARIS) for local anomaly detection.</title>
    <updated>2026-01-03T01:58:21+00:00</updated>
    <author>
      <name>/u/Limp-Regular3741</name>
      <uri>https://old.reddit.com/user/Limp-Regular3741</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share a real-world use case for local LLMs. I‚Äôve built a discovery engine called Project ARIS that uses Mistral Nemo as a reasoning layer for astronomical data.&lt;/p&gt; &lt;p&gt;The Stack:&lt;/p&gt; &lt;p&gt;Model: Mistral Nemo 12B (Q4_K_M) running via Ollama.&lt;/p&gt; &lt;p&gt;Hardware: Lenovo Yoga 7 (Ryzen AI 7, 24GB RAM) on Nobara Linux.&lt;/p&gt; &lt;p&gt;Integration: Tauri/Rust backend calling the Ollama API.&lt;/p&gt; &lt;p&gt;How I‚Äôm using the LLM:&lt;/p&gt; &lt;p&gt;Contextual Memory: It reads previous session reports from a local folder and greets me with a verbal recap on boot.&lt;/p&gt; &lt;p&gt;Intent Parsing: I built a custom terminal where Nemo translates &amp;quot;fuzzy&amp;quot; natural language into structured MAST API queries.&lt;/p&gt; &lt;p&gt;Anomaly Scoring: It parses spectral data to flag &amp;quot;out of the ordinary&amp;quot; signatures that don't fit standard star/planet profiles.&lt;/p&gt; &lt;p&gt;It‚Äôs amazing how much a 12B model can do when given a specific toolset and a sandboxed terminal. Happy to answer any questions about the Rust/Ollama bridge!&lt;/p&gt; &lt;p&gt;A preview of Project ARIS can be found here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/glowseedstudio/Project-ARIS"&gt;https://github.com/glowseedstudio/Project-ARIS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limp-Regular3741"&gt; /u/Limp-Regular3741 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2hp7u/integrated_mistral_nemo_12b_into_a_custom_space/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2hp7u/integrated_mistral_nemo_12b_into_a_custom_space/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q2hp7u/integrated_mistral_nemo_12b_into_a_custom_space/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T01:58:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q36m42</id>
    <title>What model to use and how to disable using cloud.</title>
    <updated>2026-01-03T21:24:58+00:00</updated>
    <author>
      <name>/u/ItsWappers</name>
      <uri>https://old.reddit.com/user/ItsWappers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just don't want to use credits and want to know what model is the best for offline use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ItsWappers"&gt; /u/ItsWappers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q36m42/what_model_to_use_and_how_to_disable_using_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q36m42/what_model_to_use_and_how_to_disable_using_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q36m42/what_model_to_use_and_how_to_disable_using_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T21:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2isyx</id>
    <title>Run Claude Code with ollama without losing any single feature offered by Anthropic backend</title>
    <updated>2026-01-03T02:47:39+00:00</updated>
    <author>
      <name>/u/Dangerous-Dingo-5169</name>
      <uri>https://old.reddit.com/user/Dangerous-Dingo-5169</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! Sharing an open-source project that might be useful:&lt;/p&gt; &lt;p&gt;Lynkr connects AI coding tools (like Claude Code) to multiple LLM providers with intelligent routing.&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Route between multiple providers: Databricks, Azure Ai Foundry, OpenRouter, Ollama,llama.cpp, OpenAi&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Cost optimization through hierarchical routing, heavy prompt caching&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Production-ready: circuit breakers, load shedding, monitoring&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It supports all the features offered by claude code like sub agents, skills , mcp , plugins etc unlike other proxies which only supports basic tool callings and chat completions.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Great for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Reducing API costs as it supports hierarchical routing where you can route requstes to smaller local models and later switch to cloud LLMs automatically.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Using enterprise infrastructure (Azure)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;- Local LLM experimentation&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;npm install -g lynkr&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Fast-Editor/Lynkr"&gt;https://github.com/Fast-Editor/Lynkr&lt;/a&gt; (Apache 2.0)&lt;/p&gt; &lt;p&gt;Would love to get your feedback on this one. Please drop a star on the repo if you found it helpful&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous-Dingo-5169"&gt; /u/Dangerous-Dingo-5169 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2isyx/run_claude_code_with_ollama_without_losing_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2isyx/run_claude_code_with_ollama_without_losing_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q2isyx/run_claude_code_with_ollama_without_losing_any/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T02:47:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1q37qyq</id>
    <title>Any Vision model on pair with GPT-OSS 120B?</title>
    <updated>2026-01-03T22:10:21+00:00</updated>
    <author>
      <name>/u/Altair12311</name>
      <uri>https://old.reddit.com/user/Altair12311</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! new to local ai selfhosting!&lt;/p&gt; &lt;p&gt;I do enjoy a lot my experiences and now i was having a tiny doubt... I do like GPT-OSS but i do enjoy a lot share &amp;quot;Images&amp;quot; with the AI like GPT-5 so the AI can watch the image and help me with the problem... GPT-OSS 120B doesn't have that feature and cannot recognize images as far i know... &lt;/p&gt; &lt;p&gt;Which other option i do have?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altair12311"&gt; /u/Altair12311 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q37qyq/any_vision_model_on_pair_with_gptoss_120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q37qyq/any_vision_model_on_pair_with_gptoss_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q37qyq/any_vision_model_on_pair_with_gptoss_120b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T22:10:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2wny9</id>
    <title>Offline agent testing chat mode using Ollama as the judge (EvalView)</title>
    <updated>2026-01-03T15:00:12+00:00</updated>
    <author>
      <name>/u/hidai25</name>
      <uri>https://old.reddit.com/user/hidai25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1q2wny9/offline_agent_testing_chat_mode_using_ollama_as/"&gt; &lt;img alt="Offline agent testing chat mode using Ollama as the judge (EvalView)" src="https://external-preview.redd.it/OGQksTmq-Xi-DKXl6h0CyL7yaRb404yUr8mvZQNBiSU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a5fb41b62d23c04e5d8feaa779c16a229dd5ca2f" title="Offline agent testing chat mode using Ollama as the judge (EvalView)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick demo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1q2wny9/video/z75urjhci5bg1/player"&gt;https://reddit.com/link/1q2wny9/video/z75urjhci5bg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve been working on EvalView (pytest-style regression tests for tool-using agents) and just added an interactive chat mode that runs fully local with Ollama.&lt;/p&gt; &lt;p&gt;Instead of remembering commands or writing YAML up front, you can just ask:&lt;/p&gt; &lt;p&gt;‚Äúrun my tests‚Äù&lt;/p&gt; &lt;p&gt;‚Äúwhy did checkout fail?‚Äù&lt;/p&gt; &lt;p&gt;‚Äúdiff this run vs yesterday‚Äôs golden baseline‚Äù&lt;/p&gt; &lt;p&gt;It uses your local Ollama model for the chat + for LLM-as-judge grading. No tokens leave your machine, no API costs (unless you count electricity and emotional damage).&lt;/p&gt; &lt;p&gt;Setup:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama pull llama3.2&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install evalview&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;evalview chat --provider ollama --model llama3.2&lt;/code&gt;&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;p&gt;- Runs your agent test suite + diffs against baselines&lt;/p&gt; &lt;p&gt;- Grades outputs with the local model (LLM-as-judge)&lt;/p&gt; &lt;p&gt;- Shows tool-call / latency / token (and cost estimate) diffs between runs&lt;/p&gt; &lt;p&gt;- Lets you drill into failures conversationally&lt;/p&gt; &lt;p&gt;Repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/hidai25/eval-view"&gt;https://github.com/hidai25/eval-view&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Question for the Ollama crowd:&lt;/p&gt; &lt;p&gt;What models have you found work well for &amp;quot;reasoning about agent behavior&amp;quot; and judging tool calls?&lt;/p&gt; &lt;p&gt;I‚Äôve been using llama3.2 but I‚Äôm curious if mistral or deepseek-coder style models do better for tool-use grading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hidai25"&gt; /u/hidai25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2wny9/offline_agent_testing_chat_mode_using_ollama_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q2wny9/offline_agent_testing_chat_mode_using_ollama_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q2wny9/offline_agent_testing_chat_mode_using_ollama_as/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T15:00:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q33op3</id>
    <title>Radxa Orion O6 LLM Benchmarks (Ollama, Debian 12 Headless, 64GB RAM) ‚Äì 30B on ARM is actually usable</title>
    <updated>2026-01-03T19:30:06+00:00</updated>
    <author>
      <name>/u/RasPiBuilder</name>
      <uri>https://old.reddit.com/user/RasPiBuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent some time benchmarking the Radxa Orion O6 running Debian 12 + Ollama after sorting out early thermal issues. Sharing results in case they‚Äôre helpful for anyone considering this board for local LLM inference. One important note is that the official Radxa Debian 12 image for the Orion O6 only ships with a desktop environment. For these tests, I removed the desktop and ran the system headless, which helped reduce background load and thermals.&lt;/p&gt; &lt;h1&gt;Hardware / Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Radxa Orion O6&lt;/li&gt; &lt;li&gt;64 GB RAM&lt;/li&gt; &lt;li&gt;Powered over USB-C PD&lt;/li&gt; &lt;li&gt;Radxa AI Kit case (this significantly improved thermals)&lt;/li&gt; &lt;li&gt;Debian 12 (official Radxa image, desktop removed ‚Üí headless)&lt;/li&gt; &lt;li&gt;Ollama (CPU-only)&lt;/li&gt; &lt;li&gt;CPU governor: &lt;code&gt;schedutil&lt;/code&gt; (performed better than forcing &lt;code&gt;performance&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Adequate cooling and airflow (critical on this board)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results (tokens/sec = eval rate)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Qwen3-Next&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;2.41 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 1203&lt;/li&gt; &lt;li&gt;Total time: ~8m25s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Nemotron-3-nano&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;6.04 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 836&lt;/li&gt; &lt;li&gt;Total time: ~2m21s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen3:30B (MoE)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;6.42 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 709&lt;/li&gt; &lt;li&gt;Total time: ~1m52s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen3:30B-Instruct (MoE)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: 6.81 &lt;strong&gt;tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 148&lt;/li&gt; &lt;li&gt;Total time: ~23s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen3:14B (dense)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;3.66 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 328&lt;/li&gt; &lt;li&gt;Total time: ~1m33s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GPT-OSS&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;3.01 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 543&lt;/li&gt; &lt;li&gt;Total time: ~3m09s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Llama3:8B&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;6.42 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 273&lt;/li&gt; &lt;li&gt;Total time: ~45s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-R1:1.5B&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;19.57 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 44&lt;/li&gt; &lt;li&gt;Total time: ~2.7s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Granite 3.1 MoE (3B)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eval rate: &lt;strong&gt;17.87 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Eval tokens: 66&lt;/li&gt; &lt;li&gt;Total time: ~4.8s&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Observations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;30B-class models &lt;em&gt;do&lt;/em&gt; run on the Orion O6 ‚Äî slow, but usable for experimentation.&lt;/li&gt; &lt;li&gt;Larger models (8B‚Äì30B) cluster around ~3‚Äì6 tok/s, suggesting a memory-bandwidth / ARM CPU ceiling, not a power or clock issue.&lt;/li&gt; &lt;li&gt;Smaller MoE models (Granite 3B, DeepSeek 1.5B) feel very responsive.&lt;/li&gt; &lt;li&gt;schedutil governor consistently outperformed performance in testing.&lt;/li&gt; &lt;li&gt;Thermals matter a lot: moving to the Radxa AI Kit case and running headless eliminated thermal shutdowns seen earlier.&lt;/li&gt; &lt;li&gt;USB-C PD has been stable so far with adequate cooling.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;The Orion O6 isn‚Äôt a GPU replacement, but as a compact ARM server with 64 GB RAM that can genuinely run 30B MoE models, it exceeded my expectations. Running Debian headless and using the AI Kit case makes a real difference. With realistic performance expectations, it‚Äôs a solid platform for local LLM experimentation.&lt;/p&gt; &lt;p&gt;Happy to answer questions or run additional tests if people are interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RasPiBuilder"&gt; /u/RasPiBuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q33op3/radxa_orion_o6_llm_benchmarks_ollama_debian_12/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1q33op3/radxa_orion_o6_llm_benchmarks_ollama_debian_12/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1q33op3/radxa_orion_o6_llm_benchmarks_ollama_debian_12/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-03T19:30:06+00:00</published>
  </entry>
</feed>
