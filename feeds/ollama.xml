<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-25T14:24:52+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1p4f9j0</id>
    <title>GPT-OSS 20B: Capacit√©s et Tests</title>
    <updated>2025-11-23T06:04:36+00:00</updated>
    <author>
      <name>/u/Desperate_Yellow_541</name>
      <uri>https://old.reddit.com/user/Desperate_Yellow_541</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Yellow_541"&gt; /u/Desperate_Yellow_541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ollama/comments/1p4f8iv/gptoss_20b_capacit√©s_et_tests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4f9j0/gptoss_20b_capacit√©s_et_tests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4f9j0/gptoss_20b_capacit√©s_et_tests/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T06:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4qhpr</id>
    <title>Manus</title>
    <updated>2025-11-23T16:09:16+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://manus.im/invitation/BHKBHIT0WJFORVO"&gt;https://manus.im/invitation/BHKBHIT0WJFORVO&lt;/a&gt; login with gmail or apple, or microsoft&lt;/p&gt; &lt;p&gt;Find redeem section in invite friends and use this code to get 1000 credits: njexode&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4qhpr/manus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4qhpr/manus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4qhpr/manus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T16:09:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4ie4a</id>
    <title>Help with a prompt?</title>
    <updated>2025-11-23T09:19:57+00:00</updated>
    <author>
      <name>/u/SystemAromatic</name>
      <uri>https://old.reddit.com/user/SystemAromatic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, i'm having a problem. There is this program that gives me a question and 4 Answers but i cant copypaste and i'm pretty lazy to rewrite it all. I've Tried a couple of prompt but i'm pretty new to this and i don't really know if there even is a proper way to do what i want to do, any suggestion? &lt;/p&gt; &lt;p&gt;Tl;dr: Prompt for ollama to read my screen (?) And answer without copypaste &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SystemAromatic"&gt; /u/SystemAromatic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4ie4a/help_with_a_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4ie4a/help_with_a_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4ie4a/help_with_a_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T09:19:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4jen4</id>
    <title>ToolNeuron ‚Äî Privacy-first AI hub on Android (offline + online models, plugin support)</title>
    <updated>2025-11-23T10:23:14+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p4jen4/toolneuron_privacyfirst_ai_hub_on_android_offline/"&gt; &lt;img alt="ToolNeuron ‚Äî Privacy-first AI hub on Android (offline + online models, plugin support)" src="https://external-preview.redd.it/AuuQ6Jw6VH9e6ZU098Q0G_x_fI41xouhvcXLl_FeCi8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a50a03c0422760aefd8861c44779d24f9c6b02ab" title="ToolNeuron ‚Äî Privacy-first AI hub on Android (offline + online models, plugin support)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://tool-neuron.vercel.app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4jen4/toolneuron_privacyfirst_ai_hub_on_android_offline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4jen4/toolneuron_privacyfirst_ai_hub_on_android_offline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T10:23:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4lxnr</id>
    <title>Best Local Coding Agent Model for 64GB RAM and 12GB VRAM?</title>
    <updated>2025-11-23T12:49:55+00:00</updated>
    <author>
      <name>/u/fallen0523</name>
      <uri>https://old.reddit.com/user/fallen0523</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallen0523"&gt; /u/fallen0523 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1p4lwyc/best_local_coding_agent_model_for_64gb_ram_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4lxnr/best_local_coding_agent_model_for_64gb_ram_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4lxnr/best_local_coding_agent_model_for_64gb_ram_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T12:49:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4od24</id>
    <title>Summarize and manage local files?</title>
    <updated>2025-11-23T14:42:32+00:00</updated>
    <author>
      <name>/u/cl326</name>
      <uri>https://old.reddit.com/user/cl326</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a huge number of files on my laptop. They are not well organized or named. I‚Äôve removed all duplicates that I can by comparing hashes and names. Now I‚Äôd like to use Ollama to summarize each file in a folder so ai can get an idea of what the file is about if I can‚Äôt tell by the name. The files are mostly MS Office documents, PDFs, and images. Is there a model that you‚Äôd suggest? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cl326"&gt; /u/cl326 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4od24/summarize_and_manage_local_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4od24/summarize_and_manage_local_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4od24/summarize_and_manage_local_files/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T14:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3vpni</id>
    <title>Your local Ollama agents can be just as good as closed-source models - I open-sourced Stanford's ACE framework that makes agents learn from mistakes</title>
    <updated>2025-11-22T15:17:51+00:00</updated>
    <author>
      <name>/u/cheetguy</name>
      <uri>https://old.reddit.com/user/cheetguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I implemented Stanford's &lt;a href="https://arxiv.org/abs/2510.04618"&gt;Agentic Context Engineering paper&lt;/a&gt; for Ollama. The framework makes agents learn from their own execution feedback through in-context learning instead of fine-tuning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; Agent runs task ‚Üí reflects on what worked/failed ‚Üí curates strategies into playbook ‚Üí uses playbook on next run&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Improvement:&lt;/strong&gt; Paper shows +17.1pp accuracy improvement vs base LLM (‚âà+40% relative improvement) on agent benchmarks (DeepSeek-V3.1 non-thinking mode), helping close the gap with closed-source models. All through in-context learning (no fine-tuning needed).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Open-Source Implementation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop into existing agents in ~10 lines of code&lt;/li&gt; &lt;li&gt;Works with any Ollama model (Llama, Qwen, Mistral, DeepSeek, etc.)&lt;/li&gt; &lt;li&gt;Real-world test on browser automation agent: &lt;ul&gt; &lt;li&gt;30% ‚Üí 100% success rate&lt;/li&gt; &lt;li&gt;82% fewer steps&lt;/li&gt; &lt;li&gt;65% decrease in token cost&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;https://github.com/kayba-ai/agentic-context-engine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ollama Starter Template: &lt;a href="https://github.com/kayba-ai/agentic-context-engine/blob/main/examples/ollama/ollama_starter_template.py"&gt;https://github.com/kayba-ai/agentic-context-engine/blob/main/examples/ollama/ollama_starter_template.py&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear if anyone tries this with Ollama! Especially curious how it performs with different Ollama models.&lt;/p&gt; &lt;p&gt;I'm currently actively improving this based on feedback - &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;‚≠ê the repo&lt;/a&gt; so you can stay updated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cheetguy"&gt; /u/cheetguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p3vpni/your_local_ollama_agents_can_be_just_as_good_as/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-22T15:17:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p52b8z</id>
    <title>RAG follow-ups not working ‚Äî Qwen2.5 ignores previous context and gives unrelated answers</title>
    <updated>2025-11-24T00:10:24+00:00</updated>
    <author>
      <name>/u/NoBlackberry3264</name>
      <uri>https://old.reddit.com/user/NoBlackberry3264</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm building a &lt;strong&gt;RAG-based chat system&lt;/strong&gt; using FastAPI + &lt;strong&gt;Qwen/Qwen2.5-7B-Instruct&lt;/strong&gt;, and I‚Äôm running into an issue with follow-up queries.&lt;/p&gt; &lt;p&gt;The first query works fine, retrieving relevant documents from my knowledge base. But when the user asks a follow-up question, the model completely ignores previous context and fetches unrelated information.&lt;/p&gt; &lt;h1&gt;Example Payload (Client Request)&lt;/h1&gt; &lt;p&gt;Here‚Äôs the structure of the payload my client sends:&lt;br /&gt; {&lt;/p&gt; &lt;p&gt;&amp;quot;system_persona&amp;quot;: &amp;quot;KB&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;system_prompt&amp;quot;: { ... }, &lt;/p&gt; &lt;p&gt;&amp;quot;context&amp;quot;: [&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;pageUrl&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;sourceUrl&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;pageUrl&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;sourceUrl&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;],&lt;/p&gt; &lt;p&gt;&amp;quot;chat_history&amp;quot;: [&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;query&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;response&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;query&amp;quot;: &amp;quot;...&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;response&amp;quot;: &amp;quot;...&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;],&lt;/p&gt; &lt;p&gt;&amp;quot;query&amp;quot;: &amp;quot;nabil bank ko baryama bhana?&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Issue:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Follow-ups are not linked to previous conversation.&lt;/li&gt; &lt;li&gt;Chat history is sent but not effectively used.&lt;/li&gt; &lt;li&gt;Retrieval is based only on the latest query.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoBlackberry3264"&gt; /u/NoBlackberry3264 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p52b8z/rag_followups_not_working_qwen25_ignores_previous/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p52b8z/rag_followups_not_working_qwen25_ignores_previous/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p52b8z/rag_followups_not_working_qwen25_ignores_previous/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T00:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4wq3l</id>
    <title>ollama troubles</title>
    <updated>2025-11-23T20:15:31+00:00</updated>
    <author>
      <name>/u/Remote-Ad8602</name>
      <uri>https://old.reddit.com/user/Remote-Ad8602</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi people&lt;br /&gt; I recently heard about ollama and tought of giving it a try after installing the app i tried to download some existing models like gpt and qwen when try to give a prompt for the model the screen keeps loading for long time and so on, one time i had to wait 44 mins just to see an error message so i tried removing the app and reinstall again even after multiple tries I still couldn't figure out what's wrong is it my computer or some problems with the app, I use a Macbook Air M4 chip &lt;/p&gt; &lt;p&gt;has anyone faced the same issue, please let me know is there any remedy to get it working normally let me know guys....&lt;/p&gt; &lt;p&gt;cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remote-Ad8602"&gt; /u/Remote-Ad8602 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4wq3l/ollama_troubles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4wq3l/ollama_troubles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4wq3l/ollama_troubles/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T20:15:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4z140</id>
    <title>Need some honest opinions on GPU Ai in a box</title>
    <updated>2025-11-23T21:49:26+00:00</updated>
    <author>
      <name>/u/Whyme-__-</name>
      <uri>https://old.reddit.com/user/Whyme-__-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is related to Nvidia Spark and its competitors. I have a use case where I have to deliver my Ai software to deploy on customer infrastructure. I have 3 8b models fine tuned for each use case. I want to know if using a Nvidia spark or similar GPU in a box is worth the investment for privacy, speed and economics. &lt;/p&gt; &lt;p&gt;For my use case my models and software burn about $2000 per month if I rent a pod using runpod and I have to be extra careful due to rate limits. I want to consider running my models using llamacpp or ollama or offering direct inference for customer using their own on prem machine shipped by me. &lt;/p&gt; &lt;p&gt;Here are my 2 concern:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Are these machines stable enough to deploy in production environments? I know they run Linux and my software stack is dockerized so won‚Äôt affect much. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Cost: I know as we enter the new year GPU cost might go down but should that be something to wait it out or get 1 DGX spark box and test things out to see the functionality and ease of deployment? I can always repurpose the box for my startup instead of relying on Runpod GPU. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This community has helped me a lot in the past, I‚Äôm hoping to get some answers from the community regarding these issues. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whyme-__-"&gt; /u/Whyme-__- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4z140/need_some_honest_opinions_on_gpu_ai_in_a_box/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4z140/need_some_honest_opinions_on_gpu_ai_in_a_box/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4z140/need_some_honest_opinions_on_gpu_ai_in_a_box/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T21:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1p4o31k</id>
    <title>Nanocoder VS Code Plugin is Coming Along!</title>
    <updated>2025-11-23T14:30:09+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p4o31k/nanocoder_vs_code_plugin_is_coming_along/"&gt; &lt;img alt="Nanocoder VS Code Plugin is Coming Along!" src="https://preview.redd.it/0bdpxyrvm03g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a452110a43bda75376e5ffac7e29e49df77ae63" title="Nanocoder VS Code Plugin is Coming Along!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0bdpxyrvm03g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p4o31k/nanocoder_vs_code_plugin_is_coming_along/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p4o31k/nanocoder_vs_code_plugin_is_coming_along/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-23T14:30:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5b6pd</id>
    <title>Win 7 days of unlimited API access on GLM-4.6! 7 winners</title>
    <updated>2025-11-24T07:53:03+00:00</updated>
    <author>
      <name>/u/cobra91310</name>
      <uri>https://old.reddit.com/user/cobra91310</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cobra91310"&gt; /u/cobra91310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ZaiGLM/comments/1p5b6cw/win_7_days_of_unlimited_api_access_on_glm46_7/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5b6pd/win_7_days_of_unlimited_api_access_on_glm46_7/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5b6pd/win_7_days_of_unlimited_api_access_on_glm46_7/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T07:53:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p56i8v</id>
    <title>M.I.M.I.R - Now with visual intelligence built in for embeddings - MIT licensed - use with local ollama or llama.cpp for full control over your data</title>
    <updated>2025-11-24T03:29:23+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p56i8v/mimir_now_with_visual_intelligence_built_in_for/"&gt; &lt;img alt="M.I.M.I.R - Now with visual intelligence built in for embeddings - MIT licensed - use with local ollama or llama.cpp for full control over your data" src="https://preview.redd.it/icw2e1nbj43g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6af6e5db3bde853b33115fc981027e4bea84b11" title="M.I.M.I.R - Now with visual intelligence built in for embeddings - MIT licensed - use with local ollama or llama.cpp for full control over your data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/icw2e1nbj43g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p56i8v/mimir_now_with_visual_intelligence_built_in_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p56i8v/mimir_now_with_visual_intelligence_built_in_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T03:29:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5hoqh</id>
    <title>Mistral-Small3.2:latest - Broken after a recent Ollama update?</title>
    <updated>2025-11-24T13:56:05+00:00</updated>
    <author>
      <name>/u/Comfortable_Ad_8117</name>
      <uri>https://old.reddit.com/user/Comfortable_Ad_8117</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p5hoqh/mistralsmall32latest_broken_after_a_recent_ollama/"&gt; &lt;img alt="Mistral-Small3.2:latest - Broken after a recent Ollama update?" src="https://b.thumbs.redditmedia.com/Rw0CGw1zj8CmJZE93FbqE6s_PBI7HNwH1lT2EDsSJss.jpg" title="Mistral-Small3.2:latest - Broken after a recent Ollama update?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone else seen this behavior? Repetitive characters when interacting with the 24b Mistral model? This was working fine until a recent update of Ollama. Any suggestions for an alternative model around the same size that has similar vision capability and instruction following skills. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dzdq4ybhn73g1.png?width=1030&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30993c9fa83fb47de7923593a007e0a3fc7677fa"&gt;https://preview.redd.it/dzdq4ybhn73g1.png?width=1030&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30993c9fa83fb47de7923593a007e0a3fc7677fa&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Ad_8117"&gt; /u/Comfortable_Ad_8117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5hoqh/mistralsmall32latest_broken_after_a_recent_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5hoqh/mistralsmall32latest_broken_after_a_recent_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5hoqh/mistralsmall32latest_broken_after_a_recent_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T13:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5lwse</id>
    <title>Neural Network?</title>
    <updated>2025-11-24T16:40:04+00:00</updated>
    <author>
      <name>/u/spreader123</name>
      <uri>https://old.reddit.com/user/spreader123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p5lwse/neural_network/"&gt; &lt;img alt="Neural Network?" src="https://b.thumbs.redditmedia.com/p3QvsuKFfOO8YLriXQxxpJVaY4BrKd-MO8_AO9SEQSg.jpg" title="Neural Network?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spreader123"&gt; /u/spreader123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/neuralnetworks/comments/1p5jjig/neural_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5lwse/neural_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5lwse/neural_network/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T16:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5jy43</id>
    <title>archgw (0.3.20) - removed all (500mb) python deps in the request path. Ollama and Rust-first now</title>
    <updated>2025-11-24T15:27:32+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/katanemo/archgw"&gt;archgw&lt;/a&gt; (a models-native sidecar proxy for AI agents) offered two capabilities that required loading small LLMs in memory: guardrails to prevent jailbreak attempts, and function-calling for routing requests to the right downstream tool or agent. These built-in features required the project running a thread-safe python process that used libs like transformers, torch, safetensors, etc. 500M in dependencies, not to mention all the security vulnerabilities in the dep tree. Not hating on python, but our GH project was flagged with all sorts of issues.&lt;/p&gt; &lt;p&gt;Those models are loaded as a separate out-of-process server via ollama/lama.cpp which you all know are built in C++/Go. Lighter, faster and safer. And ONLY if the developer uses these features of the product. This meant 9000 lines of less code, a total start time of &amp;lt;2 seconds (vs 30+ seconds), etc.&lt;/p&gt; &lt;p&gt;Why archgw? So that you can build AI agents in any language or framework and offload the plumbing work in AI (like agent routing/hand-off, guardrails, zero-code logs and traces, and a unified API for all LLMs) to a durable piece of infrastructure, deployed as a sidecar.&lt;/p&gt; &lt;p&gt;Proud of this release, so sharing üôè&lt;/p&gt; &lt;p&gt;P.S Sample demos, the CLI and some tests still use python. But we'll move those over to Rust in the coming months. We are punting convenience for robustness.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5jy43/archgw_0320_removed_all_500mb_python_deps_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5jy43/archgw_0320_removed_all_500mb_python_deps_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5jy43/archgw_0320_removed_all_500mb_python_deps_in_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T15:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5lmcw</id>
    <title>Need clarifications or advice with coding and ollama.</title>
    <updated>2025-11-24T16:29:37+00:00</updated>
    <author>
      <name>/u/Lotus-006</name>
      <uri>https://old.reddit.com/user/Lotus-006</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, i'm not sure if tbe models can works like in vscode for coding with ai like claude sonnet or gpt with agent mode.&lt;/p&gt; &lt;p&gt;i tried some days before but the speed to react was slow even with my hardware i9-13900k and 96gb ddr5 at 6800mhz and RTX Msi 5070ti oc 16gb gen 5&lt;/p&gt; &lt;p&gt;i tried with ollama and also on docker desktop but not very usefull as in vscode compared to claude or gpt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lotus-006"&gt; /u/Lotus-006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5lmcw/need_clarifications_or_advice_with_coding_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p5lmcw/need_clarifications_or_advice_with_coding_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p5lmcw/need_clarifications_or_advice_with_coding_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-24T16:29:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6avsq</id>
    <title>¬øSabes la diferencia real entre una Automatizaci√≥n y un Agente de IA? (Tutorial pr√°ctico con n8n + Ollama)</title>
    <updated>2025-11-25T12:13:13+00:00</updated>
    <author>
      <name>/u/jokiruiz</name>
      <uri>https://old.reddit.com/user/jokiruiz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p6avsq/sabes_la_diferencia_real_entre_una_automatizaci√≥n/"&gt; &lt;img alt="¬øSabes la diferencia real entre una Automatizaci√≥n y un Agente de IA? (Tutorial pr√°ctico con n8n + Ollama)" src="https://external-preview.redd.it/nqPKqdngYJvGcJS6ivPJyKa3KnmmSEx91P350wT-I1k.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54d6da8f05cbff9c460009be72022ed9388a71a8" title="¬øSabes la diferencia real entre una Automatizaci√≥n y un Agente de IA? (Tutorial pr√°ctico con n8n + Ollama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hola a todos. He estado trasteando con agentes de IA locales y quer√≠a compartir un flujo de trabajo que cre√©.&lt;/p&gt; &lt;p&gt;Mucha gente confunde una automatizaci√≥n cl√°sica (Si A -&amp;gt; entonces B) con un Agente (que razona y decide).&lt;/p&gt; &lt;p&gt;En este tutorial explico c√≥mo construir un agente desde cero que:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Tiene &amp;quot;Cerebro&amp;quot; (usando Ollama en local).&lt;/li&gt; &lt;li&gt;Tiene &amp;quot;Memoria&amp;quot; (para recordar el contexto).&lt;/li&gt; &lt;li&gt;Usa &amp;quot;Herramientas&amp;quot; (se conecta a una API del clima para decidir qu√© ropa recomendarte).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Lo mejor es que est√° hecho con n8n y sin escribir c√≥digo. Si os interesa el mundo de los Agentes de IA o el No-Code, echadle un vistazo:&lt;/p&gt; &lt;p&gt;¬øAlguien m√°s aqu√≠ est√° usando n8n para orquestar IAs locales? ¬°Os leo!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jokiruiz"&gt; /u/jokiruiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/H0CwMDC3cYQ?si=N7bubLYtAhkv9vEZ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p6avsq/sabes_la_diferencia_real_entre_una_automatizaci√≥n/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p6avsq/sabes_la_diferencia_real_entre_una_automatizaci√≥n/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-25T12:13:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1p66utd</id>
    <title>M.I.M.I.R - drag and drop graph task UI + lambdas - MIT License - use ollama completely local for offline task orchestration.</title>
    <updated>2025-11-25T08:03:55+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p66tku"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p66utd/mimir_drag_and_drop_graph_task_ui_lambdas_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p66utd/mimir_drag_and_drop_graph_task_ui_lambdas_mit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-25T08:03:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p62zxz</id>
    <title>formatting_func issue (unsloth)</title>
    <updated>2025-11-25T04:18:27+00:00</updated>
    <author>
      <name>/u/toavepa</name>
      <uri>https://old.reddit.com/user/toavepa</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toavepa"&gt; /u/toavepa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/unsloth/comments/1p62znc/formatting_func_issue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p62zxz/formatting_func_issue_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p62zxz/formatting_func_issue_unsloth/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-25T04:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p61c5w</id>
    <title>Computer Use with Claude Opus 4.5</title>
    <updated>2025-11-25T02:56:09+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p61c5w/computer_use_with_claude_opus_45/"&gt; &lt;img alt="Computer Use with Claude Opus 4.5" src="https://external-preview.redd.it/cmp6aDE1OHBpYjNnMQ_ZQVTLVe7PyWJ0CuMi4sbpJgazjwD6JSk5JzpvEusC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f15feb81f8963c61b65ccb94fb57dde4fdc42069" title="Computer Use with Claude Opus 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Computer Use with cua playground.&lt;/p&gt; &lt;p&gt;Claude Opus 4.5 is 80.9% on SWE Bench. Pretty good for agentic and computer use tasks.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try yourself : &lt;a href="https://cua.ai/"&gt;https://cua.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6hq5mthpib3g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p61c5w/computer_use_with_claude_opus_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p61c5w/computer_use_with_claude_opus_45/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-25T02:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6345t</id>
    <title>Anyone doing an import of AI2's open-source Olmo3 model to Ollama?</title>
    <updated>2025-11-25T04:24:46+00:00</updated>
    <author>
      <name>/u/Conser-ai</name>
      <uri>https://old.reddit.com/user/Conser-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Subject line says it all. We use Ollama and small LMs for AI research, running locally for reproducibility. The Olmo series is particularly attractive since one can also know what it is trained on, thereby eliminating potential for data contamination. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conser-ai"&gt; /u/Conser-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p6345t/anyone_doing_an_import_of_ai2s_opensource_olmo3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p6345t/anyone_doing_an_import_of_ai2s_opensource_olmo3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p6345t/anyone_doing_an_import_of_ai2s_opensource_olmo3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-25T04:24:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6co7w</id>
    <title>When to use ollama cloud models like kimi-k2-thinking:cloud?</title>
    <updated>2025-11-25T13:39:19+00:00</updated>
    <author>
      <name>/u/PrudentCondition6672</name>
      <uri>https://old.reddit.com/user/PrudentCondition6672</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to know the use cases where these models with big context window could be used.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrudentCondition6672"&gt; /u/PrudentCondition6672 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p6co7w/when_to_use_ollama_cloud_models_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p6co7w/when_to_use_ollama_cloud_models_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p6co7w/when_to_use_ollama_cloud_models_like/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-25T13:39:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p64o5b</id>
    <title>Askimo: Open source of Ollama native desktop client</title>
    <updated>2025-11-25T05:50:03+00:00</updated>
    <author>
      <name>/u/Revolutionary-Judge9</name>
      <uri>https://old.reddit.com/user/Revolutionary-Judge9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building a desktop client called Askimo, and I use it with Ollama and other AI providers every day. I know there are already a lot of Ollama GUIs out there, but I kept running into the same problems: browser tabs slowing down, long chats eating memory, and losing good prompts I wanted to reuse.&lt;/p&gt; &lt;p&gt;This app actually started as a CLI tool I wrote for automation at work. After running into slowdowns and crashes in browser-based chats, I wanted something more solid, so I built a desktop client too. I treated myself as the first customer and added the features I kept wishing other apps had but never did, so I ended up creating the one I wanted to use.&lt;/p&gt; &lt;p&gt;I‚Äôve attached a short demo video if you want to see how it works.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p64o5b/video/du0zq92pbc3g1/player"&gt;Askimo Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also wrote a quick overview of the desktop client‚Äôs features on my blog. You can find everything here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Installation page: &lt;a href="https://askimo.chat/docs/desktop/installation/"&gt;https://askimo.chat/docs/desktop/installation/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub links: &lt;a href="https://github.com/haiphucnguyen/askimo"&gt;https://github.com/haiphucnguyen/askimo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Askimo desktop features: &lt;a href="https://askimo.chat/blog/askimo-with-ollama-the-best-desktop-for-local-ai/"&gt;https://askimo.chat/blog/askimo-with-ollama-the-best-desktop-for-local-ai/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm planning to keep adding more features, so any feedback from the community is definitely welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Revolutionary-Judge9"&gt; /u/Revolutionary-Judge9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p64o5b/askimo_open_source_of_ollama_native_desktop_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p64o5b/askimo_open_source_of_ollama_native_desktop_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p64o5b/askimo_open_source_of_ollama_native_desktop_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-25T05:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p65zvj</id>
    <title>Worth using both qwen 3 and llama3.2 for Linux system engineering?</title>
    <updated>2025-11-25T07:09:15+00:00</updated>
    <author>
      <name>/u/Zecside</name>
      <uri>https://old.reddit.com/user/Zecside</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got interested in deploying an Llm model locally to help me in my daily tasks as Linux sysadmin and I wonder if it would be useful. Leaving confidentiality issues solved , For example Can it help in debugging and log analysis? In the sense , are its reponses relevant? Thanks ! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zecside"&gt; /u/Zecside &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p65zvj/worth_using_both_qwen_3_and_llama32_for_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p65zvj/worth_using_both_qwen_3_and_llama32_for_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p65zvj/worth_using_both_qwen_3_and_llama32_for_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-25T07:09:15+00:00</published>
  </entry>
</feed>
