<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-17T06:54:29+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mqrnz6</id>
    <title>ollama in window11 with rx6600</title>
    <updated>2025-08-15T08:23:53+00:00</updated>
    <author>
      <name>/u/Anxious_Scarcity_250</name>
      <uri>https://old.reddit.com/user/Anxious_Scarcity_250</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;5600x / 32GB ram / rx6600 8GB &lt;/p&gt; &lt;p&gt;I couldn't use my rx6600 with ollama app version -the latest. It was CPU 100%.&lt;/p&gt; &lt;p&gt;Finallly It works with open-webui and little old version of ollama. Some file replacement for amd rocm needed. check below.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ByronLeeeee/Ollama-For-AMD-Installer/releases"&gt;https://github.com/ByronLeeeee/Ollama-For-AMD-Installer/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It works with gpt-oss 20b for maximum, but answers slow. And if just after using other models and RAM is not free enough, It cause ollama down. CPU/GPU 50/50.&lt;/p&gt; &lt;p&gt;-Good to use&lt;/p&gt; &lt;p&gt;Qwen3:8b-q4_K_M. 5.2GB GPU 100%. Qwen3:14b-q4_K_M. 9.3GB CPU/GPU 27%/73% Gemma3:12b-it-q4_K_M. 8.1GB CPU/GPU 32%/68%&lt;/p&gt; &lt;p&gt;Ratio changed as session get longer. Cpu works much.&lt;/p&gt; &lt;p&gt;-And smaller models &lt;/p&gt; &lt;p&gt;Fast, but ust available.&lt;/p&gt; &lt;p&gt;-Works, but Sucks&lt;/p&gt; &lt;p&gt;exaone-deep , clova-x-seed&lt;/p&gt; &lt;p&gt;ÏóòÏßÄ, ÎÑ§Ïù¥Î≤Ñ ÎÑàÎÑ® „ÖÖ„ÖÇ Í∞àÍ∏∏Ïù¥ Î©ÄÎã§. Î†àÎîßÏùÄ Ï§ÑÎ∞îÍæ∏Í∏∞Í∞Ä Ï†úÎ©ãÎåÄÎ°úÎÑ§.&lt;/p&gt; &lt;p&gt;Thank you for watching.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxious_Scarcity_250"&gt; /u/Anxious_Scarcity_250 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqrnz6/ollama_in_window11_with_rx6600/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqrnz6/ollama_in_window11_with_rx6600/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqrnz6/ollama_in_window11_with_rx6600/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T08:23:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq1izk</id>
    <title>Easy RAG using Ollama</title>
    <updated>2025-08-14T14:01:12+00:00</updated>
    <author>
      <name>/u/gogozad</name>
      <uri>https://old.reddit.com/user/gogozad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama people,&lt;/p&gt; &lt;p&gt;I am the author of &lt;a href="https://github.com/ggozad/oterm"&gt;oterm&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/ggozad/haiku.rag"&gt;haiku.rag&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I created an &lt;a href="https://ggozad.github.io/oterm/rag_example"&gt;example&lt;/a&gt; on how to combine these two to get fully local RAG, running on Ollama and without the need of external vector databases or servers other than Ollama. &lt;/p&gt; &lt;p&gt;You can see a demo and detailed instructions at the &lt;code&gt;oterm&lt;/code&gt;s &lt;a href="https://ggozad.github.io/oterm/rag_example"&gt;docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking forward to your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gogozad"&gt; /u/gogozad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq1izk/easy_rag_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq1izk/easy_rag_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mq1izk/easy_rag_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T14:01:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr67o7</id>
    <title>Need help with Tool calling</title>
    <updated>2025-08-15T18:31:46+00:00</updated>
    <author>
      <name>/u/Silver7769</name>
      <uri>https://old.reddit.com/user/Silver7769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, so I am a beginner to using Ollama and AI in general. I am trying to learn how to use tools so that my AI can use them, such as web search. I was hoping that someone could explain this to me or give me a tutorial where I could learn this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silver7769"&gt; /u/Silver7769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mr67o7/need_help_with_tool_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mr67o7/need_help_with_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mr67o7/need_help_with_tool_calling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T18:31:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr3ay7</id>
    <title>AI hires ai problem or scaling??</title>
    <updated>2025-08-15T16:47:21+00:00</updated>
    <author>
      <name>/u/Odd-Reflection-8000</name>
      <uri>https://old.reddit.com/user/Odd-Reflection-8000</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Reflection-8000"&gt; /u/Odd-Reflection-8000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.linkedin.com/posts/oscarsanch_ai-news-activity-7362156480000634882-ZNjY?utm_source=share&amp;amp;utm_medium=member_android&amp;amp;rcm=ACoAADShM6wB8E-yefZSDuPTqvp-SQ2KYMJIQ1Q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mr3ay7/ai_hires_ai_problem_or_scaling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mr3ay7/ai_hires_ai_problem_or_scaling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T16:47:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqr4st</id>
    <title>Speculative decoding via Arch (candidate release 0.4.0) - requesting feedback.</title>
    <updated>2025-08-15T07:54:05+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mqr4st/speculative_decoding_via_arch_candidate_release/"&gt; &lt;img alt="Speculative decoding via Arch (candidate release 0.4.0) - requesting feedback." src="https://preview.redd.it/eh87ityv25jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5437d1a2a47a9727494605a6c6a4e3b9e7fdbccb" title="Speculative decoding via Arch (candidate release 0.4.0) - requesting feedback." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are gearing up for a pretty big release and looking for feedback. One of the advantages in being a &lt;a href="https://github.com/katanemo/archgw"&gt;universal access&lt;/a&gt; layer for LLMs is that you can do some smarts that can help all developers build faster and more responsive agentic UX. The feature we are building and exploring with design partner is first-class support for speculative decoding.&lt;/p&gt; &lt;p&gt;Speculative decoding is a technique whereby a draft model (usually smaller) is engaged to produce tokens and the candidate set is verified by a target model. The set of candidate tokens produced by a draft model can be verified via logits by the target model, and verification can happen in parallel (each token in the sequence produced can be verified concurrently) to speed response time.&lt;/p&gt; &lt;p&gt;This is what OpenAI uses to accelerate the speed of its responses especially in cases where outputs can be guaranteed to come from the same distribution. The user experience could be something along the following lines or it be configured once per model. Here the draft_window is the number of tokens to verify, the max_accept_run tells us after how many failed verifications should we give up and just send all the remaining traffic to the target model etc.&lt;/p&gt; &lt;p&gt;Of course this work assumes a low RTT between the target and draft model so that speculative decoding is faster without compromising quality.&lt;/p&gt; &lt;p&gt;Question: would you want to improve the latency of responses, lower your token cost, and how do you feel about this functionality. Or would you want something simpler?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;POST /v1/chat/completions { &amp;quot;model&amp;quot;: &amp;quot;target:gpt-large@2025-06&amp;quot;, &amp;quot;speculative&amp;quot;: { &amp;quot;draft_model&amp;quot;: &amp;quot;draft:small@v3&amp;quot;, &amp;quot;max_draft_window&amp;quot;: 8, &amp;quot;min_accept_run&amp;quot;: 2, &amp;quot;verify_logprobs&amp;quot;: false }, &amp;quot;messages&amp;quot;: [...], &amp;quot;stream&amp;quot;: true } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eh87ityv25jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqr4st/speculative_decoding_via_arch_candidate_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqr4st/speculative_decoding_via_arch_candidate_release/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T07:54:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr8y8q</id>
    <title>Ingesting time on CPU only</title>
    <updated>2025-08-15T20:13:17+00:00</updated>
    <author>
      <name>/u/Glum-Tradition-5306</name>
      <uri>https://old.reddit.com/user/Glum-Tradition-5306</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mr8y8q/ingesting_time_on_cpu_only/"&gt; &lt;img alt="Ingesting time on CPU only" src="https://a.thumbs.redditmedia.com/5ePa8pwN56T7npoBLL8Aw1fQ9iFDqhNVLofgPprJuK8.jpg" title="Ingesting time on CPU only" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick question :&lt;/p&gt; &lt;p&gt;For 288 chunks, (just one PDF file, around 4.5Mb) ingesting it locally with ollama, on a CPU (yeah I know...) Core i5 10th Gen, how much time should it normally take ?&lt;br /&gt; 1 hour ?&lt;br /&gt; Or more ?&lt;/p&gt; &lt;p&gt;I can see the computer utilized almost at max in terms or resources for over 30 minutes now.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6jtyoyvgq8jf1.png?width=654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e46a2edc781e3eebef365ed207820754ab8e729"&gt;https://preview.redd.it/6jtyoyvgq8jf1.png?width=654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e46a2edc781e3eebef365ed207820754ab8e729&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My script :&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;import os&lt;/code&gt;&lt;br /&gt; &lt;code&gt;from pathlib import Path&lt;/code&gt;&lt;br /&gt; &lt;code&gt;from langchain_ollama import OllamaEmbeddings&lt;/code&gt;&lt;br /&gt; &lt;code&gt;from langchain_chroma import Chroma&lt;/code&gt;&lt;br /&gt; &lt;code&gt;VECTORSTORE_DIR = &amp;quot;vectorstore&amp;quot;&lt;/code&gt;&lt;br /&gt; &lt;code&gt;PDF_DIR = Path(&amp;quot;pdfs&amp;quot;)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;force_ingest = True&lt;/code&gt;&lt;br /&gt; &lt;code&gt;pdf_files = list(PDF_DIR.glob(&amp;quot;*.pdf&amp;quot;))&lt;/code&gt;&lt;br /&gt; &lt;code&gt;if not pdf_files:&lt;/code&gt;&lt;br /&gt; &lt;code&gt;print(&amp;quot;‚ùå No PDFs found in folder:&amp;quot;, PDF_DIR)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;else:&lt;/code&gt;&lt;br /&gt; &lt;code&gt;print(f&amp;quot;üìÑ Found {len(pdf_files)} PDFs&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain.text_splitter import RecursiveCharacterTextSplitter&lt;/code&gt;&lt;br /&gt; &lt;code&gt;from langchain.document_loaders import PyPDFLoader&lt;/code&gt;&lt;br /&gt; &lt;code&gt;docs = []&lt;/code&gt;&lt;br /&gt; &lt;code&gt;for pdf in pdf_files:&lt;/code&gt;&lt;br /&gt; &lt;code&gt;loader = PyPDFLoader(pdf)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;for page in loader.load():&lt;/code&gt;&lt;br /&gt; &lt;code&gt;docs.append(page)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;print(f&amp;quot;‚úÇ Splitting into chunks: {len(docs)} pages&amp;quot;)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;chunks = splitter.split_documents(docs)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;print(f&amp;quot;üîπ {len(chunks)} chunks created&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;embeddings = OllamaEmbeddings(model=&amp;quot;llama3&amp;quot;)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;db = Chroma(persist_directory=VECTORSTORE_DIR, embedding_function=embeddings)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if force_ingest:&lt;/code&gt;&lt;br /&gt; &lt;code&gt;print(&amp;quot;‚ö° Forcing ingestion: clearing old documents&amp;quot;)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;db.delete_collection() # remove old data&lt;/code&gt;&lt;br /&gt; &lt;code&gt;db.add_documents(chunks)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;print(f&amp;quot;‚úÖ {len(chunks)} chunks added to vector store&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glum-Tradition-5306"&gt; /u/Glum-Tradition-5306 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mr8y8q/ingesting_time_on_cpu_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mr8y8q/ingesting_time_on_cpu_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mr8y8q/ingesting_time_on_cpu_only/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T20:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqk7wk</id>
    <title>Local Open Source Alternative to NotebookLM</title>
    <updated>2025-08-15T01:53:03+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a &lt;strong&gt;Highly Customizable AI Research Agent&lt;/strong&gt; that connects to your personal external sources and Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;üìä &lt;strong&gt;Featu&lt;/strong&gt;res&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üéôÔ∏è &lt;strong&gt;Podca&lt;/strong&gt;sts&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Support for local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt; &lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt; &lt;li&gt;Multiple TTS providers supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ÑπÔ∏è &lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search Engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Jira&lt;/li&gt; &lt;li&gt;ClickUp&lt;/li&gt; &lt;li&gt;Confluence&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;Youtube Videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;Discord&lt;/li&gt; &lt;li&gt;and more to come.....&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üîñ &lt;strong&gt;Cross-Browser Extens&lt;/strong&gt;ion&lt;/p&gt; &lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqk7wk/local_open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqk7wk/local_open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqk7wk/local_open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T01:53:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mreubk</id>
    <title>iniciar modelo ollama</title>
    <updated>2025-08-16T00:01:15+00:00</updated>
    <author>
      <name>/u/Desperate_News_5116</name>
      <uri>https://old.reddit.com/user/Desperate_News_5116</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Buenas! tengo poco conocimiento sobre esta rama, por lo que recaigo a ustedes ya que ninguna IA se supo explicar, tengo un bot, corriendo de forma local, estuve probando algunos modelos de ollama para ver cuales respondian mejor al espa√±ol etc, cuestion que uno me agrado, lo mantuve, pero quiero probar otro mas por las dudas sea mejor, el tema es que hice un duplicado de la carpeta etc, todo ok. solo cambie lo necesario para el otro modelo, model_name etc. cuando lo voy a correr en cmd, me dice que:&amp;quot; ‚ö†Ô∏è Error al conectar con Ollama: 404 Client Error: Not Found for url: http://localhost:11434/api/chat&amp;quot; supongo que es el puerto que esta ocupado, y esta ocupado por el modelo que me guarde... es eso? hay forma de separar los puertos?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_News_5116"&gt; /u/Desperate_News_5116 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mreubk/iniciar_modelo_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mreubk/iniciar_modelo_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mreubk/iniciar_modelo_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T00:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqyhhh</id>
    <title>I built a CLI tool to turn natural language into shell commands (and made my first AUR package) and i would like some honest feedback</title>
    <updated>2025-08-15T13:52:43+00:00</updated>
    <author>
      <name>/u/Kiuuby</name>
      <uri>https://old.reddit.com/user/Kiuuby</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;So, I've been diving deep into a project lately and thought it would be cool to share the adventure and maybe get some feedback. I created pls, a simple CLI tool that uses local Ollama models to convert natural language into shell commands.&lt;/p&gt; &lt;p&gt;You can check out the project here: &lt;a href="https://github.com/GaelicThunder/pls"&gt;https://github.com/GaelicThunder/pls&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The whole thing started when I saw &lt;a href="https://github.com/context-labs/uwu"&gt;https://github.com/context-labs/uwu&lt;/a&gt; and thought, &amp;quot;Hey, I could build something like that but make it run entirely locally with Ollama.&amp;quot; And then, of course, the day after I finished, uwu added local model support... but oh well, that's open source for you.&lt;/p&gt; &lt;p&gt;The real journey for me wasn't just building the tool, but doing it &amp;quot;properly&amp;quot; for the first time. I'm kind of firmware engineer, so I'm comfortable with code, but I'd never really gone through the whole process of setting up a decent GitHub repo, handling shell-specific quirks (looking at you, Fish shell quoting), and, the big one for me, creating my first AUR package.&lt;/p&gt; &lt;p&gt;I won't hide it, I got a ton of help from an AI assistant through the whole process. It felt like pair programming with a very patient, knowledgeable, but sometimes weirdly literal partner. It was a pretty cool experience, and I learned a ton, especially about the hoops you have to jump through for shell integrations and AUR packaging.&lt;/p&gt; &lt;p&gt;The tool itself is pretty straightforward:&lt;/p&gt; &lt;p&gt;It's written in shell script, so no complex build steps.&lt;/p&gt; &lt;p&gt;It supports Bash, Zsh, and Fish, with shell-aware command generation.&lt;/p&gt; &lt;p&gt;It automatically adds commands to your history (not on fish, told you i had some problems with it), so you can review them before running.&lt;/p&gt; &lt;p&gt;I know there are similar tools out there, but I'm proud of this little project, mostly because of the learning process. It‚Äôs now on the AUR as pls-cli-git if anyone wants to give it a spin.&lt;/p&gt; &lt;p&gt;I'd love to hear what you think, any feedback on the code, the PKGBUILD, or the repo itself would be awesome. I'm especially curious if anyone has tips on making shell integrations more robust or on AUR best practices.&lt;/p&gt; &lt;p&gt;Thanks for taking the time to read this, i really appreciate any kinkd of positive or negative feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kiuuby"&gt; /u/Kiuuby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqyhhh/i_built_a_cli_tool_to_turn_natural_language_into/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqyhhh/i_built_a_cli_tool_to_turn_natural_language_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqyhhh/i_built_a_cli_tool_to_turn_natural_language_into/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T13:52:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqrv10</id>
    <title>Isn't Ollama Turbo exactly the one thing that one tried to avoid by chasing Ollama in the first place?</title>
    <updated>2025-08-15T08:35:31+00:00</updated>
    <author>
      <name>/u/Tasty-Base-9900</name>
      <uri>https://old.reddit.com/user/Tasty-Base-9900</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry typo in the title... should be choosing not chasing ;-)&lt;/p&gt; &lt;p&gt;Imho the biggest selling point for Ollama is that one can run one's models locally or within one's own infrastructure so one doesn't have to trust an external infrastructure provider with say one's data. Doesn't Ollama Turbo run exactly against this philosophy?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tasty-Base-9900"&gt; /u/Tasty-Base-9900 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqrv10/isnt_ollama_turbo_exactly_the_one_thing_that_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqrv10/isnt_ollama_turbo_exactly_the_one_thing_that_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqrv10/isnt_ollama_turbo_exactly_the_one_thing_that_one/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T08:35:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr1ga3</id>
    <title>Bringing Computer Use to the Web</title>
    <updated>2025-08-15T15:40:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mr1ga3/bringing_computer_use_to_the_web/"&gt; &lt;img alt="Bringing Computer Use to the Web" src="https://external-preview.redd.it/anRqMTh2ZjJlN2pmMRcxEnlpDBBJVNjXlCDC4HUtgXjfB5ufLszRpp9PEi0H.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bdfe3f9648d7ccca224923687bacd117f3c3d894" title="Bringing Computer Use to the Web" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are bringing Computer Use to the web, you can now control cloud desktops from JavaScript right in the browser.&lt;/p&gt; &lt;p&gt;Until today computer use was Python only shutting out web devs. Now you can automate real UIs without servers, VMs, or any weird work arounds.&lt;/p&gt; &lt;p&gt;What you can now build : Pixel-perfect UI tests,Live AI demos,In app assistants that actually move the cursor, or parallel automation streams for heavy workloads.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Read more here : &lt;a href="https://www.trycua.com/blog/bringing-computer-use-to-the-web"&gt;https://www.trycua.com/blog/bringing-computer-use-to-the-web&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zrulwgp2e7jf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mr1ga3/bringing_computer_use_to_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mr1ga3/bringing_computer_use_to_the_web/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T15:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrsst2</id>
    <title>How can I generate ANSYS models directly by prompting an LLM?</title>
    <updated>2025-08-16T11:39:55+00:00</updated>
    <author>
      <name>/u/omarshoaib</name>
      <uri>https://old.reddit.com/user/omarshoaib</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm curious if anyone here has experimented with using &lt;strong&gt;large language models (LLMs)&lt;/strong&gt; to generate &lt;strong&gt;ANSYS models&lt;/strong&gt; directly from natural language prompts.&lt;/p&gt; &lt;p&gt;The idea would be:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You type something like &lt;em&gt;‚ÄúCreate a 1m x 0.1m cantilever beam, mesh at 0.01m, apply a tip load of 1000 N, solve for displacement‚Äù&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;The LLM then produces the correct &lt;strong&gt;ANSYS input&lt;/strong&gt; (APDL script, Mechanical Python script, Fluent journal, or PyAnsys code).&lt;/li&gt; &lt;li&gt;That script is then fed into ANSYS to actually build and solve the model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So instead of manually writing APDL or going step by step in Workbench, you just describe the setup in plain language and the LLM handles the code generation.&lt;/p&gt; &lt;h1&gt;Questions for the community&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Has anyone here tried prompting an LLM this way to build or solve models in ANSYS?&lt;/li&gt; &lt;li&gt;What‚Äôs the most practical route‚Äî&lt;strong&gt;APDL scripts&lt;/strong&gt;, &lt;strong&gt;Workbench journal files&lt;/strong&gt;, or &lt;strong&gt;PyAnsys (Python APIs)&lt;/strong&gt;?&lt;/li&gt; &lt;li&gt;Are there good practices for making sure the generated input is valid before running it in ANSYS?&lt;/li&gt; &lt;li&gt;Do you think this workflow is realistic for production use, or mainly a research/demo tool?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear if anyone has given this a shot (or has thoughts on how feasible it is).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omarshoaib"&gt; /u/omarshoaib &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mrsst2/how_can_i_generate_ansys_models_directly_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mrsst2/how_can_i_generate_ansys_models_directly_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mrsst2/how_can_i_generate_ansys_models_directly_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T11:39:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr9rke</id>
    <title>Is this possible or even the right tool?</title>
    <updated>2025-08-15T20:43:40+00:00</updated>
    <author>
      <name>/u/NervousMood8071</name>
      <uri>https://old.reddit.com/user/NervousMood8071</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote 1000 words a day for over 6 years and exported it all to plain ascii text files -- no markup -- no tags etc.&lt;/p&gt; &lt;p&gt;I want to know if getting an LLM to digest all of my journal entries is feasible and doable on a local PC with an I9 12th-gen CPU, 64gb RAM, and an Nvidia GPU with 16gb VRAM? &lt;/p&gt; &lt;p&gt;If so, where do I begin? I want to be able to query the resulting LLM for stuff I've written. I was terribly organized and haphazard in my writing. For example I'd start reminiscing about events in the past interspersed with chores to do this week, plot outlines for stories, aborted first chapters etc. I would love to be able to query the LLM afterward to pullout topics at will.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NervousMood8071"&gt; /u/NervousMood8071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mr9rke/is_this_possible_or_even_the_right_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mr9rke/is_this_possible_or_even_the_right_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mr9rke/is_this_possible_or_even_the_right_tool/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T20:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mruik7</id>
    <title>Is web search tool calling only available with the got-oss models?</title>
    <updated>2025-08-16T12:56:54+00:00</updated>
    <author>
      <name>/u/Equivalent-Win-1294</name>
      <uri>https://old.reddit.com/user/Equivalent-Win-1294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will the web search tool and path be available to other compatible models as well?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent-Win-1294"&gt; /u/Equivalent-Win-1294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mruik7/is_web_search_tool_calling_only_available_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mruik7/is_web_search_tool_calling_only_available_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mruik7/is_web_search_tool_calling_only_available_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T12:56:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrawvt</id>
    <title>Open Moxie - Fully Offline (ollama option) and XAI Grok API</title>
    <updated>2025-08-15T21:26:23+00:00</updated>
    <author>
      <name>/u/Over-Mix7071</name>
      <uri>https://old.reddit.com/user/Over-Mix7071</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mrawvt/open_moxie_fully_offline_ollama_option_and_xai/"&gt; &lt;img alt="Open Moxie - Fully Offline (ollama option) and XAI Grok API" src="https://external-preview.redd.it/a1zSbUhyWKbtVHaGz5nbMQPuqamAd8HDk_nVmyFL_-4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4aae6540bec8fcaa94d2dcf38e781bec38212860" title="Open Moxie - Fully Offline (ollama option) and XAI Grok API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Over-Mix7071"&gt; /u/Over-Mix7071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l7mikg3609jf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mrawvt/open_moxie_fully_offline_ollama_option_and_xai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mrawvt/open_moxie_fully_offline_ollama_option_and_xai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T21:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrrbsh</id>
    <title>RTX 5090 @ 200W | Capped Core &amp; Power | AI Inference Efficiency</title>
    <updated>2025-08-16T10:25:42+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/nvidia/comments/1mrrb7e/rtx_5090_200w_capped_core_power_ai_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mrrbsh/rtx_5090_200w_capped_core_power_ai_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mrrbsh/rtx_5090_200w_capped_core_power_ai_inference/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T10:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrwsyx</id>
    <title>Smart assist Home Assistant.</title>
    <updated>2025-08-16T14:27:50+00:00</updated>
    <author>
      <name>/u/Original-Chapter-112</name>
      <uri>https://old.reddit.com/user/Original-Chapter-112</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Original-Chapter-112"&gt; /u/Original-Chapter-112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/homeassistant/comments/1mrws8z/smart_assist_home_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mrwsyx/smart_assist_home_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mrwsyx/smart_assist_home_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T14:27:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1msclnz</id>
    <title>Phi 3 Mini needing 50gb??</title>
    <updated>2025-08-17T00:22:40+00:00</updated>
    <author>
      <name>/u/Fun-Tangerine5264</name>
      <uri>https://old.reddit.com/user/Fun-Tangerine5264</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1msclnz/phi_3_mini_needing_50gb/"&gt; &lt;img alt="Phi 3 Mini needing 50gb??" src="https://a.thumbs.redditmedia.com/ITn7NYWmJ_MOxmF9GA6WyOx8eGtT06QxvF8ZTULN108.jpg" title="Phi 3 Mini needing 50gb??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/64fvg8f44hjf1.png?width=1825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e38d036375ba95767abce394acc2cef1eeb7aff8"&gt;https://preview.redd.it/64fvg8f44hjf1.png?width=1825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e38d036375ba95767abce394acc2cef1eeb7aff8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Erm, I don't think this is supposed to happen on a mini model no?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Tangerine5264"&gt; /u/Fun-Tangerine5264 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msclnz/phi_3_mini_needing_50gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msclnz/phi_3_mini_needing_50gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msclnz/phi_3_mini_needing_50gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T00:22:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms59k7</id>
    <title>A Guide to GRPO Fine-Tuning on Windows Using the TRL Library</title>
    <updated>2025-08-16T19:33:04+00:00</updated>
    <author>
      <name>/u/oridnary_artist</name>
      <uri>https://old.reddit.com/user/oridnary_artist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ms59k7/a_guide_to_grpo_finetuning_on_windows_using_the/"&gt; &lt;img alt="A Guide to GRPO Fine-Tuning on Windows Using the TRL Library" src="https://preview.redd.it/3o9tbazgofjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecf02796e74bfbbb6661da8cb781ff654a0af2a2" title="A Guide to GRPO Fine-Tuning on Windows Using the TRL Library" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wrote a hands-on guide for fine-tuning LLMs with GRPO (Group-Relative PPO) locally on Windows, using Hugging Face's TRL library. My goal was to create a practical workflow that doesn't require Colab or Linux.&lt;/p&gt; &lt;p&gt;The guide and the accompanying script focus on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;A TRL-based implementation&lt;/strong&gt; that runs on consumer GPUs (with LoRA and optional 4-bit quantization).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A verifiable reward system&lt;/strong&gt; that uses numeric, format, and boilerplate checks to create a more reliable training signal.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Automatic data mapping&lt;/strong&gt; for most Hugging Face datasets to simplify preprocessing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Practical troubleshooting&lt;/strong&gt; and configuration notes for local setups.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is for anyone looking to experiment with reinforcement learning techniques on their own machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Read the blog post:&lt;/strong&gt; &lt;a href="https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323"&gt;&lt;code&gt;https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get the code:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/trl-ppo-fine-tuning"&gt;Reinforcement-learning-with-verifable-rewards-Learnings/projects/trl-ppo-fine-tuning at main ¬∑ Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm open to any feedback. Thanks!&lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. I'm currently looking for my next role in the LLM / Computer Vision space and would love to connect about any opportunities&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Portfolio:&lt;/em&gt; &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;Pavan Kunchala - AI Engineer &amp;amp; Full-Stack Developer&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oridnary_artist"&gt; /u/oridnary_artist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3o9tbazgofjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms59k7/a_guide_to_grpo_finetuning_on_windows_using_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ms59k7/a_guide_to_grpo_finetuning_on_windows_using_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T19:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms5cjg</id>
    <title>üöÄ New Feature in RAGLight: Effortless MCP Integration for Agentic RAG Pipelines! üîå</title>
    <updated>2025-08-16T19:36:10+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I just shipped a new feature in &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;RAGLight&lt;/a&gt;, my lightweight and modular Python framework for Retrieval-Augmented Generation, and it's a big one: &lt;strong&gt;easy MCP Server integration&lt;/strong&gt; for Agentic RAG workflows. üß†üíª&lt;/p&gt; &lt;h1&gt;What's new?&lt;/h1&gt; &lt;p&gt;You can now plug in external tools directly into your agent's reasoning process using an MCP server. No boilerplate required. Whether you're building code assistants, tool-augmented LLM agents, or just want your LLM to interact with a live backend, it's now just a few lines of config.&lt;/p&gt; &lt;h1&gt;Example:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;config = AgenticRAGConfig( provider = Settings.OPENAI, model = &amp;quot;gpt-4o&amp;quot;, k = 10, mcp_config = [ {&amp;quot;url&amp;quot;: &amp;quot;http://127.0.0.1:8001/sse&amp;quot;} # Your MCP server URL ], ... ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This automatically injects all MCP tools into the agent's toolset.&lt;/p&gt; &lt;p&gt;üìö If you're curious how to write your own MCP tool or server, you can check the &lt;code&gt;MCPClient.server_parameters&lt;/code&gt; doc from &lt;a href="https://huggingface.co/docs/smolagents/en/reference/tools#smolagents.MCPClient.server_parameters"&gt;smolagents&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;üëâ Try it out and let me know what you think: &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;https://github.com/Bessouat40/RAGLight&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms5cjg/new_feature_in_raglight_effortless_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms5cjg/new_feature_in_raglight_effortless_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ms5cjg/new_feature_in_raglight_effortless_mcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T19:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1msarft</id>
    <title>Why does gpt-oss 120b run slower in ollama than in LM Studio in my setup?</title>
    <updated>2025-08-16T23:03:33+00:00</updated>
    <author>
      <name>/u/Southern-Chain-6485</name>
      <uri>https://old.reddit.com/user/Southern-Chain-6485</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My hardware is an RTX 3090 + 64gb of ddr4 ram. LM Studio runs it at something about 10-12 tokens per second (I don't have the actual measure at hand) while ollama runs it at half the speed, at best. I'm using the lm studio community version in LM Studio and the version downloaded from ollama's site with ollama - basically, the recommended versions. Are there flags that need to be run in Ollama to match LM Studio performance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Southern-Chain-6485"&gt; /u/Southern-Chain-6485 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msarft/why_does_gptoss_120b_run_slower_in_ollama_than_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msarft/why_does_gptoss_120b_run_slower_in_ollama_than_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msarft/why_does_gptoss_120b_run_slower_in_ollama_than_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T23:03:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1msflr9</id>
    <title>Anyone have tokens per second results for gpt-oss 20-b on an ada 2000?</title>
    <updated>2025-08-17T02:50:33+00:00</updated>
    <author>
      <name>/u/RasPiBuilder</name>
      <uri>https://old.reddit.com/user/RasPiBuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for something with relatively low power draw and decent inference speeds. I don't need it to be blazing fast, but it does need to be responsive at reasonable speeds (hoping for around 7-10t/s). &lt;/p&gt; &lt;p&gt;For this particular setup power draw is the bottleneck, where my absolute max is 100w. Cost is less of an issue, though I'd lean towards the least expensive on comparable speed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RasPiBuilder"&gt; /u/RasPiBuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msflr9/anyone_have_tokens_per_second_results_for_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msflr9/anyone_have_tokens_per_second_results_for_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msflr9/anyone_have_tokens_per_second_results_for_gptoss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T02:50:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms1v2p</id>
    <title>Ollama interface with memory</title>
    <updated>2025-08-16T17:31:04+00:00</updated>
    <author>
      <name>/u/mrdougwright</name>
      <uri>https://old.reddit.com/user/mrdougwright</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;Ollama is so cool, it inspired me to do some open source!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mrdougwright/yak"&gt;https://github.com/mrdougwright/yak&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.npmjs.com/package/yak-llm"&gt;https://www.npmjs.com/package/yak-llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yak is a CLI interface with persistent chat sessions for local LLMs. Instead of losing context every time you restart, it remembers your conversations across sessions and lets you organize them by topic. &lt;/p&gt; &lt;p&gt;Key features:&lt;br /&gt; - Multiple chat sessions (work, personal, coding help, etc.)&lt;br /&gt; - Persistent memory using simple JSONL files&lt;br /&gt; - Auto-starts Ollama if needed&lt;br /&gt; - Switch models from the CLI&lt;br /&gt; - Zero config for new users &lt;/p&gt; &lt;p&gt;Install: `npm install -g yak-llm`&lt;br /&gt; Usage: `yak start` &lt;/p&gt; &lt;p&gt;Built this because I wanted something lightweight that actually remembers context and doesn't slow down with long conversations. Plus you can directly edit the chat files if needed! &lt;/p&gt; &lt;p&gt;Would love feedback from the Ollama community! ü¶ß&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrdougwright"&gt; /u/mrdougwright &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms1v2p/ollama_interface_with_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms1v2p/ollama_interface_with_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ms1v2p/ollama_interface_with_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T17:31:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1msjayu</id>
    <title>Knowledge graph using gemma3</title>
    <updated>2025-08-17T06:13:22+00:00</updated>
    <author>
      <name>/u/cython_boy</name>
      <uri>https://old.reddit.com/user/cython_boy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1msjayu/knowledge_graph_using_gemma3/"&gt; &lt;img alt="Knowledge graph using gemma3" src="https://preview.redd.it/fviiip42uijf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1890c5ec0cd777145b90a7dff123fc028801135" title="Knowledge graph using gemma3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;strong&gt;Streamlit web app&lt;/strong&gt; that generates interactive &lt;strong&gt;knowledge graphs&lt;/strong&gt; from plain text using &lt;strong&gt;Ollama's open sourcw models.(geema3 , grnaite , llama3 , gpt-oss ,....)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Two input methods&lt;/strong&gt;: Upload &lt;code&gt;.txt&lt;/code&gt; file or paste text directly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama model integration&lt;/strong&gt;: Select from available local models (e.g., Gemma, Mistral, LLaMA).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Automatic graph storage&lt;/strong&gt;: Generated graphs are saved and can be reloaded anytime.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive visualization&lt;/strong&gt;: Zoom, drag, and explore relationships between concepts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimized for speed&lt;/strong&gt;: Uses hashed filenames to prevent regenerating the same graph.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/ganeshnikhil/Kgraph"&gt;kgraph&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cython_boy"&gt; /u/cython_boy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fviiip42uijf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msjayu/knowledge_graph_using_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msjayu/knowledge_graph_using_gemma3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T06:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms8ghv</id>
    <title>I made a no-install-needed web-GUI for Ollama</title>
    <updated>2025-08-16T21:32:51+00:00</updated>
    <author>
      <name>/u/DarkTom21</name>
      <uri>https://old.reddit.com/user/DarkTom21</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ms8ghv/i_made_a_noinstallneeded_webgui_for_ollama/"&gt; &lt;img alt="I made a no-install-needed web-GUI for Ollama" src="https://b.thumbs.redditmedia.com/fUb2ZooOQRQelk4szZCBNb_brztbxvoPSTkbeSXzDhk.jpg" title="I made a no-install-needed web-GUI for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;For the last while I've been working on a solution to a problem I've had ever since getting into Ollama, that being a GUI that is both powerful and easy to set up across multiple devices. Firstly I tried using OpenWebUI, however quickly dropped it due to needing to install either Python or Docker just to run it, and I didn't want to install more runtimes just to run a GUI. I looked at alternatives, but none seemed to quite fit what I wanted.&lt;/p&gt; &lt;p&gt;That's why I decided to make LlamaPen, LlamaPen is an open-source, no-download-required web app/GUI that lets you easily interface with your local instance of Ollama without needing to download anything extra. It contains all the basics you would expect from a GUI such as chats, conversations, and model selection, but also contains additional features, such as model management, downloading, mobile, PWA &amp;amp; offline support, formatting markdown and think text, icons for each model, and more, all without needing to go through a lengthy download and setup process.&lt;/p&gt; &lt;p&gt;It is currently available live at &lt;a href="https://llamapen.app/"&gt;https://llamapen.app/&lt;/a&gt; with a GitHub repo going further into the specifics and features at &lt;a href="https://github.com/ImDarkTom/LlamaPen"&gt;https://github.com/ImDarkTom/LlamaPen&lt;/a&gt;. If you have any questions or would like to know more feel free to leave a comment here and I will try to reply as soon as possible, and if you encounter any issues you can either comment here or I recommend opening an issue on the GitHub repo for faster support.&lt;/p&gt; &lt;p&gt;Thanks for reading and I hope at least one other person than me finds this useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkTom21"&gt; /u/DarkTom21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ms8ghv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms8ghv/i_made_a_noinstallneeded_webgui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ms8ghv/i_made_a_noinstallneeded_webgui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T21:32:51+00:00</published>
  </entry>
</feed>
