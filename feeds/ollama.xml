<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-21T13:07:48+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1r87tdt</id>
    <title>packaged claude code + omi + terminator, now it's better than openclaw</title>
    <updated>2026-02-18T16:34:35+00:00</updated>
    <author>
      <name>/u/Deep_Ad1959</name>
      <uri>https://old.reddit.com/user/Deep_Ad1959</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r87tdt/packaged_claude_code_omi_terminator_now_its/"&gt; &lt;img alt="packaged claude code + omi + terminator, now it's better than openclaw" src="https://external-preview.redd.it/YTk1amRlMjg2YWtnMXjfFo5q1zD6K1Ubw5_sFCkd17GjeVIYKU9ByDARBA-a.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83f20fb8d8583165dc35e6384f12e34a07932696" title="packaged claude code + omi + terminator, now it's better than openclaw" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try asking your AI agent to do things for you, it will ask you for 10 confirmations, and still won't do most of the things for you directly, it's frustrating, kills the whole purpose...&lt;/p&gt; &lt;p&gt;What i want is I give a task, and it works on it without bothering me while using my computer, but without taking over my computer. Sounds contradicting, but it's now possible. &lt;/p&gt; &lt;p&gt;Omi + Terminator + Claude Code can work on your computer using your active browser without taking over your keyboard or mouse. See how the agent buys me a ticket with a single prompt. &lt;/p&gt; &lt;p&gt;First, I tried asking Claude Cowork to do that, and it refused, but then Omi did it all. Omi has all the context about me, all the preferences, and access to my credit card. Claude code is spinned up by Omi to work on the task, while Terminator can execute based of the function calls from Claude Code. See it in action &lt;/p&gt; &lt;p&gt;Open source &lt;a href="https://github.com/BasedHardware/omi"&gt;https://github.com/BasedHardware/omi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mediar-ai/terminator"&gt;https://github.com/mediar-ai/terminator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep_Ad1959"&gt; /u/Deep_Ad1959 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/on9h6a286akg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r87tdt/packaged_claude_code_omi_terminator_now_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r87tdt/packaged_claude_code_omi_terminator_now_its/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T16:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r90q4o</id>
    <title>Can't able access ollama through network in cline cli</title>
    <updated>2026-02-19T14:32:16+00:00</updated>
    <author>
      <name>/u/DismalBunch6739</name>
      <uri>https://old.reddit.com/user/DismalBunch6739</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi ,&lt;/p&gt; &lt;p&gt;I have ollama installed in my workstation with RTX 3090 24gb vram .&lt;/p&gt; &lt;p&gt;I tried to access ollama via internet with my domain and its running properly in command prompt.&lt;/p&gt; &lt;p&gt;But if I tried to use that API in cline cli , it can't able to access the workstation ollama.&lt;/p&gt; &lt;p&gt;I tried i LAN network same problem.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Cluade CLI is working&lt;/em&gt;&lt;/p&gt; &lt;p&gt;$env:OLLAMA_HOST=&amp;quot;&lt;a href="http://MYCUSTOMDOMAIN.in:11434"&gt;http://MYCUSTOMDOMAIN.in:11434&lt;/a&gt;&amp;quot;&lt;/p&gt; &lt;p&gt;ollama launch claude&lt;/p&gt; &lt;p&gt;This opens claude code temporarily.&lt;/p&gt; &lt;p&gt;But, I couldn't config cline CLI with ollama in every options domain/public IP/local IP .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DismalBunch6739"&gt; /u/DismalBunch6739 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r90q4o/cant_able_access_ollama_through_network_in_cline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r90q4o/cant_able_access_ollama_through_network_in_cline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r90q4o/cant_able_access_ollama_through_network_in_cline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T14:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8iea9</id>
    <title>ClawCache: Free local caching + tracking for Ollama calls â€“ cuts waste on repeats in agents/scripts inspired by efficiency threads here</title>
    <updated>2026-02-18T23:03:40+00:00</updated>
    <author>
      <name>/u/SignificantClub4279</name>
      <uri>https://old.reddit.com/user/SignificantClub4279</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In local LLM communities like this one, a common observation is that while local runs give excellent privacy and avoid cloud per-token fees, agents and scripts frequently re-process the same or similar prompts â€” especially in loops, tool calls, or debugging sessions â€” leading to wasted compute and time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Common issues I've seen in recent posts:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Long runs repeating queries and burning unnecessary GPU cycles&lt;/li&gt; &lt;li&gt;Agent frameworks chaining similar calls without reuse&lt;/li&gt; &lt;li&gt;Debugging sessions re-inferring the same content repeatedly&lt;/li&gt; &lt;li&gt;Searches for caching layers, context optimizations, or efficiency tools&lt;/li&gt; &lt;li&gt;Hybrid Ollama + API setups still incurring token costs on repeats&lt;/li&gt; &lt;li&gt;No easy built-in exact (or semantic) caching across most local workflows&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The core problem:&lt;/strong&gt; Ollama makes local inference straightforward and powerful, but when building agents or complex scripts, exact repeats are frequent â€” and without caching, every call re-runs the model from scratch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So I built ClawCache â€“ inspired by these local optimization discussions.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It does three simple things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Tracks every call&lt;/strong&gt; â€“ Logs Ollama (or any LLM) usage with token counts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Caches locally&lt;/strong&gt; â€“ SQLite-based, serves exact repeats from disk (proven ~58% hit rate in agent-like workflows â†’ skips re-inference)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gives reports&lt;/strong&gt; â€“ Daily CLI summaries: calls, hits, saved compute&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Results from my own Ollama setups (agents + scripts):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tested on repetitive tasks/loops&lt;/li&gt; &lt;li&gt;~58% of calls cached&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real savings&lt;/strong&gt; â€“ no re-running the model on repeats&lt;/li&gt; &lt;li&gt;Foundation for semantic caching (coming in Pro â€“ matches similar prompts)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why it fits Ollama/local runs:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;100% local â€“ everything on your machine, no telemetry. Works as a lightweight wrapper around any Ollama call (or other providers in hybrid).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Free forever. MIT licensed.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;bash&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;pip install clawcache-free&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt;&lt;a href="https://github.com/AbYousef739/clawcache-free"&gt; https://github.com/AbYousef739/clawcache-free&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If it saves you GPU cycles, a on GitHub helps others find it.&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SignificantClub4279"&gt; /u/SignificantClub4279 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8iea9/clawcache_free_local_caching_tracking_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8iea9/clawcache_free_local_caching_tracking_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8iea9/clawcache_free_local_caching_tracking_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T23:03:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r969u3</id>
    <title>Using Ollama to fight executive dysfunction: A local-first app that turns hourly CSV logs and Jira references into daily stand-up summaries.</title>
    <updated>2026-02-19T17:58:33+00:00</updated>
    <author>
      <name>/u/Tech_Devils</name>
      <uri>https://old.reddit.com/user/Tech_Devils</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r969u3/using_ollama_to_fight_executive_dysfunction_a/"&gt; &lt;img alt="Using Ollama to fight executive dysfunction: A local-first app that turns hourly CSV logs and Jira references into daily stand-up summaries." src="https://external-preview.redd.it/fY2XJ1XQCwNWaM0O_h5P96oPa02vBKxm3Tbx8p4T08I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e7fbd50b90b20b2880b17405e054ac42038252e" title="Using Ollama to fight executive dysfunction: A local-first app that turns hourly CSV logs and Jira references into daily stand-up summaries." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I recently stepped out of my usual C#/SQL stack to build a Python app to help manage my ADHD and time blindness. It's called SheepCat-TrackingMyWork.&lt;/p&gt; &lt;p&gt;It runs in the background, prompts you hourly for what you've done (or Jira ticket references), and saves it to a local CSV. The core engine? It hooks directly into your local Ollama setup via Docker to generate hourly summaries and a final end-of-day stand-up report, keeping all confidential enterprise data 100% private.&lt;/p&gt; &lt;p&gt;I just did a full write-up with the architecture details, the open-source GitHub links, and my commercial license plans over in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Since this sub knows the ecosystem best, Iâ€™d love your input on the original thread: Which models (8B and under) in the Ollama library are you finding best for structured CSV summarization right now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tech_Devils"&gt; /u/Tech_Devils &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1r964lb/using_ollama_to_fight_executive_dysfunction_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r969u3/using_ollama_to_fight_executive_dysfunction_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r969u3/using_ollama_to_fight_executive_dysfunction_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T17:58:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r96uu8</id>
    <title>The greatest openclaw fork ever!</title>
    <updated>2026-02-19T18:19:05+00:00</updated>
    <author>
      <name>/u/CryptographerLow6360</name>
      <uri>https://old.reddit.com/user/CryptographerLow6360</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r96uu8/the_greatest_openclaw_fork_ever/"&gt; &lt;img alt="The greatest openclaw fork ever!" src="https://external-preview.redd.it/o4H0hywG3ZDU87_JTEfOMrijrwF8Wvzu1Bq8kfuGALU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d3b3be26adb7242635de1b4fcb6dd5db66d0360a" title="The greatest openclaw fork ever!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CryptographerLow6360"&gt; /u/CryptographerLow6360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Localclaw/comments/1r96qac/the_greatest_openclaw_fork_ever/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r96uu8/the_greatest_openclaw_fork_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r96uu8/the_greatest_openclaw_fork_ever/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T18:19:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r984g4</id>
    <title>Causal Failure Anti-Patterns (RAG)</title>
    <updated>2026-02-19T19:04:04+00:00</updated>
    <author>
      <name>/u/frank_brsrk</name>
      <uri>https://old.reddit.com/user/frank_brsrk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r984g4/causal_failure_antipatterns_rag/"&gt; &lt;img alt="Causal Failure Anti-Patterns (RAG)" src="https://preview.redd.it/3t3nkygrxhkg1.png?width=140&amp;amp;height=124&amp;amp;auto=webp&amp;amp;s=f6a1710c547af4023b8139d2b1c718cdcab68534" title="Causal Failure Anti-Patterns (RAG)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frank_brsrk"&gt; /u/frank_brsrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/u_frank_brsrk/comments/1r97eyi/causal_failure_antipatterns_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r984g4/causal_failure_antipatterns_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r984g4/causal_failure_antipatterns_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T19:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r99cqy</id>
    <title>Building a lightweight Python bridge for Qwen 2.5 Coder (7B) Handling loops and context poisoning in a 3-tier memory setup?</title>
    <updated>2026-02-19T19:49:40+00:00</updated>
    <author>
      <name>/u/This-Magazine4277</name>
      <uri>https://old.reddit.com/user/This-Magazine4277</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/This-Magazine4277"&gt; /u/This-Magazine4277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1r99c0h/building_a_lightweight_python_bridge_for_qwen_25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r99cqy/building_a_lightweight_python_bridge_for_qwen_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r99cqy/building_a_lightweight_python_bridge_for_qwen_25/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T19:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9o6up</id>
    <title>I vibe coded a portable ollama with webgui and batch files</title>
    <updated>2026-02-20T06:43:50+00:00</updated>
    <author>
      <name>/u/VaguneBob</name>
      <uri>https://old.reddit.com/user/VaguneBob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r9o6up/i_vibe_coded_a_portable_ollama_with_webgui_and/"&gt; &lt;img alt="I vibe coded a portable ollama with webgui and batch files" src="https://preview.redd.it/wfg39abghlkg1.png?width=140&amp;amp;height=105&amp;amp;auto=webp&amp;amp;s=f6bae94795900b1ec7b35df267e4f4510e6ce1ee" title="I vibe coded a portable ollama with webgui and batch files" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;no installation required, just copy to any computer and run bat file.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VaguneBob"&gt; /u/VaguneBob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r9o6up"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9o6up/i_vibe_coded_a_portable_ollama_with_webgui_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r9o6up/i_vibe_coded_a_portable_ollama_with_webgui_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T06:43:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8rgg8</id>
    <title>I built a small CLI tool to help beginners see if their hardware can actually handle local LLMs</title>
    <updated>2026-02-19T06:06:29+00:00</updated>
    <author>
      <name>/u/Narrow-Detective9885</name>
      <uri>https://old.reddit.com/user/Narrow-Detective9885</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Iâ€™ve been lurking here for a while and learning a ton from all the superusers and experts here. As a beginner myself, I often found it a bit overwhelming to figure out which models would actually run &amp;quot;well&amp;quot; on my specific machine versus just running &amp;quot;slowly.&amp;quot;&lt;/p&gt; &lt;p&gt;To help myself learn and to give something back to other newcomers, I put together a small CLI tool in Go called &lt;strong&gt;RigRank&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; Itâ€™s basically a simple benchmarking suite for Ollama. It doesnâ€™t measure how &amp;quot;smart&amp;quot; a model isâ€”there are way better tools for thatâ€”but it measures the &amp;quot;snappiness&amp;quot; of your actual hardware. It runs a few stages (code gen, summarization, reasoning, etc.) and gives you a &amp;quot;Report Card&amp;quot; with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;TTFT (Time To First Token):&lt;/strong&gt; How long youâ€™re waiting for that first word.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Writing Speed:&lt;/strong&gt; How fast it actually spits out text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reading Speed:&lt;/strong&gt; How quickly it processes your prompts.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Who this is for:&lt;/strong&gt; Honestly, if you already have a complex benchmarking pipeline or a massive GPU cluster, this probably isn't for you. Itâ€™s designed for the person who just downloaded Ollama and wants to know: &lt;em&gt;&amp;quot;Is Llama3-8B too heavy for my laptop, or is it just me?&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I would love your feedback&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/rohanelukurthy/rig-rank"&gt;https://github.com/rohanelukurthy/rig-rank&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Narrow-Detective9885"&gt; /u/Narrow-Detective9885 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8rgg8/i_built_a_small_cli_tool_to_help_beginners_see_if/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8rgg8/i_built_a_small_cli_tool_to_help_beginners_see_if/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8rgg8/i_built_a_small_cli_tool_to_help_beginners_see_if/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T06:06:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9jftp</id>
    <title>We ran 5 on-device LLMs on an Android phone to post a tweet. Most failed. Here's what actually worked</title>
    <updated>2026-02-20T02:40:22+00:00</updated>
    <author>
      <name>/u/thecoder12322</name>
      <uri>https://old.reddit.com/user/thecoder12322</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r9jftp/we_ran_5_ondevice_llms_on_an_android_phone_to/"&gt; &lt;img alt="We ran 5 on-device LLMs on an Android phone to post a tweet. Most failed. Here's what actually worked" src="https://external-preview.redd.it/Ifkv_A-rFTUDpG4lceG3oh3N3IZbGJBelInXLvnH35E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1ff6f6ececb52eb7c3262dbbd738b3c8e70f3a0" title="We ran 5 on-device LLMs on an Android phone to post a tweet. Most failed. Here's what actually worked" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecoder12322"&gt; /u/thecoder12322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fw6surzpakkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9jftp/we_ran_5_ondevice_llms_on_an_android_phone_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r9jftp/we_ran_5_ondevice_llms_on_an_android_phone_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T02:40:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9a0gv</id>
    <title>I built a local-first code search tool with Ollama + CocoIndex to save tokens when chatting about codebases.</title>
    <updated>2026-02-19T20:13:57+00:00</updated>
    <author>
      <name>/u/VioletCranberryy</name>
      <uri>https://old.reddit.com/user/VioletCranberryy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to play with Ollama, CocoIndex, and how semantic vs hybrid search actually works (RRF fusion and all that) - and ended up building something I actually use daily, more and more. The idea is pretty old: index your codebase locally so AI assistants can search it semantically instead of stuffing entire files into context. Fewer tokens, better results.&lt;/p&gt; &lt;p&gt;It comes with a web dashboard, MCP server, CLI, and works as a Claude Code plugin.&lt;/p&gt; &lt;p&gt;As a DevOps engineer, I also wanted to make it expandable with custom &amp;quot;grammars&amp;quot; â€” so things like Helm charts, GitHub Actions, Docker Compose, and Terraform get proper structure-aware chunking instead of being treated as generic YAML/HCL. And of course, a Docker Compose setup to start everything with a single command :) &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/VioletCranberry/coco-search"&gt;https://github.com/VioletCranberry/coco-search&lt;/a&gt; &lt;/p&gt; &lt;p&gt;P.S.&lt;br /&gt; Yeah, I used AI assistants heavily during development which is kind of fitting since the tool is built to make AI-assisted coding better. I still don't know how to feel about it, first project of this type of mine. &lt;/p&gt; &lt;p&gt;P.P.S.&lt;br /&gt; Would love feedback, especially on embedding model choice for code. Something else as small as nomic-embed-text and as powerful? CocoIndex updates are incremental, but on large codebases you still need to wait when building the first index.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VioletCranberryy"&gt; /u/VioletCranberryy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9a0gv/i_built_a_localfirst_code_search_tool_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9a0gv/i_built_a_localfirst_code_search_tool_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r9a0gv/i_built_a_localfirst_code_search_tool_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-19T20:13:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9q3kg</id>
    <title>VPS ou second ordinateur dans sous rÃ©seau et IA locale</title>
    <updated>2026-02-20T08:42:04+00:00</updated>
    <author>
      <name>/u/PhoenixProjectAI</name>
      <uri>https://old.reddit.com/user/PhoenixProjectAI</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhoenixProjectAI"&gt; /u/PhoenixProjectAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1r9q31c/vps_ou_second_ordinateur_dans_sous_rÃ©seau_et_ia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9q3kg/vps_ou_second_ordinateur_dans_sous_rÃ©seau_et_ia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r9q3kg/vps_ou_second_ordinateur_dans_sous_rÃ©seau_et_ia/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T08:42:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9qftm</id>
    <title>ollama - telegram</title>
    <updated>2026-02-20T09:03:54+00:00</updated>
    <author>
      <name>/u/Aromatic_Radio1650</name>
      <uri>https://old.reddit.com/user/Aromatic_Radio1650</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys i was wondering if there is any way i could connect a local llm to telegram so i can access it on the go?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aromatic_Radio1650"&gt; /u/Aromatic_Radio1650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9qftm/ollama_telegram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9qftm/ollama_telegram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r9qftm/ollama_telegram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T09:03:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9kai2</id>
    <title>Error: timed out waiting for server to start</title>
    <updated>2026-02-20T03:20:22+00:00</updated>
    <author>
      <name>/u/TigerBazooka</name>
      <uri>https://old.reddit.com/user/TigerBazooka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I receive this error from the command line when trying any variation of the command to run or pull deepseek of any variation. I've reinstalled, restarted, ended the process and retried, getting to be at my wit's end. I had a working model running via Ollama before which I deleted, and apparently that was a mistake because now I can't reinstall it.&lt;/p&gt; &lt;p&gt;Windows 10. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TigerBazooka"&gt; /u/TigerBazooka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9kai2/error_timed_out_waiting_for_server_to_start/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9kai2/error_timed_out_waiting_for_server_to_start/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r9kai2/error_timed_out_waiting_for_server_to_start/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T03:20:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9s295</id>
    <title>Causal-Antipatterns (dataset ; rag; agent; open source; reasoning)</title>
    <updated>2026-02-20T10:40:55+00:00</updated>
    <author>
      <name>/u/frank_brsrk</name>
      <uri>https://old.reddit.com/user/frank_brsrk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r9s295/causalantipatterns_dataset_rag_agent_open_source/"&gt; &lt;img alt="Causal-Antipatterns (dataset ; rag; agent; open source; reasoning)" src="https://external-preview.redd.it/iQC_SpL3fx8EKPZ8l2A1UXrrJyor2WS2EB3XmA8bkUk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6efbdae4042637de8436dcf30faedd4bbc3cd9d" title="Causal-Antipatterns (dataset ; rag; agent; open source; reasoning)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frank_brsrk"&gt; /u/frank_brsrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/dataengineer/comments/1r9s1x2/causalantipatterns_dataset_rag_agent_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9s295/causalantipatterns_dataset_rag_agent_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r9s295/causalantipatterns_dataset_rag_agent_open_source/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T10:40:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9qa3i</id>
    <title>is Microsoft selectively allowing paid partner model to run in agent mode in vscode?</title>
    <updated>2026-02-20T08:53:59+00:00</updated>
    <author>
      <name>/u/Ascend-910</name>
      <uri>https://old.reddit.com/user/Ascend-910</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have tried many ollama models, unless I am missing something important, there is no way to run local ollama models in agent mode in vscode NATIVELY.&lt;/p&gt; &lt;p&gt;Other extension's capabilities from my experience is not use as good as vscode's native capabilities&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ascend-910"&gt; /u/Ascend-910 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9qa3i/is_microsoft_selectively_allowing_paid_partner/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9qa3i/is_microsoft_selectively_allowing_paid_partner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r9qa3i/is_microsoft_selectively_allowing_paid_partner/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T08:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9s3k1</id>
    <title>ðŸš€ Agentic Signal v2.2.0 is now live!</title>
    <updated>2026-02-20T10:43:02+00:00</updated>
    <author>
      <name>/u/Code-Forge-Temple</name>
      <uri>https://old.reddit.com/user/Code-Forge-Temple</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r9s3k1/agentic_signal_v220_is_now_live/"&gt; &lt;img alt="ðŸš€ Agentic Signal v2.2.0 is now live!" src="https://external-preview.redd.it/tdO68NSU_bkPNzDRSRPkbqFl1gitDx7QxC_kzWmOoR0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1a9f36721ff6ec908e71a70c7d9094fca1a4ed6" title="ðŸš€ Agentic Signal v2.2.0 is now live!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Code-Forge-Temple"&gt; /u/Code-Forge-Temple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AgenticSignal/comments/1r9s2pu/agentic_signal_v220_is_now_live/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9s3k1/agentic_signal_v220_is_now_live/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r9s3k1/agentic_signal_v220_is_now_live/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T10:43:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rab1zw</id>
    <title>How to integration Ollama in OpenClaw?</title>
    <updated>2026-02-20T23:24:42+00:00</updated>
    <author>
      <name>/u/Delinquent8438</name>
      <uri>https://old.reddit.com/user/Delinquent8438</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm running Ollama with Mixtral-8x7B on my MacBook Pro M4.&lt;/p&gt; &lt;p&gt;So far it is working and I can use it via the terminal.&lt;/p&gt; &lt;p&gt;I want to integrate it now in OpenClaw. If I got the tutorials correct, I need to skip the OpenClaw install wizard, just select any LLM in manual mode, and add the Ollama config later in the JSON file, correct?&lt;/p&gt; &lt;p&gt;I simply copied pasted this config from the OpenClaw docs, but somehow it is not working:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;models&amp;quot;: { &amp;quot;providers&amp;quot;: { &amp;quot;ollama&amp;quot;: { &amp;quot;apiKey&amp;quot;: &amp;quot;ollama-local&amp;quot;, &amp;quot;baseUrl&amp;quot;: &amp;quot;http://192.168.0.100:11434&amp;quot;, }, }, }, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Am I missing anything?&lt;/p&gt; &lt;p&gt;What is the most straight forward way to get Ollama/Mixtral work in OpenClaw without any other LLM beside?&lt;/p&gt; &lt;p&gt;Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delinquent8438"&gt; /u/Delinquent8438 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rab1zw/how_to_integration_ollama_in_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rab1zw/how_to_integration_ollama_in_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rab1zw/how_to_integration_ollama_in_openclaw/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T23:24:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1rajdu7</id>
    <title>hi! are there some local models that allow video generation from many poses of a certain character?</title>
    <updated>2026-02-21T06:00:03+00:00</updated>
    <author>
      <name>/u/DoubleSubstantial805</name>
      <uri>https://old.reddit.com/user/DoubleSubstantial805</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i got 6gbvram + limited system ram + 4050rtx, i wanted to make certain video generations for a character i made using comfyui&lt;/p&gt; &lt;p&gt;are there models that can run on my machine?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DoubleSubstantial805"&gt; /u/DoubleSubstantial805 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rajdu7/hi_are_there_some_local_models_that_allow_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rajdu7/hi_are_there_some_local_models_that_allow_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rajdu7/hi_are_there_some_local_models_that_allow_video/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T06:00:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1raat9a</id>
    <title>Using AI as part of the game play; best examples?</title>
    <updated>2026-02-20T23:14:54+00:00</updated>
    <author>
      <name>/u/apoliaki</name>
      <uri>https://old.reddit.com/user/apoliaki</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone has good examples of ai games with really cool gameplay? What I mean here isn't using ai to improve speed; etc of making a game; more so that AI is part of the gameplay; i.e. game evolves based on &amp;quot;non determnistic outputs&amp;quot;&lt;/p&gt; &lt;p&gt;I feel like it's definetly the future of gaming (particularly when we'll have good on edge LLMs which will likely be within this year)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I've tried this one and it's pretty cool but have nots een many more afterwards: &lt;a href="https://store.steampowered.com/app/3730100/Whispers_from_the_Star/"&gt;https://store.steampowered.com/app/3730100/Whispers_from_the_Star/&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I know Runway ML is working on this and they launched an early access for months ago&lt;/p&gt; &lt;p&gt;Curious if you guys have some examples/good read on this?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1raas12"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apoliaki"&gt; /u/apoliaki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1raat9a/using_ai_as_part_of_the_game_play_best_examples/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1raat9a/using_ai_as_part_of_the_game_play_best_examples/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1raat9a/using_ai_as_part_of_the_game_play_best_examples/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T23:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1radfsh</id>
    <title>Capi - Openvino GenAI alternative for Ollama</title>
    <updated>2026-02-21T01:06:55+00:00</updated>
    <author>
      <name>/u/Little_Investigator3</name>
      <uri>https://old.reddit.com/user/Little_Investigator3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1radfsh/capi_openvino_genai_alternative_for_ollama/"&gt; &lt;img alt="Capi - Openvino GenAI alternative for Ollama" src="https://preview.redd.it/lt6y65hchskg1.png?width=140&amp;amp;height=100&amp;amp;auto=webp&amp;amp;s=d52e5920cd98782da3b7a0a43516946beb3b1f53" title="Capi - Openvino GenAI alternative for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;Iâ€™m excited to launch my first open-source project: **Capi**, a local LLM Linux/Windows app designed as an alternative to Ollama for users of Intel GPUs, with a focus on Arc GPU due to their higher Xe core counts and improved throughput, though it should work with older Intel hardware&lt;/p&gt; &lt;p&gt;[&lt;a href="https://github.com/tiagoflino/capi%5D(https://github.com/tiagoflino/capi"&gt;https://github.com/tiagoflino/capi](https://github.com/tiagoflino/capi)&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;The project is called Capi (inspired by the capybara (or capivara in Portuguese, also native from my hometown), and though I'm still working on fixing Windows x64 installer, it is already working fairly stable on Linux.&lt;/p&gt; &lt;p&gt;While the Windows x64 installer is still under development, the project is already stable on Linux. I havenâ€™t benchmarked it against IPEX-LLM or Vulkan yet, but on my Lenovo Lunar Lake 258V (32GB RAM, \~110 GB/s bandwidth), it sustains almost double the TG figures for the same prompt when running Qwen3-4B compared to the Vulkan backend.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lt6y65hchskg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1bfcb5387a7077b02c2c4cbf452061e477900377"&gt;https://preview.redd.it/lt6y65hchskg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1bfcb5387a7077b02c2c4cbf452061e477900377&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am aware that there are other initiatives like Openarc heavily invested in Openvino as well, but I wanted to create a UX for less tech-savvy people, similar to what Ollama and LMStudio do, but powered by the proper Intel engine. I am far from there, but we have to start somewhere, don't we? :-)&lt;/p&gt; &lt;p&gt;The tool allows for selecting and fetching GGUF models from HF, and it runs them converted on the fly, so there is no IR conversion required.&lt;/p&gt; &lt;p&gt;I used Rust with a CXX bridge to create bindings to the C++ Openvino GenAI API, as it is the most complete and I assume (though no proper benchmarks on that as well) a little more resource efficient than the Python API available. The UI uses Tauri with Svelte.&lt;/p&gt; &lt;p&gt;Next steps: working on adding more tuning options, refining the hardware metrics, installers and library download, test it more thoroughly, and create a project webpage to add documentation and streamline installation process via script.&lt;/p&gt; &lt;p&gt;Hope you enjoy the project, and Iâ€™d love to hear your thoughts!&lt;/p&gt; &lt;p&gt;P.S. This is my first open-source project, and Iâ€™m still getting around the stack. Iâ€™m open to any tips on improving the code, design, or documentation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Little_Investigator3"&gt; /u/Little_Investigator3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1radfsh/capi_openvino_genai_alternative_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1radfsh/capi_openvino_genai_alternative_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1radfsh/capi_openvino_genai_alternative_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T01:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ra9wsv</id>
    <title>my portable ollama now has persistent memory</title>
    <updated>2026-02-20T22:39:02+00:00</updated>
    <author>
      <name>/u/VaguneBob</name>
      <uri>https://old.reddit.com/user/VaguneBob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ra9wsv/my_portable_ollama_now_has_persistent_memory/"&gt; &lt;img alt="my portable ollama now has persistent memory" src="https://preview.redd.it/0mwkg27n7qkg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=2125242ea136b89e636be8e0c012c543a9a1c392" title="my portable ollama now has persistent memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This instance of ollama you can just copy onto any machine without installing anything (no docker needed), everything ollama needs is contained within a folder, and all the memories can be reset with one click. This is work in progress.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;F:\MyOllama\ â”œâ”€â”€ Start-Ollama-Dark-Memory.bat â”œâ”€â”€ ollama-dark-memory-final.html â”œâ”€â”€ ollama.exe â”œâ”€â”€ ollama-portable.bat â”œâ”€â”€ models\ â””â”€â”€ memory\ â”œâ”€â”€ memory_server.py â””â”€â”€ memories.json &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VaguneBob"&gt; /u/VaguneBob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ra9wsv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ra9wsv/my_portable_ollama_now_has_persistent_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ra9wsv/my_portable_ollama_now_has_persistent_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T22:39:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rakz0o</id>
    <title>Best structure and models for invoice data extraction</title>
    <updated>2026-02-21T07:30:28+00:00</updated>
    <author>
      <name>/u/Fickle-Bluebird-367</name>
      <uri>https://old.reddit.com/user/Fickle-Bluebird-367</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to build an invoice processing tool that can extract data such as supplier, total, ,vat net amount etc from pdf invoices with a range of layouts and styles. So far I have been using pytesseract to perform OCR then feeding the result into a local LLM using Ollama. with this I can get to about 85-90% accuracy but want to improve. can anyone suggest an improvement structure (for example skipping the OCR and passing the invoice straight into a multimodal model?) or which models are best for this task. I am currently running locally on a mac with 8gb RAM but could run on another set up with 16gb. &lt;/p&gt; &lt;p&gt;Would be great to get any tips from anyone who has worked on something similar. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fickle-Bluebird-367"&gt; /u/Fickle-Bluebird-367 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rakz0o/best_structure_and_models_for_invoice_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rakz0o/best_structure_and_models_for_invoice_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rakz0o/best_structure_and_models_for_invoice_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T07:30:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9zea9</id>
    <title>SmarterRouter - A Smart LLM proxy for all your local models. (native Ollama support, loading/unloading models automatically)</title>
    <updated>2026-02-20T16:06:53+00:00</updated>
    <author>
      <name>/u/peva3</name>
      <uri>https://old.reddit.com/user/peva3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on this project to create a smarter LLM proxy primarily for my openwebui setup (but it's a standard openai compatible endpoint API, so it will work with anything that accepts that).&lt;/p&gt; &lt;p&gt;The idea is pretty simple, you see one frontend model in your system, but in the backend it can load whatever model is &amp;quot;best&amp;quot; for the prompt you send. When you first spin up Smarterrouter it profiles all your models, giving them scores for all the main types of prompts you could ask, as well as benchmark other things like model size, actual VRAM usage, etc. (you can even configure an external &amp;quot;Judge&amp;quot; AI to grade the responses the models give, i've found it improves the profile results, but it's optional). It will also detect and new or deleted models and start profiling them in the background, you don't need to do anything, just add your models to ollama and they will be added to SmarterRouter to be used.&lt;/p&gt; &lt;p&gt;There's a lot going on under the hood, but i've been putting it through it's paces and so far it's performing really well, It's extremely fast, It caches responses, and I'm seeing a negligible amount of time added to prompt response time. It will also automatically load and unload the models in Ollama (and any other backend that allows that).&lt;/p&gt; &lt;p&gt;The only caveat i've found is that currently it favors very small, high performing models, like Qwen coder 0.5B for example, but if small models are faster and they score really highly in the benchmarks... Is that really a bad response? I'm doing more digging, but so far it's working really well with all the test prompts i've given it to try (swapping to larger/different models for more complex questions or creative questions that are outside of the small models wheelhouse).&lt;/p&gt; &lt;p&gt;Here's a high level summary of the biggest features:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Self-Correction via Hardware Profiling&lt;/strong&gt;: Instead of guessing performance, it runs a one-time benchmark on your specific GPU/CPU setup. It learns exactly how fast and capable your models are in your unique environment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Active VRAM Guard&lt;/strong&gt;: It monitors nvidia-smi in real-time. If a model selection is about to trigger an Out-of-Memory (OOM) error, it proactively unloads idle models or chooses a smaller alternative to keep your system stable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Semantic &amp;quot;Smart&amp;quot; Caching&lt;/strong&gt;: It doesn't just match exact text. It uses vector embeddings to recognize when youâ€™re asking a similar question to a previous one, serving the cached response instantly and saving your compute cycles.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The &amp;quot;One Model&amp;quot; Illusion&lt;/strong&gt;: It presents your entire collection of 20+ models as a single OpenAI-compatible endpoint. You just select SmarterRouter in your UI, and it handles the &amp;quot;load, run, unload&amp;quot; logic behind the scenes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Intelligence-to-Task Routing&lt;/strong&gt;: It automatically analyzes your prompt's complexity. It won't waste your 70B model's time on a &amp;quot;Hello,&amp;quot; and it won't let a 0.5B model hallucinate its way through a complex Python refactor.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLM-as-Judge Feedback&lt;/strong&gt;: It can use a high-end model (like a cloud GPT-4o or a local heavy-hitter) to periodically &amp;quot;score&amp;quot; the performance of your smaller models, constantly refining its own routing weights based on actual quality.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/peva3/SmarterRouter"&gt;https://github.com/peva3/SmarterRouter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know how this works for you, I have it running perfectly with a 4060 ti 16gb, so i'm positive that it will scale well to the massive systems some of y'all have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peva3"&gt; /u/peva3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9zea9/smarterrouter_a_smart_llm_proxy_for_all_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9zea9/smarterrouter_a_smart_llm_proxy_for_all_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r9zea9/smarterrouter_a_smart_llm_proxy_for_all_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T16:06:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1rajqj6</id>
    <title>15,000+ tok/s on ChatJimmy: Is the "Model-on-Silicon" era finally starting?</title>
    <updated>2026-02-21T06:19:08+00:00</updated>
    <author>
      <name>/u/Significant-Topic433</name>
      <uri>https://old.reddit.com/user/Significant-Topic433</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"&gt; &lt;img alt="15,000+ tok/s on ChatJimmy: Is the &amp;quot;Model-on-Silicon&amp;quot; era finally starting?" src="https://preview.redd.it/bq69s0n5jskg1.jpg?width=140&amp;amp;height=66&amp;amp;auto=webp&amp;amp;s=fa3f690c9b529f18075dc6e27d8b984f7fcc4fcd" title="15,000+ tok/s on ChatJimmy: Is the &amp;quot;Model-on-Silicon&amp;quot; era finally starting?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weâ€™ve been discussing local inference for years, but chatjimmy.ai just moved the goalposts. They are hitting 15,414 tokens per second using what they call &amp;quot;mask ROM recall fabric&amp;quot;â€”basically etching the model weights directly into the silicon logic.&lt;/p&gt; &lt;p&gt;â€‹This is a massive shift from our current setups. Weâ€™re used to general-purpose compute, but this is a dedicated ASIC. No HBM, no VRAM bottlenecks, just raw, hardcoded inference.&lt;/p&gt; &lt;p&gt;â€‹ I just invested in two Gigabyte AI TOP ATOM units (the ones based on the NVIDIA Spark / Grace Blackwell architecture). They are absolute beasts for training and fine-tuning with 128GB of unified memory, but seeing a dedicated chip do 15k tok/s makes me wonder: &lt;/p&gt; &lt;p&gt;â€‹Did I make the right call with the AI TOP Spark units for local dev, or are we going to see these specialized ASIC cards hit the market soon and make general-purpose desktop AI look like dial-up?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant-Topic433"&gt; /u/Significant-Topic433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rajqj6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T06:19:08+00:00</published>
  </entry>
</feed>
