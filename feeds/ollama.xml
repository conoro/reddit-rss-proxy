<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-10T11:06:15+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1opv612</id>
    <title>Experimenting with Mistral + Ollama after reading this book- some takeaways and open questions</title>
    <updated>2025-11-06T10:13:19+00:00</updated>
    <author>
      <name>/u/FoundSomeLogic</name>
      <uri>https://old.reddit.com/user/FoundSomeLogic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I recently finished reading &lt;strong&gt;Learn Mistral: Elevating Systems with Embeddings&lt;/strong&gt; and wanted to share some of the surprising things I picked up (and a few open questions I still have), especially since many of us here are working with local LLM workflows and tools like Ollama.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What struck me&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The author really dives into the “why” behind embeddings and how they change the way we think about retrieval and alignment, so for me, it was refreshing to see a chapter not just on &lt;em&gt;“how to embed text”&lt;/em&gt;, but on “why this embedding helps integrate with a system like Ollama or similar tools”.&lt;/li&gt; &lt;li&gt;There’s a section where the book shows practical setups: pre-processing, embedding generation, combining with local models. I’m working with a Mistral-style model locally, and I found myself immediately scribbling notes about how I could adapt one of the workflows.&lt;/li&gt; &lt;li&gt;The clarity: Even though the topic is technical, it doesn’t assume you’re an elite ML researcher. It offers enough practical code snippets and real-world examples to experiment with. I tried out two of them this weekend and learned something useful (and made a few mistakes, which is always good!).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How this ties into what I do with Ollama&lt;/strong&gt;&lt;br /&gt; I run Ollama locally (on a decent machine, but nothing crazy). One of my ongoing challenges has been: “How do I get the model to really understand my domain-specific data rather than just general chat behavior?” The book’s guidance around embeddings + index + retrieval + prompt design suddenly made more sense in that context. In short: I felt like I went from “I know Ollama can load the model and respond” → “Okay, now how do I feed it knowledge and get it to reason in my domain?”.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One or two things I’m still thinking about&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The author mentions keeping embeddings fresh and versioned as your domain data grows. I wonder how folks here are doing that in production/local setups with Ollama: do you rebuild the entire index, keep incremental updates, or something else? If you’ve tried this I’d love to hear your experience.&lt;/li&gt; &lt;li&gt;There’s a trade-off discussed between embedding size/complexity and cost/time. Locally it's manageable, but if you scale up you might hit bottlenecks. I’m curious what strategies others use to strike that balance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Would I recommend it?&lt;/strong&gt;&lt;br /&gt; Yes, if you’re using Ollama (or any local LLM stack) &lt;em&gt;and&lt;/em&gt; you’re ready to go beyond “just chat with the model” and into “let the model reason with my data”, this book provides a solid step. It’s not a silver-bullet: you’ll still need to adapt for your domain and do the engineering work, but it offers a clearer map.&lt;/p&gt; &lt;p&gt;Happy to share a few of my notes (code snippet, embedding library used, one prompt trick) if anyone is interested. Also curious: if you’ve read it (or a similar book), what surprised you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FoundSomeLogic"&gt; /u/FoundSomeLogic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1opv612/experimenting_with_mistral_ollama_after_reading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1opv612/experimenting_with_mistral_ollama_after_reading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1opv612/experimenting_with_mistral_ollama_after_reading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-06T10:13:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqmk5n</id>
    <title>OpenMemory/Mem0</title>
    <updated>2025-11-07T05:56:08+00:00</updated>
    <author>
      <name>/u/AdCompetitive6193</name>
      <uri>https://old.reddit.com/user/AdCompetitive6193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdCompetitive6193"&gt; /u/AdCompetitive6193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenWebUI/comments/1oqmix5/openmemorymem0/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqmk5n/openmemorymem0/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oqmk5n/openmemorymem0/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-07T05:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq772p</id>
    <title>Advice appreciated: Here's how I'm trying to use Ollama at home</title>
    <updated>2025-11-06T18:43:59+00:00</updated>
    <author>
      <name>/u/Punnalackakememumu</name>
      <uri>https://old.reddit.com/user/Punnalackakememumu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have purchased a used Dell OptiPlex 9020 minitower that I am dedicating to use as an Ollama AI server.&lt;/p&gt; &lt;p&gt;CPU Intel(R) Core i5-4590 CPU @ 3.30GHz&lt;br /&gt; RAM 32 GB RAM&lt;br /&gt; Storage 465 GB SSD&lt;br /&gt; Graphics NVIDIA GeForce GTX 1050 Ti (4 GB)&lt;br /&gt; OS Linux Mint&lt;/p&gt; &lt;p&gt;I am trying to use AI to help me write a semi-autographical story. &lt;/p&gt; &lt;p&gt;AI on its own (Grok, DuckAi, etc.) seems to have trouble retaining character profiles the longer I interact with it. I can feed it a good descriptive character profile, and it uses it and adapts it based on the story development (characters can gain weight or get their hair cut, for example). However, if you have characters who aren't discussed after a couple of chapters, the AI seems to forget the details and create its own: suddenly Uncle Mario, the retired Italian racecar driver, is a redheaded guy who delivers baked goods.&lt;/p&gt; &lt;p&gt;I realize I have hardware constraints, so I'm planning to stick to a 7b LLM. I'm creating text only.&lt;/p&gt; &lt;p&gt;I'd like to have Ollama running on the Mint server using a fairly permissive LLM like Mistral 7b so it doesn't fuss at me about profanity, adult themes, etc. In a test, I tried to use AnythingLLM to inject data (so I could point it at a web page about a topic and have the model learn information that I want a character to know in story, but AnythingLLM complained about subject matter.&lt;/p&gt; &lt;p&gt;I'd like for it to allow me to access the server via a web browser on my regular PC or laptop in my network so that I'm not always creating while sitting in my workshop where the Mint system lives.&lt;/p&gt; &lt;p&gt;I'd like to have it store character profiles &amp;quot;offline&amp;quot; in a text file or something so it can access them if my main characters haven't interacted with someone in a little while.&lt;/p&gt; &lt;p&gt;So, I'm open to suggestions for software I can use for this effort.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Punnalackakememumu"&gt; /u/Punnalackakememumu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oq772p/advice_appreciated_heres_how_im_trying_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oq772p/advice_appreciated_heres_how_im_trying_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oq772p/advice_appreciated_heres_how_im_trying_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-06T18:43:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1opux53</id>
    <title>Asked my AI Agent to recommend me top 5 stocks to buy today :)</title>
    <updated>2025-11-06T09:58:19+00:00</updated>
    <author>
      <name>/u/FriendshipCreepy8045</name>
      <uri>https://old.reddit.com/user/FriendshipCreepy8045</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1opux53/asked_my_ai_agent_to_recommend_me_top_5_stocks_to/"&gt; &lt;img alt="Asked my AI Agent to recommend me top 5 stocks to buy today :)" src="https://preview.redd.it/zpbn8y18zlzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb36753aa1dd4e9a1fc0ece04720addd8ccb9bc9" title="Asked my AI Agent to recommend me top 5 stocks to buy today :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Everyone!&lt;/p&gt; &lt;p&gt;So some of you have seen the post about how I made my own local agent: &lt;strong&gt;&amp;quot;Agent Kurama&amp;quot;&lt;/strong&gt;, and many of you liked it. I couldn’t be happier, as some of you followed me, starred the repo, and most importantly, advised me on how to improve it.&lt;/p&gt; &lt;p&gt;Recently, I added more search tools and a summarizer for unbiased search and information handling, and this time I’ll test it for real.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;I’ll put my own ₹10,000 (or $100) into the stocks it recommends.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now, this fox made a huuuge report like 389 lines but here’s the conclusion of that report:&lt;/p&gt; &lt;p&gt;&lt;em&gt;&amp;quot;A balanced ₹10,000 portfolio of Groww’s flagship large-cap picks — Reliance, HDFC Bank, Infosys, Tata Motors, and ITC — fits the budget, offers sector diversification, and aligns with “top-stock” recommendations.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;To be honest, these recommendations seem kinda obvious, but we’ll see. Now I’ll put equal money into those top 5 stocks and check back in 6 months :)&lt;/p&gt; &lt;p&gt;This is all educational and experimental - no financial advice, just me being curious &amp;amp; dumb &amp;gt;.&amp;lt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Project link:&lt;/strong&gt; &lt;a href="https://github.com/vedas-dixit/LocalAgent"&gt;https://github.com/vedas-dixit/LocalAgent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FriendshipCreepy8045"&gt; /u/FriendshipCreepy8045 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zpbn8y18zlzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1opux53/asked_my_ai_agent_to_recommend_me_top_5_stocks_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1opux53/asked_my_ai_agent_to_recommend_me_top_5_stocks_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-06T09:58:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqchak</id>
    <title>Epoch: LLMs that generate interactive UI instead of text walls</title>
    <updated>2025-11-06T22:05:04+00:00</updated>
    <author>
      <name>/u/ItzCrazyKns</name>
      <uri>https://old.reddit.com/user/ItzCrazyKns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oqchak/epoch_llms_that_generate_interactive_ui_instead/"&gt; &lt;img alt="Epoch: LLMs that generate interactive UI instead of text walls" src="https://preview.redd.it/elog79cngpzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2a6a284595f32b69211b6ce05cbcd3fd5a10860" title="Epoch: LLMs that generate interactive UI instead of text walls" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ItzCrazyKns"&gt; /u/ItzCrazyKns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/elog79cngpzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqchak/epoch_llms_that_generate_interactive_ui_instead/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oqchak/epoch_llms_that_generate_interactive_ui_instead/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-06T22:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqvxzw</id>
    <title>Learning resources &amp; advice</title>
    <updated>2025-11-07T14:31:53+00:00</updated>
    <author>
      <name>/u/SirEblingMis</name>
      <uri>https://old.reddit.com/user/SirEblingMis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;I'm looking for some learning resources. On which models to use, quantization, etc.&lt;br /&gt; I tinkered a bit with ollama and LM studios. I have absolutely no idea which model to start with. How much training etc does a new model need?&lt;/p&gt; &lt;p&gt;My hardware: 9950x3d, 32gb 6000c28 ram, rtx5080, good ssds.&lt;/p&gt; &lt;p&gt;I'm noticing I only get 18-25 tokens/sec on models like the Qwen3 30b. I'm looking for a model that matches that hardware to do work with me on math, statistics, modelling, and admin assistant stuff.&lt;br /&gt; Basically running it while I do work by hand, like an extra brain almost (even though I don't trust their results lol).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SirEblingMis"&gt; /u/SirEblingMis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqvxzw/learning_resources_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqvxzw/learning_resources_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oqvxzw/learning_resources_advice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-07T14:31:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqwmwh</id>
    <title>A 'cookie-cutter' FLOSS LLM model + UI setup guide for the average user at three different price point GPUs?</title>
    <updated>2025-11-07T15:00:03+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(For those that may know: many years ago, &lt;a href="/r/buildapc"&gt;/r/buildapc&lt;/a&gt; used to have a cookie-cutter build guide. I'm looking for something similar, except it's software only.)&lt;/p&gt; &lt;p&gt;There are so many LLMs and so many tools surrounding them that it's becoming harder to navigate through all the information.&lt;/p&gt; &lt;p&gt;I used to just simply use Ollama + Open WebUI, but seeing that Open WebUI switched to more protective license, I've been struggling to find which is the right UI.&lt;/p&gt; &lt;p&gt;Eventually, for my GPU, I think GPT OSS 20B is the right model, just unsure about which UI to use. I understand that there are other uses that are not text-only, like photo, code, video, audio generation, so cookie-cutter setups could be expanded that way.&lt;/p&gt; &lt;p&gt;So, is there such a guide?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqwmwh/a_cookiecutter_floss_llm_model_ui_setup_guide_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqwmwh/a_cookiecutter_floss_llm_model_ui_setup_guide_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oqwmwh/a_cookiecutter_floss_llm_model_ui_setup_guide_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-07T15:00:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1orcv34</id>
    <title>update v0.6.34 (latest) lost most of my Models for Ollama are gone!</title>
    <updated>2025-11-08T01:46:12+00:00</updated>
    <author>
      <name>/u/Mudcatt101</name>
      <uri>https://old.reddit.com/user/Mudcatt101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1orcv34/update_v0634_latest_lost_most_of_my_models_for/"&gt; &lt;img alt="update v0.6.34 (latest) lost most of my Models for Ollama are gone!" src="https://b.thumbs.redditmedia.com/V2Dssjc6YOJC7znTdotuSE3rJhQQdnlIl43Ku0-o9Mc.jpg" title="update v0.6.34 (latest) lost most of my Models for Ollama are gone!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mudcatt101"&gt; /u/Mudcatt101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenWebUI/comments/1orcutk/update_v0634_latest_lost_most_of_my_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1orcv34/update_v0634_latest_lost_most_of_my_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1orcv34/update_v0634_latest_lost_most_of_my_models_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-08T01:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1orql47</id>
    <title>AI agents just got scary good. Do we still need developers?</title>
    <updated>2025-11-08T14:20:48+00:00</updated>
    <author>
      <name>/u/eworker8888</name>
      <uri>https://old.reddit.com/user/eworker8888</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eworker8888"&gt; /u/eworker8888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/eworker_ca/comments/1orqj8z/ai_agents_just_got_scary_good_do_we_still_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1orql47/ai_agents_just_got_scary_good_do_we_still_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1orql47/ai_agents_just_got_scary_good_do_we_still_need/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-08T14:20:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqtytx</id>
    <title>Smallest model you know for less powerful computers?</title>
    <updated>2025-11-07T13:09:14+00:00</updated>
    <author>
      <name>/u/Weebolt</name>
      <uri>https://old.reddit.com/user/Weebolt</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weebolt"&gt; /u/Weebolt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqtytx/smallest_model_you_know_for_less_powerful/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqtytx/smallest_model_you_know_for_less_powerful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oqtytx/smallest_model_you_know_for_less_powerful/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-07T13:09:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ori3mz</id>
    <title>Is there any draw backs to using an external dual GPU config with thunderbolt 5 with a laptop for AI?</title>
    <updated>2025-11-08T06:17:58+00:00</updated>
    <author>
      <name>/u/FX2021</name>
      <uri>https://old.reddit.com/user/FX2021</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FX2021"&gt; /u/FX2021 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ori3mz/is_there_any_draw_backs_to_using_an_external_dual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ori3mz/is_there_any_draw_backs_to_using_an_external_dual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ori3mz/is_there_any_draw_backs_to_using_an_external_dual/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-08T06:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1or24y4</id>
    <title>Want to Learn More About Agentic AI – Looking to Contribute</title>
    <updated>2025-11-07T18:26:27+00:00</updated>
    <author>
      <name>/u/Superb_Practice_4544</name>
      <uri>https://old.reddit.com/user/Superb_Practice_4544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone — I’ve built a few agentic AI systems around SaaS automation and coding tools. I’m familiar with LangChain, LangGraph, RAG, tool calling, and MCP, but I want to learn more by contributing to real projects.&lt;/p&gt; &lt;p&gt;If you’re working on something in this space or know an open-source project looking for contributors, I’d love to help out and learn from it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Superb_Practice_4544"&gt; /u/Superb_Practice_4544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1or24y4/want_to_learn_more_about_agentic_ai_looking_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1or24y4/want_to_learn_more_about_agentic_ai_looking_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1or24y4/want_to_learn_more_about_agentic_ai_looking_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-07T18:26:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1orglym</id>
    <title>Improving accuracy when extracting structured data from OCR text using Gemma 3</title>
    <updated>2025-11-08T04:54:54+00:00</updated>
    <author>
      <name>/u/Weekly_Signature_510</name>
      <uri>https://old.reddit.com/user/Weekly_Signature_510</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on a project where I extract text from U.S. driver’s license images using OCR. The OCR text itself contains all the necessary information (name, address, license number, etc.), and I also provide a version of the image with bounding boxes for context.&lt;/p&gt; &lt;p&gt;However, even though the OCR output has everything, my LLM (Gemma 3 12B running via Ollama) still misses or misclassifies some fields when structuring the data into JSON.&lt;/p&gt; &lt;p&gt;What can I do to improve extraction accuracy? Would better prompt design, fine-tuning, or additional preprocessing (like spatial grouping or text reformatting) make the biggest difference here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weekly_Signature_510"&gt; /u/Weekly_Signature_510 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1orglym/improving_accuracy_when_extracting_structured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1orglym/improving_accuracy_when_extracting_structured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1orglym/improving_accuracy_when_extracting_structured/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-08T04:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqxqvx</id>
    <title>POC: Model Context Protocol integration for native Ollama app</title>
    <updated>2025-11-07T15:41:54+00:00</updated>
    <author>
      <name>/u/Plenty_Seesaw8878</name>
      <uri>https://old.reddit.com/user/Plenty_Seesaw8878</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oqxqvx/poc_model_context_protocol_integration_for_native/"&gt; &lt;img alt="POC: Model Context Protocol integration for native Ollama app" src="https://preview.redd.it/5bp232couuzf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=d5274f89b68b48ec4343e680e68088f0703597fe" title="POC: Model Context Protocol integration for native Ollama app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;I built a small poc that lets the native ollama app connect to external tools and data sources through the Model Context Protocol.&lt;/p&gt; &lt;p&gt;Made it for personal use and wanted to check if the community would value this before I open a PR.&lt;/p&gt; &lt;p&gt;It’s based on Anthropic’s Go SDK and integrates into the app lifecycle.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plenty_Seesaw8878"&gt; /u/Plenty_Seesaw8878 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5bp232couuzf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oqxqvx/poc_model_context_protocol_integration_for_native/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oqxqvx/poc_model_context_protocol_integration_for_native/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-07T15:41:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1os3aou</id>
    <title>chaTTY - A fast AI chat for the terminal</title>
    <updated>2025-11-08T23:04:27+00:00</updated>
    <author>
      <name>/u/kekePower</name>
      <uri>https://old.reddit.com/user/kekePower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I just pushed a few updates to chaTTY to git. Added Sqlite3 on the backend to save chats that can be loaded in later. Also added liner so that you can use the left and right arrow keys to go back and forth to edit the text instead of having to delete everything as it was before.&lt;/p&gt; &lt;p&gt;Works with the Ollama OpenAI-compatible API.&lt;/p&gt; &lt;p&gt;Check it out at &lt;a href="https://labs.promptshield.io/experiments/chatty"&gt;https://labs.promptshield.io/experiments/chatty&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MIT License.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kekePower"&gt; /u/kekePower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1os3aou/chatty_a_fast_ai_chat_for_the_terminal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1os3aou/chatty_a_fast_ai_chat_for_the_terminal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1os3aou/chatty_a_fast_ai_chat_for_the_terminal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-08T23:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1os2jgk</id>
    <title>Enabling web search in a modelfile</title>
    <updated>2025-11-08T22:31:29+00:00</updated>
    <author>
      <name>/u/AdministrativeBlock0</name>
      <uri>https://old.reddit.com/user/AdministrativeBlock0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I use gpt-oss:20b in the GUI I can enable thinking and web search and everything works great. But if I make a modelfile with FROM gpt-oss:20b I don't have those options. Is there something I need to enable or a parameter I have to define in the modelfile? I can't see anything in the docs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdministrativeBlock0"&gt; /u/AdministrativeBlock0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1os2jgk/enabling_web_search_in_a_modelfile/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1os2jgk/enabling_web_search_in_a_modelfile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1os2jgk/enabling_web_search_in_a_modelfile/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-08T22:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oso3jb</id>
    <title>We made a multi-agent framework . Here’s the demo. Break it harder.</title>
    <updated>2025-11-09T16:45:55+00:00</updated>
    <author>
      <name>/u/wikkid_lizard</name>
      <uri>https://old.reddit.com/user/wikkid_lizard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oso3jb/we_made_a_multiagent_framework_heres_the_demo/"&gt; &lt;img alt="We made a multi-agent framework . Here’s the demo. Break it harder." src="https://external-preview.redd.it/4BEVVDWWk0p8l-rmKtxNg3NJqXG3x5Xaq8LtHSZqFmg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3a4d662253c1885f12d0b7f2ea6294a2977e1e3" title="We made a multi-agent framework . Here’s the demo. Break it harder." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since we dropped Laddr about a week ago, a bunch of people on our last post said “cool idea, but show it actually working.”&lt;br /&gt; So we put together a short demo of how to get started with Laddr.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo video:&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=ISeaVNfH4aM"&gt;https://www.youtube.com/watch?v=ISeaVNfH4aM&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/AgnetLabs/laddr"&gt;https://github.com/AgnetLabs/laddr&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Docs:&lt;/strong&gt; &lt;a href="https://laddr.agnetlabs.com"&gt;https://laddr.agnetlabs.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to try weird workflows, force edge cases, or just totally break the orchestration logic.&lt;br /&gt; We’re actively improving based on what hurts.&lt;/p&gt; &lt;p&gt;Also, tell us what you want to see Laddr do next.&lt;br /&gt; Browser agent? research assistant? something chaotic?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wikkid_lizard"&gt; /u/wikkid_lizard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=ISeaVNfH4aM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oso3jb/we_made_a_multiagent_framework_heres_the_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oso3jb/we_made_a_multiagent_framework_heres_the_demo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-09T16:45:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1osq7gs</id>
    <title>Memory architecture</title>
    <updated>2025-11-09T18:07:49+00:00</updated>
    <author>
      <name>/u/Equivalent-Ad-9798</name>
      <uri>https://old.reddit.com/user/Equivalent-Ad-9798</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. So ive been tinkering with a framework I built called SoulCore to see how far a local LLM can go with real persistence and self modeling. Instead of a stateless chat buffer, SoulCore keeps a structured autobiographical memory. It can recall people or schemas that the model created itself dynamically, through detectors, then reflects on them between sessions and updates its beliefs. The goal is to test whether continuity and reflection can make small local models feel more context aware. &lt;/p&gt; &lt;p&gt;It’s still early dev (lots of logging and clean up right now), but so far it maintains stable identity, recalls past sessions, and shows consistent personality over time. &lt;/p&gt; &lt;p&gt;I’m mainly sharing to compare notes. Has anyone here tried similar memory/ reflection setups for local models? Any big issues you’ve managed to overcome? &lt;/p&gt; &lt;p&gt;Sorry if this isn’t allowed. Oh, and I’ve been using Ollama models. I’ve tested it on a few other models as well but I’m currently using dolphin3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent-Ad-9798"&gt; /u/Equivalent-Ad-9798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1osq7gs/memory_architecture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1osq7gs/memory_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1osq7gs/memory_architecture/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-09T18:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ost8w6</id>
    <title>Ollama + Python project: myguru</title>
    <updated>2025-11-09T20:05:24+00:00</updated>
    <author>
      <name>/u/Germfreekai</name>
      <uri>https://old.reddit.com/user/Germfreekai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;br /&gt; Created the following project: &lt;a href="https://github.com/germfreekai/myguru"&gt;https://github.com/germfreekai/myguru&lt;/a&gt;&lt;/p&gt; &lt;p&gt;myguru aims to help developers work on projects that they are not familiarized with, or not even familiarized with the used language, by providing a guru assistant, which is an expert on any project.&lt;/p&gt; &lt;p&gt;You should be able to ask your guru things such as: &amp;quot;How are files being created?&amp;quot; or &amp;quot;Where is the request to this api done?&amp;quot;, etc.&lt;/p&gt; &lt;p&gt;It works integrating ollama and chromedb, 100% python.&lt;/p&gt; &lt;p&gt;Lmk any feedback, and if anyone finds it useful, I would be glad!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Germfreekai"&gt; /u/Germfreekai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ost8w6/ollama_python_project_myguru/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ost8w6/ollama_python_project_myguru/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ost8w6/ollama_python_project_myguru/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-09T20:05:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1osqpvq</id>
    <title>OpenwebUI from other PC</title>
    <updated>2025-11-09T18:27:46+00:00</updated>
    <author>
      <name>/u/paradoxunlimited2022</name>
      <uri>https://old.reddit.com/user/paradoxunlimited2022</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have ollama and openwebUI running in my localhost port 8080. using mostly mixtral amd codellema model. Tried to connect my PC running AI model from other PC in same network using the http:&amp;lt;ip&amp;gt;.8080; doesnt work. Any idea how can I achieve this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paradoxunlimited2022"&gt; /u/paradoxunlimited2022 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1osqpvq/openwebui_from_other_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1osqpvq/openwebui_from_other_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1osqpvq/openwebui_from_other_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-09T18:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1osrrxi</id>
    <title>CPU on self host ollama 1000%</title>
    <updated>2025-11-09T19:08:45+00:00</updated>
    <author>
      <name>/u/Super-Professor519</name>
      <uri>https://old.reddit.com/user/Super-Professor519</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I host a ollama app with gemma3:4b model in a server with 16gb ram. I use caddy as reserve proxy to the ollama port. What I send a request it takes 20+ seconds to respond. &lt;/p&gt; &lt;p&gt;Note I use the /chat endpoint with 2 messages one for system and one for user.&lt;/p&gt; &lt;p&gt;I set the OLLAMA_KEEP_ALLIVE to 86400 so it never sleeps.&lt;/p&gt; &lt;p&gt;How can I speed up the respond time? Any idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Super-Professor519"&gt; /u/Super-Professor519 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1osrrxi/cpu_on_self_host_ollama_1000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1osrrxi/cpu_on_self_host_ollama_1000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1osrrxi/cpu_on_self_host_ollama_1000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-09T19:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ospgkw</id>
    <title>GPT 5 for Computer Use agents</title>
    <updated>2025-11-09T17:39:02+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ospgkw/gpt_5_for_computer_use_agents/"&gt; &lt;img alt="GPT 5 for Computer Use agents" src="https://external-preview.redd.it/OHA3ZWVjMW5wOTBnMRG6-oWujtVtwWnIPYQYAphLJPDZc9z94p-KY-4O-UR8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff9163fdb86010d07646c010c115ee643423195b" title="GPT 5 for Computer Use agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same tasks, same grounding model we just swapped GPT 4o with GPT 5 as the thinking model.&lt;/p&gt; &lt;p&gt;Left = 4o, right = 5.&lt;/p&gt; &lt;p&gt;Watch GPT 5 pull through.&lt;/p&gt; &lt;p&gt;Grounding model: Salesforce GTA1-7B&lt;/p&gt; &lt;p&gt;Action space: CUA Cloud Instances (macOS/Linux/Windows)&lt;/p&gt; &lt;p&gt;The task is: &amp;quot;Navigate to {random_url} and play the game until you reach a score of 5/5”....each task is set up by having claude generate a random app from a predefined list of prompts (multiple choice trivia, form filling, or color matching)&amp;quot;&lt;/p&gt; &lt;p&gt;Try it yourself here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs : &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agent"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agent&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.gg/cua-ai"&gt;https://discord.gg/cua-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ojia28enp90g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ospgkw/gpt_5_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ospgkw/gpt_5_for_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-09T17:39:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ota5ou</id>
    <title>GLM-4.6-REAP any good for coding? Min VRAM+RAM?</title>
    <updated>2025-11-10T10:17:23+00:00</updated>
    <author>
      <name>/u/WaitformeBumblebee</name>
      <uri>https://old.reddit.com/user/WaitformeBumblebee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using mostly QWEN3 variants (&amp;lt;20GB) for python coding tasks. Would 16GB VRAM + 64GB RAM be able to &amp;quot;run&amp;quot; (I don't mind waiting some minutes if the answer is much better) 72GB model like &lt;a href="https://ollama.com/MichelRosselli/GLM-4.6-REAP-218B-A32B-FP8-mixed-AutoRound"&gt;https://ollama.com/MichelRosselli/GLM-4.6-REAP-218B-A32B-FP8-mixed-AutoRound&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and how good is it? Been hearing high praise for GLM-4.5-AIR, but don't want to download &amp;gt;70GB for nothing. Perhaps I'd be better of with GLM-4.5-Air:Q2_K at 45GB ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WaitformeBumblebee"&gt; /u/WaitformeBumblebee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ota5ou/glm46reap_any_good_for_coding_min_vramram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ota5ou/glm46reap_any_good_for_coding_min_vramram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ota5ou/glm46reap_any_good_for_coding_min_vramram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T10:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot3pp8</id>
    <title>Granite 4 micro-h doing great on my older pc</title>
    <updated>2025-11-10T03:47:18+00:00</updated>
    <author>
      <name>/u/newz2000</name>
      <uri>https://old.reddit.com/user/newz2000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"&gt; &lt;img alt="Granite 4 micro-h doing great on my older pc" src="https://a.thumbs.redditmedia.com/bbtxjoQlGAnti-gN1X_64JqMNQSAnCRZif4YFLrMqj0.jpg" title="Granite 4 micro-h doing great on my older pc" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/nars9rownc0g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2d27ce0136bd0fc08a26e32900ff809de4c39dd"&gt;https://preview.redd.it/nars9rownc0g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2d27ce0136bd0fc08a26e32900ff809de4c39dd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My older 7th Gen i5 gaming computer has been repurposed into my local llm workhorse for most of this year. I use it to automate tasks. In this example, extract key dates and information from an email producing the results in JSON format. &lt;/p&gt; &lt;p&gt;I have been using Qwen 3 and Gemma 3 and I'd say if I want to have a conversation, Qwen 3:8b is my favorite. But it's not good at instruction following. Gemma 3:4b really does great all around and is very quick on this computer. But for instruction following, Granite 4 micro-h is tough to beat.&lt;/p&gt; &lt;p&gt;I have not yet tested it with tool calling, but this is something I want to do and is what made me check out Granite.&lt;/p&gt; &lt;p&gt;Since you can kinda see my prompt through the translucent window, I'll save you the effort and put it in here. &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;You are an assistant that extracts litigation-relevant structured data from incoming court notices that arrive via email or plaintext.&lt;/p&gt; &lt;p&gt;Read the following email or document VERY CAREFULLY.&lt;/p&gt; &lt;p&gt;Then output ONLY JSON.&lt;/p&gt; &lt;p&gt;Do not summarize.&lt;/p&gt; &lt;p&gt;Do not infer beyond what is explicitly written.&lt;/p&gt; &lt;p&gt;If a field cannot be determined, return null — do NOT guess.&lt;/p&gt; &lt;p&gt;You must return ONLY this exact JSON structure (no explanation):&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;case_name&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;case_number&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;court&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;hearing_date&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;hearing_time&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;presiding_judge&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;filing_date_of_order&amp;quot;: &amp;quot;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;required_filing_deadlines&amp;quot;: [],&lt;/p&gt; &lt;p&gt;&amp;quot;parties_involved&amp;quot;: [],&lt;/p&gt; &lt;p&gt;&amp;quot;topic_or_subject_matter&amp;quot;: &amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;Return ONLY a JSON object with EXACTLY these keys:&lt;/p&gt; &lt;p&gt;case_name, case_number, court, hearing_date, hearing_time, presiding_judge,&lt;/p&gt; &lt;p&gt;filing_date_of_order, required_filing_deadlines, parties_involved, topic_or_subject_matter.&lt;/p&gt; &lt;p&gt;If a value is unknown, set null. Do NOT add any other keys or sections.&lt;/p&gt; &lt;p&gt;Dates = YYYY-MM-DD. Times = 24-hour local-to-court (e.g., 13:30).&lt;/p&gt; &lt;p&gt;Include ONLY human names in `parties_involved` (no emails). Remove HTML entities.&lt;/p&gt; &lt;p&gt;Rules:&lt;/p&gt; &lt;p&gt;hearing_date and hearing_time must be extracted if a hearing is set.&lt;/p&gt; &lt;p&gt;required_filing_deadlines must list ONLY dates that represent “something is due” by “a date certain.”&lt;/p&gt; &lt;p&gt;parties_involved should list all names referenced as parties, attorneys, or counsel receiving service.&lt;/p&gt; &lt;p&gt;The topic_or_subject_matter is a single short clause describing WHAT the order is about (motion type, hearing type, etc).&lt;/p&gt; &lt;p&gt;Dates must be formatted YYYY-MM-DD.&lt;/p&gt; &lt;p&gt;Times must be formatted 24 hour format, local to the court when stated (CST → convert to 24h).&lt;/p&gt; &lt;p&gt;DO NOT return anything not inside the JSON block.&lt;/p&gt; &lt;p&gt;EMAIL:&lt;/p&gt; &lt;p&gt;…&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newz2000"&gt; /u/newz2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ot3pp8/granite_4_microh_doing_great_on_my_older_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T03:47:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot3u7f</id>
    <title>Speculative decoding: Faster inference for local LLMs over the network?</title>
    <updated>2025-11-10T03:53:54+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ot3u7f/speculative_decoding_faster_inference_for_local/"&gt; &lt;img alt="Speculative decoding: Faster inference for local LLMs over the network?" src="https://preview.redd.it/70p6li1poc0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6daaaeff166a74d20a883ce88ad7a5a9b3feaf6" title="Speculative decoding: Faster inference for local LLMs over the network?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am gearing up for a big release to add support for speculative decoding for LLMs and looking for early feedback.&lt;/p&gt; &lt;p&gt;First a bit of context, speculative decoding is a technique whereby a draft model (usually a smaller LLM) is engaged to produce tokens and the candidate set produced is verified by a target model (usually a larger model). The set of candidate tokens produced by a draft model must be verifiable via logits by the target model. While tokens produced are serial, verification can happen in parallel which can lead to significant improvements in speed.&lt;/p&gt; &lt;p&gt;This is what OpenAI uses to accelerate the speed of its responses especially in cases where outputs can be guaranteed to come from the same distribution, where:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;propose(x, k) → τ # Draft model proposes k tokens based on context x verify(x, τ) → m # Target verifies τ, returns accepted count m continue_from(x) # If diverged, resume from x with target model &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So I am thinking of adding support to &lt;a href="https://github.com/katanemo/archgw"&gt;arch&lt;/a&gt; (a models-native sidecar proxy for agents). And the developer experience could be something along the following lines:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;POST /v1/chat/completions { &amp;quot;model&amp;quot;: &amp;quot;target:gpt-large@2025-06&amp;quot;, &amp;quot;speculative&amp;quot;: { &amp;quot;draft_model&amp;quot;: &amp;quot;draft:small@v3&amp;quot;, &amp;quot;max_draft_window&amp;quot;: 8, &amp;quot;min_accept_run&amp;quot;: 2, &amp;quot;verify_logprobs&amp;quot;: false }, &amp;quot;messages&amp;quot;: [...], &amp;quot;stream&amp;quot;: true } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here the max_draft_window is the number of tokens to verify, the max_accept_run tells us after how many failed verifications should we give up and just send all the remaining traffic to the target model etc. Of course this work assumes a low RTT between the target and draft model so that speculative decoding is faster without compromising quality.&lt;/p&gt; &lt;p&gt;Question: how would you feel about this functionality? Could you see it being useful for your LLM-based applications? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/70p6li1poc0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ot3u7f/speculative_decoding_faster_inference_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ot3u7f/speculative_decoding_faster_inference_for_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-10T03:53:54+00:00</published>
  </entry>
</feed>
