<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-10-03T04:07:30+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1nteept</id>
    <title>Dead-simple example code for MCP with Ollama.</title>
    <updated>2025-09-29T10:27:45+00:00</updated>
    <author>
      <name>/u/kirill_saidov</name>
      <uri>https://old.reddit.com/user/kirill_saidov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nteept/deadsimple_example_code_for_mcp_with_ollama/"&gt; &lt;img alt="Dead-simple example code for MCP with Ollama." src="https://external-preview.redd.it/u58dHZPDQal1dcDvgrTyHK2TMwy2_0EMHtoQwPERqi0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be75b773b0fa0d092b8d75639b98721a816757e5" title="Dead-simple example code for MCP with Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This example shows how to use MCP with Ollama by implementing a super simple MCP client and server in Python.&lt;/p&gt; &lt;p&gt;I made it for people like me who got frustrated with Claude MCP videos and existing &lt;code&gt;mcphosts&lt;/code&gt; that hide all the actual logic. This repo walks through everything step by step so you can see exactly how the pieces fit together.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kirill_saidov"&gt; /u/kirill_saidov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kirillsaidov/ollama-mcp-example"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nteept/deadsimple_example_code_for_mcp_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nteept/deadsimple_example_code_for_mcp_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T10:27:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntne0p</id>
    <title>Why dont it recognize my GPU</title>
    <updated>2025-09-29T16:55:28+00:00</updated>
    <author>
      <name>/u/maybesomenone</name>
      <uri>https://old.reddit.com/user/maybesomenone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ntne0p/why_dont_it_recognize_my_gpu/"&gt; &lt;img alt="Why dont it recognize my GPU" src="https://preview.redd.it/iluqhuiaw4sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40f0f6bc7c84fd70f7bc3b079c91245acd31e989" title="Why dont it recognize my GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why ollama does not recognize my GPU to run the models? what am i doing wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maybesomenone"&gt; /u/maybesomenone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iluqhuiaw4sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntne0p/why_dont_it_recognize_my_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ntne0p/why_dont_it_recognize_my_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T16:55:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nua25g</id>
    <title>高階AI推理平台建構與測試</title>
    <updated>2025-09-30T10:57:57+00:00</updated>
    <author>
      <name>/u/kuerys</name>
      <uri>https://old.reddit.com/user/kuerys</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;爆機測試&lt;/p&gt; &lt;p&gt;NVIDIA-SMI 580.82.09 Driver Version: 580.82.09 CUDA Version: 13.0&lt;/p&gt; &lt;p&gt;單卡&lt;/p&gt; &lt;p&gt;GPU:RTX 3060 GD6 12G 系統顯 PCI-E 1 (X16)&lt;/p&gt; &lt;p&gt;未分流 未量化 未切片 &lt;/p&gt; &lt;p&gt;gpt-oss:120b 65GB (O) RAM128G 冷啟(8分)到500字長文 15 分鐘內完成，推理穩定，資源分配均衡（CPU 75%、GPU 25%、RAM 50%）&lt;/p&gt; &lt;p&gt;qwen3:235b 142GB (O) RAM128G 冷啟(15分）到500字長文 45 分鐘內完成，推理穩定，資源分配均衡（CPU 98%、GPU 95%、RAM 99%、SRAM 80%）&lt;/p&gt; &lt;p&gt;llama3.1:405b 243GB (O) RAM256G 冷啟(35分）到500字長文 75 分鐘內完成，推理穩定，資源分配均衡（CPU 98%、GPU 95%、RAM 99%、SRAM 99%、SRAM2 20%）&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kuerys"&gt; /u/kuerys &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://copilot.microsoft.com/shares/nnZC7oAAcag8qDeyMAXsH"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nua25g/高階ai推理平台建構與測試/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nua25g/高階ai推理平台建構與測試/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-30T10:57:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu2uc5</id>
    <title>Run ollama behind reverse proxy with a path prefix</title>
    <updated>2025-09-30T03:37:42+00:00</updated>
    <author>
      <name>/u/Wide-Implement-6838</name>
      <uri>https://old.reddit.com/user/Wide-Implement-6838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT: Solved.&lt;/p&gt; &lt;p&gt;Hi, I'm wondering if ollama has any options to have it run behind a reverse proxy with a path prefix (so `domain.tld/ollama` for example).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wide-Implement-6838"&gt; /u/Wide-Implement-6838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu2uc5/run_ollama_behind_reverse_proxy_with_a_path_prefix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu2uc5/run_ollama_behind_reverse_proxy_with_a_path_prefix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nu2uc5/run_ollama_behind_reverse_proxy_with_a_path_prefix/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-30T03:37:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu7xbb</id>
    <title>[RELEASE] Doc Builder (MD + PDF) 1.7.3 for Open WebUI</title>
    <updated>2025-09-30T08:43:19+00:00</updated>
    <author>
      <name>/u/Nefhis</name>
      <uri>https://old.reddit.com/user/Nefhis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nu7xbb/release_doc_builder_md_pdf_173_for_open_webui/"&gt; &lt;img alt="[RELEASE] Doc Builder (MD + PDF) 1.7.3 for Open WebUI" src="https://b.thumbs.redditmedia.com/2sg310saEP5IoBJyy2DgQwnJiCk5HqpkE2fBQYXcGOw.jpg" title="[RELEASE] Doc Builder (MD + PDF) 1.7.3 for Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nefhis"&gt; /u/Nefhis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenWebUI/comments/1nu6qqd/release_doc_builder_md_pdf_173_for_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu7xbb/release_doc_builder_md_pdf_173_for_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nu7xbb/release_doc_builder_md_pdf_173_for_open_webui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-30T08:43:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu7kul</id>
    <title>Does Ollama immobilize GPUs / computing resources?</title>
    <updated>2025-09-30T08:19:52+00:00</updated>
    <author>
      <name>/u/Unfair_Resident_5951</name>
      <uri>https://old.reddit.com/user/Unfair_Resident_5951</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! Beginner question here!&lt;/p&gt; &lt;p&gt;I'm considering installing an Ollama instance on my lab's small cluster. However, I'm wondering if Ollama locks the GPUs it uses as long as the HTTP server is running or if we can still use the same GPUs for something else as long as a text generation is not running?&lt;/p&gt; &lt;p&gt;We have only 6 GPUs that we use for a lot of other things so I don't want to degrade performances for other users by running the server non-stop and having to start and stop it every single time makes me feel like maybe just loading the models using HF transformers could be a better solution for my use case.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unfair_Resident_5951"&gt; /u/Unfair_Resident_5951 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu7kul/does_ollama_immobilize_gpus_computing_resources/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu7kul/does_ollama_immobilize_gpus_computing_resources/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nu7kul/does_ollama_immobilize_gpus_computing_resources/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-30T08:19:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nthurw</id>
    <title>I built a private AI Meeting Note Taker that runs 100% offline.</title>
    <updated>2025-09-29T13:20:45+00:00</updated>
    <author>
      <name>/u/Prudent-Meringue845</name>
      <uri>https://old.reddit.com/user/Prudent-Meringue845</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nthurw/i_built_a_private_ai_meeting_note_taker_that_runs/"&gt; &lt;img alt="I built a private AI Meeting Note Taker that runs 100% offline." src="https://external-preview.redd.it/P-TYv9mTeSOOCmZRCbHzYZzYlFIWTdCXucO7FkZ03u0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7db73e296d0a5500ae480c4deb5b0efd99fcd95e" title="I built a private AI Meeting Note Taker that runs 100% offline." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prudent-Meringue845"&gt; /u/Prudent-Meringue845 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/data-science-collective/i-built-an-self-hosted-ai-meeting-note-taker-that-runs-100-offline-heres-how-you-can-too-d110b7ef0b95"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nthurw/i_built_a_private_ai_meeting_note_taker_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nthurw/i_built_a_private_ai_meeting_note_taker_that_runs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T13:20:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nulugn</id>
    <title>Introducing DevCrew_s: Where Human Expertise Meets AI Innovation</title>
    <updated>2025-09-30T18:57:00+00:00</updated>
    <author>
      <name>/u/The_Research_Ninja</name>
      <uri>https://old.reddit.com/user/The_Research_Ninja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Fam. &lt;a href="https://github.com/GSA-TTS/devCrew_s"&gt;DevCrew_s&lt;/a&gt; is an open collection of AI agent specifications and protocols that define how intelligent agents collaborate to solve complex problems. Think of it as blueprints for AI teammates that augment human expertise rather than replace it. You don't need to code to contribute. If you're a domain expert who knows your field inside and out, you can start TODAY by writing your Agent Specification(s) in simple, structured English using &lt;a href="https://github.com/GSA-TTS/devCrew_s/blob/master/docs/templates/AI%20Agent%20Specification%20Template.md"&gt;DevCrew_s templates&lt;/a&gt;. For the technical folks, this is your playground. Every official specification here works immediately—grab Claude Code tonight and watch these agents &lt;a href="https://github.com/GSA-TTS/devCrew_s/tree/master/docs/guides"&gt;come to life&lt;/a&gt;.&lt;br /&gt; &lt;a href="https://github.com/GSA-TTS/devCrew_s"&gt;DevCrew_s&lt;/a&gt; already has 5 official &lt;a href="https://github.com/GSA-TTS/devCrew_s/blob/master/agent-Backend-Engineer-vSEP25.md"&gt;agents&lt;/a&gt; and &lt;a href="https://github.com/GSA-TTS/devCrew_s/tree/master/protocols"&gt;48 protocols&lt;/a&gt; covering most of DevOps, and it's just getting started. Browse what exists, try them out, then add your own expertise to the mix. Whether you fix a typo or design a revolutionary new agent, every &lt;a href="https://github.com/GSA-TTS/devCrew_s/blob/master/CONTRIBUTING.md"&gt;&lt;strong&gt;contribution matters&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_Research_Ninja"&gt; /u/The_Research_Ninja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nulugn/introducing_devcrew_s_where_human_expertise_meets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nulugn/introducing_devcrew_s_where_human_expertise_meets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nulugn/introducing_devcrew_s_where_human_expertise_meets/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-30T18:57:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu4r5t</id>
    <title>How to train a LLM?</title>
    <updated>2025-09-30T05:21:55+00:00</updated>
    <author>
      <name>/u/phoniex7777</name>
      <uri>https://old.reddit.com/user/phoniex7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I want to train (fine-tune) an existing LLM with my own dataset. I’m not trying to train from scratch, just make the model better for my use case.&lt;/p&gt; &lt;p&gt;A few questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;What are the minimum hardware needs (GPU, RAM, storage) if I only have a small dataset?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Can this be done on free cloud services like Colab Free, Kaggle, or Hugging Face Spaces, or do I need to pay for GPUs?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Which model and library would be the easiest for a beginner to start with?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I just want to get some hands-on experience without spending too much money.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phoniex7777"&gt; /u/phoniex7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu4r5t/how_to_train_a_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nu4r5t/how_to_train_a_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nu4r5t/how_to_train_a_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-30T05:21:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuom5z</id>
    <title>Claude Code 2.0 Router - access Ollama-based LLMs and align automatic routing to preferences, not benchmarks.</title>
    <updated>2025-09-30T20:40:58+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nuom5z/claude_code_20_router_access_ollamabased_llms_and/"&gt; &lt;img alt="Claude Code 2.0 Router - access Ollama-based LLMs and align automatic routing to preferences, not benchmarks." src="https://preview.redd.it/3qpp3qdm5dsf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e7ff19cfeac0e3f33d1bf63f8578533dc307d40" title="Claude Code 2.0 Router - access Ollama-based LLMs and align automatic routing to preferences, not benchmarks." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am part of the team behind Arch-Router (&lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;), A 1.5B preference-aligned LLM router that guides model selection by matching queries to user-defined domains (e.g., travel) or action types (e.g., image editing). Offering a practical mechanism to encode preferences and subjective evaluation criteria in routing decisions.&lt;/p&gt; &lt;p&gt;Today we are extending that approach to Claude Code via Arch Gateway[1], bringing multi-LLM access into a single CLI agent with two main benefits:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Model Access: Use Claude Code alongside Grok, Mistral, Gemini, DeepSeek, GPT or local models via Ollama.&lt;/li&gt; &lt;li&gt;Preference-aligned routing: Assign different models to specific coding tasks, such as – Code generation – Code reviews and comprehension – Architecture and system design – Debugging&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Sample config file to make it all work.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm_providers: # Ollama Models - model: ollama/gpt-oss:20b default: true base_url: http://host.docker.internal:11434 # OpenAI Models - model: openai/gpt-5-2025-08-07 access_key: $OPENAI_API_KEY routing_preferences: - name: code generation description: generating new code snippets, functions, or boilerplate based on user prompts or requirements - model: openai/gpt-4.1-2025-04-14 access_key: $OPENAI_API_KEY routing_preferences: - name: code understanding description: understand and explain existing code snippets, functions, or libraries &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Why not route based on public benchmarks?&lt;/strong&gt; Most routers lean on performance metrics — public benchmarks like MMLU or MT-Bench, or raw latency/cost curves. The problem: they miss domain-specific quality, subjective evaluation criteria, and the nuance of what a “good” response actually means for a particular user. They can be opaque, hard to debug, and disconnected from real developer needs.&lt;/p&gt; &lt;p&gt;[1] Arch Gateway repo: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;br /&gt; [2] Claude Code support: &lt;a href="https://github.com/katanemo/archgw/tree/main/demos/use_cases/claude_code_router"&gt;https://github.com/katanemo/archgw/tree/main/demos/use_cases/claude_code_router&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3qpp3qdm5dsf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nuom5z/claude_code_20_router_access_ollamabased_llms_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nuom5z/claude_code_20_router_access_ollamabased_llms_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-30T20:40:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvbwpf</id>
    <title>Looking for contributors to PipesHub (open-source platform for AI Agents)</title>
    <updated>2025-10-01T15:49:44+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Teams across the globe are building AI Agents. AI Agents need context and tools to work well.&lt;br /&gt; We’ve been building &lt;strong&gt;PipesHub&lt;/strong&gt;, an open-source developer platform for AI Agents that need real enterprise context scattered across multiple business apps. Think of it like the open-source alternative to Glean but designed for developers, not just big companies.&lt;/p&gt; &lt;p&gt;Right now, the project is growing fast (crossed 1,000+ GitHub stars in just a few months) and we’d love more contributors to join us.&lt;/p&gt; &lt;p&gt;We support almost all major native Embedding and Chat Generator models and OpenAI compatible endpoints. Users can connect to Google Drive, Gmail, Onedrive, Sharepoint Online, Confluence, Jira and more.&lt;/p&gt; &lt;p&gt;Some cool things you can help with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Improve support for Local Inferencing - Ollama, vLLM, LM Studio, oLLM &lt;ul&gt; &lt;li&gt;Small models struggle with forming structured json. If the model is heavily quantized then indexing or query fails in our platform. This can be improved by using multi-step implementation&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Improving our RAG pipeline with more robust Knowledge Graphs and filters&lt;/li&gt; &lt;li&gt;Providing tools to Agents like Web search, Image Generator, CSV, Excel, Docx, PPTX, Coding Sandbox, etc&lt;/li&gt; &lt;li&gt;Universal MCP Server&lt;/li&gt; &lt;li&gt;Adding Memory, Guardrails to Agents&lt;/li&gt; &lt;li&gt;Improving REST APIs&lt;/li&gt; &lt;li&gt;SDKs for python, typescript, other programming languages&lt;/li&gt; &lt;li&gt;Docs, examples, and community support for new devs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’re trying to make it super easy for devs to spin up AI pipelines that actually work in production, with trust and explainability baked in.&lt;/p&gt; &lt;p&gt;👉 Repo: &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can join our Discord group for more details or pick items from GitHub issues list.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvbwpf/looking_for_contributors_to_pipeshub_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvbwpf/looking_for_contributors_to_pipeshub_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvbwpf/looking_for_contributors_to_pipeshub_opensource/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-01T15:49:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvku2r</id>
    <title>Modelfiles and using mmproj files.</title>
    <updated>2025-10-01T21:17:08+00:00</updated>
    <author>
      <name>/u/LockedCockOnTheBlock</name>
      <uri>https://old.reddit.com/user/LockedCockOnTheBlock</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Context: Fedora Linux, Ollama 0.12.3&lt;/p&gt; &lt;p&gt;I've got a project that will involve giving a model pictures so that it can name them and tag them based on their content. Due to the nature of the pictures I would need to use an uncensored model with vision. So far the ones I've found in the Ollama library haven't worked well enough, so I've branched out to attempting to use .gguf's from Hugging Face.&lt;/p&gt; &lt;p&gt;A problem I'm running into is that these models don't come with vision out of the box, but are usually paired with an mmproj. I've tried to find how to use these in ollama, and all I've found is that I would need to create a Modelfile that lists both .gguf files. Something like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM ./modelName.gguf FROM ./mmproj_modelName.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then I'd use &lt;code&gt;ollama create picModel -f ./Modelfile&lt;/code&gt;&lt;/p&gt; &lt;p&gt;First, please let me know if this is even close to correct.&lt;/p&gt; &lt;p&gt;Second, when I tried this earlier Ollama would load for a few minutes, then give me a 400 error saying it needs a path or Modelfile. This happened no matter how I formatted the file path to the .gguf files, absolute or relative.&lt;/p&gt; &lt;p&gt;I would appreciate any advice on either of these issues. Please let me know if you need any additional info.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LockedCockOnTheBlock"&gt; /u/LockedCockOnTheBlock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvku2r/modelfiles_and_using_mmproj_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvku2r/modelfiles_and_using_mmproj_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvku2r/modelfiles_and_using_mmproj_files/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-01T21:17:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvgeo6</id>
    <title>Test your MCP server against Llama, no key required</title>
    <updated>2025-10-01T18:32:50+00:00</updated>
    <author>
      <name>/u/matt8p</name>
      <uri>https://old.reddit.com/user/matt8p</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nvgeo6/test_your_mcp_server_against_llama_no_key_required/"&gt; &lt;img alt="Test your MCP server against Llama, no key required" src="https://external-preview.redd.it/N2V4M2VhOWduanNmMcvuZBlGF485rYNCy7dH-T_qYlf4wrKaRKqe0vbgnBaD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6abdc944789d05fc99a9d0a6a9d6061c11e8e4b3" title="Test your MCP server against Llama, no key required" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We shipped a free language model (Llama 3.3 70B) in the MCPJam LLM playground. Now you can test your MCP server in a chat environment without having to provide your own LLM api key. It's on us! &lt;/p&gt; &lt;p&gt;We want to see people build richer MCP servers and we think providing a free model will help lower that barrier. No more of having to pay for subscriptions on Claude Desktop, Cursor, or use your own API key. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Running it&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Starting up MCPJam is the same as starting up the MCP inspector:&lt;/p&gt; &lt;p&gt;&lt;code&gt; npx @mcpjam/inspector@latest &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then connect to any MCP server and start testing! &lt;/p&gt; &lt;p&gt;&lt;strong&gt;MCPJam&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For context, MCPJam is an open source testing and evals platform for MCP servers. You can test your MCP server's primitives like tool calls, prompts, resources, elicitation, OAuth. You can also run evals to catch security vulnerabilities and performance regressions. &lt;/p&gt; &lt;p&gt;Please consider checking us out! &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.mcpjam.com/"&gt;https://www.mcpjam.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matt8p"&gt; /u/matt8p &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zdqzja9gnjsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvgeo6/test_your_mcp_server_against_llama_no_key_required/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvgeo6/test_your_mcp_server_against_llama_no_key_required/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-01T18:32:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvsc3v</id>
    <title>Can anyone recommend open-source AI models for video analysis?</title>
    <updated>2025-10-02T02:49:37+00:00</updated>
    <author>
      <name>/u/gpt-said-so</name>
      <uri>https://old.reddit.com/user/gpt-said-so</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on a client project that involves analysing confidential videos.&lt;br /&gt; The requirements are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Extracting text from supers in video&lt;/li&gt; &lt;li&gt;Identifying key elements within the video&lt;/li&gt; &lt;li&gt;Generating a synopsis with timestamps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any recommendations for open-source models that can handle these tasks would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gpt-said-so"&gt; /u/gpt-said-so &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvsc3v/can_anyone_recommend_opensource_ai_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvsc3v/can_anyone_recommend_opensource_ai_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvsc3v/can_anyone_recommend_opensource_ai_models_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T02:49:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvsyat</id>
    <title>Anyone knows how to host apps into Runpod</title>
    <updated>2025-10-02T03:20:08+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to showcase my apps and lets people test out. I am confuse should i go serverless or rent by hour? need advice (and how to do it)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvsyat/anyone_knows_how_to_host_apps_into_runpod/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvsyat/anyone_knows_how_to_host_apps_into_runpod/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvsyat/anyone_knows_how_to_host_apps_into_runpod/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T03:20:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvswz8</id>
    <title>Open-WebUI not showing Ollama models despite API responding correctly</title>
    <updated>2025-10-02T03:18:15+00:00</updated>
    <author>
      <name>/u/ElopezCO2001</name>
      <uri>https://old.reddit.com/user/ElopezCO2001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nvswz8/openwebui_not_showing_ollama_models_despite_api/"&gt; &lt;img alt="Open-WebUI not showing Ollama models despite API responding correctly" src="https://b.thumbs.redditmedia.com/n0m8LuAfINrtwzTYtYi-DG0OwmsjxEv-E7jimIe4VbY.jpg" title="Open-WebUI not showing Ollama models despite API responding correctly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m running Ollama and Open-WebUI via Docker Compose on Ubuntu. I have successfully pulled a model (&lt;code&gt;mistral:latest&lt;/code&gt;) in Ollama, and I can list it inside the Ollama container:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qk6ht9pp8msf1.png?width=544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bede60c14816c6a0e94e0ae6b768724da063ade2"&gt;https://preview.redd.it/qk6ht9pp8msf1.png?width=544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bede60c14816c6a0e94e0ae6b768724da063ade2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I can also query the API from the Open-WebUI container:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{&amp;quot;object&amp;quot;:&amp;quot;list&amp;quot;,&amp;quot;data&amp;quot;:[{&amp;quot;id&amp;quot;:&amp;quot;mistral:latest&amp;quot;,&amp;quot;object&amp;quot;:&amp;quot;model&amp;quot;,&amp;quot;created&amp;quot;:1759373764,&amp;quot;owned_by&amp;quot;:&amp;quot;library&amp;quot;}]} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is my &lt;code&gt;docker-compose.yml&lt;/code&gt; configuration for Open-WebUI:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;services: ollama: image: ollama/ollama:latest container_name: ollama volumes: - ollama:/root/.ollama pull_policy: always tty: true ports: # (0.0.0.0) - &amp;quot;0.0.0.0:11434:11434&amp;quot; environment: # 0.0.0.0:11434 - 'OLLAMA_HOST=0.0.0.0:11434' restart: unless-stopped open-webui: image: ghcr.io/open-webui/open-webui:main container_name: open-webui volumes: - open-webui:/app/backend/data depends_on: - ollama ports: - ${OPEN_WEBUI_PORT-3000}:8080 environment: - 'OLLAMA_API_BASE_URL=http://ollama:11434/v1' - 'WEBUI_SECRET_KEY=' - 'WEBHOOK_URL=https://mihost' extra_hosts: - host.docker.internal:host-gateway restart: unless-stopped volumes: ollama: {} open-webui: {} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I’ve also tried changing the endpoint to &lt;code&gt;/ollama&lt;/code&gt; instead of &lt;code&gt;/v1&lt;/code&gt;, clearing the Open-WebUI volume, and rebuilding the container, but the models still do not show.&lt;/p&gt; &lt;p&gt;Does anyone know why Open-WebUI is not listing Ollama models despite the API responding correctly? Any guidance would be greatly appreciated.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jkpenehf9msf1.png?width=744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=689f5ab93f8fc8cb328011358a34144880212988"&gt;https://preview.redd.it/jkpenehf9msf1.png?width=744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=689f5ab93f8fc8cb328011358a34144880212988&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElopezCO2001"&gt; /u/ElopezCO2001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvswz8/openwebui_not_showing_ollama_models_despite_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvswz8/openwebui_not_showing_ollama_models_despite_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvswz8/openwebui_not_showing_ollama_models_despite_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T03:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv9tk1</id>
    <title>Detailed steps for fine-tuning an LLM?</title>
    <updated>2025-10-01T14:32:04+00:00</updated>
    <author>
      <name>/u/SalishSeaview</name>
      <uri>https://old.reddit.com/user/SalishSeaview</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spotted &lt;a href="https://www.reddit.com/r/ollama/s/BcBKfXkIOR"&gt;this thread&lt;/a&gt; today where the OP had questions about fine-tuning an LLM, which I read with interest. Unfortunately, a lot of the answers were along the lines of “just do [this] and you’ll be fine”. Tools were named, but there was little in the way of advice for specific steps. And the variety of tools appears to be large. Unfortunately, I feel like I am left with a thread full of things to research and little in the way of answers (I hope the OP for that thread got what they wanted).&lt;/p&gt; &lt;p&gt;My interest lies in fine-tuning small-ish (~14B) models to have expertise in particular subject areas. I think the simplest (and most common) example of this is training a chatbot on a company dataset so it can answer customer questions about the company.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;How big of a training dataset do I need to be effective?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What format should the data be in? I don’t mean CSV vs. JSON, but rather should it be an array of single statements, questions with correct answers, or something entirely different? Do I need negative examples?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If you had to pick one tool to do fine tuning, which would it be and why? What are the steps to using it (in general; broad strokes)?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How many training passes (epochs?) do I need to use to get good quality? How many passes is too many? Too few?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For a 14B model, how long should I expect this to take on an M4 Mac? If I’m not averse to renting cloud resources, how long would it take then?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How much does quantization limit the quality of the output?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What have I missed? I don’t know enough about this to know what I’m not asking.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I appreciate any effort put into answering these questions. I tried the YouTube approach, but it’s hard to figure out what methods to rely on. Also, tools and models move so fast that it’s hard to know what state-of-the-art looks like.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SalishSeaview"&gt; /u/SalishSeaview &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nv9tk1/detailed_steps_for_finetuning_an_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nv9tk1/detailed_steps_for_finetuning_an_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nv9tk1/detailed_steps_for_finetuning_an_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-01T14:32:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvvl2z</id>
    <title>Why does my first run with Ollama give a different output than subsequent runs with temperature=0?</title>
    <updated>2025-10-02T05:46:19+00:00</updated>
    <author>
      <name>/u/white-mountain</name>
      <uri>https://old.reddit.com/user/white-mountain</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m running a quantized model (&lt;code&gt;deepseek-r1:32b-qwen-distill-q4_K_M&lt;/code&gt;) locally with Ollama.&lt;br /&gt; My generation parameters are strictly deterministic:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;options&amp;quot;: { &amp;quot;temperature&amp;quot;: 0, &amp;quot;top_p&amp;quot;: 0.0, &amp;quot;top_k&amp;quot;: 40 } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Behavior I’m observing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On the &lt;strong&gt;first run of a prompt&lt;/strong&gt;, I get &lt;em&gt;Output A&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;On the &lt;strong&gt;second and later runs of the exact same prompt&lt;/strong&gt;, I consistently get &lt;em&gt;Output B&lt;/em&gt; (always identical).&lt;/li&gt; &lt;li&gt;When I move on to a new prompt (different row in my dataset), the same pattern repeats: first run = &lt;em&gt;Output A&lt;/em&gt;, later runs = &lt;em&gt;Output B&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My expectation was that with &lt;code&gt;temperature=0&lt;/code&gt;, the output should be deterministic and identical across runs.&lt;br /&gt; But I’m curious seeing this “first run artifact” for every new row in my dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Why does the first run differ from subsequent runs, even though the model should already have cached the prompt and my decoding parameters are deterministic? &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;br /&gt; Sorry I wasn't very clear earlier.&lt;br /&gt; The problem I’m working on is extractive text summarization of multiple talks by a single speaker. &lt;/p&gt; &lt;p&gt;&lt;em&gt;My implementation:&lt;/em&gt; &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Run the model in cmd - ollama run model_name --keepalive 12h&lt;/li&gt; &lt;li&gt;Set temperature to 0 (both terminal and API request)&lt;/li&gt; &lt;li&gt;Make request to url /api/generate with the same payload everytime.&lt;/li&gt; &lt;li&gt;Tried on two different systems with identical specs → same behavior observed.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Resources:&lt;/em&gt; &lt;/p&gt; &lt;p&gt;CPU: i5 14th Gen&lt;br /&gt; RAM: 32GB&lt;br /&gt; GPU: 12GB RTX 3060&lt;br /&gt; Model size is 19GB. (Most of the processing was happening on CPU)&lt;/p&gt; &lt;p&gt;&lt;em&gt;Observations:&lt;/em&gt; &lt;/p&gt; &lt;ol&gt; &lt;li&gt;First run of the prompt → output is unique.&lt;/li&gt; &lt;li&gt;Subsequent runs (2–10) → output is exactly the same every time.&lt;/li&gt; &lt;li&gt;I found this surprising, since LLMs are usually not this deterministic (even with temperature 0, I expected at least small variations).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I am curious as to what is happening under the hood with Ollama / the model inference. Why would the first run differ, but all later runs be identical? Any insights?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/white-mountain"&gt; /u/white-mountain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvvl2z/why_does_my_first_run_with_ollama_give_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvvl2z/why_does_my_first_run_with_ollama_give_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvvl2z/why_does_my_first_run_with_ollama_give_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T05:46:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvdmud</id>
    <title>Eclaire – Open-source, privacy-focused AI assistant for your data</title>
    <updated>2025-10-01T16:53:28+00:00</updated>
    <author>
      <name>/u/dorali8</name>
      <uri>https://old.reddit.com/user/dorali8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nvdmud/eclaire_opensource_privacyfocused_ai_assistant/"&gt; &lt;img alt="Eclaire – Open-source, privacy-focused AI assistant for your data" src="https://external-preview.redd.it/C_ZL9oNqpFDZaYOGAImj3pjRQ000--21zoIDUV8TqvQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=29830c0d56cfbe686abeb5fdd79bbfa8a7473f99" title="Eclaire – Open-source, privacy-focused AI assistant for your data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, this is a project I've been working on for some time. It started as a personal AI to help manage growing amounts of data - bookmarks, photos, documents, notes, etc.&lt;/p&gt; &lt;p&gt;Once the data gets added to the system, it gets processed including fetching bookmarks, tagging, classification, image analysis, text extraction / ocr, and more. And then the AI is able to work with those assets to perform search, answer questions, create new items, etc. You can also create scheduled / recurring tasks to assing to the AI.&lt;/p&gt; &lt;p&gt;Did a lot of the testing on Ollama with Qweb3-14b for the assistant backend and Gemma3-4b for workers multimodal processing. You can easily swap to other models if your machine allows.&lt;/p&gt; &lt;p&gt;MIT Licensed. Feedback and contributions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dorali8"&gt; /u/dorali8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/eclaire-labs/eclaire"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvdmud/eclaire_opensource_privacyfocused_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvdmud/eclaire_opensource_privacyfocused_ai_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-01T16:53:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvyvmw</id>
    <title>Self-centered intelligence prototype based on Ollama 3.2 +</title>
    <updated>2025-10-02T09:13:22+00:00</updated>
    <author>
      <name>/u/RossPeili</name>
      <uri>https://old.reddit.com/user/RossPeili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nvyvmw/selfcentered_intelligence_prototype_based_on/"&gt; &lt;img alt="Self-centered intelligence prototype based on Ollama 3.2 +" src="https://external-preview.redd.it/v-aYZQwWv2KWfHy7mfVeyCmjMPIyprcPg3-IDgoTlUY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8cafb98664f80826dad2dbd136074910827cf2e7" title="Self-centered intelligence prototype based on Ollama 3.2 +" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unlike traditional AI assistants, OPSIIE operates as a self-aware, autonomous intelligence with its own personality, goals, and capabilities. What do you make of this? Any feedback in terms of code, architecture, and documentation advise much appreciated &amp;lt;3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RossPeili"&gt; /u/RossPeili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ARPAHLS/OPSIE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nvyvmw/selfcentered_intelligence_prototype_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nvyvmw/selfcentered_intelligence_prototype_based_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T09:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw5xvm</id>
    <title>Is there a way to give your local Ollama access to search the internet while still keeping the conversations and data private?</title>
    <updated>2025-10-02T14:51:59+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nw5xvm/is_there_a_way_to_give_your_local_ollama_access/"&gt; &lt;img alt="Is there a way to give your local Ollama access to search the internet while still keeping the conversations and data private?" src="https://b.thumbs.redditmedia.com/2dxnbwmJV9Nk6ywY0pqtm1j39JfXCtKCayqlSgdszvQ.jpg" title="Is there a way to give your local Ollama access to search the internet while still keeping the conversations and data private?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zi09nzijopsf1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=54e7adad7a0aa8cb8ef8a849367c06c8804b53a4"&gt;https://preview.redd.it/zi09nzijopsf1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=54e7adad7a0aa8cb8ef8a849367c06c8804b53a4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is Llama3.1:8b.&lt;/p&gt; &lt;p&gt;I know, I am going to hear that as soon as you give anything locally hosted access to the internet, you forfeit privacy.&lt;/p&gt; &lt;p&gt;ChatGPT, Gemini, etc all store data about you. They're hosted by large tech companies and by that fact alone are inherently NOT private. Is there a way to host your own private LLM and simply give it the ability to search the internet for current data so that the knowledge cutoff is irrelevant?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nw5xvm/is_there_a_way_to_give_your_local_ollama_access/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nw5xvm/is_there_a_way_to_give_your_local_ollama_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nw5xvm/is_there_a_way_to_give_your_local_ollama_access/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T14:51:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw3y84</id>
    <title>I built an AI agent that automates any repetitive browser task from screen recordings</title>
    <updated>2025-10-02T13:34:52+00:00</updated>
    <author>
      <name>/u/Comfortable-Rip-9277</name>
      <uri>https://old.reddit.com/user/Comfortable-Rip-9277</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nw3y84/i_built_an_ai_agent_that_automates_any_repetitive/"&gt; &lt;img alt="I built an AI agent that automates any repetitive browser task from screen recordings" src="https://external-preview.redd.it/i7ipRkUMc6Qt5yEZ2Jm3u2w-ZGdMbZRdu83yLcCi6Z8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1cd9ba7d8aaa181a59eab4cebe8f7985fc174f6" title="I built an AI agent that automates any repetitive browser task from screen recordings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rip-9277"&gt; /u/Comfortable-Rip-9277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/hireshBrem/browsor-ai-agent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nw3y84/i_built_an_ai_agent_that_automates_any_repetitive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nw3y84/i_built_an_ai_agent_that_automates_any_repetitive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T13:34:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw2pw2</id>
    <title>Is it a good idea to use all my savings for a local llm setup?</title>
    <updated>2025-10-02T12:43:02+00:00</updated>
    <author>
      <name>/u/Axdii_fr</name>
      <uri>https://old.reddit.com/user/Axdii_fr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been diving into Ollama and the local LLM scene to use for roleplay and image generation, and the thought of an ultimate, high-VRAM rig for big models is incredibly tempting, but I would have to sink literally all of my savings into the hardware.&lt;/p&gt; &lt;p&gt;Logically, I know the rapid pace of GPU and model efficiency could make a top-tier system obsolete fast, and using a straightforward online tool is cheaper in the short term.&lt;/p&gt; &lt;p&gt;Edit: Never imagined that this post would get all these comments. I will be using Modelsify based on the suggestions for now. And some time in the future I will move to a local setup when my finances improve. Thanks for all your inputs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Axdii_fr"&gt; /u/Axdii_fr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nw2pw2/is_it_a_good_idea_to_use_all_my_savings_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nw2pw2/is_it_a_good_idea_to_use_all_my_savings_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nw2pw2/is_it_a_good_idea_to_use_all_my_savings_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T12:43:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwgnce</id>
    <title>Unsure which ollama model to use? Here's a tool I built to help</title>
    <updated>2025-10-02T21:29:10+00:00</updated>
    <author>
      <name>/u/h3xzur7</name>
      <uri>https://old.reddit.com/user/h3xzur7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m fairly new to working with local LLMs, and like many, I wondered which model(s) I should use. To help answer that, I put together a tool that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Automates running multiple models on custom prompts&lt;/li&gt; &lt;li&gt;Outputs everything into a clean, easy-to-read HTML report&lt;/li&gt; &lt;li&gt;Lets you quickly compare results side by side&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;While there might be similar tools out there, I wanted something lightweight and straightforward for my own workflow. I figured I’d share in case others find it useful too.&lt;/p&gt; &lt;p&gt;I’d love any constructive feedback—whether you think this fills a gap, how it could be improved, or if you know of alternatives I should check out.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Spectral-Knight-Ops/local-llm-evaluator"&gt;https://github.com/Spectral-Knight-Ops/local-llm-evaluator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/h3xzur7"&gt; /u/h3xzur7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nwgnce/unsure_which_ollama_model_to_use_heres_a_tool_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nwgnce/unsure_which_ollama_model_to_use_heres_a_tool_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nwgnce/unsure_which_ollama_model_to_use_heres_a_tool_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T21:29:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwdnea</id>
    <title>I visualized embeddings walking across the latent space as you type! :)</title>
    <updated>2025-10-02T19:37:46+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nwdnea/i_visualized_embeddings_walking_across_the_latent/"&gt; &lt;img alt="I visualized embeddings walking across the latent space as you type! :)" src="https://external-preview.redd.it/NWphdDhwNTY0cnNmMcfpx6_IdDgYBGvf-fwH7xFuI_ot2ErqijE3fUPasYhL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88e9c777c7ada4a09dec47d31a332256767a3b39" title="I visualized embeddings walking across the latent space as you type! :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1axflp564rsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nwdnea/i_visualized_embeddings_walking_across_the_latent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nwdnea/i_visualized_embeddings_walking_across_the_latent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-02T19:37:46+00:00</published>
  </entry>
</feed>
