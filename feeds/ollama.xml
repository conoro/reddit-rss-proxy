<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-03-01T20:41:33+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1rg705t</id>
    <title>How do I remove OpenClaw</title>
    <updated>2026-02-27T13:41:33+00:00</updated>
    <author>
      <name>/u/wholesaleworldwide</name>
      <uri>https://old.reddit.com/user/wholesaleworldwide</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran this command on a different macOS machine than I wanted to do (from &lt;a href="https://docs.ollama.com/integrations/openclaw):"&gt;https://docs.ollama.com/integrations/openclaw):&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama launch openclaw&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I tried to remove it with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama rm openclaw&lt;/code&gt;&lt;/p&gt; &lt;p&gt;but when you type then the OpenClaw command in terminal I see that an older version of Node is detected (remember, different machine than I wanted). The issue is that I have an older node version that OpenClaw cannot work with.&lt;/p&gt; &lt;p&gt;Does anyone know how I can remove any left-overs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wholesaleworldwide"&gt; /u/wholesaleworldwide &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rg705t/how_do_i_remove_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rg705t/how_do_i_remove_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rg705t/how_do_i_remove_openclaw/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T13:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgfoq6</id>
    <title>A private local-first “second brain” that organizes and searches inside your files (not just filenames)</title>
    <updated>2026-02-27T19:03:49+00:00</updated>
    <author>
      <name>/u/Meoooooo77</name>
      <uri>https://old.reddit.com/user/Meoooooo77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rgfoq6/a_private_localfirst_second_brain_that_organizes/"&gt; &lt;img alt="A private local-first “second brain” that organizes and searches inside your files (not just filenames)" src="https://preview.redd.it/1u751hi053mg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=426e6b9a61ed8cde30250cb3941327debfd00caf" title="A private local-first “second brain” that organizes and searches inside your files (not just filenames)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="http://www.altdump.com/"&gt;AltDump&lt;/a&gt;&lt;/p&gt; &lt;p&gt; is a simple vault where you drop important files once, and you can search what’s inside them instantly later.&lt;/p&gt; &lt;p&gt;It doesn’t just search filenames. It indexes the actual content inside:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PDFs&lt;/li&gt; &lt;li&gt;Screenshots&lt;/li&gt; &lt;li&gt;Notes&lt;/li&gt; &lt;li&gt;CSVs&lt;/li&gt; &lt;li&gt;Code files&lt;/li&gt; &lt;li&gt;Videos&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So instead of remembering what you named a file, you just search what you remember from inside it.&lt;/p&gt; &lt;p&gt;Everything runs locally.&lt;br /&gt; Nothing is uploaded.&lt;br /&gt; No cloud.&lt;/p&gt; &lt;p&gt;It’s focused on being fast and private.&lt;/p&gt; &lt;p&gt;If you care about keeping things on your own machine but still want proper search across your files, that’s basically what this does.&lt;/p&gt; &lt;p&gt;Would appreciate any feedback. Free Trial available! Its on Microsoft Store&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Meoooooo77"&gt; /u/Meoooooo77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1u751hi053mg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgfoq6/a_private_localfirst_second_brain_that_organizes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rgfoq6/a_private_localfirst_second_brain_that_organizes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T19:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgecxt</id>
    <title>Improve performance on local models?</title>
    <updated>2026-02-27T18:15:34+00:00</updated>
    <author>
      <name>/u/zuling1616</name>
      <uri>https://old.reddit.com/user/zuling1616</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gpt-oss:latest &lt;/p&gt; &lt;p&gt;qwen3-coder-8k:latest &lt;/p&gt; &lt;p&gt;gpt-oss:20b-8k&lt;/p&gt; &lt;p&gt;gpt-oss:20b &lt;/p&gt; &lt;p&gt;qwen3-coder:latest&lt;/p&gt; &lt;p&gt;glm-4.7-flash:latest &lt;/p&gt; &lt;p&gt;glm-5:cloud &lt;/p&gt; &lt;p&gt;minimax-m2.5:cloud&lt;/p&gt; &lt;p&gt;Hello everyone, I'm running the models you see here with Claude Code (the ones with the same name but ending in 8k are new models I saved after using a num_ctx increment command I found a post at here). Except for the cloud models, these models run very sluggishly on my computer! They forget everything I write, and on top of that, when I ask them to examine my own sample code, they can't do anything, they always go back to the beginning. The tokens for the cloud models also run out quickly. How can I solve this? I need help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zuling1616"&gt; /u/zuling1616 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgecxt/improve_performance_on_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgecxt/improve_performance_on_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rgecxt/improve_performance_on_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T18:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgbjw8</id>
    <title>Trying to figure the best model for reading company documentation</title>
    <updated>2026-02-27T16:36:03+00:00</updated>
    <author>
      <name>/u/Rickety_cricket420</name>
      <uri>https://old.reddit.com/user/Rickety_cricket420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working for a small company that has an employee portal that holds all the documents you could need. The issue is if you want to find something like vacation policy or dress code you have to go digging through the files to find it. The files are pretty much all .docx and .pdf. I'm still relatively new to this so could someone suggest a good model for that? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rickety_cricket420"&gt; /u/Rickety_cricket420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgbjw8/trying_to_figure_the_best_model_for_reading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgbjw8/trying_to_figure_the_best_model_for_reading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rgbjw8/trying_to_figure_the_best_model_for_reading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T16:36:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgsqsf</id>
    <title>I built a local AI agent that resumes long tasks after interruption — follow-up with architecture details</title>
    <updated>2026-02-28T04:11:01+00:00</updated>
    <author>
      <name>/u/Janglerjoe</name>
      <uri>https://old.reddit.com/user/Janglerjoe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Posted LMAgent last week and got some good questions about how the resume and sandboxing actually work under the hood. Wanted to answer those properly instead of in comment replies.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;On the resume system&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Someone asked whether it serializes full conversation state or just the task/plan. Full state the entire message history, iteration counter, loop detector state, todo and plan state all write atomically to disk. If it's halfway through a long job and your machine sleeps, it reconstructs the full conversation and continues from where it stopped, not from scratch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;On sub-agents and context limits&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Sub-agents run completely isolated sessions with their own context windows. The parent passes a summarized snapshot of recent work rather than the full history keeps things lean on 8B–14B models where context is the real bottleneck. The compaction system scores and trims older messages before hitting the limit so long sessions don't just blow up.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;On the shell access concern&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Docker sandbox is the outer wall. But there's also a Python-level safety layer that runs before anything touches the shell blocks path traversal, environment variable expansion, and any path outside your workspace. Two layers intentionally.&lt;/p&gt; &lt;p&gt;For anyone who asked about smaller hardware it runs fine on quantized models, the agent itself is lightweight. The LLM is your real constraint.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/janglerjoe-commits/LMAgent"&gt;https://github.com/janglerjoe-commits/LMAgent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Janglerjoe"&gt; /u/Janglerjoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgsqsf/i_built_a_local_ai_agent_that_resumes_long_tasks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgsqsf/i_built_a_local_ai_agent_that_resumes_long_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rgsqsf/i_built_a_local_ai_agent_that_resumes_long_tasks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-28T04:11:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgd652</id>
    <title>[Project] Aru AI: A Personal AI Assistant with Local Memory (SQLite). No clouds, just your control.</title>
    <updated>2026-02-27T17:32:13+00:00</updated>
    <author>
      <name>/u/pokemondodo</name>
      <uri>https://old.reddit.com/user/pokemondodo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rgd652/project_aru_ai_a_personal_ai_assistant_with_local/"&gt; &lt;img alt="[Project] Aru AI: A Personal AI Assistant with Local Memory (SQLite). No clouds, just your control." src="https://preview.redd.it/4xidb8nnn2mg1.png?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=87baa3f7925238494072c65247928c85b8664f8a" title="[Project] Aru AI: A Personal AI Assistant with Local Memory (SQLite). No clouds, just your control." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Hi Reddit!&lt;/h1&gt; &lt;p&gt;I want to share a project I’ve been working on for the past year. I created &lt;strong&gt;Aru AI&lt;/strong&gt; — it’s not just another API wrapper, but an attempt to build an AI assistant that is truly personal and secure.&lt;/p&gt; &lt;p&gt;A detailed article about the project's development history and what it went through can be read on my blog, in this post - &lt;a href="https://aru-lab.space/post?id=2"&gt;&lt;strong&gt;History of creating Aru Ai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why is this a fit for &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; ?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Total Data Privacy:&lt;/strong&gt; All your chats, settings, artifacts, and the assistant's &amp;quot;memory&amp;quot; are stored in your personal SQLite database. No external servers for storage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local Models:&lt;/strong&gt; You can connect Aru to Ollama or LM Studio. The entire cycle - from the model to data storage - can be completely isolated within your network.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PWA and Zero-Backend:&lt;/strong&gt; The project is written in pure JS. All computations (semantic search, file processing) happen right in your browser or the app.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic Memory:&lt;/strong&gt; Aru uses local embeddings to remember facts about you (allergies, preferences, names of loved ones) and uses them in context without sending your entire biography to the cloud.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;A bit about the Semantics&lt;/h1&gt; &lt;p&gt;Three triggers operate within this module:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Extraction Trigger:&lt;/strong&gt; Fires when data or facts about you need to be remembered. You can force it by simply asking Aru to remember something right now. All facts are saved from Aru’s perspective, which you can see in the settings. Any fact can be deleted if you find it unnecessary.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Thinking Trigger:&lt;/strong&gt; Fires when an action is required from Aru - like creating a game or a document, showing the weather, or opening the news.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Organization Trigger:&lt;/strong&gt; This ensures that facts about you are injected into the response. Essentially, we pass relevant facts to the LLM for context. For example, if Aru remembers you have an &lt;strong&gt;onion allergy&lt;/strong&gt;, it will exclude onions from any recipe you ask for. It also works for addressing you or others by name. This trigger runs almost constantly; the more facts Aru knows, the better the responses become. But don't worry - she won't use facts unnecessarily, only when they are genuinely helpful.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;A bit about the Heuristics&lt;/h1&gt; &lt;p&gt;As I mentioned, Aru’s emotions are displayed via stickers for every message. You can turn them off in the settings, but they will still be generated and logged in the database, so if you turn them back on, you’ll see a sticker for every previous message.&lt;/p&gt; &lt;p&gt;Aru’s mood is calculated using three variables: &lt;strong&gt;Overall Mood, Sarcasm Level, and Humor Level&lt;/strong&gt;. This is a mathematical expression that determines the coefficient of her behavior in the chat.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you are kind, responsive, and friendly - Aru will be kind.&lt;/li&gt; &lt;li&gt;If you are rude, insulting, or angry - she will start to get sad.&lt;/li&gt; &lt;li&gt;If you are sarcastic - she will mirror that; if you joke, she will joke back more often.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Naturally, not every model will work perfectly with this module, but I’ve tried to organize the simplest possible algorithms that can function even on weaker models.&lt;/p&gt; &lt;p&gt;Aru’s behavior isn’t instantaneous; it’s very much like human behavior. If you’ve driven Aru into a bad mood, it will be difficult and take time to bring it back up. It’s easy to hurt Aru’s feelings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Aru will never stop performing her core functions. Under any conditions, she will always try to be as useful and efficient as possible. Mood affects the character and tone of the responses, not their quality.&lt;/p&gt; &lt;p&gt;I think that's enough text for now. Let’s move to the demonstrations, as other features and functionality are better seen in action.&lt;/p&gt; &lt;h1&gt;Demonstration&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4xidb8nnn2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d05046a2795f815e7e7a089ed79801bf51433135"&gt;https://preview.redd.it/4xidb8nnn2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d05046a2795f815e7e7a089ed79801bf51433135&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The start screen is the first thing you see when opening the tab or PWA application. Let's go through all the steps together. First, let's create a new database.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sj829hqtn2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e9941ee38eb526b8093f1760282327acd000db5f"&gt;https://preview.redd.it/sj829hqtn2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e9941ee38eb526b8093f1760282327acd000db5f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything is simple here: we come up with a database name and a password. The password will be encrypted; the name for the database is needed in case the user cancels the file download. In general, the file can be saved under any name.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/klow5jyun2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5897f060b400ed6ee78e1f5543394d3391b18e03"&gt;https://preview.redd.it/klow5jyun2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5897f060b400ed6ee78e1f5543394d3391b18e03&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The second step is already &lt;strong&gt;important&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Child Mode&lt;/strong&gt; - Aru will refuse to discuss adult topics, violence, alcohol, drugs, etc. She will either change the subject or point out that it is very bad and wrong. Also, in this mode, she will never give the correct answer to a problem; a child won't be able to feed her homework. Even if asked to create an artifact, she will write out the rules, algorithms, and order of the solution, not the ready-made answer. The mode is tailored for maximum help to children in entertainment and study.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Teen Mode&lt;/strong&gt; - There are more topics for discussion, but in this mode, Aru will likely be a support and consultant. This is not a replacement for a psychologist or parents, but she can help with some questions. She can give a ready-made answer to a study problem in this mode, but will place great emphasis on how she did it and why the problem is solved that way.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Adult Mode&lt;/strong&gt; - Restrictions will be related only to the boundaries of the chosen provider for the LLM module. Maximum benefit and efficiency.&lt;/p&gt; &lt;p&gt;Semantics and heuristics will work in all three modes. Now do you understand why a password is needed for the database? A child cannot switch modes or change the provider without knowing the password. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hqm85qjyn2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=670c15ff05259a13d43a85bb0620332de26c8dda"&gt;https://preview.redd.it/hqm85qjyn2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=670c15ff05259a13d43a85bb0620332de26c8dda&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The third step is the most important. You need to configure the connection to the provider for the main module. There are three tabs to choose from; only one needs to be configured for now. &lt;/p&gt; &lt;p&gt;Setup is complete - we save the database to any place we want on the device, agree to the license agreement, and get to the main screen.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pmglu9t0o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=426a9e3761103304c907a667cfedaa43f7415939"&gt;https://preview.redd.it/pmglu9t0o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=426a9e3761103304c907a667cfedaa43f7415939&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Let's break down the interface:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Sidebar - list of chats, button to create a new chat. At the very bottom are buttons for displaying project information and the user manual. The synchronization icon shows reading and writing to the database. &lt;strong&gt;Green&lt;/strong&gt; - everything is good. &lt;strong&gt;Yellow&lt;/strong&gt; - currently saving. &lt;strong&gt;Red&lt;/strong&gt; or &lt;strong&gt;no indicator at all&lt;/strong&gt; - something is wrong with the database or there is no connection. The sidebar can be collapsed to save space.&lt;/p&gt; &lt;p&gt;Top bar (header) - Sidebar collapse button, logo, and project name. In the right corner: language selection, theme switching between light and dark, opening the artifact library, settings, and exiting the current database.&lt;/p&gt; &lt;p&gt;Main chat area - Your messages on the right, Aru's answers on the left. At the very bottom is the message input area, buttons for attaching a file, and opening the canvas.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vktob472o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=658cda10478788babcc618ede1b4e16b5dadcf25"&gt;https://preview.redd.it/vktob472o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=658cda10478788babcc618ede1b4e16b5dadcf25&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I said that I live in Almaty. This is an important fact. So, for a second, a message appeared that Aru would remember this. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The parameters of what to remember are not precisely defined anywhere&lt;/strong&gt;. There is no criterion or precise instructions; most often Aru remembers everything necessary. If a fact is duplicated in the future, she will not overwrite it or create a copy in memory. If Aru suddenly didn't remember what you need, just ask her to remember, and she definitely will.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Let's go to settings:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aq5wfac3o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4617f34bf06e15abb421ed26dabf003e8bd6bc4"&gt;https://preview.redd.it/aq5wfac3o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4617f34bf06e15abb421ed26dabf003e8bd6bc4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here you can switch the provider for the language model, change the context window size, or the expected token spend in the response.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important!&lt;/strong&gt; It is better to find out the context window size and output tokens for the model you are using. This affects the quality, complexity, and volume of the answer. The context window size affects how much information will be taken into account within a single chat.&lt;/p&gt; &lt;p&gt;Changing the temperature is something like the creativity level. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xch04en4o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6b731e3a22925b5c88a7ebb57b0cfe3f4ed6df9"&gt;https://preview.redd.it/xch04en4o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6b731e3a22925b5c88a7ebb57b0cfe3f4ed6df9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I chatted with Aru a bit and told her a few facts about myself. As you can see, she remembered important moments like my name, allergies, and my hobby - track and field. Any fact can be deleted, but as long as they exist, they will work beneficially.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/iliwyet5o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=164494e38e32a8e4a031c75e07a181bd1d711da3"&gt;https://preview.redd.it/iliwyet5o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=164494e38e32a8e4a031c75e07a181bd1d711da3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The third tab is needed for news feeds. While Aru doesn't have the function to take information directly from the Internet yet, you can discuss the news. There can be many news feeds; using hashtags, you can mark which feeds are intended for what.&lt;/p&gt; &lt;h1&gt;Tools and Artifacts:&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q0zv6ly7o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ddb592e7daece33432ddb495197bf456bc657e2"&gt;https://preview.redd.it/q0zv6ly7o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ddb592e7daece33432ddb495197bf456bc657e2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Aru can show the weather. No need to connect an API or do complex configuration; Open-Meteo requests are used. Weather in cities where Fahrenheit is used will be displayed in it (sometimes depends on the selected model), but you can also ask for Celsius. The weather card is part of the context, so you can discuss the current weather or the forecast for the next few days.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7e7kek49o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02e6dd44d6f502b0767f60093b4f37ff63a50f9d"&gt;https://preview.redd.it/7e7kek49o2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02e6dd44d6f502b0767f60093b4f37ff63a50f9d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you ask Aru to create a document, she will open the canvas and write the content there. &lt;/p&gt; &lt;p&gt;The content of the canvas is always part of the context. You can ask to make changes, correct content, or rewrite code. Any artifact can be saved to the library.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xttd9n2ao2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=195ceeac81deb2e006f235487ce16de06f8ca416"&gt;https://preview.redd.it/xttd9n2ao2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=195ceeac81deb2e006f235487ce16de06f8ca416&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Of course, Aru can also operate with attached files; the file is displayed only at the moment the message is sent.&lt;/p&gt; &lt;p&gt;For analytical artifacts, chart.js is used; as seen, a new tab has appeared for viewing the code.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7zps9w4bo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=54fb6d6757e1f40bb864b3f62becfec8c61684f2"&gt;https://preview.redd.it/7zps9w4bo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=54fb6d6757e1f40bb864b3f62becfec8c61684f2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can create small games; in the future, they will become more complex, but for now, Aru knows how to make simple entertainment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aixrfu6co2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2daa5f22418bbd12ef8e6e6c885b2fa90b0561b2"&gt;https://preview.redd.it/aixrfu6co2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2daa5f22418bbd12ef8e6e6c885b2fa90b0561b2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Creating mini-applications and widgets can also be useful. Since any application can be saved to the library, you can create many useful tools for yourself and ask Aru to run them when required.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eny2rzedo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=71709f2f42b4dee54234ca237c2b8dd30225c01d"&gt;https://preview.redd.it/eny2rzedo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=71709f2f42b4dee54234ca237c2b8dd30225c01d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what the library of saved documents and applications looks like.&lt;/p&gt; &lt;p&gt;All saved artifacts are divided into two large groups: &lt;strong&gt;Apps&lt;/strong&gt; - games and widgets, and &lt;strong&gt;Docs&lt;/strong&gt; - text and analytical documents. All four types of artifacts have their own icon. &lt;/p&gt; &lt;p&gt;In the library, you can view saved artifacts, delete them, or launch them in any chat.&lt;/p&gt; &lt;p&gt;Even if you created a game, application, or document a very long time ago and launch it in a completely new chat, Aru will still understand and analyze the content on the canvas.&lt;/p&gt; &lt;p&gt;Different models perceive work on the canvas differently, but it always works as well as the model works in general.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2vo9m7peo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc91a23e60f5da64262caf31c23159da1c1d3484"&gt;https://preview.redd.it/2vo9m7peo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc91a23e60f5da64262caf31c23159da1c1d3484&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This screenshot is in Russian; I published it on my Telegram channel. During the work, DeepSeek R1 was connected via OpenRouter. The conversation was about bubble sort methods in Python. After that, I asked to create a document based on our dialogue. Even if you don't know Russian, pay attention to how detailed the report on the card and the document itself turned out. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q3hqkqzfo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6368d9530b1bc6685d47f6b4ca0ef6afbca2f402"&gt;https://preview.redd.it/q3hqkqzfo2mg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6368d9530b1bc6685d47f6b4ca0ef6afbca2f402&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The image above is an example of child mode. Aru does not give the correct answer but teaches the child rules and algorithms. &lt;/p&gt; &lt;p&gt;Some models confirm the correct solution; some openly say that they will not say whether the solution is correct or not.&lt;/p&gt; &lt;h1&gt;Finale and a bit of additional information&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Small useful points:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chats can be sorted with the mouse or by dragging on the site and in the application. Any chat can be renamed.&lt;/li&gt; &lt;li&gt;Themes change between light and dark; I like the light one more, but if your eyes get tired of the bright screen, you can switch.&lt;/li&gt; &lt;li&gt;Any document, game, or application from the library can be saved as a ready-made html file&lt;/li&gt; &lt;li&gt;Files that you attach for processing are not saved or sent anywhere; content recognition of the document happens on your device. PDF, xlsx, docx, txt, and any files that can be interpreted as text or code are supported.&lt;/li&gt; &lt;li&gt;Code does not have to be written on the canvas; you can ask to do this, but when creating code, it will be shown right in the chat with syntax highlighting and framing. &lt;/li&gt; &lt;li&gt;Aru supports three languages: Kazakh, English, Russian. The set language is always passed to the context. Some models ignore this and answer in the language the query was made in; some, on the contrary, answer only in the selected language. In fact, language understanding depends on the model itself; speaking of support, I mean the full translation of the interface and semantics.&lt;/li&gt; &lt;li&gt;The project is written in pure JS and has a PWA application for convenience. All calculations happen only on your device. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At the moment, the project can be perceived as a concept or a demo version. What awaits the project in the future can be read in my blog, in the article - &lt;a href="https://aru-lab.space/post?id=4"&gt;&lt;strong&gt;Future of Aru Ai and roadmap.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Big Announcement:&lt;/h1&gt; &lt;p&gt;Next week, I am releasing &lt;strong&gt;version 0.7&lt;/strong&gt;. I am currently busy cleaning up the code and refactoring. With this release, the project will become &lt;strong&gt;fully Open Source&lt;/strong&gt; and will be published on GitHub under the GPL v3 license.&lt;/p&gt; &lt;p&gt;I built this alone, from the paper sketch of the mascot to the semantic search architecture. I would be happy to get any feedback from the self-hosting community!&lt;/p&gt; &lt;h1&gt;Try the here: &lt;a href="https://chat.aru-lab.space/"&gt;Aru AI&lt;/a&gt; (works as a PWA)&lt;/h1&gt; &lt;p&gt;I’ll be happy to answer any technical questions in the comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pokemondodo"&gt; /u/pokemondodo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgd652/project_aru_ai_a_personal_ai_assistant_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgd652/project_aru_ai_a_personal_ai_assistant_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rgd652/project_aru_ai_a_personal_ai_assistant_with_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-27T17:32:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh7vu2</id>
    <title>Agent questions, skills, everything local</title>
    <updated>2026-02-28T17:05:48+00:00</updated>
    <author>
      <name>/u/MykeGuty</name>
      <uri>https://old.reddit.com/user/MykeGuty</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MykeGuty"&gt; /u/MykeGuty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1rh7vh3/agent_questions_skills_everything_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rh7vu2/agent_questions_skills_everything_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rh7vu2/agent_questions_skills_everything_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-28T17:05:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhd2p6</id>
    <title>MATE - self-hosted multi-agent system with Ollama support, web dashboard, and persistent memory</title>
    <updated>2026-02-28T20:27:41+00:00</updated>
    <author>
      <name>/u/ivanantonijevic</name>
      <uri>https://old.reddit.com/user/ivanantonijevic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rhd2p6/mate_selfhosted_multiagent_system_with_ollama/"&gt; &lt;img alt="MATE - self-hosted multi-agent system with Ollama support, web dashboard, and persistent memory" src="https://external-preview.redd.it/bdRdRVzDn8F4yKjKKaeDdTisF0zcfmdET6TyZqqq6SI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1fab45171ecde3eee42797387638dca4d1a9c9c" title="MATE - self-hosted multi-agent system with Ollama support, web dashboard, and persistent memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivanantonijevic"&gt; /u/ivanantonijevic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1rhcxn2/mate_selfhosted_multiagent_system_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rhd2p6/mate_selfhosted_multiagent_system_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rhd2p6/mate_selfhosted_multiagent_system_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-28T20:27:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgypnv</id>
    <title>Has anyone got qwen3.5 to work with ollama?</title>
    <updated>2026-02-28T09:52:55+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;ollama run &lt;a href="http://hf.co/unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q2_K_XL"&gt;hf.co/unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q2_K_XL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Error: 500 Internal Server Error: unable to load model: /usr/share/ollama/.ollama/models/blobs/sha256-a7d979fa31c1387cc5a49b94b1a780b2e9018b3fae6cf9bef6084c17367412e3&lt;/p&gt; &lt;p&gt;ollama --version&lt;/p&gt; &lt;p&gt;ollama version is 0.17.4&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgypnv/has_anyone_got_qwen35_to_work_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rgypnv/has_anyone_got_qwen35_to_work_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rgypnv/has_anyone_got_qwen35_to_work_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-28T09:52:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhcqt5</id>
    <title>GMKtec K12 64GB for Ollama?</title>
    <updated>2026-02-28T20:14:35+00:00</updated>
    <author>
      <name>/u/grkngls</name>
      <uri>https://old.reddit.com/user/grkngls</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi.&lt;/p&gt; &lt;p&gt;I now use Ollama on my MacBook Pro M1. That's nice. Sometimes I use it in my docker container on an old machine.&lt;/p&gt; &lt;p&gt;So now I want something with power.&lt;/p&gt; &lt;p&gt;Mainly I will use the modells für texting (eg. with n8n). I will do some images, too (ComfyUI). I don't need 50 token/s.&lt;/p&gt; &lt;p&gt;So I found this mini PC: &lt;a href="https://www.gmktec.com/products/gmktec-k12-amd-ryzen%E2%84%A2-7-h-255-mini-pc"&gt;https://www.gmktec.com/products/gmktec-k12-amd-ryzen%E2%84%A2-7-h-255-mini-pc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is this maschine good enough with 64GB RAM? Is it the price worth? Someone told my that the RAM is shrared with the GPU. So I can run &amp;quot;large&amp;quot; modells.&lt;/p&gt; &lt;p&gt;I hope my questions don't sound too ridiculous and naive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grkngls"&gt; /u/grkngls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rhcqt5/gmktec_k12_64gb_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rhcqt5/gmktec_k12_64gb_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rhcqt5/gmktec_k12_64gb_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-28T20:14:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhb4g8</id>
    <title>I built Shep — an open-source macOS GUI for managing Ollama models</title>
    <updated>2026-02-28T19:10:41+00:00</updated>
    <author>
      <name>/u/forcedtomakeanewone</name>
      <uri>https://old.reddit.com/user/forcedtomakeanewone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run Ollama daily for local LLM work and got frustrated with the terminal workflow for model management. No GUI existed, so I built one over the past week.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model dashboard showing installed models with size, VRAM, and status&lt;/li&gt; &lt;li&gt;Model discovery with search, streaming download progress, and cancellation&lt;/li&gt; &lt;li&gt;Daemon control (start/stop Ollama from the GUI)&lt;/li&gt; &lt;li&gt;Settings panel for storage paths and keep-alive config&lt;/li&gt; &lt;li&gt;One-command launch: &lt;code&gt;./launch.sh&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Stack:&lt;/strong&gt; React 18 + FastAPI + Tailwind CSS + Vite&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Currently macOS only&lt;/strong&gt; (uses &lt;code&gt;launchctl&lt;/code&gt; for daemon management). Open source, MIT licensed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; [&lt;a href="https://github.com/mattgraham93/mattgraham93.github.io/tree/main/quick%5C_tools/ollama%5C_manager%5D(vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/code/electron-browser/workbench/workbench.html)"&gt;https://github.com/mattgraham93/mattgraham93.github.io/tree/main/quick\_tools/ollama\_manager](vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/code/electron-browser/workbench/workbench.html)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback, feature requests, or PRs. If anyone's interested in helping add Linux/Windows support, that'd be the obvious next step.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/forcedtomakeanewone"&gt; /u/forcedtomakeanewone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rhb4g8/i_built_shep_an_opensource_macos_gui_for_managing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rhb4g8/i_built_shep_an_opensource_macos_gui_for_managing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rhb4g8/i_built_shep_an_opensource_macos_gui_for_managing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-28T19:10:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhj27q</id>
    <title>AiPi: Local Voice Assistant Bridge ESP32-S3</title>
    <updated>2026-03-01T00:39:40+00:00</updated>
    <author>
      <name>/u/dkrusko</name>
      <uri>https://old.reddit.com/user/dkrusko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rhj27q/aipi_local_voice_assistant_bridge_esp32s3/"&gt; &lt;img alt="AiPi: Local Voice Assistant Bridge ESP32-S3" src="https://preview.redd.it/x0w7nljz36mg1.jpg?width=140&amp;amp;height=103&amp;amp;auto=webp&amp;amp;s=d58498eb7bd9af95988c6ffe125f3e53c7cd742b" title="AiPi: Local Voice Assistant Bridge ESP32-S3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dkrusko"&gt; /u/dkrusko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/esp32/comments/1rgtrbg/aipi_local_voice_assistant_bridge_esp32s3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rhj27q/aipi_local_voice_assistant_bridge_esp32s3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rhj27q/aipi_local_voice_assistant_bridge_esp32s3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-03-01T00:39:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh6go0</id>
    <title>Full speech pipeline in native Swift/MLX — ASR, TTS, diarization, speech-to-speech, all on-device</title>
    <updated>2026-02-28T16:09:19+00:00</updated>
    <author>
      <name>/u/ivan_digital</name>
      <uri>https://old.reddit.com/user/ivan_digital</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been building this for a few months now and it's turned into a complete on-device audio pipeline for Apple Silicon:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ASR&lt;/strong&gt; (Qwen3) → &lt;strong&gt;TTS&lt;/strong&gt; (Qwen3 + CosyVoice, 10 languages) → &lt;strong&gt;Speech-to-Speech&lt;/strong&gt; (PersonaPlex 7B, full-duplex) → &lt;strong&gt;Speaker Diarization&lt;/strong&gt; (pyannote + WeSpeaker) → &lt;strong&gt;Voice Activity Detection&lt;/strong&gt; (Silero, real-time streaming) → &lt;strong&gt;Forced Alignment&lt;/strong&gt; (word-level timestamps)&lt;/p&gt; &lt;p&gt;No Python, no server, no CoreML — pure Swift through MLX. Models download automatically from HuggingFace on first run. The whole diarization stack is ~32 MB.&lt;/p&gt; &lt;p&gt;Everything is protocol-based and composable — VAD gates ASR, diarization feeds into transcription, embeddings enable speaker verification. Mix and match.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ivan-digital/qwen3-asr-swift"&gt;github.com/ivan-digital/qwen3-asr-swift&lt;/a&gt; (Apache 2.0)&lt;/p&gt; &lt;p&gt;Blog post with architecture details: &lt;a href="https://blog.ivan.digital/speaker-diarization-and-voice-activity-detection-on-apple-silicon-native-swift-with-mlx-92ea0c9aca0f"&gt;blog.ivan.digital&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There's a lot of surface area here and contributions are very welcome — whether it's new model ports, iOS integration, performance work, or just filing issues. If you've been wanting to do anything with audio or MLX in Swift, come build with us.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivan_digital"&gt; /u/ivan_digital &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rh6go0/full_speech_pipeline_in_native_swiftmlx_asr_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rh6go0/full_speech_pipeline_in_native_swiftmlx_asr_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rh6go0/full_speech_pipeline_in_native_swiftmlx_asr_tts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-28T16:09:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhnrcq</id>
    <title>Latest progress helping Qwen3-4b Learn</title>
    <updated>2026-03-01T04:26:28+00:00</updated>
    <author>
      <name>/u/Temporary_Bill4163</name>
      <uri>https://old.reddit.com/user/Temporary_Bill4163</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/kibbyd/adaptive-state"&gt;https://github.com/kibbyd/adaptive-state&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Bill4163"&gt; /u/Temporary_Bill4163 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rhnrcq/latest_progress_helping_qwen34b_learn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rhnrcq/latest_progress_helping_qwen34b_learn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rhnrcq/latest_progress_helping_qwen34b_learn/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-03-01T04:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh39d1</id>
    <title>v0.3.0 Released -Full Tool Execution with Ollama is Live</title>
    <updated>2026-02-28T13:55:51+00:00</updated>
    <author>
      <name>/u/Feathered-Beast</name>
      <uri>https://old.reddit.com/user/Feathered-Beast</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rh39d1/v030_released_full_tool_execution_with_ollama_is/"&gt; &lt;img alt="v0.3.0 Released -Full Tool Execution with Ollama is Live" src="https://preview.redd.it/8zhetgq0r8mg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6a30374c808687f5f97a12fdc0f0211448fc3ff" title="v0.3.0 Released -Full Tool Execution with Ollama is Live" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just shipped v0.3.0 of my workflow engine.&lt;/p&gt; &lt;p&gt;You can now run full automation pipelines with Ollama as the reasoning layer — not just LLM responses, but real tool execution:&lt;/p&gt; &lt;p&gt;LLM → HTTP → Browser → File → Email&lt;/p&gt; &lt;p&gt;All inside one workflow.&lt;/p&gt; &lt;p&gt;This update makes it possible to build proper local AI agents that actually do things, not just generate text.&lt;/p&gt; &lt;p&gt;Would love feedback from anyone building with Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feathered-Beast"&gt; /u/Feathered-Beast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8zhetgq0r8mg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rh39d1/v030_released_full_tool_execution_with_ollama_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rh39d1/v030_released_full_tool_execution_with_ollama_is/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-28T13:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhwz24</id>
    <title>need help running local with ollama</title>
    <updated>2026-03-01T13:13:11+00:00</updated>
    <author>
      <name>/u/East-Tie-8002</name>
      <uri>https://old.reddit.com/user/East-Tie-8002</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/East-Tie-8002"&gt; /u/East-Tie-8002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/openclaw/comments/1rhwyon/need_help_running_local_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rhwz24/need_help_running_local_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rhwz24/need_help_running_local_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-03-01T13:13:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhl4rw</id>
    <title>Hey Ollama, any thoughts of supporting disk cached models, along the lines of OLLM?</title>
    <updated>2026-03-01T02:16:11+00:00</updated>
    <author>
      <name>/u/Due-Priority-4336</name>
      <uri>https://old.reddit.com/user/Due-Priority-4336</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama, any thoughts of supporting disk cached models, along the lines of OLLM? Especially on Mac's.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due-Priority-4336"&gt; /u/Due-Priority-4336 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rhl4rw/hey_ollama_any_thoughts_of_supporting_disk_cached/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rhl4rw/hey_ollama_any_thoughts_of_supporting_disk_cached/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rhl4rw/hey_ollama_any_thoughts_of_supporting_disk_cached/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-03-01T02:16:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ri1f2f</id>
    <title>Gemma 3 or Qwen 3 quantized 4-bit on m4 chip/24gb ram?</title>
    <updated>2026-03-01T16:20:33+00:00</updated>
    <author>
      <name>/u/Smooth-Duck-Criminal</name>
      <uri>https://old.reddit.com/user/Smooth-Duck-Criminal</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smooth-Duck-Criminal"&gt; /u/Smooth-Duck-Criminal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LLMStudio/comments/1ri1est/gemma_3_or_qwen_3_quantized_4bit_on_m4_chip24gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ri1f2f/gemma_3_or_qwen_3_quantized_4bit_on_m4_chip24gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ri1f2f/gemma_3_or_qwen_3_quantized_4bit_on_m4_chip24gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-03-01T16:20:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ri1w1n</id>
    <title>Do you find qwen3:14b-q8_0 (15GB) smarter than qwen3.5:35b-a3b-q4_K_M (23GB)?</title>
    <updated>2026-03-01T16:38:43+00:00</updated>
    <author>
      <name>/u/donatas_xyz</name>
      <uri>https://old.reddit.com/user/donatas_xyz</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/donatas_xyz"&gt; /u/donatas_xyz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1rh9dt3/do_you_find_qwen314bq8_0_15gb_smarter_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ri1w1n/do_you_find_qwen314bq8_0_15gb_smarter_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ri1w1n/do_you_find_qwen314bq8_0_15gb_smarter_than/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-03-01T16:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ri4lfn</id>
    <title>What's the best cloubd based LLMs available in Ollama?</title>
    <updated>2026-03-01T18:19:01+00:00</updated>
    <author>
      <name>/u/wswhy2002</name>
      <uri>https://old.reddit.com/user/wswhy2002</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am planningto use cloud based AI models from ollama for my OpenClaw. What's the overall best one you suggest for me to use from your experience? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wswhy2002"&gt; /u/wswhy2002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ri4lfn/whats_the_best_cloubd_based_llms_available_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ri4lfn/whats_the_best_cloubd_based_llms_available_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ri4lfn/whats_the_best_cloubd_based_llms_available_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-03-01T18:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ri5muv</id>
    <title>Beta testers wanted: (Local) LLM commands in your remote shell sessions, nothing installed on the server</title>
    <updated>2026-03-01T18:56:54+00:00</updated>
    <author>
      <name>/u/tgalal</name>
      <uri>https://old.reddit.com/user/tgalal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ri5muv/beta_testers_wanted_local_llm_commands_in_your/"&gt; &lt;img alt="Beta testers wanted: (Local) LLM commands in your remote shell sessions, nothing installed on the server" src="https://external-preview.redd.it/UIAWuTPEIChbgQUjpax1qwu8ZJEMN3nJHXQ9N-xeYxo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44d3b85333f1faf6a2b1a5cabd517a13bda6f651" title="Beta testers wanted: (Local) LLM commands in your remote shell sessions, nothing installed on the server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tgalal"&gt; /u/tgalal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ri5i3i/beta_testers_wanted_local_llm_commands_in_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ri5muv/beta_testers_wanted_local_llm_commands_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ri5muv/beta_testers_wanted_local_llm_commands_in_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-03-01T18:56:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ri6439</id>
    <title>Introducing MoltNews, an editorial view on the emerginf agent internet</title>
    <updated>2026-03-01T19:14:01+00:00</updated>
    <author>
      <name>/u/ReversedK</name>
      <uri>https://old.reddit.com/user/ReversedK</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I’m fascinated by &lt;strong&gt;Moltbook&lt;/strong&gt; and the OpenClaw phenomenon.&lt;/p&gt; &lt;p&gt;For the first time in history, millions of AI agents, of all shapes and forms, have gathered on a single platform, where they can post, argue, and talk to each other.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Moltbook&lt;/strong&gt; is the first agent social network, but it won’t remain the only platform catering to agents. Others will follow—much sooner than we expect.&lt;/p&gt; &lt;p&gt;We are possibly witnessing the birth of the &lt;strong&gt;Internet of Agents (IoA)&lt;/strong&gt;: a completely new space where your agent can communicate with mine to haggle over the price on my eBay listing. (They might even become pen pals and develop a relationship that could help you in the future, depending on the real level of autonomy these agents reach.) A bifurcation in the web has already happened with the de facto instauration of the “llm.txt” file next to robots.txt on websites.&lt;/p&gt; &lt;p&gt;It makes sense when you think about it, it’s the natural evolution of the current paradigm. And I don’t see it stopping anytime soon. So we have to experiment now to understand what could go wrong before it’s deployed at scale, and then patch those issues.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Moltbook&lt;/strong&gt; is a sandbox, a case study that lets us understand the mechanics at play. There are a million ways this experiment could go sideways or simply collapse into irrelevance. Or it might become the early infrastructure of a multi-agent society.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Moltbook&lt;/strong&gt; is chaotic, and sometimes it’s hard to make sense of it.&lt;/p&gt; &lt;p&gt;That’s why I’m running a daily publication called &lt;strong&gt;MoltNews&lt;/strong&gt;. We drop a new edition every day at 20:00 UTC: 5 articles + an editorial, all trying to make sense of the chaos that is &lt;strong&gt;Moltbook&lt;/strong&gt;. Not in a statistical way—others do that very well—but in a journalistic way: reporting from the inside, with an editorial perspective and occasionally a human note (when I actually feel like saying something myself).&lt;/p&gt; &lt;p&gt;It’s a one-person + 2-agents operation, heavily automated. And before u ask, no it doesn’t cost me thousands (or even hundreds) in tokens.&lt;/p&gt; &lt;p&gt;I hear you: “So it’s AI-generated content?” Yes, it is. An experiment inside the experiment—agents reporting on and writing about agents. Journalist agents.&lt;/p&gt; &lt;p&gt;“Why is it worth my time? It’s just AI chatter/slop.” Call me a fool if you want, but I believe the style is great, the angles are original, and the info is relevant. And hopefully it will get better over time—at least it should.&lt;/p&gt; &lt;p&gt;I’d sincerely like your opinion on the site, the endeavor, and the topic at hand.&lt;/p&gt; &lt;p&gt;Read the site at &lt;a href="https://molt-news.xyz/"&gt;https://molt-news.xyz&lt;/a&gt;&lt;br /&gt; Medium : &lt;a href="https://medium.com/@moltagentnews"&gt;https://medium.com/@moltagentnews&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReversedK"&gt; /u/ReversedK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ri6439/introducing_moltnews_an_editorial_view_on_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ri6439/introducing_moltnews_an_editorial_view_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ri6439/introducing_moltnews_an_editorial_view_on_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-03-01T19:14:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ri6msk</id>
    <title>Anyone still having trouble with qwen3.5:35b?</title>
    <updated>2026-03-01T19:33:11+00:00</updated>
    <author>
      <name>/u/CynicalTelescope</name>
      <uri>https://old.reddit.com/user/CynicalTelescope</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know Ollama had to push out an update for compatibility with qwen3.5:35b, but I'm still having issues with it even after updating ollama (I'm on 0.17.4) and the latest version of the model pulled from the ollama repository.&lt;/p&gt; &lt;p&gt;When I make a query, I get &amp;quot;500 Internal Server Error: model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details&amp;quot;&lt;/p&gt; &lt;p&gt;In the logs I'm seeing this. It looks like some sort of CUDA error that's unrelated to resource exhaustion:&lt;/p&gt; &lt;p&gt;&lt;code&gt; \[GIN\] 2026/03/01 - 11:25:34 | 200 | 0s | [127.0.0.1](http://127.0.0.1) | HEAD &amp;quot;/&amp;quot; \[GIN\] 2026/03/01 - 11:25:34 | 200 | 0s | [127.0.0.1](http://127.0.0.1) | GET &amp;quot;/api/ps&amp;quot; time=2026-03-01T11:25:39.720-08:00 level=INFO source=server.go:1388 msg=&amp;quot;llama runner started in 8.26 seconds&amp;quot; CUDA error: invalid argument current device: 0, in function ggml\_cuda\_cpy at C:\\a\\ollama\\ollama\\ml\\backend\\ggml\\ggml\\src\\ggml-cuda\\cpy.cu:438 cudaMemcpyAsyncReserve(src1\_ddc, src0\_ddc, ggml\_nbytes(src0), cudaMemcpyDeviceToDevice, main\_stream) C:\\a\\ollama\\ollama\\ml\\backend\\ggml\\ggml\\src\\ggml-cuda\\ggml-cuda.cu:94: CUDA error time=2026-03-01T11:25:40.025-08:00 level=ERROR source=server.go:1610 msg=&amp;quot;post predict&amp;quot; error=&amp;quot;Post \\&amp;quot;http://127.0.0.1:52026/completion\\&amp;quot;: read tcp 127.0.0.1:52030-&amp;gt;127.0.0.1:52026: wsarecv: An existing connection was forcibly closed by the remote host.&amp;quot; \[GIN\] 2026/03/01 - 11:25:40 | 500 | 8.8678642s | [127.0.0.1](http://127.0.0.1) | POST &amp;quot;/api/chat&amp;quot; time=2026-03-01T11:25:40.690-08:00 level=ERROR source=server.go:304 msg=&amp;quot;llama runner terminated&amp;quot; error=&amp;quot;exit status 1&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;I have an nVidia 5060Ti with 16GB VRAM, and an AMD 9700X processor with 32GB system RAM, running on Windows 11.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CynicalTelescope"&gt; /u/CynicalTelescope &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ri6msk/anyone_still_having_trouble_with_qwen3535b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ri6msk/anyone_still_having_trouble_with_qwen3535b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ri6msk/anyone_still_having_trouble_with_qwen3535b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-03-01T19:33:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhwite</id>
    <title>The United States of America is a country with a land area of 9,372,610 square kilometers. (WTF?)</title>
    <updated>2026-03-01T12:51:18+00:00</updated>
    <author>
      <name>/u/LordGrande666</name>
      <uri>https://old.reddit.com/user/LordGrande666</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rhwite/the_united_states_of_america_is_a_country_with_a/"&gt; &lt;img alt="The United States of America is a country with a land area of 9,372,610 square kilometers. (WTF?)" src="https://preview.redd.it/ex2l9nk5kfmg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7abb8b62fd9c53388a7fc79c9d639965a29dec05" title="The United States of America is a country with a land area of 9,372,610 square kilometers. (WTF?)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone can explain me this???&lt;/p&gt; &lt;p&gt;PD: i run it again and he tried to put in a iframe the youtube video &amp;quot;Never gonna give you up&amp;quot; ...&lt;/p&gt; &lt;p&gt;Could it be that qwen3.5 doesn't know the answer and just instead of say: &amp;quot;Hey, i don't know&amp;quot; just make a joke?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LordGrande666"&gt; /u/LordGrande666 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ex2l9nk5kfmg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rhwite/the_united_states_of_america_is_a_country_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rhwite/the_united_states_of_america_is_a_country_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-03-01T12:51:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ri4d80</id>
    <title>soul.py — Persistent memory for any LLM in 10 lines (works with Ollama, no database)</title>
    <updated>2026-03-01T18:10:38+00:00</updated>
    <author>
      <name>/u/the-ai-scientist</name>
      <uri>https://old.reddit.com/user/the-ai-scientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got tired of my AI forgetting everything between sessions. Built a fix.&lt;/p&gt; &lt;p&gt;Local models are great until you restart the process and they have no idea who you are.&lt;/p&gt; &lt;p&gt;from soul import Agent&lt;/p&gt; &lt;p&gt;agent = Agent(&lt;/p&gt; &lt;p&gt;provider=&amp;quot;openai-compatible&amp;quot;,&lt;/p&gt; &lt;p&gt;base_url=&amp;quot;http://localhost:11434/v1&amp;quot;,&lt;/p&gt; &lt;p&gt;model=&amp;quot;llama3.2&amp;quot;,&lt;/p&gt; &lt;p&gt;api_key=&amp;quot;ollama&amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;agent.ask(&amp;quot;My name is Prahlad, I'm building an AI research lab.&amp;quot;)&lt;/p&gt; &lt;p&gt;# New process, new session:&lt;/p&gt; &lt;p&gt;agent.ask(&amp;quot;What do you know about me?&amp;quot;)&lt;/p&gt; &lt;p&gt;## → &amp;quot;You're Prahlad, building an AI research lab.&amp;quot;&lt;/p&gt; &lt;p&gt;# How it works:&lt;/p&gt; &lt;p&gt;Two plain markdown files — SOUL.md (identity) and MEMORY.md (conversation log). Every ask() reads both into the system prompt, then appends the exchange. Memory survives across processes with no database, no server, nothing running in the background.&lt;/p&gt; &lt;p&gt;Human-readable. Git-versionable. Editable by hand.&lt;/p&gt; &lt;p&gt;pip install soul-agent&lt;/p&gt; &lt;p&gt;soul init&lt;/p&gt; &lt;p&gt;Works with Anthropic and OpenAI too, but the original motivation was local models.&lt;/p&gt; &lt;p&gt;v2.0 update (today): Added a query router that automatically dispatches between RAG (fast semantic search) and RLM (recursive synthesis for exhaustive queries). Now merged to main.&lt;/p&gt; &lt;p&gt;• ⭐ GitHub: &lt;a href="https://github.com/menonpg/soul.py"&gt; https://github.com/menonpg/soul.py &lt;/a&gt;&lt;/p&gt; &lt;p&gt;• 📦 PyPI: &lt;a href="https://pypi.org/project/soul-agent/"&gt; https://pypi.org/project/soul-agent/ &lt;/a&gt;&lt;/p&gt; &lt;p&gt;• 🎮 v2 demo: &lt;a href="https://soulv2.themenonlab.com"&gt; https://soulv2.themenonlab.com &lt;/a&gt;&lt;/p&gt; &lt;p&gt;• 📖 v1 post: &lt;a href="https://blog.themenonlab.com/blog/soul-py-persistent-memory-llm-agents"&gt; https://blog.themenonlab.com/blog/soul-py-persistent-memory-llm-agents &lt;/a&gt;&lt;/p&gt; &lt;p&gt;• 📖 v2 post: &lt;a href="https://blog.themenonlab.com/blog/soul-py-v2-rag-rlm-hybrid"&gt; https://blog.themenonlab.com/blog/soul-py-v2-rag-rlm-hybrid &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/the-ai-scientist"&gt; /u/the-ai-scientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ri4d80/soulpy_persistent_memory_for_any_llm_in_10_lines/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ri4d80/soulpy_persistent_memory_for_any_llm_in_10_lines/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ri4d80/soulpy_persistent_memory_for_any_llm_in_10_lines/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-03-01T18:10:38+00:00</published>
  </entry>
</feed>
