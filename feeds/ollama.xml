<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-22T14:44:59+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1rab1zw</id>
    <title>How to integration Ollama in OpenClaw?</title>
    <updated>2026-02-20T23:24:42+00:00</updated>
    <author>
      <name>/u/Delinquent8438</name>
      <uri>https://old.reddit.com/user/Delinquent8438</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm running Ollama with Mixtral-8x7B on my MacBook Pro M4.&lt;/p&gt; &lt;p&gt;So far it is working and I can use it via the terminal.&lt;/p&gt; &lt;p&gt;I want to integrate it now in OpenClaw. If I got the tutorials correct, I need to skip the OpenClaw install wizard, just select any LLM in manual mode, and add the Ollama config later in the JSON file, correct?&lt;/p&gt; &lt;p&gt;I simply copied pasted this config from the OpenClaw docs, but somehow it is not working:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;models&amp;quot;: { &amp;quot;providers&amp;quot;: { &amp;quot;ollama&amp;quot;: { &amp;quot;apiKey&amp;quot;: &amp;quot;ollama-local&amp;quot;, &amp;quot;baseUrl&amp;quot;: &amp;quot;http://192.168.0.100:11434&amp;quot;, }, }, }, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Am I missing anything?&lt;/p&gt; &lt;p&gt;What is the most straight forward way to get Ollama/Mixtral work in OpenClaw without any other LLM beside?&lt;/p&gt; &lt;p&gt;Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delinquent8438"&gt; /u/Delinquent8438 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rab1zw/how_to_integration_ollama_in_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rab1zw/how_to_integration_ollama_in_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rab1zw/how_to_integration_ollama_in_openclaw/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T23:24:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1rajdu7</id>
    <title>hi! are there some local models that allow video generation from many poses of a certain character?</title>
    <updated>2026-02-21T06:00:03+00:00</updated>
    <author>
      <name>/u/DoubleSubstantial805</name>
      <uri>https://old.reddit.com/user/DoubleSubstantial805</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i got 6gbvram + limited system ram + 4050rtx, i wanted to make certain video generations for a character i made using comfyui&lt;/p&gt; &lt;p&gt;are there models that can run on my machine?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DoubleSubstantial805"&gt; /u/DoubleSubstantial805 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rajdu7/hi_are_there_some_local_models_that_allow_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rajdu7/hi_are_there_some_local_models_that_allow_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rajdu7/hi_are_there_some_local_models_that_allow_video/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T06:00:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1raat9a</id>
    <title>Using AI as part of the game play; best examples?</title>
    <updated>2026-02-20T23:14:54+00:00</updated>
    <author>
      <name>/u/apoliaki</name>
      <uri>https://old.reddit.com/user/apoliaki</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone has good examples of ai games with really cool gameplay? What I mean here isn't using ai to improve speed; etc of making a game; more so that AI is part of the gameplay; i.e. game evolves based on &amp;quot;non determnistic outputs&amp;quot;&lt;/p&gt; &lt;p&gt;I feel like it's definetly the future of gaming (particularly when we'll have good on edge LLMs which will likely be within this year)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I've tried this one and it's pretty cool but have nots een many more afterwards: &lt;a href="https://store.steampowered.com/app/3730100/Whispers_from_the_Star/"&gt;https://store.steampowered.com/app/3730100/Whispers_from_the_Star/&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I know Runway ML is working on this and they launched an early access for months ago&lt;/p&gt; &lt;p&gt;Curious if you guys have some examples/good read on this?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1raas12"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apoliaki"&gt; /u/apoliaki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1raat9a/using_ai_as_part_of_the_game_play_best_examples/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1raat9a/using_ai_as_part_of_the_game_play_best_examples/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1raat9a/using_ai_as_part_of_the_game_play_best_examples/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T23:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1radfsh</id>
    <title>Capi - Openvino GenAI alternative for Ollama</title>
    <updated>2026-02-21T01:06:55+00:00</updated>
    <author>
      <name>/u/Little_Investigator3</name>
      <uri>https://old.reddit.com/user/Little_Investigator3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1radfsh/capi_openvino_genai_alternative_for_ollama/"&gt; &lt;img alt="Capi - Openvino GenAI alternative for Ollama" src="https://preview.redd.it/lt6y65hchskg1.png?width=140&amp;amp;height=100&amp;amp;auto=webp&amp;amp;s=d52e5920cd98782da3b7a0a43516946beb3b1f53" title="Capi - Openvino GenAI alternative for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I‚Äôm excited to launch my first open-source project: **Capi**, a local LLM Linux/Windows app designed as an alternative to Ollama for users of Intel GPUs, with a focus on Arc GPU due to their higher Xe core counts and improved throughput, though it should work with older Intel hardware&lt;/p&gt; &lt;p&gt;[&lt;a href="https://github.com/tiagoflino/capi%5D(https://github.com/tiagoflino/capi"&gt;https://github.com/tiagoflino/capi](https://github.com/tiagoflino/capi)&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;The project is called Capi (inspired by the capybara (or capivara in Portuguese, also native from my hometown), and though I'm still working on fixing Windows x64 installer, it is already working fairly stable on Linux.&lt;/p&gt; &lt;p&gt;While the Windows x64 installer is still under development, the project is already stable on Linux. I haven‚Äôt benchmarked it against IPEX-LLM or Vulkan yet, but on my Lenovo Lunar Lake 258V (32GB RAM, \~110 GB/s bandwidth), it sustains almost double the TG figures for the same prompt when running Qwen3-4B compared to the Vulkan backend.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lt6y65hchskg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1bfcb5387a7077b02c2c4cbf452061e477900377"&gt;https://preview.redd.it/lt6y65hchskg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1bfcb5387a7077b02c2c4cbf452061e477900377&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am aware that there are other initiatives like Openarc heavily invested in Openvino as well, but I wanted to create a UX for less tech-savvy people, similar to what Ollama and LMStudio do, but powered by the proper Intel engine. I am far from there, but we have to start somewhere, don't we? :-)&lt;/p&gt; &lt;p&gt;The tool allows for selecting and fetching GGUF models from HF, and it runs them converted on the fly, so there is no IR conversion required.&lt;/p&gt; &lt;p&gt;I used Rust with a CXX bridge to create bindings to the C++ Openvino GenAI API, as it is the most complete and I assume (though no proper benchmarks on that as well) a little more resource efficient than the Python API available. The UI uses Tauri with Svelte.&lt;/p&gt; &lt;p&gt;Next steps: working on adding more tuning options, refining the hardware metrics, installers and library download, test it more thoroughly, and create a project webpage to add documentation and streamline installation process via script.&lt;/p&gt; &lt;p&gt;Hope you enjoy the project, and I‚Äôd love to hear your thoughts!&lt;/p&gt; &lt;p&gt;P.S. This is my first open-source project, and I‚Äôm still getting around the stack. I‚Äôm open to any tips on improving the code, design, or documentation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Little_Investigator3"&gt; /u/Little_Investigator3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1radfsh/capi_openvino_genai_alternative_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1radfsh/capi_openvino_genai_alternative_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1radfsh/capi_openvino_genai_alternative_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T01:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb32lx</id>
    <title>A100</title>
    <updated>2026-02-21T21:29:06+00:00</updated>
    <author>
      <name>/u/Leading_Jury_6868</name>
      <uri>https://old.reddit.com/user/Leading_Jury_6868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How good is the a100 gpu for llm,s &lt;/p&gt; &lt;p&gt;I have 4 in a dell powereage r750xa &lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leading_Jury_6868"&gt; /u/Leading_Jury_6868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb32lx/a100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb32lx/a100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb32lx/a100/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T21:29:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9zea9</id>
    <title>SmarterRouter - A Smart LLM proxy for all your local models. (native Ollama support, loading/unloading models automatically)</title>
    <updated>2026-02-20T16:06:53+00:00</updated>
    <author>
      <name>/u/peva3</name>
      <uri>https://old.reddit.com/user/peva3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on this project to create a smarter LLM proxy primarily for my openwebui setup (but it's a standard openai compatible endpoint API, so it will work with anything that accepts that).&lt;/p&gt; &lt;p&gt;The idea is pretty simple, you see one frontend model in your system, but in the backend it can load whatever model is &amp;quot;best&amp;quot; for the prompt you send. When you first spin up Smarterrouter it profiles all your models, giving them scores for all the main types of prompts you could ask, as well as benchmark other things like model size, actual VRAM usage, etc. (you can even configure an external &amp;quot;Judge&amp;quot; AI to grade the responses the models give, i've found it improves the profile results, but it's optional). It will also detect and new or deleted models and start profiling them in the background, you don't need to do anything, just add your models to ollama and they will be added to SmarterRouter to be used.&lt;/p&gt; &lt;p&gt;There's a lot going on under the hood, but i've been putting it through it's paces and so far it's performing really well, It's extremely fast, It caches responses, and I'm seeing a negligible amount of time added to prompt response time. It will also automatically load and unload the models in Ollama (and any other backend that allows that).&lt;/p&gt; &lt;p&gt;The only caveat i've found is that currently it favors very small, high performing models, like Qwen coder 0.5B for example, but if small models are faster and they score really highly in the benchmarks... Is that really a bad response? I'm doing more digging, but so far it's working really well with all the test prompts i've given it to try (swapping to larger/different models for more complex questions or creative questions that are outside of the small models wheelhouse).&lt;/p&gt; &lt;p&gt;Here's a high level summary of the biggest features:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Self-Correction via Hardware Profiling&lt;/strong&gt;: Instead of guessing performance, it runs a one-time benchmark on your specific GPU/CPU setup. It learns exactly how fast and capable your models are in your unique environment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Active VRAM Guard&lt;/strong&gt;: It monitors nvidia-smi in real-time. If a model selection is about to trigger an Out-of-Memory (OOM) error, it proactively unloads idle models or chooses a smaller alternative to keep your system stable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Semantic &amp;quot;Smart&amp;quot; Caching&lt;/strong&gt;: It doesn't just match exact text. It uses vector embeddings to recognize when you‚Äôre asking a similar question to a previous one, serving the cached response instantly and saving your compute cycles.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The &amp;quot;One Model&amp;quot; Illusion&lt;/strong&gt;: It presents your entire collection of 20+ models as a single OpenAI-compatible endpoint. You just select SmarterRouter in your UI, and it handles the &amp;quot;load, run, unload&amp;quot; logic behind the scenes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Intelligence-to-Task Routing&lt;/strong&gt;: It automatically analyzes your prompt's complexity. It won't waste your 70B model's time on a &amp;quot;Hello,&amp;quot; and it won't let a 0.5B model hallucinate its way through a complex Python refactor.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLM-as-Judge Feedback&lt;/strong&gt;: It can use a high-end model (like a cloud GPT-4o or a local heavy-hitter) to periodically &amp;quot;score&amp;quot; the performance of your smaller models, constantly refining its own routing weights based on actual quality.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/peva3/SmarterRouter"&gt;https://github.com/peva3/SmarterRouter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know how this works for you, I have it running perfectly with a 4060 ti 16gb, so i'm positive that it will scale well to the massive systems some of y'all have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peva3"&gt; /u/peva3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9zea9/smarterrouter_a_smart_llm_proxy_for_all_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r9zea9/smarterrouter_a_smart_llm_proxy_for_all_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r9zea9/smarterrouter_a_smart_llm_proxy_for_all_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T16:06:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1rasrie</id>
    <title>Small ready-to-use v0.16.3 Agent Builder I built today</title>
    <updated>2026-02-21T14:40:47+00:00</updated>
    <author>
      <name>/u/PythonToolFactory</name>
      <uri>https://old.reddit.com/user/PythonToolFactory</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I just put together a small Gradio tool with 4 agents using the new v0.16.3 features.&lt;/p&gt; &lt;p&gt;- Code Reviewer &lt;/p&gt; &lt;p&gt;- Web Researcher &lt;/p&gt; &lt;p&gt;- File Analyzer with real file upload &lt;/p&gt; &lt;p&gt;- General Task Agent &lt;/p&gt; &lt;p&gt;Runs 100 % local, 15‚Äì35 seconds response time on normal laptops.&lt;/p&gt; &lt;p&gt;Would love some feedback if anyone tries it!&lt;/p&gt; &lt;p&gt;Link in comments.&lt;/p&gt; &lt;p&gt;‚Äì PythonToolFactory&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PythonToolFactory"&gt; /u/PythonToolFactory &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rasrie/small_readytouse_v0163_agent_builder_i_built_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rasrie/small_readytouse_v0163_agent_builder_i_built_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rasrie/small_readytouse_v0163_agent_builder_i_built_today/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T14:40:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ra9wsv</id>
    <title>my portable ollama now has persistent memory</title>
    <updated>2026-02-20T22:39:02+00:00</updated>
    <author>
      <name>/u/VaguneBob</name>
      <uri>https://old.reddit.com/user/VaguneBob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ra9wsv/my_portable_ollama_now_has_persistent_memory/"&gt; &lt;img alt="my portable ollama now has persistent memory" src="https://preview.redd.it/0mwkg27n7qkg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=2125242ea136b89e636be8e0c012c543a9a1c392" title="my portable ollama now has persistent memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This instance of ollama you can just copy onto any machine without installing anything (no docker needed), everything ollama needs is contained within a folder, and all the memories can be reset with one click. This is work in progress.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;F:\MyOllama\ ‚îú‚îÄ‚îÄ Start-Ollama-Dark-Memory.bat ‚îú‚îÄ‚îÄ ollama-dark-memory-final.html ‚îú‚îÄ‚îÄ ollama.exe ‚îú‚îÄ‚îÄ ollama-portable.bat ‚îú‚îÄ‚îÄ models\ ‚îî‚îÄ‚îÄ memory\ ‚îú‚îÄ‚îÄ memory_server.py ‚îî‚îÄ‚îÄ memories.json &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VaguneBob"&gt; /u/VaguneBob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ra9wsv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ra9wsv/my_portable_ollama_now_has_persistent_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ra9wsv/my_portable_ollama_now_has_persistent_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-20T22:39:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rakz0o</id>
    <title>Best structure and models for invoice data extraction</title>
    <updated>2026-02-21T07:30:28+00:00</updated>
    <author>
      <name>/u/Fickle-Bluebird-367</name>
      <uri>https://old.reddit.com/user/Fickle-Bluebird-367</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to build an invoice processing tool that can extract data such as supplier, total, ,vat net amount etc from pdf invoices with a range of layouts and styles. So far I have been using pytesseract to perform OCR then feeding the result into a local LLM using Ollama. with this I can get to about 85-90% accuracy but want to improve. can anyone suggest an improvement structure (for example skipping the OCR and passing the invoice straight into a multimodal model?) or which models are best for this task. I am currently running locally on a mac with 8gb RAM but could run on another set up with 16gb. &lt;/p&gt; &lt;p&gt;Would be great to get any tips from anyone who has worked on something similar. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fickle-Bluebird-367"&gt; /u/Fickle-Bluebird-367 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rakz0o/best_structure_and_models_for_invoice_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rakz0o/best_structure_and_models_for_invoice_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rakz0o/best_structure_and_models_for_invoice_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T07:30:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb95yc</id>
    <title>Got roasted a few weeks ago for "not knowing how to code." Here's what I was actually building....</title>
    <updated>2026-02-22T01:53:56+00:00</updated>
    <author>
      <name>/u/Decent-Freedom5374</name>
      <uri>https://old.reddit.com/user/Decent-Freedom5374</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rb95yc/got_roasted_a_few_weeks_ago_for_not_knowing_how/"&gt; &lt;img alt="Got roasted a few weeks ago for &amp;quot;not knowing how to code.&amp;quot; Here's what I was actually building...." src="https://preview.redd.it/mkktb226bykg1.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=a7a16515a6a16113936847ae7374b686f87bb4a3" title="Got roasted a few weeks ago for &amp;quot;not knowing how to code.&amp;quot; Here's what I was actually building...." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Decent-Freedom5374"&gt; /u/Decent-Freedom5374 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rb95yc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb95yc/got_roasted_a_few_weeks_ago_for_not_knowing_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb95yc/got_roasted_a_few_weeks_ago_for_not_knowing_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T01:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1raui9d</id>
    <title>Computron 9000: The Autonomous AI That Redefines Digital Intelligence</title>
    <updated>2026-02-21T15:52:22+00:00</updated>
    <author>
      <name>/u/larz01larz</name>
      <uri>https://old.reddit.com/user/larz01larz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1raui9d/computron_9000_the_autonomous_ai_that_redefines/"&gt; &lt;img alt="Computron 9000: The Autonomous AI That Redefines Digital Intelligence" src="https://external-preview.redd.it/fztGDgUm_rlUEpoDco3Z49Bt8hogEjaDCtScZ3bcYJA.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ffeae09c0c1e9e0083d73139c266df20d43357ff" title="Computron 9000: The Autonomous AI That Redefines Digital Intelligence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/lefoulkrod/computron_9000"&gt;https://github.com/lefoulkrod/computron_9000&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/larz01larz"&gt; /u/larz01larz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=BG1px6cmwp4&amp;amp;si=d2sOr9Yr0mHSf4gU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1raui9d/computron_9000_the_autonomous_ai_that_redefines/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1raui9d/computron_9000_the_autonomous_ai_that_redefines/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T15:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rauy6b</id>
    <title>70B llm on 4gb android phone !</title>
    <updated>2026-02-21T16:09:44+00:00</updated>
    <author>
      <name>/u/Vast_Lingonberry7259</name>
      <uri>https://old.reddit.com/user/Vast_Lingonberry7259</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rauy6b/70b_llm_on_4gb_android_phone/"&gt; &lt;img alt="70B llm on 4gb android phone !" src="https://external-preview.redd.it/oy08tsZOgcbAmrxT3DWi6J-5B2GiEpwBGCRypU_3Wpg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c755c3a24868f6032710875f987082a1b3f71993" title="70B llm on 4gb android phone !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An 32b q4km on 4gb ram based on virtualization.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Lingonberry7259"&gt; /u/Vast_Lingonberry7259 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/78l5uaosfvkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rauy6b/70b_llm_on_4gb_android_phone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rauy6b/70b_llm_on_4gb_android_phone/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T16:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ravwue</id>
    <title>Help with hardware for OpenClaw/Ollama combo</title>
    <updated>2026-02-21T16:47:30+00:00</updated>
    <author>
      <name>/u/sp0okymuffin</name>
      <uri>https://old.reddit.com/user/sp0okymuffin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello - hope I‚Äôm posting this in the appropriate place. &lt;/p&gt; &lt;p&gt;I‚Äôm reasonably far down an agentic rabbit hole with OpenClaw running on an Proxmox VM and am concluding it‚Äôs time to invest in a set up that can scale and provide me with utility for at least a year. I also want to feed the beast more sensitive information, where I‚Äôd love to do local processing.&lt;/p&gt; &lt;p&gt;My plan is to buy a Mac Mini, where OpenClaw would run and have more power including desktop interaction. I‚Äôm also thinking I‚Äôd get a Mac Studio to serve as my primary PC, on which I‚Äôd love to run a beefy local LLM with good performance for sensitive document processing (think bank statements, business financials, etc.). &lt;/p&gt; &lt;p&gt;I envisage OpenClaw using a combination of the cloud LLMs (primarily Claude) and the local LLM when told to, and for heartbeats, etc. &lt;/p&gt; &lt;p&gt;I‚Äôm trying to gauge what the appropriate horsepower is to throw at this setup. Juggling between M4 16/24GB on the Mac Mini and perhaps even all the way up to 256GB unified memory on the Mac Studio. &lt;/p&gt; &lt;p&gt;But I‚Äôm also wondering if this is overkill; I am not a coder or engineer, and while I‚Äôm an experienced self hoster, I‚Äôm new to Ollama. I‚Äòd be very grateful for some pointers here ‚Äî e.g. Would I be just as well served getting an M4 Pro Mac Mini with 64GB memory for my use case? LLM would then run on the Mac Mini alongside OpenClaw and I‚Äôd hold off getting a primary PC upgrade for a while (and save some money!)&lt;/p&gt; &lt;p&gt;I‚Äôd also like to do text to speech and give my OpenClaw agent a voice. I‚Äôd love to process this locally with some push-to-talk wifi mics that can connect to speakers via AirPlay. speech should be transcribed locally and then prompts could be processed with a cloud provider if needed, just as long as the voice itself doesn‚Äôt get sent to Sam Altman‚Äôs beast (figuratively speaking) &lt;/p&gt; &lt;p&gt;I do care about reasoning models and make quite extensive use of ChatGPT 5.2 and Opus 4.6. &lt;/p&gt; &lt;p&gt;Any guidance much appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sp0okymuffin"&gt; /u/sp0okymuffin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ravwue/help_with_hardware_for_openclawollama_combo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ravwue/help_with_hardware_for_openclawollama_combo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ravwue/help_with_hardware_for_openclawollama_combo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T16:47:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1rawjod</id>
    <title>Ollama will not bind 11434 to any IPv4 interface. Works fine for localhost and IPv6. This is driving me *nuts*</title>
    <updated>2026-02-21T17:11:21+00:00</updated>
    <author>
      <name>/u/Big_Wave9732</name>
      <uri>https://old.reddit.com/user/Big_Wave9732</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rawjod/ollama_will_not_bind_11434_to_any_ipv4_interface/"&gt; &lt;img alt="Ollama will not bind 11434 to any IPv4 interface. Works fine for localhost and IPv6. This is driving me *nuts*" src="https://preview.redd.it/1kek8e77rvkg1.png?width=140&amp;amp;height=27&amp;amp;auto=webp&amp;amp;s=b1bc48b44cc6557c1d6ba76b4c8eb212f7f4a7da" title="Ollama will not bind 11434 to any IPv4 interface. Works fine for localhost and IPv6. This is driving me *nuts*" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit 3: I'm fuckng dumb. I thought the firewall service was disabled because I was able to ssh in remotely. It wasn't.&lt;/p&gt; &lt;p&gt;systemctl stop firewalld.service&lt;br /&gt; systemctl disable firewalld.service&lt;/p&gt; &lt;p&gt;------&lt;/p&gt; &lt;p&gt;Newly installed RHEL 9.7.&lt;/p&gt; &lt;p&gt;I had Ollama working on this machine previously before reinstalling. Now, however, Ollama absolutely will not listen on IPv4.&lt;/p&gt; &lt;p&gt;Attached is the systemctl file, a netstat showing the service listening on IPv6, the output from journalctl -u ollama --no-pager --follow, and the output in /var/log/messages when the service is started.&lt;/p&gt; &lt;p&gt;Anyone? What the hell is up here??&lt;/p&gt; &lt;p&gt;Edit: I have tried the line &amp;quot;Environment=&amp;quot;OLLAMA_HOST=0.0.0.0&amp;quot; with and without port 11434. Didn't work either way.&lt;/p&gt; &lt;p&gt;Things I have also tried:&lt;br /&gt; Selinux is disabled;&lt;br /&gt; Have completely uninstalled and removed all remnants and config files;&lt;br /&gt; Have disabled IPv6 everywhere that I can find it (kernel, OS, deleted the IPv6 interface);&lt;/p&gt; &lt;p&gt;Edit 2:&lt;br /&gt; &amp;quot;OLLAMA_HOST=&lt;a href="http://0.0.0.0:11434"&gt;http://0.0.0.0:11434&lt;/a&gt; ollama serve&amp;quot; gets the same result.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big_Wave9732"&gt; /u/Big_Wave9732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rawjod"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rawjod/ollama_will_not_bind_11434_to_any_ipv4_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rawjod/ollama_will_not_bind_11434_to_any_ipv4_interface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T17:11:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1razv53</id>
    <title>Version Mismatch help</title>
    <updated>2026-02-21T19:18:03+00:00</updated>
    <author>
      <name>/u/UpYourQuality</name>
      <uri>https://old.reddit.com/user/UpYourQuality</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;Not sure whats going on but the only ollama.exe i have is showing that its the wrong version.&lt;/p&gt; &lt;p&gt;I'm using the most recent installer direct from ollama.com. Even tried the winget version and same error.&lt;/p&gt; &lt;p&gt;On Windows 11&lt;/p&gt; &lt;p&gt;No ollama services or task anywhere&lt;/p&gt; &lt;p&gt;Deleted all ollama folders.&lt;/p&gt; &lt;p&gt;Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UpYourQuality"&gt; /u/UpYourQuality &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1razv53/version_mismatch_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1razv53/version_mismatch_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1razv53/version_mismatch_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T19:18:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rara7u</id>
    <title>Faster &amp; Cheaper LLM Apps with Semantic Caching</title>
    <updated>2026-02-21T13:36:10+00:00</updated>
    <author>
      <name>/u/Special_Community179</name>
      <uri>https://old.reddit.com/user/Special_Community179</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rara7u/faster_cheaper_llm_apps_with_semantic_caching/"&gt; &lt;img alt="Faster &amp;amp; Cheaper LLM Apps with Semantic Caching" src="https://external-preview.redd.it/oOZJdgmTkHw77V31kL7N1jm08j8e9Y-FJAqFULVQOvI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f8d267e8b0a6fddff7d4ecde829bd01253f232f" title="Faster &amp;amp; Cheaper LLM Apps with Semantic Caching" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special_Community179"&gt; /u/Special_Community179 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/NrqvtsnjIHU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rara7u/faster_cheaper_llm_apps_with_semantic_caching/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rara7u/faster_cheaper_llm_apps_with_semantic_caching/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T13:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1raw6z8</id>
    <title>qwen3-coder-next on desktop</title>
    <updated>2026-02-21T16:58:15+00:00</updated>
    <author>
      <name>/u/BobcatLegitimate1497</name>
      <uri>https://old.reddit.com/user/BobcatLegitimate1497</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"&gt; &lt;img alt="qwen3-coder-next on desktop" src="https://preview.redd.it/2t8c4wugnvkg1.png?width=140&amp;amp;height=112&amp;amp;auto=webp&amp;amp;s=b4c92b7d5d4f3747b4fa2068f7da52d3878b28fa" title="qwen3-coder-next on desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Old computer with low-end GPUs (RTX 4060 Ti 16 Gb and 1660 6 Gb)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2t8c4wugnvkg1.png?width=1202&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f426423c92b99569186820894b7fa3a3557b5d82"&gt;https://preview.redd.it/2t8c4wugnvkg1.png?width=1202&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f426423c92b99569186820894b7fa3a3557b5d82&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CPU half-loaded, GPU loaded about 20%. Are there any ways to optimize it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BobcatLegitimate1497"&gt; /u/BobcatLegitimate1497 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1raw6z8/qwen3codernext_on_desktop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T16:58:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb5t3c</id>
    <title>I have a substantial codebase that I want to analyse and build a proof-of-concept around for demonstration purposes</title>
    <updated>2026-02-21T23:22:44+00:00</updated>
    <author>
      <name>/u/eufemiapiccio77</name>
      <uri>https://old.reddit.com/user/eufemiapiccio77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;which local LLM options would allow me to work without the usage restrictions imposed by mainstream hosted providers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eufemiapiccio77"&gt; /u/eufemiapiccio77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb5t3c/i_have_a_substantial_codebase_that_i_want_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb5t3c/i_have_a_substantial_codebase_that_i_want_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb5t3c/i_have_a_substantial_codebase_that_i_want_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T23:22:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1rarsuo</id>
    <title>TIFU/PSA: didn‚Äôt check which GPU ollama was using and was stuck wondering why so slow</title>
    <updated>2026-02-21T13:59:43+00:00</updated>
    <author>
      <name>/u/IAmANobodyAMA</name>
      <uri>https://old.reddit.com/user/IAmANobodyAMA</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure where to tell this story (megathread?) but it made me laugh and has a teachable moment so I thought I would share.&lt;/p&gt; &lt;p&gt;TL;DR: I was running ollama on an old GPU by mistake ü§¶‚Äç‚ôÇÔ∏è&lt;/p&gt; &lt;p&gt;I have two local machines running ollama. My gaming rig has a 5070ti 16gb but isn‚Äôt always on, and my ‚Äúdedicated‚Äù unraid server **had** a 3060ti 8gb that was going to be my AI workhorse.&lt;/p&gt; &lt;p&gt;I chose models that would kick ass on the 5070 when online and more conservative models for the 3060 otherwise.&lt;/p&gt; &lt;p&gt;This is all still experimental/for learning so this setup is fines for me ‚Ä¶ except the unraid server was painfully slow. Took me way too long to figure out that‚Äôs because it was hitting an old GTX 1650 4gb card!! I forgot I swapped out the cards because I was going to build a gaming rig for my kid with the 3060.&lt;/p&gt; &lt;p&gt;I spent way too long researching models and trying to figure out why my ‚Äú3060‚Äù was offloading over 50% of qwen3:4b to my CPU. Since this is hosted on unraid I was convinced that another service (plex?) was using my GPU without permission. Nope, I‚Äôm just a doofus.&lt;/p&gt; &lt;p&gt;It wasn‚Äôt until running `nvidia-smi` in terminal that I realized my error.&lt;/p&gt; &lt;p&gt;Anyways, hope this makes someone chuckle as much as me. Anyone else have some fun ‚Äúdoh‚Äù moments?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IAmANobodyAMA"&gt; /u/IAmANobodyAMA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rarsuo/tifupsa_didnt_check_which_gpu_ollama_was_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T13:59:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb8q6p</id>
    <title>Your Model Doesn't Know What It's About to Do. Mine Does. Introducing The Cradle. (V1.0)</title>
    <updated>2026-02-22T01:33:25+00:00</updated>
    <author>
      <name>/u/BiscottiDisastrous19</name>
      <uri>https://old.reddit.com/user/BiscottiDisastrous19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rb8q6p/your_model_doesnt_know_what_its_about_to_do_mine/"&gt; &lt;img alt="Your Model Doesn't Know What It's About to Do. Mine Does. Introducing The Cradle. (V1.0)" src="https://preview.redd.it/jeeinozfmwkg1.png?width=140&amp;amp;height=58&amp;amp;auto=webp&amp;amp;s=0bb84b45e3850cd11ca230d83904e5ff34b94573" title="Your Model Doesn't Know What It's About to Do. Mine Does. Introducing The Cradle. (V1.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BiscottiDisastrous19"&gt; /u/BiscottiDisastrous19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/u_BiscottiDisastrous19/comments/1rb12vw/your_model_doesnt_know_what_its_about_to_do_mine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb8q6p/your_model_doesnt_know_what_its_about_to_do_mine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb8q6p/your_model_doesnt_know_what_its_about_to_do_mine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T01:33:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb8w1n</id>
    <title>NPU for local models?</title>
    <updated>2026-02-22T01:40:59+00:00</updated>
    <author>
      <name>/u/MemeGLS</name>
      <uri>https://old.reddit.com/user/MemeGLS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm running Qwen 3 on my laptop which has an NPU but I have no idea on how to use it (I was hoping that I could use a bigger model if I‚Äôm able to find a way to use the NPU).&lt;/p&gt; &lt;p&gt;Thanks a lot &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MemeGLS"&gt; /u/MemeGLS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb8w1n/npu_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb8w1n/npu_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb8w1n/npu_for_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T01:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1rb1wzf</id>
    <title>RX 9060 XT 16GB</title>
    <updated>2026-02-21T20:41:26+00:00</updated>
    <author>
      <name>/u/ButterscotchTop4598</name>
      <uri>https://old.reddit.com/user/ButterscotchTop4598</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Leute,&lt;/p&gt; &lt;p&gt;Gibt es irgendwo eine Anleitung f√ºr die Verwendung der oben genannten Grafikkarte unter Olama? Leider finde ich dazu nichts. Zudem w√ºrde mich interessieren welches Modell ihr f√ºr die Grafikkarte empfehlen k√∂nnt.&lt;/p&gt; &lt;p&gt;Vielen Dank f√ºr eure Hilfe!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ButterscotchTop4598"&gt; /u/ButterscotchTop4598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb1wzf/rx_9060_xt_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rb1wzf/rx_9060_xt_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rb1wzf/rx_9060_xt_16gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T20:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbha7b</id>
    <title>Unable to pull model from ollama</title>
    <updated>2026-02-22T09:12:51+00:00</updated>
    <author>
      <name>/u/badasssravikumae</name>
      <uri>https://old.reddit.com/user/badasssravikumae</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to pull models from ollama but I am unable to do &lt;/p&gt; &lt;p&gt;I did ollama serve&lt;br /&gt; I deleted the cache and checked if the models is available and tried pulling the model even though I see this error: &lt;/p&gt; &lt;p&gt;pulling manifest &lt;/p&gt; &lt;p&gt;Error: pull model manifest: file does not exist&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badasssravikumae"&gt; /u/badasssravikumae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbha7b/unable_to_pull_model_from_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T09:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbdq0m</id>
    <title>Agent architectures for SLMs</title>
    <updated>2026-02-22T05:43:51+00:00</updated>
    <author>
      <name>/u/PangolinPossible7674</name>
      <uri>https://old.reddit.com/user/PangolinPossible7674</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;What kind of agent architectures are generally used with Small Language Models? In the past, I had tried ReAct with some 8B param models, and they failed. Recently, I have been trying out tool calling models via Ollama. Even with function calling, Qwen 3 8B appears to somewhat work, but some other 8B models don't seem to be so great.&lt;/p&gt; &lt;p&gt;Therefore, I was wondering what SLM-agent gas worked for the others. Does verbose docstrings for tools affect performance with SLMs? Alternatively, what smallest model size generally allows diverse tool usage in a reliable way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PangolinPossible7674"&gt; /u/PangolinPossible7674 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rbdq0m/agent_architectures_for_slms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-22T05:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1rajqj6</id>
    <title>15,000+ tok/s on ChatJimmy: Is the "Model-on-Silicon" era finally starting?</title>
    <updated>2026-02-21T06:19:08+00:00</updated>
    <author>
      <name>/u/Significant-Topic433</name>
      <uri>https://old.reddit.com/user/Significant-Topic433</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"&gt; &lt;img alt="15,000+ tok/s on ChatJimmy: Is the &amp;quot;Model-on-Silicon&amp;quot; era finally starting?" src="https://preview.redd.it/bq69s0n5jskg1.jpg?width=140&amp;amp;height=66&amp;amp;auto=webp&amp;amp;s=fa3f690c9b529f18075dc6e27d8b984f7fcc4fcd" title="15,000+ tok/s on ChatJimmy: Is the &amp;quot;Model-on-Silicon&amp;quot; era finally starting?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôve been discussing local inference for years, but chatjimmy.ai just moved the goalposts. They are hitting 15,414 tokens per second using what they call &amp;quot;mask ROM recall fabric&amp;quot;‚Äîbasically etching the model weights directly into the silicon logic.&lt;/p&gt; &lt;p&gt;‚ÄãThis is a massive shift from our current setups. We‚Äôre used to general-purpose compute, but this is a dedicated ASIC. No HBM, no VRAM bottlenecks, just raw, hardcoded inference.&lt;/p&gt; &lt;p&gt;‚Äã I just invested in two Gigabyte AI TOP ATOM units (the ones based on the NVIDIA Spark / Grace Blackwell architecture). They are absolute beasts for training and fine-tuning with 128GB of unified memory, but seeing a dedicated chip do 15k tok/s makes me wonder: &lt;/p&gt; &lt;p&gt;‚ÄãDid I make the right call with the AI TOP Spark units for local dev, or are we going to see these specialized ASIC cards hit the market soon and make general-purpose desktop AI look like dial-up?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant-Topic433"&gt; /u/Significant-Topic433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rajqj6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rajqj6/15000_toks_on_chatjimmy_is_the_modelonsilicon_era/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-21T06:19:08+00:00</published>
  </entry>
</feed>
