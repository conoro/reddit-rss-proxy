<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-14T07:48:40+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pjbf6w</id>
    <title>ClaraVerse</title>
    <updated>2025-12-10T19:07:30+00:00</updated>
    <author>
      <name>/u/Scary_Salamander_114</name>
      <uri>https://old.reddit.com/user/Scary_Salamander_114</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone using the local hosted ClaraVerse (currently in 0.3x) . How has your experience been. I have other local-hosted LLM set-ups, but I am really intrigued by ClaraVerse's focus on privacy. I know that it is a single-DEV project, so not expecting rapid upgrades. But..if you you have used it-what are your feelings about it's potential. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Scary_Salamander_114"&gt; /u/Scary_Salamander_114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pjbf6w/claraverse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pjbf6w/claraverse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pjbf6w/claraverse/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-10T19:07:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj29u1</id>
    <title>Letting a local Ollama model judge my AI agents and it‚Äôs surprisingly usable</title>
    <updated>2025-12-10T13:14:57+00:00</updated>
    <author>
      <name>/u/hidai25</name>
      <uri>https://old.reddit.com/user/hidai25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been hacking on a little testing framework for AI agents, and I just wired it up to &lt;strong&gt;Ollama&lt;/strong&gt; so you can use a &lt;em&gt;local&lt;/em&gt; model as the judge instead of always hitting cloud APIs.&lt;/p&gt; &lt;p&gt;Basic idea: you write test cases for your agent, the tool runs them, and a model checks ‚Äúdid this response look right / use the right tools?‚Äù. Until now I was only using OpenAI; now you can point it at whatever you‚Äôve pulled in Ollama.&lt;/p&gt; &lt;p&gt;Setup is pretty simple:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;brew install ollama # or curl install for Linux ollama serve ollama pull llama3.2 pip install evalview evalview run --judge-provider ollama --judge-model llama3.2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Why I bothered doing this: I was sick of burning API credits just to tweak prompts and tools. Local judge means I can iterate tests all day without caring about tokens, my test data never leaves the machine, and it still works offline. For serious / prod evals you can still swap back to cloud models if you want.&lt;/p&gt; &lt;p&gt;Example of a test (YAML):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;name: &amp;quot;Weather agent test&amp;quot; input: query: &amp;quot;What's the weather in NYC?&amp;quot; expected: tools: - get_weather thresholds: min_score: 80 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Repo is here if you want to poke at it:&lt;br /&gt; &lt;a href="https://github.com/hidai25/eval-view"&gt;https://github.com/hidai25/eval-view&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious what people here use as a &lt;em&gt;judge&lt;/em&gt; model in Ollama. I‚Äôve been playing with &lt;code&gt;llama3.2&lt;/code&gt;, but if you‚Äôve found something that works better for grading agent outputs, I‚Äôd love to hear about your setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hidai25"&gt; /u/hidai25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pj29u1/letting_a_local_ollama_model_judge_my_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pj29u1/letting_a_local_ollama_model_judge_my_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pj29u1/letting_a_local_ollama_model_judge_my_ai_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-10T13:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjbggq</id>
    <title>Ubuntu Linux, ollama service uses CPU instead of GPU "seemingly randomly"</title>
    <updated>2025-12-10T19:08:49+00:00</updated>
    <author>
      <name>/u/BloodyIron</name>
      <uri>https://old.reddit.com/user/BloodyIron</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm still teh newb to ollama so please don't hit me with too many trouts...&lt;/p&gt; &lt;p&gt;My workstation is pretty beefy, Ryzen 9600x (with on-die GPU naturally) and RX 9070 XT.&lt;/p&gt; &lt;p&gt;I'm on Ubuntu Desktop, 25.04. Rocking ollama, and I think I have ROCm active.&lt;/p&gt; &lt;p&gt;I'm generally just using a deepseek model via CLI.&lt;/p&gt; &lt;p&gt;Seemingly at random (I haven't identified a pattern) ollama will just use my CPU instead of my GPU, until I restart the ollama service.&lt;/p&gt; &lt;p&gt;Anyone have any advice on what I can do about this? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BloodyIron"&gt; /u/BloodyIron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pjbggq/ubuntu_linux_ollama_service_uses_cpu_instead_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pjbggq/ubuntu_linux_ollama_service_uses_cpu_instead_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pjbggq/ubuntu_linux_ollama_service_uses_cpu_instead_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-10T19:08:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk0h7a</id>
    <title>Ollama connection abortedd</title>
    <updated>2025-12-11T15:34:47+00:00</updated>
    <author>
      <name>/u/SantiagoEtcheberrito</name>
      <uri>https://old.reddit.com/user/SantiagoEtcheberrito</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a server with a powerful video card dedicated to AI.&lt;/p&gt; &lt;p&gt;I am making connections with n8n, but when I run the flows, it keeps thinking and thinking for long minutes until I get this error: The connection was aborted, perhaps the server is offline [item 0].&lt;/p&gt; &lt;p&gt;I'm trying to run Qwen3:14b models, which are models that should support my 32GB VRAM. Does anyone have any idea what might be happening?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SantiagoEtcheberrito"&gt; /u/SantiagoEtcheberrito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pk0h7a/ollama_connection_abortedd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pk0h7a/ollama_connection_abortedd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pk0h7a/ollama_connection_abortedd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T15:34:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjxz8a</id>
    <title>Anthropic claims to have solved the AI Memory problem for Agents</title>
    <updated>2025-12-11T13:49:55+00:00</updated>
    <author>
      <name>/u/Far-Photo4379</name>
      <uri>https://old.reddit.com/user/Far-Photo4379</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pjxz8a/anthropic_claims_to_have_solved_the_ai_memory/"&gt; &lt;img alt="Anthropic claims to have solved the AI Memory problem for Agents" src="https://external-preview.redd.it/5UymBFDYHaFKRZHHNo_0E-ClIRE2tai814BUmhhXnlQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=316fa4774949d65d4f2ba50b1d837047c8b98c80" title="Anthropic claims to have solved the AI Memory problem for Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Photo4379"&gt; /u/Far-Photo4379 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pjxz8a/anthropic_claims_to_have_solved_the_ai_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pjxz8a/anthropic_claims_to_have_solved_the_ai_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T13:49:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjpbg5</id>
    <title>Darkmode website please.</title>
    <updated>2025-12-11T05:14:39+00:00</updated>
    <author>
      <name>/u/Maltz42</name>
      <uri>https://old.reddit.com/user/Maltz42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That is all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maltz42"&gt; /u/Maltz42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pjpbg5/darkmode_website_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pjpbg5/darkmode_website_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pjpbg5/darkmode_website_please/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T05:14:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk4fcw</id>
    <title>Llm locally</title>
    <updated>2025-12-11T18:06:48+00:00</updated>
    <author>
      <name>/u/Lucky-Divide-2633</name>
      <uri>https://old.reddit.com/user/Lucky-Divide-2633</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Better to run llm locally on two mac mini m4 16gb each or one mac mini m4 pro with 24 gb ram? Any tips? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lucky-Divide-2633"&gt; /u/Lucky-Divide-2633 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pk4fcw/llm_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pk4fcw/llm_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pk4fcw/llm_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T18:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkbtqh</id>
    <title>Local project</title>
    <updated>2025-12-11T23:03:29+00:00</updated>
    <author>
      <name>/u/BackUpBiii</name>
      <uri>https://old.reddit.com/user/BackUpBiii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please check out my GitHub it features a full ml ide that uses custom made local models and normal local models hugging face models and gguf!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ItsMehRAWRXD/RawrXD/tree/production-lazy-init"&gt;https://github.com/ItsMehRAWRXD/RawrXD/tree/production-lazy-init&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I need as much feedback as possible! Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BackUpBiii"&gt; /u/BackUpBiii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkbtqh/local_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkbtqh/local_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkbtqh/local_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T23:03:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkbvjs</id>
    <title>How do you eject a model in the Ollama GUI?</title>
    <updated>2025-12-11T23:05:44+00:00</updated>
    <author>
      <name>/u/Lacooooo</name>
      <uri>https://old.reddit.com/user/Lacooooo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When using Ollama with the GUI, how can you unload or stop a model‚Äîsimilar to running &lt;code&gt;ollama stop &amp;lt;model-name&amp;gt;&lt;/code&gt; in the terminal‚Äîwithout using the terminal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lacooooo"&gt; /u/Lacooooo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkbvjs/how_do_you_eject_a_model_in_the_ollama_gui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkbvjs/how_do_you_eject_a_model_in_the_ollama_gui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkbvjs/how_do_you_eject_a_model_in_the_ollama_gui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T23:05:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk7chx</id>
    <title>Same Hardware, but Linux 5√ó Slower Than Windows? What's Going On?</title>
    <updated>2025-12-11T20:00:23+00:00</updated>
    <author>
      <name>/u/Al1x-ai</name>
      <uri>https://old.reddit.com/user/Al1x-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm working on an open-source speech‚Äëto‚Äëtext project called Murmure. It includes a new feature that uses Ollama to refine or transform the transcription produced by an ASR model.&lt;/p&gt; &lt;p&gt;To do this, I call Ollama‚Äôs API with models like ministral‚Äë3 or Qwen‚Äë3, and while running tests on the software, I noticed something surprising.&lt;/p&gt; &lt;p&gt;On Windows, the model response time is very fast (under 1-2 seconds), but on Linux Mint, using the exact same hardware (i5‚Äë13600KF and an Nvidia GeForce RTX 4070), the same operation easily takes 6-7 seconds on the same short audio.&lt;/p&gt; &lt;p&gt;It doesn‚Äôt seem to be a model‚Äëloading issue (I‚Äôm warming up the models in both cases, so the slowdown isn‚Äôt related to the initial load.), and the drivers look fine (inxi -G):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Device-1: NVIDIA AD104 [GeForce RTX 4070] driver: nvidia v: 580.95.05 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Ollama is also definitely using the GPU:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ministral-3:latest a5e54193fd34 16 GB 32%/68% CPU/GPU 4096 3 minutes from now &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm not sure what's causing this difference. Are any other Linux users experiencing the same slowdown compared to Windows? And if so, is there a known way to fix it or at least understand where the bottleneck comes from?&lt;/p&gt; &lt;p&gt;EDIT 1:&lt;br /&gt; On Windows:&lt;/p&gt; &lt;p&gt;&lt;code&gt; ministral-3:latest a5e54193fd34 7.5 GB 100% GPU 4096 4 minutes from now &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Same model, same hardware, but on Windows &lt;strong&gt;it runs 100% on GPU&lt;/strong&gt;, unlike on Linux and size is not the same at all.&lt;/p&gt; &lt;p&gt;EDIT 2 (SOLVED) : Updating Ollama from 0.13.1 to 0.13.3 fixed the issue, the models now have the correct sizes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Al1x-ai"&gt; /u/Al1x-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pk7chx/same_hardware_but_linux_5_slower_than_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pk7chx/same_hardware_but_linux_5_slower_than_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pk7chx/same_hardware_but_linux_5_slower_than_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T20:00:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkwzon</id>
    <title>Help me outt</title>
    <updated>2025-12-12T16:58:17+00:00</updated>
    <author>
      <name>/u/Ok_Cap3333</name>
      <uri>https://old.reddit.com/user/Ok_Cap3333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys i am new in here, I need ai model that is uncensored and unrestricted, help me out how do I find one?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Cap3333"&gt; /u/Ok_Cap3333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkwzon/help_me_outt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkwzon/help_me_outt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkwzon/help_me_outt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T16:58:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk1l6w</id>
    <title>Introducing TreeThinkerAgent: A Lightweight Autonomous Reasoning Agent for LLMs</title>
    <updated>2025-12-11T16:17:59+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pk1l6w/introducing_treethinkeragent_a_lightweight/"&gt; &lt;img alt="Introducing TreeThinkerAgent: A Lightweight Autonomous Reasoning Agent for LLMs" src="https://external-preview.redd.it/eHI2cjY1YzdvbDZnMbDWimRkn0jd8-nhfiwI9sUY7kTZ1aYtNE_bmUfANVp8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5a78127f6b07fdaf0ec1e187c532661354ded20" title="Introducing TreeThinkerAgent: A Lightweight Autonomous Reasoning Agent for LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone ! I‚Äôm excited to share my latest project: &lt;strong&gt;TreeThinkerAgent&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It‚Äôs an open-source orchestration layer that &lt;strong&gt;turns any Large Language Model into an autonomous, multi-step reasoning agent&lt;/strong&gt;, built entirely from scratch &lt;strong&gt;without any framework&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Try it locally using your favourite Ollama model.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Bessouat40/TreeThinkerAgent?utm_source=chatgpt.com"&gt;https://github.com/Bessouat40/TreeThinkerAgent&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What it does&lt;/h1&gt; &lt;p&gt;TreeThinkerAgent helps you:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Build a reasoning tree&lt;/strong&gt; so that every decision is structured and traceable&lt;br /&gt; - Turn an LLM into a &lt;strong&gt;multi-step planner and executor&lt;/strong&gt;&lt;br /&gt; - Perform &lt;strong&gt;step-by-step reasoning&lt;/strong&gt; with tool support&lt;br /&gt; - Execute complex tasks by planning and following through independently&lt;/p&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;p&gt;Most LLM interactions are ‚Äúone shot‚Äù: you ask a question and get an answer.&lt;/p&gt; &lt;p&gt;But many real-world problems require higher-level thinking: planning, decomposing into steps, and using tools like web search. TreeThinkerAgent tackles exactly that by making the reasoning process explicit and autonomous.&lt;/p&gt; &lt;p&gt;Check it out and let me know what you think. Your feedback, feature ideas, or improvements are more than welcome.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Bessouat40/TreeThinkerAgent?utm_source=chatgpt.com"&gt;https://github.com/Bessouat40/TreeThinkerAgent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h4k3w3b7ol6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pk1l6w/introducing_treethinkeragent_a_lightweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pk1l6w/introducing_treethinkeragent_a_lightweight/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-11T16:17:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkfv0i</id>
    <title>In OllaMan, using the Qwen3-Next model</title>
    <updated>2025-12-12T02:08:46+00:00</updated>
    <author>
      <name>/u/ComfyTightwad</name>
      <uri>https://old.reddit.com/user/ComfyTightwad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pkfv0i/in_ollaman_using_the_qwen3next_model/"&gt; &lt;img alt="In OllaMan, using the Qwen3-Next model" src="https://external-preview.redd.it/emRicXR0ZG9sbzZnMdW2ajc4pd0IkwIxB2RsxHZaHcmU2fwx0RdQoBvo_mLe.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb5c8330f13bf10d85a8aafb9df92bf8714eddbc" title="In OllaMan, using the Qwen3-Next model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComfyTightwad"&gt; /u/ComfyTightwad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cflbcgdolo6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkfv0i/in_ollaman_using_the_qwen3next_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkfv0i/in_ollaman_using_the_qwen3next_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T02:08:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkn6bl</id>
    <title>How to get rid of rendering glitches in browser</title>
    <updated>2025-12-12T08:52:09+00:00</updated>
    <author>
      <name>/u/Hot-Finger3903</name>
      <uri>https://old.reddit.com/user/Hot-Finger3903</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I recently installed ollama upon my device which has Intel iris xe ... And I noticed some issues concerned with mouse click and rendering any way to overcome this!?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hot-Finger3903"&gt; /u/Hot-Finger3903 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkn6bl/how_to_get_rid_of_rendering_glitches_in_browser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkn6bl/how_to_get_rid_of_rendering_glitches_in_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkn6bl/how_to_get_rid_of_rendering_glitches_in_browser/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T08:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pl6mgg</id>
    <title>Crypto Bot</title>
    <updated>2025-12-12T23:34:50+00:00</updated>
    <author>
      <name>/u/Fantastic_Active9334</name>
      <uri>https://old.reddit.com/user/Fantastic_Active9334</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pl6mgg/crypto_bot/"&gt; &lt;img alt="Crypto Bot" src="https://external-preview.redd.it/cSi8uSxKAQ292UK5DvDBWv7Qq0EtCv5xhgeNsqgPq-o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63389a644987b7a2e8e73505741fa7c1e6f15a73" title="Crypto Bot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;TLDR;&lt;/p&gt; &lt;p&gt;I wrote an open-source crypto trading bot. It actively manages positions, trades and activity in the market checking recent news and determining if it should wait for a better time to trade in market or act now.&lt;/p&gt; &lt;p&gt;‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî&lt;/p&gt; &lt;p&gt;It‚Äôs thinking logic is dictated by an LLM, it uses tavily search for browsing the web and integrates directly with Alpacas API to manage a portfolio actively. It checks periodically by determining the next best time to check the news and portfolio -&amp;gt; gives a probability score, based on determined sentiment as well as a brief summary of how it views the market before taking another step with the tools it‚Äôs been provided. Currently prompt has been predefined for solana and a claude model is the default but this can be changed easily simply by switching whether it be an open-source llm on Ollama or a closed-source model like Claude.&lt;/p&gt; &lt;p&gt;sqlite is used for state management, and it can be deployed using docker or purely locally. &lt;/p&gt; &lt;p&gt;Code is complete free to use if you have any ideas on how to improve and make it better - just message me or create a PR&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic_Active9334"&gt; /u/Fantastic_Active9334 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/GB153/AlpacaAgent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pl6mgg/crypto_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pl6mgg/crypto_bot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T23:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkn0j5</id>
    <title>Europe's devstral-small-2, available in the ollama library, looks promising</title>
    <updated>2025-12-12T08:40:23+00:00</updated>
    <author>
      <name>/u/New_Cranberry_6451</name>
      <uri>https://old.reddit.com/user/New_Cranberry_6451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share with you that yesterday I tested the devstral-small-2 model and it surprised me for good. With 24B params, it runs at 2,5 tokens per sec on my 8Gb VRAM laptop on Windows. (GPT-OSS, with 20B, runs at 20 tokens per sec, don't know how they do it...).&lt;/p&gt; &lt;p&gt;Despite this substantial performance difference, the quality of the answers is very high in my opinion, obtaining great results with simple prompts, and working great on instruction processing and system prompt following.&lt;/p&gt; &lt;p&gt;I am very happy, give it a try and tell me what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Cranberry_6451"&gt; /u/New_Cranberry_6451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkn0j5/europes_devstralsmall2_available_in_the_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkn0j5/europes_devstralsmall2_available_in_the_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkn0j5/europes_devstralsmall2_available_in_the_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T08:40:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkycev</id>
    <title>üöÄ New: Olmo 3.1 Think 32B &amp; Olmo 3.1 Instruct 32B</title>
    <updated>2025-12-12T17:50:44+00:00</updated>
    <author>
      <name>/u/Glittering-Fish3178</name>
      <uri>https://old.reddit.com/user/Glittering-Fish3178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pkycev/new_olmo_31_think_32b_olmo_31_instruct_32b/"&gt; &lt;img alt="üöÄ New: Olmo 3.1 Think 32B &amp;amp; Olmo 3.1 Instruct 32B" src="https://preview.redd.it/37012o2u5t6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6b659c4d8abf3b542adbe99af324e1df0c0bbcc" title="üöÄ New: Olmo 3.1 Think 32B &amp;amp; Olmo 3.1 Instruct 32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Fish3178"&gt; /u/Glittering-Fish3178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/37012o2u5t6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkycev/new_olmo_31_think_32b_olmo_31_instruct_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkycev/new_olmo_31_think_32b_olmo_31_instruct_32b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T17:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkvopv</id>
    <title>AI Agent from scratch: Django + Ollama + Pydantic AI - A Step-by-Step Guide</title>
    <updated>2025-12-12T16:07:00+00:00</updated>
    <author>
      <name>/u/tom-mart</name>
      <uri>https://old.reddit.com/user/tom-mart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone, &lt;/p&gt; &lt;p&gt;I just published Part 2 of the article series, which dives deep into creating a multi-layered memory system.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The agent has:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Short-term memory&lt;/strong&gt; for the current chat (with auto-pruning).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-term memory&lt;/strong&gt; using &lt;code&gt;pgvector&lt;/code&gt; to find relevant info from past conversations (RAG).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Summarization&lt;/strong&gt; to create condensed memories of old chats.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structured Memory&lt;/strong&gt; using tools to save/retrieve data from a Django model (I used a fitness tracker as an example).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Django &amp;amp; Django Ninja&lt;/li&gt; &lt;li&gt;Ollama (to run models like Llama 3 or Gemma locally)&lt;/li&gt; &lt;li&gt;Pydantic AI (for agent logic and tools)&lt;/li&gt; &lt;li&gt;PostgreSQL + &lt;code&gt;pgvector&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's a step-by-step guide meant to be easy to follow. I tried to explain the &amp;quot;why&amp;quot; behind the design, not just the &amp;quot;how.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;You can read the full article here:&lt;/strong&gt; &lt;a href="https://medium.com/@tom.mart/build-self-hosted-ai-agent-with-ollama-pydantic-ai-and-django-ninja-65214a3afb35"&gt;https://medium.com/@tom.mart/build-self-hosted-ai-agent-with-ollama-pydantic-ai-and-django-ninja-65214a3afb35&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The full code is on GitHub if you just want to browse. Happy to answer any questions!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/tom-mart/ai-agent"&gt;https://github.com/tom-mart/ai-agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tom-mart"&gt; /u/tom-mart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkvopv/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkvopv/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkvopv/ai_agent_from_scratch_django_ollama_pydantic_ai_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T16:07:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkzlxq</id>
    <title>I turned my computer into a war room. Quorum: A CLI for local model debates (Ollama zero-config)</title>
    <updated>2025-12-12T18:40:13+00:00</updated>
    <author>
      <name>/u/C12H16N2HPO4</name>
      <uri>https://old.reddit.com/user/C12H16N2HPO4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone.&lt;/p&gt; &lt;p&gt;I got tired of manually copy-pasting prompts between local &lt;strong&gt;Llama 4&lt;/strong&gt; and Mistral to verify facts, so I built &lt;strong&gt;Quorum&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It‚Äôs a CLI tool that orchestrates debates between 2‚Äì6 models. You can mix and match‚Äîfor example, have your local &lt;strong&gt;Llama 4&lt;/strong&gt; argue against &lt;strong&gt;GPT-5.2&lt;/strong&gt;, or run a fully offline debate.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features for this sub:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ollama Auto-discovery:&lt;/strong&gt; It detects your local models automatically. No config files or YAML hell.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;7 Debate Methods:&lt;/strong&gt; Includes &amp;quot;Oxford Debate&amp;quot; (For/Against), &amp;quot;Devil's Advocate&amp;quot;, and &amp;quot;Delphi&amp;quot; (consensus building).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Privacy:&lt;/strong&gt; Local-first. Your data stays on your rig unless you explicitly add an API model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Heads-up:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;VRAM Warning:&lt;/strong&gt; Running multiple simultaneous &lt;strong&gt;405B or 70B&lt;/strong&gt; models will eat your VRAM for breakfast. Make sure your hardware can handle the concurrency.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; It‚Äôs BSL 1.1. It‚Äôs free for personal/internal use, but stops cloud corps from reselling it as a SaaS. Just wanted to be upfront about that.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/Detrol/quorum-cli"&gt;https://github.com/Detrol/quorum-cli&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Install:&lt;/strong&gt; &lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/Detrol/quorum-cli.git"&gt;&lt;code&gt;https://github.com/Detrol/quorum-cli.git&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if the auto-discovery works on your specific setup!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/C12H16N2HPO4"&gt; /u/C12H16N2HPO4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkzlxq/i_turned_my_computer_into_a_war_room_quorum_a_cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pkzlxq/i_turned_my_computer_into_a_war_room_quorum_a_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pkzlxq/i_turned_my_computer_into_a_war_room_quorum_a_cli/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-12T18:40:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1plmm5h</id>
    <title>ollama on mac m1 has bugÔºå i dont know how to run?</title>
    <updated>2025-12-13T14:27:05+00:00</updated>
    <author>
      <name>/u/Worldly-Badger-937</name>
      <uri>https://old.reddit.com/user/Worldly-Badger-937</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Error: 500 Internal Server Error: model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details&lt;/p&gt; &lt;p&gt;(base) sun2022@sun2022deMacBook-Pro ~ % ollama run qwen2.5:7b&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; Send a message (/? for help)Ôºåi try to use deepseek and qwenÔºå and they are all not to be run.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worldly-Badger-937"&gt; /u/Worldly-Badger-937 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1plmm5h/ollama_on_mac_m1_has_bug_i_dont_know_how_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1plmm5h/ollama_on_mac_m1_has_bug_i_dont_know_how_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1plmm5h/ollama_on_mac_m1_has_bug_i_dont_know_how_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-13T14:27:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pld98r</id>
    <title>HTML BASED UI for Ollama Models and Other Local Models. Because I Respect Privacy.</title>
    <updated>2025-12-13T05:05:12+00:00</updated>
    <author>
      <name>/u/Cummanaati</name>
      <uri>https://old.reddit.com/user/Cummanaati</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TBH, I used AI Vibecoding to make this Entire UI but atleast it is useful and not complicated to setup and it doesn't need a dedicated server or anything like that. Atleast this is not a random ai slop though. I made this for people to utilize offline models at ease and that's all. Hope y'all like it and i would apprecitae if u star my github repository. &lt;/p&gt; &lt;p&gt;Note: As a Privacy Enthusiast myself, there is no telemetry other than the google fonts lol, there's no ads or nothing related to monetization. I made this app out of passion and boredom ofcourse lmao.&lt;/p&gt; &lt;p&gt;Adiyos gang : )&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/one-man-studios/Shinzo-UI"&gt;https://github.com/one-man-studios/Shinzo-UI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cummanaati"&gt; /u/Cummanaati &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pld98r/html_based_ui_for_ollama_models_and_other_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pld98r/html_based_ui_for_ollama_models_and_other_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pld98r/html_based_ui_for_ollama_models_and_other_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-13T05:05:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1plv5xv</id>
    <title>A Brief Primer on Embeddings - Intuition, History &amp; Their Role in LLMs</title>
    <updated>2025-12-13T20:28:25+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1plv5xv/a_brief_primer_on_embeddings_intuition_history/"&gt; &lt;img alt="A Brief Primer on Embeddings - Intuition, History &amp;amp; Their Role in LLMs" src="https://external-preview.redd.it/wDzZFd-8OO5EoBmlfyxqfNu6wZlvgh0Jd9Ml6gS39fY.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=975e62061a08bdadf708e9ca8fa0c0fe8e4d0c93" title="A Brief Primer on Embeddings - Intuition, History &amp;amp; Their Role in LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/Cv5kSs2Jcu4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1plv5xv/a_brief_primer_on_embeddings_intuition_history/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1plv5xv/a_brief_primer_on_embeddings_intuition_history/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-13T20:28:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1plgvss</id>
    <title>I stopped using the Prompt Engineering manual. Quick guide to setting up a Local RAG with Python and Ollama (Code included)</title>
    <updated>2025-12-13T08:49:59+00:00</updated>
    <author>
      <name>/u/jokiruiz</name>
      <uri>https://old.reddit.com/user/jokiruiz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd been frustrated for a while with the context limitations of ChatGPT and the privacy issues. I started investigating and realized that traditional Prompt Engineering is a workaround. The real solution is RAG (Retrieval-Augmented Generation).&lt;/p&gt; &lt;p&gt;I've put together a simple Python script (less than 30 lines) to chat with my PDF documents/websites using Ollama (Llama 3) and LangChain. It all runs locally and is free.&lt;/p&gt; &lt;p&gt;The Stack: Python + LangChain Llama (Inference Engine) ChromaDB (Vector Database)&lt;/p&gt; &lt;p&gt;If you're interested in seeing a step-by-step explanation and how to install everything from scratch, I've uploaded a visual tutorial here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/sj1yzbXVXM0?si=oZnmflpHWqoCBnjr"&gt;https://youtu.be/sj1yzbXVXM0?si=oZnmflpHWqoCBnjr&lt;/a&gt; I've also uploaded the Gist to GitHub: &lt;a href="https://gist.github.com/JoaquinRuiz/e92bbf50be2dffd078b57febb3d961b2"&gt;https://gist.github.com/JoaquinRuiz/e92bbf50be2dffd078b57febb3d961b2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is anyone else tinkering with Llama 3 locally? How's the performance for you?&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jokiruiz"&gt; /u/jokiruiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1plgvss/i_stopped_using_the_prompt_engineering_manual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1plgvss/i_stopped_using_the_prompt_engineering_manual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1plgvss/i_stopped_using_the_prompt_engineering_manual/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-13T08:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1plbnq1</id>
    <title>Ollama now supports Mistral AI‚Äôs Devstral 2 models</title>
    <updated>2025-12-13T03:39:40+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1plbnq1/ollama_now_supports_mistral_ais_devstral_2_models/"&gt; &lt;img alt="Ollama now supports Mistral AI‚Äôs Devstral 2 models" src="https://preview.redd.it/6rvjqky86w6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a03bcaf3abf5411f898c4e4855e36735885785b" title="Ollama now supports Mistral AI‚Äôs Devstral 2 models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6rvjqky86w6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1plbnq1/ollama_now_supports_mistral_ais_devstral_2_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1plbnq1/ollama_now_supports_mistral_ais_devstral_2_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-13T03:39:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm5mg9</id>
    <title>Odd Ollama behavior on Strix Halo</title>
    <updated>2025-12-14T04:52:46+00:00</updated>
    <author>
      <name>/u/arlaneenalra</name>
      <uri>https://old.reddit.com/user/arlaneenalra</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed that anytime Ollama gets to or exceeds somewhere around 25~32k tokens in context the model I'm using starts repeating itself over and over again. It doesn't seem to matter what model I'm using or what the context is set to (I'm usually using 131072, roughly 128k and models that are know to handle that context size like ministral-3:14b, llama3.3:70b, qwen3:32b, qwen3-next:80b, etc. It doesn't seem to matter which one I use.&lt;/p&gt; &lt;p&gt;Any suggestions on what to try?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arlaneenalra"&gt; /u/arlaneenalra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pm5mg9/odd_ollama_behavior_on_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pm5mg9/odd_ollama_behavior_on_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pm5mg9/odd_ollama_behavior_on_strix_halo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-14T04:52:46+00:00</published>
  </entry>
</feed>
