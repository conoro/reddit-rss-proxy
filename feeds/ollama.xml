<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-06T05:49:02+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ontvwe</id>
    <title>Claudette Mini - 1.0.0 for quantized models</title>
    <updated>2025-11-04T01:04:10+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/GithubCopilot/comments/1on3dh3/claudette_mini_100_for_quantized_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ontvwe/claudette_mini_100_for_quantized_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ontvwe/claudette_mini_100_for_quantized_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-04T01:04:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1onlte5</id>
    <title>First LangFlow Flow Official Release - Elephant v1.0</title>
    <updated>2025-11-03T19:44:46+00:00</updated>
    <author>
      <name>/u/LoserLLM</name>
      <uri>https://old.reddit.com/user/LoserLLM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I started a YouTube channel a few weeks ago called LoserLLM. The goal of the channel is to teach others how they can download and host open source models on their own hardware using only two tools; LM Studio and LangFlow.&lt;/p&gt; &lt;p&gt;Last night I completed my first goal with an open source LangFlow flow. It has custom components for accessing the file system, using Playwright to access the internet, and a code runner component for running code, including bash commands.&lt;/p&gt; &lt;p&gt;Here is the video which also contains the link to download the flow that can then be imported:&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/qhJUEVHvYQo?si=-pvLI-YCQP0p9ggM"&gt;Official Flow Release: Elephant v1.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you have any ideas for future flows or have a prompt you'd like me to run through the flow. I will make a video about the first 5 prompts that people share with results.&lt;/p&gt; &lt;p&gt;Link directly to the flow on Google Drive: &lt;a href="https://drive.google.com/file/d/1HgDRiReQDdU3R2xMYzYv7UL6Cwbhzhuf/view?usp=sharing"&gt;https://drive.google.com/file/d/1HgDRiReQDdU3R2xMYzYv7UL6Cwbhzhuf/view?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoserLLM"&gt; /u/LoserLLM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1onlte5/first_langflow_flow_official_release_elephant_v10/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1onlte5/first_langflow_flow_official_release_elephant_v10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1onlte5/first_langflow_flow_official_release_elephant_v10/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-03T19:44:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1on99fw</id>
    <title>Voice-to-AI app with Whisper transcription, Ollama AI integration, and TTS</title>
    <updated>2025-11-03T11:28:12+00:00</updated>
    <author>
      <name>/u/crhylove3</name>
      <uri>https://old.reddit.com/user/crhylove3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's an early beta, but it works well for me on Linux Mint. Kick the tires and let me know how it goes! The Linux release is still building, but Mac and Windows should be up already!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crhylove3"&gt; /u/crhylove3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1on99fw/voicetoai_app_with_whisper_transcription_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1on99fw/voicetoai_app_with_whisper_transcription_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1on99fw/voicetoai_app_with_whisper_transcription_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-03T11:28:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1onzq5k</id>
    <title>[Project] I built a small Python tool to track how your directories get messy (and clean again)</title>
    <updated>2025-11-04T05:55:12+00:00</updated>
    <author>
      <name>/u/VegetableSense</name>
      <uri>https://old.reddit.com/user/VegetableSense</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VegetableSense"&gt; /u/VegetableSense &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1onzq5k/project_i_built_a_small_python_tool_to_track_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1onzq5k/project_i_built_a_small_python_tool_to_track_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-04T05:55:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo95yl</id>
    <title>Why AI Memory Is So Hard to Build</title>
    <updated>2025-11-04T14:51:12+00:00</updated>
    <author>
      <name>/u/Far-Photo4379</name>
      <uri>https://old.reddit.com/user/Far-Photo4379</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Photo4379"&gt; /u/Far-Photo4379 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AIMemory/comments/1oo3ybf/why_ai_memory_is_so_hard_to_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oo95yl/why_ai_memory_is_so_hard_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oo95yl/why_ai_memory_is_so_hard_to_build/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-04T14:51:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo4695</id>
    <title>Ideal size of llm to make</title>
    <updated>2025-11-04T10:41:55+00:00</updated>
    <author>
      <name>/u/MoreIndependent5967</name>
      <uri>https://old.reddit.com/user/MoreIndependent5967</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoreIndependent5967"&gt; /u/MoreIndependent5967 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1oo461u/ideal_size_of_llm_to_make/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oo4695/ideal_size_of_llm_to_make/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oo4695/ideal_size_of_llm_to_make/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-04T10:41:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo0440</id>
    <title>Open-webui not showing any models</title>
    <updated>2025-11-04T06:18:11+00:00</updated>
    <author>
      <name>/u/statsom</name>
      <uri>https://old.reddit.com/user/statsom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying to fix this for HOURS and I've yet to find a solution. I installed ollama, and open-webui in docker on linux mint (cinnamon), but after going to localhost:3000 it shows no models.&lt;/p&gt; &lt;p&gt;I've uninstalled everything and reinstalled it multiple times, changed ports on-and-on, and looked at so many forums and documentation. PLEASE HELP ME&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/statsom"&gt; /u/statsom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oo0440/openwebui_not_showing_any_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oo0440/openwebui_not_showing_any_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oo0440/openwebui_not_showing_any_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-04T06:18:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo7wp6</id>
    <title>Is z.AI MCPsless on Lite plan??</title>
    <updated>2025-11-04T14:01:16+00:00</updated>
    <author>
      <name>/u/East_Standard8864</name>
      <uri>https://old.reddit.com/user/East_Standard8864</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/East_Standard8864"&gt; /u/East_Standard8864 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oo7uv1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oo7wp6/is_zai_mcpsless_on_lite_plan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oo7wp6/is_zai_mcpsless_on_lite_plan/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-04T14:01:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1oniap8</id>
    <title>We trained SLM-powered assistants for personal expenses summaries that you can run locally via Ollama.</title>
    <updated>2025-11-03T17:38:19+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oniap8/we_trained_slmpowered_assistants_for_personal/"&gt; &lt;img alt="We trained SLM-powered assistants for personal expenses summaries that you can run locally via Ollama." src="https://preview.redd.it/kke8wd12w2zf1.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db056662492b7bd150e7f6fce4e0fb007e663c30" title="We trained SLM-powered assistants for personal expenses summaries that you can run locally via Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We trained SLM assistants for personal expenses summaries - two Llama 3.2 models (1B and 3B parameters) that you can run &lt;em&gt;locally&lt;/em&gt; via Ollama! SLMs which are not finetuned perform poorly on function calling - on our demo task, the 3B model called the correct tool only in 24% cases. By comparison, GPT-OSS was correct 88% of the time. Our knowledge distillation and fine-tuning setup bridges this performance gap between SLMs and LLMs. Details in &lt;a href="https://github.com/distil-labs/Distil-expenses"&gt;https://github.com/distil-labs/Distil-expenses&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;1. Installation&lt;/h3&gt; &lt;p&gt;First, install &lt;a href="https://ollama.com"&gt;Ollama&lt;/a&gt;, following the instructions on their website.&lt;/p&gt; &lt;p&gt;Then set up the virtual environment: &lt;code&gt; python -m venv .venv . .venv/bin/activate pip install huggingface_hub pandas openai &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Available models hosted on huggingface: - &lt;a href="https://huggingface.co/distil-labs/Distil-expenses-Llama-3.2-3B-Instruct"&gt;distil-labs/Distil-expenses-Llama-3.2-3B-Instruct&lt;/a&gt; - &lt;a href="https://huggingface.co/distil-labs/Distil-expenses-Llama-3.2-1B-Instruct"&gt;distil-labs/Distil-expenses-Llama-3.2-1B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Finally, download the models from huggingface and build them locally: ``` hf download distil-labs/Distil-expenses-Llama-3.2-3B-Instruct --local-dir distil-model&lt;/p&gt; &lt;p&gt;cd distil-model ollama create expense_llama3.2 -f Modelfile ```&lt;/p&gt; &lt;h3&gt;2. Examples&lt;/h3&gt; &lt;p&gt;Sum: ``` What was my total spending on dining in January 2024?&lt;/p&gt; &lt;h2&gt;ANSWER: From 2024-01-01 to 2024-01-31 you spent 24.5 total on dining.&lt;/h2&gt; &lt;p&gt;Give me my total expenses from 5th February to 11th March 2024&lt;/p&gt; &lt;h2&gt;ANSWER: From 2024-02-05 to 2024-03-11 you spent 348.28 total.&lt;/h2&gt; &lt;p&gt;&lt;code&gt; Count: &lt;/code&gt; How many times did I go shopping over $100 in 2024?&lt;/p&gt; &lt;h2&gt;ANSWER: From 2024-01-01 to 2024-12-31 you spent 8 times over 100 on shopping.&lt;/h2&gt; &lt;p&gt;Count all my shopping under $100 in the first half of 2024&lt;/p&gt; &lt;h2&gt;ANSWER: From 2024-01-01 to 2024-06-30 you spent 6 times under 100 on shopping.&lt;/h2&gt; &lt;p&gt;```&lt;/p&gt; &lt;h3&gt;3. Fine-tuning setup&lt;/h3&gt; &lt;p&gt;The tuned models were trained using knowledge distillation, leveraging the teacher model GPT-OSS 120B. We used 24 train examples and complemented them with 2500 synthetic examples.&lt;/p&gt; &lt;p&gt;We compare the teacher model and both student models on 25 held-out test examples:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Correct (25)&lt;/th&gt; &lt;th&gt;Tool call accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;GPT-OSS&lt;/td&gt; &lt;td&gt;22&lt;/td&gt; &lt;td&gt;0.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama3.2 3B (tuned)&lt;/td&gt; &lt;td&gt;21&lt;/td&gt; &lt;td&gt;0.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama3.2 1B (tuned)&lt;/td&gt; &lt;td&gt;22&lt;/td&gt; &lt;td&gt;0.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama3.2 3B (base)&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;0.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama3.2 1B (base)&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The training config file and train/test data splits are available under &lt;code&gt;data/&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;FAQ&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Q: Why don't we just use Llama3.X yB for this??&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We focus on small models (&amp;lt; 8B parameters), and these make errors when used out of the box (see 5.)&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Q: The model does not work as expected&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: The tool calling on our platform is in active development! &lt;a href="https://www.linkedin.com/company/distil-labs/"&gt;Follow us on LinkedIn&lt;/a&gt; for updates, or &lt;a href="https://join.slack.com/t/distil-labs-community/shared_invite/zt-36zqj87le-i3quWUn2bjErRq22xoE58g"&gt;join our community&lt;/a&gt;. You can also try to rephrase your query.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Q: I want to use tool calling for my use-case&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: Visit our &lt;a href="https://www.distillabs.ai"&gt;website&lt;/a&gt; and reach out to us, we offer custom solutions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kke8wd12w2zf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oniap8/we_trained_slmpowered_assistants_for_personal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oniap8/we_trained_slmpowered_assistants_for_personal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-03T17:38:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooi2my</id>
    <title>Has anyone tested ollama on Whisplay HAT with Raspberry pi zero 2W?</title>
    <updated>2025-11-04T20:17:45+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Nwu2DruSuyI"&gt;https://www.youtube.com/watch?v=Nwu2DruSuyI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/PiSugar/whisplay-ai-chatbot"&gt;https://github.com/PiSugar/whisplay-ai-chatbot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ooi2my/has_anyone_tested_ollama_on_whisplay_hat_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ooi2my/has_anyone_tested_ollama_on_whisplay_hat_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ooi2my/has_anyone_tested_ollama_on_whisplay_hat_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-04T20:17:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oop0t7</id>
    <title>Built my own IDE</title>
    <updated>2025-11-05T00:57:21+00:00</updated>
    <author>
      <name>/u/BackUpBiii</name>
      <uri>https://old.reddit.com/user/BackUpBiii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ItsMehRAWRXD?tab=repositories"&gt;https://github.com/ItsMehRAWRXD?tab=repositories&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That‚Äôs my repo and you can use your ollama models! I‚Äôm using my own custom made model that‚Äôs 800GB and was trained over 1.2GB of assembly and hardcore coding ie security reverse engineering game hacking etc. It includes 36 power shell compilers I wrote from scratch! Lemme know what ya think thanks! And ya it was sorta supposed to NOT be a clone of anything! Everything here was written from scratch! Yes the compilers compile actual code without runtimes! Build anything anywhere no matter your internet connection!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BackUpBiii"&gt; /u/BackUpBiii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oop0t7/built_my_own_ide/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oop0t7/built_my_own_ide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oop0t7/built_my_own_ide/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T00:57:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo8p81</id>
    <title>Hardware recommendations for Ollama for homelab</title>
    <updated>2025-11-04T14:33:00+00:00</updated>
    <author>
      <name>/u/alex-gee</name>
      <uri>https://old.reddit.com/user/alex-gee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I just started with n8n and I‚Äôm thinking to run Ollama in my homelab to use it as my LLM for AI agents in n8n. No commercial use - just for fun.&lt;/p&gt; &lt;p&gt;I understand that loads of GPU VRAM is important, but not sure about the other components.&lt;/p&gt; &lt;p&gt;I have a 16GB AMD Radeon 6900XT in my Windows workstation (with Ryzen 7600X and 64GB RAM), and I have a fileserver with AM4 Ryzen 4650G and 128GB ECC RAM. I also have a spare AM4 Mainboard with 2x PCIe slots.&lt;/p&gt; &lt;p&gt;I can imagine different routes:&lt;/p&gt; &lt;p&gt;Running Ollama on my workstation, but I would need to ensure it‚Äôs running, when an n8n AI agent runs.&lt;/p&gt; &lt;p&gt;Adding a GPU to my fileserver - pro: always on&lt;/p&gt; &lt;p&gt;Additional dedicated LLM server&lt;/p&gt; &lt;p&gt;I will try to run Ollama on my Windows workstation for sure and I could add Ollama as docker app on my TrueNAS Scale fileserver (without GPU, as I think, that the iGPU is not supported.&lt;/p&gt; &lt;p&gt;I was thinking about a Radeon VII as an additional LLM GPU, which should be around 200 ‚Ç¨.&lt;/p&gt; &lt;p&gt;What are the recommendations for CPU, RAM and SSD - or is it only GPU related?&lt;/p&gt; &lt;p&gt;Thank you for your input&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alex-gee"&gt; /u/alex-gee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oo8p81/hardware_recommendations_for_ollama_for_homelab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oo8p81/hardware_recommendations_for_ollama_for_homelab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oo8p81/hardware_recommendations_for_ollama_for_homelab/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-04T14:33:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oos8sb</id>
    <title>Is Deepseek Cloud broken right now?</title>
    <updated>2025-11-05T03:26:02+00:00</updated>
    <author>
      <name>/u/Quadralox</name>
      <uri>https://old.reddit.com/user/Quadralox</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1oos8sb/is_deepseek_cloud_broken_right_now/"&gt; &lt;img alt="Is Deepseek Cloud broken right now?" src="https://preview.redd.it/txne1ppoxczf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3715f556890d5593ccdb3daef47447daf985cd70" title="Is Deepseek Cloud broken right now?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use this version of Deepseek on the Cloud because my computer is a potato. This error has persisted for about two hours now. How can I rectify it?&lt;/p&gt; &lt;p&gt;I don't particularly want to switch to another LLM on the Cloud either, Deepseek is the one I prefer for fiction writing, as its memory recall is superior to the others out there.&lt;/p&gt; &lt;p&gt;(If it helps, I paid for the subscription service, I love Ollama's cloud servers!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quadralox"&gt; /u/Quadralox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/txne1ppoxczf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oos8sb/is_deepseek_cloud_broken_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oos8sb/is_deepseek_cloud_broken_right_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T03:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oowa13</id>
    <title>ollama client light weight local</title>
    <updated>2025-11-05T07:09:32+00:00</updated>
    <author>
      <name>/u/sunole123</name>
      <uri>https://old.reddit.com/user/sunole123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for an ollama client that:&lt;br /&gt; 1- can run on windows or mac,&lt;br /&gt; 2- light weight,&lt;br /&gt; 3- can access ollama from local machine and local network,&lt;br /&gt; 4- without the docker or bloats,&lt;br /&gt; 5- with some advanced functions like RAG&lt;br /&gt; 6- same app for both platforms or even on mobile phone too, &lt;/p&gt; &lt;p&gt;Thanks in advance, what do you guys recommend?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunole123"&gt; /u/sunole123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oowa13/ollama_client_light_weight_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oowa13/ollama_client_light_weight_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oowa13/ollama_client_light_weight_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T07:09:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ootqj8</id>
    <title>Ollama for bank data analysis</title>
    <updated>2025-11-05T04:41:46+00:00</updated>
    <author>
      <name>/u/Past-Attitude-9612</name>
      <uri>https://old.reddit.com/user/Past-Attitude-9612</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which Ollama model would you recommend for automatically analyzing bank account data (statements, transactions, expenses), and how can I train or customize this model to improve analysis accuracy?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Past-Attitude-9612"&gt; /u/Past-Attitude-9612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ootqj8/ollama_for_bank_data_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ootqj8/ollama_for_bank_data_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ootqj8/ollama_for_bank_data_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T04:41:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo48q8</id>
    <title>What do you use your local LLMs for</title>
    <updated>2025-11-04T10:46:00+00:00</updated>
    <author>
      <name>/u/Solid_Vermicelli_510</name>
      <uri>https://old.reddit.com/user/Solid_Vermicelli_510</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Simple curiosity, for what purposes do you use them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Vermicelli_510"&gt; /u/Solid_Vermicelli_510 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oo48q8/what_do_you_use_your_local_llms_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oo48q8/what_do_you_use_your_local_llms_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oo48q8/what_do_you_use_your_local_llms_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-04T10:46:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1op9qam</id>
    <title>K√πzu is no more - what now?</title>
    <updated>2025-11-05T17:47:32+00:00</updated>
    <author>
      <name>/u/Far-Photo4379</name>
      <uri>https://old.reddit.com/user/Far-Photo4379</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Photo4379"&gt; /u/Far-Photo4379 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AIMemory/comments/1op9pnb/k√πzu_is_no_more_what_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1op9qam/k√πzu_is_no_more_what_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1op9qam/k√πzu_is_no_more_what_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T17:47:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooul29</id>
    <title>Stream Ollama conversations through a Matrix rain visual ‚Äî open-source</title>
    <updated>2025-11-05T05:27:30+00:00</updated>
    <author>
      <name>/u/spreader123</name>
      <uri>https://old.reddit.com/user/spreader123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- I built an Ollama-powered mode that streams LLM responses across the screen in a Matrix rain style with color palettes and pattern-specific effects. Each message gets a distinct color, and the renderer cycles through visual patterns (classic, rainbow, pentad, harmonic) with unique effects. It‚Äôs open-source and easy to run locally.&lt;/p&gt; &lt;p&gt;- What it does:&lt;/p&gt; &lt;p&gt;- Streams full AI conversations across ALL columns (full-screen width)&lt;/p&gt; &lt;p&gt;- Assigns random vibrant colors per message (10-color palette)&lt;/p&gt; &lt;p&gt;- Automatically cycles visual patterns with tailored render effects&lt;/p&gt; &lt;p&gt;- ‚ÄúExclusive mode‚Äù system: Ollama/Audio/Orchestrator won‚Äôt conflict&lt;/p&gt; &lt;p&gt;- Links:&lt;/p&gt; &lt;p&gt;- GitHub (code + Ollama mode): &lt;a href="https://github.com/Yufok1/Matrix-Rain-HTML-Background"&gt;https://github.com/Yufok1/Matrix-Rain-HTML-Background&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Steam Workshop (Wallpaper Engine build): &lt;a href="https://steamcommunity.com/sharedfiles/filedetails/?id=3599704378"&gt;https://steamcommunity.com/sharedfiles/filedetails/?id=3599704378&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- LIVE DEMO (audio-only, browser): &lt;a href="https://yufok1.github.io/Matrix-Rain-HTML-Background/"&gt;https://yufok1.github.io/Matrix-Rain-HTML-Background/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Notes:&lt;/p&gt; &lt;p&gt;- The Steam build is tuned for Wallpaper Engine and does not enable Ollama mode.&lt;/p&gt; &lt;p&gt;- The GitHub version includes the Ollama streaming mode (requires a small local backend).&lt;/p&gt; &lt;p&gt;- Looking for:&lt;/p&gt; &lt;p&gt;- Feedback on color/palette choices and pattern cycling during AI streams&lt;/p&gt; &lt;p&gt;- Suggestions for message pacing, visual emphasis, and readability&lt;/p&gt; &lt;p&gt;- Ideas for palette rules (e.g., semantic colors by role/system vs. user/assistant)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spreader123"&gt; /u/spreader123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ooul29/stream_ollama_conversations_through_a_matrix_rain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ooul29/stream_ollama_conversations_through_a_matrix_rain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ooul29/stream_ollama_conversations_through_a_matrix_rain/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T05:27:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooz2v1</id>
    <title>How is an LLM created?</title>
    <updated>2025-11-05T10:12:24+00:00</updated>
    <author>
      <name>/u/eworker8888</name>
      <uri>https://old.reddit.com/user/eworker8888</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eworker8888"&gt; /u/eworker8888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/eworker_ca/comments/1ooyzrp/how_is_an_llm_created/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ooz2v1/how_is_an_llm_created/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ooz2v1/how_is_an_llm_created/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T10:12:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1op4nze</id>
    <title>HELP me create an answer generating RAG AI setup</title>
    <updated>2025-11-05T14:41:26+00:00</updated>
    <author>
      <name>/u/Professional_Lake682</name>
      <uri>https://old.reddit.com/user/Professional_Lake682</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys.....Basically I want to feed the AI model my curriculum textbook Pdfs(around 500mb for a subject) without having to cut it in size because relevant info is spread through out the book. Then I‚Äôll make it generate theory specific answers for my prof exams to study from Preferably citing the info from the resources, including flow charts and relevant tables of info and at the very least mentioning (if not inputting) what diagrams would be related to my query/question. I need help from this community in choosing the right AI tool / work flow setting / LLM model and 101 setup tutorial for it I just really want this to stream line my preparation so that I can focus more on competitive exams. Thanks yall in advance!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional_Lake682"&gt; /u/Professional_Lake682 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1op4nze/help_me_create_an_answer_generating_rag_ai_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1op4nze/help_me_create_an_answer_generating_rag_ai_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1op4nze/help_me_create_an_answer_generating_rag_ai_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T14:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ophkbj</id>
    <title>PromptShield Labs - An open-source playground for new AI experiments</title>
    <updated>2025-11-05T22:34:18+00:00</updated>
    <author>
      <name>/u/kekePower</name>
      <uri>https://old.reddit.com/user/kekePower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I recently created &lt;strong&gt;PromptShield Labs&lt;/strong&gt; - a place where I post new open-source projects and experiments I‚Äôm testing or just having fun with.&lt;/p&gt; &lt;p&gt;Thought I‚Äôd share it here in case anyone wants to check it out, use something, or maybe even contribute.&lt;/p&gt; &lt;p&gt;üîó &lt;a href="https://labs.promptshield.io"&gt;https://labs.promptshield.io&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kekePower"&gt; /u/kekePower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ophkbj/promptshield_labs_an_opensource_playground_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ophkbj/promptshield_labs_an_opensource_playground_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ophkbj/promptshield_labs_an_opensource_playground_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T22:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1op7tmy</id>
    <title>SQL Chat Agent</title>
    <updated>2025-11-05T16:40:06+00:00</updated>
    <author>
      <name>/u/stefsk8</name>
      <uri>https://old.reddit.com/user/stefsk8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone here worked with advanced SQL chat agents ones that can translate natural language into SQL queries and return results intelligently using ollama and potential other tools?&lt;/p&gt; &lt;p&gt;I‚Äôm not talking about the simple ‚Äútext-to-SQL‚Äù demos, but more advanced setups where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The LLM actually understands the connected database (schema, relationships, etc.)&lt;/li&gt; &lt;li&gt;Existing data is leveraged to train or fine-tune the model on the database structure and relationships&lt;/li&gt; &lt;li&gt;The system can accurately map business language to technical terms, so it truly understands what the user is asking for&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious if anyone has built or experimented with something like this and how you approached it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stefsk8"&gt; /u/stefsk8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1op7tmy/sql_chat_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1op7tmy/sql_chat_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1op7tmy/sql_chat_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T16:40:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1openho</id>
    <title>"On-the-fly" code reviews with ollama. It kinda works..</title>
    <updated>2025-11-05T20:45:14+00:00</updated>
    <author>
      <name>/u/EMurph55</name>
      <uri>https://old.reddit.com/user/EMurph55</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I created this library for a bit of fun to see if it would work, and I am finding it to be somewhat helpful tbh. Thought I'd share it here to see if anyone had any similar tools or ideas: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/whatever555/ollama-watcher"&gt;https://github.com/whatever555/ollama-watcher&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EMurph55"&gt; /u/EMurph55 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1openho/onthefly_code_reviews_with_ollama_it_kinda_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1openho/onthefly_code_reviews_with_ollama_it_kinda_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1openho/onthefly_code_reviews_with_ollama_it_kinda_works/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T20:45:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1op72oi</id>
    <title>GLM-4.5V model for local computer use</title>
    <updated>2025-11-05T16:12:45+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1op72oi/glm45v_model_for_local_computer_use/"&gt; &lt;img alt="GLM-4.5V model for local computer use" src="https://external-preview.redd.it/MWVndHJ0dmxxZ3pmMbWADQNBSkImjVESNjfi_q43l9ostHKNGAFX_QJdfnS0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0dd92a331e5c40438053cd04ccd3ee2ddc99f10a" title="GLM-4.5V model for local computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either: Locally via Hugging Face Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vy2u914mqgzf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1op72oi/glm45v_model_for_local_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1op72oi/glm45v_model_for_local_computer_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T16:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1opat7j</id>
    <title>We just released a multi-agent framework. Please break it.</title>
    <updated>2025-11-05T18:25:06+00:00</updated>
    <author>
      <name>/u/wikkid_lizard</name>
      <uri>https://old.reddit.com/user/wikkid_lizard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1opat7j/we_just_released_a_multiagent_framework_please/"&gt; &lt;img alt="We just released a multi-agent framework. Please break it." src="https://preview.redd.it/xjebyb25ehzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67e99671e15cf6167186161f13f586fbf2286b9b" title="We just released a multi-agent framework. Please break it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! We just released Laddr, a lightweight multi-agent architecture framework for building AI systems where multiple agents can talk, coordinate, and scale together.&lt;/p&gt; &lt;p&gt;If you're experimenting with agent workflows, orchestration, automation tools, or just want to play with agent systems, would love for you to check it out.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/AgnetLabs/laddr"&gt;https://github.com/AgnetLabs/laddr&lt;/a&gt; &lt;br /&gt; Docs: &lt;a href="https://laddr.agnetlabs.com/"&gt;https://laddr.agnetlabs.com&lt;/a&gt; &lt;br /&gt; Questions / Feedback: [&lt;a href="mailto:info@agnetlabs.com"&gt;info@agnetlabs.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@agnetlabs.com"&gt;info@agnetlabs.com&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;It's super fresh, so feel free to break it, fork it, star it, and tell us what sucks or what works. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wikkid_lizard"&gt; /u/wikkid_lizard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xjebyb25ehzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1opat7j/we_just_released_a_multiagent_framework_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1opat7j/we_just_released_a_multiagent_framework_please/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-05T18:25:06+00:00</published>
  </entry>
</feed>
