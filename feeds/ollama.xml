<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-05T11:05:45+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1n71bil</id>
    <title>Any actual downside to 4 x 3090 ($2400 total) vs RTX pro 6000 ($9000) other than power?</title>
    <updated>2025-09-03T01:09:30+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n71bil/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n71bil/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T01:09:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7kz78</id>
    <title>Conseils IA pour 3 use cases (email, briefing, chatbot) sur serveur local modeste</title>
    <updated>2025-09-03T17:20:56+00:00</updated>
    <author>
      <name>/u/Maleficent-Hotel8207</name>
      <uri>https://old.reddit.com/user/Maleficent-Hotel8207</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Salut, Je cherche des id√©es d‚ÄôIA √† faire tourner en local sur ma config : ‚Ä¢ GTX 1050 low profile (2 Go VRAM) ‚Ä¢ i3-3400 ‚Ä¢ 16 Go de RAM&lt;/p&gt; &lt;p&gt;J‚Äôai 3 besoins : ‚Ä¢ IA pour g√©n√©rer des emails : environ 500 tokens en entr√©e, 30 tokens en sortie. R√©ponse en moins de 5 minutes. ‚Ä¢ IA pour faire un briefing du matin : environ 3000 tokens en entr√©e, 100 tokens en sortie. R√©sum√© clair et rapide. ‚Ä¢ Chatbot ultra-rapide : environ 20 tokens en entr√©e, 20 tokens en sortie. R√©ponse en moins de 5 secondes.&lt;/p&gt; &lt;p&gt;Je cherche des mod√®les l√©gers (quantifi√©s, optimis√©s, open-source si possible) pour que √ßa tourne sur cette config limit√©e. Si vous avez des id√©es de mod√®les, de frameworks ou de tips pour que √ßa passe, je suis preneur !&lt;/p&gt; &lt;p&gt;Merci d‚Äôavance !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Hotel8207"&gt; /u/Maleficent-Hotel8207 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T17:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7vnwy</id>
    <title>Hate AI frameworks? I may have something for you...</title>
    <updated>2025-09-04T00:22:55+00:00</updated>
    <author>
      <name>/u/BeautifulQuote6295</name>
      <uri>https://old.reddit.com/user/BeautifulQuote6295</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you're building with AI you may have found yourself grappling with one of the mainstream frameworks. Since I never really liked no having granular control over what's happening, last year I built a lib called `grafo` for easily AI workflows. It's rules are simple:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nodes contain coroutines to be run&lt;/li&gt; &lt;li&gt;A node only starts executing once all it's parent's have finished running&lt;/li&gt; &lt;li&gt;State is not passed around automatically, but you can do it manually&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These rules come together to make building AI-driven workflows generally easy. However, building around AI has more than DAGs: we need prompt building and mode calling - in comes `grafo ai tools`.&lt;/p&gt; &lt;p&gt;`Grafo AI Tools` is basically a wrapper lib where I've added some very simple prompt managing &amp;amp; model calling, coupled with `grafo`. It's built around the big guys, like `jinja2` and `instructor`.&lt;/p&gt; &lt;p&gt;My goal here is not to create a framework or any set of abstractions that take away from our control of the program as developers - I just wanted to bundle a toolkit which I found useful. In any case, here's the URL: &lt;a href="https://github.com/paulomtts/Grafo-AI-Tools"&gt;https://github.com/paulomtts/Grafo-AI-Tools&lt;/a&gt; . Let me know if you find this interesting at all. I'll be updating it going forward.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeautifulQuote6295"&gt; /u/BeautifulQuote6295 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7vnwy/hate_ai_frameworks_i_may_have_something_for_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7vnwy/hate_ai_frameworks_i_may_have_something_for_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7vnwy/hate_ai_frameworks_i_may_have_something_for_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T00:22:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7m86q</id>
    <title>Build a Visual Document Index from multiple formats all at once - PDFs, Images, Slides - with ColPali without OCR</title>
    <updated>2025-09-03T18:06:40+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would love to share my latest project that builds visual document index from multiple formats in the same flow for PDFs, images using Colpali without OCR. Incremental processing out-of-box and can connect to google drive, s3, azure blob store.&lt;/p&gt; &lt;p&gt;- Detailed write up: &lt;a href="https://cocoindex.io/blogs/multi-format-indexing"&gt;https://cocoindex.io/blogs/multi-format-indexing&lt;/a&gt;&lt;br /&gt; - Fully open sourced: &lt;a href="https://github.com/cocoindex-io/cocoindex/tree/main/examples/multi_format_indexing"&gt;https://github.com/cocoindex-io/cocoindex/tree/main/examples/multi_format_indexing&lt;/a&gt;&lt;br /&gt; (70 lines python on index path)&lt;/p&gt; &lt;p&gt;Looking forward to your suggestions&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T18:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1n831ad</id>
    <title>my ranking and I am not sure whether it is your ranking</title>
    <updated>2025-09-04T06:48:32+00:00</updated>
    <author>
      <name>/u/Zestyclose-Duty3239</name>
      <uri>https://old.reddit.com/user/Zestyclose-Duty3239</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;best level: claude (for programming) &amp;gt; copilot research&lt;/p&gt; &lt;p&gt;good level: gpt o3 &amp;gt; gpt 4o (before $200/month plan) &amp;gt; copliot deeper thinking&lt;/p&gt; &lt;p&gt;normal level: gpt 4o (after $200/month plan) &amp;gt; supergrok fast and expert (they are working well for uncensored but both of them are almost shitty in ocr) &amp;gt; gemini colab (for python)&lt;/p&gt; &lt;p&gt;almost shitty level: deepseek r1 (tankman everywhere) &amp;gt; gemini &amp;gt; gpt 5 &amp;gt; copilot normal&lt;/p&gt; &lt;p&gt;shitty level: gpt o4-mini &amp;gt; gpt 4.5&lt;/p&gt; &lt;p&gt;I am using a mac pro 2019 with gv100. it is very difficult on running local ollama. I have to use online model.&lt;/p&gt; &lt;p&gt;I believe no company is actually earning money from their ai competition. $30-$300/month subscription is still much lower than the actual cost of the llm model and their gpu base. microsoft, google, meta, amazon, and openai are just wasting money for the market share. they will eventually let us use the weaker model in the next 2-3 years.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zestyclose-Duty3239"&gt; /u/Zestyclose-Duty3239 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n831ad/my_ranking_and_i_am_not_sure_whether_it_is_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n831ad/my_ranking_and_i_am_not_sure_whether_it_is_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n831ad/my_ranking_and_i_am_not_sure_whether_it_is_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T06:48:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n739d3</id>
    <title>ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization</title>
    <updated>2025-09-03T02:40:16+00:00</updated>
    <author>
      <name>/u/cornucopea</name>
      <uri>https://old.reddit.com/user/cornucopea</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt; &lt;img alt="ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization" src="https://b.thumbs.redditmedia.com/xqOJbGWme_Lnii3nAdQLvJli58h2TtNMtqIsZum6xqs.jpg" title="ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;This refactors the main run loop of the ollama runner to perform the main GPU intensive tasks (Compute+Floats) in a go routine so we can prepare the next batch in parallel to reduce the amount of time the GPU stalls waiting for the next batch of work.&lt;/p&gt; &lt;p&gt;On metal, I see a 2-3% speedup in token rate. On a single RTX 4090 I see a ~7% speedup.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cs98ja944vmf1.jpg?width=650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=01fd1804e5580b7cc7e85287b110a5cece68865d"&gt;https://preview.redd.it/cs98ja944vmf1.jpg?width=650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=01fd1804e5580b7cc7e85287b110a5cece68865d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.phoronix.com/news/ollama-0.11.9-More-Performance"&gt;https://www.phoronix.com/news/ollama-0.11.9-More-Performance&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cornucopea"&gt; /u/cornucopea &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T02:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7h8ox</id>
    <title>Anyone else frustrated with AI assistants forgetting context?</title>
    <updated>2025-09-03T15:03:43+00:00</updated>
    <author>
      <name>/u/PrestigiousBet9342</name>
      <uri>https://old.reddit.com/user/PrestigiousBet9342</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep bouncing between ChatGPT, Claude, and Perplexity depending on the task. The problem is every new session feels like starting over‚ÄîI have to re-explain everything.&lt;/p&gt; &lt;p&gt;Just yesterday I wasted 10+ minutes walking perplexity through my project direction again just to get related search if not it is just useless. This morning, ChatGPT didn‚Äôt remember anything about my client‚Äôs requirements.&lt;/p&gt; &lt;p&gt;The result? I lose a couple of hours each week just re-establishing context. It also makes it hard to keep project discussions consistent across tools. Switching platforms means resetting, and there‚Äôs no way to keep a running history of decisions or knowledge.&lt;/p&gt; &lt;p&gt;I‚Äôve tried copy-pasting old chats (messy and unreliable), keeping manual notes (which defeats the point of using AI), and sticking to just one tool (but each has its strengths).&lt;/p&gt; &lt;p&gt;Has anyone actually found a fix for this? I‚Äôm especially interested in something that works across different platforms, not just one. On my end, I‚Äôve started tinkering with a solution and would love to hear what features people would find most useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrestigiousBet9342"&gt; /u/PrestigiousBet9342 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T15:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n868ux</id>
    <title>Local Code Analyser</title>
    <updated>2025-09-04T10:15:26+00:00</updated>
    <author>
      <name>/u/r00tdr1v3</name>
      <uri>https://old.reddit.com/user/r00tdr1v3</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/r00tdr1v3"&gt; /u/r00tdr1v3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n868dj/local_code_analyser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n868ux/local_code_analyser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n868ux/local_code_analyser/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T10:15:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7ypzr</id>
    <title>MoE models benchmarked on AMD iGPU</title>
    <updated>2025-09-04T02:47:16+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n7ypio/moe_models_benchmarked_on_igpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7ypzr/moe_models_benchmarked_on_amd_igpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7ypzr/moe_models_benchmarked_on_amd_igpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T02:47:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7q8cx</id>
    <title>Ollama model most similar to GPT-4o?</title>
    <updated>2025-09-03T20:36:55+00:00</updated>
    <author>
      <name>/u/amstlicht</name>
      <uri>https://old.reddit.com/user/amstlicht</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been researching AI models and am looking for models similar to 4o in terms of personality, mostly. I remember 4o would often suggest interesting paths when I used it for research, it would remember the context and relate it to previous ideas. Does anyone have a recommendation of something similar for Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amstlicht"&gt; /u/amstlicht &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7q8cx/ollama_model_most_similar_to_gpt4o/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7q8cx/ollama_model_most_similar_to_gpt4o/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7q8cx/ollama_model_most_similar_to_gpt4o/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T20:36:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8bpyp</id>
    <title>RX570 compatibility issues</title>
    <updated>2025-09-04T14:28:38+00:00</updated>
    <author>
      <name>/u/Famous-Economics9054</name>
      <uri>https://old.reddit.com/user/Famous-Economics9054</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I‚Äôm completely new to LLMs and I tried out ollama on my homeserver which is running on an i5 4570 and RX570 8gb. As far as I understand ollama uses cuda cores on nvidia and rocm on amd. I‚Äôve had issues making it use the rx570 as it is ‚Äúgfx803‚Äù and doesn‚Äôt directly support rocm. Does anyone know a fix or workaround for this? &lt;/p&gt; &lt;p&gt;Also I‚Äôm sorry if I said something stupid, I‚Äôm new to this. &lt;/p&gt; &lt;p&gt;Thanks in advance guys!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Famous-Economics9054"&gt; /u/Famous-Economics9054 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8bpyp/rx570_compatibility_issues/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8bpyp/rx570_compatibility_issues/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8bpyp/rx570_compatibility_issues/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T14:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8e7uu</id>
    <title>System Crash while Running Local AI Models on MBA M1 ‚Äì Need Help</title>
    <updated>2025-09-04T16:02:10+00:00</updated>
    <author>
      <name>/u/Separate-Road-3668</name>
      <uri>https://old.reddit.com/user/Separate-Road-3668</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey Guys,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm currently using a MacBook Air M1 to run some local AI models, but recently I‚Äôve encountered an issue where my system crashes and restarts when I run a model. This has happened a few times, and I‚Äôm trying to figure out the exact cause.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Issue:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;When running the model, my system crashes and restarts.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I‚Äôve tried:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;I‚Äôve checked the system logs via the Console app, but there‚Äôs nothing helpful there‚Äîperhaps the logs got cleared, but I‚Äôm not sure.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Could this be related to swap usage, GPU, or CPU pressure? How can I pinpoint the exact cause of the crash? I‚Äôm looking for some evidence or debugging tips that can help confirm this.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Bonus Question:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Is there a way to control the resource usage dynamically while running AI models? For instance, can I tell a model to use only a certain percentage (like 40%) of the system‚Äôs resources, to prevent crashing while still running other tasks?&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Specs:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MacBook Air M1 (8GB RAM)&lt;br /&gt; Used MLX for the MPS support&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Separate-Road-3668"&gt; /u/Separate-Road-3668 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8e7uu/system_crash_while_running_local_ai_models_on_mba/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8e7uu/system_crash_while_running_local_ai_models_on_mba/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8e7uu/system_crash_while_running_local_ai_models_on_mba/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T16:02:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n87ngf</id>
    <title>Can I use Ollama + OpenWebUI through Docker Engine (In Terminal) or only through Desktop version?</title>
    <updated>2025-09-04T11:33:50+00:00</updated>
    <author>
      <name>/u/PracticalAd6966</name>
      <uri>https://old.reddit.com/user/PracticalAd6966</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently on Linux PC and I really need to use Docker Engine and as I understand they have conflicting files so I can use only one of them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PracticalAd6966"&gt; /u/PracticalAd6966 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n87ngf/can_i_use_ollama_openwebui_through_docker_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n87ngf/can_i_use_ollama_openwebui_through_docker_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n87ngf/can_i_use_ollama_openwebui_through_docker_engine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T11:33:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8c48y</id>
    <title>Clustering apple silicon and nvidia gpu based server.</title>
    <updated>2025-09-04T14:43:45+00:00</updated>
    <author>
      <name>/u/Pedroxns</name>
      <uri>https://old.reddit.com/user/Pedroxns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, &lt;/p&gt; &lt;p&gt;I was just scrolling through reddit and saw a post about the Apple silicon performance.&lt;/p&gt; &lt;p&gt;I already have 2 mac minis here, one M2 + 8gb and one M4+16gb, nothing too fancy, and i'm already running ollama on a PVE VM with a ryzen 5600g + 3060 12gb + 10gb ram( server has 32gb total), nothing fancy either but runs 4b and 7b models for my frigate and my home assistant instances.&lt;/p&gt; &lt;p&gt;My question is; whould I see any high gain by running ollama on the M4 rather than on the 3060? Could I/should I try clustering the 3 machines to run faster/bigger models?&lt;/p&gt; &lt;p&gt;Thanks in advance for the advices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pedroxns"&gt; /u/Pedroxns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8c48y/clustering_apple_silicon_and_nvidia_gpu_based/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8c48y/clustering_apple_silicon_and_nvidia_gpu_based/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8c48y/clustering_apple_silicon_and_nvidia_gpu_based/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T14:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7uhkv</id>
    <title>Hows your experience running Ollama on Apple Sillicon M1, M2, M3 or M4</title>
    <updated>2025-09-03T23:29:58+00:00</updated>
    <author>
      <name>/u/Cultural-You-7096</name>
      <uri>https://old.reddit.com/user/Cultural-You-7096</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How's the experience, Does it run welll like web versions or is it slow. I'm concerned becuase I want to get a Macbook Pro just to run models .&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cultural-You-7096"&gt; /u/Cultural-You-7096 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T23:29:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8irsb</id>
    <title>Posible Ollama usage on this MCP server</title>
    <updated>2025-09-04T18:53:36+00:00</updated>
    <author>
      <name>/u/FreddyDEE90</name>
      <uri>https://old.reddit.com/user/FreddyDEE90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does any of you know if it's possible to use this MCP server &lt;a href="https://github.com/rinadelph/Agent-MCP"&gt;https://github.com/rinadelph/Agent-MCP&lt;/a&gt; with Ollama instead of the openai API key ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FreddyDEE90"&gt; /u/FreddyDEE90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8irsb/posible_ollama_usage_on_this_mcp_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8irsb/posible_ollama_usage_on_this_mcp_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8irsb/posible_ollama_usage_on_this_mcp_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T18:53:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8l2hu</id>
    <title>Built an offline AI CLI that generates apps and runs code safely</title>
    <updated>2025-09-04T20:21:45+00:00</updated>
    <author>
      <name>/u/Sea-Reception-2697</name>
      <uri>https://old.reddit.com/user/Sea-Reception-2697</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Reception-2697"&gt; /u/Sea-Reception-2697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/selfhosted/comments/1n8l20c/built_an_offline_ai_cli_that_generates_apps_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8l2hu/built_an_offline_ai_cli_that_generates_apps_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8l2hu/built_an_offline_ai_cli_that_generates_apps_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T20:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n89sa4</id>
    <title>Most affordable AI computer with GPU (‚ÄúGPUter‚Äù) you can build in 2025?</title>
    <updated>2025-09-04T13:13:36+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n89sa4/most_affordable_ai_computer_with_gpu_gputer_you/"&gt; &lt;img alt="Most affordable AI computer with GPU (‚ÄúGPUter‚Äù) you can build in 2025?" src="https://preview.redd.it/bk6tf5l2e5nf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8da7afc16f4d8ff260c98ad24de5cc8adc50a222" title="Most affordable AI computer with GPU (‚ÄúGPUter‚Äù) you can build in 2025?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bk6tf5l2e5nf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n89sa4/most_affordable_ai_computer_with_gpu_gputer_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n89sa4/most_affordable_ai_computer_with_gpu_gputer_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T13:13:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8cmcq</id>
    <title>Faster Ollama</title>
    <updated>2025-09-04T15:02:27+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One of my favorite Linux benchmark sites. &lt;/p&gt; &lt;p&gt;ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization - Phoronix &lt;a href="https://share.google/2lCqH4Imkt2dmeS2G"&gt;https://share.google/2lCqH4Imkt2dmeS2G&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;On metal, I see a 2-3% speedup in token rate. On a single RTX 4090 I see a ~7% speedup.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8cmcq/faster_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8cmcq/faster_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8cmcq/faster_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T15:02:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n91fzn</id>
    <title>Enhanched Chat Interface (You can locally use Google AI mode or Perplexity without setting up anything, just install claraverse and download any small model and good to go)</title>
    <updated>2025-09-05T10:21:02+00:00</updated>
    <author>
      <name>/u/aruntemme</name>
      <uri>https://old.reddit.com/user/aruntemme</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n91fzn/enhanched_chat_interface_you_can_locally_use/"&gt; &lt;img alt="Enhanched Chat Interface (You can locally use Google AI mode or Perplexity without setting up anything, just install claraverse and download any small model and good to go)" src="https://external-preview.redd.it/3ht2kM5gZ_4ghWmHZyOjxU4vR3qpoqFiFfmo_5HzM4U.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9bf626a6e6e572ad9a4677a3e8da97a67fe21f51" title="Enhanched Chat Interface (You can locally use Google AI mode or Perplexity without setting up anything, just install claraverse and download any small model and good to go)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aruntemme"&gt; /u/aruntemme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=bGW0-554RZU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n91fzn/enhanched_chat_interface_you_can_locally_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n91fzn/enhanched_chat_interface_you_can_locally_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T10:21:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8hfm7</id>
    <title>I made a simple C# agent that uses local Ollama models to manage my file system</title>
    <updated>2025-09-04T18:02:32+00:00</updated>
    <author>
      <name>/u/cride20</name>
      <uri>https://old.reddit.com/user/cride20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm a huge fan of running models locally and wanted to build something practical with them. So, I created &lt;strong&gt;AI Slop&lt;/strong&gt;: a C# console agent that lets you use natural language to create folders, write files, and manage a workspace.&lt;/p&gt; &lt;p&gt;It's all powered by Ollama and a model capable of tool use (I've had great success with qwen3-coder:30b-a3b-q4_K_M). The agent follows a strict &amp;quot;think-act-observe&amp;quot; loop.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Prompting Strategies:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Strict JSON Output:&lt;/strong&gt; The prompt demands that the only output is a single raw JSON object with two keys: &amp;quot;thought&amp;quot; and &amp;quot;tool_call&amp;quot;. No markdown, no preamble. This makes parsing super reliable.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;One Tool at a Time:&lt;/strong&gt; This is the most critical rule in the prompt. I explicitly forbid the model from trying to chain commands in one response. This forces it to wait for feedback from the environment after every single action, which prevents it from getting lost or making assumptions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Situational Awareness:&lt;/strong&gt; I encourage it to constantly use the GetWorkspaceEntries tool to check the contents of its current directory before acting, which dramatically reduces errors.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Defined Toolset:&lt;/strong&gt; The prompt includes a &amp;quot;manual&amp;quot; for all the available C# functions, including the tool name, description, and argument format (e.g., CreateFile, OpenFolder, TaskDone).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It's been fascinating to see how a well-structured prompt can turn a general-purpose LLM into a reliable tool-using agent.&lt;/p&gt; &lt;p&gt;The project is open source if you want to check out the full system prompt or run it yourself!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/cride9/AISlop"&gt;cride9/AISlop&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What other tools do you think would be useful for an agent like this?&lt;br /&gt; Inspired by the Manus project&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cride20"&gt; /u/cride20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8hfm7/i_made_a_simple_c_agent_that_uses_local_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8hfm7/i_made_a_simple_c_agent_that_uses_local_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8hfm7/i_made_a_simple_c_agent_that_uses_local_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T18:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8td0n</id>
    <title>Can you offload the entire LLM functionality to Ollama Turbo which means local hardware does not require GPU?</title>
    <updated>2025-09-05T02:23:53+00:00</updated>
    <author>
      <name>/u/ComedianObjective572</name>
      <uri>https://old.reddit.com/user/ComedianObjective572</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! Idk if this is a stupid idea for this Ollama Turbo. I want to run GPT-OSS but I don‚Äôt have the necessary hardware for it. Is it possible to offload the entire Ollama functionality to Ollama Turbo. &lt;/p&gt; &lt;p&gt;For example, local server with Ubuntu Server, Intel Core I5, 8GB RAM, NO GPU which servers Front End and Back End functionality to 3 computers. If I need to run an GPT-OSS, can I offload the entire thing to Ollama Turbo?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComedianObjective572"&gt; /u/ComedianObjective572 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8td0n/can_you_offload_the_entire_llm_functionality_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8td0n/can_you_offload_the_entire_llm_functionality_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8td0n/can_you_offload_the_entire_llm_functionality_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T02:23:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8vkv7</id>
    <title>Spam ending up being published?</title>
    <updated>2025-09-05T04:16:32+00:00</updated>
    <author>
      <name>/u/gnu-trix</name>
      <uri>https://old.reddit.com/user/gnu-trix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So... has anyone seen this?&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/puffymattresscode2025/puffy-mattress-coupon-code-2025-verified"&gt;https://ollama.com/puffymattresscode2025/puffy-mattress-coupon-code-2025-verified&lt;/a&gt;&lt;/p&gt; &lt;p&gt;^^ DISCLAIMER: you almost certainly should not pull this. I'm just pointing it out.&lt;/p&gt; &lt;p&gt;It says it was published 5 days ago. I'm REALLY super curious as to what it is, so if anyone has a VLAN'd Qubes with a VPN'd remote desktop running Ollama, could you pull it and try it out and report back here what it does? (Only suggesting this as a testing ground because there's AI malware now. I have no idea what makes models &amp;quot;run&amp;quot; - maybe it's not executable and wouldn't matter?)&lt;/p&gt; &lt;p&gt;But yeah anyway, do spam AI models often end up being published on Ollama, or is this a rare occurrence?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gnu-trix"&gt; /u/gnu-trix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8vkv7/spam_ending_up_being_published/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8vkv7/spam_ending_up_being_published/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8vkv7/spam_ending_up_being_published/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-05T04:16:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8kigw</id>
    <title>Got Gemma running locally on a Raspberry Pi 5 with Ollama</title>
    <updated>2025-09-04T20:00:23+00:00</updated>
    <author>
      <name>/u/Ricardo_Sappia</name>
      <uri>https://old.reddit.com/user/Ricardo_Sappia</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n8kigw/got_gemma_running_locally_on_a_raspberry_pi_5/"&gt; &lt;img alt="Got Gemma running locally on a Raspberry Pi 5 with Ollama" src="https://b.thumbs.redditmedia.com/ncJBXF0mM3RxpxHtBdAzWA1EyKrG2x3mjY__CLeR-qM.jpg" title="Got Gemma running locally on a Raspberry Pi 5 with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share a quick win: I got Gemma 2B (GGUF) running fully offline on a Raspberry Pi 5 (4GB) using Ollama.&lt;/p&gt; &lt;p&gt;It‚Äôs part of a side project called Dashi ‚Äî a modular e-paper dashboard that displays Strava, Garmin, and weather data‚Ä¶ plus motivational messages generated by a local AI fox .&lt;/p&gt; &lt;p&gt;No cloud, no API keys ‚Äî just local inference and surprisingly smooth performance for short outputs.&lt;/p&gt; &lt;p&gt;You can see more details here if curious: üëâ &lt;a href="https://www.hackster.io/rsappia/e-paper-dashboard-where-sport-ai-and-paper-meet-10c0f0"&gt;https://www.hackster.io/rsappia/e-paper-dashboard-where-sport-ai-and-paper-meet-10c0f0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer any questions about the setup or the integration!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ricardo_Sappia"&gt; /u/Ricardo_Sappia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n8kigw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8kigw/got_gemma_running_locally_on_a_raspberry_pi_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8kigw/got_gemma_running_locally_on_a_raspberry_pi_5/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T20:00:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8k3j3</id>
    <title>Power Up your Ollama Models! Thanks to you guys, I made this framework that lets your models watch the screen and help you out! (Open Source and Local)</title>
    <updated>2025-09-04T19:44:13+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n8k3j3/power_up_your_ollama_models_thanks_to_you_guys_i/"&gt; &lt;img alt="Power Up your Ollama Models! Thanks to you guys, I made this framework that lets your models watch the screen and help you out! (Open Source and Local)" src="https://external-preview.redd.it/dW9pNmwzeGxiN25mMc1Nh3OUDLTuFDtnMrFXEDpwYIUIEihHJF3jJPncl3qU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf64148d6af2db710bc91a6b36681250b2ab77fc" title="Power Up your Ollama Models! Thanks to you guys, I made this framework that lets your models watch the screen and help you out! (Open Source and Local)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; Observer now has an Overlay and Shortcut features! Now you can run agents that help you out at any time while watching your screen.&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;I'm back with another Observer update c:&lt;/p&gt; &lt;p&gt;Thank you so much for your support and feedback! I'm still working hard to make Observer useful in a variety of ways. And i'm trying to make Local models accessible to everyone!&lt;/p&gt; &lt;p&gt;So this update is an Overlay that lets your agents give you information on top of whatever you're doing. The obvious use case is helping out in coding problems, but there are other really cool things you can do with it! (specially adding the overlay to other already working agents). These are some cases where the Overlay can be useful:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Coding Assistant:&lt;/strong&gt; Use a shortcut and send whatever problem you're seeing to an LLM for it to solve it.&lt;br /&gt; &lt;strong&gt;Writing Assistant:&lt;/strong&gt; Send the text you're looking at to an LLM to get suggestions on what to write better or how to construct a better story.&lt;br /&gt; &lt;strong&gt;Activity Tracker:&lt;/strong&gt; Have an agent log on the overlay the last time you were doing something specific, then just by glancing at it you can get an idea of how much time you've spent doing something.&lt;br /&gt; &lt;strong&gt;Distraction Logger:&lt;/strong&gt; Same as the activity tracker, you just get messages passively when it thinks you're distracted.&lt;br /&gt; &lt;strong&gt;Video Watching Companion:&lt;/strong&gt; Watch a video and have a model label every new topic discussed and see it in the overlay!&lt;/p&gt; &lt;p&gt;Or any other agent you already had working, just &lt;strong&gt;power it up&lt;/strong&gt; by seeing what it's doing with the Overlay!&lt;/p&gt; &lt;p&gt;This is the projects &lt;a href="https://github.com/Roy3838/Observer"&gt;Github&lt;/a&gt; (completely open source)&lt;br /&gt; And the discord: &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you have any questions or ideas i'll be hanging out here for a while!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3h656ewlb7nf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8k3j3/power_up_your_ollama_models_thanks_to_you_guys_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8k3j3/power_up_your_ollama_models_thanks_to_you_guys_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T19:44:13+00:00</published>
  </entry>
</feed>
