<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-25T19:35:42+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1rc3srb</id>
    <title>I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review</title>
    <updated>2026-02-23T01:19:58+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"&gt; &lt;img alt="I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review" src="https://external-preview.redd.it/anBvOHE4ZWxiNWxnMYmmpi9UU3yP9yrC87ePDCyv5Mn4iZk_AUHCQZq2TOQ_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=452fe5fc9cf576221ea71aff1d15b07c8fa36f35" title="I ran ClawBot with Ollama locally on my Mac ‚Äî setup, gotchas, and honest review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all ‚Äî been experimenting with running a local AI agent using ClawBot + Ollama and wanted to share what actually happened.&lt;/p&gt; &lt;p&gt;Link to full tutorial: &lt;a href="https://www.youtube.com/watch?v=FxyQkj95VXs"&gt;https://www.youtube.com/watch?v=FxyQkj95VXs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yes, ClawBot + Ollama works on Mac. Does it work well? Depends on what you mean by &amp;quot;work&amp;quot;&lt;/li&gt; &lt;li&gt;With an 8B model, agentic tasks are limited. Basic Q&amp;amp;A? Fine. Anything complex? It'll humble you real quick&lt;/li&gt; &lt;li&gt;Should you expect ChatGPT-level speed? Absolutely not. Go make a coffee while you wait üòÖ&lt;/li&gt; &lt;li&gt;Is it worth it for learning the stack and experimenting locally for free? Honestly yes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What worked&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Setup is cleaner than expected - VS Code, JSON config, localhost dashboard, done. I have no luck setting up ollama using their onboarding. So...I went straight to config file.&lt;/li&gt; &lt;li&gt;Ollama model switching is straightforward once you understand the config structure&lt;/li&gt; &lt;li&gt;Great for understanding how local AI agent setups actually work under the hood&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What didn't&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Speed is rough on anything under 32GB RAM&lt;/li&gt; &lt;li&gt;8B models hit their ceiling fast on multi-step reasoning and real agentic workflows. Keep context window low.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1ee663elb5lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rc3srb/i_ran_clawbot_with_ollama_locally_on_my_mac_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T01:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcf94q</id>
    <title>What GPU do you use?</title>
    <updated>2026-02-23T11:43:10+00:00</updated>
    <author>
      <name>/u/Keensworth</name>
      <uri>https://old.reddit.com/user/Keensworth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've recently started using Ollama with an old GPU I had laying around.&lt;/p&gt; &lt;p&gt;Problem is that my GTX 1660S only got 6Gb VRAM and you can't do much with that.&lt;/p&gt; &lt;p&gt;I can run Mistral 7B Instruct but he sucks.&lt;/p&gt; &lt;p&gt;What hardware are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Keensworth"&gt; /u/Keensworth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcf94q/what_gpu_do_you_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T11:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd9l5i</id>
    <title>Best practices for running local LLMs for ~70‚Äì150 developers (agentic coding use case)</title>
    <updated>2026-02-24T07:16:14+00:00</updated>
    <author>
      <name>/u/Resident_Potential97</name>
      <uri>https://old.reddit.com/user/Resident_Potential97</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Resident_Potential97"&gt; /u/Resident_Potential97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1rd9kpk/best_practices_for_running_local_llms_for_70150/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd9l5i/best_practices_for_running_local_llms_for_70150/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rd9l5i/best_practices_for_running_local_llms_for_70150/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T07:16:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdjoo6</id>
    <title>Ollama Cloud Model Free Limit</title>
    <updated>2026-02-24T15:44:30+00:00</updated>
    <author>
      <name>/u/madmanari</name>
      <uri>https://old.reddit.com/user/madmanari</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rdjoo6/ollama_cloud_model_free_limit/"&gt; &lt;img alt="Ollama Cloud Model Free Limit" src="https://preview.redd.it/ese9rgoqqglg1.jpg?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=4ad9e01ae9b4659db3410adbd2cd5785fdb0c65e" title="Ollama Cloud Model Free Limit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama Free Limit around 3M per Day / 6M per Week..&lt;/p&gt; &lt;p&gt;#ollama&lt;/p&gt; &lt;p&gt;#openclaw&lt;/p&gt; &lt;p&gt;#ollamatokenprice&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madmanari"&gt; /u/madmanari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rdjoo6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdjoo6/ollama_cloud_model_free_limit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdjoo6/ollama_cloud_model_free_limit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T15:44:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcxt3m</id>
    <title>How are you monitoring your Ollama calls/usage?</title>
    <updated>2026-02-23T23:32:55+00:00</updated>
    <author>
      <name>/u/gkarthi280</name>
      <uri>https://old.reddit.com/user/gkarthi280</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"&gt; &lt;img alt="How are you monitoring your Ollama calls/usage?" src="https://preview.redd.it/b8gxcch9xblg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=ced5ee353224c286bd1ab54c4819b57ff238fad4" title="How are you monitoring your Ollama calls/usage?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Ollama in my LLM applications and wanted some feedback on what type of metrics people here would find useful to track in an app that eventually would go into prod. I used OpenTelemetry to instrument my app by following this&lt;a href="https://signoz.io/docs/ollama-monitoring/"&gt; Ollama observability guide&lt;/a&gt; and was able to create this dashboard.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b8gxcch9xblg1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc90458a61e2e80c8ed5e283edc3e914ccbddcd6"&gt;Ollama dashboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It tracks things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;token usage&lt;/li&gt; &lt;li&gt;error rate&lt;/li&gt; &lt;li&gt;number of requests&lt;/li&gt; &lt;li&gt;latency&lt;/li&gt; &lt;li&gt;LLM provider and model &amp;amp; token distribution&lt;/li&gt; &lt;li&gt;logs and errors&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Are there any important metrics that you would want to keep track of in prod for monitoring your Ollama usage that aren't included here? And have you guys found any other ways to monitor these llm calls made through ollama?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1r8j5ob"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gkarthi280"&gt; /u/gkarthi280 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rcxt3m/how_are_you_monitoring_your_ollama_callsusage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-23T23:32:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdjiu7</id>
    <title>How exactly do I go about making AI video?</title>
    <updated>2026-02-24T15:38:35+00:00</updated>
    <author>
      <name>/u/Sol33t303</name>
      <uri>https://old.reddit.com/user/Sol33t303</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm new to running local AI instances. I have an AMD 7800 XT, so 16GB of VRAM, and I use Alpaca in a flatpak for running my stuff, rocm seems to fail for whatever reason so I'm using vulkan which works fine.&lt;/p&gt; &lt;p&gt;What would be the best model for playing around with video generation for my hardware? And could somebody give an example prompt to get it to generate a video? In particular I would be interested in animating an existing image, I'd like to make it seem like it's breathing. &lt;/p&gt; &lt;p&gt;Thank you for the help :), I can't seem to find many resources on this in regards to running locally. I see references to hunyuan but they are a year old and I for some reason can't find it in alpaca.&lt;/p&gt; &lt;p&gt;EDIT: Ok so I think I have just realized alpaca simply doesn't have image generation support.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sol33t303"&gt; /u/Sol33t303 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdjiu7/how_exactly_do_i_go_about_making_ai_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdjiu7/how_exactly_do_i_go_about_making_ai_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdjiu7/how_exactly_do_i_go_about_making_ai_video/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T15:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd5ely</id>
    <title>Full GPU Acceleration for Ollama on Mac Pro 2013 (Dual FirePro D700) - Linux</title>
    <updated>2026-02-24T04:23:21+00:00</updated>
    <author>
      <name>/u/manu7irl</name>
      <uri>https://old.reddit.com/user/manu7irl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;# [Guide] Full GPU Acceleration for Ollama on Mac Pro 2013 (Dual FirePro D700) - Linux&lt;/p&gt; &lt;p&gt;Hey everyone! I finally managed to get full GPU acceleration working for **Ollama** on the legendary **Mac Pro 6.1 (2013 &amp;quot;Trashcan&amp;quot;)** running Nobara Linux (and it should work on other distros too).&lt;/p&gt; &lt;p&gt;The problem with these machines is that they have dual **AMD FirePro D700s (Tahiti XT)**. By default, Linux uses the legacy `radeon` driver for these cards. While `radeon` works for display, it **does not support Vulkan or ROCm**, meaning Ollama defaults to the CPU, which is slow as molasses.&lt;/p&gt; &lt;p&gt;### My Setup:&lt;/p&gt; &lt;p&gt;- **Model:** Mac Pro 6,1 (Late 2013)&lt;/p&gt; &lt;p&gt;- **CPU:** Xeon E5-1680 v2 (8C/16T @ 3.0 GHz)&lt;/p&gt; &lt;p&gt;- **RAM:** 32GB&lt;/p&gt; &lt;p&gt;- **GPU:** Dual AMD FirePro D700 (6GB each, 12GB total VRAM)&lt;/p&gt; &lt;p&gt;- **OS:** Nobara Linux (Fedora 40/41 base)&lt;/p&gt; &lt;p&gt;### The Solution:&lt;/p&gt; &lt;p&gt;We need to force the `amdgpu` driver for the Southern Islands (SI) architecture. Once `amdgpu` is active, Vulkan is enabled, and Ollama picks up both GPUs automatically!&lt;/p&gt; &lt;p&gt;### Performance (The Proof):&lt;/p&gt; &lt;p&gt;I'm currently testing **`qwen2.5-coder:14b`** (9GB model). &lt;/p&gt; &lt;p&gt;- **GPU Offload:** 100% (49/49 layers)&lt;/p&gt; &lt;p&gt;- **VRAM Split:** Perfectly balanced across both D700s (~4GB each)&lt;/p&gt; &lt;p&gt;- **Speed:** **~11.5 tokens/second** üöÄ&lt;/p&gt; &lt;p&gt;- **Total Response Time:** ~13.8 seconds for a standard coding prompt.&lt;/p&gt; &lt;p&gt;On CPU alone, this model was barely usable at &amp;lt;2 tokens/sec. This fix makes the Trashcan a viable local LLM workstation in 2026!&lt;/p&gt; &lt;p&gt;### How to do it:&lt;/p&gt; &lt;p&gt;**1. Update Kernel Parameters**&lt;/p&gt; &lt;p&gt;Add these to your GRUB configuration:&lt;/p&gt; &lt;p&gt;`radeon.si_support=0 amdgpu.si_support=1`&lt;/p&gt; &lt;p&gt;On Fedora/Nobara:&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;sudo sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;/GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;radeon.si_support=0 amdgpu.si_support=1 /' /etc/default/grub&lt;/p&gt; &lt;p&gt;sudo grub2-mkconfig -o /boot/grub2/grub.cfg&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;**2. Reboot**&lt;/p&gt; &lt;p&gt;`sudo reboot`&lt;/p&gt; &lt;p&gt;**3. Container Config (Crucial!)**&lt;/p&gt; &lt;p&gt;If you're running Ollama in a container (Podman or Docker), you MUST:&lt;/p&gt; &lt;p&gt;- Pass `/dev/dri` to the container.&lt;/p&gt; &lt;p&gt;- Set `OLLAMA_VULKAN=1`.&lt;/p&gt; &lt;p&gt;- Disable security labels (SecurityLabel=disable in Quadlet).&lt;/p&gt; &lt;p&gt;**Result:**&lt;/p&gt; &lt;p&gt;My D700s are now identified as **Vulkan0** and **Vulkan1** in Ollama logs, and they split the model VRAM perfectly! üöÄ&lt;/p&gt; &lt;p&gt;I've put together a GitHub-ready folder with scripts and configs here: [Link to your repo]&lt;/p&gt; &lt;p&gt;Hope this helps any fellow Trashcan owners out there trying to run local LLMs!&lt;/p&gt; &lt;p&gt;#MacPro #Linux #Ollama #SelfHosted #AMD #FireProD700&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/manu7irl"&gt; /u/manu7irl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd5ely/full_gpu_acceleration_for_ollama_on_mac_pro_2013/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd5ely/full_gpu_acceleration_for_ollama_on_mac_pro_2013/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rd5ely/full_gpu_acceleration_for_ollama_on_mac_pro_2013/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T04:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdmak0</id>
    <title>PersonaPlex-7B on Apple Silicon: full-duplex speech-to-speech in native Swift (MLX)</title>
    <updated>2026-02-24T17:17:39+00:00</updated>
    <author>
      <name>/u/ivan_digital</name>
      <uri>https://old.reddit.com/user/ivan_digital</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivan_digital"&gt; /u/ivan_digital &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1rd94lb/personaplex7b_on_apple_silicon_fullduplex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdmak0/personaplex7b_on_apple_silicon_fullduplex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdmak0/personaplex7b_on_apple_silicon_fullduplex/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T17:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd8cu5</id>
    <title>Built an open-source Ollama/MLX/OpenAI benchmark and leaderboard site with in-app submissions. Trying to test and collect more data.</title>
    <updated>2026-02-24T06:07:36+00:00</updated>
    <author>
      <name>/u/peppaz</name>
      <uri>https://old.reddit.com/user/peppaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rd8cu5/built_an_opensource_ollamamlxopenai_benchmark_and/"&gt; &lt;img alt="Built an open-source Ollama/MLX/OpenAI benchmark and leaderboard site with in-app submissions. Trying to test and collect more data." src="https://preview.redd.it/tcn61r39rdlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=386c5ea0626fa055a8b52140758687ce7f84b8c9" title="Built an open-source Ollama/MLX/OpenAI benchmark and leaderboard site with in-app submissions. Trying to test and collect more data." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peppaz"&gt; /u/peppaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tcn61r39rdlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd8cu5/built_an_opensource_ollamamlxopenai_benchmark_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rd8cu5/built_an_opensource_ollamamlxopenai_benchmark_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T06:07:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdeex0</id>
    <title>GPU issue with running Models locally</title>
    <updated>2026-02-24T12:02:07+00:00</updated>
    <author>
      <name>/u/Badincomputer</name>
      <uri>https://old.reddit.com/user/Badincomputer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have setup my rig with 5 Geforce GTX titan X maxwell gpus. &lt;/p&gt; &lt;p&gt;Nvidia driver is 532. 7b models are running fine but when i try to run modes 30b or higher. I just loads on only one gpu amd then just crashed. I have installed 64 gb ram as well but i have having issues running bigger models. I think this is the software or driver issue. Can anyone help me please&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Badincomputer"&gt; /u/Badincomputer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdeex0/gpu_issue_with_running_models_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdeex0/gpu_issue_with_running_models_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdeex0/gpu_issue_with_running_models_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T12:02:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdriz3</id>
    <title>Ho creato un assistente vocale completamente offline per Windows, senza cloud e senza chiavi API</title>
    <updated>2026-02-24T20:23:00+00:00</updated>
    <author>
      <name>/u/Immediate-Ice-9989</name>
      <uri>https://old.reddit.com/user/Immediate-Ice-9989</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rdriz3/ho_creato_un_assistente_vocale_completamente/"&gt; &lt;img alt="Ho creato un assistente vocale completamente offline per Windows, senza cloud e senza chiavi API" src="https://external-preview.redd.it/MVpQDQPKITwW08B1LLAzfF0SGmW9RjJPZbT0m70bl8I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66c300a100fdabfc64a6425e8875a8655dc75d77" title="Ho creato un assistente vocale completamente offline per Windows, senza cloud e senza chiavi API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Immediate-Ice-9989"&gt; /u/Immediate-Ice-9989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/u_Immediate-Ice-9989/comments/1rdrazk/i_built_a_fully_offline_voice_assistant_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdriz3/ho_creato_un_assistente_vocale_completamente/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdriz3/ho_creato_un_assistente_vocale_completamente/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T20:23:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdg1a8</id>
    <title>Stop treating every bug as ‚Äòhallucination‚Äô: a 16-problem atlas for Ollama + RAG</title>
    <updated>2026-02-24T13:19:24+00:00</updated>
    <author>
      <name>/u/StarThinker2025</name>
      <uri>https://old.reddit.com/user/StarThinker2025</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rdg1a8/stop_treating_every_bug_as_hallucination_a/"&gt; &lt;img alt="Stop treating every bug as ‚Äòhallucination‚Äô: a 16-problem atlas for Ollama + RAG" src="https://preview.redd.it/qasweqos0glg1.png?width=140&amp;amp;height=107&amp;amp;auto=webp&amp;amp;s=36eaf9cdbcd91b5c71f76376cdd00ab7fc9b2621" title="Stop treating every bug as ‚Äòhallucination‚Äô: a 16-problem atlas for Ollama + RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, i‚Äôve been building local RAG / agent stacks on top of Ollama for a while, and at some point I got tired of describing every bug as ‚Äúhallucination‚Äù or ‚ÄúRAG is trash‚Äù. so I did something slightly obsessive: I turned all the weird failure patterns into a fixed 16-slot ‚ÄúProblem Map‚Äù that I now use to debug my pipelines.&lt;/p&gt; &lt;p&gt;this map is now referenced in a few places (including the LlamaIndex RAG troubleshooting docs), but this post is not about ‚Äúlook at my repo‚Äù. it‚Äôs about: how to use a 16-problem checklist to debug your &lt;strong&gt;Ollama-based&lt;/strong&gt; RAG stack in a systematic way.&lt;/p&gt; &lt;p&gt;you don‚Äôt need to adopt any framework to use it. it is just text.&lt;/p&gt; &lt;h1&gt;0. link first, so you can skim while reading&lt;/h1&gt; &lt;p&gt;the map lives here as a single README:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;WFGY ProblemMap ‚Äì 16 real RAG / LLM failure modes with fixes&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md"&gt;https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;MIT-licensed, text only, no telemetry, no API.&lt;br /&gt; you can read it like a long blog post, or you can literally paste it into a model running under &lt;code&gt;ollama&lt;/code&gt; and ask it to reason with the map.&lt;/p&gt; &lt;h1&gt;1. what this ‚ÄúProblem Map‚Äù is (and is not)&lt;/h1&gt; &lt;p&gt;most tools we use for RAG focus on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;vector stores, indexes, retrievers&lt;/li&gt; &lt;li&gt;orchestration frameworks, agents, graphs&lt;/li&gt; &lt;li&gt;prompt templates and guardrails&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;this map is none of those.&lt;/p&gt; &lt;p&gt;it is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a &lt;strong&gt;16-slot failure catalog&lt;/strong&gt; for RAG / agent pipelines&lt;/li&gt; &lt;li&gt;each slot has: &lt;ul&gt; &lt;li&gt;a stable number (No.1 ‚Ä¶ No.16) that never changes&lt;/li&gt; &lt;li&gt;a short name you can say out loud&lt;/li&gt; &lt;li&gt;how it looks from the outside (user complaints, logs, traces)&lt;/li&gt; &lt;li&gt;which layer to inspect first (ingestion, embeddings, retriever, routing, reasoning)&lt;/li&gt; &lt;li&gt;a minimal structural fix that tends to stay fixed&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the basic idea is:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;instead of saying ‚Äúthe model hallucinated again‚Äù,&lt;br /&gt; you say ‚Äúthis is Problem No.3 plus a bit of No.7‚Äù,&lt;br /&gt; and you know roughly where to look.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;it works well with Ollama for a simple reason: a lot of us are already running everything locally, with multiple small components glued together. the map gives you a consistent vocabulary for the glue.&lt;/p&gt; &lt;h1&gt;2. where the ‚Äúsemantic firewall‚Äù sits: after vs before&lt;/h1&gt; &lt;p&gt;most of the ecosystem is about &lt;strong&gt;patching after the fact&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM gives a bad answer&lt;/li&gt; &lt;li&gt;you add a reranker, one more retrieval hop, another guardrail, maybe a second LLM call&lt;/li&gt; &lt;li&gt;your pipeline slowly becomes a jungle of patches&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the ProblemMap is designed as a &lt;strong&gt;semantic firewall before generation&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;you look at what the pipeline is about to do: &lt;ul&gt; &lt;li&gt;what got retrieved&lt;/li&gt; &lt;li&gt;how it was chunked and routed&lt;/li&gt; &lt;li&gt;how much of the user‚Äôs intent is actually covered&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;you classify the situation into one or two of the 16 problems&lt;/li&gt; &lt;li&gt;if the semantic state looks unstable, you loop, reset, or refuse to answer&lt;/li&gt; &lt;li&gt;only if things look healthy, you let the model speak&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;in other words:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;most people add more logic &lt;strong&gt;after&lt;/strong&gt; the model talks&lt;/li&gt; &lt;li&gt;this map helps you decide when the model should probably &lt;strong&gt;not&lt;/strong&gt; talk yet&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;you can still use whatever framework you like on top of Ollama (LlamaIndex, LangChain, custom Python, plain HTTP). the firewall is a way of thinking, not a competing framework.&lt;/p&gt; &lt;h1&gt;3. illusions vs reality in Ollama-based stacks&lt;/h1&gt; &lt;p&gt;here are a few patterns that keep coming up when people build local RAG / agents on top of Ollama.&lt;/p&gt; &lt;h1&gt;3.1 ‚Äúthe model is too small, it keeps hallucinating‚Äù&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;what it looks like&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you run a local 7B / 8B model through Ollama&lt;/li&gt; &lt;li&gt;user asks a detailed question about your docs&lt;/li&gt; &lt;li&gt;answer sounds fluent but misses critical constraints, or confidently mixes up two similar products&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;easy narrative:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;‚Äúthis tiny model is just dumb, I should upgrade.‚Äù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;what‚Äôs actually broken, very often:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;chunking ignores document structure&lt;/li&gt; &lt;li&gt;sections that belong together are split across different chunks&lt;/li&gt; &lt;li&gt;your retriever pulls top-k chunks that each have ‚Äúa bit of truth‚Äù, but never the full picture in one place&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;in the ProblemMap this is one of the ‚Äúsemantic chunking / segmentation‚Äù problems:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;from the outside it looks like hallucination&lt;/li&gt; &lt;li&gt;from the inside the retrieval was ‚Äúformally correct but semantically broken‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;practical fix directions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;switch from naive fixed-token chunking to structure-aware chunking (headings, sections, bullet groups)&lt;/li&gt; &lt;li&gt;ensure that logically indivisible blocks (like ‚Äúdefinition + exceptions‚Äù) live in the same chunk&lt;/li&gt; &lt;li&gt;for critical flows, add a pre-answer check that forces the model to list which clauses / IDs it sees in context, and refuse to answer if key ones are missing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the point is: this is a &lt;strong&gt;structural fix&lt;/strong&gt;, not a ‚Äúchange model‚Äù fix.&lt;/p&gt; &lt;h1&gt;3.2 ‚ÄúRAG is trash, my vector DB keeps pulling the wrong file‚Äù&lt;/h1&gt; &lt;p&gt;common setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you use Ollama as the LLM layer&lt;/li&gt; &lt;li&gt;embeddings come from somewhere (local or remote)&lt;/li&gt; &lt;li&gt;you toss them into qdrant / chroma / milvus / pgvector / faiss&lt;/li&gt; &lt;li&gt;your code is something like ‚Äútake top-k, join into a context, send to &lt;code&gt;ollama&lt;/code&gt;‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;symptoms:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;tiny wording changes cause completely different retrieved docs&lt;/li&gt; &lt;li&gt;sometimes an obviously relevant doc is ranked way down or missing&lt;/li&gt; &lt;li&gt;adding new files degrades older use cases&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;easy narrative:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;‚Äúthis vector DB sucks, I should switch, or embeddings are bad.‚Äù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;ProblemMap view:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;metric choice and normalization do not match the embedding family&lt;/li&gt; &lt;li&gt;blending multiple sources created index skew&lt;/li&gt; &lt;li&gt;query transformation step is pushing queries too far away from the actual semantics&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;in the map, this cluster lives under ‚Äúmetric / normalization mismatch‚Äù and ‚Äúindex &amp;amp; refresh skew‚Äù:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;it‚Äôs not that ‚Äúvector DB is low quality‚Äù&lt;/li&gt; &lt;li&gt;it‚Äôs that the geometry you assumed does not match what is actually stored&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;fix directions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;make sure metric (cosine / L2 / dot) matches the embedding‚Äôs training assumptions&lt;/li&gt; &lt;li&gt;normalize vectors consistently (or explicitly choose not to) across all indexes&lt;/li&gt; &lt;li&gt;monitor a small, fixed set of ‚Äúprobe queries‚Äù every time you change the corpus or indexing code, and compare their nearest neighbors over time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;again, the firewall idea is to name this as ‚Äúa metric / index problem‚Äù, not ‚ÄúRAG is trash‚Äù.&lt;/p&gt; &lt;h1&gt;3.3 ‚Äúmy agent sometimes just goes crazy‚Äù&lt;/h1&gt; &lt;p&gt;setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you run Ollama behind some orchestration: &lt;ul&gt; &lt;li&gt;small Python agent you wrote&lt;/li&gt; &lt;li&gt;or a graph in LlamaIndex / LangChain&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;tools include: &lt;ul&gt; &lt;li&gt;a docs index&lt;/li&gt; &lt;li&gt;an internal notes index&lt;/li&gt; &lt;li&gt;maybe a web search or code interpreter&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;sometimes it picks a beautiful chain, sometimes it does something that looks insane&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;easy narrative:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;‚Äúagents are chaotic toys, this is just the LLM being random.‚Äù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;ProblemMap view:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;tool / index routing spec is underspecified&lt;/li&gt; &lt;li&gt;a few edge cases are left completely to the LLM‚Äôs guess&lt;/li&gt; &lt;li&gt;there is no explicit ‚Äúresource policy‚Äù that says which sources are allowed for which intents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;symptoms:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;for a certain slice of queries, traces show the agent picking the same wrong tool over and over&lt;/li&gt; &lt;li&gt;retries bounce between totally different tool chains&lt;/li&gt; &lt;li&gt;prod and dev behave differently because some tools are wired slightly differently&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;this maps onto ‚Äúrouting contract mismatch‚Äù and ‚Äúsafety boundary leaks‚Äù in the ProblemMap.&lt;/p&gt; &lt;p&gt;fix directions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;explicitly classify queries into a small intent set first (even with a cheap classification model)&lt;/li&gt; &lt;li&gt;attach a strict ‚Äúallowed tools / indexes per intent‚Äù map, and apply hard filters before the LLM plans a chain&lt;/li&gt; &lt;li&gt;add a cheap pre-flight step that inspects the planned tool chain and vetoes it if it violates your policies (for example, ‚Äúnever use internal notes for external users‚Äù)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the key is: you fix the &lt;strong&gt;tool graph and routing spec&lt;/strong&gt;, not the answer wording.&lt;/p&gt; &lt;h1&gt;3.4 ‚Äúi fixed this last week, why is it broken again in a new place‚Äù&lt;/h1&gt; &lt;p&gt;this one is everywhere.&lt;/p&gt; &lt;p&gt;symptoms:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you fix a bug by adding a special case in a prompt or post-processor&lt;/li&gt; &lt;li&gt;users are happy for a week&lt;/li&gt; &lt;li&gt;then a new endpoint, new document set, or new type of question shows exactly the same failure pattern in a fresh coat of paint&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ProblemMap view:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you never gave the failure a stable name and number&lt;/li&gt; &lt;li&gt;you only treated it as ‚Äúthat one weird ticket from last week‚Äù&lt;/li&gt; &lt;li&gt;so when it reappears elsewhere, everyone treats it as brand new&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the 16-problem map forces you to say things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚Äúthis is yet another instance of No.4‚Äù&lt;/li&gt; &lt;li&gt;‚Äúthis incident smells like No.2 plus a bit of No.9‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;that may sound like a small naming trick, but over time it changes how teams debug.&lt;/p&gt; &lt;h1&gt;4. a concrete Ollama RAG example, step by step&lt;/h1&gt; &lt;p&gt;let‚Äôs make this less abstract.&lt;/p&gt; &lt;p&gt;say you have a simple local docs QA stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;documents in a &lt;code&gt;./docs&lt;/code&gt; folder (PDFs, Markdown, whatever)&lt;/li&gt; &lt;li&gt;you embed them and store vectors in your favorite DB&lt;/li&gt; &lt;li&gt;you wrap it with a small API that sends prompts to an Ollama model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;in pseudocode, the flow is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;user asks a question about a product, policy, or contract&lt;/li&gt; &lt;li&gt;you search the vector DB and fetch top-k chunks&lt;/li&gt; &lt;li&gt;you build a prompt like:&lt;/li&gt; &lt;li&gt;you call &lt;code&gt;ollama&lt;/code&gt; with that prompt&lt;/li&gt; &lt;li&gt;you stream back the answer&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;now imagine this incident:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;user asks: ‚Äúsummarize the warranty exclusions for product X‚Äù&lt;/li&gt; &lt;li&gt;you expect: a careful list of exclusions that match the PDF&lt;/li&gt; &lt;li&gt;you get: a very confident paragraph that sounds legal-ish, but: &lt;ul&gt; &lt;li&gt;misses a few critical clauses&lt;/li&gt; &lt;li&gt;mixes in a condition that belongs to product Y&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;how to run this through the 16-problem map:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;look at the retrieved chunks&lt;/strong&gt;, not just the answer &lt;ul&gt; &lt;li&gt;do we see all relevant sections for product X?&lt;/li&gt; &lt;li&gt;are chunks from product X and Y mixed?&lt;/li&gt; &lt;li&gt;are ‚Äúunless / except‚Äù clauses cut off at chunk boundaries?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;if chunks look bad: &lt;ul&gt; &lt;li&gt;this is mostly a chunking / segmentation issue (one of the early ProblemMap numbers)&lt;/li&gt; &lt;li&gt;maybe also an index organization issue (different product families not separated)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;fix in the pipeline: &lt;ul&gt; &lt;li&gt;change ingestion to split by headings or semantic units&lt;/li&gt; &lt;li&gt;create separate indexes per product family and add a very simple router&lt;/li&gt; &lt;li&gt;reduce &lt;code&gt;k&lt;/code&gt; once retrieval is more precise, to avoid mixing product lines&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;optionally, add a tiny semantic firewall check: &lt;ul&gt; &lt;li&gt;before answering, ask the model: &lt;ul&gt; &lt;li&gt;‚Äúlist all product names and IDs you see in the retrieved context‚Äù&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;if the list is more than one product family, refuse to answer and log it as a routing issue&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;once you‚Äôve done this once, you can label the incident as ‚ÄúProblemMap No.X‚Äù in your notes. the next time something similar happens on a different dataset, you‚Äôre not starting from zero.&lt;/p&gt; &lt;h1&gt;5. how to actually use the map with Ollama in practice&lt;/h1&gt; &lt;p&gt;you don‚Äôt need to implement a big framework. here are three simple modes that work well in a local setting.&lt;/p&gt; &lt;h1&gt;5.1 as a mental model + incident vocabulary&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;read the ProblemMap README once&lt;/li&gt; &lt;li&gt;when something weird happens, write a short incident note: &lt;ul&gt; &lt;li&gt;what the user asked&lt;/li&gt; &lt;li&gt;what you expected&lt;/li&gt; &lt;li&gt;what actually happened&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;skim the 16 problems and assign: &lt;ul&gt; &lt;li&gt;‚Äúthis feels like mostly No.A‚Äù&lt;/li&gt; &lt;li&gt;‚Äúthis smells like No.B + a bit of No.C‚Äù&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;put that in your commit message or issue title&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;over time you will see patterns:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚Äúwe hit No.3 and No.7 all the time‚Äù&lt;/li&gt; &lt;li&gt;‚Äúwe almost never see No.12‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;that is already useful.&lt;/p&gt; &lt;h1&gt;5.2 using an LLM (maybe through Ollama) as a triage helper&lt;/h1&gt; &lt;p&gt;if you are willing to involve an LLM in the debugging loop:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;take the ProblemMap README and load it into a strong model &lt;ul&gt; &lt;li&gt;can be local or remote&lt;/li&gt; &lt;li&gt;you can split it into chunks and keep it in a ‚Äúsystem memory‚Äù&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;for each tricky incident, feed: &lt;ul&gt; &lt;li&gt;the user query&lt;/li&gt; &lt;li&gt;the retrieved context&lt;/li&gt; &lt;li&gt;the answer&lt;/li&gt; &lt;li&gt;a short description of why it‚Äôs wrong&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;ask something like:&lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;‚ÄúUsing the 16-problem WFGY ProblemMap as ground truth, which problem numbers best explain this failure in my Ollama-based RAG pipeline, and what should I inspect first?‚Äù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;this is how many teams end up discovering that they have 2‚Äì3 ‚Äúfavorite failure modes‚Äù that dominate all their bug reports.&lt;/p&gt; &lt;h1&gt;5.3 turning it into a lightweight semantic firewall&lt;/h1&gt; &lt;p&gt;if you want to go a bit further, you can bake a small semantic firewall into your stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;add a pre-answer step: &lt;ul&gt; &lt;li&gt;inspect retrieved context and planned tool calls&lt;/li&gt; &lt;li&gt;optionally ask a model: ‚Äúdoes this look like any of the high-risk ProblemMap modes?‚Äù&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;if the answer is yes for some numbers you care about: &lt;ul&gt; &lt;li&gt;refuse to answer&lt;/li&gt; &lt;li&gt;or re-run retrieval with different parameters&lt;/li&gt; &lt;li&gt;or surface a warning to the caller&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;this does not require extra infra:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;it‚Äôs just another small function in your code&lt;/li&gt; &lt;li&gt;the ‚Äúfirewall spec‚Äù is the text of the ProblemMap itself&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the goal is not perfection. it‚Äôs to catch the worst ‚Äúlandmines‚Äù before they reach users.&lt;/p&gt; &lt;h1&gt;6. why I trust this map enough to share it here&lt;/h1&gt; &lt;p&gt;quick context so this doesn‚Äôt feel like a random checklist.&lt;/p&gt; &lt;p&gt;over time, this 16-problem view has been:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;adopted into the &lt;strong&gt;LlamaIndex RAG troubleshooting docs&lt;/strong&gt; as a structured failure-mode checklist&lt;/li&gt; &lt;li&gt;wrapped as a tool in &lt;strong&gt;Harvard MIMS Lab‚Äôs ToolUniverse&lt;/strong&gt; to triage LLM / RAG incidents by ProblemMap number&lt;/li&gt; &lt;li&gt;used as a failure taxonomy by the &lt;strong&gt;Rankify&lt;/strong&gt; project (University of Innsbruck)&lt;/li&gt; &lt;li&gt;listed as a practical debugging atlas in a &lt;strong&gt;Multimodal RAG survey&lt;/strong&gt; by QCRI‚Äôs LLM Lab&lt;/li&gt; &lt;li&gt;included in several curated ‚Äúawesome-style‚Äù lists under RAG / LLM debugging and reliability&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;all of that happened while keeping it very boring on purpose:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MIT license&lt;/li&gt; &lt;li&gt;text only&lt;/li&gt; &lt;li&gt;no binaries, no hosted API, no data collection&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;it‚Äôs basically a frozen notebook of hard-earned failures, cleaned up into 16 reusable patterns.&lt;/p&gt; &lt;h1&gt;7. what I‚Äôd love to learn from the Ollama community&lt;/h1&gt; &lt;p&gt;since many of the original incidents came from local / self-hosted stacks, it feels natural to bring this back to Ollama and see how it lands here.&lt;/p&gt; &lt;p&gt;if you are running:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;an Ollama-based RAG stack (with any vector DB)&lt;/li&gt; &lt;li&gt;a local agent / tool system&lt;/li&gt; &lt;li&gt;or just a bunch of scripts that glue Ollama to your own data&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôd be very curious about:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;which of the 16 problems you see the most in your own traces&lt;/li&gt; &lt;li&gt;which failure patterns you are hitting that don‚Äôt fit cleanly into any of the 16 slots&lt;/li&gt; &lt;li&gt;whether adding a bit of ‚Äúsemantic firewall before generation‚Äù feels realistic in your environment, or if your constraints make that too heavy&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;again, the entry point is just this README:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md"&gt;https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;if you have a weird incident and you‚Äôre comfortable sharing a redacted version, drop it in the comments and I can try to map it to ProblemMap numbers and suggest where in the stack to look first.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qasweqos0glg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3456b740e00b53b9469e5ed78ba44359e326e07c"&gt;16 Problem Map&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StarThinker2025"&gt; /u/StarThinker2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdg1a8/stop_treating_every_bug_as_hallucination_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdg1a8/stop_treating_every_bug_as_hallucination_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdg1a8/stop_treating_every_bug_as_hallucination_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T13:19:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd920r</id>
    <title>Built an app that connects Ollama to your clipboard with ‚å•C (macOS, open source)</title>
    <updated>2026-02-24T06:44:32+00:00</updated>
    <author>
      <name>/u/morning-cereals</name>
      <uri>https://old.reddit.com/user/morning-cereals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rd920r/built_an_app_that_connects_ollama_to_your/"&gt; &lt;img alt="Built an app that connects Ollama to your clipboard with ‚å•C (macOS, open source)" src="https://external-preview.redd.it/b3I0ajYxMzQxZWxnMRYe2Ofy5EHN92E2iHf3x_Xw6DIJymXP2cpGHUmRCdgH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eee47683fdc493bda6a62a1bc30963579bd0d86" title="Built an app that connects Ollama to your clipboard with ‚å•C (macOS, open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to skip the &lt;em&gt;copy ‚Üí switch window ‚Üí prompt + paste ‚Üí copy result ‚Üí paste&lt;/em&gt; loop and just have Ollama be &lt;em&gt;there&lt;/em&gt; whenever I copy something.&lt;/p&gt; &lt;p&gt;So I built Cai. Press Option+C on any text, it detects what you copied and shows actions powered by your Ollama models. It talks to your local server, so whatever model you're running just works.&lt;/p&gt; &lt;p&gt;Instead of sending everything to the LLM, it detects content type first:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üìç Addresses (Open in Maps)&lt;/li&gt; &lt;li&gt;üóìÔ∏è Meetings (Create Calendar Event)&lt;/li&gt; &lt;li&gt;üìù Short Text (Define, Reply, Explain)&lt;/li&gt; &lt;li&gt;üåç Long Text (Summarize, Translate)&lt;/li&gt; &lt;li&gt;üíª Code/JSON (Beautify, Explain)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can also trigger custom prompts on-the-fly, and save ones you reuse as shortcuts :)&lt;/p&gt; &lt;p&gt;It should automatically detect the Ollama endpoint after installing, otherwise you can manually set it via settings.&lt;br /&gt; This is free and open source project: &lt;a href="https://getcai.app/"&gt;https://getcai.app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been using Ministral 3B on my Macbook Air M2 16GB RAM, curious what works best for you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/morning-cereals"&gt; /u/morning-cereals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hhklml241elg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rd920r/built_an_app_that_connects_ollama_to_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rd920r/built_an_app_that_connects_ollama_to_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T06:44:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rduv42</id>
    <title>opencode with local ollama image-to-text model</title>
    <updated>2026-02-24T22:24:18+00:00</updated>
    <author>
      <name>/u/NickMcGurkThe3rd</name>
      <uri>https://old.reddit.com/user/NickMcGurkThe3rd</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NickMcGurkThe3rd"&gt; /u/NickMcGurkThe3rd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/opencodeCLI/comments/1rduuqr/opencode_with_local_ollama_imagetotext_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rduv42/opencode_with_local_ollama_imagetotext_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rduv42/opencode_with_local_ollama_imagetotext_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T22:24:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdl1fd</id>
    <title>What's the best model to run on mac m1 pro 16gb?</title>
    <updated>2026-02-24T16:33:37+00:00</updated>
    <author>
      <name>/u/Embarrassed-Baby3964</name>
      <uri>https://old.reddit.com/user/Embarrassed-Baby3964</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an old m1 mac pro with 16gb ram. Was wondering if there are any good performing models in 2026 that I can run on this hardware? And if so, what is the best one in your opinion?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Baby3964"&gt; /u/Embarrassed-Baby3964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdl1fd/whats_the_best_model_to_run_on_mac_m1_pro_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdl1fd/whats_the_best_model_to_run_on_mac_m1_pro_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdl1fd/whats_the_best_model_to_run_on_mac_m1_pro_16gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T16:33:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdw6oc</id>
    <title>Need a recommendation for a machine</title>
    <updated>2026-02-24T23:14:52+00:00</updated>
    <author>
      <name>/u/wavz89</name>
      <uri>https://old.reddit.com/user/wavz89</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wavz89"&gt; /u/wavz89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1rdw6d9/need_a_recommendation_for_a_machine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdw6oc/need_a_recommendation_for_a_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdw6oc/need_a_recommendation_for_a_machine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-24T23:14:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1re4kp7</id>
    <title>Does Ollama cloud feel slow?</title>
    <updated>2026-02-25T05:29:46+00:00</updated>
    <author>
      <name>/u/Safe_Concern2889</name>
      <uri>https://old.reddit.com/user/Safe_Concern2889</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bought 20$ subscription plan and trying to use glm and minimax. GLM takes almost 15-20 mins for simple tasks. Wanted to understand is this specific to ollama cloud or with other providers too?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Safe_Concern2889"&gt; /u/Safe_Concern2889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1re4kp7/does_ollama_cloud_feel_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1re4kp7/does_ollama_cloud_feel_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1re4kp7/does_ollama_cloud_feel_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T05:29:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdzgyb</id>
    <title>AI toolkit ‚Äî LiteLLM + n8n + Open WebUI in one Docker Compose</title>
    <updated>2026-02-25T01:30:39+00:00</updated>
    <author>
      <name>/u/Puzzleheaded-Dig-492</name>
      <uri>https://old.reddit.com/user/Puzzleheaded-Dig-492</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rdzgyb/ai_toolkit_litellm_n8n_open_webui_in_one_docker/"&gt; &lt;img alt="AI toolkit ‚Äî LiteLLM + n8n + Open WebUI in one Docker Compose" src="https://external-preview.redd.it/a8eQoVMnXY-S4cgiOUtyBMg1XNcN5q3piKq1KRHUsj8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50218f6c6108c3188b1bb498cf5c008d8d37a7f6" title="AI toolkit ‚Äî LiteLLM + n8n + Open WebUI in one Docker Compose" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just to make things simpler, I put together a simple Docker Compose stack that bundles everything you need to run a &lt;strong&gt;local AI environment&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LiteLLM&lt;/strong&gt; ‚Äî LLM proxy / API gateway (OpenAI-compatible)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;n8n&lt;/strong&gt; ‚Äî Workflow automation (think Zapier but self-hosted)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open WebUI&lt;/strong&gt; ‚Äî Chat interface for your LLMs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;30+ Ollama Cloud free models are pre-configured out of the box. Just create an [Ollama](vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/code/electron-browser/workbench/workbench.html) account, grab an API key, and &lt;code&gt;docker compose up -d&lt;/code&gt; üöÄ&lt;/p&gt; &lt;p&gt;You can also add more models later through LiteLLM ‚Äî whether local (e.g. Ollama, vLLM) or cloud (e.g. OpenAI, Anthropic, Azure).&lt;/p&gt; &lt;p&gt;üîó [&lt;a href="https://github.com/wa91h/local-ai-toolkit%5D(vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/code/electron-browser/workbench/workbench.html)"&gt;https://github.com/wa91h/local-ai-toolkit](vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/code/electron-browser/workbench/workbench.html)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Puzzleheaded-Dig-492"&gt; /u/Puzzleheaded-Dig-492 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/wa91h/local-ai-toolkit"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdzgyb/ai_toolkit_litellm_n8n_open_webui_in_one_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdzgyb/ai_toolkit_litellm_n8n_open_webui_in_one_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T01:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1refx2p</id>
    <title>Need recommendations or advice to do with my servers (coding - automations)</title>
    <updated>2026-02-25T15:15:05+00:00</updated>
    <author>
      <name>/u/Lotus-006</name>
      <uri>https://old.reddit.com/user/Lotus-006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1refx2p/need_recommendations_or_advice_to_do_with_my/"&gt; &lt;img alt="Need recommendations or advice to do with my servers (coding - automations)" src="https://preview.redd.it/xc7lvuveqnlg1.png?width=140&amp;amp;height=100&amp;amp;auto=webp&amp;amp;s=8e1c1cda81ded4381aaf3aa70a939c15f55f6c9f" title="Need recommendations or advice to do with my servers (coding - automations)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello , i'm not sure how to start i have 2 big servers, the MSI with the ultra core 7 i use it for my daily use on windows 11 and the other one the i9 for proxmox with media tool and hosting or vm.&lt;/p&gt; &lt;p&gt;yeah i know i'm stuck with my gpu on my MSI because the cpu in no integrated graphic i made a mistake when i purchased it and i have just one dGPU MSI Ventus 3x OC 16gb RTX 5070ti on it.&lt;/p&gt; &lt;p&gt;i want to use ollama with vscode but when i do ollama serve in windows and connect to the localhost:11434 i cant use the model in it its say&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Sorry, your request failed. Please try again.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Copilot Request id: b885a817-c6b5-4cb2-8111-1f34174b2481&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Reason: 404 page not found: Error: 404 page not found at u5._provideLanguageModelResponse&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;this is the computers i have :&lt;/p&gt; &lt;p&gt;(Z790 AORUS ELITE AX + i9 13900k + 96GB DDR5 6800Mhz - iGPU)&lt;/p&gt; &lt;p&gt;(MSI PRO Z890-S WIFI PZ LGA 1851 Intel Z890 + Ultra Core 7 265k + 32GB DDR5 6000Mhz)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xc7lvuveqnlg1.png?width=1547&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0bad8ef97db6a0766b7ba949dd5de95d200f239"&gt;https://preview.redd.it/xc7lvuveqnlg1.png?width=1547&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0bad8ef97db6a0766b7ba949dd5de95d200f239&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k4lln6uvqnlg1.png?width=1437&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38bff1fe6c7e6b2678f431229dd4f467b9209950"&gt;https://preview.redd.it/k4lln6uvqnlg1.png?width=1437&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38bff1fe6c7e6b2678f431229dd4f467b9209950&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lotus-006"&gt; /u/Lotus-006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1refx2p/need_recommendations_or_advice_to_do_with_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1refx2p/need_recommendations_or_advice_to_do_with_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1refx2p/need_recommendations_or_advice_to_do_with_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T15:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1reguw7</id>
    <title>Bosgame M5 / Ryzen AI MAX+ 395 (Radeon 8060S gfx1103) ‚Äî AMDGPU ‚ÄúMES failed / SDMA timeout / GPU reset‚Äù on Ubuntu 24.04.1 kernel 6.14 ‚Äî ROCm unusable, Ollama stuck on CPU</title>
    <updated>2026-02-25T15:49:39+00:00</updated>
    <author>
      <name>/u/CaterpillarCultural1</name>
      <uri>https://old.reddit.com/user/CaterpillarCultural1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all ‚Äî I‚Äôm running a Bosgame M5 mini PC with an AMD Ryzen AI MAX+ 395 APU on Ubuntu 24.04.1 LTS (kernel 6.14.0-1017-oem) with 128GB unified RAM. I‚Äôm trying to use the integrated Radeon 8060S GPU for local LLM inference via Ollama and/or llama.cpp, but I‚Äôm seeing repeated GPU resets that make compute completely unreliable.&lt;/p&gt; &lt;p&gt;Hardware&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô Machine: Bosgame M5 ‚àô APU: AMD Ryzen AI MAX+ 395 ‚àô GPU: Radeon 8060S (RDNA 3.5, gfx1103, Device ID 1586) ‚àô RAM: 128GB unified memory (shared CPU/GPU) ‚àô OS: Ubuntu 24.04.1 LTS ‚àô Kernel: 6.14.0-1017-oem &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Symptoms&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô Ollama runs on CPU only ‚Äî GPU either not picked up or immediately falls back after a reset ‚àô Random slowdowns; occasionally fast again after a GPU reset cycle ‚àô Kernel logs show continuous AMDGPU failures &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Relevant kernel log snippets (journalctl -k):&lt;/p&gt; &lt;p&gt;MES failed to respond to msg=REMOVE_QUEUE&lt;/p&gt; &lt;p&gt;MES might be in unrecoverable state, issue a GPU reset&lt;/p&gt; &lt;p&gt;[gfxhub] page fault ‚Ä¶ address 0x0000000000000000&lt;/p&gt; &lt;p&gt;ring sdma0 timeout ‚Ä¶ Starting sdma0 ring reset&lt;/p&gt; &lt;p&gt;GPU reset succeeded ‚Ä¶ VRAM is lost due to GPU reset!&lt;/p&gt; &lt;p&gt;resume of IP block &amp;lt;vpe\_v6\_1&amp;gt; failed -110&lt;/p&gt; &lt;p&gt;What I‚Äôve tried&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚àô OLLAMA\_VULKAN=1 set in systemd service ‚Äî no improvement ‚àô ROCm install attempts ‚Äî compute still falls back to CPU ‚àô OEM kernel 6.14 (Ubuntu‚Äôs latest for this hardware) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What I need&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. Has anyone gotten stable AMD GPU compute (ROCm or Vulkan) working on this specific APU (Ryzen AI MAX+ 395 / gfx1103) on Linux? 2. Are the MES/SDMA reset errors a known issue with this kernel or firmware stack ‚Äî and is there a known fix (different kernel, Mesa version, firmware package)? 3. If you‚Äôve solved this, what exact combination worked ‚Äî kernel version, Mesa, firmware, ROCm version? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note: The 128GB unified memory means VRAM is carved from system RAM ‚Äî not sure if this affects the instability. Any pointers on kernel params, firmware packages, or whether the OEM kernel is the problem would be hugely appreciated.üôèüòä&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CaterpillarCultural1"&gt; /u/CaterpillarCultural1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1reguw7/bosgame_m5_ryzen_ai_max_395_radeon_8060s_gfx1103/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1reguw7/bosgame_m5_ryzen_ai_max_395_radeon_8060s_gfx1103/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1reguw7/bosgame_m5_ryzen_ai_max_395_radeon_8060s_gfx1103/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T15:49:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rek5yy</id>
    <title>Flowise alternatives?</title>
    <updated>2026-02-25T17:44:47+00:00</updated>
    <author>
      <name>/u/warlocc_</name>
      <uri>https://old.reddit.com/user/warlocc_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing Flowise to set up a multi-model path for feeding context to my final model, but it seems like Flowise itself is very glitchy. Has anyone found alternatives that work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/warlocc_"&gt; /u/warlocc_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rek5yy/flowise_alternatives/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rek5yy/flowise_alternatives/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rek5yy/flowise_alternatives/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T17:44:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1rekdbi</id>
    <title>Getting Goose to actually work with local Ollama models ‚Äî what I ran into and what I built</title>
    <updated>2026-02-25T17:51:38+00:00</updated>
    <author>
      <name>/u/BenevolentJoker</name>
      <uri>https://old.reddit.com/user/BenevolentJoker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1rekdbi/getting_goose_to_actually_work_with_local_ollama/"&gt; &lt;img alt="Getting Goose to actually work with local Ollama models ‚Äî what I ran into and what I built" src="https://external-preview.redd.it/pbgfqwlMXm6IOOu8BQ8ITwlsb3n0jqJch4zsZIR-Pe8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09c91535c6e4b95c05f5c704c34d8baac8eee6d2" title="Getting Goose to actually work with local Ollama models ‚Äî what I ran into and what I built" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama adapted goose so that ollama on goose actually is able to more reliably use more than just 1-2 ollama models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BenevolentJoker"&gt; /u/BenevolentJoker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ravhqi/getting_goose_to_actually_work_with_local_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rekdbi/getting_goose_to_actually_work_with_local_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rekdbi/getting_goose_to_actually_work_with_local_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T17:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1re6pgx</id>
    <title>qwen3.5:35b-a3b is here.</title>
    <updated>2026-02-25T07:26:55+00:00</updated>
    <author>
      <name>/u/Space__Whiskey</name>
      <uri>https://old.reddit.com/user/Space__Whiskey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;yey. that is all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Space__Whiskey"&gt; /u/Space__Whiskey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1re6pgx/qwen3535ba3b_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1re6pgx/qwen3535ba3b_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1re6pgx/qwen3535ba3b_is_here/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T07:26:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1reeixj</id>
    <title>From Pikachu to ZYRON: We Built a Fully Local AI Desktop Assistant That Runs Completely Offline</title>
    <updated>2026-02-25T14:22:10+00:00</updated>
    <author>
      <name>/u/No-Mess-8224</name>
      <uri>https://old.reddit.com/user/No-Mess-8224</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago I posted here about a small personal project I was building called Pikachu, a local desktop voice assistant. Since then the project has grown way bigger than I expected, got contributions from some really talented people, and evolved into something much more serious. We renamed it to ZYRON and it has basically turned into a full local AI desktop assistant that runs entirely on your own machine.&lt;/p&gt; &lt;p&gt;The main goal has always been simple. I love the idea of AI assistants, but I hate the idea of my files, voice, screenshots, and daily computer activity being uploaded to cloud services. So we built the opposite. ZYRON runs fully offline using a local LLM through Ollama, and the entire system is designed around privacy first. Nothing gets sent anywhere unless I explicitly ask it to send something to my own Telegram.&lt;/p&gt; &lt;p&gt;You can control the PC with voice by saying a wake word and then speaking normally. It can open apps, control media, set volume, take screenshots, shut down the PC, search the web in the background, and run chained commands like opening a browser and searching something in one go. It also responds back using offline text to speech, which makes it feel surprisingly natural to use day to day.&lt;/p&gt; &lt;p&gt;The remote control side became one of the most interesting parts. From my phone I can message a Telegram bot and basically control my laptop from anywhere. If I forget a file, I can ask it to find the document I opened earlier and it sends the file directly to me. It keeps a 30 day history of file activity and lets me search it using natural language. That feature alone has already saved me multiple times.&lt;/p&gt; &lt;p&gt;We also leaned heavily into security and monitoring. ZYRON can silently capture screenshots, take webcam photos, record short audio clips, and send them to Telegram. If a laptop gets stolen and connects to the internet, it can report IP address, ISP, city, coordinates, and a Google Maps link. Building and testing that part honestly felt surreal the first time it worked.&lt;/p&gt; &lt;p&gt;On the productivity side it turned into a full system monitor. It can report CPU, RAM, battery, storage, running apps, and even read all open browser tabs. There is a clipboard history logger so copied text is never lost. There is a focus mode that kills distracting apps and closes blocked websites automatically. There is even a ‚Äúzombie process‚Äù monitor that detects apps eating RAM in the background and lets you kill them remotely.&lt;/p&gt; &lt;p&gt;One feature I personally love is the stealth research mode. There is a Firefox extension that creates a bridge between the browser and the assistant, so it can quietly open a background tab, read content, and close it without any window appearing. Asking random questions and getting answers from a laptop that looks idle is strangely satisfying.&lt;/p&gt; &lt;p&gt;The whole philosophy of the project is that it does not try to compete with giant cloud models at writing essays. Instead it focuses on being a powerful local system automation assistant that respects privacy. The local model is smaller, but for controlling a computer it is more than enough, and the tradeoff feels worth it.&lt;/p&gt; &lt;p&gt;We are planning a lot next. Linux and macOS support, geofence alerts, motion triggered camera capture, scheduling and automation, longer memory, and eventually a proper mobile companion app instead of Telegram. As local models improve, the assistant will naturally get smarter too.&lt;/p&gt; &lt;p&gt;This started as a weekend experiment and slowly turned into something I now use daily. I would genuinely love feedback, ideas, or criticism from people here. If you have ever wanted an AI assistant that lives only on your own machine, I think you might find this interesting.&lt;/p&gt; &lt;p&gt;GitHub Repo - &lt;a href="https://github.com/Surajkumar5050/zyron-assistant"&gt;Link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mess-8224"&gt; /u/No-Mess-8224 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1reeixj/from_pikachu_to_zyron_we_built_a_fully_local_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T14:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdyosq</id>
    <title>I built a locally-hosted AI agent that runs entirely on your own hardware no cloud, no subscriptions</title>
    <updated>2026-02-25T00:56:51+00:00</updated>
    <author>
      <name>/u/Janglerjoe</name>
      <uri>https://old.reddit.com/user/Janglerjoe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built LMAgent a pure Python AI agent that connects to any OpenAI-compatible LLM (LM Studio, Ollama, etc.) and actually does things on your computer.&lt;/p&gt; &lt;p&gt;No cloud. No API fees. No subscriptions. Runs 100% on your own hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it can do autonomously:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Read and write files&lt;/li&gt; &lt;li&gt;Run shell commands (bash / PowerShell)&lt;/li&gt; &lt;li&gt;Manage git (status, diff, commit, branch)&lt;/li&gt; &lt;li&gt;Track todos and multi-step plans&lt;/li&gt; &lt;li&gt;Spawn sub-agents to delegate tasks&lt;/li&gt; &lt;li&gt;Connect to external tools via MCP servers (web search, browsers, databases)&lt;/li&gt; &lt;li&gt;Schedule itself to wake up at a future time and resume work&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Three ways to run it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Terminal REPL ‚Äî conversational loop with a live background scheduler&lt;/li&gt; &lt;li&gt;One-shot CLI ‚Äî give it a task, get a result, exit&lt;/li&gt; &lt;li&gt;Web UI ‚Äî streaming tokens, inline tool calls, session browser, mobile-friendly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup is dead simple:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;pip install requests flask colorama&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Point it at your local LLM server&lt;/li&gt; &lt;li&gt;Set a workspace directory in a &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; &lt;li&gt;Run &lt;code&gt;python agent_main.py&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Works on Windows, macOS, and Linux. MIT licensed.&lt;/p&gt; &lt;p&gt;Would love feedback especially from anyone running it with larger models or unconventional LLM backends.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/janglerjoe-commits/LMAgent"&gt;https://github.com/janglerjoe-commits/LMAgent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Janglerjoe"&gt; /u/Janglerjoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdyosq/i_built_a_locallyhosted_ai_agent_that_runs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1rdyosq/i_built_a_locallyhosted_ai_agent_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1rdyosq/i_built_a_locallyhosted_ai_agent_that_runs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-25T00:56:51+00:00</published>
  </entry>
</feed>
