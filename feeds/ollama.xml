<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-29T23:34:13+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ns53ks</id>
    <title>Looking for Deepseek R1 model for essay writing with M3 MBA (16GB)</title>
    <updated>2025-09-27T20:14:01+00:00</updated>
    <author>
      <name>/u/avidrunner84</name>
      <uri>https://old.reddit.com/user/avidrunner84</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a quantized model that is recommended for essay writing - one that can run locally on M3 MBA with 16GB? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avidrunner84"&gt; /u/avidrunner84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ns53ks/looking_for_deepseek_r1_model_for_essay_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ns53ks/looking_for_deepseek_r1_model_for_essay_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ns53ks/looking_for_deepseek_r1_model_for_essay_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-27T20:14:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nse8va</id>
    <title>Triton: The Secret Sauce Behind Faster AI on Your Own GPU</title>
    <updated>2025-09-28T03:34:43+00:00</updated>
    <author>
      <name>/u/AggravatingGiraffe46</name>
      <uri>https://old.reddit.com/user/AggravatingGiraffe46</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggravatingGiraffe46"&gt; /u/AggravatingGiraffe46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf?utm_source=chatgpt.com"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nse8va/triton_the_secret_sauce_behind_faster_ai_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nse8va/triton_the_secret_sauce_behind_faster_ai_on_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T03:34:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsbk3c</id>
    <title>Training models</title>
    <updated>2025-09-28T01:13:25+00:00</updated>
    <author>
      <name>/u/jonahgcarpenter</name>
      <uri>https://old.reddit.com/user/jonahgcarpenter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying to train some super light AI models for smaller task in my applications architecture. Maybe 3-4 weeks ago I found a video from TechWithTim with a working baseline to build off of, that worked great for training an initial baseline. &lt;/p&gt; &lt;p&gt;Since then my architecture has changed and I went to revisit that code and now no matter what I do I always get an error about recompiling lama.cpp. I even explored other videos and Gemini to help fix this problem to no avail. &lt;/p&gt; &lt;p&gt;Has something changed to render these tutorials obsolete? Is there some existing application or place to make training new models easier? I’m just stepping my foot in the door with local ai usage and development so any tips would be much appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jonahgcarpenter"&gt; /u/jonahgcarpenter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsbk3c/training_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsbk3c/training_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nsbk3c/training_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T01:13:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrz1tx</id>
    <title>AppUse : Create virtual desktops for AI agents to focus on specific apps</title>
    <updated>2025-09-27T16:08:28+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nrz1tx/appuse_create_virtual_desktops_for_ai_agents_to/"&gt; &lt;img alt="AppUse : Create virtual desktops for AI agents to focus on specific apps" src="https://external-preview.redd.it/YWl6d3psZmFlcXJmMX6ZB2qtngjb8gjMyThUUgd5eO-QeupzbFEkT8WNsDs6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=237793ca093746459bc8a192609aa34e6f8a6e7e" title="AppUse : Create virtual desktops for AI agents to focus on specific apps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;App-Use lets you scope agents to just the apps they need. Instead of full desktop access, say &amp;quot;only work with Safari and Notes&amp;quot; or &amp;quot;just control iPhone Mirroring&amp;quot; - visual isolation without new processes for perfectly focused automation.&lt;/p&gt; &lt;p&gt;Running computer use on the entire desktop often causes agent hallucinations and loss of focus when they see irrelevant windows and UI elements. AppUse solves this by creating composited views where agents only see what matters, dramatically improving task completion accuracy&lt;/p&gt; &lt;p&gt;Currently macOS only (Quartz compositing engine).&lt;/p&gt; &lt;p&gt;Read the full guide: &lt;a href="https://trycua.com/blog/app-use"&gt;https://trycua.com/blog/app-use&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bz1tqpqaeqrf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nrz1tx/appuse_create_virtual_desktops_for_ai_agents_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nrz1tx/appuse_create_virtual_desktops_for_ai_agents_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-27T16:08:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns5f5h</id>
    <title>SearchAI can work with Ollama directly for RAG and Copilot use cases</title>
    <updated>2025-09-27T20:27:22+00:00</updated>
    <author>
      <name>/u/searchblox_searchai</name>
      <uri>https://old.reddit.com/user/searchblox_searchai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 SearchAI now works natively with Ollama for inference&lt;/p&gt; &lt;p&gt;You don’t need extra wrappers or connectors—SearchAI can directly call Ollama to run models locally or in your private setup. That means: • 🔒 Private + secure inference • ⚡ Lower latency (no external API calls) • 💸 On Prem, predictable deployments • 🔌 Plug into your RAG + Hybrid Search + Chatbot + Agent workflows out of the box&lt;/p&gt; &lt;p&gt;If you’re already using Ollama, you can now power enterprise-grade search + GenAI with SearchAI without leaving your environment.&lt;/p&gt; &lt;p&gt;👉 Anyone here already experimenting with SearchAI + Ollama? &lt;a href="https://developer.searchblox.com/docs/collection-dashboard"&gt;https://developer.searchblox.com/docs/collection-dashboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/searchblox_searchai"&gt; /u/searchblox_searchai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ns5f5h/searchai_can_work_with_ollama_directly_for_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ns5f5h/searchai_can_work_with_ollama_directly_for_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ns5f5h/searchai_can_work_with_ollama_directly_for_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-27T20:27:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nslgop</id>
    <title>best LLM for reasoning and analysis</title>
    <updated>2025-09-28T11:00:35+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;which is the best model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nslgop/best_llm_for_reasoning_and_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nslgop/best_llm_for_reasoning_and_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nslgop/best_llm_for_reasoning_and_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T11:00:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsw5jw</id>
    <title>AI-Built Products, Architectures, and the Future of the Industry</title>
    <updated>2025-09-28T18:46:01+00:00</updated>
    <author>
      <name>/u/umutkrts</name>
      <uri>https://old.reddit.com/user/umutkrts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I’m not very close to AI-native companies in the industry, but I’ve been curious about something for a while. I’d really appreciate it if you could answer and explain. (By AI-native, I mean companies building services on top of models, not the model developers themselves.) &lt;/p&gt; &lt;p&gt;1- How are AI-native companies doing? Are there any examples of companies that are profitable, successful, and achieving exponential user growth? What AI service do you provide to your users? Or, from your network, who is doing what? &lt;/p&gt; &lt;p&gt;2-How do these companies and products handle their architectures? How do they find the best architecture to run their services, and how do they manage costs? With these costs, how do they design and build services— is fine-tuning frequently used as a method? &lt;/p&gt; &lt;p&gt;3- What’s your take on the future of business models that create specific services using AI models? Do you think it can be a successful and profitable new business model, or is it just a trend filling temporary gaps?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umutkrts"&gt; /u/umutkrts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsw5jw/aibuilt_products_architectures_and_the_future_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsw5jw/aibuilt_products_architectures_and_the_future_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nsw5jw/aibuilt_products_architectures_and_the_future_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T18:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nswzde</id>
    <title>[Launch Ollama compatible] ShareAI (open beta) — decentralized AI gateway, Ollama-native</title>
    <updated>2025-09-28T19:18:20+00:00</updated>
    <author>
      <name>/u/Zealousideal_Toe6119</name>
      <uri>https://old.reddit.com/user/Zealousideal_Toe6119</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;ShareAI&lt;/strong&gt; lets anyone—power users, crypto-rig owners, even datacenters—share idle compute for AI inference and get paid.&lt;/p&gt; &lt;h1&gt;What it is (and why)&lt;/h1&gt; &lt;p&gt;Most &lt;strong&gt;AI gateways&lt;/strong&gt; today only let a handful of big inference providers plug in and profit—even when serving open models. &lt;strong&gt;We’re democratizing that&lt;/strong&gt;: with &lt;strong&gt;ShareAI&lt;/strong&gt;, we want to let &lt;strong&gt;anyone with a powerful PC, GPU rig, crypto miner, or even a full datacenter&lt;/strong&gt; join the supply side, &lt;strong&gt;share&lt;/strong&gt; capacity, and &lt;strong&gt;earn&lt;/strong&gt;. The network routes requests across independent providers so you can contribute when you’re free and &lt;strong&gt;burst to the network&lt;/strong&gt; when you’re busy.&lt;/p&gt; &lt;h1&gt;Ollama under the hood&lt;/h1&gt; &lt;p&gt;Install the &lt;strong&gt;ShareAI application&lt;/strong&gt; on your device. It integrates with the &lt;strong&gt;Ollama SDK/runtime&lt;/strong&gt; so you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Install new Ollama models&lt;/strong&gt; (pull, version, quantize)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Manage models&lt;/strong&gt; — decide exactly &lt;strong&gt;which models&lt;/strong&gt; to share into the network&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Operate locally&lt;/strong&gt; — start/stop, set limits, and monitor token streaming&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Ways to participate&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Rewards (earnings):&lt;/strong&gt; earn &lt;strong&gt;70%&lt;/strong&gt; of each inference routed to your device that completes successfully. Withdraw &lt;strong&gt;monthly&lt;/strong&gt; once you reach &lt;strong&gt;€100&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Exchange — Become an AI Prosumer:&lt;/strong&gt; share capacity on your schedule (idle windows or 24/7). When your SaaS demand exceeds your infra, &lt;strong&gt;offload overflow&lt;/strong&gt; to the network. ShareAI acts as a &lt;strong&gt;load balancer&lt;/strong&gt;, &lt;strong&gt;credits tokens to you&lt;/strong&gt;, and lets you &lt;strong&gt;redeem&lt;/strong&gt; them when you need extra capacity.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mission (give back):&lt;/strong&gt; optionally donate a percentage of earnings to NGOs (choose from five major categories).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Status / roadmap&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Windows client:&lt;/strong&gt; available now&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ubuntu, macOS, Docker:&lt;/strong&gt; targeted by end of November&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’d love &lt;strong&gt;developer feedback&lt;/strong&gt; on operator UX, lifecycle, metrics, scheduling/fairness, and pricing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kick the tires →&lt;/strong&gt; &lt;a href="https://shareai.now"&gt;&lt;strong&gt;shareai.now&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal_Toe6119"&gt; /u/Zealousideal_Toe6119 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nswzde/launch_ollama_compatible_shareai_open_beta/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nswzde/launch_ollama_compatible_shareai_open_beta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nswzde/launch_ollama_compatible_shareai_open_beta/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T19:18:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsy9q2</id>
    <title>Building Real Local AI Agents w/ Braintrust served off Ollama Experiments and Lessons Learned</title>
    <updated>2025-09-28T20:09:23+00:00</updated>
    <author>
      <name>/u/AIForOver50Plus</name>
      <uri>https://old.reddit.com/user/AIForOver50Plus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im using on my local dev rig GPT-OSS:120b served up on Ollama and I wanted to see evals and observability with those local models and frontier models so I ran a few experiments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Experiment Alpha:&lt;/strong&gt; Email Management Agent → lessons on modularity, logging, brittleness.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Experiment Bravo:&lt;/strong&gt; Turning logs into automated evaluations → catching regressions + selective re-runs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Next up:&lt;/strong&gt; model swapping, continuous regression tests, and human-in-the-loop feedback.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This isn’t theory. It’s running code + experiments you can check out here:&lt;br /&gt; 👉 &lt;a href="https://go.fabswill.com/braintrustdeepdive"&gt;https://go.fabswill.com/braintrustdeepdive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’d love feedback from this community — especially on failure modes or additional evals to add. What would &lt;em&gt;you&lt;/em&gt; test next?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIForOver50Plus"&gt; /u/AIForOver50Plus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsy9q2/building_real_local_ai_agents_w_braintrust_served/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsy9q2/building_real_local_ai_agents_w_braintrust_served/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nsy9q2/building_real_local_ai_agents_w_braintrust_served/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T20:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nswk4c</id>
    <title>🚀 Prompt Engineering Contest — Week 1 is LIVE! ✨</title>
    <updated>2025-09-28T19:01:42+00:00</updated>
    <author>
      <name>/u/Comfortable_Device50</name>
      <uri>https://old.reddit.com/user/Comfortable_Device50</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We wanted to create something fun for the community — a place where anyone who enjoys experimenting with AI and prompts can take part, challenge themselves, and learn along the way. That’s why we started the first ever Prompt Engineering Contest on Luna Prompts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://lunaprompts.com/contests"&gt;https://lunaprompts.com/contests&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here’s what you can do:&lt;/p&gt; &lt;p&gt;💡 Write creative prompts&lt;/p&gt; &lt;p&gt;🧩 Solve exciting AI challenges&lt;/p&gt; &lt;p&gt;🎁 Win prizes, certificates, and XP points&lt;/p&gt; &lt;p&gt;It’s simple, fun, and open to everyone. Jump in and be part of the very first contest — let’s make it big together! 🙌&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Device50"&gt; /u/Comfortable_Device50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nswk4c/prompt_engineering_contest_week_1_is_live/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nswk4c/prompt_engineering_contest_week_1_is_live/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nswk4c/prompt_engineering_contest_week_1_is_live/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T19:01:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsfdxs</id>
    <title>ArchGW 🚀 - Use Ollama-based LLMs with Anthropic client (release 0.3.13)</title>
    <updated>2025-09-28T04:38:00+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nsfdxs/archgw_use_ollamabased_llms_with_anthropic_client/"&gt; &lt;img alt="ArchGW 🚀 - Use Ollama-based LLMs with Anthropic client (release 0.3.13)" src="https://preview.redd.it/esrgrn3z3urf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2b09fb4a8a1066f72a29fbc36d10f78c44b73ea" title="ArchGW 🚀 - Use Ollama-based LLMs with Anthropic client (release 0.3.13)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just added support for cross-client streaming &lt;a href="https://github.com/katanemo/archgw"&gt;ArchGW 0.3.13&lt;/a&gt;, which lets you call Ollama compatible models through the Anthropic-clients (via the&lt;code&gt;/v1/messages&lt;/code&gt; API).&lt;/p&gt; &lt;p&gt;With Anthropic becoming popular (and a default) for many developers now this gives them native support for v1/messages for Ollama based models while enabling them to swap models in their agents without changing any client side code or do custom integration work for local models or 3rd party API-based models.&lt;/p&gt; &lt;p&gt;🙏🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/esrgrn3z3urf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsfdxs/archgw_use_ollamabased_llms_with_anthropic_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nsfdxs/archgw_use_ollamabased_llms_with_anthropic_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T04:38:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsojap</id>
    <title>Help with running Ai models with internet connectivity</title>
    <updated>2025-09-28T13:37:02+00:00</updated>
    <author>
      <name>/u/jimminecraftguy</name>
      <uri>https://old.reddit.com/user/jimminecraftguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have successfully installed ollama and open web ui in a Linux server vm on my proxmox server. Everything works nice and im very impressed. Im new to this and Im currently looking for a way for my models to connect and pull info from the internet. Id like it to be like how DeepSeek has an online search function. Im sorry in advanced, im very new to AI and Linux in general&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jimminecraftguy"&gt; /u/jimminecraftguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsojap/help_with_running_ai_models_with_internet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsojap/help_with_running_ai_models_with_internet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nsojap/help_with_running_ai_models_with_internet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T13:37:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntayc3</id>
    <title>Hardware for training/finetuning LLMs?</title>
    <updated>2025-09-29T06:37:28+00:00</updated>
    <author>
      <name>/u/Fantastic_Mud_389</name>
      <uri>https://old.reddit.com/user/Fantastic_Mud_389</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am considering getting a GPU of my own to train and finetune LLMs and other AI models, what do you usually use? Both locally and by renting. No way somebody actually has an H100 at their home&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic_Mud_389"&gt; /u/Fantastic_Mud_389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntayc3/hardware_for_trainingfinetuning_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntayc3/hardware_for_trainingfinetuning_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ntayc3/hardware_for_trainingfinetuning_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T06:37:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsyg5x</id>
    <title>What’s the closest I can get to gpt 5 mini performance with a mid tier gpu</title>
    <updated>2025-09-28T20:16:36+00:00</updated>
    <author>
      <name>/u/Practical_Employ4041</name>
      <uri>https://old.reddit.com/user/Practical_Employ4041</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve got a pc with a amd 6800 gpu with 16gb of vram, and I’m trying to get as close to gpt5 mini performance as I can from a locally hosted model. What do you reccomend for my hardware? I’m liking gemma3:12b so far but I’d be interested in what other options are out there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Practical_Employ4041"&gt; /u/Practical_Employ4041 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsyg5x/whats_the_closest_i_can_get_to_gpt_5_mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nsyg5x/whats_the_closest_i_can_get_to_gpt_5_mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nsyg5x/whats_the_closest_i_can_get_to_gpt_5_mini/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T20:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntdo6w</id>
    <title>LLM Visualization (by Bycroft / bbycroft.net) — An interactive 3D animation of GPT-style inference: walk through layers, see tensor shapes, attention flows, etc.</title>
    <updated>2025-09-29T09:40:39+00:00</updated>
    <author>
      <name>/u/AggravatingGiraffe46</name>
      <uri>https://old.reddit.com/user/AggravatingGiraffe46</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggravatingGiraffe46"&gt; /u/AggravatingGiraffe46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bbycroft.net/llm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntdo6w/llm_visualization_by_bycroft_bbycroftnet_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ntdo6w/llm_visualization_by_bycroft_bbycroftnet_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T09:40:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt2jhl</id>
    <title>Ollama Desktop</title>
    <updated>2025-09-28T23:10:23+00:00</updated>
    <author>
      <name>/u/bbzzo</name>
      <uri>https://old.reddit.com/user/bbzzo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nt2jhl/ollama_desktop/"&gt; &lt;img alt="Ollama Desktop" src="https://preview.redd.it/ktejon4fmzrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b34778c315d171ebd28722c7c5d522e436cf5a7" title="Ollama Desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I’m an Ollama enthusiast and I use Ollama Desktop for Mac. Recently, there were some updates, and I noticed in the documentation that there are new features. I downloaded the latest version, but they’re not showing up. Does anyone know what I need to do to enable these features? I’ve highlighted what I’m talking about in the image.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bbzzo"&gt; /u/bbzzo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ktejon4fmzrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nt2jhl/ollama_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nt2jhl/ollama_desktop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-28T23:10:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntegzo</id>
    <title>LLM Evaluations with different quantizations</title>
    <updated>2025-09-29T10:31:37+00:00</updated>
    <author>
      <name>/u/leki483</name>
      <uri>https://old.reddit.com/user/leki483</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I usually check Artificial Analysis and some LLM arena leaderboards to get a rough idea of the intelligence of open-weight models. However, I have always wondered about the performance of those models after quantization (given that ollama provides all those models in different quantized versions).&lt;/p&gt; &lt;p&gt;Do you know any place where I could find those results in any of the main evals (MMLU-Pro, GPQA, LiveCodeBench, SciCode, HumanEval, Humanity's last exam, etc.)? So that I don't have to evaluate them myself.&lt;/p&gt; &lt;p&gt;Thank you so much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leki483"&gt; /u/leki483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntegzo/llm_evaluations_with_different_quantizations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntegzo/llm_evaluations_with_different_quantizations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ntegzo/llm_evaluations_with_different_quantizations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T10:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt5fcr</id>
    <title>How do I get ollama to use the igpu on the AMD AI Max+ 395?</title>
    <updated>2025-09-29T01:29:30+00:00</updated>
    <author>
      <name>/u/StartupTim</name>
      <uri>https://old.reddit.com/user/StartupTim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On debian 13 on a framework desktop (amd ai max+ 395), so I have the trixie-backports firmware-amd-graphics installed as well as the ollama rocm as seen &lt;a href="https://ollama.com/download/ollama-linux-amd64-rocm.tgz"&gt;https://ollama.com/download/ollama-linux-amd64-rocm.tgz&lt;/a&gt; yet when I run ollama it still uses 100% CPU. I can't get it to see the GPU at all.&lt;/p&gt; &lt;p&gt;Any idea on what to do? &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StartupTim"&gt; /u/StartupTim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nt5fcr/how_do_i_get_ollama_to_use_the_igpu_on_the_amd_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nt5fcr/how_do_i_get_ollama_to_use_the_igpu_on_the_amd_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nt5fcr/how_do_i_get_ollama_to_use_the_igpu_on_the_amd_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T01:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nta4sf</id>
    <title>Low memory models</title>
    <updated>2025-09-29T05:45:40+00:00</updated>
    <author>
      <name>/u/Punkygdog</name>
      <uri>https://old.reddit.com/user/Punkygdog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to run ollama on a low resource system. It only has about 8gb of memory available, am I reading correctly that there are very few models that I can get to work in this situation (models that support image analysis)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Punkygdog"&gt; /u/Punkygdog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nta4sf/low_memory_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nta4sf/low_memory_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nta4sf/low_memory_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T05:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntcpam</id>
    <title>Incomplete output from finetuned llama3.1.</title>
    <updated>2025-09-29T08:35:12+00:00</updated>
    <author>
      <name>/u/PurpleCheap1285</name>
      <uri>https://old.reddit.com/user/PurpleCheap1285</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone&lt;/p&gt; &lt;p&gt;I run Ollama with finetuned llama3.1 on 3 PowerShell terminals in parallel. I get correct output on first terminal, but I get incomplete output on 2nd and 3rd terminal. Can someone guide me about this problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PurpleCheap1285"&gt; /u/PurpleCheap1285 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntcpam/incomplete_output_from_finetuned_llama31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntcpam/incomplete_output_from_finetuned_llama31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ntcpam/incomplete_output_from_finetuned_llama31/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T08:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nthjju</id>
    <title>Made a hosted UI for local LLM, originally for docker model runner, can be used with ollama too</title>
    <updated>2025-09-29T13:07:09+00:00</updated>
    <author>
      <name>/u/binuuday</name>
      <uri>https://old.reddit.com/user/binuuday</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nthjju/made_a_hosted_ui_for_local_llm_originally_for/"&gt; &lt;img alt="Made a hosted UI for local LLM, originally for docker model runner, can be used with ollama too" src="https://preview.redd.it/r8dqk7guq3sf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=f5ce95bd2e9fbf898711fb9141cbdd8b371b164e" title="Made a hosted UI for local LLM, originally for docker model runner, can be used with ollama too" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Made a simple online chat ui for docker model runner. But there is a CORS option request failing on docker model runner implemenation (have updated an existing bug)&lt;/p&gt; &lt;p&gt;I know there are so many UI's for docker. But do try this out, if you have time.&lt;/p&gt; &lt;p&gt;&lt;a href="https://binuud.com/staging/aiChat"&gt;https://binuud.com/staging/aiChat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It requires Google Chrome or Firefox to run. Instructions on enabling CORS in the tool itself.&lt;/p&gt; &lt;p&gt;For ollama issue start same using&lt;/p&gt; &lt;p&gt;export OLLAMA_ORIGINS=&amp;quot;&lt;a href="https://binuud.com"&gt;https://binuud.com&lt;/a&gt;&amp;quot;&lt;/p&gt; &lt;p&gt;ollama serve&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/binuuday"&gt; /u/binuuday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r8dqk7guq3sf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nthjju/made_a_hosted_ui_for_local_llm_originally_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nthjju/made_a_hosted_ui_for_local_llm_originally_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T13:07:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nteept</id>
    <title>Dead-simple example code for MCP with Ollama.</title>
    <updated>2025-09-29T10:27:45+00:00</updated>
    <author>
      <name>/u/kirill_saidov</name>
      <uri>https://old.reddit.com/user/kirill_saidov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nteept/deadsimple_example_code_for_mcp_with_ollama/"&gt; &lt;img alt="Dead-simple example code for MCP with Ollama." src="https://external-preview.redd.it/u58dHZPDQal1dcDvgrTyHK2TMwy2_0EMHtoQwPERqi0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be75b773b0fa0d092b8d75639b98721a816757e5" title="Dead-simple example code for MCP with Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This example shows how to use MCP with Ollama by implementing a super simple MCP client and server in Python.&lt;/p&gt; &lt;p&gt;I made it for people like me who got frustrated with Claude MCP videos and existing &lt;code&gt;mcphosts&lt;/code&gt; that hide all the actual logic. This repo walks through everything step by step so you can see exactly how the pieces fit together.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kirill_saidov"&gt; /u/kirill_saidov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kirillsaidov/ollama-mcp-example"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nteept/deadsimple_example_code_for_mcp_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nteept/deadsimple_example_code_for_mcp_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T10:27:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntpy87</id>
    <title>Windows ollama using CPU</title>
    <updated>2025-09-29T18:30:43+00:00</updated>
    <author>
      <name>/u/fcnealv</name>
      <uri>https://old.reddit.com/user/fcnealv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using 5060ti 16gb and amd r5 5600x. I pull qwen coder 2.5 14b.. I noticed that my CPU doing the workload? What's the solution to force it to use my gpu&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fcnealv"&gt; /u/fcnealv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntpy87/windows_ollama_using_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntpy87/windows_ollama_using_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ntpy87/windows_ollama_using_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T18:30:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntne0p</id>
    <title>Why dont it recognize my GPU</title>
    <updated>2025-09-29T16:55:28+00:00</updated>
    <author>
      <name>/u/maybesomenone</name>
      <uri>https://old.reddit.com/user/maybesomenone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ntne0p/why_dont_it_recognize_my_gpu/"&gt; &lt;img alt="Why dont it recognize my GPU" src="https://preview.redd.it/iluqhuiaw4sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40f0f6bc7c84fd70f7bc3b079c91245acd31e989" title="Why dont it recognize my GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why ollama does not recognize my GPU to run the models? what am i doing wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maybesomenone"&gt; /u/maybesomenone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iluqhuiaw4sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ntne0p/why_dont_it_recognize_my_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ntne0p/why_dont_it_recognize_my_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T16:55:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nthurw</id>
    <title>I built a private AI Meeting Note Taker that runs 100% offline.</title>
    <updated>2025-09-29T13:20:45+00:00</updated>
    <author>
      <name>/u/Prudent-Meringue845</name>
      <uri>https://old.reddit.com/user/Prudent-Meringue845</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nthurw/i_built_a_private_ai_meeting_note_taker_that_runs/"&gt; &lt;img alt="I built a private AI Meeting Note Taker that runs 100% offline." src="https://external-preview.redd.it/P-TYv9mTeSOOCmZRCbHzYZzYlFIWTdCXucO7FkZ03u0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7db73e296d0a5500ae480c4deb5b0efd99fcd95e" title="I built a private AI Meeting Note Taker that runs 100% offline." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prudent-Meringue845"&gt; /u/Prudent-Meringue845 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/data-science-collective/i-built-an-self-hosted-ai-meeting-note-taker-that-runs-100-offline-heres-how-you-can-too-d110b7ef0b95"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nthurw/i_built_a_private_ai_meeting_note_taker_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nthurw/i_built_a_private_ai_meeting_note_taker_that_runs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-29T13:20:45+00:00</published>
  </entry>
</feed>
