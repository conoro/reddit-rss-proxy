<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-31T08:45:19+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qp9c9x</id>
    <title>AI started speaking in Russian out of nowhere</title>
    <updated>2026-01-28T11:54:45+00:00</updated>
    <author>
      <name>/u/No-Sky2462</name>
      <uri>https://old.reddit.com/user/No-Sky2462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qp9c9x/ai_started_speaking_in_russian_out_of_nowhere/"&gt; &lt;img alt="AI started speaking in Russian out of nowhere" src="https://preview.redd.it/jckp73fxv2gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3eaf242a017b1f3ab7b8bb92952977659fe2d5e" title="AI started speaking in Russian out of nowhere" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I am a hobbyist, who is interested in AI and LLM. I don't know much about how these LLM function but i was interested so i installed one using terminal. The installation completed successfully but the ai started speaking in russian out of nowhere. Is this intentional behaviour for a LLM or did i do something wrong? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Sky2462"&gt; /u/No-Sky2462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jckp73fxv2gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp9c9x/ai_started_speaking_in_russian_out_of_nowhere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qp9c9x/ai_started_speaking_in_russian_out_of_nowhere/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T11:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq8waa</id>
    <title>usage notice</title>
    <updated>2026-01-29T13:43:03+00:00</updated>
    <author>
      <name>/u/Steus_au</name>
      <uri>https://old.reddit.com/user/Steus_au</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qq8waa/usage_notice/"&gt; &lt;img alt="usage notice" src="https://b.thumbs.redditmedia.com/SqHPvTVMklCt0JHRkpI5_QoTK-g4JjPLMe70_uKSY0c.jpg" title="usage notice" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;in their cloud - take note that an hour usage is the weekly usage:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x79i5jddlagg1.png?width=791&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=73586b6e6a7125072ddd0d57e886cfa249ac355e"&gt;https://preview.redd.it/x79i5jddlagg1.png?width=791&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=73586b6e6a7125072ddd0d57e886cfa249ac355e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Steus_au"&gt; /u/Steus_au &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq8waa/usage_notice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq8waa/usage_notice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qq8waa/usage_notice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T13:43:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqwp3i</id>
    <title>I taught a 2023 model about the year 2025. Here is the proof.</title>
    <updated>2026-01-30T05:26:43+00:00</updated>
    <author>
      <name>/u/ChikenNugetBBQSauce</name>
      <uri>https://old.reddit.com/user/ChikenNugetBBQSauce</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qqwp3i/i_taught_a_2023_model_about_the_year_2025_here_is/"&gt; &lt;img alt="I taught a 2023 model about the year 2025. Here is the proof." src="https://a.thumbs.redditmedia.com/uLdZiZAzKRTeW-sfJqHO8SLaU6yRYQ_7XYV2q8i07r0.jpg" title="I taught a 2023 model about the year 2025. Here is the proof." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just broke the training cutoff.&lt;/p&gt; &lt;p&gt;TinyLlama 1.1B was trained back in 2023. Rust 1.85 releases Feb 20, 2025.&lt;/p&gt; &lt;p&gt;By all logic, this model should not know this date exists. But as you can see, it didn't guess. It retrieved the exact date and feature from my local Rust memory graph.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bkjcgk9o9fgg1.png?width=2388&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b227144107a227c555b4da1a6cb72155ca61974d"&gt;https://preview.redd.it/bkjcgk9o9fgg1.png?width=2388&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b227144107a227c555b4da1a6cb72155ca61974d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We don't need bigger models. We need systems that remember&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChikenNugetBBQSauce"&gt; /u/ChikenNugetBBQSauce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqwp3i/i_taught_a_2023_model_about_the_year_2025_here_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqwp3i/i_taught_a_2023_model_about_the_year_2025_here_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qqwp3i/i_taught_a_2023_model_about_the_year_2025_here_is/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T05:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqt0dm</id>
    <title>I keep going.</title>
    <updated>2026-01-30T02:31:31+00:00</updated>
    <author>
      <name>/u/Alone-Competition863</name>
      <uri>https://old.reddit.com/user/Alone-Competition863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qqt0dm/i_keep_going/"&gt; &lt;img alt="I keep going." src="https://preview.redd.it/kh9ksbwwdegg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05e38dd7afe2dea6ec9fb1eb57bd1b53be536ee7" title="I keep going." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Qwen-30B Coding Agent: 50k to 15k Context Optimization &amp;amp; Custom Assembler&lt;/p&gt; &lt;p&gt;I managed to tune my autonomous agent so that it can efficiently handle Qwen-30B-Coder (Q5_K_M) even during demanding web development. The biggest improvement was in memory management and code merging logic.&lt;/p&gt; &lt;p&gt;Main highlights:&lt;/p&gt; &lt;p&gt;Context optimization: I went from unstable 50,000 tokens to highly efficient 15,000 tokens. Thanks to a smarter prompter, the model does not lose logic and saves VRAM.&lt;/p&gt; &lt;p&gt;Custom Code Assembler (v3.0): I implemented a system that does not constantly rewrite entire files, but intelligently &amp;quot;stitches&amp;quot; changes. If merging fails, the system has a &amp;quot;fallback&amp;quot; to a proven legacy version.&lt;/p&gt; &lt;p&gt;Local infrastructure (No LM Studio needed): Models do not run through external interfaces like LM Studio. I have them stored directly in my own folder (/models/) and I call them directly via Hybrid Engine (llama-cpp-python/ollama integration), which radically reduces latency and system resource requirements.&lt;/p&gt; &lt;p&gt;30B Model on home HW: I have reached a state where the 30B model generates complex Cyberpunk UI and Flask backends with the logic of a senior developer, while Code Fixer (Qwen-7B) runs in parallel in the background and cleans up the syntax.&lt;/p&gt; &lt;p&gt;Current success:&lt;/p&gt; &lt;p&gt;My agent just generated a fully functional AI Assistant with its own SQLite database, Cyberpunk visuals and dynamic station filtering. The prompt length was over 33,000 characters, but thanks to segmentation the system processed it in a 15k window without crashing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alone-Competition863"&gt; /u/Alone-Competition863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kh9ksbwwdegg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqt0dm/i_keep_going/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qqt0dm/i_keep_going/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T02:31:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqkzfr</id>
    <title>Running a second Moltbot (Clawdbot) instance on the same Mac as a watchdog?</title>
    <updated>2026-01-29T21:03:16+00:00</updated>
    <author>
      <name>/u/Cool_Metal1606</name>
      <uri>https://old.reddit.com/user/Cool_Metal1606</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi :)&lt;/p&gt; &lt;p&gt;I'm running Moltbot (formerly Clawd.bot) on my MacBook. Occasionally, the agent hangs or becomes unresponsive via Telegram.&lt;/p&gt; &lt;p&gt;I had the idea of setting up a second, separate Moltbot instance on the same machine acting as a &amp;quot;watchdog.&amp;quot; The plan is: if the main bot freezes, I message the second bot (via a different Telegram token), and it executes a command to kill/restart the main Moltbot process.&lt;/p&gt; &lt;p&gt;And couldn't this watchdog also act as a kind of cybersecurity bouncer, keeping an eye on the mainbot?&lt;/p&gt; &lt;p&gt;Does this even make sense, and is it technically possible? Has anyone tried it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool_Metal1606"&gt; /u/Cool_Metal1606 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqkzfr/running_a_second_moltbot_clawdbot_instance_on_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqkzfr/running_a_second_moltbot_clawdbot_instance_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qqkzfr/running_a_second_moltbot_clawdbot_instance_on_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T21:03:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqmly2</id>
    <title>Trying to learn how to use Claude Code with Ollama models but it won't even create a CLAUDE.md file with /init</title>
    <updated>2026-01-29T22:04:26+00:00</updated>
    <author>
      <name>/u/o-rka</name>
      <uri>https://old.reddit.com/user/o-rka</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/o-rka"&gt; /u/o-rka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ClaudeCode/comments/1qqml46/trying_to_learn_how_to_use_claude_code_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqmly2/trying_to_learn_how_to_use_claude_code_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qqmly2/trying_to_learn_how_to_use_claude_code_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T22:04:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq8mas</id>
    <title>figured out how to use ollama + moltbot together (local thinking, cloud doing)</title>
    <updated>2026-01-29T13:31:35+00:00</updated>
    <author>
      <name>/u/Round_Net_225</name>
      <uri>https://old.reddit.com/user/Round_Net_225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;saw that post yesterday asking about ollama with moltbot and had the same question last week.&lt;/p&gt; &lt;p&gt;here's what worked: don't run full moltbot locally (too heavy). use ollama for thinking, cloud for execution.&lt;/p&gt; &lt;p&gt;my setup:&lt;/p&gt; &lt;p&gt;¬∑ ollama local with llama3.1 for reasoning&lt;/p&gt; &lt;p&gt;¬∑ shell_clawd_bot for actual tasks&lt;/p&gt; &lt;p&gt;¬∑ they talk through simple api&lt;/p&gt; &lt;p&gt;basically ollama plans it, cloud does it.&lt;/p&gt; &lt;p&gt;why this works:&lt;/p&gt; &lt;p&gt;¬∑ ollama stays free for back-and-forth thinking&lt;/p&gt; &lt;p&gt;¬∑ only pay for execution (like 10% of calls)&lt;/p&gt; &lt;p&gt;¬∑ agent runs 24/7 without rpi staying on&lt;/p&gt; &lt;p&gt;been running on raspberry pi 5 with 16gb for 2 weeks. free trial covered testing, now ~$20/month for the cloud part.&lt;/p&gt; &lt;p&gt;their telegram group helped with the setup. surprisingly easy to connect.&lt;/p&gt; &lt;p&gt;not sure if this is what that other person meant but it's been solid for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Round_Net_225"&gt; /u/Round_Net_225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq8mas/figured_out_how_to_use_ollama_moltbot_together/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq8mas/figured_out_how_to_use_ollama_moltbot_together/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qq8mas/figured_out_how_to_use_ollama_moltbot_together/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T13:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qprl7a</id>
    <title>[Opinion] Why I believe the $20/month Ollama Cloud is a better investment than ChatGPT or Claude</title>
    <updated>2026-01-28T23:23:23+00:00</updated>
    <author>
      <name>/u/AlexHardy08</name>
      <uri>https://old.reddit.com/user/AlexHardy08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; I am not affiliated with Ollama in any way. This is purely based on my personal experience as a long-term user.&lt;/p&gt; &lt;p&gt;I‚Äôve been using Ollama since it first launched, and it has genuinely changed my workflow. Even with a powerful local machine, there are certain walls you eventually hit. Lately, I‚Äôve been testing the $20/month Cloud plan, and I wanted to share why I think it‚Äôs worth every penny.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The &amp;quot;Large Model&amp;quot; Barrier&lt;/strong&gt;&lt;br /&gt; We are seeing incredible models being released, like &lt;strong&gt;Kimi-k2.5&lt;/strong&gt;, DeepSeek, GLM, and various Open-Source versions of top-tier models. For 99% of us, running these locally is simply impossible unless you have a $30,000+ rig.&lt;/p&gt; &lt;p&gt;Yes, there is a free tier for Ollama Cloud, but we have to be realistic: running these massive models requires serious computation power. The paid plan gives you the stability and speed that a professional workflow requires.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I chose this over a ChatGPT/Claude subscription:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;The Ecosystem:&lt;/strong&gt; Instead of being locked into one model like GPT-5, I have immediate access to a variety of state-of-the-art models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simplicity:&lt;/strong&gt; If you have Ollama installed, you already know the drill. Switching to a cloud-hosted massive model is as simple as ollama run kimi-k2.5. No complex configurations, no manual weight management. It just works, and it‚Äôs incredibly fast.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ROI (Return on Investment):&lt;/strong&gt; If you are building something or doing serious work and don't have the budget for a custom local cluster, this $20 investment pays for itself almost immediately. It bridges the gap between &amp;quot;hobbyist&amp;quot; and &amp;quot;enterprise-level&amp;quot; capabilities.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Only Downside&lt;/strong&gt;&lt;br /&gt; If I had to nitpick, it would be the transparency regarding limits. Much like the free plan, on the $20 plan, it‚Äôs sometimes hard to tell exactly when you‚Äôll hit a rate limit. It‚Äôs a bit of a &amp;quot;black box&amp;quot; experience, but in my daily use, the performance has been worth the uncertainty.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Suggestion:&lt;/strong&gt;&lt;br /&gt; If you are doing research or building tools and you need the power of models that your local VRAM can‚Äôt handle, stop hesitating. It‚Äôs a solid investment that democratizes access to high-end AI.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I‚Äôm curious to hear from others:&lt;/strong&gt;&lt;br /&gt; Is anyone else here using the $20/month Ollama Cloud plan? What has your experience been like so far? Any &amp;quot;pro-tips&amp;quot; or secrets you‚Äôve discovered to get the most out of it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexHardy08"&gt; /u/AlexHardy08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qprl7a/opinion_why_i_believe_the_20month_ollama_cloud_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qprl7a/opinion_why_i_believe_the_20month_ollama_cloud_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qprl7a/opinion_why_i_believe_the_20month_ollama_cloud_is/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T23:23:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqan74</id>
    <title>Effects of quantized KV cache on an already quantized model.</title>
    <updated>2026-01-29T14:52:44+00:00</updated>
    <author>
      <name>/u/Pyrore</name>
      <uri>https://old.reddit.com/user/Pyrore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run a QwQ 32B model variant in LM studio, and after the update today, I can finally use KV quantization without absolutely tanking my performance. My question is, &lt;strong&gt;if I'm running QwQ at four bit, will dropping my K/V cache to 4 bits notably impact the accuracy?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm happy at 4 bits for QwQ, I only have 24BG VRAM, and that fits nicely at around 19GB (I understand it's better to have more parameters than higher quants). But I can only fit about 10k of context into the remaining 4GB of VRAM (need to leave about 1GB spare for system overheads), no where near enough for the conversational/role-play I use local LLMs for. So I've bee running the KV cache in main memory with the CPU, easily runs up to 64k, but I never really go past 32k, because by then I'm around 1.5 tokens a second (compared to 15/s when there is negligible context).&lt;/p&gt; &lt;p&gt;But with KV cache at 4 bit I can hit 40k context without overloading my VRAM, and my tests so far indicate three times the token rate for a given context size compared to main memory/CPU. But accuracy is more subjective, I'd love to hear your opinions or links to any studies. My model is already running well at 4 bits, and it seems sensible to run the KV at the same accuracy as the model, anything more seems wasteful, unless there's something I'm not understanding...&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pyrore"&gt; /u/Pyrore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqan74/effects_of_quantized_kv_cache_on_an_already/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqan74/effects_of_quantized_kv_cache_on_an_already/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qqan74/effects_of_quantized_kv_cache_on_an_already/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T14:52:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqsxoz</id>
    <title>Which API or cloud-based models work so well in Clawdbot?</title>
    <updated>2026-01-30T02:28:20+00:00</updated>
    <author>
      <name>/u/Friendly-Fig-6015</name>
      <uri>https://old.reddit.com/user/Friendly-Fig-6015</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;But local models lose all context, don't handle navigation, etc.?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Friendly-Fig-6015"&gt; /u/Friendly-Fig-6015 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqsxoz/which_api_or_cloudbased_models_work_so_well_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqsxoz/which_api_or_cloudbased_models_work_so_well_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qqsxoz/which_api_or_cloudbased_models_work_so_well_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T02:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr986b</id>
    <title>Thought local LLM = uncensored. Installed Ollama + Mistral‚Ä¶ yeah not really</title>
    <updated>2026-01-30T15:53:49+00:00</updated>
    <author>
      <name>/u/urfavgemini_x3</name>
      <uri>https://old.reddit.com/user/urfavgemini_x3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay so I installed Ollama on my laptop recently just to try the whole local AI thing.&lt;/p&gt; &lt;p&gt;Laptop specs btw:&lt;br /&gt; 16gb RAM&lt;br /&gt; no dedicated GPU (intel iris xe)&lt;br /&gt; ubuntu 24.04&lt;/p&gt; &lt;p&gt;Downloaded Mistral (around 4gb model). Setup was honestly smooth, performance is fine on CPU, no complaints there. But the thing is‚Ä¶ I thought running it locally means it‚Äôs gonna be fully uncensored / no filters.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;That‚Äôs not what happened.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It still refuses certain stuff or gives those soft ‚Äúcan‚Äôt help with that‚Äù answers. It‚Äôs definitely less strict than chatgpt but it‚Äôs not the wild west people hype it up to be. I‚Äôm guessing the restrictions are baked into the model itself and Ollama is just running it locally, so yeah lesson learned.&lt;/p&gt; &lt;p&gt;Now I‚Äôm kinda stuck here ‚Äî for a &lt;strong&gt;16gb RAM, CPU only setup&lt;/strong&gt;, what models are actually better if you want more blunt / raw / technical answers without constant moral lectures? I‚Äôm not trying to do illegal nonsense, I just want straight answers without it acting like my school principal.&lt;/p&gt; &lt;p&gt;someone help me please!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/urfavgemini_x3"&gt; /u/urfavgemini_x3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr986b/thought_local_llm_uncensored_installed_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr986b/thought_local_llm_uncensored_installed_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qr986b/thought_local_llm_uncensored_installed_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T15:53:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr1xqr</id>
    <title>how do I use ollama with vulkan?</title>
    <updated>2026-01-30T10:32:59+00:00</updated>
    <author>
      <name>/u/cranberrie_sauce</name>
      <uri>https://old.reddit.com/user/cranberrie_sauce</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;how do I use ollama with vulkan?&lt;/p&gt; &lt;p&gt;ai max 395 - I think vulkan is still faster than rocm.&lt;/p&gt; &lt;p&gt;edit: I think I got it: &lt;/p&gt; &lt;p&gt;```&lt;br /&gt; cat /etc/systemd/system/ollama.service.d/override.conf&lt;br /&gt; [Service]&lt;br /&gt; Environment=&amp;quot;OLLAMA_MODELS=/mnt/drive/ollama&amp;quot;&lt;br /&gt; Environment=&amp;quot;GGML_VULKAN=1&lt;br /&gt; ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cranberrie_sauce"&gt; /u/cranberrie_sauce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr1xqr/how_do_i_use_ollama_with_vulkan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr1xqr/how_do_i_use_ollama_with_vulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qr1xqr/how_do_i_use_ollama_with_vulkan/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T10:32:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr24r9</id>
    <title>How to respond with a tool call</title>
    <updated>2026-01-30T10:44:24+00:00</updated>
    <author>
      <name>/u/Super_Nova02</name>
      <uri>https://old.reddit.com/user/Super_Nova02</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm creating a little chatbot, which main function is to call some tools and create an answer with the info the tool give it. &lt;/p&gt; &lt;p&gt;I'm using Ollama in a javascript environment (so ollama-js). The tool call use Functiongemma:270m as model and works fine (it is a &lt;a href="http://ollama.chat"&gt;ollama.chat&lt;/a&gt; request).&lt;br /&gt; Then I try to rewrite the info so that the aswer is more &amp;quot;human-like&amp;quot;: for example, if the tool returns an array of objects, it would be perfect if the chatbot answers with list with the info well laid out.&lt;br /&gt; This is the code of this second request:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;const toolResult = await executeTool( toolCall.name, toolCall.arguments || {}, { BACKEND_URL }, ); const bullets = formatSensors(toolResult); const chunks = chunkArray(bullets, 5); let summaries = []; for (let i = 0; i &amp;lt; chunks.length; i++) { const chunk = chunks[i]; const result = await ollama.generate({ model: &amp;quot;gemma3:270m&amp;quot;, options: { temperature: 0 }, prompt: ` You are an assistant. Respond to the user as follows: - If the user requested the sensors, start with a natural intro like &amp;quot;Sure, here's the list of all the sensors:&amp;quot; and then immediately list all the items exactly as provided below. - If the user added sensors, start with &amp;quot;I've added the sensors with the information you provided me, here's how it looks:&amp;quot; followed by the list exactly as provided. - Do NOT modify, remove, or reorder any items in the list. - Include the list exactly as it appears below in your output. SENSOR LIST START ${chunk.join(&amp;quot;\n&amp;quot;)} SENSOR LIST END `, }); summaries.push(result.response); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The problem is that the llm just prints &amp;quot;Okay, I understand. I will respond to the user as requested, keeping the list exactly as it appears in the user's message.&amp;quot; or similar messages, without really printing the tool info I've given it.&lt;br /&gt; Please keep in mind that I can't use bigger models: my pc would not be able to run them and also for the specific purpose of my chatbot I don't think I need bigger models.&lt;/p&gt; &lt;p&gt;In the end I would like to have something like &amp;quot;Here's the list of elements you asked for&amp;quot; or &amp;quot;sure, i've added the element with the info you provided, here's how it looks&amp;quot;, and so on for my various functionalities. &lt;/p&gt; &lt;p&gt;I don't really understand what I'm doing wrong. Is it the model? Is it my code?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Super_Nova02"&gt; /u/Super_Nova02 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr24r9/how_to_respond_with_a_tool_call/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr24r9/how_to_respond_with_a_tool_call/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qr24r9/how_to_respond_with_a_tool_call/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T10:44:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr3zvt</id>
    <title>Does Ollama respect parameters and system prompts for cloud models?</title>
    <updated>2026-01-30T12:23:10+00:00</updated>
    <author>
      <name>/u/soteko</name>
      <uri>https://old.reddit.com/user/soteko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using OpenWebUI and for local models I have workspace with system prompt and parameters for different use cases. &lt;/p&gt; &lt;p&gt;How that works with cloud models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/soteko"&gt; /u/soteko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr3zvt/does_ollama_respect_parameters_and_system_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr3zvt/does_ollama_respect_parameters_and_system_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qr3zvt/does_ollama_respect_parameters_and_system_prompts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T12:23:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr40ns</id>
    <title>Recommendation for Best Offline Ollama Models for Tailored CV Generation</title>
    <updated>2026-01-30T12:24:11+00:00</updated>
    <author>
      <name>/u/MavFir</name>
      <uri>https://old.reddit.com/user/MavFir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I am currently developing a script that uses offline Ollama models locally on my laptop to generate a tailored CV based on the following inputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Job description&lt;/li&gt; &lt;li&gt;Required skills&lt;/li&gt; &lt;li&gt;Original CV&lt;/li&gt; &lt;li&gt;Custom prompt&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I tested LLaMA 2, but the model mostly copies the original CV text instead of effectively tailoring it to the job requirements.&lt;/p&gt; &lt;p&gt;Due to memory constraints, I cannot download or experiment with many models. Therefore, I would really appreciate recommendations for one or two offline models that perform well in tasks like CV rewriting, summarization, and content adaptation.&lt;/p&gt; &lt;p&gt;Thank you in advance for your suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MavFir"&gt; /u/MavFir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr40ns/recommendation_for_best_offline_ollama_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr40ns/recommendation_for_best_offline_ollama_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qr40ns/recommendation_for_best_offline_ollama_models_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T12:24:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr16fg</id>
    <title>ask about Mac mini and Ollama</title>
    <updated>2026-01-30T09:48:59+00:00</updated>
    <author>
      <name>/u/Slow_Consequence5401</name>
      <uri>https://old.reddit.com/user/Slow_Consequence5401</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to use ollama locally on a Mac mini.&lt;/p&gt; &lt;p&gt;What is the performance and speed like on a Mac mini m4 with 24GB RAM?&lt;/p&gt; &lt;p&gt;Which LLM is currently the best?&lt;/p&gt; &lt;p&gt;I want it to generate text.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slow_Consequence5401"&gt; /u/Slow_Consequence5401 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr16fg/ask_about_mac_mini_and_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr16fg/ask_about_mac_mini_and_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qr16fg/ask_about_mac_mini_and_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T09:48:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqiyot</id>
    <title>I wrote a biological memory layer for Ollama in Rust to replace stateless RAG</title>
    <updated>2026-01-29T19:48:35+00:00</updated>
    <author>
      <name>/u/ChikenNugetBBQSauce</name>
      <uri>https://old.reddit.com/user/ChikenNugetBBQSauce</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using Ollama heavily for local development but the lack of state persistence is a major bottleneck. The moment you terminate the session, the context is lost. Standard RAG implementations are often inefficient wrappers that flood the context window with low-relevance tokens.&lt;/p&gt; &lt;p&gt;I decided to solve this by engineering a dedicated memory server called Vestige.&lt;/p&gt; &lt;p&gt;It acts as a biological hippocampus for local agents. Instead of a flat vector search, I implemented the FSRS 6 algorithm directly in Rust to handle memory decay and reinforcement.&lt;/p&gt; &lt;p&gt;Here is the architecture.&lt;/p&gt; &lt;p&gt;The system uses a directed graph where nodes represent memories and edges represent synaptic weights. When Llama 3 queries the system, it calculates a retrievability score based on the spacing effect. Information you access frequently is reinforced, while irrelevant data naturally decays over time. This mimics biological neuroplasticity and keeps the context window efficient.&lt;/p&gt; &lt;p&gt;I initially prototyped this in Python but the serialization overhead during the graph traversal was unacceptable for real time chat. I rewrote the core logic in Rust using tokio and serde. The retrieval latency is now consistently under 8ms.&lt;/p&gt; &lt;p&gt;I designed this to run as a Model Context Protocol server. It sits alongside Ollama and handles the long-term state management so your agent actually remembers project details across sessions.&lt;/p&gt; &lt;p&gt;If you are tired of your local models resetting every time you close the terminal, you can check the code here.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/samvallad33/vestige"&gt;https://github.com/samvallad33/vestige&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChikenNugetBBQSauce"&gt; /u/ChikenNugetBBQSauce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqiyot/i_wrote_a_biological_memory_layer_for_ollama_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqiyot/i_wrote_a_biological_memory_layer_for_ollama_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qqiyot/i_wrote_a_biological_memory_layer_for_ollama_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T19:48:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrit9k</id>
    <title>Ollama AMD apprechiation post</title>
    <updated>2026-01-30T21:35:20+00:00</updated>
    <author>
      <name>/u/SnowTim07</name>
      <uri>https://old.reddit.com/user/SnowTim07</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnowTim07"&gt; /u/SnowTim07 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1qriswe/ollama_amd_apprechiation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrit9k/ollama_amd_apprechiation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qrit9k/ollama_amd_apprechiation_post/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T21:35:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrh5a9</id>
    <title>How do you choose a model and estimate hardware specs for a LangChain app (Ollama) ?</title>
    <updated>2026-01-30T20:33:04+00:00</updated>
    <author>
      <name>/u/XxDarkSasuke69xX</name>
      <uri>https://old.reddit.com/user/XxDarkSasuke69xX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. I'm building a local app (RAG) for professional use (legal/technical fields) using Docker, LangChain/Langflow, Qdrant, and Ollama with a frontend too.&lt;/p&gt; &lt;p&gt;The goal is a strict, reliable agent that answers based only on the provided files, cites sources, and states its confidence level. Since this is for professionals, accuracy is more important than speed, but I don't want it to take forever either. Also it would be nice if it could also look for an answer online if no relevant info was found in the files.&lt;/p&gt; &lt;p&gt;I'm struggling to figure out how to find the right model/hardware balance for this and would love some input.&lt;/p&gt; &lt;p&gt;How to choose a model for my need and that is available on Ollama ? I need something that follows system prompts well (like &amp;quot;don't guess if you don't know&amp;quot;) and handles a lot of context well. How to decide on number of parameters for example ? How to find the sweetspot without testing each and every model ?&lt;/p&gt; &lt;p&gt;How do you calculate the requirements for this ? If I'm loading a decent sized vector store and need a decently big context window, how much VRAM/RAM should I be targeting to run the LLM + embedding model + Qdrant smoothly ?&lt;/p&gt; &lt;p&gt;Like are there any benchmarks to estimate this ? I looked online but it's still pretty vague to me. Thx in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XxDarkSasuke69xX"&gt; /u/XxDarkSasuke69xX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrh5a9/how_do_you_choose_a_model_and_estimate_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrh5a9/how_do_you_choose_a_model_and_estimate_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qrh5a9/how_do_you_choose_a_model_and_estimate_hardware/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T20:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrhttt</id>
    <title>Porting prompts from OpenAI/Claude to local Ollama models - best practices?</title>
    <updated>2026-01-30T20:58:38+00:00</updated>
    <author>
      <name>/u/gogeta1202</name>
      <uri>https://old.reddit.com/user/gogeta1202</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community üëã&lt;/p&gt; &lt;p&gt;Love the local-first approach. But I'm hitting a wall with prompt portability.&lt;/p&gt; &lt;p&gt;My prompts were developed on GPT-4/Claude and don't translate cleanly to local models.&lt;/p&gt; &lt;p&gt;Issues I'm seeing:&lt;br /&gt; ‚Ä¢ Instruction following is different&lt;br /&gt; ‚Ä¢ System prompt handling varies by model&lt;br /&gt; ‚Ä¢ Function calling support is inconsistent&lt;br /&gt; ‚Ä¢ Context window differences change behavior&lt;/p&gt; &lt;p&gt;How do you handle this?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Do you rewrite prompts from scratch for Ollama?&lt;/li&gt; &lt;li&gt;Is there a &amp;quot;universal&amp;quot; prompt style that works across models?&lt;/li&gt; &lt;li&gt;Any tools that help with conversion?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What I've built:&lt;/p&gt; &lt;p&gt;A prompt conversion tool focused on OpenAI ‚Üî Anthropic right now. Quality validation using embeddings, checkpoint/rollback support.&lt;/p&gt; &lt;p&gt;Honest note: Local model support (Ollama/vLLM) isn't fully built yet. I'm validating if cloud ‚Üí local conversion is a real pain point worth solving.&lt;/p&gt; &lt;p&gt;Would love to hear:&lt;br /&gt; ‚Ä¢ What local models do you primarily use?&lt;br /&gt; ‚Ä¢ Biggest friction moving from cloud ‚Üí local?&lt;br /&gt; ‚Ä¢ Would you test a converter if local models were supported?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gogeta1202"&gt; /u/gogeta1202 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrhttt/porting_prompts_from_openaiclaude_to_local_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrhttt/porting_prompts_from_openaiclaude_to_local_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qrhttt/porting_prompts_from_openaiclaude_to_local_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T20:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr4blw</id>
    <title>My first Local LLM</title>
    <updated>2026-01-30T12:38:17+00:00</updated>
    <author>
      <name>/u/swipegod43</name>
      <uri>https://old.reddit.com/user/swipegod43</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qr4blw/my_first_local_llm/"&gt; &lt;img alt="My first Local LLM" src="https://preview.redd.it/bxspnf0rehgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3e8245ed998f16b0a863ce0e65130cdb230e130" title="My first Local LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek-R1 q4_k_m 14b parameters on 12gb Vram it seems pretty fast üò≥ never woulda thought my old gaming PC could run an LLM this is pretty fascinating to me üòÇ i literally just wanted to try it and got it up and running in a few hours im never using copilot again üíØ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swipegod43"&gt; /u/swipegod43 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bxspnf0rehgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qr4blw/my_first_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qr4blw/my_first_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T12:38:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qro11g</id>
    <title>AMD AI bundle</title>
    <updated>2026-01-31T01:03:43+00:00</updated>
    <author>
      <name>/u/Daine06</name>
      <uri>https://old.reddit.com/user/Daine06</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! I'm new to Local LLM so please bear with me. &lt;/p&gt; &lt;p&gt;I purchased a new card last week (9070 xt, if it matters). While I was fiddling with AMD software, I saw the AI bundle it offers to install. Intrigued, I tried installing Ollama. &lt;/p&gt; &lt;p&gt;Tried using their UI, prompted, entered, and I noticed that it was not using my GPU. Instead, it is using my CPU. Is it possible to offload from CPU to GPU? Is there any tutorial I can follow so I can set up Ollama properly?&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;What I kinda want to experiment on is Claude code and n8n.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daine06"&gt; /u/Daine06 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qro11g/amd_ai_bundle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qro11g/amd_ai_bundle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qro11g/amd_ai_bundle/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-31T01:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrtl3a</id>
    <title>Best local llm coding &amp; reasoning (Mac M1) ?</title>
    <updated>2026-01-31T05:21:28+00:00</updated>
    <author>
      <name>/u/Sherlock_holmes0007</name>
      <uri>https://old.reddit.com/user/Sherlock_holmes0007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says which is the best llm for coding and reasoning for Mac M1, doesn't have to be fully optimised a little slow is also okay but would prefer suggestions for both.&lt;/p&gt; &lt;p&gt;I'm trying to build a whole pipeline for my Mac that controls every task and even captures what's on the screen and debugs it live.&lt;/p&gt; &lt;p&gt;let's say I gave it a task of coding something and it creates code now ask it to debug and it's able to do that by capturing the content on screen.&lt;/p&gt; &lt;p&gt;Was also thinking about doing a hybrid setup where I have local model for normal tasks and Claude API for high reasoning and coding tasks.&lt;/p&gt; &lt;p&gt;Other suggestions and whole pipeline setup ideas would be very welcomed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sherlock_holmes0007"&gt; /u/Sherlock_holmes0007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AgentsOfAI/comments/1qrtg8y/best_local_llm_coding_reasoning_mac_m1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrtl3a/best_local_llm_coding_reasoning_mac_m1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qrtl3a/best_local_llm_coding_reasoning_mac_m1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-31T05:21:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrkbsr</id>
    <title>Run Ollama on your Android!</title>
    <updated>2026-01-30T22:32:38+00:00</updated>
    <author>
      <name>/u/DutchOfBurdock</name>
      <uri>https://old.reddit.com/user/DutchOfBurdock</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Want to put this out here. I have a Samsung S20 and a Pixel 8 Pro. Both of these devices pack 12GB of RAM, one an octacore arrangement and the other a nonacore. Now, this is pure CPU and even Vulkan (despite hardware support), doesn't work.&lt;/p&gt; &lt;p&gt;First, get yourself Termux from F-Droid or GitHub. &lt;strong&gt;Don't use the Play Store version.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Upon launching Termux, update the package manager and install some things needed..&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pkg up pkg i build-essential git cmake golang git clone https://github.com/ollama/ollama.git cd ollama go generate ./... go build . &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If all went well, you'll end up with an &lt;code&gt;ollama&lt;/code&gt; executable in the folder.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./ollama serve &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Open a new terminal in the gitted ollama folder &lt;/p&gt; &lt;pre&gt;&lt;code&gt;./ollama pull smollm2 ./ollama run smollm2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This model should be small enough for even 4GB devices and is pretty fast.&lt;/p&gt; &lt;p&gt;Enjoy and start exploring!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DutchOfBurdock"&gt; /u/DutchOfBurdock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrkbsr/run_ollama_on_your_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrkbsr/run_ollama_on_your_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qrkbsr/run_ollama_on_your_android/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-30T22:32:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrt70f</id>
    <title>Output quality of Ollama integrated Coding Agent with the local LLMs</title>
    <updated>2026-01-31T05:01:52+00:00</updated>
    <author>
      <name>/u/FrontRegular6113</name>
      <uri>https://old.reddit.com/user/FrontRegular6113</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve spent the last few days running reasoning and coding tasks using Ollama‚Äôs integrated coding agents. I specifically tested the gpt-oss:20b model(with two A6000 GPUs), but even compared to agents like Claude Code, OpenCode, Cline or Codex, the output quality was not that great for my specific project and the speed was quite slow (such as when the results had errors, it tried to repeat until it fixed). What level of model would I need to run locally with Ollama to achieve results comparable to Claude Code Opus or Sonnet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrontRegular6113"&gt; /u/FrontRegular6113 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrt70f/output_quality_of_ollama_integrated_coding_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qrt70f/output_quality_of_ollama_integrated_coding_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qrt70f/output_quality_of_ollama_integrated_coding_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-31T05:01:52+00:00</published>
  </entry>
</feed>
