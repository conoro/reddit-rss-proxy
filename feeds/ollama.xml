<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-25T17:49:29+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qiug46</id>
    <title>Fine-tuned Qwen3 0.6B for Text2SQL using a claude skill. The result tiny model matches a Deepseek 3.1 and runs locally on CPU.</title>
    <updated>2026-01-21T10:30:24+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qiug46/finetuned_qwen3_06b_for_text2sql_using_a_claude/"&gt; &lt;img alt="Fine-tuned Qwen3 0.6B for Text2SQL using a claude skill. The result tiny model matches a Deepseek 3.1 and runs locally on CPU." src="https://preview.redd.it/fy13421qjoeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5f9926ff9e7f48668957408f4d674a40d2079b2" title="Fine-tuned Qwen3 0.6B for Text2SQL using a claude skill. The result tiny model matches a Deepseek 3.1 and runs locally on CPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing a workflow for training custom models and deploying them to Ollama.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Base small models aren't great at specialized tasks. I needed Text2SQL and Qwen3 0.6B out of the box gave me things like:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sql -- Question: &amp;quot;Which artists have total album sales over 1 million?&amp;quot; SELECT artists.name FROM artists WHERE artists.genre IS NULL OR artists.country IS NULL; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Completely ignores the question. Fine-tuning is the obvious answer, but usually means setting up training infrastructure, formatting datasets, debugging CUDA errors...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The workflow I used:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;distil-cli with a Claude skill that handles the training setup, to get started I installed &lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;Setup&lt;/h1&gt; &lt;p&gt;curl -fsSL &lt;a href="https://cli-assets.distillabs.ai/install.sh"&gt;https://cli-assets.distillabs.ai/install.sh&lt;/a&gt; | sh distil login&lt;/p&gt; &lt;h1&gt;In Claude Code ‚Äî add the skill&lt;/h1&gt; &lt;p&gt;/plugin marketplace add &lt;a href="https://github.com/distil-labs/distil-cli-skill"&gt;https://github.com/distil-labs/distil-cli-skill&lt;/a&gt; /plugin install distil-cli@distil-cli-skill ```&lt;/p&gt; &lt;p&gt;And then, Claude guides me through the training workflow:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash 1. Create a model (`distil model create`) 2. Pick a task type (QA, classification, tool calling, or RAG) 3. Prepare data files (job description, config, train/test sets) 4. Upload data 5. Run teacher evaluation 6. Train the model 7. Download and deploy &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What training produces:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt; downloaded-model/ ‚îú‚îÄ‚îÄ model.gguf (2.2 GB) ‚Äî quantized, Ollama-ready ‚îú‚îÄ‚îÄ Modelfile (system prompt baked in) ‚îú‚îÄ‚îÄ model_client.py (Python wrapper) ‚îú‚îÄ‚îÄ model/ (full HF format) ‚îî‚îÄ‚îÄ model-adapter/ (LoRA weights if you want to merge yourself) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deploying to Ollama:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash ollama create my-text2sql -f Modelfile ollama run my-text2sql &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Custom fine-tuned model, running locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;LLM-as-a-Judge&lt;/th&gt; &lt;th&gt;ROUGE&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Base Qwen3 0.6B&lt;/td&gt; &lt;td&gt;36%&lt;/td&gt; &lt;td&gt;69.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek-V3 (teacher)&lt;/td&gt; &lt;td&gt;80%&lt;/td&gt; &lt;td&gt;88.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Fine-tuned 0.6B&lt;/td&gt; &lt;td&gt;74%&lt;/td&gt; &lt;td&gt;88.5%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Started at 36%, ended at 74% ‚Äî nearly matching the teacher at a fraction of the size.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Before/after:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Question: &amp;quot;How many applicants applied for each position?&amp;quot;&lt;/p&gt; &lt;p&gt;Base: &lt;code&gt;sql SELECT COUNT(DISTINCT position) AS num_applicants FROM applicants; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Fine-tuned: &lt;code&gt;sql SELECT position, COUNT(*) AS applicant_count FROM applicants GROUP BY position; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo app:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Built a quick script that loads CSVs into SQLite and queries via the model:&lt;/p&gt; &lt;p&gt;```bash python app.py --csv employees.csv \ --question &amp;quot;What is the average salary per department?&amp;quot; --show-sql&lt;/p&gt; &lt;h1&gt;Generated SQL: SELECT department, AVG(salary) FROM employees GROUP BY department;&lt;/h1&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;All local.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fy13421qjoeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qiug46/finetuned_qwen3_06b_for_text2sql_using_a_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qiug46/finetuned_qwen3_06b_for_text2sql_using_a_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-21T10:30:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjxwu7</id>
    <title>Trying to analyze personal transactions and use ollama/qwen2.5:7b to provide a report without success</title>
    <updated>2026-01-22T15:44:07+00:00</updated>
    <author>
      <name>/u/hpgm</name>
      <uri>https://old.reddit.com/user/hpgm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This might not be the right place, however I'm hoping that someone might have done this themselves and that I can build on a working model. I'm trying to analyze my last year's spending habits, and have the LLM analyze my back account and credit card transactions. The transactions don't have a category, so that is the LLM part, and then to create an updated csv/xlsx file that I can use to pivot.&lt;/p&gt; &lt;p&gt;The transactions look like the below, date, a descriptor and location, debit, credit, and card number. I tried a number of prompts and haven't been successful, the LLM latches onto one descriptior and then calls everything else that. Because it's my personal finance, I want to keep everything local. I can use a different model, but I only have a 3060TI w/ 16GB VRAM to power it.&lt;/p&gt; &lt;p&gt;Anyone done anything like this?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;2025-12-31,&amp;quot;NATIONAL PARKS, STATE&amp;quot;,,15.75,4********6 2025-12-31,&amp;quot;WENDYS CITY, STATE&amp;quot;,82.11,,4********6 2025-12-30,&amp;quot;WALMART CITY, STATE&amp;quot;,31.60,,4********6 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hpgm"&gt; /u/hpgm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjxwu7/trying_to_analyze_personal_transactions_and_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjxwu7/trying_to_analyze_personal_transactions_and_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qjxwu7/trying_to_analyze_personal_transactions_and_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-22T15:44:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjvhs6</id>
    <title>I can't get qwen2.5-coder:7b working with claude code</title>
    <updated>2026-01-22T14:09:18+00:00</updated>
    <author>
      <name>/u/acidiceyes</name>
      <uri>https://old.reddit.com/user/acidiceyes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qjvhs6/i_cant_get_qwen25coder7b_working_with_claude_code/"&gt; &lt;img alt="I can't get qwen2.5-coder:7b working with claude code" src="https://a.thumbs.redditmedia.com/Rwmz0jhbrtAqKKtq8YVfIQ6QKQIR8WPrTvoJu5FTzU8.jpg" title="I can't get qwen2.5-coder:7b working with claude code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I just read that we can use ollama with claude code now, but I have been trying to get qwen2.5-coder:7b working with claude code, but tool calling just doesn't work.&lt;br /&gt; What am i doing wrong?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mc5u9eoorweg1.png?width=1376&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=403d76d563760d11c890855a3b03e6a62bbc27fd"&gt;https://preview.redd.it/mc5u9eoorweg1.png?width=1376&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=403d76d563760d11c890855a3b03e6a62bbc27fd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/acidiceyes"&gt; /u/acidiceyes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjvhs6/i_cant_get_qwen25coder7b_working_with_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjvhs6/i_cant_get_qwen25coder7b_working_with_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qjvhs6/i_cant_get_qwen25coder7b_working_with_claude_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-22T14:09:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk16na</id>
    <title>Would a p100 be useful?</title>
    <updated>2026-01-22T17:42:35+00:00</updated>
    <author>
      <name>/u/dnielso5</name>
      <uri>https://old.reddit.com/user/dnielso5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, my current setup is a 3060 12g + 1060 6G. I was thinking of getting a p100 (~$90) to replace the 1060. my main focus is running qwen-coder2.5 for some basic coding projects in open webui. &lt;/p&gt; &lt;p&gt;I have some decent success running qwen2.5-coder:14b-instruct-q8_0 but wondering how much it might help with more vram but older card.&lt;/p&gt; &lt;p&gt;and because this is a side project i dont need suggestions to buy a 3090, im looking for around ~$100&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnielso5"&gt; /u/dnielso5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qk16na/would_a_p100_be_useful/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qk16na/would_a_p100_be_useful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qk16na/would_a_p100_be_useful/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-22T17:42:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjqcqv</id>
    <title>Built an open-source, self-hosted AI agent automation platform ‚Äî feedback welcome</title>
    <updated>2026-01-22T09:44:30+00:00</updated>
    <author>
      <name>/u/Feathered-Beast</name>
      <uri>https://old.reddit.com/user/Feathered-Beast</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks üëã&lt;/p&gt; &lt;p&gt;I‚Äôve been building an open-source, self-hosted AI agent automation platform that runs locally and keeps all data under your control. It‚Äôs focused on agent workflows, scheduling, execution logs, and document chat (RAG) without relying on hosted SaaS tools.&lt;/p&gt; &lt;p&gt;I recently put together a small website with docs and a project overview.&lt;/p&gt; &lt;p&gt;Links to the website and GitHub are in the comments.&lt;/p&gt; &lt;p&gt;Would really appreciate feedback from people building or experimenting with open-source AI systems üôå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feathered-Beast"&gt; /u/Feathered-Beast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjqcqv/built_an_opensource_selfhosted_ai_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjqcqv/built_an_opensource_selfhosted_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qjqcqv/built_an_opensource_selfhosted_ai_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-22T09:44:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjtnqm</id>
    <title>How to implement a RAG (Retrieval Augmented Generation) on your laptop</title>
    <updated>2026-01-22T12:48:36+00:00</updated>
    <author>
      <name>/u/Unique_Winner_5927</name>
      <uri>https://old.reddit.com/user/Unique_Winner_5927</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qjtnqm/how_to_implement_a_rag_retrieval_augmented/"&gt; &lt;img alt="How to implement a RAG (Retrieval Augmented Generation) on your laptop" src="https://b.thumbs.redditmedia.com/Ak9Ybl34eS3k6YJHJqbnpAOgTmOIwBEmPTZCS8SWXsw.jpg" title="How to implement a RAG (Retrieval Augmented Generation) on your laptop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This guide explains how to implement a RAG (Retrieval Augmented Generation) on your laptop.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ftsddeqtcweg1.png?width=2184&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=640e3013e9113c3c7780a88b39d6992cd34b8d6f"&gt;https://preview.redd.it/ftsddeqtcweg1.png?width=2184&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=640e3013e9113c3c7780a88b39d6992cd34b8d6f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With n8n, Ollama and Qdrant (with Docker).&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ThomasPlantain/n8n"&gt;https://github.com/ThomasPlantain/n8n&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I put a lot of screenshots to explain how to configure each component.&lt;/p&gt; &lt;p&gt;#Ollama #n8n #Qdrant #dataSovereignty #embeddedAI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unique_Winner_5927"&gt; /u/Unique_Winner_5927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjtnqm/how_to_implement_a_rag_retrieval_augmented/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qjtnqm/how_to_implement_a_rag_retrieval_augmented/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qjtnqm/how_to_implement_a_rag_retrieval_augmented/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-22T12:48:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkcg6a</id>
    <title>Boom, the greatest repo yet from Lucas Valbuena ( x1xhlol)</title>
    <updated>2026-01-23T00:57:26+00:00</updated>
    <author>
      <name>/u/OriginalZebraPoo</name>
      <uri>https://old.reddit.com/user/OriginalZebraPoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qkcg6a/boom_the_greatest_repo_yet_from_lucas_valbuena/"&gt; &lt;img alt="Boom, the greatest repo yet from Lucas Valbuena ( x1xhlol)" src="https://preview.redd.it/dspr44juxzeg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42f0ce90b21eba01096601e2cea14bf419aa729d" title="Boom, the greatest repo yet from Lucas Valbuena ( x1xhlol)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OriginalZebraPoo"&gt; /u/OriginalZebraPoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dspr44juxzeg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qkcg6a/boom_the_greatest_repo_yet_from_lucas_valbuena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qkcg6a/boom_the_greatest_repo_yet_from_lucas_valbuena/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-23T00:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkl6rr</id>
    <title>GitHub - FlorinAndrei/pipe-llama: Put an LLM in your shell scripts and command-line pipelines. Dead simple.</title>
    <updated>2026-01-23T08:14:30+00:00</updated>
    <author>
      <name>/u/florinandrei</name>
      <uri>https://old.reddit.com/user/florinandrei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qkl6rr/github_florinandreipipellama_put_an_llm_in_your/"&gt; &lt;img alt="GitHub - FlorinAndrei/pipe-llama: Put an LLM in your shell scripts and command-line pipelines. Dead simple." src="https://external-preview.redd.it/iUptUgrV_6EbjMiLnEF2Mw13nDhOzuFsw7UJBdH5Zis.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c66bdbfc756f3774c7377ca7b749c33b00edcf9f" title="GitHub - FlorinAndrei/pipe-llama: Put an LLM in your shell scripts and command-line pipelines. Dead simple." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/florinandrei"&gt; /u/florinandrei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/FlorinAndrei/pipe-llama"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qkl6rr/github_florinandreipipellama_put_an_llm_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qkl6rr/github_florinandreipipellama_put_an_llm_in_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-23T08:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlkc1h</id>
    <title>Renting out the cheapest GPUs!!!</title>
    <updated>2026-01-24T11:09:15+00:00</updated>
    <author>
      <name>/u/Comfortable-Wall-465</name>
      <uri>https://old.reddit.com/user/Comfortable-Wall-465</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Renting out the cheapest GPUs, e.g &lt;strong&gt;4090 for just $0.15/hr&lt;/strong&gt;, cheaper if you go for long-term! Probably the lowest price you‚Äôll find anywhere.&lt;/p&gt; &lt;p&gt;Other GPUs also available.&lt;/p&gt; &lt;p&gt;Whatever your project, you can run it on a top-tier GPU without breaking the bank.&lt;/p&gt; &lt;p&gt;Interested? Drop a comment or DM me!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Wall-465"&gt; /u/Comfortable-Wall-465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlkc1h/renting_out_the_cheapest_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlkc1h/renting_out_the_cheapest_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qlkc1h/renting_out_the_cheapest_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-24T11:09:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql4iag</id>
    <title>RTX Pro 6000 $7999.99</title>
    <updated>2026-01-23T22:05:48+00:00</updated>
    <author>
      <name>/u/I_like_fragrances</name>
      <uri>https://old.reddit.com/user/I_like_fragrances</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_like_fragrances"&gt; /u/I_like_fragrances &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1ql4hwj/rtx_pro_6000_799999/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ql4iag/rtx_pro_6000_799999/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ql4iag/rtx_pro_6000_799999/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-23T22:05:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qldoyv</id>
    <title>Will an M5 MacBook Pro with 16GB RAM be able to run Claude Code with @ollama, Clawdbot etc.?</title>
    <updated>2026-01-24T04:50:35+00:00</updated>
    <author>
      <name>/u/saintforlife1</name>
      <uri>https://old.reddit.com/user/saintforlife1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/saintforlife1"&gt; /u/saintforlife1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/macbookpro/comments/1qldojy/will_an_m5_macbook_pro_with_16gb_ram_be_able_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qldoyv/will_an_m5_macbook_pro_with_16gb_ram_be_able_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qldoyv/will_an_m5_macbook_pro_with_16gb_ram_be_able_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-24T04:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkokhv</id>
    <title>Ollama Image Generator</title>
    <updated>2026-01-23T11:40:26+00:00</updated>
    <author>
      <name>/u/nickinnov</name>
      <uri>https://old.reddit.com/user/nickinnov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qkokhv/ollama_image_generator/"&gt; &lt;img alt="Ollama Image Generator" src="https://b.thumbs.redditmedia.com/HKgzzb-cAawGjz1uOOeBVBeQasjGPZ4WUE-D0xydFVY.jpg" title="Ollama Image Generator" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey fellow Ollama fans, I'm delighted that image generation is available so I have written a web app you can run on your own computer (alongside Ollama) to make it easier to generate, save and delete images.&lt;/p&gt; &lt;p&gt;OK it ain't no ComfyUI but makes things tidier and, unlike using the terminal Ollama CLI, images don't clutter up your home folder!&lt;br /&gt; Repo at &lt;a href="https://github.com/nicklansley/OllamaImageGenerator"&gt;https://github.com/nicklansley/OllamaImageGenerator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;... but all you really need to download is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py"&gt;&lt;strong&gt;server.py&lt;/strong&gt;&lt;/a&gt; - small proxying server which you can run without extra packages on your machine as long as you have python 3.9 or higher. No need for a venv as I've just used built-in packages like &lt;em&gt;http.server&lt;/em&gt; - run from terminal as &lt;strong&gt;&lt;em&gt;python3&lt;/em&gt;&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py"&gt;&lt;strong&gt;&lt;em&gt;server.py&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/index.html"&gt;index.html&lt;/a&gt; - does all the grunt work in your web browser on port 8080. Generated images are saved to local storage along with generation settings, and can be deleted individually. Once &lt;a href="https://raw.githubusercontent.com/nicklansley/OllamaImageGenerator/refs/heads/main/server.py"&gt;server.py&lt;/a&gt; is up and running, take your web browser to &lt;a href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/nicklansley/OllamaImageGenerator/blob/main/README.md"&gt;README.md&lt;/a&gt; gives more info but quick instructions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Download an image generation model in terminal - currently '&lt;strong&gt;&lt;em&gt;ollama pull x/z-image-turbo:bf16&lt;/em&gt;&lt;/strong&gt;' and '&lt;strong&gt;&lt;em&gt;ollama pull x/flux2-klein:latest&lt;/em&gt;&lt;/strong&gt;' are supported.&lt;/li&gt; &lt;li&gt;Type a prompt, set image width and height, choose a seed and the number of steps, then click 'Generate Image'.&lt;/li&gt; &lt;li&gt;During image progression, a 'step N of X' message appears to denote progress.&lt;/li&gt; &lt;li&gt;Images are saved to a side panel (actually they are in localStorage so they survive from one session to the next).&lt;/li&gt; &lt;li&gt;Save an image onto your machine with right-click 'Save Image As..' or drag if out of the main window and into a folder.&lt;/li&gt; &lt;li&gt;Double click a saved image to move it to the main window along with the settings that created it (the image can be recreated as long as seed 0 (Ollama internal random seed) was not used.&lt;/li&gt; &lt;li&gt;Single click an image then click 'x' to delete it. The image is removed from the image list saved in localStorage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When more image models become available:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Download a model with 'image pull image_gen_model_name:tag'&lt;/li&gt; &lt;li&gt;Update image list variable IMAGE_GEN_MODEL_LIST with the same model name and tag at the top of index.html&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n8kap3a823fg1.png?width=1369&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=accb2258e20e62c356823ad8b85439aab8424f4a"&gt;Screenshot of Ollama Image Generator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nickinnov"&gt; /u/nickinnov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qkokhv/ollama_image_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qkokhv/ollama_image_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qkokhv/ollama_image_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-23T11:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkrpnx</id>
    <title>I gave my local LLM pipeline a brain - now it thinks before it speaks</title>
    <updated>2026-01-23T14:06:41+00:00</updated>
    <author>
      <name>/u/danny_094</name>
      <uri>https://old.reddit.com/user/danny_094</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qkrpnx/i_gave_my_local_llm_pipeline_a_brain_now_it/"&gt; &lt;img alt="I gave my local LLM pipeline a brain - now it thinks before it speaks" src="https://b.thumbs.redditmedia.com/_GXmiaE1Ea6_QZUV_vL2CWw8kZchIyXUNhv6uZUihVw.jpg" title="I gave my local LLM pipeline a brain - now it thinks before it speaks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jarvis/TRION has received a major update after weeks of implementation. Jarvis (soon to be TRION) has now been provided with a self-developed SEQUENTIAL THINKING MCP.&lt;/p&gt; &lt;p&gt;I would love to explain everything it can do in this Reddit post. But I don't have the space, and neither do you have the patience. &lt;a href="/u/frank_brsrk"&gt;u/frank_brsrk&lt;/a&gt; Provided a self-developed CIM framework That's hard twisted with Sequential Thinking. So Claude help for the answer:&lt;/p&gt; &lt;p&gt;üß† Gave my local Ollama setup &amp;quot;extended thinking&amp;quot; - like Claude, but 100% local&lt;/p&gt; &lt;p&gt;TL;DR: Built a Sequential Thinking system that lets DeepSeek-R1&lt;/p&gt; &lt;p&gt;&amp;quot;think out loud&amp;quot; step-by-step before answering. All local, all Ollama.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;p&gt;- Complex questions ‚Üí AI breaks them into steps&lt;/p&gt; &lt;p&gt;- You SEE the reasoning live (not just the answer)&lt;/p&gt; &lt;p&gt;- Reduces hallucinations significantly&lt;/p&gt; &lt;p&gt;The cool part: The AI decides WHEN to use deep thinking.&lt;/p&gt; &lt;p&gt;Simple questions ‚Üí instant answer.&lt;/p&gt; &lt;p&gt;Complex questions ‚Üí step-by-step reasoning first.&lt;/p&gt; &lt;p&gt;Built with: Ollama + DeepSeek-R1 + custom MCP servers&lt;/p&gt; &lt;p&gt;Shoutout to &lt;a href="/u/frank_brsrk"&gt;u/frank_brsrk&lt;/a&gt; for the CIM framework that makes&lt;/p&gt; &lt;p&gt;the reasoning actually make sense.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/danny094/Jarvis/tree/main"&gt;https://github.com/danny094/Jarvis/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions! This took weeks to build üòÖ&lt;/p&gt; &lt;p&gt;Other known issues:&lt;/p&gt; &lt;p&gt;- excessively long texts, skipping the control layer - Solution in progress&lt;/p&gt; &lt;p&gt;- The side panel is still being edited and will be integrated as a canvas with MCP support.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qkrpnx/video/zb6z5muax3fg1/player"&gt;https://reddit.com/link/1qkrpnx/video/zb6z5muax3fg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/el6uhfy6q4fg1.png?width=1147&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a16a9525fc50ba59b710f6932cdb3626c2562074"&gt;https://preview.redd.it/el6uhfy6q4fg1.png?width=1147&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a16a9525fc50ba59b710f6932cdb3626c2562074&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j1ol6fy6q4fg1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7726aee3e5079419dc665959fc0b779b6d37571"&gt;https://preview.redd.it/j1ol6fy6q4fg1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7726aee3e5079419dc665959fc0b779b6d37571&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danny_094"&gt; /u/danny_094 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qkrpnx/i_gave_my_local_llm_pipeline_a_brain_now_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qkrpnx/i_gave_my_local_llm_pipeline_a_brain_now_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qkrpnx/i_gave_my_local_llm_pipeline_a_brain_now_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-23T14:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlq6ug</id>
    <title>RTX 3070 Vulkan faster than Cuda?</title>
    <updated>2026-01-24T15:41:34+00:00</updated>
    <author>
      <name>/u/Kenta_Hirono</name>
      <uri>https://old.reddit.com/user/Kenta_Hirono</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I recently upgraded my old RX570 8GB to a less old RTX3070 8GB.&lt;br /&gt; I'm using ollama by Alpaca on Fedora43, mainly to translate books with aya-expanse model (8B Q4_K_M) and Calibre's ebook translator plugin.&lt;/p&gt; &lt;p&gt;Testing the &amp;quot;new&amp;quot; card, I noticed using Vulkan (CUDA_VISIBLE_DEVICE=1 + OLLAMA_VULKAN=1) is slightly faster than using Cuda (CUDA_VISIBLE_DEVICE=0 + OLLAMA_VULKAN=0), 1.094x .&lt;/p&gt; &lt;p&gt;Is this a normal behaviour?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kenta_Hirono"&gt; /u/Kenta_Hirono &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlq6ug/rtx_3070_vulkan_faster_than_cuda/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlq6ug/rtx_3070_vulkan_faster_than_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qlq6ug/rtx_3070_vulkan_faster_than_cuda/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-24T15:41:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlwyu4</id>
    <title>Nvidia dual gpu docker setup</title>
    <updated>2026-01-24T19:52:36+00:00</updated>
    <author>
      <name>/u/Alternative_Room_822</name>
      <uri>https://old.reddit.com/user/Alternative_Room_822</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Afternoon all - Hate to ask for help, but i'm getting desperate:)&lt;/p&gt; &lt;p&gt;Setup is thus:&lt;/p&gt; &lt;p&gt;Ollama - Docker - Linux - Nvidia 3060 12gb and 3050 8gb. nvidia drivers from 580-590 have been tried.&lt;/p&gt; &lt;p&gt;Hoping to use it as 20 to load slightly larger models. Just playing around.&lt;/p&gt; &lt;p&gt;Container runs fine with one card physically installed. Runs proplery with both cards physically installed, but only one seleted using something like --gpus '&amp;quot;device=0&amp;quot;'. Card is detected shows vram etc.&lt;/p&gt; &lt;p&gt;Running with --gpus all causes neither card to work. Detection in the container fails before timeout, and attempts to continue as cpu. This locks things up.&lt;/p&gt; &lt;p&gt;nvidia-smi -L is successful with 1 or 2 cards.&lt;/p&gt; &lt;p&gt;Afer container trying to load, nvidia-smi can be run, but hangs with no output. This also crashed the docker img. After trying 2 cards at the same time, the img gets &amp;quot;stuck&amp;quot; so it cannot be unmounted for reboot etc. Only path forward it forced reboot.&lt;/p&gt; &lt;p&gt;Anyone have any ideas? (hopefull look) I've tried searching around, and didn't have to much luck searching. This site &lt;a href="https://markaicode.com/multi-gpu-ollama-setup-large-model-inference/"&gt;https://markaicode.com/multi-gpu-ollama-setup-large-model-inference/&lt;/a&gt; suggests that I should edit /etc/systemd/system/ollama.service.d/override.conf. I assume they mean the one inside the docker? But that's ephemerial, isn't it?&lt;/p&gt; &lt;p&gt;Assuming that doesn't work, would running two containers, each assigned a card, using this: &lt;a href="https://localai.io/features/distribute/"&gt;https://localai.io/features/distribute/&lt;/a&gt; be functionally the same?&lt;/p&gt; &lt;p&gt;Am I just completely not grasping what I'm doing? &lt;/p&gt; &lt;p&gt;Thanks :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alternative_Room_822"&gt; /u/Alternative_Room_822 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlwyu4/nvidia_dual_gpu_docker_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlwyu4/nvidia_dual_gpu_docker_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qlwyu4/nvidia_dual_gpu_docker_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-24T19:52:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlz8y4</id>
    <title>Cline + Ollama Qwen3</title>
    <updated>2026-01-24T21:19:06+00:00</updated>
    <author>
      <name>/u/sinan_online</name>
      <uri>https://old.reddit.com/user/sinan_online</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sinan_online"&gt; /u/sinan_online &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1qlbph2/cline_ollama_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlz8y4/cline_ollama_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qlz8y4/cline_ollama_qwen3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-24T21:19:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlzbwk</id>
    <title>[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp; OpenAI-Compatible API</title>
    <updated>2026-01-24T21:22:14+00:00</updated>
    <author>
      <name>/u/blackstoreonline</name>
      <uri>https://old.reddit.com/user/blackstoreonline</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackstoreonline"&gt; /u/blackstoreonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlzbwk/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qlzbwk/release_qwen3tts_ultralow_latency_97ms_voice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-24T21:22:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlw48b</id>
    <title>‚ÄúLocal LLHAMA‚Äù project - Orchestration Middleware for Ollama and HomeAssistant</title>
    <updated>2026-01-24T19:20:31+00:00</updated>
    <author>
      <name>/u/NicolaZanarini533</name>
      <uri>https://old.reddit.com/user/NicolaZanarini533</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qlw48b/local_llhama_project_orchestration_middleware_for/"&gt; &lt;img alt="‚ÄúLocal LLHAMA‚Äù project - Orchestration Middleware for Ollama and HomeAssistant" src="https://b.thumbs.redditmedia.com/wmwG-5gFBvSU1f_PeAAO9_irl1ewGvzBDRnSBDFPXzU.jpg" title="‚ÄúLocal LLHAMA‚Äù project - Orchestration Middleware for Ollama and HomeAssistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What‚Äôs the problem/why build this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In a world where, to cite some other posts, the &amp;quot;enshittification of AI&amp;quot; is a trend, having the ability to run effective AI systems locally, even on modest hardware, becomes more and more important. This of course comes with its own problems, which this project aims to address.&lt;/p&gt; &lt;p&gt;The main idea here is that raw model size isn‚Äôt the blocker for smart‚Äëhome control and smart-home assistants ‚Äì it‚Äôs &lt;em&gt;routing &amp;amp; context&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Typical setups struggle with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi‚Äëintent utterances (e.g., ‚Äúturn off lights AND set alarm AND check weather‚Äù)&lt;/li&gt; &lt;li&gt;Exact device names / lack of fuzzy/multi‚Äëlang matching&lt;/li&gt; &lt;li&gt;Base‚Äëprompt control &amp;amp; external‚Äëdata integration&lt;/li&gt; &lt;li&gt;Conversation memory &amp;amp; user/system management&lt;/li&gt; &lt;li&gt;Working without cloud APIs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I‚Äôm building&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;An &lt;strong&gt;orchestration middleware&lt;/strong&gt; that sits &lt;em&gt;between Home Assistant and Ollama&lt;/em&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Decomposes intents in parallel&lt;/li&gt; &lt;li&gt;Routes each to the right backend (HA API, PostgreSQL, weather API, etc.)&lt;/li&gt; &lt;li&gt;Injects only the needed context&lt;/li&gt; &lt;li&gt;Auto‚Äëscales the prompt window&lt;/li&gt; &lt;li&gt;Synthesizes a single, natural‚Äëlanguage reply&lt;/li&gt; &lt;li&gt;Uses memory to include previous conversation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Result: 2‚Äì5 s for multi‚Äëintent commands; sub‚Äëminute even with web searches ‚Äì all offline.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware‚Äëvalidated presets&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;VRAM&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Languages&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;8 GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen2.5‚Äë8B&lt;/td&gt; &lt;td align="left"&gt;English only&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;16 GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen2.5‚Äë14B&lt;/td&gt; &lt;td align="left"&gt;6+ languages&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;24 GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;GPT‚ÄëOSS‚Äë20B&lt;/td&gt; &lt;td align="left"&gt;6+ languages&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Tested on:&lt;/p&gt; &lt;p&gt;- Xeon E5‚Äë2640 v4 + RTX 4060 Ti 16 GB&lt;/p&gt; &lt;p&gt;- i7‚Äë12700H + RTX 4060 8 GB (mobile)&lt;/p&gt; &lt;p&gt;- Xeon E5‚Äë2640 v4 + RTX 2080 Ti + Ollama VM with RTX 4060 Ti 16 GB.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example commands (single utterance)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ÄúTurn off the kitchen light, set my 7 am alarm and tell me the weather for tomorrow‚Äù&lt;/li&gt; &lt;li&gt;‚Äú¬øCu√°les son las noticias de Par√≠s? ¬øQu√© lugares interesantes hay para ver all√≠? ‚Äù&lt;/li&gt; &lt;li&gt;‚ÄúRappelle‚Äëmoi d‚Äôaller √† l‚ÄôAlexanderplatz demain ‚Äì comment devrais‚Äëje m‚Äôhabiller ? Aussi r√®gle le thermostat √† 22 ¬∞C ‚Äù&lt;/li&gt; &lt;li&gt;‚ÄúSpegni la luce della cucina e parlami di Roma‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The system auto‚Äëdetects language, fuzzy‚Äëmatches entities, and calls the appropriate functions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Architecture highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi‚Äëpass prompt engineering (base ‚Üí decision ‚Üí safety ‚Üí format)&lt;/li&gt; &lt;li&gt;Adaptive context windows&lt;/li&gt; &lt;li&gt;Parallel backend routing (HA + PostgreSQL + web APIs)&lt;/li&gt; &lt;li&gt;Reflection‚Äëbased function discovery&lt;/li&gt; &lt;li&gt;Per‚Äëuser conversation memory&lt;/li&gt; &lt;li&gt;Zero‚Äëcloud, privacy‚Äëfirst (no telemetry)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tech stack&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Python 3.10+ (3.12 recommended)&lt;/li&gt; &lt;li&gt;Ollama (any model; Qwen2.5 / GPT‚ÄëOSS tested)&lt;/li&gt; &lt;li&gt;Home Assistant (local or remote)&lt;/li&gt; &lt;li&gt;PostgreSQL (history + embeddings)&lt;/li&gt; &lt;li&gt;OpenWakeWord + Whisper + Piper TTS&lt;/li&gt; &lt;li&gt;Flask + WebSocket chat UI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;One‚Äëcommand setup with an interactive wizard.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Potential Other Uses&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The base structure of the project allows creating RAG-enhanced assistants, integrating with other systems and in general having full control over an Ai assistant that runs locally, but which can perform close to cloud solutions. I've used it to create a translation bot, a URS analysis bot, and many others.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;License &amp;amp; repo&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CC BY 4.0&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/Nemesis533/Local_LLHAMA"&gt;https://github.com/Nemesis533/Local_LLHAMA&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The project had started a while back but after the recent trends in &amp;quot;public AI&amp;quot;, has evolved to the state it is in today - happy to answer questions and get your feedback!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q4oyj5hhlcfg1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7140dd5fe64945e4622f108184ae27792a3e1731"&gt;https://preview.redd.it/q4oyj5hhlcfg1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7140dd5fe64945e4622f108184ae27792a3e1731&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NicolaZanarini533"&gt; /u/NicolaZanarini533 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlw48b/local_llhama_project_orchestration_middleware_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlw48b/local_llhama_project_orchestration_middleware_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qlw48b/local_llhama_project_orchestration_middleware_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-24T19:20:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm8g8t</id>
    <title>DGX Spark worth it for finetuning larger models?</title>
    <updated>2026-01-25T03:52:23+00:00</updated>
    <author>
      <name>/u/goldcakes</name>
      <uri>https://old.reddit.com/user/goldcakes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I already have a 5090 and have been having a lot of fun with Unsloth, and I‚Äôm ready to move on to larger models.&lt;/p&gt; &lt;p&gt;However, as I‚Äôm also a professional video editor, my 5090 often needs to be rendering/exporting, and it‚Äôs a pain constantly switching workloads. &lt;/p&gt; &lt;p&gt;I already know it‚Äôs a terrible rig for LLM inference, but can people confirm it‚Äôs okay for hobbyist local fine-tuning of larger LLMs? &lt;/p&gt; &lt;p&gt;With prices increasing on Strix Halo, the DGX premium seems a lot easier to justify‚Ä¶ lets me stay within CUDA and keeps my partner happier than building another big, power hungry rig.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goldcakes"&gt; /u/goldcakes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qm8g8t/dgx_spark_worth_it_for_finetuning_larger_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qm8g8t/dgx_spark_worth_it_for_finetuning_larger_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qm8g8t/dgx_spark_worth_it_for_finetuning_larger_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T03:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlssc0</id>
    <title>HashIndex: An alternative to a page that doesn't require RAG but can still perform indexing well.</title>
    <updated>2026-01-24T17:19:08+00:00</updated>
    <author>
      <name>/u/jasonhon2013</name>
      <uri>https://old.reddit.com/user/jasonhon2013</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Pardus AI team has decided to open source our memory system, which is similar to PageIndex. However, instead of using a B+ tree, we use a hash map to handle data. This feature allows you to parse the document only once, while achieving retrieval performance on par with PageIndex and significantly better than embedding vector search. It also supports Ollama. Give it a try and consider implementing it in your system ‚Äî you might like it! Give us a star maybe hahahaha&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/JasonHonKL/HashIndex/tree/main"&gt; https://github.com/JasonHonKL/HashIndex/tree/main &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonhon2013"&gt; /u/jasonhon2013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlssc0/hashindex_an_alternative_to_a_page_that_doesnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlssc0/hashindex_an_alternative_to_a_page_that_doesnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qlssc0/hashindex_an_alternative_to_a_page_that_doesnt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-24T17:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmeuon</id>
    <title>Image generation API</title>
    <updated>2026-01-25T09:39:22+00:00</updated>
    <author>
      <name>/u/empios</name>
      <uri>https://old.reddit.com/user/empios</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As I saw there is image generation on macOS using flux or z-image models and I tested it out in terminal looking promising! Is there any api currently working to connect it via local network similar to chat generation? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/empios"&gt; /u/empios &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmeuon/image_generation_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmeuon/image_generation_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qmeuon/image_generation_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T09:39:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlvud9</id>
    <title>Mac Studio as host for Ollama</title>
    <updated>2026-01-24T19:10:24+00:00</updated>
    <author>
      <name>/u/amgsus</name>
      <uri>https://old.reddit.com/user/amgsus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I‚Äôm wondering if it worth buying Mac Studio M4 Max (64 GB) for hosting Ollama. Does anyone have experience with this box? Or better to build a cluster of GPUs like RTX 3090, etc.?&lt;/p&gt; &lt;p&gt;Primarily, I will be using LLMs for coding. Rarely, for media content generation.&lt;/p&gt; &lt;p&gt;Kind regards&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amgsus"&gt; /u/amgsus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlvud9/mac_studio_as_host_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qlvud9/mac_studio_as_host_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qlvud9/mac_studio_as_host_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-24T19:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmlc8p</id>
    <title>My AI Open Source Workflow</title>
    <updated>2026-01-25T15:01:05+00:00</updated>
    <author>
      <name>/u/candidosales</name>
      <uri>https://old.reddit.com/user/candidosales</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately, I have been studying AI and Open Source Workflows. I thought it would be interesting to share a bit of what I'm learning: &lt;a href="https://www.candidosales.me/blog/my-ai-open-source-workflow"&gt;https://www.candidosales.me/blog/my-ai-open-source-workflow&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/candidosales"&gt; /u/candidosales &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmlc8p/my_ai_open_source_workflow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmlc8p/my_ai_open_source_workflow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qmlc8p/my_ai_open_source_workflow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T15:01:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmmp22</id>
    <title>Claude Code stuck on &lt;function=TaskList&gt; when using Ollama + Qwen3-Coder</title>
    <updated>2026-01-25T15:52:04+00:00</updated>
    <author>
      <name>/u/Healthy-Laugh-6745</name>
      <uri>https://old.reddit.com/user/Healthy-Laugh-6745</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm struggling to get Claude Code working with Ollama on my Mac M4 Max (48GB RAM). I strictly followed the official Ollama integration guide (&lt;a href="https://docs.ollama.com/integrations/claude-code"&gt;https://docs.ollama.com/integrations/claude-code&lt;/a&gt;), but I'm stuck in a loop.&lt;/p&gt; &lt;p&gt;Every time I ask the model to perform a file-based task (e.g., &amp;quot;create a txt file&amp;quot;), the process hangs indefinitely.&lt;/p&gt; &lt;p&gt;The model acknowledges the request.&lt;/p&gt; &lt;p&gt;It outputs: ‚ùØ &amp;lt;function=TaskList&amp;gt; ‚è∫&lt;/p&gt; &lt;p&gt;Nothing happens after that. No file is created, and the terminal just sits there with the &amp;quot;active&amp;quot; dot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Laugh-6745"&gt; /u/Healthy-Laugh-6745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qmmp22/claude_code_stuck_on_functiontasklist_when_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T15:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm9cgp</id>
    <title>Ollama Models Ranked by VRAM Requirements</title>
    <updated>2026-01-25T04:36:31+00:00</updated>
    <author>
      <name>/u/AdventurousLion9548</name>
      <uri>https://old.reddit.com/user/AdventurousLion9548</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1250.08 GB | cogito-2.1:latest&lt;/p&gt; &lt;p&gt;1250.08 GB | cogito-2.1:671b&lt;/p&gt; &lt;p&gt;376.71 GB | deepseek-v3.1:latest&lt;/p&gt; &lt;p&gt;376.71 GB | deepseek-v3.1:671b&lt;/p&gt; &lt;p&gt;376.65 GB | deepseek-r1:671b&lt;/p&gt; &lt;p&gt;376.65 GB | deepseek-v3:latest&lt;/p&gt; &lt;p&gt;376.65 GB | deepseek-v3:671b&lt;/p&gt; &lt;p&gt;376.65 GB | r1-1776:671b&lt;/p&gt; &lt;p&gt;270.14 GB | qwen3-coder:480b&lt;/p&gt; &lt;p&gt;226.38 GB | llama3.1:405b&lt;/p&gt; &lt;p&gt;213.14 GB | hermes3:405b&lt;/p&gt; &lt;p&gt;133.43 GB | qwen3-vl:235b&lt;/p&gt; &lt;p&gt;132.39 GB | qwen3:235b&lt;/p&gt; &lt;p&gt;123.78 GB | deepseek-coder-v2:236b&lt;/p&gt; &lt;p&gt;123.78 GB | deepseek-v2:236b&lt;/p&gt; &lt;p&gt;123.78 GB | deepseek-v2.5:latest&lt;/p&gt; &lt;p&gt;123.78 GB | deepseek-v2.5:236b&lt;/p&gt; &lt;p&gt;94.51 GB | falcon:180b&lt;/p&gt; &lt;p&gt;74.05 GB | zephyr:141b&lt;/p&gt; &lt;p&gt;69.75 GB | devstral-2:latest&lt;/p&gt; &lt;p&gt;69.75 GB | devstral-2:123b&lt;/p&gt; &lt;p&gt;69.1 GB | dbrx:latest&lt;/p&gt; &lt;p&gt;69.1 GB | dbrx:132b&lt;/p&gt; &lt;p&gt;68.19 GB | mistral-large:latest&lt;/p&gt; &lt;p&gt;68.19 GB | mistral-large:123b&lt;/p&gt; &lt;p&gt;63.1 GB | megadolphin:latest&lt;/p&gt; &lt;p&gt;63.1 GB | megadolphin:120b&lt;/p&gt; &lt;p&gt;62.81 GB | llama4:latest&lt;/p&gt; &lt;p&gt;62.52 GB | command-a:latest&lt;/p&gt; &lt;p&gt;62.52 GB | command-a:111b&lt;/p&gt; &lt;p&gt;60.88 GB | gpt-oss:120b&lt;/p&gt; &lt;p&gt;60.88 GB | gpt-oss-safeguard:120b&lt;/p&gt; &lt;p&gt;58.57 GB | qwen:110b&lt;/p&gt; &lt;p&gt;55.15 GB | command-r-plus:latest&lt;/p&gt; &lt;p&gt;55.15 GB | command-r-plus:104b&lt;/p&gt; &lt;p&gt;50.87 GB | llama3.2-vision:90b&lt;/p&gt; &lt;p&gt;46.89 GB | qwen3-next:latest&lt;/p&gt; &lt;p&gt;46.89 GB | qwen3-next:80b&lt;/p&gt; &lt;p&gt;45.36 GB | qwen2.5vl:72b&lt;/p&gt; &lt;p&gt;44.16 GB | athene-v2:latest&lt;/p&gt; &lt;p&gt;44.16 GB | athene-v2:72b&lt;/p&gt; &lt;p&gt;44.16 GB | qwen2.5:72b&lt;/p&gt; &lt;p&gt;39.6 GB | cogito:70b&lt;/p&gt; &lt;p&gt;39.6 GB | deepseek-r1:70b&lt;/p&gt; &lt;p&gt;39.6 GB | llama3.1:70b&lt;/p&gt; &lt;p&gt;39.6 GB | llama3.3:latest&lt;/p&gt; &lt;p&gt;39.6 GB | llama3.3:70b&lt;/p&gt; &lt;p&gt;39.6 GB | nemotron:latest&lt;/p&gt; &lt;p&gt;39.6 GB | nemotron:70b&lt;/p&gt; &lt;p&gt;39.6 GB | r1-1776:latest&lt;/p&gt; &lt;p&gt;39.6 GB | r1-1776:70b&lt;/p&gt; &lt;p&gt;39.6 GB | tulu3:70b&lt;/p&gt; &lt;p&gt;38.4 GB | qwen2:72b&lt;/p&gt; &lt;p&gt;38.4 GB | qwen2-math:72b&lt;/p&gt; &lt;p&gt;38.18 GB | qwen:72b&lt;/p&gt; &lt;p&gt;37.22 GB | dolphin-llama3:70b&lt;/p&gt; &lt;p&gt;37.22 GB | firefunction-v2:latest&lt;/p&gt; &lt;p&gt;37.22 GB | firefunction-v2:70b&lt;/p&gt; &lt;p&gt;37.22 GB | hermes3:70b&lt;/p&gt; &lt;p&gt;37.22 GB | llama3:70b&lt;/p&gt; &lt;p&gt;37.22 GB | llama3-chatqa:70b&lt;/p&gt; &lt;p&gt;37.22 GB | llama3-gradient:70b&lt;/p&gt; &lt;p&gt;37.22 GB | llama3-groq-tool-use:70b&lt;/p&gt; &lt;p&gt;37.22 GB | reflection:latest&lt;/p&gt; &lt;p&gt;37.22 GB | reflection:70b&lt;/p&gt; &lt;p&gt;36.2 GB | codellama:70b&lt;/p&gt; &lt;p&gt;36.2 GB | llama2:70b&lt;/p&gt; &lt;p&gt;36.2 GB | llama2-uncensored:70b&lt;/p&gt; &lt;p&gt;36.2 GB | meditron:70b&lt;/p&gt; &lt;p&gt;36.2 GB | orca-mini:70b&lt;/p&gt; &lt;p&gt;36.2 GB | stable-beluga:70b&lt;/p&gt; &lt;p&gt;36.2 GB | wizard-math:70b&lt;/p&gt; &lt;p&gt;35.53 GB | deepseek-llm:67b&lt;/p&gt; &lt;p&gt;24.63 GB | dolphin-mixtral:latest&lt;/p&gt; &lt;p&gt;24.63 GB | mixtral:latest&lt;/p&gt; &lt;p&gt;24.63 GB | notux:latest&lt;/p&gt; &lt;p&gt;24.63 GB | nous-hermes2-mixtral:latest&lt;/p&gt; &lt;p&gt;22.6 GB | nemotron-3-nano:latest&lt;/p&gt; &lt;p&gt;22.6 GB | nemotron-3-nano:30b&lt;/p&gt; &lt;p&gt;22.17 GB | alfred:latest&lt;/p&gt; &lt;p&gt;22.17 GB | alfred:40b&lt;/p&gt; &lt;p&gt;22.17 GB | falcon:40b&lt;/p&gt; &lt;p&gt;19.71 GB | qwen2.5vl:32b&lt;/p&gt; &lt;p&gt;19.47 GB | qwen3-vl:32b&lt;/p&gt; &lt;p&gt;18.84 GB | aya:35b&lt;/p&gt; &lt;p&gt;18.81 GB | qwen3:32b&lt;/p&gt; &lt;p&gt;18.78 GB | llava:34b&lt;/p&gt; &lt;p&gt;18.49 GB | cogito:32b&lt;/p&gt; &lt;p&gt;18.49 GB | deepseek-r1:32b&lt;/p&gt; &lt;p&gt;18.49 GB | openthinker:32b&lt;/p&gt; &lt;p&gt;18.49 GB | qwen2.5:32b&lt;/p&gt; &lt;p&gt;18.49 GB | qwen2.5-coder:32b&lt;/p&gt; &lt;p&gt;18.49 GB | qwq:latest&lt;/p&gt; &lt;p&gt;18.49 GB | qwq:32b&lt;/p&gt; &lt;p&gt;18.44 GB | aya-expanse:32b&lt;/p&gt; &lt;p&gt;18.25 GB | qwen3-vl:30b&lt;/p&gt; &lt;p&gt;18.14 GB | olmo-3:32b&lt;/p&gt; &lt;p&gt;18.14 GB | olmo-3.1:latest&lt;/p&gt; &lt;p&gt;18.14 GB | olmo-3.1:32b&lt;/p&gt; &lt;p&gt;18.13 GB | nous-hermes2:34b&lt;/p&gt; &lt;p&gt;18.13 GB | yi:34b&lt;/p&gt; &lt;p&gt;18.02 GB | exaone-deep:32b&lt;/p&gt; &lt;p&gt;18.02 GB | exaone3.5:32b&lt;/p&gt; &lt;p&gt;17.92 GB | granite-code:34b&lt;/p&gt; &lt;p&gt;17.74 GB | codebooga:latest&lt;/p&gt; &lt;p&gt;17.74 GB | codebooga:34b&lt;/p&gt; &lt;p&gt;17.74 GB | codellama:34b&lt;/p&gt; &lt;p&gt;17.74 GB | phind-codellama:latest&lt;/p&gt; &lt;p&gt;17.74 GB | phind-codellama:34b&lt;/p&gt; &lt;p&gt;17.53 GB | deepseek-coder:33b&lt;/p&gt; &lt;p&gt;17.53 GB | wizardcoder:33b&lt;/p&gt; &lt;p&gt;17.43 GB | command-r:latest&lt;/p&gt; &lt;p&gt;17.43 GB | command-r:35b&lt;/p&gt; &lt;p&gt;17.28 GB | qwen3:30b&lt;/p&gt; &lt;p&gt;17.28 GB | qwen3-coder:latest&lt;/p&gt; &lt;p&gt;17.28 GB | qwen3-coder:30b&lt;/p&gt; &lt;p&gt;17.23 GB | qwen:32b&lt;/p&gt; &lt;p&gt;17.1 GB | vicuna:33b&lt;/p&gt; &lt;p&gt;17.1 GB | wizard-vicuna-uncensored:30b&lt;/p&gt; &lt;p&gt;16.2 GB | gemma3:27b&lt;/p&gt; &lt;p&gt;16.17 GB | translategemma:27b&lt;/p&gt; &lt;p&gt;15.5 GB | shieldgemma:27b&lt;/p&gt; &lt;p&gt;14.56 GB | gemma2:27b&lt;/p&gt; &lt;p&gt;14.42 GB | mistral-small3.1:latest&lt;/p&gt; &lt;p&gt;14.42 GB | mistral-small3.1:24b&lt;/p&gt; &lt;p&gt;14.14 GB | devstral-small-2:latest&lt;/p&gt; &lt;p&gt;14.14 GB | devstral-small-2:24b&lt;/p&gt; &lt;p&gt;14.14 GB | mistral-small3.2:latest&lt;/p&gt; &lt;p&gt;14.14 GB | mistral-small3.2:24b&lt;/p&gt; &lt;p&gt;13.35 GB | devstral:latest&lt;/p&gt; &lt;p&gt;13.35 GB | devstral:24b&lt;/p&gt; &lt;p&gt;13.35 GB | magistral:latest&lt;/p&gt; &lt;p&gt;13.35 GB | magistral:24b&lt;/p&gt; &lt;p&gt;13.35 GB | mistral-small:latest&lt;/p&gt; &lt;p&gt;13.35 GB | mistral-small:24b&lt;/p&gt; &lt;p&gt;12.85 GB | gpt-oss:latest&lt;/p&gt; &lt;p&gt;12.85 GB | gpt-oss:20b&lt;/p&gt; &lt;p&gt;12.85 GB | gpt-oss-safeguard:latest&lt;/p&gt; &lt;p&gt;12.85 GB | gpt-oss-safeguard:20b&lt;/p&gt; &lt;p&gt;12.4 GB | solar-pro:latest&lt;/p&gt; &lt;p&gt;12.4 GB | solar-pro:22b&lt;/p&gt; &lt;p&gt;11.71 GB | codestral:latest&lt;/p&gt; &lt;p&gt;11.71 GB | codestral:22b&lt;/p&gt; &lt;p&gt;11.71 GB | mistral-small:22b&lt;/p&gt; &lt;p&gt;10.82 GB | sailor2:20b&lt;/p&gt; &lt;p&gt;10.76 GB | granite-code:20b&lt;/p&gt; &lt;p&gt;10.55 GB | internlm2:20b&lt;/p&gt; &lt;p&gt;10.35 GB | phi4-reasoning:latest&lt;/p&gt; &lt;p&gt;10.35 GB | phi4-reasoning:14b&lt;/p&gt; &lt;p&gt;8.64 GB | qwen3:14b&lt;/p&gt; &lt;p&gt;8.46 GB | ministral-3:14b&lt;/p&gt; &lt;p&gt;8.44 GB | dolphincoder:15b&lt;/p&gt; &lt;p&gt;8.44 GB | starcoder2:15b&lt;/p&gt; &lt;p&gt;8.43 GB | phi4:latest&lt;/p&gt; &lt;p&gt;8.43 GB | phi4:14b&lt;/p&gt; &lt;p&gt;8.37 GB | cogito:14b&lt;/p&gt; &lt;p&gt;8.37 GB | deepcoder:latest&lt;/p&gt; &lt;p&gt;8.37 GB | deepcoder:14b&lt;/p&gt; &lt;p&gt;8.37 GB | deepseek-r1:14b&lt;/p&gt; &lt;p&gt;8.37 GB | qwen2.5:14b&lt;/p&gt; &lt;p&gt;8.37 GB | qwen2.5-coder:14b&lt;/p&gt; &lt;p&gt;8.37 GB | sqlcoder:15b&lt;/p&gt; &lt;p&gt;8.37 GB | starcoder:15b&lt;/p&gt; &lt;p&gt;8.29 GB | deepseek-coder-v2:latest&lt;/p&gt; &lt;p&gt;8.29 GB | deepseek-coder-v2:16b&lt;/p&gt; &lt;p&gt;8.29 GB | deepseek-v2:latest&lt;/p&gt; &lt;p&gt;8.29 GB | deepseek-v2:16b&lt;/p&gt; &lt;p&gt;7.78 GB | olmo2:13b&lt;/p&gt; &lt;p&gt;7.62 GB | qwen:14b&lt;/p&gt; &lt;p&gt;7.59 GB | gemma3:12b&lt;/p&gt; &lt;p&gt;7.55 GB | translategemma:12b&lt;/p&gt; &lt;p&gt;7.46 GB | llava:13b&lt;/p&gt; &lt;p&gt;7.35 GB | phi3:14b&lt;/p&gt; &lt;p&gt;7.28 GB | llama3.2-vision:latest&lt;/p&gt; &lt;p&gt;7.28 GB | llama3.2-vision:11b&lt;/p&gt; &lt;p&gt;7.03 GB | gemma3n:latest&lt;/p&gt; &lt;p&gt;6.86 GB | codellama:13b&lt;/p&gt; &lt;p&gt;6.86 GB | codeup:latest&lt;/p&gt; &lt;p&gt;6.86 GB | codeup:13b&lt;/p&gt; &lt;p&gt;6.86 GB | everythinglm:latest&lt;/p&gt; &lt;p&gt;6.86 GB | everythinglm:13b&lt;/p&gt; &lt;p&gt;6.86 GB | llama2:13b&lt;/p&gt; &lt;p&gt;6.86 GB | llama2-chinese:13b&lt;/p&gt; &lt;p&gt;6.86 GB | nexusraven:latest&lt;/p&gt; &lt;p&gt;6.86 GB | nexusraven:13b&lt;/p&gt; &lt;p&gt;6.86 GB | nous-hermes:13b&lt;/p&gt; &lt;p&gt;6.86 GB | open-orca-platypus2:latest&lt;/p&gt; &lt;p&gt;6.86 GB | open-orca-platypus2:13b&lt;/p&gt; &lt;p&gt;6.86 GB | orca-mini:13b&lt;/p&gt; &lt;p&gt;6.86 GB | orca2:13b&lt;/p&gt; &lt;p&gt;6.86 GB | stable-beluga:13b&lt;/p&gt; &lt;p&gt;6.86 GB | vicuna:13b&lt;/p&gt; &lt;p&gt;6.86 GB | wizard-math:13b&lt;/p&gt; &lt;p&gt;6.86 GB | wizard-vicuna:latest&lt;/p&gt; &lt;p&gt;6.86 GB | wizard-vicuna:13b&lt;/p&gt; &lt;p&gt;6.86 GB | wizard-vicuna-uncensored:13b&lt;/p&gt; &lt;p&gt;6.86 GB | wizardlm-uncensored:latest&lt;/p&gt; &lt;p&gt;6.86 GB | wizardlm-uncensored:13b&lt;/p&gt; &lt;p&gt;6.86 GB | xwinlm:13b&lt;/p&gt; &lt;p&gt;6.86 GB | yarn-llama2:13b&lt;/p&gt; &lt;p&gt;6.59 GB | mistral-nemo:latest&lt;/p&gt; &lt;p&gt;6.59 GB | mistral-nemo:12b&lt;/p&gt; &lt;p&gt;6.49 GB | stablelm2:12b&lt;/p&gt; &lt;p&gt;6.23 GB | deepseek-ocr:latest&lt;/p&gt; &lt;p&gt;6.23 GB | deepseek-ocr:3b&lt;/p&gt; &lt;p&gt;5.94 GB | falcon2:latest&lt;/p&gt; &lt;p&gt;5.94 GB | falcon2:11b&lt;/p&gt; &lt;p&gt;5.86 GB | falcon3:10b&lt;/p&gt; &lt;p&gt;5.72 GB | qwen3-vl:latest&lt;/p&gt; &lt;p&gt;5.72 GB | qwen3-vl:8b&lt;/p&gt; &lt;p&gt;5.66 GB | nous-hermes2:latest&lt;/p&gt; &lt;p&gt;5.66 GB | nous-hermes2:10.7b&lt;/p&gt; &lt;p&gt;5.66 GB | solar:latest&lt;/p&gt; &lt;p&gt;5.66 GB | solar:10.7b&lt;/p&gt; &lt;p&gt;5.61 GB | ministral-3:latest&lt;/p&gt; &lt;p&gt;5.61 GB | ministral-3:8b&lt;/p&gt; &lt;p&gt;5.56 GB | qwen2.5vl:latest&lt;/p&gt; &lt;p&gt;5.56 GB | qwen2.5vl:7b&lt;/p&gt; &lt;p&gt;5.4 GB | granite3-guardian:8b&lt;/p&gt; &lt;p&gt;5.37 GB | shieldgemma:latest&lt;/p&gt; &lt;p&gt;5.37 GB | shieldgemma:9b&lt;/p&gt; &lt;p&gt;5.16 GB | llava-llama3:latest&lt;/p&gt; &lt;p&gt;5.16 GB | llava-llama3:8b&lt;/p&gt; &lt;p&gt;5.1 GB | minicpm-v:latest&lt;/p&gt; &lt;p&gt;5.1 GB | minicpm-v:8b&lt;/p&gt; &lt;p&gt;5.08 GB | codegeex4:latest&lt;/p&gt; &lt;p&gt;5.08 GB | codegeex4:9b&lt;/p&gt; &lt;p&gt;5.08 GB | glm4:latest&lt;/p&gt; &lt;p&gt;5.08 GB | glm4:9b&lt;/p&gt; &lt;p&gt;5.07 GB | gemma2:latest&lt;/p&gt; &lt;p&gt;5.07 GB | gemma2:9b&lt;/p&gt; &lt;p&gt;4.88 GB | sailor2:latest&lt;/p&gt; &lt;p&gt;4.88 GB | sailor2:8b&lt;/p&gt; &lt;p&gt;4.87 GB | deepseek-r1:latest&lt;/p&gt; &lt;p&gt;4.87 GB | deepseek-r1:8b&lt;/p&gt; &lt;p&gt;4.87 GB | qwen3:latest&lt;/p&gt; &lt;p&gt;4.87 GB | qwen3:8b&lt;/p&gt; &lt;p&gt;4.76 GB | rnj-1:latest&lt;/p&gt; &lt;p&gt;4.76 GB | rnj-1:8b&lt;/p&gt; &lt;p&gt;4.71 GB | aya-expanse:latest&lt;/p&gt; &lt;p&gt;4.71 GB | aya-expanse:8b&lt;/p&gt; &lt;p&gt;4.71 GB | command-r7b:latest&lt;/p&gt; &lt;p&gt;4.71 GB | command-r7b:7b&lt;/p&gt; &lt;p&gt;4.71 GB | command-r7b-arabic:latest&lt;/p&gt; &lt;p&gt;4.71 GB | command-r7b-arabic:7b&lt;/p&gt; &lt;p&gt;4.69 GB | yi:9b&lt;/p&gt; &lt;p&gt;4.69 GB | yi-coder:latest&lt;/p&gt; &lt;p&gt;4.69 GB | yi-coder:9b&lt;/p&gt; &lt;p&gt;4.67 GB | codegemma:latest&lt;/p&gt; &lt;p&gt;4.67 GB | codegemma:7b&lt;/p&gt; &lt;p&gt;4.67 GB | gemma:latest&lt;/p&gt; &lt;p&gt;4.67 GB | gemma:7b&lt;/p&gt; &lt;p&gt;4.65 GB | granite3.1-dense:latest&lt;/p&gt; &lt;p&gt;4.65 GB | granite3.1-dense:8b&lt;/p&gt; &lt;p&gt;4.6 GB | granite3-dense:8b&lt;/p&gt; &lt;p&gt;4.6 GB | granite3.2:latest&lt;/p&gt; &lt;p&gt;4.6 GB | granite3.2:8b&lt;/p&gt; &lt;p&gt;4.6 GB | granite3.3:latest&lt;/p&gt; &lt;p&gt;4.6 GB | granite3.3:8b&lt;/p&gt; &lt;p&gt;4.58 GB | cogito:latest&lt;/p&gt; &lt;p&gt;4.58 GB | cogito:8b&lt;/p&gt; &lt;p&gt;4.58 GB | dolphin3:latest&lt;/p&gt; &lt;p&gt;4.58 GB | dolphin3:8b&lt;/p&gt; &lt;p&gt;4.58 GB | llama-guard3:latest&lt;/p&gt; &lt;p&gt;4.58 GB | llama-guard3:8b&lt;/p&gt; &lt;p&gt;4.58 GB | llama3.1:latest&lt;/p&gt; &lt;p&gt;4.58 GB | llama3.1:8b&lt;/p&gt; &lt;p&gt;4.58 GB | tulu3:latest&lt;/p&gt; &lt;p&gt;4.58 GB | tulu3:8b&lt;/p&gt; &lt;p&gt;4.47 GB | aya:latest&lt;/p&gt; &lt;p&gt;4.47 GB | aya:8b&lt;/p&gt; &lt;p&gt;4.44 GB | exaone-deep:latest&lt;/p&gt; &lt;p&gt;4.44 GB | exaone-deep:7.8b&lt;/p&gt; &lt;p&gt;4.44 GB | exaone3.5:latest&lt;/p&gt; &lt;p&gt;4.44 GB | exaone3.5:7.8b&lt;/p&gt; &lt;p&gt;4.41 GB | bakllava:latest&lt;/p&gt; &lt;p&gt;4.41 GB | bakllava:7b&lt;/p&gt; &lt;p&gt;4.41 GB | llama-pro:latest&lt;/p&gt; &lt;p&gt;4.41 GB | llava:latest&lt;/p&gt; &lt;p&gt;4.41 GB | llava:7b&lt;/p&gt; &lt;p&gt;4.41 GB | opencoder:latest&lt;/p&gt; &lt;p&gt;4.41 GB | opencoder:8b&lt;/p&gt; &lt;p&gt;4.39 GB | bespoke-minicheck:latest&lt;/p&gt; &lt;p&gt;4.39 GB | bespoke-minicheck:7b&lt;/p&gt; &lt;p&gt;4.36 GB | deepseek-r1:7b&lt;/p&gt; &lt;p&gt;4.36 GB | marco-o1:latest&lt;/p&gt; &lt;p&gt;4.36 GB | marco-o1:7b&lt;/p&gt; &lt;p&gt;4.36 GB | openthinker:latest&lt;/p&gt; &lt;p&gt;4.36 GB | openthinker:7b&lt;/p&gt; &lt;p&gt;4.36 GB | qwen2.5:latest&lt;/p&gt; &lt;p&gt;4.36 GB | qwen2.5:7b&lt;/p&gt; &lt;p&gt;4.36 GB | qwen2.5-coder:latest&lt;/p&gt; &lt;p&gt;4.36 GB | qwen2.5-coder:7b&lt;/p&gt; &lt;p&gt;4.36 GB | qwen3-embedding:latest&lt;/p&gt; &lt;p&gt;4.36 GB | qwen3-embedding:8b&lt;/p&gt; &lt;p&gt;4.34 GB | dolphin-llama3:latest&lt;/p&gt; &lt;p&gt;4.34 GB | dolphin-llama3:8b&lt;/p&gt; &lt;p&gt;4.34 GB | hermes3:latest&lt;/p&gt; &lt;p&gt;4.34 GB | hermes3:8b&lt;/p&gt; &lt;p&gt;4.34 GB | llama3:latest&lt;/p&gt; &lt;p&gt;4.34 GB | llama3:8b&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-chatqa:latest&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-chatqa:8b&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-gradient:latest&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-gradient:8b&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-groq-tool-use:latest&lt;/p&gt; &lt;p&gt;4.34 GB | llama3-groq-tool-use:8b&lt;/p&gt; &lt;p&gt;4.28 GB | granite-code:8b&lt;/p&gt; &lt;p&gt;4.26 GB | falcon3:latest&lt;/p&gt; &lt;p&gt;4.26 GB | falcon3:7b&lt;/p&gt; &lt;p&gt;4.2 GB | qwen:7b&lt;/p&gt; &lt;p&gt;4.16 GB | olmo-3:latest&lt;/p&gt; &lt;p&gt;4.16 GB | olmo-3:7b&lt;/p&gt; &lt;p&gt;4.16 GB | olmo2:latest&lt;/p&gt; &lt;p&gt;4.16 GB | olmo2:7b&lt;/p&gt; &lt;p&gt;4.15 GB | internlm2:latest&lt;/p&gt; &lt;p&gt;4.15 GB | internlm2:7b&lt;/p&gt; &lt;p&gt;4.13 GB | qwen2:latest&lt;/p&gt; &lt;p&gt;4.13 GB | qwen2:7b&lt;/p&gt; &lt;p&gt;4.13 GB | qwen2-math:latest&lt;/p&gt; &lt;p&gt;4.13 GB | qwen2-math:7b&lt;/p&gt; &lt;p&gt;4.07 GB | mistral:latest&lt;/p&gt; &lt;p&gt;4.07 GB | mistral:7b&lt;/p&gt; &lt;p&gt;4.0 GB | starcoder:7b&lt;/p&gt; &lt;p&gt;3.94 GB | dolphincoder:latest&lt;/p&gt; &lt;p&gt;3.94 GB | dolphincoder:7b&lt;/p&gt; &lt;p&gt;3.92 GB | falcon:latest&lt;/p&gt; &lt;p&gt;3.92 GB | falcon:7b&lt;/p&gt; &lt;p&gt;3.89 GB | codeqwen:latest&lt;/p&gt; &lt;p&gt;3.89 GB | codeqwen:7b&lt;/p&gt; &lt;p&gt;3.83 GB | dolphin-mistral:latest&lt;/p&gt; &lt;p&gt;3.83 GB | dolphin-mistral:7b&lt;/p&gt; &lt;p&gt;3.83 GB | mathstral:latest&lt;/p&gt; &lt;p&gt;3.83 GB | mathstral:7b&lt;/p&gt; &lt;p&gt;3.83 GB | mistral-openorca:latest&lt;/p&gt; &lt;p&gt;3.83 GB | mistral-openorca:7b&lt;/p&gt; &lt;p&gt;3.83 GB | mistrallite:latest&lt;/p&gt; &lt;p&gt;3.83 GB | mistrallite:7b&lt;/p&gt; &lt;p&gt;3.83 GB | neural-chat:latest&lt;/p&gt; &lt;p&gt;3.83 GB | neural-chat:7b&lt;/p&gt; &lt;p&gt;3.83 GB | notus:latest&lt;/p&gt; &lt;p&gt;3.83 GB | notus:7b&lt;/p&gt; &lt;p&gt;3.83 GB | openchat:latest&lt;/p&gt; &lt;p&gt;3.83 GB | openchat:7b&lt;/p&gt; &lt;p&gt;3.83 GB | openhermes:latest&lt;/p&gt; &lt;p&gt;3.83 GB | samantha-mistral:latest&lt;/p&gt; &lt;p&gt;3.83 GB | samantha-mistral:7b&lt;/p&gt; &lt;p&gt;3.83 GB | sqlcoder:latest&lt;/p&gt; &lt;p&gt;3.83 GB | sqlcoder:7b&lt;/p&gt; &lt;p&gt;3.83 GB | starling-lm:latest&lt;/p&gt; &lt;p&gt;3.83 GB | starling-lm:7b&lt;/p&gt; &lt;p&gt;3.83 GB | wizard-math:latest&lt;/p&gt; &lt;p&gt;3.83 GB | wizard-math:7b&lt;/p&gt; &lt;p&gt;3.83 GB | wizardlm2:latest&lt;/p&gt; &lt;p&gt;3.83 GB | wizardlm2:7b&lt;/p&gt; &lt;p&gt;3.83 GB | yarn-mistral:latest&lt;/p&gt; &lt;p&gt;3.83 GB | yarn-mistral:7b&lt;/p&gt; &lt;p&gt;3.83 GB | zephyr:latest&lt;/p&gt; &lt;p&gt;3.83 GB | zephyr:7b&lt;/p&gt; &lt;p&gt;3.77 GB | starcoder2:7b&lt;/p&gt; &lt;p&gt;3.73 GB | deepseek-llm:latest&lt;/p&gt; &lt;p&gt;3.73 GB | deepseek-llm:7b&lt;/p&gt; &lt;p&gt;3.56 GB | codellama:latest&lt;/p&gt; &lt;p&gt;3.56 GB | codellama:7b&lt;/p&gt; &lt;p&gt;3.56 GB | deepseek-coder:6.7b&lt;/p&gt; &lt;p&gt;3.56 GB | duckdb-nsql:latest&lt;/p&gt; &lt;p&gt;3.56 GB | duckdb-nsql:7b&lt;/p&gt; &lt;p&gt;3.56 GB | llama2:latest&lt;/p&gt; &lt;p&gt;3.56 GB | llama2:7b&lt;/p&gt; &lt;p&gt;3.56 GB | llama2-chinese:latest&lt;/p&gt; &lt;p&gt;3.56 GB | llama2-chinese:7b&lt;/p&gt; &lt;p&gt;3.56 GB | llama2-uncensored:latest&lt;/p&gt; &lt;p&gt;3.56 GB | llama2-uncensored:7b&lt;/p&gt; &lt;p&gt;3.56 GB | magicoder:latest&lt;/p&gt; &lt;p&gt;3.56 GB | magicoder:7b&lt;/p&gt; &lt;p&gt;3.56 GB | meditron:latest&lt;/p&gt; &lt;p&gt;3.56 GB | meditron:7b&lt;/p&gt; &lt;p&gt;3.56 GB | medllama2:latest&lt;/p&gt; &lt;p&gt;3.56 GB | medllama2:7b&lt;/p&gt; &lt;p&gt;3.56 GB | nous-hermes:latest&lt;/p&gt; &lt;p&gt;3.56 GB | nous-hermes:7b&lt;/p&gt; &lt;p&gt;3.56 GB | orca-mini:7b&lt;/p&gt; &lt;p&gt;3.56 GB | orca2:latest&lt;/p&gt; &lt;p&gt;3.56 GB | orca2:7b&lt;/p&gt; &lt;p&gt;3.56 GB | stable-beluga:latest&lt;/p&gt; &lt;p&gt;3.56 GB | stable-beluga:7b&lt;/p&gt; &lt;p&gt;3.56 GB | vicuna:latest&lt;/p&gt; &lt;p&gt;3.56 GB | vicuna:7b&lt;/p&gt; &lt;p&gt;3.56 GB | wizard-vicuna-uncensored:latest&lt;/p&gt; &lt;p&gt;3.56 GB | wizard-vicuna-uncensored:7b&lt;/p&gt; &lt;p&gt;3.56 GB | xwinlm:latest&lt;/p&gt; &lt;p&gt;3.56 GB | xwinlm:7b&lt;/p&gt; &lt;p&gt;3.56 GB | yarn-llama2:latest&lt;/p&gt; &lt;p&gt;3.56 GB | yarn-llama2:7b&lt;/p&gt; &lt;p&gt;3.37 GB | smallthinker:latest&lt;/p&gt; &lt;p&gt;3.37 GB | smallthinker:3b&lt;/p&gt; &lt;p&gt;3.32 GB | deepscaler:latest&lt;/p&gt; &lt;p&gt;3.32 GB | deepscaler:1.5b&lt;/p&gt; &lt;p&gt;3.24 GB | yi:latest&lt;/p&gt; &lt;p&gt;3.24 GB | yi:6b&lt;/p&gt; &lt;p&gt;3.11 GB | gemma3:latest&lt;/p&gt; &lt;p&gt;3.11 GB | gemma3:4b&lt;/p&gt; &lt;p&gt;3.07 GB | qwen3-vl:4b&lt;/p&gt; &lt;p&gt;3.07 GB | translategemma:latest&lt;/p&gt; &lt;p&gt;3.07 GB | translategemma:4b&lt;/p&gt; &lt;p&gt;3.04 GB | granite4:1b&lt;/p&gt; &lt;p&gt;2.98 GB | qwen2.5vl:3b&lt;/p&gt; &lt;p&gt;2.94 GB | phi4-mini-reasoning:latest&lt;/p&gt; &lt;p&gt;2.94 GB | phi4-mini-reasoning:3.8b&lt;/p&gt; &lt;p&gt;2.75 GB | ministral-3:3b&lt;/p&gt; &lt;p&gt;2.73 GB | llava-phi3:latest&lt;/p&gt; &lt;p&gt;2.73 GB | llava-phi3:3.8b&lt;/p&gt; &lt;p&gt;2.51 GB | granite3-guardian:latest&lt;/p&gt; &lt;p&gt;2.51 GB | granite3-guardian:2b&lt;/p&gt; &lt;p&gt;2.51 GB | nemotron-mini:latest&lt;/p&gt; &lt;p&gt;2.51 GB | nemotron-mini:4b&lt;/p&gt; &lt;p&gt;2.33 GB | qwen3:4b&lt;/p&gt; &lt;p&gt;2.33 GB | qwen3-embedding:4b&lt;/p&gt; &lt;p&gt;2.32 GB | phi4-mini:latest&lt;/p&gt; &lt;p&gt;2.32 GB | phi4-mini:3.8b&lt;/p&gt; &lt;p&gt;2.27 GB | granite3.2-vision:latest&lt;/p&gt; &lt;p&gt;2.27 GB | granite3.2-vision:2b&lt;/p&gt; &lt;p&gt;2.17 GB | qwen:latest&lt;/p&gt; &lt;p&gt;2.17 GB | qwen:4b&lt;/p&gt; &lt;p&gt;2.09 GB | cogito:3b&lt;/p&gt; &lt;p&gt;2.03 GB | nuextract:latest&lt;/p&gt; &lt;p&gt;2.03 GB | nuextract:3.8b&lt;/p&gt; &lt;p&gt;2.03 GB | phi3:latest&lt;/p&gt; &lt;p&gt;2.03 GB | phi3:3.8b&lt;/p&gt; &lt;p&gt;2.03 GB | phi3.5:latest&lt;/p&gt; &lt;p&gt;2.03 GB | phi3.5:3.8b&lt;/p&gt; &lt;p&gt;1.96 GB | granite4:3b&lt;/p&gt; &lt;p&gt;1.92 GB | granite3-moe:3b&lt;/p&gt; &lt;p&gt;1.9 GB | granite3.1-moe:latest&lt;/p&gt; &lt;p&gt;1.9 GB | granite3.1-moe:3b&lt;/p&gt; &lt;p&gt;1.88 GB | hermes3:3b&lt;/p&gt; &lt;p&gt;1.88 GB | llama3.2:latest&lt;/p&gt; &lt;p&gt;1.88 GB | llama3.2:3b&lt;/p&gt; &lt;p&gt;1.87 GB | falcon3:3b&lt;/p&gt; &lt;p&gt;1.86 GB | granite-code:latest&lt;/p&gt; &lt;p&gt;1.86 GB | granite-code:3b&lt;/p&gt; &lt;p&gt;1.84 GB | orca-mini:latest&lt;/p&gt; &lt;p&gt;1.84 GB | orca-mini:3b&lt;/p&gt; &lt;p&gt;1.8 GB | qwen2.5:3b&lt;/p&gt; &lt;p&gt;1.8 GB | qwen2.5-coder:3b&lt;/p&gt; &lt;p&gt;1.76 GB | qwen3-vl:2b&lt;/p&gt; &lt;p&gt;1.71 GB | starcoder:latest&lt;/p&gt; &lt;p&gt;1.71 GB | starcoder:3b&lt;/p&gt; &lt;p&gt;1.7 GB | smollm2:latest&lt;/p&gt; &lt;p&gt;1.7 GB | smollm2:1.7b&lt;/p&gt; &lt;p&gt;1.66 GB | falcon3:1b&lt;/p&gt; &lt;p&gt;1.62 GB | moondream:latest&lt;/p&gt; &lt;p&gt;1.62 GB | moondream:1.8b&lt;/p&gt; &lt;p&gt;1.59 GB | shieldgemma:2b&lt;/p&gt; &lt;p&gt;1.59 GB | starcoder2:latest&lt;/p&gt; &lt;p&gt;1.59 GB | starcoder2:3b&lt;/p&gt; &lt;p&gt;1.56 GB | gemma:2b&lt;/p&gt; &lt;p&gt;1.53 GB | exaone-deep:2.4b&lt;/p&gt; &lt;p&gt;1.53 GB | exaone3.5:2.4b&lt;/p&gt; &lt;p&gt;1.52 GB | gemma2:2b&lt;/p&gt; &lt;p&gt;1.5 GB | stable-code:latest&lt;/p&gt; &lt;p&gt;1.5 GB | stable-code:3b&lt;/p&gt; &lt;p&gt;1.5 GB | stablelm-zephyr:latest&lt;/p&gt; &lt;p&gt;1.5 GB | stablelm-zephyr:3b&lt;/p&gt; &lt;p&gt;1.49 GB | dolphin-phi:latest&lt;/p&gt; &lt;p&gt;1.49 GB | dolphin-phi:2.7b&lt;/p&gt; &lt;p&gt;1.49 GB | granite3-dense:latest&lt;/p&gt; &lt;p&gt;1.49 GB | granite3-dense:2b&lt;/p&gt; &lt;p&gt;1.49 GB | llama-guard3:1b&lt;/p&gt; &lt;p&gt;1.49 GB | phi:latest&lt;/p&gt; &lt;p&gt;1.49 GB | phi:2.7b&lt;/p&gt; &lt;p&gt;1.46 GB | granite3.1-dense:2b&lt;/p&gt; &lt;p&gt;1.44 GB | codegemma:2b&lt;/p&gt; &lt;p&gt;1.44 GB | granite3.2:2b&lt;/p&gt; &lt;p&gt;1.44 GB | granite3.3:2b&lt;/p&gt; &lt;p&gt;1.32 GB | granite3.1-moe:1b&lt;/p&gt; &lt;p&gt;1.32 GB | opencoder:1.5b&lt;/p&gt; &lt;p&gt;1.27 GB | qwen3:1.7b&lt;/p&gt; &lt;p&gt;1.23 GB | llama3.2:1b&lt;/p&gt; &lt;p&gt;1.08 GB | bge-m3:latest&lt;/p&gt; &lt;p&gt;1.08 GB | snowflake-arctic-embed2:latest&lt;/p&gt; &lt;p&gt;1.04 GB | deepcoder:1.5b&lt;/p&gt; &lt;p&gt;1.04 GB | deepseek-r1:1.5b&lt;/p&gt; &lt;p&gt;1.04 GB | internlm2:1.8b&lt;/p&gt; &lt;p&gt;1.04 GB | qwen:1.8b&lt;/p&gt; &lt;p&gt;0.98 GB | sailor2:1b&lt;/p&gt; &lt;p&gt;0.92 GB | qwen2.5:1.5b&lt;/p&gt; &lt;p&gt;0.92 GB | qwen2.5-coder:1.5b&lt;/p&gt; &lt;p&gt;0.92 GB | smollm:latest&lt;/p&gt; &lt;p&gt;0.92 GB | smollm:1.7b&lt;/p&gt; &lt;p&gt;0.92 GB | stablelm2:latest&lt;/p&gt; &lt;p&gt;0.92 GB | stablelm2:1.6b&lt;/p&gt; &lt;p&gt;0.87 GB | qwen2:1.5b&lt;/p&gt; &lt;p&gt;0.87 GB | qwen2-math:1.5b&lt;/p&gt; &lt;p&gt;0.87 GB | reader-lm:latest&lt;/p&gt; &lt;p&gt;0.87 GB | reader-lm:1.5b&lt;/p&gt; &lt;p&gt;0.81 GB | yi-coder:1.5b&lt;/p&gt; &lt;p&gt;0.77 GB | granite3-moe:latest&lt;/p&gt; &lt;p&gt;0.77 GB | granite3-moe:1b&lt;/p&gt; &lt;p&gt;0.76 GB | gemma3:1b&lt;/p&gt; &lt;p&gt;0.72 GB | deepseek-coder:latest&lt;/p&gt; &lt;p&gt;0.72 GB | deepseek-coder:1.3b&lt;/p&gt; &lt;p&gt;0.68 GB | lfm2.5-thinking:latest&lt;/p&gt; &lt;p&gt;0.68 GB | lfm2.5-thinking:1.2b&lt;/p&gt; &lt;p&gt;0.68 GB | starcoder:1b&lt;/p&gt; &lt;p&gt;0.62 GB | bge-large:latest&lt;/p&gt; &lt;p&gt;0.62 GB | mxbai-embed-large:latest&lt;/p&gt; &lt;p&gt;0.62 GB | snowflake-arctic-embed:latest&lt;/p&gt; &lt;p&gt;0.6 GB | qwen3-embedding:0.6b&lt;/p&gt; &lt;p&gt;0.59 GB | tinydolphin:latest&lt;/p&gt; &lt;p&gt;0.59 GB | tinydolphin:1.1b&lt;/p&gt; &lt;p&gt;0.59 GB | tinyllama:latest&lt;/p&gt; &lt;p&gt;0.59 GB | tinyllama:1.1b&lt;/p&gt; &lt;p&gt;0.58 GB | embeddinggemma:latest&lt;/p&gt; &lt;p&gt;0.52 GB | paraphrase-multilingual:latest&lt;/p&gt; &lt;p&gt;0.49 GB | qwen3:0.6b&lt;/p&gt; &lt;p&gt;0.37 GB | qwen:0.5b&lt;/p&gt; &lt;p&gt;0.37 GB | qwen2.5:0.5b&lt;/p&gt; &lt;p&gt;0.37 GB | qwen2.5-coder:0.5b&lt;/p&gt; &lt;p&gt;0.33 GB | qwen2:0.5b&lt;/p&gt; &lt;p&gt;0.33 GB | reader-lm:0.5b&lt;/p&gt; &lt;p&gt;0.28 GB | functiongemma:latest&lt;/p&gt; &lt;p&gt;0.26 GB | nomic-embed-text:latest&lt;/p&gt; &lt;p&gt;0.06 GB | granite-embedding:latest&lt;/p&gt; &lt;p&gt;0.04 GB | all-minilm:latest&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventurousLion9548"&gt; /u/AdventurousLion9548 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qm9cgp/ollama_models_ranked_by_vram_requirements/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-25T04:36:31+00:00</published>
  </entry>
</feed>
