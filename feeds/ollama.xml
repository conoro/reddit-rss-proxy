<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-18T22:30:17+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1r79ktf</id>
    <title>AI generated text detection model needed</title>
    <updated>2026-02-17T15:48:56+00:00</updated>
    <author>
      <name>/u/szutcxzh</name>
      <uri>https://old.reddit.com/user/szutcxzh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per subject, looking for this. a model that can take text input and determine a percentage whether it was AI or human generated. Ty!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/szutcxzh"&gt; /u/szutcxzh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r79ktf/ai_generated_text_detection_model_needed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r79ktf/ai_generated_text_detection_model_needed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r79ktf/ai_generated_text_detection_model_needed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T15:48:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r71lx5</id>
    <title>CodeSolver Pro - Browser extension for ollama</title>
    <updated>2026-02-17T09:33:32+00:00</updated>
    <author>
      <name>/u/Fun-Zookeepergame700</name>
      <uri>https://old.reddit.com/user/Fun-Zookeepergame700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just built CodeSolver Pro ‚Äì a browser extension that automatically detects coding problems from LeetCode, HackerRank, and other platforms, then uses local AI running entirely on your machine to generate complete solutions with approach explanations, time complexity analysis, and code. Your problems never leave your computer ‚Äì no cloud API calls, no privacy concerns, works offline. It runs in a side panel for seamless workflow, supports Ollama and LM Studio, and includes focus protection for platforms that detect extensions. Free, open-source, Chrome/Firefox. Would love feedback from fellow devs who value privacy! &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/sourjatilak/CodeSolverPro"&gt;https://github.com/sourjatilak/CodeSolverPro&lt;/a&gt;&lt;br /&gt; Youtube: &lt;a href="https://www.youtube.com/watch?v=QX0T8DcmDpw"&gt;https://www.youtube.com/watch?v=QX0T8DcmDpw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Zookeepergame700"&gt; /u/Fun-Zookeepergame700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r71lx5/codesolver_pro_browser_extension_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r71lx5/codesolver_pro_browser_extension_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r71lx5/codesolver_pro_browser_extension_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T09:33:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7axk3</id>
    <title>Stop guessing which AI model your GPU can handle</title>
    <updated>2026-02-17T16:33:31+00:00</updated>
    <author>
      <name>/u/Soul__Reaper_</name>
      <uri>https://old.reddit.com/user/Soul__Reaper_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a small comparison tool for one simple reason:&lt;/p&gt; &lt;p&gt;Every time I wanted to try a new model, I had to ask:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Can my GPU even run this?&lt;/li&gt; &lt;li&gt;Do I need 4-bit quantization?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So instead of checking random Reddit threads and Hugging Face comments, I made a tool where you can:&lt;/p&gt; &lt;p&gt;‚Ä¢ Compare model sizes&lt;br /&gt; ‚Ä¢ See estimated VRAM requirements&lt;br /&gt; ‚Ä¢ Roughly understand what changes when you quantize&lt;/p&gt; &lt;p&gt;Just a practical comparison layer to answer:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚ÄúCan my hardware actually handle this model?‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Try It and let me know: &lt;a href="https://umer-farooq230.github.io/Can-My-GPU-Run-It/"&gt;https://umer-farooq230.github.io/Can-My-GPU-Run-It/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still improving it. Open to suggestions on what would make it more useful. Or if you guys think I should scale it with more GPUs, models and more in-depth hardware/software details&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Soul__Reaper_"&gt; /u/Soul__Reaper_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7axk3/stop_guessing_which_ai_model_your_gpu_can_handle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7axk3/stop_guessing_which_ai_model_your_gpu_can_handle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r7axk3/stop_guessing_which_ai_model_your_gpu_can_handle/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T16:33:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r71km5</id>
    <title>Hey everyone! Ollama llm-checker has been growing and i need help</title>
    <updated>2026-02-17T09:31:12+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r71km5/hey_everyone_ollama_llmchecker_has_been_growing/"&gt; &lt;img alt="Hey everyone! Ollama llm-checker has been growing and i need help" src="https://preview.redd.it/l6lzywpqx0kg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06ad9e939f017f018c335c60461294a517db62cd" title="Hey everyone! Ollama llm-checker has been growing and i need help" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;I created a Discord server to build it together with the community. Your feedback and ideas matter the most whether it's new&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;model&lt;/em&gt; &lt;em&gt;support,&lt;/em&gt; &lt;em&gt;hardware&lt;/em&gt; &lt;em&gt;detection,&lt;/em&gt; &lt;em&gt;scoring&lt;/em&gt; &lt;em&gt;improvements,&lt;/em&gt; &lt;em&gt;or&lt;/em&gt; &lt;em&gt;anything&lt;/em&gt; &lt;em&gt;else.&lt;/em&gt; &lt;em&gt;Come&lt;/em&gt; &lt;em&gt;join&lt;/em&gt; &lt;em&gt;and&lt;/em&gt; &lt;em&gt;help&lt;/em&gt; &lt;em&gt;shape&lt;/em&gt; &lt;em&gt;the&lt;/em&gt; &lt;em&gt;tool:&lt;/em&gt; &lt;a href="https://discord.com/invite/mnmYrA7T"&gt;&lt;em&gt;https://discord.com/invite/mnmYrA7T&lt;/em&gt;&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l6lzywpqx0kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r71km5/hey_everyone_ollama_llmchecker_has_been_growing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r71km5/hey_everyone_ollama_llmchecker_has_been_growing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T09:31:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6tj0j</id>
    <title>Lots of pain, finally a small breakthrough, is it enough? Sharing what I've done</title>
    <updated>2026-02-17T02:17:04+00:00</updated>
    <author>
      <name>/u/Minimum-Two-8093</name>
      <uri>https://old.reddit.com/user/Minimum-Two-8093</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r6tj0j/lots_of_pain_finally_a_small_breakthrough_is_it/"&gt; &lt;img alt="Lots of pain, finally a small breakthrough, is it enough? Sharing what I've done" src="https://preview.redd.it/jgt5b0hmnyjg1.png?width=140&amp;amp;height=81&amp;amp;auto=webp&amp;amp;s=c0ee1c4b62e6541654901aef808a587bc5986976" title="Lots of pain, finally a small breakthrough, is it enough? Sharing what I've done" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not overly knowledgeable about self-hosting models, but I do have a software engineering background and time on my hands to try to make it work, as well as an employer that's encouraging us to figure shit out (read: I'm being paid to do this).&lt;/p&gt; &lt;p&gt;Comparing cloud agents with local agents is never a good idea, but that's been my only frame of reference. I have an absolutely huge solution for an economic simulation that I've been building, recently in conjunction with cloud agents.&lt;/p&gt; &lt;p&gt;Up until now, I've been using Chat-GPT (web) as a designer and prompt engineer, and Opus 4.5 as a coding model through Claude Code. I've been treating this approach as if I'm the architect, and the agents are my junior to middling developers. This has been working very well.&lt;/p&gt; &lt;p&gt;But, I've wanted to find a use-case where my local machine is used for some of the work - I'm sick of paying for cloud agents, running out of quota continually, and sharing my information freely.&lt;/p&gt; &lt;p&gt;For the past 3 weeks, I've been struggling with Ollama hosted models, trying to find the right use-cases. I have an RTX4090 and have been dancing between Qwen, GPT-OSS, and DeepSeek derivatives of Qwen. I have docker running on an old 1U server in my garage, currently only hosting Open WebUI, this is exposing Ollama hosted models to all of my devices via Tailscale.&lt;/p&gt; &lt;p&gt;I'm using &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; in VS Code.&lt;/p&gt; &lt;p&gt;That's the background, now the problem statement:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;When trying to use my self-hosted &amp;quot;agents&amp;quot; for coding tasks, nothing &lt;em&gt;felt&lt;/em&gt; good. Attempting to edit files and failing to reference them properly just felt like &lt;em&gt;friction&lt;/em&gt;. Unintended files ended up overwritten.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I have the following folder structure for Continue to load when VS Code is opened:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;./.continue/rules/&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And inside that folder are the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;00-path-grounding .md&lt;/li&gt; &lt;li&gt;00-project-context .md&lt;/li&gt; &lt;li&gt;08-path-grounding-hard-stop .md&lt;/li&gt; &lt;li&gt;08b-no-fake-tools .md&lt;/li&gt; &lt;li&gt;09-repomap-maintenance .md&lt;/li&gt; &lt;li&gt;10-determinism .md&lt;/li&gt; &lt;li&gt;11-simulation-boundaries .md&lt;/li&gt; &lt;li&gt;12-contract-invariants .md&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've also got another repo with my project agnostic rules files and a script which copies them into the same &lt;em&gt;rules&lt;/em&gt; folder. This is so that I can keep all of my projects consistent with each other if they're using my local models.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;shared-01-general .md&lt;/li&gt; &lt;li&gt;shared-02-safe-edits .md&lt;/li&gt; &lt;li&gt;shared-03-tests-first .md&lt;/li&gt; &lt;li&gt;shared-04-diff-discipline .md&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This has been hit and miss, mainly because tool usage has also been hit and miss. What's improved this however are the built-in custom providers for Continue.&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.continue.dev/customize/deep-dives/custom-providers#built-in-context-providers"&gt;https://docs.continue.dev/customize/deep-dives/custom-providers#built-in-context-providers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's the Continue config.yaml file I've settled on, including the providers I've chosen:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// config.yaml name: Local Config version: 1.0.0 schema: v1 context: - provider: file - provider: code - provider: open params: onlyPinned: true - provider: clipboard - provider: tree - provider: repo-map params: includeSignatures: false # default true models: - name: GPT-OSS Chat provider: ollama apiBase: http://localhost:11434 model: gpt-oss:20b roles: - chat - name: Qwen3 Coder provider: ollama apiBase: http://localhost:11434 model: qwen3-coder:30b roles: - chat - edit - apply - name: Qwen2.5 Autocomplete provider: ollama apiBase: http://localhost:11434 model: qwen2.5-coder:1.5b-base roles: - autocomplete - name: Nomic Embed provider: ollama apiBase: http://localhost:11434 model: nomic-embed-text:latest roles: - embed &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The end result seems to be that I have finally settled on something &lt;em&gt;kinda&lt;/em&gt; useful.&lt;/p&gt; &lt;p&gt;This may look simple (it is), but it's the first scoped refactor of an existing piece of code where the agent hasn't screwed &lt;em&gt;something&lt;/em&gt; up.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jgt5b0hmnyjg1.png?width=2728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=25e57ca54e0d0d35116c47df872e7d7dc7f1e18a"&gt;https://preview.redd.it/jgt5b0hmnyjg1.png?width=2728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=25e57ca54e0d0d35116c47df872e7d7dc7f1e18a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As mentioned, this code base is quite large and set to get bigger.&lt;/p&gt; &lt;p&gt;I've been used to cloud agents achieving quite amazing things while significantly boosting throughput (at least 20x what I'm capable of, probably more).&lt;/p&gt; &lt;p&gt;I'd wanted to have my local models do the same, but in reality that was completely unrealistic; a 4090 cannot compete with cloud inference.&lt;/p&gt; &lt;p&gt;Where I &lt;em&gt;think&lt;/em&gt; I've settled now (especially since a significant portion of my simulation is complete) is that my local agents can probably help to augment me more now that I am moving onto front-end implementation. I think that if I can constrain myself to only expect the local agents to help me with &lt;em&gt;the boring shit&lt;/em&gt; like boilerplate and mass data entry, I will still save myself significant amounts of time; e.g. that edit would have taken me a couple of minutes of data entry, the prompt took a few seconds to write, and a few seconds to execute. I think it's likely that I'll keep using cloud agents for gnarly work, and local agents for the simpler things. That's not bad.&lt;/p&gt; &lt;p&gt;Perhaps that's the sweet-spot.&lt;/p&gt; &lt;p&gt;I don't really know what I want to get from this post, perhaps just to start a conversation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Are you working on large projects with locally hosted models?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How has your experience been?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If I'm missing anything obvious, let me know.&lt;/p&gt; &lt;p&gt;Edit: The markdown filenames attempted to be clickable links, edited to add a space to stop it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Minimum-Two-8093"&gt; /u/Minimum-Two-8093 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6tj0j/lots_of_pain_finally_a_small_breakthrough_is_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r6tj0j/lots_of_pain_finally_a_small_breakthrough_is_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r6tj0j/lots_of_pain_finally_a_small_breakthrough_is_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T02:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7iyzg</id>
    <title>Ollama Glass - Inference in SVG</title>
    <updated>2026-02-17T21:13:48+00:00</updated>
    <author>
      <name>/u/Worldly_Evidence9113</name>
      <uri>https://old.reddit.com/user/Worldly_Evidence9113</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r7iyzg/ollama_glass_inference_in_svg/"&gt; &lt;img alt="Ollama Glass - Inference in SVG" src="https://preview.redd.it/lqno9y03f4kg1.png?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=66e585c78681f5be598ad4311768e1619c185b11" title="Ollama Glass - Inference in SVG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lqno9y03f4kg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7dbb839f4ca88dd9c9429260a7cd7297a6c9f6fb"&gt;https://preview.redd.it/lqno9y03f4kg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7dbb839f4ca88dd9c9429260a7cd7297a6c9f6fb&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;svg width=&amp;quot;600&amp;quot; height=&amp;quot;500&amp;quot; viewBox=&amp;quot;0 0 600 500&amp;quot; xmlns=&amp;quot;http://www.w3.org/2000/svg&amp;quot;&amp;gt; &amp;lt;rect width=&amp;quot;600&amp;quot; height=&amp;quot;500&amp;quot; rx=&amp;quot;15&amp;quot; fill=&amp;quot;#18181b&amp;quot; /&amp;gt; &amp;lt;rect width=&amp;quot;600&amp;quot; height=&amp;quot;60&amp;quot; rx=&amp;quot;15&amp;quot; fill=&amp;quot;#27272a&amp;quot; /&amp;gt; &amp;lt;text x=&amp;quot;20&amp;quot; y=&amp;quot;38&amp;quot; font-family=&amp;quot;sans-serif&amp;quot; font-size=&amp;quot;18&amp;quot; font-weight=&amp;quot;bold&amp;quot; fill=&amp;quot;#f4f4f5&amp;quot;&amp;gt;ü¶ô Ollama SVG Interface&amp;lt;/text&amp;gt; &amp;lt;circle cx=&amp;quot;560&amp;quot; cy=&amp;quot;30&amp;quot; r=&amp;quot;6&amp;quot; fill=&amp;quot;#22c55e&amp;quot; /&amp;gt; &amp;lt;foreignObject x=&amp;quot;20&amp;quot; y=&amp;quot;80&amp;quot; width=&amp;quot;560&amp;quot; height=&amp;quot;320&amp;quot;&amp;gt; &amp;lt;div xmlns=&amp;quot;http://www.w3.org/1999/xhtml&amp;quot; style=&amp;quot;height: 100%; color: #d4d4d8; font-family: sans-serif; font-size: 14px; overflow-y: auto; display: flex; flex-direction: column; gap: 10px;&amp;quot; id=&amp;quot;chat-container&amp;quot;&amp;gt; &amp;lt;div style=&amp;quot;background: #3f3f46; padding: 10px; border-radius: 8px; align-self: flex-start; max-width: 80%;&amp;quot;&amp;gt; System: Ready. Make sure Ollama is running locally. &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/foreignObject&amp;gt; &amp;lt;rect x=&amp;quot;20&amp;quot; y=&amp;quot;420&amp;quot; width=&amp;quot;560&amp;quot; height=&amp;quot;60&amp;quot; rx=&amp;quot;30&amp;quot; fill=&amp;quot;#27272a&amp;quot; stroke=&amp;quot;#3f3f46&amp;quot; stroke-width=&amp;quot;2&amp;quot; /&amp;gt; &amp;lt;foreignObject x=&amp;quot;40&amp;quot; y=&amp;quot;430&amp;quot; width=&amp;quot;460&amp;quot; height=&amp;quot;40&amp;quot;&amp;gt; &amp;lt;input xmlns=&amp;quot;http://www.w3.org/1999/xhtml&amp;quot; type=&amp;quot;text&amp;quot; id=&amp;quot;user-input&amp;quot; placeholder=&amp;quot;Ask Llama 3...&amp;quot; style=&amp;quot;width: 100%; height: 35px; background: transparent; border: none; color: white; outline: none; font-size: 14px;&amp;quot; /&amp;gt; &amp;lt;/foreignObject&amp;gt; &amp;lt;g id=&amp;quot;send-btn&amp;quot; cursor=&amp;quot;pointer&amp;quot; onclick=&amp;quot;askOllama()&amp;quot;&amp;gt; &amp;lt;circle cx=&amp;quot;550&amp;quot; cy=&amp;quot;450&amp;quot; r=&amp;quot;20&amp;quot; fill=&amp;quot;#3b82f6&amp;quot; /&amp;gt; &amp;lt;path d=&amp;quot;M545 442 L558 450 L545 458&amp;quot; stroke=&amp;quot;white&amp;quot; stroke-width=&amp;quot;2&amp;quot; fill=&amp;quot;none&amp;quot; /&amp;gt; &amp;lt;/g&amp;gt; &amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt; async function askOllama() { const inputField = document.getElementById('user-input'); const chatContainer = document.getElementById('chat-container'); const prompt = inputField.value; if (!prompt) return; // Add User Message to UI const userMsg = document.createElement('div'); userMsg.style = &amp;quot;background: #1d4ed8; padding: 10px; border-radius: 8px; align-self: flex-end; max-width: 80%; color: white;&amp;quot;; userMsg.textContent = prompt; chatContainer.appendChild(userMsg); inputField.value = ''; // Prepare AI Message placeholder const aiMsg = document.createElement('div'); aiMsg.style = &amp;quot;background: #3f3f46; padding: 10px; border-radius: 8px; align-self: flex-start; max-width: 80%;&amp;quot;; aiMsg.textContent = &amp;quot;...&amp;quot;; chatContainer.appendChild(aiMsg); try { const response = await fetch('http://localhost:11434/api/generate', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ model: 'hf.co/Edge-Quant/Nanbeige4.1-3B-Q4_K_M-GGUF:latest', // Change this to your preferred model prompt: prompt, stream: false }) }); const data = await response.json(); aiMsg.textContent = data.response; } catch (err) { aiMsg.textContent = &amp;quot;Error: Could not connect to Ollama. Check CORS settings.&amp;quot;; aiMsg.style.color = &amp;quot;#ef4444&amp;quot;; } chatContainer.scrollTop = chatContainer.scrollHeight; } // Allow &amp;quot;Enter&amp;quot; key to send document.getElementById('user-input').addEventListener('keypress', (e) =&amp;gt; { if (e.key === 'Enter') askOllama(); }); &amp;lt;/script&amp;gt; &amp;lt;/svg&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worldly_Evidence9113"&gt; /u/Worldly_Evidence9113 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7iyzg/ollama_glass_inference_in_svg/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7iyzg/ollama_glass_inference_in_svg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r7iyzg/ollama_glass_inference_in_svg/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T21:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7km00</id>
    <title>Recommendations for uncensored open source models for cybersecurity research</title>
    <updated>2026-02-17T22:15:11+00:00</updated>
    <author>
      <name>/u/dumbelco</name>
      <uri>https://old.reddit.com/user/dumbelco</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dumbelco"&gt; /u/dumbelco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1r7jcrp/recommendations_for_uncensored_open_source_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7km00/recommendations_for_uncensored_open_source_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r7km00/recommendations_for_uncensored_open_source_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T22:15:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7ghg3</id>
    <title>Open source LLM for detecting AI generated content</title>
    <updated>2026-02-17T19:43:48+00:00</updated>
    <author>
      <name>/u/szutcxzh</name>
      <uri>https://old.reddit.com/user/szutcxzh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I'm looking for any open source LLM's (that might work in Ollama) that can take a text input and determine if it was created by AI or a human.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/szutcxzh"&gt; /u/szutcxzh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7ghg3/open_source_llm_for_detecting_ai_generated_content/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7ghg3/open_source_llm_for_detecting_ai_generated_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r7ghg3/open_source_llm_for_detecting_ai_generated_content/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T19:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bdpv</id>
    <title>Create Apps with Claude Code on Ollama</title>
    <updated>2026-02-17T16:48:12+00:00</updated>
    <author>
      <name>/u/piotr_minkowski</name>
      <uri>https://old.reddit.com/user/piotr_minkowski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r7bdpv/create_apps_with_claude_code_on_ollama/"&gt; &lt;img alt="Create Apps with Claude Code on Ollama" src="https://external-preview.redd.it/pNxhJpK9phX4KGulGxiylw8iyaLCg7jyze6z43SCkPI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b6f53c45d17adcc101eb6aebf85c6fb851cde16" title="Create Apps with Claude Code on Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/piotr_minkowski"&gt; /u/piotr_minkowski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://piotrminkowski.com/2026/02/17/create-apps-with-claude-code-on-ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7bdpv/create_apps_with_claude_code_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r7bdpv/create_apps_with_claude_code_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T16:48:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r77o7d</id>
    <title>REASONING AUGMENTED RETRIEVAL (RAR) is the production-grade successor to single-pass RAG.</title>
    <updated>2026-02-17T14:37:05+00:00</updated>
    <author>
      <name>/u/frank_brsrk</name>
      <uri>https://old.reddit.com/user/frank_brsrk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Single-pass rag retrieves once and hopes the model stitches fragments into coherent reasoning. It fails on multi-hop questions, contradictions, temporal dependencies, or cases needing follow-up fetches.Rar puts reasoning first. The system decomposes the problem, identifies gaps, issues precise (often multiple, reformulated, or negated) retrievals.&lt;br /&gt; integrates results into an ongoing chain-of-thought, discards noise or conflicts, and loops until the logic closes with high confidence.&lt;/p&gt; &lt;p&gt;Measured gains in production:&lt;/p&gt; &lt;p&gt;-35‚Äì60% accuracy lift on multi-hop, regulatory, and long-document tasks&lt;br /&gt; -far fewer confident-but-wrong answers&lt;br /&gt; -built-in uncertainty detection and gap admission&lt;br /&gt; -traceable retrieval decisions&lt;/p&gt; &lt;p&gt;Training data must include:&lt;br /&gt; -interleaved reasoning + retrieval + reflection traces&lt;br /&gt; -negative examples forcing rejection of misleading chunks&lt;br /&gt; -synthetic trajectories with hidden multi-hop needs&lt;br /&gt; -confidence rules that trigger extra cycles&lt;/p&gt; &lt;p&gt;Rar turns retrieval into an active part of thinking instead of a one time lookup. Systems still using single pass dense retrieval in 2026 accept unnecessary limits on depth, reliability, and explainability. RAR is the necessary direction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frank_brsrk"&gt; /u/frank_brsrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r77o7d/reasoning_augmented_retrieval_rar_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r77o7d/reasoning_augmented_retrieval_rar_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r77o7d/reasoning_augmented_retrieval_rar_is_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T14:37:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7cfaz</id>
    <title>I built a beginner‚Äëfriendly PowerShell installer for custom GGUF models in Ollama (auto‚Äëdetects GGUF, builds Modelfile, no file moving needed</title>
    <updated>2026-02-17T17:23:14+00:00</updated>
    <author>
      <name>/u/lAVENTUSl</name>
      <uri>https://old.reddit.com/user/lAVENTUSl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I kept seeing people struggle with installing custom GGUF models into Ollama on Windows, so I built a &lt;strong&gt;simple, color‚Äëcoded PowerShell installer&lt;/strong&gt; that handles the whole process automatically.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run the script from &lt;em&gt;any&lt;/em&gt; folder&lt;/li&gt; &lt;li&gt;Auto‚Äëdetects your GGUF file&lt;/li&gt; &lt;li&gt;Creates the model folder for you&lt;/li&gt; &lt;li&gt;Generates a valid Modelfile&lt;/li&gt; &lt;li&gt;Registers the model with Ollama&lt;/li&gt; &lt;li&gt;Optional: run the model immediately&lt;/li&gt; &lt;li&gt;Beginner‚Äëfriendly, clean output, no guesswork&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also added a fix + explanation for the common PowerShell error:&lt;br /&gt; &lt;strong&gt;‚Äúrunning scripts is disabled on this system‚Äù&lt;/strong&gt;&lt;br /&gt; so new users don‚Äôt get stuck.&lt;/p&gt; &lt;p&gt;GitHub repo:&lt;br /&gt; &lt;a href="https://github.com/Glenn762702/ollama-model-installer"&gt;&lt;strong&gt;Ollama-model-installer&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you try it, I‚Äôd love feedback or suggestions for new features.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lAVENTUSl"&gt; /u/lAVENTUSl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7cfaz/i_built_a_beginnerfriendly_powershell_installer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7cfaz/i_built_a_beginnerfriendly_powershell_installer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r7cfaz/i_built_a_beginnerfriendly_powershell_installer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T17:23:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7k0mp</id>
    <title>Threads or Cores? Will Ollama perform better using one over the other?</title>
    <updated>2026-02-17T21:52:17+00:00</updated>
    <author>
      <name>/u/firestorm_v1</name>
      <uri>https://old.reddit.com/user/firestorm_v1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I decided to go down the rabbit hole of running ollama and local LLMs and while I don't have a GPU, I do have some pretty good servers with a lot of RAM and decent high core processors. I am running the glm-4.7-flash model which appears to work decently. I'm running dual E5-2620 chips (8 cores * 2 sockets) which comes out to 32 threads. Some applications work better using cores, others using threads.&lt;/p&gt; &lt;p&gt;Does ollama (and the running LLM) work better on threads versus real CPU cores? I tried searching online but didn't come across any specific info either way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/firestorm_v1"&gt; /u/firestorm_v1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7k0mp/threads_or_cores_will_ollama_perform_better_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7k0mp/threads_or_cores_will_ollama_perform_better_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r7k0mp/threads_or_cores_will_ollama_perform_better_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T21:52:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7c951</id>
    <title>Is a AI HX370 with 96GB Laptop Good enough for LLMs?</title>
    <updated>2026-02-17T17:17:24+00:00</updated>
    <author>
      <name>/u/PersonSuitTV</name>
      <uri>https://old.reddit.com/user/PersonSuitTV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking to get a laptop and was wondering if one with an AMD Ryzen AI 9 HX PRO 370 and 96GB of unified memory would be good for LLMs. And I mean in the sense of will this Laptop perform well enough, or is that CPU just not going to cut it for running a LLM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PersonSuitTV"&gt; /u/PersonSuitTV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7c951/is_a_ai_hx370_with_96gb_laptop_good_enough_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7c951/is_a_ai_hx370_with_96gb_laptop_good_enough_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r7c951/is_a_ai_hx370_with_96gb_laptop_good_enough_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T17:17:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7wr06</id>
    <title>New feature ¬´Subagents¬ª for cloud models released, but will this feature be available for local llms?</title>
    <updated>2026-02-18T07:40:14+00:00</updated>
    <author>
      <name>/u/jerrygreenest1</name>
      <uri>https://old.reddit.com/user/jerrygreenest1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôd love to run subagents locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jerrygreenest1"&gt; /u/jerrygreenest1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7wr06/new_feature_subagents_for_cloud_models_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7wr06/new_feature_subagents_for_cloud_models_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r7wr06/new_feature_subagents_for_cloud_models_released/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T07:40:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r72my6</id>
    <title>My experience with running small scale open source models on my own PC.</title>
    <updated>2026-02-17T10:36:47+00:00</updated>
    <author>
      <name>/u/Dibru9109_4259</name>
      <uri>https://old.reddit.com/user/Dibru9109_4259</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got exposed to &lt;strong&gt;Ollama&lt;/strong&gt; and the realization that I could take the 2 Billion 3 Billion parameter models and run them locally in my small pc with limited capacity of &lt;strong&gt;8 GB RAM&lt;/strong&gt; and just an &lt;strong&gt;Intel i3&lt;/strong&gt; CPU and without any GPU made me so excited and amazed. &lt;/p&gt; &lt;p&gt;Though the experience of running such Billions parameter models with 2-5 GB RAM consumption was not a smooth experience. Firstly I run the &amp;quot;&lt;strong&gt;Mistral 7B&lt;/strong&gt;&amp;quot; model in my ollama. The response was well structured and the reasoning was good but given the limitations of my hardwares, it took about &lt;strong&gt;3-4 minutes&lt;/strong&gt; in generating every response.&lt;/p&gt; &lt;p&gt;For a smoother expereience, I decided to run a smaller model. I choose Microsoft's &lt;strong&gt;phi3:mini&lt;/strong&gt; model which was trained on around &lt;strong&gt;3.8 Billion&lt;/strong&gt; parameters. The experience with this model was quite smoother compared to the pervious Minstral 7B model. phi3:mini took about 7-8 secods for the cold start and once it was started, it was generating responses with &lt;strong&gt;less than 0.5 seconds&lt;/strong&gt; of prompting. I tried to measure the token generating speed using my phone's stopwatch and the number of words generated by the model (NOTE: &lt;strong&gt;1 token = 0.75 word&lt;/strong&gt;, on average). I found out that this model was generating 7.5 tokens per second on my PC. The experience was pretty smooth with such a speed and it was also able to do all kinds of basic chat and reasoning.&lt;/p&gt; &lt;p&gt;After this I decided to test the limits so I downloaded two even smaller models - &lt;strong&gt;tinyLLama&lt;/strong&gt;. While the model was much compact with just &lt;strong&gt;1.1 Billion&lt;/strong&gt; parameters and just 0.67GB download size for the &lt;strong&gt;4-bit (Q4_K_M) version&lt;/strong&gt;, its performance deteriorated sharply.&lt;/p&gt; &lt;p&gt;When I first gave a simple Hi to this model it responded with a random unrelated texts about &amp;quot;nothingness&amp;quot; and the paradox of nothingness. I tried to make it talk to me but it kept elaborating in its own cilo about the great philosophies around the concept of nothingness thereby not responding to whatever prompt I gave to it. Afterwards I also tried my hand at the &lt;strong&gt;smoLlm&lt;/strong&gt; and this one also hallucinated massively.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Conclusion :&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My &lt;em&gt;hardware&lt;/em&gt; capacity affected the &lt;em&gt;speed&lt;/em&gt; of Token generated by the different models. While the 7B parameter Mistral model took several minutes to respond each time, &lt;em&gt;this problem was eliminated entirely once I went 3.8 Billion parameters and less.&lt;/em&gt; All of the phi3:mini and even the ones that hallucinated heavily - smolLm and tinyLlama generated tokens instantly.&lt;/p&gt; &lt;p&gt;The &lt;em&gt;number of parameters determines the extent of intelligence&lt;/em&gt; of the LLM. Going below the 3.8 Billion parameter phi3:mini f, all the tiny models hallucinated excessively even though they were generating those rubbish responses very quickly and almost instantly.&lt;/p&gt; &lt;p&gt;There was &lt;em&gt;a tradeoff between&lt;/em&gt; &lt;strong&gt;&lt;em&gt;speed&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;and&lt;/em&gt; &lt;strong&gt;&lt;em&gt;accuracy.&lt;/em&gt;&lt;/strong&gt; Given the limited hardware capacity of my pc, going below 3.8 Billion parameter model gave instant speed but extremely bad accuracy while going above it gave slow speed but higher accuracy.&lt;/p&gt; &lt;p&gt;So this was my experience about experimenting with Edge AI and various open source models. &lt;strong&gt;Please feel free to correct me whereever you think I might be wrong. Questions are absolutely welcome!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dibru9109_4259"&gt; /u/Dibru9109_4259 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r72my6/my_experience_with_running_small_scale_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r72my6/my_experience_with_running_small_scale_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r72my6/my_experience_with_running_small_scale_open/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T10:36:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1r735eo</id>
    <title>Self Hosted Alternative to NotebookLM</title>
    <updated>2026-02-17T11:06:53+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, SurfSense is an open-source alternative to NotebookLM, Perplexity, and Glean.&lt;/p&gt; &lt;p&gt;It connects any LLM to your internal knowledge sources, then lets teams chat, comment, and collaborate in real time. Think of it as a team-first research workspace with citations, connectors, and agentic workflows.&lt;/p&gt; &lt;p&gt;I‚Äôm looking for contributors. If you‚Äôre into AI agents, RAG, search, browser extensions, or open-source research tooling, would love your help.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Self-hostable (Docker)&lt;/li&gt; &lt;li&gt;25+ external connectors (search engines, Drive, Slack, Teams, Jira, Notion, GitHub, Discord, and more)&lt;/li&gt; &lt;li&gt;Realtime Group Chats&lt;/li&gt; &lt;li&gt;Hybrid retrieval (semantic + full-text) with cited answers&lt;/li&gt; &lt;li&gt;Deep agent architecture (planning + subagents + filesystem access)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs and 6000+ embedding models (via OpenAI-compatible APIs + LiteLLM)&lt;/li&gt; &lt;li&gt;50+ file formats (including Docling/local parsing options)&lt;/li&gt; &lt;li&gt;Podcast generation (multiple TTS providers)&lt;/li&gt; &lt;li&gt;Cross-browser extension to save dynamic/authenticated web pages&lt;/li&gt; &lt;li&gt;RBAC roles for teams&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Slide creation support&lt;/li&gt; &lt;li&gt;Multilingual podcast support&lt;/li&gt; &lt;li&gt;Video creation agent&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r735eo/self_hosted_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r735eo/self_hosted_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r735eo/self_hosted_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-17T11:06:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r84oeh</id>
    <title>"Cognitive Steering" Instructions for Agentic RAG</title>
    <updated>2026-02-18T14:37:51+00:00</updated>
    <author>
      <name>/u/frank_brsrk</name>
      <uri>https://old.reddit.com/user/frank_brsrk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r84oeh/cognitive_steering_instructions_for_agentic_rag/"&gt; &lt;img alt="&amp;quot;Cognitive Steering&amp;quot; Instructions for Agentic RAG" src="https://external-preview.redd.it/HXb-D9eU2P5vIXUt6gq3lvWTQEJlvBhE48-2XXbWXhI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c69018b0d010ba9192b19df33508bcb8979f250" title="&amp;quot;Cognitive Steering&amp;quot; Instructions for Agentic RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frank_brsrk"&gt; /u/frank_brsrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/u_frank_brsrk/comments/1r8401f/cognitive_steering_instructions_for_agentic_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r84oeh/cognitive_steering_instructions_for_agentic_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r84oeh/cognitive_steering_instructions_for_agentic_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T14:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7tga7</id>
    <title>Intelligent (local + cloud) routing for OpenClaw via Plano</title>
    <updated>2026-02-18T04:36:05+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r7tga7/intelligent_local_cloud_routing_for_openclaw_via/"&gt; &lt;img alt="Intelligent (local + cloud) routing for OpenClaw via Plano" src="https://preview.redd.it/g9cwxqtwl6kg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5201cb2344d316a29a063feea66b959fcfdbfa03" title="Intelligent (local + cloud) routing for OpenClaw via Plano" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenClaw is notorious about its token usage, and for many the price of Opus 4.6 can be cost prohibitive for personal projects. The usual workaround is ‚Äújust switch to a cheaper model‚Äù (Kimi k2.5, etc.), but then you are accepting a trade off: you either eat a noticeable drop in quality or you end up constantly swapping models back and forth based on usage patterns&lt;/p&gt; &lt;p&gt;I packaged Arch-Router (used b HF: &lt;a href="https://x.com/ClementDelangue/status/1979256873669849195"&gt;https://x.com/ClementDelangue/status/1979256873669849195&lt;/a&gt;) into Plano and now calls from OpenClaw can get automatically routed to the right upstream LLM based on preferences you set. Preference could be anything that you can encapsulate as a task. For e.g. for daily calendar and email work you could redirect calls to Ollama-based models locally and for building apps with OpenClaw you could redirect that traffic to Opus 4.6&lt;/p&gt; &lt;p&gt;This hard choice of choosing one model over another goes away with this release. Links to the project below&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g9cwxqtwl6kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r7tga7/intelligent_local_cloud_routing_for_openclaw_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r7tga7/intelligent_local_cloud_routing_for_openclaw_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T04:36:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r88jyj</id>
    <title>I got tired of on-device LLMs crashing my mobile apps, so I built a "Managed" runtime (14k LOC</title>
    <updated>2026-02-18T17:00:32+00:00</updated>
    <author>
      <name>/u/Mundane-Tea-3488</name>
      <uri>https://old.reddit.com/user/Mundane-Tea-3488</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have played around with loads of on-device AI demo for 30 sec they look mesmerising, then the phone turns into a heater and the OS kilss the app dies to memory spikes .&lt;/p&gt; &lt;p&gt;Spent last few months building &lt;a href="https://pub.dev/packages/edge_veda"&gt;Edge-Veda&lt;/a&gt;. I's nt just another wrapper; its a supervised runtime that treats LLMs like prod workloads.&lt;/p&gt; &lt;p&gt;Whats init that makes it cooler:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The Scheduler: Monitors ios/android thermal and battery levels in real time. If the phone gets too hot, it downscales the token/sec&lt;/li&gt; &lt;li&gt;Full StackL Support for GGUF(Text), Whisper(Speech), and VLMs(Vision)&lt;/li&gt; &lt;li&gt;Local RAG: Built in Vector Search(HNSW) thats stays 100% offline&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Its completely opensource &amp;amp; runs via FFI with zero cloud dependencies&lt;/p&gt; &lt;p&gt;Github &lt;a href="https://github.com/ramanujammv1988/edge-veda"&gt;link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane-Tea-3488"&gt; /u/Mundane-Tea-3488 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r88jyj/i_got_tired_of_ondevice_llms_crashing_my_mobile/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r88jyj/i_got_tired_of_ondevice_llms_crashing_my_mobile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r88jyj/i_got_tired_of_ondevice_llms_crashing_my_mobile/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T17:00:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8fjpk</id>
    <title>OllamaFX 0.5.0 Agrupar Chats en carpetas</title>
    <updated>2026-02-18T21:14:39+00:00</updated>
    <author>
      <name>/u/Electronic-Reason582</name>
      <uri>https://old.reddit.com/user/Electronic-Reason582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r8fjpk/ollamafx_050_agrupar_chats_en_carpetas/"&gt; &lt;img alt="OllamaFX 0.5.0 Agrupar Chats en carpetas" src="https://preview.redd.it/ab9qezp3kbkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=516aee86a17e3019fbeab55654ce4fd182df9d3c" title="OllamaFX 0.5.0 Agrupar Chats en carpetas" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sigo en el desarrollo de OllamaFX para su proxima versi√≥n 0.5.0 y una de sus principales caracterisitcas ser√° poder agrupar los chats con tus LLms en carpetas, mover chats entre carpetas y la implementaci√≥n de la papelera de reciclaje, que tal les parece esta funcionalidad, que mas le agregarian, a quienes deseen probar este es el repo de GitHub:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/fredericksalazar/OllamaFX"&gt;https://github.com/fredericksalazar/OllamaFX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="/submit/?source_id=t3_1r8feu9"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Reason582"&gt; /u/Electronic-Reason582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ab9qezp3kbkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8fjpk/ollamafx_050_agrupar_chats_en_carpetas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8fjpk/ollamafx_050_agrupar_chats_en_carpetas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T21:14:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r87oyn</id>
    <title>Mac mini m4 24gb</title>
    <updated>2026-02-18T16:30:21+00:00</updated>
    <author>
      <name>/u/Aromatic_Radio1650</name>
      <uri>https://old.reddit.com/user/Aromatic_Radio1650</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi guys i was wondering what models i could run on a mac mini m4 24gb ram, would i be able to run gpt-oss:20b? or even a 30b model? or do i need to lower my standards and run a 14b model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aromatic_Radio1650"&gt; /u/Aromatic_Radio1650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r87oyn/mac_mini_m4_24gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r87oyn/mac_mini_m4_24gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r87oyn/mac_mini_m4_24gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T16:30:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r858n6</id>
    <title>new interactive UI for my CLI OLLAMA (LLM Checker) command palette, hardware-aware model picks, MCP support .....</title>
    <updated>2026-02-18T15:00:17+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r858n6/new_interactive_ui_for_my_cli_ollama_llm_checker/"&gt; &lt;img alt="new interactive UI for my CLI OLLAMA (LLM Checker) command palette, hardware-aware model picks, MCP support ....." src="https://external-preview.redd.it/eWprdDhpOHRtOWtnMQcNrfIPwOqic-_j14Vn0j5QNYVM7Lkrrt4-l7TtShiW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2406c7a9d06eb3534d5020101ade4189ba7d206" title="new interactive UI for my CLI OLLAMA (LLM Checker) command palette, hardware-aware model picks, MCP support ....." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What changed in this release:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;llm-checker now opens an interactive panel when run with no args&lt;/li&gt; &lt;li&gt;/ opens the full command list, arrows + Enter to run&lt;/li&gt; &lt;li&gt;you can add extra flags before executing a command&lt;/li&gt; &lt;li&gt;new animated banner and cleaner help output (llm-checker help)&lt;/li&gt; &lt;li&gt;recommendations now score against &lt;strong&gt;200+ dynamic Ollama models&lt;/strong&gt; (with &lt;strong&gt;35+ fallback&lt;/strong&gt; when needed)&lt;/li&gt; &lt;li&gt;deterministic &lt;strong&gt;4D scoring&lt;/strong&gt;: Quality / Speed / Fit / Context&lt;/li&gt; &lt;li&gt;better hardware detection across Apple Silicon, NVIDIA CUDA, AMD ROCm, Intel, CPU&lt;/li&gt; &lt;li&gt;calibration + routing flow: calibrate ‚Üí --calibrated / --policy in recommend and ai-run&lt;/li&gt; &lt;li&gt;policy audit export support (JSON / CSV / SARIF)&lt;/li&gt; &lt;li&gt;MCP server mode for Claude/Codex workflows (llm-checker-mcp)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Install/update: npm i -g llm-checker&lt;br /&gt; Repo: &lt;a href="https://github.com/Pavelevich/llm-checker"&gt;https://github.com/Pavelevich/llm-checker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback on the UI/flow is welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cymz3w7tm9kg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r858n6/new_interactive_ui_for_my_cli_ollama_llm_checker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r858n6/new_interactive_ui_for_my_cli_ollama_llm_checker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T15:00:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r87tdt</id>
    <title>packaged claude code + omi + terminator, now it's better than openclaw</title>
    <updated>2026-02-18T16:34:35+00:00</updated>
    <author>
      <name>/u/Deep_Ad1959</name>
      <uri>https://old.reddit.com/user/Deep_Ad1959</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r87tdt/packaged_claude_code_omi_terminator_now_its/"&gt; &lt;img alt="packaged claude code + omi + terminator, now it's better than openclaw" src="https://external-preview.redd.it/YTk1amRlMjg2YWtnMXjfFo5q1zD6K1Ubw5_sFCkd17GjeVIYKU9ByDARBA-a.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83f20fb8d8583165dc35e6384f12e34a07932696" title="packaged claude code + omi + terminator, now it's better than openclaw" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try asking your AI agent to do things for you, it will ask you for 10 confirmations, and still won't do most of the things for you directly, it's frustrating, kills the whole purpose...&lt;/p&gt; &lt;p&gt;What i want is I give a task, and it works on it without bothering me while using my computer, but without taking over my computer. Sounds contradicting, but it's now possible. &lt;/p&gt; &lt;p&gt;Omi + Terminator + Claude Code can work on your computer using your active browser without taking over your keyboard or mouse. See how the agent buys me a ticket with a single prompt. &lt;/p&gt; &lt;p&gt;First, I tried asking Claude Cowork to do that, and it refused, but then Omi did it all. Omi has all the context about me, all the preferences, and access to my credit card. Claude code is spinned up by Omi to work on the task, while Terminator can execute based of the function calls from Claude Code. See it in action &lt;/p&gt; &lt;p&gt;Open source &lt;a href="https://github.com/BasedHardware/omi"&gt;https://github.com/BasedHardware/omi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mediar-ai/terminator"&gt;https://github.com/mediar-ai/terminator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep_Ad1959"&gt; /u/Deep_Ad1959 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/on9h6a286akg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r87tdt/packaged_claude_code_omi_terminator_now_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r87tdt/packaged_claude_code_omi_terminator_now_its/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T16:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8a164</id>
    <title>üñãÔ∏è Just released AI-Writer: A free, offline desktop app for AI-assisted writing powered by Ollama + PyQt5 Body:</title>
    <updated>2026-02-18T17:52:42+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r8a164/just_released_aiwriter_a_free_offline_desktop_app/"&gt; &lt;img alt="üñãÔ∏è Just released AI-Writer: A free, offline desktop app for AI-assisted writing powered by Ollama + PyQt5 Body:" src="https://preview.redd.it/3oi0wof5kakg1.png?width=140&amp;amp;height=95&amp;amp;auto=webp&amp;amp;s=0d09f41acfbd80bf2da6d5e7479c010906c5c7d7" title="üñãÔ∏è Just released AI-Writer: A free, offline desktop app for AI-assisted writing powered by Ollama + PyQt5 Body:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to share a project I've been working on: **AI-Writer** ‚Äî a sleek, privacy-focused desktop app that lets you write with your *local* LLMs via Ollama. No API keys, no cloud, no telemetry. Just you, your words, and your model.&lt;/p&gt; &lt;p&gt;üîó **GitHub:** &lt;a href="https://github.com/Laszlobeer/AI-Writer"&gt;https://github.com/Laszlobeer/AI-Writer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3oi0wof5kakg1.png?width=1076&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1687bc6dd7da2820e48afdf040d6365e689416a1"&gt;https://preview.redd.it/3oi0wof5kakg1.png?width=1076&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1687bc6dd7da2820e48afdf040d6365e689416a1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;### ‚ú® What it does:&lt;/p&gt; &lt;p&gt;- ü§ñ **AI Text Completion**: Highlight text or place your cursor and let your local model continue your story, article, or notes&lt;/p&gt; &lt;p&gt;- üé® **Light/Dark Mode**: Because eyes matter&lt;/p&gt; &lt;p&gt;- üå°Ô∏è **Temperature &amp;amp; Token Controls**: Fine-tune creativity vs. focus on the fly&lt;/p&gt; &lt;p&gt;- üìö **Model Switching**: Instantly swap between any Ollama models you have installed&lt;/p&gt; &lt;p&gt;- üíæ **Export Flexibility**: Save your work as `.txt` or `.docx` (Word-compatible)&lt;/p&gt; &lt;p&gt;- ‚å®Ô∏è **Keyboard Shortcuts**: Write faster with intuitive hotkeys&lt;/p&gt; &lt;p&gt;### üõ†Ô∏è Built with:&lt;/p&gt; &lt;p&gt;- Python 3.8+&lt;/p&gt; &lt;p&gt;- PyQt5 for the GUI&lt;/p&gt; &lt;p&gt;- Requests for Ollama API communication&lt;/p&gt; &lt;p&gt;- python-docx for Word export&lt;/p&gt; &lt;p&gt;### üöÄ Quick Start:&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;ollama pull thewindmom/hermes-3-llama-3.1-8b&lt;/p&gt; &lt;p&gt;# 2. Clone &amp;amp; install&lt;/p&gt; &lt;p&gt;git clone &lt;a href="https://github.com/Laszlobeer/AI-Writer.git"&gt;https://github.com/Laszlobeer/AI-Writer.git&lt;/a&gt;&lt;/p&gt; &lt;p&gt;cd AI-Writer&lt;/p&gt; &lt;p&gt;pip install -r requirements.txt&lt;/p&gt; &lt;p&gt;# 3. Launch&lt;/p&gt; &lt;p&gt;python ai_writer.py&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;### üí° Why I built this:&lt;/p&gt; &lt;p&gt;I wanted a distraction-free writing environment that leverages local AI *without* sending my drafts to the cloud. Whether you're drafting fiction, technical docs, or journaling ‚Äî AI-Writer keeps your workflow private and under your control.&lt;/p&gt; &lt;p&gt;### üôè Feedback welcome!&lt;/p&gt; &lt;p&gt;This is my first major PyQt5 project, so I'd love to hear:&lt;/p&gt; &lt;p&gt;- What features would make your writing workflow better?&lt;/p&gt; &lt;p&gt;- Any bugs or UX quirks you spot?&lt;/p&gt; &lt;p&gt;- Ideas for export formats or integrations?&lt;/p&gt; &lt;p&gt;All contributions and suggestions are welcome on GitHub! üôå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8a164/just_released_aiwriter_a_free_offline_desktop_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8a164/just_released_aiwriter_a_free_offline_desktop_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8a164/just_released_aiwriter_a_free_offline_desktop_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T17:52:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8gfwa</id>
    <title>Schedule mode is coming to Nanocoder... Run project background tasks on a cron schedule üöÄ</title>
    <updated>2026-02-18T21:48:14+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r8gfwa/schedule_mode_is_coming_to_nanocoder_run_project/"&gt; &lt;img alt="Schedule mode is coming to Nanocoder... Run project background tasks on a cron schedule üöÄ" src="https://external-preview.redd.it/s-ytFBPksextywodml2eXUVDHp5dUg_7-gDMuxnUgPU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97c257e47d5c846e99be28e236a586039dc7e347" title="Schedule mode is coming to Nanocoder... Run project background tasks on a cron schedule üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4c72kn5rnbkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r8gfwa/schedule_mode_is_coming_to_nanocoder_run_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r8gfwa/schedule_mode_is_coming_to_nanocoder_run_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-18T21:48:14+00:00</published>
  </entry>
</feed>
