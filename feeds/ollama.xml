<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-02-10T07:10:01+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qz3mta</id>
    <title>questions about experience with ollama pro</title>
    <updated>2026-02-08T08:28:34+00:00</updated>
    <author>
      <name>/u/Slow_Consequence5401</name>
      <uri>https://old.reddit.com/user/Slow_Consequence5401</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm interested in the Ollama Pro subscription; what are the limits? Do you have any experience with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slow_Consequence5401"&gt; /u/Slow_Consequence5401 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qz3mta/questions_about_experience_with_ollama_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qz3mta/questions_about_experience_with_ollama_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qz3mta/questions_about_experience_with_ollama_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T08:28:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz4pth</id>
    <title>Deterministic Thinking for Probabilistic Minds</title>
    <updated>2026-02-08T09:34:43+00:00</updated>
    <author>
      <name>/u/frank_brsrk</name>
      <uri>https://old.reddit.com/user/frank_brsrk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qz4pth/deterministic_thinking_for_probabilistic_minds/"&gt; &lt;img alt="Deterministic Thinking for Probabilistic Minds" src="https://preview.redd.it/vfqzxws5q8ig1.jpg?width=140&amp;amp;height=51&amp;amp;auto=webp&amp;amp;s=31ff1abe6d6423de2dd79b0bd5cf50bde010f9e1" title="Deterministic Thinking for Probabilistic Minds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;**Working on a passion, which i call &amp;quot;intelligence module&amp;quot; composed of decoupled retrievals, and graph build on the fly, composed only of vectors and code. I am building the Reasoning-as-a-Service.**&lt;/p&gt; &lt;p&gt;*CIM - Causal Intelligence Module&lt;/p&gt; &lt;p&gt;The causal workflow handles a user input , analyzes the query, and recognizes which is the most likely steering pattern for the type of causal reasoning style, the aggregator snipes down the highest in confidence pattern of query. That done passes the query to 5 specific designed of causal origin namespaces filled with high signal datasets synthetized through and cross frontier AI models.&lt;br /&gt; The retrieval consists into bringing into surface the common sense and biases of causal perception, the causal cognitive procedures, the ability at the prompt level injection for the AI model receiving final output ( causal thinking styles ), causal math methods, and how the causality propagates ( all datasets graph augmented with necessary nodes and adges).&lt;br /&gt; All of this goes through a graph merger and multiple Context Graph Builders, which maps temporal topology, causal DAGs, entities and possibly connecting cross domain data from previous rags, and concluding to novel hypotheses.&lt;br /&gt; The final row, reasons on all connections, validates against anti patterns, it executes the math to prove information are stable, it conducts propagation math, does complete 50 simulations through monte carlo and zooms in the graph in order to dont lose any important sub graph , needed for reasoning incentives. to be continued with complete Audit Trail ( AI compliance) , Reasoning trace mermaid visualization, Execution Logger, and Final LLM Prompt.&lt;/p&gt; &lt;p&gt;sincerely i am really excited about this development of mine, almost at 97%, i am looking to deploy it as an API service, and i will be looking for testers soon, so please come along.&lt;/p&gt; &lt;p&gt;frank :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vfqzxws5q8ig1.jpg?width=1544&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4bad6f66a2d1bea2efb65ad28aea7d276fd87b81"&gt;https://preview.redd.it/vfqzxws5q8ig1.jpg?width=1544&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4bad6f66a2d1bea2efb65ad28aea7d276fd87b81&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frank_brsrk"&gt; /u/frank_brsrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qz4pth/deterministic_thinking_for_probabilistic_minds/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qz4pth/deterministic_thinking_for_probabilistic_minds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qz4pth/deterministic_thinking_for_probabilistic_minds/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T09:34:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz6w8f</id>
    <title>What graphics card models will you be using with an RTX 3060 12GB in 2026?</title>
    <updated>2026-02-08T11:45:24+00:00</updated>
    <author>
      <name>/u/DespeShaha</name>
      <uri>https://old.reddit.com/user/DespeShaha</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DespeShaha"&gt; /u/DespeShaha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1qz6w36/what_models_are_you_running_on_rtx_3060_12gb_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qz6w8f/what_graphics_card_models_will_you_be_using_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qz6w8f/what_graphics_card_models_will_you_be_using_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T11:45:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyrotc</id>
    <title>Releasing 1.22. 0 of Nanocoder - an update breakdown üî•</title>
    <updated>2026-02-07T22:37:49+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qyrotc/releasing_122_0_of_nanocoder_an_update_breakdown/"&gt; &lt;img alt="Releasing 1.22. 0 of Nanocoder - an update breakdown üî•" src="https://external-preview.redd.it/IFFJsgrmmVsD9q-i5nEflRo3mXp-4vSYGfldoMnuGDA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4266ff125f5b909fec66b3497e1b578b53adf7f" title="Releasing 1.22. 0 of Nanocoder - an update breakdown üî•" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t790s2gjg5ig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyrotc/releasing_122_0_of_nanocoder_an_update_breakdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyrotc/releasing_122_0_of_nanocoder_an_update_breakdown/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T22:37:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz3ykj</id>
    <title>llm-use ‚Äì An Open-Source Framework for Routing and Orchestrating Multi-LLM Agent Workflows</title>
    <updated>2026-02-08T08:48:57+00:00</updated>
    <author>
      <name>/u/Just_Vugg_PolyMCP</name>
      <uri>https://old.reddit.com/user/Just_Vugg_PolyMCP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qz3ykj/llmuse_an_opensource_framework_for_routing_and/"&gt; &lt;img alt="llm-use ‚Äì An Open-Source Framework for Routing and Orchestrating Multi-LLM Agent Workflows" src="https://external-preview.redd.it/wdg8khR5ZOlxHpHG07yHIDTSwn_qqpgJFwZQsc7I-t8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0ee09820ada5531c9ca9f6d61db150ecd49759" title="llm-use ‚Äì An Open-Source Framework for Routing and Orchestrating Multi-LLM Agent Workflows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Vugg_PolyMCP"&gt; /u/Just_Vugg_PolyMCP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/llm-use"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qz3ykj/llmuse_an_opensource_framework_for_routing_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qz3ykj/llmuse_an_opensource_framework_for_routing_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T08:48:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyghp3</id>
    <title>Ollie | A Friendly, Local-First AI Companion for Ollama</title>
    <updated>2026-02-07T15:21:30+00:00</updated>
    <author>
      <name>/u/MoonXPlayer</name>
      <uri>https://old.reddit.com/user/MoonXPlayer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/"&gt; &lt;img alt="Ollie | A Friendly, Local-First AI Companion for Ollama" src="https://preview.redd.it/fh544zreb3ig1.png?width=140&amp;amp;height=80&amp;amp;auto=webp&amp;amp;s=16a686c3d99dee138869217dabb9a9cd246fe905" title="Ollie | A Friendly, Local-First AI Companion for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm sharing &lt;strong&gt;Ollie&lt;/strong&gt;, a Linux-native, local-first personal AI assistant built on top of &lt;strong&gt;Ollama&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fh544zreb3ig1.png?width=1682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23c108dff77d288035dbc0d1dff64503bcd370dd"&gt;https://preview.redd.it/fh544zreb3ig1.png?width=1682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23c108dff77d288035dbc0d1dff64503bcd370dd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ollie runs entirely on your machine ‚Äî no cloud (I'm considering optional cloud APIs like Anthropic), no tracking, no CLI. It offers a polished desktop experience for chatting with local LLMs, managing models, analyzing files and images, and monitoring system usage in real time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clean chat UI with full Markdown, code, tables, and math&lt;/li&gt; &lt;li&gt;Built-in model management (pull / delete / switch)&lt;/li&gt; &lt;li&gt;Vision + PDF / text file analysis (drag &amp;amp; drop)&lt;/li&gt; &lt;li&gt;AppImage distribution (download &amp;amp; run)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Built with &lt;strong&gt;Tauri v2 (Rust) + React + TypeScript&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Feedback and technical criticism are very welcome.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MedGm/Ollie"&gt;https://github.com/MedGm/Ollie&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoonXPlayer"&gt; /u/MoonXPlayer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qyghp3/ollie_a_friendly_localfirst_ai_companion_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-07T15:21:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzdhyy</id>
    <title>What hardware is needed to run qwen3-code locally?</title>
    <updated>2026-02-08T16:33:33+00:00</updated>
    <author>
      <name>/u/Either_Address8955</name>
      <uri>https://old.reddit.com/user/Either_Address8955</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Either_Address8955"&gt; /u/Either_Address8955 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzdhyy/what_hardware_is_needed_to_run_qwen3code_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzdhyy/what_hardware_is_needed_to_run_qwen3code_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzdhyy/what_hardware_is_needed_to_run_qwen3code_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T16:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzfjev</id>
    <title>Ollama does not appear to be running on my gpu(s)</title>
    <updated>2026-02-08T17:49:20+00:00</updated>
    <author>
      <name>/u/WitchesSphincter</name>
      <uri>https://old.reddit.com/user/WitchesSphincter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I have recently installed Ollama / OpenWebUI on a proxmox LXC using the script linked below. Everything appears to be functioning as expected, I can connect to Ollama and interact with it but I am fairly certain it is not using my GPU.&lt;/p&gt; &lt;p&gt;On the lxc container I can run nvidia-smi and see the card is loaded just as I do on other containers but no processes are shown. My understanding is that I should see some process in there.&lt;/p&gt; &lt;p&gt;Are there any troubleshooting or configuration guides? I included the output of systemctl status ollama if that helps&lt;/p&gt; &lt;p&gt;&lt;a href="https://community-scripts.github.io/ProxmoxVE/scripts?id=openwebui"&gt;https://community-scripts.github.io/ProxmoxVE/scripts?id=openwebui&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚óè ollama.service - Ollama Service Loaded: loaded (/etc/systemd/system/ollama.service; enabled; preset: enabled) Active: active (running) since Fri 2026-02-06 15:29:40 EST; 1 day 21h ago Invocation: 385660c640f24a069bfd156bb621250a Main PID: 150 (ollama) Tasks: 36 (limit: 173018) Memory: 5.6G (peak: 5.6G) CPU: 43min 14.073s CGroup: /system.slice/ollama.service ‚îú‚îÄ 150 /usr/bin/ollama serve ‚îî‚îÄ6298 /usr/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --port 45509&lt;/p&gt; &lt;p&gt;Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=device.go:245 msg=&amp;quot;model weights&amp;quot; device=CPU size=&amp;quot;4.9 GiB&amp;quot; Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=ggml.go:494 msg=&amp;quot;offloaded 0/37 layers to GPU&amp;quot; Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=device.go:256 msg=&amp;quot;kv cache&amp;quot; device=CPU size=&amp;quot;576.0 MiB&amp;quot; Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=device.go:267 msg=&amp;quot;compute graph&amp;quot; device=CPU size=&amp;quot;100.0 MiB&amp;quot; Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=device.go:272 msg=&amp;quot;total memory&amp;quot; size=&amp;quot;5.5 GiB&amp;quot; Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=sched.go:537 msg=&amp;quot;loaded runners&amp;quot; count=1 Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=server.go:1349 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; Feb 08 12:39:50 openwebui ollama[150]: time=2026-02-08T12:39:50.310-05:00 level=INFO source=server.go:1383 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server loading model&amp;quot; Feb 08 12:39:56 openwebui ollama[150]: time=2026-02-08T12:39:56.843-05:00 level=INFO source=server.go:1387 msg=&amp;quot;llama runner started in 7.02 seconds&amp;quot; Feb 08 12:41:12 openwebui ollama[150]: [GIN] 2026/02/08 - 12:41:12 | 200 | 1m23s | ::1 | POST &amp;quot;/api/chat&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WitchesSphincter"&gt; /u/WitchesSphincter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzfjev/ollama_does_not_appear_to_be_running_on_my_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzfjev/ollama_does_not_appear_to_be_running_on_my_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzfjev/ollama_does_not_appear_to_be_running_on_my_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T17:49:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzg2zw</id>
    <title>What do we have for Ollama that works like Perplexity‚Äôs Comet Browser?</title>
    <updated>2026-02-08T18:09:16+00:00</updated>
    <author>
      <name>/u/Comfortable_Ad_8117</name>
      <uri>https://old.reddit.com/user/Comfortable_Ad_8117</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does something exist that will do a similar job as Perplexity Comet browser? Full interaction with web pages and tabs and run on my local Ollama server? I tried Ask Steve but could never get it to talk to my remote Ollama server (only an Ollama running on the same box as the plugin) I have a pair of 5060‚Äôs that are part of my dedicated Ai sever running Ollama and I want my workstation browser to be able to utilize the remote server. &lt;/p&gt; &lt;p&gt;Anyone have a chrome plugin for this (other than Steve) or even an entire chromium browser like Comet that can plug into my local Ai? &lt;/p&gt; &lt;p&gt;My use case - I buy antiques at auction and I like to ask the Ai if it sees any lots on the page that may be worth my time to look at, then I want it to find comps that may have sold and give me an idea of what my ROI is going to be. I built an app to do this and it works, but having it right in the web browser side bar using perplexity is nice, but would like to give my hardware a chance at it too. &lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Ad_8117"&gt; /u/Comfortable_Ad_8117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzg2zw/what_do_we_have_for_ollama_that_works_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzg2zw/what_do_we_have_for_ollama_that_works_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzg2zw/what_do_we_have_for_ollama_that_works_like/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T18:09:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzqasm</id>
    <title>What happened to ollama . org? Tried using it after sometim. The URL is redirecting to FreeGPT</title>
    <updated>2026-02-09T01:00:30+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzqasm/what_happened_to_ollama_org_tried_using_it_after/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzqasm/what_happened_to_ollama_org_tried_using_it_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzqasm/what_happened_to_ollama_org_tried_using_it_after/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T01:00:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r01i5f</id>
    <title>I need help so I got openclaw</title>
    <updated>2026-02-09T11:12:19+00:00</updated>
    <author>
      <name>/u/MoreLavishness1061</name>
      <uri>https://old.reddit.com/user/MoreLavishness1061</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which is good to run I have a i900k rtx 2080 looking for a good one to run local&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoreLavishness1061"&gt; /u/MoreLavishness1061 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r01i5f/i_need_help_so_i_got_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r01i5f/i_need_help_so_i_got_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r01i5f/i_need_help_so_i_got_openclaw/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T11:12:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzttub</id>
    <title>Loading model into RAM before VRM, PROBLEM</title>
    <updated>2026-02-09T03:52:20+00:00</updated>
    <author>
      <name>/u/No_Mango7658</name>
      <uri>https://old.reddit.com/user/No_Mango7658</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Strix Halo 128gb, 96gb dedicated to GPU. When trying to load models larger than 32gb (CPU RAM allocation) I get an error that I don't have enough ram. How do I bypass this RAM check and just load the model into GPU memory?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Mango7658"&gt; /u/No_Mango7658 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzttub/loading_model_into_ram_before_vrm_problem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzttub/loading_model_into_ram_before_vrm_problem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzttub/loading_model_into_ram_before_vrm_problem/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T03:52:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzrwlb</id>
    <title>Recommend model for openclaw clawdbot running locally on old laptop 4gb vram 16g ram asus</title>
    <updated>2026-02-09T02:21:02+00:00</updated>
    <author>
      <name>/u/Flux-of-Time</name>
      <uri>https://old.reddit.com/user/Flux-of-Time</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flux-of-Time"&gt; /u/Flux-of-Time &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzrwlb/recommend_model_for_openclaw_clawdbot_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzrwlb/recommend_model_for_openclaw_clawdbot_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzrwlb/recommend_model_for_openclaw_clawdbot_running/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T02:21:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0jo10</id>
    <title>HoopyClaw: Run your OpenClaw agent 24/7 in the cloud without buying dedicated hardware</title>
    <updated>2026-02-09T23:02:22+00:00</updated>
    <author>
      <name>/u/ResourcePuzzled2387</name>
      <uri>https://old.reddit.com/user/ResourcePuzzled2387</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone&lt;/p&gt; &lt;p&gt;If you've been following the OpenClaw hype (and it's well deserved ‚Äî the project is incredible), you know the biggest barrier to entry: &lt;strong&gt;you need dedicated hardware running 24/7&lt;/strong&gt;. Most people are buying Mac Minis, repurposing old laptops, or spinning up VPS instances and configuring everything manually.&lt;/p&gt; &lt;p&gt;We're building&lt;a href="https://hoopyclaw.com"&gt; &lt;strong&gt;HoopyClaw&lt;/strong&gt;&lt;/a&gt; to solve exactly that ‚Äî a managed cloud platform where you can deploy OpenClaw or nanobot agents in seconds, without touching a terminal or managing infrastructure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create an account&lt;/li&gt; &lt;li&gt;Choose your engine ‚Äî &lt;strong&gt;OpenClaw&lt;/strong&gt; (full power: deep reasoning, browser control, skills, persistent memory) or &lt;strong&gt;nanobot&lt;/strong&gt; (ultra-lightweight, ~4,000 lines of code, perfect for simple assistants)&lt;/li&gt; &lt;li&gt;Connect your platforms ‚Äî Discord, Telegram, WhatsApp, Slack, Email, and more&lt;/li&gt; &lt;li&gt;Deploy ‚Äî your agent is live and running 24/7&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why we're building this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;OpenClaw is amazing, but self-hosting isn't for everyone. Beyond the hassle of keeping a machine running 24/7, there's a real &lt;strong&gt;security concern&lt;/strong&gt;: running an AI agent with full system access on your personal computer means exposing your files, credentials, and private data. If something goes wrong ‚Äî a misconfigured skill, an unexpected command ‚Äî it's &lt;em&gt;your&lt;/em&gt; machine at risk.&lt;/p&gt; &lt;p&gt;With HoopyClaw, &lt;strong&gt;your agent runs in an isolated cloud instance&lt;/strong&gt;, completely separated from your personal environment. Your data stays safe, your machine stays yours, and your agent gets its own sandboxed space to operate. All on European infrastructure, fully GDPR compliant.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What you get:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Always-on cloud deployment&lt;/strong&gt; ‚Äî no Mac Mini sitting in your closet&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Isolated instances&lt;/strong&gt; ‚Äî your agent never touches your personal machine or data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;9+ platforms&lt;/strong&gt; connected out of the box&lt;/li&gt; &lt;li&gt;&lt;strong&gt;European infrastructure&lt;/strong&gt;, encrypted and GDPR compliant&lt;/li&gt; &lt;li&gt;&lt;strong&gt;One-click setup&lt;/strong&gt; ‚Äî no CLI, no DevOps knowledge required&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Monitor &amp;amp; optimize&lt;/strong&gt; your agent with real conversation data&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Pricing:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Nano plan&lt;/strong&gt; (nanobot engine): ‚Ç¨39/mo&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Claw plan&lt;/strong&gt; (OpenClaw engine): ‚Ç¨59/mo&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom/Enterprise&lt;/strong&gt;: contact us&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear your feedback ‚Äî especially from people who tried self-hosting OpenClaw and found it painful, or from those who wanted to try it but didn't have the hardware.&lt;/p&gt; &lt;p&gt;üîó&lt;a href="https://hoopyclaw.com"&gt; https://hoopyclaw.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResourcePuzzled2387"&gt; /u/ResourcePuzzled2387 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0jo10/hoopyclaw_run_your_openclaw_agent_247_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0jo10/hoopyclaw_run_your_openclaw_agent_247_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0jo10/hoopyclaw_run_your_openclaw_agent_247_in_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T23:02:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzh0op</id>
    <title>I created a small AI Agent</title>
    <updated>2026-02-08T18:43:20+00:00</updated>
    <author>
      <name>/u/Rough_Philosopher877</name>
      <uri>https://old.reddit.com/user/Rough_Philosopher877</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys.. I know it's not so big thing.. just for fun I created a Small AI Agent:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/tysonchamp/Small-AI-Agent"&gt;https://github.com/tysonchamp/Small-AI-Agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love the feedback of the community.. and any suggestions of new ideas.&lt;/p&gt; &lt;p&gt;I created this for my day to day activities.. such as setup reminders, take notes, monitor all my client's website (if they are all ok or not).. monitor all my servers, connecting it to my custom erp for due invoice fetching, project management etc &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rough_Philosopher877"&gt; /u/Rough_Philosopher877 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzh0op/i_created_a_small_ai_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-08T18:43:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzrd8g</id>
    <title>DaveLovable is an open-source, AI-powered web UI/UX development platform</title>
    <updated>2026-02-09T01:54:32+00:00</updated>
    <author>
      <name>/u/LeadingFun1849</name>
      <uri>https://old.reddit.com/user/LeadingFun1849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on this project for a while.&lt;/p&gt; &lt;p&gt;DaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch. It combines cutting-edge AI orchestration with browser-based execution to offer the most advanced open-source alternative for rapid frontend prototyping.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/davidmonterocrespo24/DaveLovable"&gt;https://github.com/davidmonterocrespo24/DaveLovable&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeadingFun1849"&gt; /u/LeadingFun1849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzrd8g/davelovable_is_an_opensource_aipowered_web_uiux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T01:54:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0b835</id>
    <title>Free Strix Halo performance!</title>
    <updated>2026-02-09T17:55:04+00:00</updated>
    <author>
      <name>/u/Potential_Block4598</name>
      <uri>https://old.reddit.com/user/Potential_Block4598</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Potential_Block4598"&gt; /u/Potential_Block4598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1r0b7p8/free_strix_halo_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0b835/free_strix_halo_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0b835/free_strix_halo_performance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T17:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r03gd0</id>
    <title>Ollama and claude code</title>
    <updated>2026-02-09T12:54:47+00:00</updated>
    <author>
      <name>/u/Separate_Coconut_592</name>
      <uri>https://old.reddit.com/user/Separate_Coconut_592</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, i dont know what im doing wrong but my models can't write to local files. Regardless of model . Am i doing something wrong. Currently on fedora 42. Gemini CLI is fine , claude code is fine just local models cant do it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Separate_Coconut_592"&gt; /u/Separate_Coconut_592 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r03gd0/ollama_and_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r03gd0/ollama_and_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r03gd0/ollama_and_claude_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T12:54:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzzd5x</id>
    <title>Qwen 3 coder next for R coding (academic)</title>
    <updated>2026-02-09T09:01:29+00:00</updated>
    <author>
      <name>/u/Bahaal_1981</name>
      <uri>https://old.reddit.com/user/Bahaal_1981</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am an academic. I have claude via work and it excels at R coding and building Shiny apps with little prompting (Opus 4.5 but Sonnet does fairly well also). This is both for teaching / research. But I also want local models (for various reasons, privacy, reproducibility, etc). I have ollama with cohere / Mistral Large / phi reasoning, running on an M4 Max with 128 gb ram. Reading up I think qwen coder next might do better:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/qwen3-coder-next"&gt;https://ollama.com/library/qwen3-coder-next&lt;/a&gt; --&amp;gt; 85GB model -- additional settings needed?&lt;/p&gt; &lt;p&gt;I also looked for Kimi but could only find the cloud version. Any advice? Many thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bahaal_1981"&gt; /u/Bahaal_1981 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qzzd5x/qwen_3_coder_next_for_r_coding_academic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T09:01:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r07fwp</id>
    <title>Izwi - A local audio inference engine written in Rust</title>
    <updated>2026-02-09T15:39:06+00:00</updated>
    <author>
      <name>/u/zinyando</name>
      <uri>https://old.reddit.com/user/zinyando</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1r07fwp/izwi_a_local_audio_inference_engine_written_in/"&gt; &lt;img alt="Izwi - A local audio inference engine written in Rust" src="https://external-preview.redd.it/eXviKCvfn4aFzZdhc6p4q23Sa8NTTdYj4kpCRejt2Io.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82e5ee56fd8e2f07ab6f70585c28ac58010947bd" title="Izwi - A local audio inference engine written in Rust" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been building Izwi, a fully local audio inference stack for speech workflows. No cloud APIs, no data leaving your machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's inside:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text-to-speech &amp;amp; speech recognition (ASR)&lt;/li&gt; &lt;li&gt;Voice cloning &amp;amp; voice design&lt;/li&gt; &lt;li&gt;Chat/audio-chat models&lt;/li&gt; &lt;li&gt;OpenAI-compatible API (&lt;code&gt;/v1&lt;/code&gt; routes)&lt;/li&gt; &lt;li&gt;Apple Silicon acceleration (Metal)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Stack:&lt;/strong&gt; Rust backend (Candle/MLX), React/Vite UI, CLI-first workflow.&lt;/p&gt; &lt;p&gt;Everything runs locally. Pull models from Hugging Face, benchmark throughput, or just &lt;code&gt;izwi tts &amp;quot;Hello world&amp;quot;&lt;/code&gt; and go.&lt;/p&gt; &lt;p&gt;Apache 2.0, actively developed. Would love feedback from anyone working on local ML in Rust!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/agentem-ai/izwi"&gt;https://github.com/agentem-ai/izwi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zinyando"&gt; /u/zinyando &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/agentem-ai/izwi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r07fwp/izwi_a_local_audio_inference_engine_written_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r07fwp/izwi_a_local_audio_inference_engine_written_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T15:39:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0p7b2</id>
    <title>Model reccomendations</title>
    <updated>2026-02-10T03:00:15+00:00</updated>
    <author>
      <name>/u/No-Mortgage4154</name>
      <uri>https://old.reddit.com/user/No-Mortgage4154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so I recently found out about ollama and was wondering about like what models are good to use for coding and other stuff&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mortgage4154"&gt; /u/No-Mortgage4154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0p7b2/model_reccomendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0p7b2/model_reccomendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0p7b2/model_reccomendations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T03:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0pu1y</id>
    <title>Need suggestion</title>
    <updated>2026-02-10T03:28:38+00:00</updated>
    <author>
      <name>/u/harneetsingh17</name>
      <uri>https://old.reddit.com/user/harneetsingh17</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/harneetsingh17"&gt; /u/harneetsingh17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalAIServers/comments/1r0phxl/need_suggestion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0pu1y/need_suggestion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0pu1y/need_suggestion/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T03:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0q55x</id>
    <title>any good models?</title>
    <updated>2026-02-10T03:43:05+00:00</updated>
    <author>
      <name>/u/No-Mortgage4154</name>
      <uri>https://old.reddit.com/user/No-Mortgage4154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i recently found out about ollama and how its like a local ai and was wondering what are some good models out there my pc specs are: ryzen 7 7800x3d, 4070ti super nvidia, and ddr5 32gb ram.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mortgage4154"&gt; /u/No-Mortgage4154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0q55x/any_good_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0q55x/any_good_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0q55x/any_good_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T03:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0d432</id>
    <title>Local AI for small company</title>
    <updated>2026-02-09T19:02:02+00:00</updated>
    <author>
      <name>/u/LiteLive</name>
      <uri>https://old.reddit.com/user/LiteLive</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I‚Äòm looking into options to get local AI running for my company.&lt;/p&gt; &lt;p&gt;We do technical consulting and love to use AI for skimming through technical documents and pinpointing information down.&lt;/p&gt; &lt;p&gt;We are burning through Tokens and I‚Äòm trying to save us some money but having local AI would actually allow us to use it on sensible data. Not all our customers allow cloud based AI assistance, because even when the providers say they don‚Äôt train / store data, we cannot be certain.&lt;/p&gt; &lt;p&gt;What do we want to do?&lt;/p&gt; &lt;p&gt;I envision a paperless-ngx instance where we can upload a shitton of unsorted / unknown data. We have a solid promt&lt;/p&gt; &lt;p&gt;That categorizes the data and indexes the files. Allocates it to the right customer / project. And makes it accessible, searchable and tags them according to our need.&lt;/p&gt; &lt;p&gt;Right now we use cloud providers to do this, but as I mentioned before we are burning through tokens. Especially in the beginning of projects when we digitalize a wheelbarrow full of hard copies folders.&lt;/p&gt; &lt;p&gt;My colleague said we should just buy a Mac mini and use that as an Ollama host, but I hate Apple with a passion (while writing this on an iPhone‚Ä¶).&lt;/p&gt; &lt;p&gt;I was looking at the Minisforum MS-S1 Max, hardware looks promising. I want to run Proxmox PVE 9 on it, then pass the GPU to the LXC where Ollama will reside.&lt;/p&gt; &lt;p&gt;Is this a viable path? &lt;/p&gt; &lt;p&gt;My calculation is, if we spent 500‚Ç¨ on Tokens per month, and we can save half of that with this device, it would basically pay itself off within a year. And looking back at the last 12 months, I can see a steady increase in tokens for us. While enabling us to also process highly sensible data with AI.&lt;/p&gt; &lt;p&gt;What models can I realistically run on this hardware? I was thinking something like Llama4:Maverik will probably work for us.&lt;/p&gt; &lt;p&gt;Would you guys maybe recommend a different model for our ‚Äûbackground‚Äú usecase? Are there other ways to streamline our workflow maybe?&lt;/p&gt; &lt;p&gt;To be fair I don‚Äôt want to get rid of all cloud AI, as I fully understand that their models will always be more sophisticated and faster.&lt;/p&gt; &lt;p&gt;Looking forward for you comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiteLive"&gt; /u/LiteLive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0d432/local_ai_for_small_company/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0d432/local_ai_for_small_company/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0d432/local_ai_for_small_company/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-09T19:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0n1sg</id>
    <title>We won a hackathon with this project using Ollama. But is it actually useful?</title>
    <updated>2026-02-10T01:25:10+00:00</updated>
    <author>
      <name>/u/BriefAd2120</name>
      <uri>https://old.reddit.com/user/BriefAd2120</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: I built a 3d memory layer to visualize your chats with a custom MCP server to inject relevant context, Looking for feedback!&lt;/p&gt; &lt;p&gt;Cortex turns raw chat history into reusable context using hybrid retrieval (about 65% keyword, 35% semantic), local summaries with Qwen 2.5 8B, and auto system prompts so setup goes from minutes to seconds.&lt;/p&gt; &lt;p&gt;It also runs through a custom MCP server with search + fetch tools, so external LLMs like Claude can pull the right memory at inference time.&lt;/p&gt; &lt;p&gt;And because scrolling is pain, I added a 3D brain-style map built with UMAP, K-Means, and Three.js so you can explore conversations like a network instead of a timeline.&lt;/p&gt; &lt;p&gt;We won the hackathon with it, but I want a reality check: is this actually useful, or just a cool demo?&lt;/p&gt; &lt;p&gt;YouTube demo: &lt;a href="https://www.youtube.com/watch?v=SC_lDydnCF4"&gt;https://www.youtube.com/watch?v=SC_lDydnCF4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LinkedIn post: &lt;a href="https://www.linkedin.com/feed/update/urn:li:activity:7426518101162205184/"&gt;https://www.linkedin.com/feed/update/urn:li:activity:7426518101162205184/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github Link (pls star itü•∫): &lt;a href="https://github.com/Vibhor7-7/Cortex-CxC"&gt;https://github.com/Vibhor7-7/Cortex-CxC&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BriefAd2120"&gt; /u/BriefAd2120 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1r0n1sg/we_won_a_hackathon_with_this_project_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-02-10T01:25:10+00:00</published>
  </entry>
</feed>
