<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-10-10T07:24:18+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1nytxfy</id>
    <title>CoexistAI Now Supports Docker Setup, Also now you can turn any text into Podcasts and Speech Easily</title>
    <updated>2025-10-05T17:12:21+00:00</updated>
    <author>
      <name>/u/Optimalutopic</name>
      <uri>https://old.reddit.com/user/Optimalutopic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nytxfy/coexistai_now_supports_docker_setup_also_now_you/"&gt; &lt;img alt="CoexistAI Now Supports Docker Setup, Also now you can turn any text into Podcasts and Speech Easily" src="https://external-preview.redd.it/1lbtuTIGlXroGvtLVbMfebEYVWfSR6Nw8inE4I1SvqU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63bc74c711389d587961e85da7aa68d748e4f6b6" title="CoexistAI Now Supports Docker Setup, Also now you can turn any text into Podcasts and Speech Easily" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thanks for all the continued love and feedback! Based on what we’ve heard, we’ve added a simple &lt;strong&gt;Docker setup&lt;/strong&gt; (&lt;a href="https://github.com/SPThole/CoexistAI/blob/main/README.docker.md"&gt;https://github.com/SPThole/CoexistAI/blob/main/README.docker.md&lt;/a&gt;) to make it much easier to get started across different platforms.&lt;/p&gt; &lt;h1&gt;Key Updates&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Text to Podcast:&lt;/strong&gt; Convert written content into full podcast episodes — perfect for repurposing articles or blogs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text to Speech:&lt;/strong&gt; Generate high-quality audio from text, with flexible integration for agents or standalone use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docker Installation:&lt;/strong&gt; A new containerized setup with a quick script (&lt;code&gt;./quick_setup_docker.sh&lt;/code&gt;) that handles everything — including an admin UI to tweak settings effortlessly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Just run one script, and you’ll have a &lt;strong&gt;powerful set of tools&lt;/strong&gt; at your fingertips —&lt;br /&gt; get answers from sources across the web, YouTube, Reddit, Maps, Git, or local files;&lt;br /&gt; turn your research into a podcast;&lt;/p&gt; &lt;p&gt;summarise any web page, any youtube video, get newsletters from reddit based on days hot topics,&lt;br /&gt; convert text into speech when you don’t want to read; I can think off so many use cases&lt;br /&gt; and connect your local LLMs/Embedders to powerful tools to build your own &lt;strong&gt;Perplexity/Exa-like research engine/you can achieve much more than that.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Some head to head Comparison in comments&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optimalutopic"&gt; /u/Optimalutopic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/SPThole/CoexistAI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nytxfy/coexistai_now_supports_docker_setup_also_now_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nytxfy/coexistai_now_supports_docker_setup_also_now_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-05T17:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyuamb</id>
    <title>What I Would Like</title>
    <updated>2025-10-05T17:26:04+00:00</updated>
    <author>
      <name>/u/booknerdcarp</name>
      <uri>https://old.reddit.com/user/booknerdcarp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What I would really like is to be able to vibe code with an Ollama model and have it be able to read and write files in my project folder. I am new to this, learning, and having fun but I am just not sure how to accomplish this. Does any one have detailed info or can help. I would appreciate it. The recent changes to Anthropic have made it unbearable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/booknerdcarp"&gt; /u/booknerdcarp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nyuamb/what_i_would_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nyuamb/what_i_would_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nyuamb/what_i_would_like/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-05T17:26:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nykt3u</id>
    <title>Sneak Preview: Ollama Bench</title>
    <updated>2025-10-05T10:34:31+00:00</updated>
    <author>
      <name>/u/phantagom</name>
      <uri>https://old.reddit.com/user/phantagom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nykt3u/sneak_preview_ollama_bench/"&gt; &lt;img alt="Sneak Preview: Ollama Bench" src="https://preview.redd.it/0nec59t9p9tf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1da048e9acbe9b8959f540c8d86b301f7837805c" title="Sneak Preview: Ollama Bench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantagom"&gt; /u/phantagom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0nec59t9p9tf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nykt3u/sneak_preview_ollama_bench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nykt3u/sneak_preview_ollama_bench/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-05T10:34:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nytsku</id>
    <title>Holo1.5 3B as UI Grounding model + Claude as thinking model for Computer Use</title>
    <updated>2025-10-05T17:07:18+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nytsku/holo15_3b_as_ui_grounding_model_claude_as/"&gt; &lt;img alt="Holo1.5 3B as UI Grounding model + Claude as thinking model for Computer Use" src="https://external-preview.redd.it/ZXU5cjkxcjJzYnRmMW31g_v519zh4lqVlORxH-jtbfthhY6srytiSohAXo_q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=627c44e3b263045f5843d0148fdbf93efc7cc36b" title="Holo1.5 3B as UI Grounding model + Claude as thinking model for Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Runner H making some sense of GIMP.&lt;/p&gt; &lt;p&gt;Try yourself : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lbh6if03sbtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nytsku/holo15_3b_as_ui_grounding_model_claude_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nytsku/holo15_3b_as_ui_grounding_model_claude_as/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-05T17:07:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o005ie</id>
    <title>Type of maps</title>
    <updated>2025-10-07T00:23:33+00:00</updated>
    <author>
      <name>/u/Several-Mongoose3571</name>
      <uri>https://old.reddit.com/user/Several-Mongoose3571</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o005ie/type_of_maps/"&gt; &lt;img alt="Type of maps" src="https://preview.redd.it/mfahivws2ltf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ed4b9cff5302609bbe03d73b8f8c8c86bfe63f2" title="Type of maps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on this setting for a few weeks, it’s a world where oceanic trade empires dominate, and most inland territories are scattered and underdeveloped due to ancient magical floods.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Several-Mongoose3571"&gt; /u/Several-Mongoose3571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mfahivws2ltf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o005ie/type_of_maps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o005ie/type_of_maps/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T00:23:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzc25a</id>
    <title>We made an Open Source Llama Agent Kit, memory-enabled Node.js agent framework for Ollama</title>
    <updated>2025-10-06T06:54:40+00:00</updated>
    <author>
      <name>/u/AideTop8744</name>
      <uri>https://old.reddit.com/user/AideTop8744</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At our company I was investigating the possibility of switching to a self hosted Agentic AI using docker for a product we have, it didnt turn out as i hoped to be honest. The response times are not as fast as the API solutions. And i dont think we can really use it in a commercial setting without building our own AI Server with GPUs. I was hoping the CPU based models would perform better. &lt;/p&gt; &lt;p&gt;However the result of this work we decided to open-source, I hope it helps with someone elses work. It does integrate tool calling, threading and memory so it is usable if you are okay with waiting a zibbilion hours. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/EmreMutlu99/Ollama-Agent-Kit"&gt;https://github.com/EmreMutlu99/Ollama-Agent-Kit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AideTop8744"&gt; /u/AideTop8744 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nzc25a/we_made_an_open_source_llama_agent_kit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nzc25a/we_made_an_open_source_llama_agent_kit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nzc25a/we_made_an_open_source_llama_agent_kit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-06T06:54:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyvinr</id>
    <title>Ally finally got RAG – everything runs local now</title>
    <updated>2025-10-05T18:11:18+00:00</updated>
    <author>
      <name>/u/YassinK97</name>
      <uri>https://old.reddit.com/user/YassinK97</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nyvinr/ally_finally_got_rag_everything_runs_local_now/"&gt; &lt;img alt="Ally finally got RAG – everything runs local now" src="https://b.thumbs.redditmedia.com/tl_M5Cc-ul8MmqrX8T3MH0xhoym2th1IDoyyts6xSYU.jpg" title="Ally finally got RAG – everything runs local now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thanks everyone for the support (and stars) from my first posts featuring &lt;a href="https://github.com/YassWorks/Ally"&gt;Ally&lt;/a&gt;, the fully local agentic CLI.&lt;/p&gt; &lt;p&gt;As promised, I've been working on the RAG feature and it's finally here (v0.4.0 as of writing this post). There are currently only local embedding options (HuggingFace or Ollama). You can choose the embedding settings during setup which is really easy and you'll be ready to dive in.&lt;/p&gt; &lt;p&gt;Ally is instructed to only ever answer based on the data provided during RAG sessions. But you can give it permission to use external data as well like the web.&lt;/p&gt; &lt;p&gt;Because the workflow runs entirely locally, you can turn off your internet connection and still have a fully private chat with all your documents (of any types!).&lt;/p&gt; &lt;p&gt;Continuing old conversations is now an option as well with the &lt;code&gt;-i &amp;lt;conversation_id&amp;gt;&lt;/code&gt; flag.&lt;/p&gt; &lt;p&gt;Give it a try and let me know what to improve!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/YassWorks/Ally"&gt;https://github.com/YassWorks/Ally&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YassinK97"&gt; /u/YassinK97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nyvinr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nyvinr/ally_finally_got_rag_everything_runs_local_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nyvinr/ally_finally_got_rag_everything_runs_local_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-05T18:11:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0a0gp</id>
    <title>What the point of Ollama? Need agent builder inside it , it's sole purpose to run when system on and work in background.</title>
    <updated>2025-10-07T09:28:19+00:00</updated>
    <author>
      <name>/u/WestMurky1658</name>
      <uri>https://old.reddit.com/user/WestMurky1658</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WestMurky1658"&gt; /u/WestMurky1658 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0a0gp/what_the_point_of_ollama_need_agent_builder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0a0gp/what_the_point_of_ollama_need_agent_builder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0a0gp/what_the_point_of_ollama_need_agent_builder/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T09:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzip8q</id>
    <title>What coding tools do you use with ollama?</title>
    <updated>2025-10-06T13:13:03+00:00</updated>
    <author>
      <name>/u/SeaBit7159</name>
      <uri>https://old.reddit.com/user/SeaBit7159</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I seek a tool to rapidly implement simple features. Since my computer can run LLMs locally, I want to test its limits.&lt;/p&gt; &lt;p&gt;Claude Code with Claude Code Router fails to modify files and codex seems to have a weak MCP service.&lt;/p&gt; &lt;p&gt;Previously, I used Trae and Kiro, but they can’t run with Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SeaBit7159"&gt; /u/SeaBit7159 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nzip8q/what_coding_tools_do_you_use_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nzip8q/what_coding_tools_do_you_use_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nzip8q/what_coding_tools_do_you_use_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-06T13:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzdnio</id>
    <title>I created an open-source Invisible AI Assistant called Pluely - now at 890+ GitHub stars. You can add and use Ollama for free. Better interface for all your works.</title>
    <updated>2025-10-06T08:39:21+00:00</updated>
    <author>
      <name>/u/iam-neighbour</name>
      <uri>https://old.reddit.com/user/iam-neighbour</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nzdnio/i_created_an_opensource_invisible_ai_assistant/"&gt; &lt;img alt="I created an open-source Invisible AI Assistant called Pluely - now at 890+ GitHub stars. You can add and use Ollama for free. Better interface for all your works." src="https://external-preview.redd.it/aGNzcTFhZTRkZ3RmMYn6P6th22FDriw78c6Xj5wjFaOcKQQ0FizRfMPuGFUP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=019a15ba0d0a6ab04bc4afffa7341ad159d4441b" title="I created an open-source Invisible AI Assistant called Pluely - now at 890+ GitHub stars. You can add and use Ollama for free. Better interface for all your works." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pluely is Your Invisible AI Assistant: Lightning-fast, privacy-first AI assistant that works seamlessly during meetings, interviews, and conversations without anyone knowing. Completely undetectable in video calls, screen shares. All your data is stored locally on your system. Pluely is designed with privacy as a priority, so no external calls are made to our servers. This applies to both free and Pro users.&lt;/p&gt; &lt;p&gt;By far pluely is the best invisible open-source ai assistant, compared to big firms like Cluely, interviewCoder or any.&lt;/p&gt; &lt;p&gt;all with: solo contribution, $0 funding, and endless nights.&lt;/p&gt; &lt;p&gt;Menu you need on your desktop:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;System audio capture&lt;/li&gt; &lt;li&gt;Microphone audio capture&lt;/li&gt; &lt;li&gt;Input for all your queries&lt;/li&gt; &lt;li&gt;Screenshots (auto/manual)&lt;/li&gt; &lt;li&gt;Attach images&lt;/li&gt; &lt;li&gt;History&lt;/li&gt; &lt;li&gt;Settings&lt;/li&gt; &lt;li&gt;Drag handle&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;On free plan: Pluely supports all major LLM providers just bring your own api key, you can also add your own custom providers with cURL commands, same for speech to text providers as well.&lt;/p&gt; &lt;p&gt;On Pro plan: Pluely now has 80+ premium AI models with instant access including with GPT-5 and many other openai models, One-click model switching, Advanced speech-to-text with highest accuracy, and generating system prompts with AI.&lt;/p&gt; &lt;p&gt;Downloads: &lt;a href="https://pluely.com/downloads"&gt;https://pluely.com/downloads&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://pluely.com"&gt;https://pluely.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/iamsrikanthnani/pluely"&gt;https://github.com/iamsrikanthnani/pluely&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know your experience, and how i can improve more. Features to add are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iam-neighbour"&gt; /u/iam-neighbour &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vpmva3e4dgtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nzdnio/i_created_an_opensource_invisible_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nzdnio/i_created_an_opensource_invisible_ai_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-06T08:39:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0fh7w</id>
    <title>Managing Prompts in Python</title>
    <updated>2025-10-07T13:56:36+00:00</updated>
    <author>
      <name>/u/Ultralytics_Burhan</name>
      <uri>https://old.reddit.com/user/Ultralytics_Burhan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of working with strings for prompts in Python code, so I put together a small package I'm calling &lt;code&gt;proompt&lt;/code&gt; to use more object oriented prompts. It's definitely opinionated, and I 'm sure not everyone will like it, but I'd appreciate anyone's feedback on it that tries it out (even if you don't like it).&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Burhan-Q/proompt"&gt;https://github.com/Burhan-Q/proompt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The package is mostly scaffolding to use with custom defined prompt classes, so it's not something fully featured, it's more of a boiler plate structure to use as a foundational interface. I've included some &lt;a href="https://github.com/Burhan-Q/proompt/tree/main/examples"&gt;examples&lt;/a&gt; in the repo to see how to use it and what the advantages are. The principle is kind of like Lego, building blocks to make something that suits your specific needs, using basic Python code.&lt;/p&gt; &lt;p&gt;As a quick overview, here are the interfaces provided:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Context&lt;/code&gt;: Container for runtime information, inherit into a custom concrete class to use the standard interface&lt;/p&gt; &lt;p&gt;&lt;code&gt;ToolContext&lt;/code&gt;: Container for tool calls (callable), inherits from &lt;code&gt;Context&lt;/code&gt; interface, helps with modularity.&lt;/p&gt; &lt;p&gt;&lt;code&gt;BaseProvider&lt;/code&gt;: Core interface for objects that inject information into prompts (I think of it as a data-provider), use to define a custom concrete class for adding information/data to prompts at runtime.&lt;/p&gt; &lt;p&gt;&lt;code&gt;PromptSection&lt;/code&gt;: Defines a subsection of the overall prompt and inherited into custom concrete class as needed, makes prompts a bit more modular since sections can be removed/added quickly; accepts &lt;code&gt;Context&lt;/code&gt;, &lt;code&gt;ToolContext&lt;/code&gt;, and/or &lt;code&gt;BaseProvider&lt;/code&gt; objects.&lt;/p&gt; &lt;p&gt;&lt;code&gt;BasePrompt&lt;/code&gt;: This is the full prompt interface, create a concrete class that inherits &lt;code&gt;BasePrompt&lt;/code&gt; to define your custom prompt, accepts any number of &lt;code&gt;PromptSection&lt;/code&gt; objects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ultralytics_Burhan"&gt; /u/Ultralytics_Burhan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0fh7w/managing_prompts_in_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0fh7w/managing_prompts_in_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0fh7w/managing_prompts_in_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T13:56:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0osx0</id>
    <title>Ollama Throttling</title>
    <updated>2025-10-07T19:34:41+00:00</updated>
    <author>
      <name>/u/geesuth</name>
      <uri>https://old.reddit.com/user/geesuth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o0osx0/ollama_throttling/"&gt; &lt;img alt="Ollama Throttling" src="https://b.thumbs.redditmedia.com/ILv_i23BOc1p51HT5LYnCSM6MEEmx_IWgggvX_FIrkE.jpg" title="Ollama Throttling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I Just rent a VPS I want to trying to host LLM my self, after I download ollama and start it, it's started when I trying to connect the server ollama from the n8n I saw the request coming to ollama server and this what I saw in console. it's take a long and not reply then I stop the request, &lt;/p&gt; &lt;p&gt;My VPS: AMD 16-Core Processor, 32 GB &lt;/p&gt; &lt;p&gt;What this exactly means? &lt;/p&gt; &lt;p&gt;Thank you. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6eb3o332rqtf1.png?width=457&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9996e39f5dd1948bcd09d3f2d5c17e7bddd505ba"&gt;https://preview.redd.it/6eb3o332rqtf1.png?width=457&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9996e39f5dd1948bcd09d3f2d5c17e7bddd505ba&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geesuth"&gt; /u/geesuth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0osx0/ollama_throttling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0osx0/ollama_throttling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0osx0/ollama_throttling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T19:34:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0jo95</id>
    <title>new "decentralised" ai art model, sounds like bs but does it actually works pretty well?</title>
    <updated>2025-10-07T16:31:54+00:00</updated>
    <author>
      <name>/u/Westlake029</name>
      <uri>https://old.reddit.com/user/Westlake029</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(not relevant to this sub but anyways) found this model called &lt;a href="https://github.com/bageldotcom/paris"&gt;paris&lt;/a&gt; today and i wont lie i was super skeptical at first. the whole &amp;quot;decentralised training&amp;quot; thing sounded more like some crypto marketing nonsense but after trying it i am kinda impressed by it. basically instead of training one huge model they trained 8 separate ones and use some router thing to pick which one to use (pretty smart). might sound weird but the results are legit better than i expected for something thats completely free not gonna lie, still prefer my midjourney subscription for serious stuff but for just messing around this is pretty solid. no rate limits, no watermarks, you just name it. just download and go.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Westlake029"&gt; /u/Westlake029 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0jo95/new_decentralised_ai_art_model_sounds_like_bs_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0jo95/new_decentralised_ai_art_model_sounds_like_bs_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0jo95/new_decentralised_ai_art_model_sounds_like_bs_but/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T16:31:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0h2av</id>
    <title>Proxmox or BareMetal to run Ollama?</title>
    <updated>2025-10-07T14:56:04+00:00</updated>
    <author>
      <name>/u/Wonderfullyboredme</name>
      <uri>https://old.reddit.com/user/Wonderfullyboredme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What’s the best option to run Ollama and LLMs long term?&lt;/p&gt; &lt;p&gt;Currently running llama on a LXC and then WebUI in another LXC but it some issues with the pass through getting snags when I update. So I wanted to know what’s the best practice? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wonderfullyboredme"&gt; /u/Wonderfullyboredme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0h2av/proxmox_or_baremetal_to_run_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0h2av/proxmox_or_baremetal_to_run_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0h2av/proxmox_or_baremetal_to_run_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T14:56:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0q2i9</id>
    <title>Ollama error</title>
    <updated>2025-10-07T20:21:25+00:00</updated>
    <author>
      <name>/u/lora_in_lichen_</name>
      <uri>https://old.reddit.com/user/lora_in_lichen_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o0q2i9/ollama_error/"&gt; &lt;img alt="Ollama error" src="https://preview.redd.it/bolbaeqj0rtf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4de6d184091067b21e1c783e32707c6d2c7340de" title="Ollama error" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I need to set up a model to use it with python script with ollama. After pulling cyberlis/saiga-mistral:7b-lora-q8_0 from ollama repository, i tried to run it? but got nothing as a respond. I checked ollama server output and saw the following: (picture)&lt;/p&gt; &lt;p&gt;I would be glad if anyone knows how to fix it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lora_in_lichen_"&gt; /u/lora_in_lichen_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bolbaeqj0rtf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0q2i9/ollama_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0q2i9/ollama_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T20:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0s6we</id>
    <title>MemoryLLM - A conversational AI system using Ollama with persistent memory capabilities.</title>
    <updated>2025-10-07T21:41:16+00:00</updated>
    <author>
      <name>/u/maranone5</name>
      <uri>https://old.reddit.com/user/maranone5</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o0s6we/memoryllm_a_conversational_ai_system_using_ollama/"&gt; &lt;img alt="MemoryLLM - A conversational AI system using Ollama with persistent memory capabilities." src="https://external-preview.redd.it/2KXnTfjRF7fYN7gSWUGWKW5ZuPplML-5c1Y2NAD-nBA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f38a9d6da1ab27070cb74725e72327b05b0f22f9" title="MemoryLLM - A conversational AI system using Ollama with persistent memory capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1o0s6we/video/t56td2xqertf1/player"&gt;https://reddit.com/link/1o0s6we/video/t56td2xqertf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi this is an experiment I've wanted to do for quite a while, It's been done again and again here but this is just a simple implementation.&lt;/p&gt; &lt;p&gt;The idea behind it is to have memory extracted during &amp;quot;idle&amp;quot; time between responses so there's both the memories for rag and the conversation history.&lt;/p&gt; &lt;p&gt;Then just a simple tweak of the system prompt to try and have the model adapt to your tone/mood nothing fancy just prompt.&lt;/p&gt; &lt;p&gt;Anyways it is an attempt of having a conversational local llm that with time get's to know you, with gemma3:4b not so much though :)&lt;/p&gt; &lt;p&gt;Then there is a very bad result of trying to have the llm have a perception of time elapsed between sessions (model gets timestamps and also current date-time)&lt;/p&gt; &lt;p&gt;Anyways here is the repo if someone wants to use or have a blueprint &lt;a href="https://github.com/maranone/MemoryLLM"&gt;MemoryLLM - Github&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maranone5"&gt; /u/maranone5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0s6we/memoryllm_a_conversational_ai_system_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0s6we/memoryllm_a_conversational_ai_system_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0s6we/memoryllm_a_conversational_ai_system_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T21:41:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0ivqz</id>
    <title>Hi folks, sorry for the self‑promo. I’ve built an open‑source project that could be useful to some of you</title>
    <updated>2025-10-07T16:02:24+00:00</updated>
    <author>
      <name>/u/panos_s_</name>
      <uri>https://old.reddit.com/user/panos_s_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o0ivqz/hi_folks_sorry_for_the_selfpromo_ive_built_an/"&gt; &lt;img alt="Hi folks, sorry for the self‑promo. I’ve built an open‑source project that could be useful to some of you" src="https://preview.redd.it/61oh2s9aqptf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a22f4bd151e1659d6c522d89b6519dd0237f6d1" title="Hi folks, sorry for the self‑promo. I’ve built an open‑source project that could be useful to some of you" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Web dashboard for NVIDIA GPUs with 30+ real-time metrics (utilisation, memory, temps, clocks, power, processes). Live charts over WebSockets, multi‑GPU support, and one‑command Docker deployment. No agents, minimal setup.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/psalias2006/gpu-hot"&gt;https://github.com/psalias2006/gpu-hot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I built it&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Wanted simple, real‑time visibility without standing up a full metrics stack.&lt;/li&gt; &lt;li&gt;Needed clear insight into temps, throttling, clocks, and active processes during GPU work.&lt;/li&gt; &lt;li&gt;A lightweight dashboard that’s easy to run at home or on a workstation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Polls nvidia-smi and streams 30+ metrics every ~2s via WebSockets.&lt;/li&gt; &lt;li&gt;Tracks per‑GPU utilization, memory (used/free/total), temps, power draw/limits, fan, clocks, PCIe, P‑State, encoder/decoder stats, driver/VBIOS, throttle status.&lt;/li&gt; &lt;li&gt;Shows active GPU processes with PIDs and memory usage.&lt;/li&gt; &lt;li&gt;Clean, responsive UI with live historical charts and basic stats (min/max/avg).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup (Docker)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/psalias2006/gpu-hot cd gpu-hot docker-compose up --build # open http://localhost:1312 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Looking for feedback&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panos_s_"&gt; /u/panos_s_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/61oh2s9aqptf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o0ivqz/hi_folks_sorry_for_the_selfpromo_ive_built_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o0ivqz/hi_folks_sorry_for_the_selfpromo_ive_built_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-07T16:02:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1o11o3u</id>
    <title>Meer CLI — an open-source Claude Code Alternative</title>
    <updated>2025-10-08T05:06:02+00:00</updated>
    <author>
      <name>/u/msaifeldeen</name>
      <uri>https://old.reddit.com/user/msaifeldeen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 I built Meer CLI — an open-source AI command-line tool that talks to any model (Ollama, OpenAI, Claude, etc.)&lt;/p&gt; &lt;p&gt;Hey folks 👋 I’ve been working on a developer-first CLI called Meer AI, now live at meerai.dev.&lt;/p&gt; &lt;p&gt;It’s designed for builders who love the terminal and want to use AI locally or remotely without switching between dashboards or UIs.&lt;/p&gt; &lt;p&gt;🧠 What it does • 🔗 Model-agnostic — works with Ollama, OpenAI, Claude, Gemini, etc. • 🧰 Plug-and-play CLI — run prompts, analyze code, or run agents directly from your terminal • 💾 Local memory — remembers your context across sessions • ⚙️ Configurable providers — choose or self-host your backend (e.g., Ollama on your own server) • 🌊 “Meer” = Sea — themed around ocean intelligence 🌊&lt;/p&gt; &lt;p&gt;💡 Why I built it&lt;/p&gt; &lt;p&gt;I wanted a simple way to unify my self-hosted models and APIs without constant context loss or UI juggling. The goal is to make AI interaction feel native to the command line.&lt;/p&gt; &lt;p&gt;🐳 Try it&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://meerai.dev"&gt;https://meerai.dev&lt;/a&gt; It’s early but functional — you can chat with models, run commands, and customize providers.&lt;/p&gt; &lt;p&gt;Would love feedback, ideas, or contributors who want to shape the future of CLI-based AI tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/msaifeldeen"&gt; /u/msaifeldeen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o11o3u/meer_cli_an_opensource_claude_code_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o11o3u/meer_cli_an_opensource_claude_code_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o11o3u/meer_cli_an_opensource_claude_code_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-08T05:06:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1qbwr</id>
    <title>Web Research Tool For Ollama - Does this exist?</title>
    <updated>2025-10-08T23:10:50+00:00</updated>
    <author>
      <name>/u/Comfortable_Ad_8117</name>
      <uri>https://old.reddit.com/user/Comfortable_Ad_8117</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an Etsy and EBay business where I sell antiques and I make a number of new listings a week for items that sometimes I don’t have much information on. Currently I use the Ollama with Open WebUi to help make my listings. &lt;/p&gt; &lt;p&gt;I know a lot of folks in this community have great open source projects and i was hoping one might be able to help me. &lt;/p&gt; &lt;p&gt;Ideally - I would like to feed an Ollama model a prompt with some basics about an item, maybe even an image or images and have it do the following - analyze the image - understand my brief description - web search (research) for similar items and lean about the item - produce a listing with: A brief description and history of the item - Features of the item (in bullet points) Size, color, material, condition, etc. - Keywords / tags - It’s guess on value (just for fun)&lt;/p&gt; &lt;p&gt;Right now Open Webui is OK at doing this, it struggles with the research, but usually generates a halfway decent listing&lt;/p&gt; &lt;p&gt;The biggest model I can run with acceptable performance is mistral-small3.2: 24B Oh and My Ollama is on its own (Windows) hardware so whatever app would have to be able to connect to it.&lt;/p&gt; &lt;p&gt;Any ideas would be appreciated. Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Ad_8117"&gt; /u/Comfortable_Ad_8117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o1qbwr/web_research_tool_for_ollama_does_this_exist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o1qbwr/web_research_tool_for_ollama_does_this_exist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o1qbwr/web_research_tool_for_ollama_does_this_exist/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-08T23:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o260nw</id>
    <title>llama3.1:8b-text-q8_0 allucinating</title>
    <updated>2025-10-09T13:31:29+00:00</updated>
    <author>
      <name>/u/doleron</name>
      <uri>https://old.reddit.com/user/doleron</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm just testing the model. However, in my first attempts, the model just allucinated. This is an example:&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;sander@sanderbox:~$ ollama run llama3.1:8b-text-q8_0&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; Olá, você pode falar português brasileiro?&lt;/p&gt; &lt;p&gt;If you can speak Portuguese, please leave a comment. This is the first ever YouTube video I’ve created in Portuguese and my first attempt at learning it! Thanks for watching!&lt;/p&gt; &lt;p&gt;P.S. My friend who is also learning the language told me that the expression “you’re welcome” in Brazilian Portuguese is literally translated as “my pleasure.” So, if you want to say &lt;/p&gt; &lt;p&gt;thank you after someone has thanked you, you would reply with “my pleasure,” not “you’re welcome.”&lt;/p&gt; &lt;p&gt;Tagged brazil, brasil, brazilian, brasilianos, capoeira, capoeirista, capoeiristas, daniel, fala, foreign, learning, language, learn, lingua, new, olympics, portuguese, rio, spanish&lt;br /&gt; Previous postHow I Learned to Read Faster and Remember What I Read (Part 2)&lt;br /&gt; Next postMy Favorite Books in 2016&lt;/p&gt; &lt;h1&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/h1&gt; &lt;p&gt;The answer I got from llama3.1 is way better:&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;sander@sanderbox:~$ ollama run llama3.1&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; Olá, você pode falar português brasileiro?&lt;/p&gt; &lt;p&gt;Sim, posso falar em português brasileiro. Como posso ajudar?&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;Can someone spot what I'm doing wrong here? Is not llama3.1:8b-text-q8_0 more likely to provide better responses than llama3.1?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doleron"&gt; /u/doleron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o260nw/llama318btextq8_0_allucinating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o260nw/llama318btextq8_0_allucinating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o260nw/llama318btextq8_0_allucinating/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-09T13:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2cywq</id>
    <title>I uploaded a Qwen3 distilled model to Ollama with the &lt;think&gt; tag working properly.</title>
    <updated>2025-10-09T17:57:05+00:00</updated>
    <author>
      <name>/u/East-Engineering-653</name>
      <uri>https://old.reddit.com/user/East-Engineering-653</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-FP32-i1-GGUF"&gt;https://huggingface.co/mradermacher/Qwen3-30B-A3B-Thinking-2507-Deepseek-v3.1-Distill-V2-FP32-i1-GGUF&lt;/a&gt;&lt;br /&gt; Using the same &lt;strong&gt;Modelfile&lt;/strong&gt; as the &lt;code&gt;qwen3:30b&lt;/code&gt; model, I fixed an issue where the &lt;strong&gt;&amp;lt;think&amp;gt; tag was not displayed&lt;/strong&gt; when running the model from the link above.&lt;br /&gt; The &lt;strong&gt;model itself is identical&lt;/strong&gt; — only the &lt;strong&gt;Modelfile&lt;/strong&gt; has been modified.&lt;br /&gt; &lt;a href="https://ollama.com/uaysk0327/qwen3-30b-distilled"&gt;https://ollama.com/uaysk0327/qwen3-30b-distilled&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/East-Engineering-653"&gt; /u/East-Engineering-653 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2cywq/i_uploaded_a_qwen3_distilled_model_to_ollama_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2cywq/i_uploaded_a_qwen3_distilled_model_to_ollama_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o2cywq/i_uploaded_a_qwen3_distilled_model_to_ollama_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-09T17:57:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o23zby</id>
    <title>Large RAM Macs</title>
    <updated>2025-10-09T11:59:29+00:00</updated>
    <author>
      <name>/u/Dependent_Price_1306</name>
      <uri>https://old.reddit.com/user/Dependent_Price_1306</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking at the possibility of getting a m4 pro mac mini with 64GB of unified ram (or wait for m5). I can reasonably run the Gemma 12:b on my m4 mba (well it generates text as fast as I read. Has anyone got any experience on a similar setup to what I envision?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dependent_Price_1306"&gt; /u/Dependent_Price_1306 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o23zby/large_ram_macs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o23zby/large_ram_macs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o23zby/large_ram_macs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-09T11:59:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2b3g4</id>
    <title>Moondream3 and Salesforce GTA-1 for UI grounding in computer-use agents</title>
    <updated>2025-10-09T16:46:40+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o2b3g4/moondream3_and_salesforce_gta1_for_ui_grounding/"&gt; &lt;img alt="Moondream3 and Salesforce GTA-1 for UI grounding in computer-use agents" src="https://external-preview.redd.it/NGg4OXdqcDA4NHVmMeWLBg1mAXwAigO5PzLkc4zOrANcV84YU3-0ubuxy7cx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76e8c14970d13001a77fa285bc7ac5697606974e" title="Moondream3 and Salesforce GTA-1 for UI grounding in computer-use agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Moondream3 and Salesforce GTA-1 for UI grounding in computer-use agents&lt;/p&gt; &lt;p&gt;The numbers on ScreenSpot-v2 benchmark:&lt;/p&gt; &lt;p&gt;GTA-1 leads in accuracy (96% vs 84%), but Moondream3 is 2x faster (1.04s vs 1.97s avg).&lt;/p&gt; &lt;p&gt;The median time gap is even bigger: 0.78s vs 1.96s - that's a 2.5x speedup.&lt;/p&gt; &lt;p&gt;Both models are open-weight, self-hostable and work out-of-the-box with Cua: &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Run the benchmark yourself: &lt;a href="https://docs.trycua.com/docs/agent-sdk/benchmarks/screenspot-v2"&gt;https://docs.trycua.com/docs/agent-sdk/benchmarks/screenspot-v2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b99iyk6184uf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2b3g4/moondream3_and_salesforce_gta1_for_ui_grounding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o2b3g4/moondream3_and_salesforce_gta1_for_ui_grounding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-09T16:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2myew</id>
    <title>ollama based aps</title>
    <updated>2025-10-10T00:47:29+00:00</updated>
    <author>
      <name>/u/Ok-Function-7101</name>
      <uri>https://old.reddit.com/user/Ok-Function-7101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;🤯 I Built a Massive Collection of Ollama-Powered Desktop Apps (From Private Chatbots to Mind Maps)&lt;/h1&gt; &lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I've been spending a ton of time building open-source desktop applications that are fully powered by &lt;strong&gt;Ollama&lt;/strong&gt; and local Large Language Models (LLMs). My goal is to showcase the power of local AI by creating a suite of tools that are &lt;strong&gt;private, secure, and run right on your machine.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I wanted to share my work with the Ollama community—maybe some of these will inspire your next project or become your new favorite tool! Everything is open source, mostly built with Python/PySide6, and designed to make local LLMs genuinely useful for everyday tasks.&lt;/p&gt; &lt;h1&gt;🛠️ Core Ollama-Powered Applications&lt;/h1&gt; &lt;p&gt;These are the projects I think will be most relevant and exciting to the local LLM community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cortex&lt;/strong&gt;: Your self-hosted, personal desktop chatbot. A private, secure, and highly responsive &lt;strong&gt;AI assistant&lt;/strong&gt; for seamless interaction with local LLMs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Autonomous-AI-Web-Search-Assistant&lt;/strong&gt;: An advanced AI research assistant that provides trustworthy, &lt;strong&gt;real-time answers from the web&lt;/strong&gt;. It uses local models to intelligently break down, search, and validate online sources.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fine-Tuned&lt;/strong&gt;: A desktop application designed to bridge the gap between &lt;strong&gt;model fine-tuning and a user-friendly graphical interface&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tree-Graph-MindMap&lt;/strong&gt;: Transforms raw, unstructured text into an &lt;strong&gt;interactive mind map&lt;/strong&gt;. It uses Ollama to intelligently structure the information.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ITT-Qwen&lt;/strong&gt;: A sleek desktop app for &lt;strong&gt;image-to-text analysis&lt;/strong&gt; powered by the Qwen Vision Language Model via Ollama, featuring custom UI and region selection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;File2MD&lt;/strong&gt;: A sleek desktop app that converts text to &lt;strong&gt;Markdown using private, local AI&lt;/strong&gt; with a live rendered preview. Your data stays yours!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;genisis-mini&lt;/strong&gt;: A powerful tool for &lt;strong&gt;generating structured data&lt;/strong&gt; (e.g., synthetic data) for educational purposes or fine-tuning smaller models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;clarity&lt;/strong&gt;: A sophisticated desktop application designed for &lt;strong&gt;in-depth text analysis&lt;/strong&gt; (summaries, structural breakdowns) leveraging LLMs via Ollama.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local-Deepseek-R1&lt;/strong&gt;: A modern desktop interface for local language models through Ollama, featuring persistent chat history and real-time model switching.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;👉 Where to find them&lt;/h1&gt; &lt;p&gt;You can check out all the repos on my GitHub profile: Link - &lt;a href="https://github.com/dovvnloading"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think! Which one are you trying first?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Function-7101"&gt; /u/Ok-Function-7101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2myew/ollama_based_aps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2myew/ollama_based_aps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o2myew/ollama_based_aps/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-10T00:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2a5i5</id>
    <title>Nanocoder Continues to Grow - A Small Update</title>
    <updated>2025-10-09T16:11:30+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1o2a5i5/nanocoder_continues_to_grow_a_small_update/"&gt; &lt;img alt="Nanocoder Continues to Grow - A Small Update" src="https://external-preview.redd.it/bnZiODl4bGx6M3VmMUcr7w_XMPgkCFzq8MGyFqa-tX1dbJAve2bfR2mbBlv_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cf796ece074ccffbbde5b5a24880c1ead16996b" title="Nanocoder Continues to Grow - A Small Update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I just wanted to share an update post on Nanocoder, the open-source, open-community coding CLI.&lt;/p&gt; &lt;p&gt;Since the last post a couple weeks ago we've surpassed 500 GitHub stars which is epic and I can't thank everyone enough - I know it's still small but we're growing everyday!&lt;/p&gt; &lt;p&gt;The community, the amount of contributors and ideas flowing has also been beyond amazing as we aim to build a coding tool that truly takes advantage of local-first technology and is built for the community.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Here are some highlights of what the last couple of weeks has entailed:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;- Nanocoder has been moved to be under the Nano Collective org on GitHub. This is a new collective which I hope will continue to foster people wanting to build and grow local-first and open-source AI tools for the community whether that be Nanocoder or other packages and software.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Highlight of Features Added:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- A models database, run &lt;code&gt;/recommendations&lt;/code&gt; to let Nanocoder scan your system and make recommendations on models to have the best experience with.&lt;/p&gt; &lt;p&gt;- New agent tools: &lt;code&gt;web_search&lt;/code&gt;, &lt;code&gt;fetch_url&lt;/code&gt; and &lt;code&gt;search_files&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;- Modes, run Nanocoder on normal, auto-accept or planning mode.&lt;/p&gt; &lt;p&gt;- &lt;code&gt;/init&lt;/code&gt; to generate an &lt;code&gt;AGENTS.md&lt;/code&gt; file for your project.&lt;/p&gt; &lt;p&gt;- Lots more.&lt;/p&gt; &lt;p&gt;We've also been making a lot of progress in agent frameworks to offset tasks to tiny models to keep things local and private as much as possible. More on this soon.&lt;/p&gt; &lt;p&gt;Thank you to everyone that is getting involved and supporting the project. It continues to be very early days but we're rapidly taking on feedback and trying to improve the software 😊&lt;/p&gt; &lt;p&gt;That being said, any help within any domain is appreciated and welcomed.&lt;/p&gt; &lt;p&gt;If you want to get involved the links are below.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Link&lt;/strong&gt;: &lt;a href="https://github.com/Nano-Collective/nanocoder"&gt;https://github.com/Nano-Collective/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord Link&lt;/strong&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n0huywllz3uf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1o2a5i5/nanocoder_continues_to_grow_a_small_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1o2a5i5/nanocoder_continues_to_grow_a_small_update/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-10-09T16:11:30+00:00</published>
  </entry>
</feed>
