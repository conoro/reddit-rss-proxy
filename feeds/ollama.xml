<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-18T16:25:15+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ngrits</id>
    <title>Comment utiliser le GPU ?</title>
    <updated>2025-09-14T13:37:17+00:00</updated>
    <author>
      <name>/u/Maleficent-Hotel8207</name>
      <uri>https://old.reddit.com/user/Maleficent-Hotel8207</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ngrits/comment_utiliser_le_gpu/"&gt; &lt;img alt="Comment utiliser le GPU ?" src="https://preview.redd.it/xb4qwochv4pf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e16ed4fdbfab7be2cfc1832b6e804dfecad39491" title="Comment utiliser le GPU ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Comment utiliser le GPU sur ollama j’ai une GTX 1050 et je n’arrive pas à l’utiliser pour exécuter des modèles &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Hotel8207"&gt; /u/Maleficent-Hotel8207 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xb4qwochv4pf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ngrits/comment_utiliser_le_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ngrits/comment_utiliser_le_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-14T13:37:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfytlv</id>
    <title>Gemma 3 12B versus GPT 5 Nano</title>
    <updated>2025-09-13T14:25:54+00:00</updated>
    <author>
      <name>/u/fundal_alb</name>
      <uri>https://old.reddit.com/user/fundal_alb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is just me or that Gemma version is better or equal to GPT 5 Nano?&lt;/p&gt; &lt;p&gt;In my case...:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nano is responding with the first token after 6-10 seconds&lt;/li&gt; &lt;li&gt;Gemma has better language understanding than 5 Nano.&lt;/li&gt; &lt;li&gt;Gemma is structuring the output in a more readable way&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fundal_alb"&gt; /u/fundal_alb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfytlv/gemma_3_12b_versus_gpt_5_nano/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nfytlv/gemma_3_12b_versus_gpt_5_nano/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nfytlv/gemma_3_12b_versus_gpt_5_nano/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-13T14:25:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng9tdq</id>
    <title>Ollama start all models on CPU instead GPU [Arch/Nvidia]</title>
    <updated>2025-09-13T21:52:04+00:00</updated>
    <author>
      <name>/u/MrDoc79</name>
      <uri>https://old.reddit.com/user/MrDoc79</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ng9tdq/ollama_start_all_models_on_cpu_instead_gpu/"&gt; &lt;img alt="Ollama start all models on CPU instead GPU [Arch/Nvidia]" src="https://b.thumbs.redditmedia.com/t2wX7z3e3ylFI8LXPaCJf9wbkIBJekACEeCKiiZ4G4Y.jpg" title="Ollama start all models on CPU instead GPU [Arch/Nvidia]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Idk why, but all models, what i started, are running on CPU, and, had small speed for generate answer. However, nvidia-smi works, and driver is available. I'm on EndeavourOS (Arch-based), with RTX 2060 on 6gb. All screenshots pinned&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrDoc79"&gt; /u/MrDoc79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ng9tdq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ng9tdq/ollama_start_all_models_on_cpu_instead_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ng9tdq/ollama_start_all_models_on_cpu_instead_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-13T21:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhhdac</id>
    <title>GPT-OSS-120B Performance Benchmarks and Provider Trade-Offs</title>
    <updated>2025-09-15T09:40:37+00:00</updated>
    <author>
      <name>/u/Sumanth_077</name>
      <uri>https://old.reddit.com/user/Sumanth_077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking at the latest Artificial Analysis benchmarks for GPT-OSS-120B and noticed some interesting differences between providers, especially for those using it in production.&lt;/p&gt; &lt;p&gt;Time to first token (TTFT) ranges from under 0.3 seconds to nearly a second depending on the provider. That can be significant for applications where responsiveness matters. Throughput also varies, from under 200 tokens per second to over 400.&lt;/p&gt; &lt;p&gt;Cost per million tokens adds another consideration. Some providers offer high throughput at a higher cost, while others like CompactifAI are cheaper but very slower. Clarifai, for example, delivers low TTFT, solid throughput, and relatively low cost.&lt;/p&gt; &lt;p&gt;The takeaway is that no single metric tells the full story. Latency affects responsiveness, throughput matters for larger tasks, and cost impacts scaling. The best provider depends on which of these factors is most important for your use case.&lt;/p&gt; &lt;p&gt;For those using GPT-OSS-120B in production, which of these do you find the hardest to manage: step latency, throughput, or cost?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/92e9yu2ctapf1.png?width=3408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ff0bd76c8ac202aad4c9e453dc975c7e67cfa6d7"&gt;https://preview.redd.it/92e9yu2ctapf1.png?width=3408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ff0bd76c8ac202aad4c9e453dc975c7e67cfa6d7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sumanth_077"&gt; /u/Sumanth_077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhhdac/gptoss120b_performance_benchmarks_and_provider/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhhdac/gptoss120b_performance_benchmarks_and_provider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nhhdac/gptoss120b_performance_benchmarks_and_provider/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-15T09:40:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhl7h0</id>
    <title>Advice on building an enterprise-scale, privacy-first conversational assistant (local LLMs with Ollama vs fine-tuning)</title>
    <updated>2025-09-15T13:01:05+00:00</updated>
    <author>
      <name>/u/jamalhassouni</name>
      <uri>https://old.reddit.com/user/jamalhassouni</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m working on a project to design a &lt;strong&gt;conversational AI assistant for employee well-being and productivity&lt;/strong&gt; inside a large enterprise (think thousands of staff, high compliance/security requirements). The assistant should provide personalized nudges, lightweight recommendations, and track anonymized engagement data — without sending sensitive data outside the organization.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key constraints:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Must be &lt;strong&gt;privacy-first&lt;/strong&gt; (local deployment or private cloud — no SaaS APIs).&lt;/li&gt; &lt;li&gt;Needs to support &lt;strong&gt;personalized recommendations&lt;/strong&gt; and &lt;strong&gt;ongoing employee state tracking&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Must handle &lt;strong&gt;enterprise scale&lt;/strong&gt; (hundreds–thousands of concurrent users).&lt;/li&gt; &lt;li&gt;Regulatory requirements: &lt;strong&gt;PII protection, anonymization, auditability&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I’d love advice on:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Local LLM deployment&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Is using &lt;strong&gt;Ollama with models like Gemma/MedGemma&lt;/strong&gt; a solid foundation for production at enterprise scale?&lt;/li&gt; &lt;li&gt;What are the pros/cons of Ollama vs more MLOps-oriented solutions (vLLM, TGI, LM Studio, custom Dockerized serving)?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model strategy: RAG vs fine-tuning&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;For delivering contextual, evolving guidance: would you start with &lt;strong&gt;RAG (vector DB + retrieval)&lt;/strong&gt; or jump straight into &lt;strong&gt;fine-tuning a domain model&lt;/strong&gt;?&lt;/li&gt; &lt;li&gt;Any rule of thumb on when fine-tuning becomes necessary in real-world enterprise use cases?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model choice&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Experiences with &lt;strong&gt;Gemma/MedGemma&lt;/strong&gt; or other open-source models for well-being / health-adjacent guidance?&lt;/li&gt; &lt;li&gt;Alternatives you’d recommend (Mistral, LLaMA 3, Phi-3, Qwen, etc.) in terms of reasoning, safety, and multilingual support?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Infrastructure &amp;amp; scaling&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Minimum GPU/CPU/RAM targets to support &lt;strong&gt;hundreds of concurrent chats&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Vector DB choices: FAISS, Milvus, Weaviate, Pinecone — what works best at enterprise scale?&lt;/li&gt; &lt;li&gt;Monitoring, evaluation, and safe deployment patterns (A/B testing, hallucination mitigation, guardrails).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Security &amp;amp; compliance&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Best practices to prevent &lt;strong&gt;PII leakage into embeddings/prompts&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Recommended architectures for &lt;strong&gt;GDPR/HIPAA-like compliance&lt;/strong&gt; when dealing with well-being data.&lt;/li&gt; &lt;li&gt;Any proven strategies to balance personalization with strict privacy requirements?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation &amp;amp; KPIs&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;How to measure assistant effectiveness (safety checks, employee satisfaction, retention impact).&lt;/li&gt; &lt;li&gt;Tooling for anonymized analytics dashboards at the org level.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jamalhassouni"&gt; /u/jamalhassouni &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhl7h0/advice_on_building_an_enterprisescale/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhl7h0/advice_on_building_an_enterprisescale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nhl7h0/advice_on_building_an_enterprisescale/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-15T13:01:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhkjk8</id>
    <title>What are the ways to use Ollama 120B without breaking the bank?</title>
    <updated>2025-09-15T12:32:02+00:00</updated>
    <author>
      <name>/u/Significant_Loss_541</name>
      <uri>https://old.reddit.com/user/Significant_Loss_541</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hello, i have been looking into running the ollama 120b model for a project, but honestly the hardware/hosting side looks kinda tough to setup for me. i really dont want to set up big servers or spend a lot initially just to try it out.&lt;/p&gt; &lt;p&gt;are there any ways people here are running it cheaper? like cloud setups, colab hacks, lighter quantized versions, or anything similar?&lt;/p&gt; &lt;p&gt;also curious if it even makes sense to skip self-hosting and just use a service that already runs it (saw deepinfra has it with an api, and it’s way less than openai prices but still not free). has anyone tried going that route vs rolling your own?&lt;/p&gt; &lt;p&gt;what’s the most practical way for someone who doesn’t want to melt their credit card on gpu rentals?&lt;/p&gt; &lt;p&gt;thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Loss_541"&gt; /u/Significant_Loss_541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhkjk8/what_are_the_ways_to_use_ollama_120b_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhkjk8/what_are_the_ways_to_use_ollama_120b_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nhkjk8/what_are_the_ways_to_use_ollama_120b_without/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-15T12:32:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1niendu</id>
    <title>how to make custom chatbot for my website</title>
    <updated>2025-09-16T11:19:31+00:00</updated>
    <author>
      <name>/u/Comfortable-Fan-8931</name>
      <uri>https://old.reddit.com/user/Comfortable-Fan-8931</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i am student ,&lt;br /&gt; how to make custom chatbot for my website . &lt;/p&gt; &lt;p&gt;when i ask question related to my website then, chatbot gives answer .&lt;br /&gt; And please suggest best approach and steps to create this chatbot&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Fan-8931"&gt; /u/Comfortable-Fan-8931 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1niendu/how_to_make_custom_chatbot_for_my_website/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1niendu/how_to_make_custom_chatbot_for_my_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1niendu/how_to_make_custom_chatbot_for_my_website/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T11:19:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhiixf</id>
    <title>Was working in RAG recently got to know how well Gemma3 4B performs</title>
    <updated>2025-09-15T10:49:31+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nhiixf/was_working_in_rag_recently_got_to_know_how_well/"&gt; &lt;img alt="Was working in RAG recently got to know how well Gemma3 4B performs" src="https://preview.redd.it/91u5300g6bpf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=fc7c57a6e1b897ac550462d08b77359bb65c2b95" title="Was working in RAG recently got to know how well Gemma3 4B performs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got this working and had to share because wtf, this tiny model is way better than expected.&lt;/p&gt; &lt;p&gt;Built a RAG system that renders docs as a knowledge graph you can actually navigate through. Using Gemma3 4B via Ollama and honestly shocked at how well it clusters related content.&lt;/p&gt; &lt;p&gt;The crazy part? Sub-200ms responses and the semantic relationships actually make sense. Running smooth on small GPU&lt;/p&gt; &lt;p&gt;Anyone else trying local models for RAG? Kinda nice not sending everything to OpenAI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/91u5300g6bpf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nhiixf/was_working_in_rag_recently_got_to_know_how_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nhiixf/was_working_in_rag_recently_got_to_know_how_well/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-15T10:49:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nisb0j</id>
    <title>A PHP Proxy script to work with Ollama from HTTPS apps</title>
    <updated>2025-09-16T20:09:35+00:00</updated>
    <author>
      <name>/u/New_Cranberry_6451</name>
      <uri>https://old.reddit.com/user/New_Cranberry_6451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Ollama friends!&lt;/p&gt; &lt;p&gt;I have written a small PHP script that allows you to have a Proxy to work with your Ollama API from web apps under HTTPS. I probably reinvented a wheel here, but the thing is that I wasn't able to find a small, dependency free, PHP script that did this job for me. Others I tried couldn't handle streaming for example, or had too many things I don't need for my use case. That's why I ended up with this and as I wished to find something similar when I needed it, I am sharing it with you hoping someone finds it useful.&lt;/p&gt; &lt;p&gt;All feedback is welcome, let me know if there's another proxy option better than this solution (I am sure it will) or if you find any security concerns. This is not intended to work in production, it's just a straight-forward script that does the job. &lt;/p&gt; &lt;p&gt;Repo here: &lt;a href="https://github.com/aritzolaba/ow-proxy-ollama"&gt;OllamaProxy on Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope it helps someone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Cranberry_6451"&gt; /u/New_Cranberry_6451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nisb0j/a_php_proxy_script_to_work_with_ollama_from_https/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nisb0j/a_php_proxy_script_to_work_with_ollama_from_https/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nisb0j/a_php_proxy_script_to_work_with_ollama_from_https/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T20:09:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj5z71</id>
    <title>Just downloaded Ollama. Complete beginner. What all do I need to know?</title>
    <updated>2025-09-17T06:48:58+00:00</updated>
    <author>
      <name>/u/ElectronicPlankton12</name>
      <uri>https://old.reddit.com/user/ElectronicPlankton12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what settings and all that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectronicPlankton12"&gt; /u/ElectronicPlankton12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nj5z71/just_downloaded_ollama_complete_beginner_what_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nj5z71/just_downloaded_ollama_complete_beginner_what_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nj5z71/just_downloaded_ollama_complete_beginner_what_all/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T06:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nicqtd</id>
    <title>Need a simple UI/UX for chat (similar to OpenAI Chatgpt) using Ollama</title>
    <updated>2025-09-16T09:28:13+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Appreciate any advice. I ask chatgpt to create but not getting the right look. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nicqtd/need_a_simple_uiux_for_chat_similar_to_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nicqtd/need_a_simple_uiux_for_chat_similar_to_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nicqtd/need_a_simple_uiux_for_chat_similar_to_openai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T09:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nisqoc</id>
    <title>[Release] Doc Builder (MD + PDF) v1.7 for Open WebUI Store – clean Markdown + styled PDF exports</title>
    <updated>2025-09-16T20:25:54+00:00</updated>
    <author>
      <name>/u/Nefhis</name>
      <uri>https://old.reddit.com/user/Nefhis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nisqoc/release_doc_builder_md_pdf_v17_for_open_webui/"&gt; &lt;img alt="[Release] Doc Builder (MD + PDF) v1.7 for Open WebUI Store – clean Markdown + styled PDF exports" src="https://b.thumbs.redditmedia.com/LmvS5C2MZB0TEDMGq5XmzAqMtL2As73VlizrAZO8S2Q.jpg" title="[Release] Doc Builder (MD + PDF) v1.7 for Open WebUI Store – clean Markdown + styled PDF exports" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nefhis"&gt; /u/Nefhis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenWebUI/comments/1nismqj/release_doc_builder_md_pdf_v17_for_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nisqoc/release_doc_builder_md_pdf_v17_for_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nisqoc/release_doc_builder_md_pdf_v17_for_open_webui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T20:25:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nizasb</id>
    <title>computron_9000</title>
    <updated>2025-09-17T00:59:45+00:00</updated>
    <author>
      <name>/u/larz01larz</name>
      <uri>https://old.reddit.com/user/larz01larz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nizasb/computron_9000/"&gt; &lt;img alt="computron_9000" src="https://external-preview.redd.it/rx0LgLIbfwDLaMi2xA7oRUXabQDybdqzNQj1X9cd914.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b126cb1c8b3bcdc563ad80a3688099487c5fad9f" title="computron_9000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Still working on &lt;a href="https://github.com/lefoulkrod/computron_9000"&gt;computron&lt;/a&gt;. It's not really just a chat UI on top of ollama, althought it does do that. It is more like my own personal AI assistant. I've been adding a bunch of tools and agents to it so it can do web research, write and run code, execute shell commands. It's kind of big heap of agents and tools but I'm slowly stitching it together into something useful. Take a look and if interested in contributing feel free to submit a PR.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mih4jhyzimpf1.png?width=3744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc36c55b82bae69f66cad127da1441c7bcb8bbd1"&gt;https://preview.redd.it/mih4jhyzimpf1.png?width=3744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc36c55b82bae69f66cad127da1441c7bcb8bbd1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/larz01larz"&gt; /u/larz01larz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nizasb/computron_9000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nizasb/computron_9000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nizasb/computron_9000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T00:59:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1niknpf</id>
    <title>Fix AI pipeline bugs before they hit your local stack: a semantic firewall + grandma clinic (beginner friendly, MIT)</title>
    <updated>2025-09-16T15:28:20+00:00</updated>
    <author>
      <name>/u/onestardao</name>
      <uri>https://old.reddit.com/user/onestardao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1niknpf/fix_ai_pipeline_bugs_before_they_hit_your_local/"&gt; &lt;img alt="Fix AI pipeline bugs before they hit your local stack: a semantic firewall + grandma clinic (beginner friendly, MIT)" src="https://external-preview.redd.it/HmNdz-XJDpdQ-3JLiqDAFOCdjg6rPrU9CAiH44bGSXo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6cd3a31987288764c4bae70846352bac786ad4a5" title="Fix AI pipeline bugs before they hit your local stack: a semantic firewall + grandma clinic (beginner friendly, MIT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;last time i shared the 16-problem checklist for AI failures. many here are pros running ollama with custom RAG, agents, or tool flows. today is the beginner-friendly version. same math and guardrails, but explained like you’re showing a junior teammate. the idea is simple: install a tiny “semantic firewall” that runs before output, so unstable answers never reach your pipeline.&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;why this matters&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;most stacks fix things after generation. model talks, you add a reranker, a regex, a few if-elses. the same bug returns in a new shape.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;a semantic firewall flips the order. it inspects meaning first. if the state is unstable it loops, narrows, or resets. only a stable state is allowed to speak. once a failure mode is mapped, you fix it once and it stays fixed.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;what “before vs after” feels like&lt;/p&gt; &lt;ul&gt; &lt;li&gt;after: firefighting, patch debt, fragile flows.&lt;/li&gt; &lt;li&gt;before: a gate that checks drift against the question, demands a source card, and blocks ungrounded text. fewer retries. fewer wrong triggers. cleaner audits.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;copy-paste “grandma gate” into your ollama prompt or system section put this at the top of your system prompt or prepend to each user question. it’s provider-agnostic and text-only.&lt;/p&gt; &lt;p&gt;``` grandma gate (pre-output):&lt;/p&gt; &lt;p&gt;1) show a source card before any answer: - doc or dataset name (id ok) - exact location (page or lines, or section id) - one sentence why this matches the question&lt;/p&gt; &lt;p&gt;2) mid-chain checkpoint: - if reasoning drifts, reset once and try a narrower route&lt;/p&gt; &lt;p&gt;3) only continue when both hold: - meaning matches clearly (small drift) - coverage is high (most of the answer is supported by the citation)&lt;/p&gt; &lt;p&gt;4) if either fails: - do not answer - ask me to pick a file, a section, or to narrow the question ```&lt;/p&gt; &lt;p&gt;ollama quick-start: 3 ways&lt;/p&gt; &lt;p&gt;way 1: Modelfile system policy&lt;/p&gt; &lt;p&gt;``` FROM llama3 SYSTEM &amp;quot;&amp;quot;&amp;quot; you are behind a semantic firewall. &amp;lt;paste the grandma gate here&amp;gt; when answering, first print:&lt;/p&gt; &lt;p&gt;source: doc: &amp;lt;name or id&amp;gt; location: &amp;lt;page/lines/section&amp;gt; why this matches: &amp;lt;one sentence&amp;gt;&lt;/p&gt; &lt;p&gt;answer: &amp;lt;keep it inside the cited scope.&amp;gt; &amp;quot;&amp;quot;&amp;quot; PARAMETER temperature 0.3 ```&lt;/p&gt; &lt;p&gt;then:&lt;/p&gt; &lt;p&gt;&lt;code&gt; ollama create safe-llama -f Modelfile ollama run safe-llama &lt;/code&gt;&lt;/p&gt; &lt;p&gt;way 2: one-off CLI with a prelude&lt;/p&gt; &lt;p&gt;&lt;code&gt; PRELUDE=&amp;quot;&amp;lt;&amp;lt;grandma gate text here&amp;gt;&amp;gt;&amp;quot; QUESTION=&amp;quot;summarize section 2 of our faq about refunds&amp;quot; echo -e &amp;quot;$PRELUDE\n\n$QUESTION&amp;quot; | ollama run llama3 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;way 3: local HTTP call&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash curl http://localhost:11434/api/generate \ -d '{ &amp;quot;model&amp;quot;:&amp;quot;llama3&amp;quot;, &amp;quot;prompt&amp;quot;:&amp;quot;'&amp;quot;$(printf &amp;quot;%s\n\n%s&amp;quot; &amp;quot;$PRELUDE&amp;quot; &amp;quot;extract the steps from policy v3, section refunds&amp;quot;)&amp;quot;'&amp;quot;, &amp;quot;options&amp;quot;:{&amp;quot;temperature&amp;quot;:0.3} }' &lt;/code&gt;&lt;/p&gt; &lt;p&gt;rag and embeddings: 3 sanity checks for ollama users&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;dimensions and normalization: do not mix 384-dim and 768-dim vectors. if you swap embed models, rebuild the store. normalize vectors consistently.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;chunk→embed contract: keep code, tables, and headers as blocks. do not flatten to prose. store chunk ids and line ranges so your source card can point back.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;citation first: require the card to print before prose. if you only see text, block the automation step and ask the user to pick a section. —&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;fast “before” recipes that work well with ollama&lt;/p&gt; &lt;p&gt;recipe a: card-first filter for shell pipelines&lt;/p&gt; &lt;ul&gt; &lt;li&gt;many people pipe ollama into jq, awk, or a webhook. add a tiny gate.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt; ollama run safe-llama -p &amp;quot;$INPUT&amp;quot; | awk ' BEGIN{card=0} /^source:/ {card=1} END{ if(card==0) { exit 42 } } ' || { echo &amp;quot;blocked: missing source card&amp;quot;; exit 1; } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;recipe b: warm the model to avoid first-call collapse&lt;/p&gt; &lt;ul&gt; &lt;li&gt;first request after load often looks confident but wrong. warm it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;``` ollama run llama3 &amp;quot;ready check. say ok.&amp;quot; &amp;gt;/dev/null&lt;/p&gt; &lt;h1&gt;or keep the model warm for 5 minutes&lt;/h1&gt; &lt;p&gt;ollama run --keep-alive 5m llama3 &amp;quot;ready check&amp;quot; &amp;gt;/dev/null ```&lt;/p&gt; &lt;p&gt;recipe c: small canary before production action&lt;/p&gt; &lt;ul&gt; &lt;li&gt;before the agent writes to disk or calls a tool, force a tiny canary question and verify the card prints a real section. if not, stop the run.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;common pipeline failures this firewall prevents&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;hallucination and chunk drift: pretty cosine neighbor, wrong meaning. the gate demands the card and rejects the output if the card is off.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;interpretation collapse: the chunk is correct, the reading is wrong. mid-chain checkpoint catches drift and resets once.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;debugging black box: answers with no trace. the card glues answer to a real location, so you can redo and audit.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;bootstrap ordering: calling tools or indexes before they are warm. run a warmup, then allow speech.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;pre-deploy collapse: empty vector store or wrong env vars on first call. verify store size and secrets before the agent speaks.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;acceptance targets, so you know it is working&lt;/p&gt; &lt;ul&gt; &lt;li&gt;drift small. the cited text clearly belongs to the question.&lt;/li&gt; &lt;li&gt;coverage high. most of the answer is inside the cited scope.&lt;/li&gt; &lt;li&gt;card first. proof appears before prose.&lt;/li&gt; &lt;li&gt;hold across two paraphrases. if it swings, keep the gate closed and ask the user to pick a file or narrow scope.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;mini before/after demo you can try now&lt;/p&gt; &lt;ol&gt; &lt;li&gt;ask normally: “what are the refund steps” against your policy doc. watch it improvise or hedge.&lt;/li&gt; &lt;li&gt;ask with the gate + “card first.” you should see a doc id, section, and a one-sentence why. if the citation is wrong, the model must refuse and ask for a narrower query or a file pick. result: fewer wrong runs get past your terminal, scripts, or webhooks.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;faq&lt;/p&gt; &lt;p&gt;q: do i need a library or sdk a: no. it is a text policy plus tiny filters. works in ollama, claude, openrouter, and inside automations.&lt;/p&gt; &lt;p&gt;q: will this slow me down a: it usually speeds you up. you skip broken runs early instead of repairing them downstream.&lt;/p&gt; &lt;p&gt;q: can i keep creative formatting a: yes. ground the factual part first with a real card, then allow formatting. for freeform tasks, ask for a small example before the full answer.&lt;/p&gt; &lt;p&gt;q: what if the model keeps saying “unstable” a: your question is too broad or your store lacks the right chunk. pick a file and section, or ingest the missing page. once the card matches, the flow unlocks.&lt;/p&gt; &lt;p&gt;q: where is the plain language guide a: “Grandma Clinic” explains the 16 common failure modes with tiny fixes. beginner friendly.&lt;/p&gt; &lt;p&gt;closing if mods limit links, reply “drop one-file” and i’ll paste a single text you can save as a Modelfile or prelude. if you post a screenshot of a failure, i can map which failure number it is and give the smallest patch that fits an ollama stack.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onestardao"&gt; /u/onestardao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/onestardao/WFGY/blob/main/ProblemMap/GrandmaClinic/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1niknpf/fix_ai_pipeline_bugs_before_they_hit_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1niknpf/fix_ai_pipeline_bugs_before_they_hit_your_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-16T15:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj75jk</id>
    <title>Can I use Cursor Agent (or similar) with a local LLM setup (8B / 13B)?</title>
    <updated>2025-09-17T08:05:00+00:00</updated>
    <author>
      <name>/u/BudgetPurple3002</name>
      <uri>https://old.reddit.com/user/BudgetPurple3002</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BudgetPurple3002"&gt; /u/BudgetPurple3002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1nj758c/can_i_use_cursor_agent_or_similar_with_a_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nj75jk/can_i_use_cursor_agent_or_similar_with_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nj75jk/can_i_use_cursor_agent_or_similar_with_a_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T08:05:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj9o91</id>
    <title>Autonomous Pen testing AI.</title>
    <updated>2025-09-17T10:43:38+00:00</updated>
    <author>
      <name>/u/SkillPatient6465</name>
      <uri>https://old.reddit.com/user/SkillPatient6465</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkillPatient6465"&gt; /u/SkillPatient6465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AI_Agents/comments/1nj98dl/autonomous_pen_testing_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nj9o91/autonomous_pen_testing_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nj9o91/autonomous_pen_testing_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T10:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj3s85</id>
    <title>ArchGW 0.3.12 🚀 Model aliases: allow clients to use friendly, semantic names and swap out underlying models without changing application code.</title>
    <updated>2025-09-17T04:39:53+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nj3s85/archgw_0312_model_aliases_allow_clients_to_use/"&gt; &lt;img alt="ArchGW 0.3.12 🚀 Model aliases: allow clients to use friendly, semantic names and swap out underlying models without changing application code." src="https://preview.redd.it/igehvjyamnpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db6eae6aeac91ce9c0e5bfab3913e58f5380f1d9" title="ArchGW 0.3.12 🚀 Model aliases: allow clients to use friendly, semantic names and swap out underlying models without changing application code." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I added this lightweight abstraction to &lt;a href="https://github.com/katanemo/archgw"&gt;archgw&lt;/a&gt; to decouple app code from specific model names. Instead of sprinkling hardcoded model names like&lt;code&gt;gpt-4o-mini&lt;/code&gt; or &lt;code&gt;llama3.2&lt;/code&gt; everywhere, you point to an &lt;em&gt;alias&lt;/em&gt; that encodes intent, and allows you to test new models, swap out the config safely without having to do codewide search/replace every time you want to experiment with a new model or version.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;arch.summarize.v1 → cheap/fast summarization arch.v1 → default “latest” general-purpose model arch.reasoning.v1 → heavier reasoning &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The app calls the alias, not the vendor. Swap the model in config, and the entire system updates without touching code. Of course, you would want to use models compatible. Meaning if you map an embedding model to an alias, when the application expects a chat model, it won't be a good day.&lt;/p&gt; &lt;p&gt;Where are we headed with this...&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Guardrails -&amp;gt; Apply safety, cost, or latency rules at the alias level: arch.reasoning.v1:&lt;/p&gt; &lt;p&gt;arch.reasoning.v1: target: gpt-oss-120b guardrails: max_latency: 5s block_categories: [“jailbreak”, “PII”]&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Fallbacks -&amp;gt; Provide a chain if a model fails or hits quota:&lt;/p&gt; &lt;p&gt;arch.summarize.v1: target: gpt-4o-mini fallback: llama3.2&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Traffic splitting &amp;amp; canaries -&amp;gt; Let an alias fan out traffic across multiple targets:&lt;/p&gt; &lt;p&gt;arch.v1: targets: - model: llama3.2 weight: 80 - model: gpt-4o-mini weight: 20&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/igehvjyamnpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nj3s85/archgw_0312_model_aliases_allow_clients_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nj3s85/archgw_0312_model_aliases_allow_clients_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T04:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjuoo</id>
    <title>Made a tutorial app for LLM basics: A.I. DelvePad - iOS Opensource</title>
    <updated>2025-09-17T17:39:56+00:00</updated>
    <author>
      <name>/u/Other_Passion_4710</name>
      <uri>https://old.reddit.com/user/Other_Passion_4710</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1njjuoo/made_a_tutorial_app_for_llm_basics_ai_delvepad/"&gt; &lt;img alt="Made a tutorial app for LLM basics: A.I. DelvePad - iOS Opensource" src="https://b.thumbs.redditmedia.com/HIVCeWKeDd_gE60MPzg0Fp_7OEUdurpTcssJl-2hNaw.jpg" title="Made a tutorial app for LLM basics: A.I. DelvePad - iOS Opensource" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I saw there are lots of AI wrapper apps made, but few having tutorials about LLM training and specs.&lt;/p&gt; &lt;p&gt;I built one called A.I. DelvePad — a free Opensource iOS app designed for anyone who wants to get a basic foundation in generative AI.&lt;/p&gt; &lt;p&gt;It has :&lt;/p&gt; &lt;p&gt;•Bite-sized video tutorials you can watch on the go&lt;/p&gt; &lt;p&gt;•A glossary of key AI terms&lt;/p&gt; &lt;p&gt;•A quick overview of how LLMs are trained&lt;/p&gt; &lt;p&gt;•A tutorial sharing function so you can pass what you learn to friends&lt;/p&gt; &lt;p&gt;•All tutorials are all free.&lt;/p&gt; &lt;p&gt;Looking to get more feedback, would love to hear yours. Some LLM development is done in Go and Rust. If you’ve been curious about AI models but didn’t know where to start, this might be a good starter pack for you.&lt;/p&gt; &lt;p&gt;App Store link : &lt;a href="https://apps.apple.com/us/app/a-i-delvepad/id6743481267"&gt;https://apps.apple.com/us/app/a-i-delvepad/id6743481267&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/leapdeck/AIDelvePad"&gt;https://github.com/leapdeck/AIDelvePad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Site: &lt;a href="http://aidelvepad.com"&gt;http://aidelvepad.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love any input you’ve got, please share. And if you’re building too — keep going! Enjoy making mobile projects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Passion_4710"&gt; /u/Other_Passion_4710 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1njjuoo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1njjuoo/made_a_tutorial_app_for_llm_basics_ai_delvepad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1njjuoo/made_a_tutorial_app_for_llm_basics_ai_delvepad/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T17:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1njxih7</id>
    <title>how to hide thoughts</title>
    <updated>2025-09-18T03:17:03+00:00</updated>
    <author>
      <name>/u/yasniy97</name>
      <uri>https://old.reddit.com/user/yasniy97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What command to add at prompt to hide thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yasniy97"&gt; /u/yasniy97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1njxih7/how_to_hide_thoughts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1njxih7/how_to_hide_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1njxih7/how_to_hide_thoughts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-18T03:17:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1njkndb</id>
    <title>How do I get ollama to show only the installed models in the app?</title>
    <updated>2025-09-17T18:08:12+00:00</updated>
    <author>
      <name>/u/temploupegarou</name>
      <uri>https://old.reddit.com/user/temploupegarou</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1njkndb/how_do_i_get_ollama_to_show_only_the_installed/"&gt; &lt;img alt="How do I get ollama to show only the installed models in the app?" src="https://a.thumbs.redditmedia.com/xVSTNenrT_UGgiT2x27GmNMcQiKZF4bkuIncOlj3iW0.jpg" title="How do I get ollama to show only the installed models in the app?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently built a new pc and sold my old laptop that had ollama on it and had been away from the scene for a bit. Next thing I know there's a whole app and no need to install openWebUI - win! but this app shows me ALL the available models and the setting screen doesn't have anything to make this happen. &lt;/p&gt; &lt;p&gt;The app:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9xhxv2bclrpf1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12e49860f4475d746fec1ee75a279419337d6d8d"&gt;https://preview.redd.it/9xhxv2bclrpf1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12e49860f4475d746fec1ee75a279419337d6d8d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Installed models: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zzudxwgcmrpf1.png?width=634&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b13e00b9a1156bfea4c6491eb3e03d57fefadd0d"&gt;https://preview.redd.it/zzudxwgcmrpf1.png?width=634&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b13e00b9a1156bfea4c6491eb3e03d57fefadd0d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want only these to be shown in the app. A few times now I've clicked on a model that didn't exist and it starts downloading it which is annoying. I can install models manually. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/temploupegarou"&gt; /u/temploupegarou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1njkndb/how_do_i_get_ollama_to_show_only_the_installed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1njkndb/how_do_i_get_ollama_to_show_only_the_installed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1njkndb/how_do_i_get_ollama_to_show_only_the_installed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T18:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nje7r4</id>
    <title>Coding on CLI</title>
    <updated>2025-09-17T14:10:30+00:00</updated>
    <author>
      <name>/u/booknerdcarp</name>
      <uri>https://old.reddit.com/user/booknerdcarp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a particular model that will function like Claude Code (especially writing to files) that can be used with Ollama? The costs and limits are a pain!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/booknerdcarp"&gt; /u/booknerdcarp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nje7r4/coding_on_cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nje7r4/coding_on_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nje7r4/coding_on_cli/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-17T14:10:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk4cbi</id>
    <title>Flashy sentient agi</title>
    <updated>2025-09-18T10:12:41+00:00</updated>
    <author>
      <name>/u/Adventurous-Lunch332</name>
      <uri>https://old.reddit.com/user/Adventurous-Lunch332</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sentient GRID hype: flashy multi-agent orchestration, passing summaries, marketing spectacle. Reality: it is not AGI. Multi-step reasoning fades quickly, context fragments, and infrastructure costs rise sharply. GRID focuses on complexity and modularity rather than practical performance or deep understanding.&lt;/p&gt; &lt;p&gt;A better approach is to fine-tune specific parameters in a single model, activating only the most relevant ones for each task. Combine this with detailed Chain-of-Thought reasoning, integrate relevant tools dynamically for fact-checking and information retrieval, and feed in high-quality, curated data. Flexible tool budgets allow the model to explore deeply without wasting compute or losing efficiency, preserving reasoning, coherence, and output quality across complex tasks.&lt;/p&gt; &lt;p&gt;Benefits of this approach include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full context reasoning preserved, avoiding the degradation seen in multi-agent GRID setups&lt;/li&gt; &lt;li&gt;Efficient compute usage while maintaining high performance&lt;/li&gt; &lt;li&gt;Anti-fragile design that adapts locally and handles dynamic or unexpected data&lt;/li&gt; &lt;li&gt;Flexible, dynamic tool calls triggered by uncertainty, ensuring depth where needed&lt;/li&gt; &lt;li&gt;Transparent, traceable reasoning steps that make debugging and validation easier&lt;/li&gt; &lt;li&gt;Multi-step reasoning maintained across tasks and domains&lt;/li&gt; &lt;li&gt;Dynamic integration of external knowledge without breaking context or flow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Tradeoff: GRID is flashy and modular, but reasoning is shallow, brittle, and costly. This fine-tuned single-model system is practical, efficient, deeply reasoning, anti-fragile, and optimized for real-world AI applications.&lt;/p&gt; &lt;p&gt;Full in-depth discussion covers edge-level AI workflow, CoT reasoning, tool orchestration strategies, and task-specific parameter activation for maximum performance and efficiency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Lunch332"&gt; /u/Adventurous-Lunch332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk4cbi/flashy_sentient_agi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk4cbi/flashy_sentient_agi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nk4cbi/flashy_sentient_agi/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-18T10:12:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk5oba</id>
    <title>Uncensored AI model for from 4b Max 8b</title>
    <updated>2025-09-18T11:27:54+00:00</updated>
    <author>
      <name>/u/Francetor</name>
      <uri>https://old.reddit.com/user/Francetor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I want to host an AI on a mini PC with Linux/Ubuntu operating system (Beelink MINI-S13 Pro Mini PC, Intel Twin Alder Lake-N150 Processor (up to 3.60 GHz), Mini Computer, 16 GB RAM, 500 GB SSD, Office Desktop, Dual HDMI/WiFi 6/BT 5.2/RJ45/WOL). &lt;/p&gt; &lt;p&gt;I have an existential problem and I don't know which model to use, I tried one from 1.5b and one from 3.8b (I don't remember the names) but unfortunately they suffer from various hallucinations (the moon is full of lava wtf). Could you recommend me a preferably uncensored model that goes in a range of 4b maximum 8b (I would like to have a bit of speed). Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Francetor"&gt; /u/Francetor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk5oba/uncensored_ai_model_for_from_4b_max_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk5oba/uncensored_ai_model_for_from_4b_max_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nk5oba/uncensored_ai_model_for_from_4b_max_8b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-18T11:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk9a6z</id>
    <title>How to calculate and estimate GPU usage of Foundation Model</title>
    <updated>2025-09-18T14:07:42+00:00</updated>
    <author>
      <name>/u/Tough_Wrangler_6075</name>
      <uri>https://old.reddit.com/user/Tough_Wrangler_6075</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nk9a6z/how_to_calculate_and_estimate_gpu_usage_of/"&gt; &lt;img alt="How to calculate and estimate GPU usage of Foundation Model" src="https://external-preview.redd.it/l8zbwfivRPZFibDrfAVyiwk3xzzVRMZEG-8qAUEaMrk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=398873bb225b3ffa88d2fe1166113ba87f9957a0" title="How to calculate and estimate GPU usage of Foundation Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I wrote an article about how to actually calculate the cost of gpu in term's you used open model and using your own setup. I used reference from AI Engineering book and actually compare by my own. I found that, open model with greater parameter of course better at reasoning but very consume more computation. Hope it will help you to understanding the the calculation. Happy reading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tough_Wrangler_6075"&gt; /u/Tough_Wrangler_6075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@zackydzacky/how-to-calculate-and-estimate-gpu-usage-of-foundation-model-f2e493af339b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk9a6z/how_to_calculate_and_estimate_gpu_usage_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nk9a6z/how_to_calculate_and_estimate_gpu_usage_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-18T14:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk1qkf</id>
    <title>LLM VRAM/RAM Calculator</title>
    <updated>2025-09-18T07:22:22+00:00</updated>
    <author>
      <name>/u/SmilingGen</name>
      <uri>https://old.reddit.com/user/SmilingGen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a simple tool to estimate how much memory is needed to run GGUF models locally, based on your desired maximum context size.&lt;/p&gt; &lt;p&gt;You just paste the direct download URL of a GGUF model (for example, from Hugging Face), enter the context length you plan to use, and it will give you an approximate memory requirement.&lt;/p&gt; &lt;p&gt;It’s especially useful if you're trying to figure out whether a model will fit in your available VRAM or RAM, or when comparing different quantization levels like Q4_K_M vs Q8_0.&lt;/p&gt; &lt;p&gt;The tool is completely free and open-source. You can try it here: &lt;a href="https://www.kolosal.ai/memory-calculator"&gt;https://www.kolosal.ai/memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And check out the code on GitHub: &lt;a href="https://github.com/KolosalAI/model-memory-calculator"&gt;https://github.com/KolosalAI/model-memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd really appreciate any feedback, suggestions, or bug reports if you decide to give it a try.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SmilingGen"&gt; /u/SmilingGen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk1qkf/llm_vramram_calculator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nk1qkf/llm_vramram_calculator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nk1qkf/llm_vramram_calculator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-18T07:22:22+00:00</published>
  </entry>
</feed>
