<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-15T10:48:52+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ov7wch</id>
    <title>How high of a spec do you have to have in order to install ollama in local environment?</title>
    <updated>2025-11-12T15:17:29+00:00</updated>
    <author>
      <name>/u/SnooRegrets3378</name>
      <uri>https://old.reddit.com/user/SnooRegrets3378</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently work in a virtual machine environment where any website is unavailable. Is it possible to bring ollama into this type of setting? Exasperated by having to do everything with excel when you can use ai models and i work with sensitive datas so i would have to do the work locally.&lt;/p&gt; &lt;p&gt;Sorry in advance for the possible inaccurate word choice im not a computer guy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooRegrets3378"&gt; /u/SnooRegrets3378 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7wch/how_high_of_a_spec_do_you_have_to_have_in_order/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ov7wch/how_high_of_a_spec_do_you_have_to_have_in_order/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ov7wch/how_high_of_a_spec_do_you_have_to_have_in_order/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-12T15:17:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovmy2a</id>
    <title>Nothink in the gui</title>
    <updated>2025-11-13T00:43:19+00:00</updated>
    <author>
      <name>/u/sceadwian</name>
      <uri>https://old.reddit.com/user/sceadwian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to /set nothink in the ollama GUI? I'm not seeing any places to pass command line paramaters and it doesn't take the command in the chat dialog box.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sceadwian"&gt; /u/sceadwian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovmy2a/nothink_in_the_gui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovmy2a/nothink_in_the_gui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovmy2a/nothink_in_the_gui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T00:43:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovt199</id>
    <title>Local Llama API</title>
    <updated>2025-11-13T05:36:53+00:00</updated>
    <author>
      <name>/u/TuLiSTua</name>
      <uri>https://old.reddit.com/user/TuLiSTua</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ovt199/local_llama_api/"&gt; &lt;img alt="Local Llama API" src="https://preview.redd.it/o87hysrgoy0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75d5a1be150cde933fbeac9cec14dee93e4c2a45" title="Local Llama API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following Situation: -self hosted Tandoor Recipes 2.3.3 instance -self hosted Ollama instance with llama3.2:latest&lt;/p&gt; &lt;p&gt;I want Tandoor to use my local AI to work as AI provider, but i need an API. The question is, where do I get it from?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TuLiSTua"&gt; /u/TuLiSTua &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o87hysrgoy0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovt199/local_llama_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovt199/local_llama_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T05:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow0x3g</id>
    <title>Thanks to Gowtham Boyina for featuring my library in his latest article üôè</title>
    <updated>2025-11-13T13:21:57+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ow0x3g/thanks_to_gowtham_boyina_for_featuring_my_library/"&gt; &lt;img alt="Thanks to Gowtham Boyina for featuring my library in his latest article üôè" src="https://external-preview.redd.it/y0dgtZ0gVjnRje7BRhie_7gJWNn7zzoil-XFJAfPX90.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80e455756943072492fbf6c6135f659142eced5f" title="Thanks to Gowtham Boyina for featuring my library in his latest article üôè" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://levelup.gitconnected.com/why-your-python-functions-arent-ai-tools-yet-and-how-polymcp-fixes-it-in-one-line-d8e62550ac53"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow0x3g/thanks_to_gowtham_boyina_for_featuring_my_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow0x3g/thanks_to_gowtham_boyina_for_featuring_my_library/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T13:21:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow31je</id>
    <title>Anyone running code model in cpu only VPS?</title>
    <updated>2025-11-13T14:50:13+00:00</updated>
    <author>
      <name>/u/Gcloud-AI</name>
      <uri>https://old.reddit.com/user/Gcloud-AI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to use codeseeker model in my vps but it's not good, not able to get any output from my model üòî. My vps spec is: 8 cpu core 32gb ram 1tb nvme storage Any guidance for me???&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gcloud-AI"&gt; /u/Gcloud-AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow31je/anyone_running_code_model_in_cpu_only_vps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow31je/anyone_running_code_model_in_cpu_only_vps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow31je/anyone_running_code_model_in_cpu_only_vps/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T14:50:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow3ne9</id>
    <title>Qual a melhor GPU para o llama 3(.1 ou .3)</title>
    <updated>2025-11-13T15:13:39+00:00</updated>
    <author>
      <name>/u/No_Progress432</name>
      <uri>https://old.reddit.com/user/No_Progress432</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Progress432"&gt; /u/No_Progress432 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ow3my9/qual_a_melhor_gpu_para_o_llama_31_ou_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow3ne9/qual_a_melhor_gpu_para_o_llama_31_ou_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow3ne9/qual_a_melhor_gpu_para_o_llama_31_ou_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T15:13:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovyfb6</id>
    <title>Everything at Your Fingertips: How PolyMCP Makes Multi-Tool AI Simple</title>
    <updated>2025-11-13T11:14:36+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ovyfb6/everything_at_your_fingertips_how_polymcp_makes/"&gt; &lt;img alt="Everything at Your Fingertips: How PolyMCP Makes Multi-Tool AI Simple" src="https://external-preview.redd.it/OsRUlAPOIfUajUERak6rJLdgfe46_oa2w3fJR_8dpWg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0bc545ee48ac58062b8aa9b228116950bbd4c676" title="Everything at Your Fingertips: How PolyMCP Makes Multi-Tool AI Simple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/Polymcp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovyfb6/everything_at_your_fingertips_how_polymcp_makes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovyfb6/everything_at_your_fingertips_how_polymcp_makes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T11:14:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow5e4z</id>
    <title>MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration</title>
    <updated>2025-11-13T16:20:51+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ow5e4z/mcp_server_for_industrial_iot_built_for_polymcp/"&gt; &lt;img alt="MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration" src="https://external-preview.redd.it/7DcHkpBMRVBJAoq05xem0Cu6v1pmCb6s2RmtluiBv_4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84327b5adc51d58c3e3d8bd34d3475931cf4f24a" title="MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/IoT-Edge-MCP-Server"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow5e4z/mcp_server_for_industrial_iot_built_for_polymcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow5e4z/mcp_server_for_industrial_iot_built_for_polymcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T16:20:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1owcm4k</id>
    <title>Can't find Model in Ollama</title>
    <updated>2025-11-13T20:51:35+00:00</updated>
    <author>
      <name>/u/dissmami</name>
      <uri>https://old.reddit.com/user/dissmami</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1owcm4k/cant_find_model_in_ollama/"&gt; &lt;img alt="Can't find Model in Ollama" src="https://a.thumbs.redditmedia.com/QbxUx1f9O6EMpaAGb1OChkJFe9G_94B_EhqL1IhiUH0.jpg" title="Can't find Model in Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I use &amp;quot;Ollama list&amp;quot; the latest model I downloaded doesnt show. But when I try to redownload the model it says that the model already exists.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zifob79n731g1.png?width=1444&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=481fe9c76ed0520b66cff4cf327f4815a67fe4bf"&gt;https://preview.redd.it/zifob79n731g1.png?width=1444&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=481fe9c76ed0520b66cff4cf327f4815a67fe4bf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dissmami"&gt; /u/dissmami &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owcm4k/cant_find_model_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owcm4k/cant_find_model_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owcm4k/cant_find_model_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T20:51:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1owliwq</id>
    <title>Mimir - Parallel Agent task orchestration - Drag and drop UI (preview)</title>
    <updated>2025-11-14T03:16:09+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1owliwq/mimir_parallel_agent_task_orchestration_drag_and/"&gt; &lt;img alt="Mimir - Parallel Agent task orchestration - Drag and drop UI (preview)" src="https://preview.redd.it/zddnm50z351g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a523074097ef22f041a6b965715a2f35a15bc328" title="Mimir - Parallel Agent task orchestration - Drag and drop UI (preview)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zddnm50z351g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owliwq/mimir_parallel_agent_task_orchestration_drag_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owliwq/mimir_parallel_agent_task_orchestration_drag_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T03:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1owjmiu</id>
    <title>Use case-analyze my energy use to plan a solar panel/ battery setup</title>
    <updated>2025-11-14T01:46:43+00:00</updated>
    <author>
      <name>/u/SaltbushBillJP</name>
      <uri>https://old.reddit.com/user/SaltbushBillJP</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Be gentle, noob here. How's this for an AI use case? &lt;/p&gt; &lt;p&gt;I want to have my last 12 months of electricity bills summarised, to understand total energy consumption and average daily consumption. &lt;/p&gt; &lt;p&gt;I want to use the summary as an input to determine whether or not to proceed with an investment in solar panels and a battery, from there to determine size of system and determine time of payback on the system. &lt;/p&gt; &lt;p&gt;I'm happy to be told whatever you can share. Thanks in advance for your generosity and patience!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SaltbushBillJP"&gt; /u/SaltbushBillJP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owjmiu/use_caseanalyze_my_energy_use_to_plan_a_solar/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owjmiu/use_caseanalyze_my_energy_use_to_plan_a_solar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owjmiu/use_caseanalyze_my_energy_use_to_plan_a_solar/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T01:46:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovzgqx</id>
    <title>Most powerful LLM for 10GB RTX 3080?</title>
    <updated>2025-11-13T12:12:34+00:00</updated>
    <author>
      <name>/u/HUG0gamingHD</name>
      <uri>https://old.reddit.com/user/HUG0gamingHD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for a llm that can fully take advantage of this gpu.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HUG0gamingHD"&gt; /u/HUG0gamingHD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovzgqx/most_powerful_llm_for_10gb_rtx_3080/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ovzgqx/most_powerful_llm_for_10gb_rtx_3080/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ovzgqx/most_powerful_llm_for_10gb_rtx_3080/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T12:12:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1owqa4u</id>
    <title>Idea validation: ‚ÄúRAG as a Service‚Äù for AI agents. Would you use it?</title>
    <updated>2025-11-14T07:38:04+00:00</updated>
    <author>
      <name>/u/Feisty-Promise-78</name>
      <uri>https://old.reddit.com/user/Feisty-Promise-78</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm exploring an idea and would like some feedback before building the full thing.&lt;/p&gt; &lt;p&gt;The concept is a simple, developer-focused &lt;strong&gt;‚ÄúRAG as a Service‚Äù&lt;/strong&gt; that handles all the messy parts of retrieval-augmented generation:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Upload files (PDF, text, markdown, docs)&lt;/li&gt; &lt;li&gt;Automatic text extraction, chunking, and embedding&lt;/li&gt; &lt;li&gt;Support for &lt;strong&gt;multiple embedding providers&lt;/strong&gt; (OpenAI, Cohere, etc.)&lt;/li&gt; &lt;li&gt;Support for &lt;strong&gt;different search/query techniques&lt;/strong&gt; (vector search, hybrid, keyword, etc.)&lt;/li&gt; &lt;li&gt;Ability to &lt;strong&gt;compare and evaluate different RAG configurations&lt;/strong&gt; to choose the best one for your agent&lt;/li&gt; &lt;li&gt;Clean REST API + SDKs + MCP integration&lt;/li&gt; &lt;li&gt;Web dashboard where you can test queries in a chat interface&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically: an easy way to plug RAG into your agent workflows without maintaining any retrieval infrastructure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I‚Äôd like feedback on:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Would a flexible, developer-focused ‚ÄúRAG as a Service‚Äù be useful in your AI agent projects?&lt;/li&gt; &lt;li&gt;How important is the ability to switch between embedding providers and search techniques?&lt;/li&gt; &lt;li&gt;Would an evaluation/benchmarking feature help you choose the best RAG setup for your agent?&lt;/li&gt; &lt;li&gt;Which interface would you want to use: API, SDK, MCP, or dashboard chat?&lt;/li&gt; &lt;li&gt;What would you realistically be willing to pay for 100MB of file for something like this? (Monthly or per-usage pricing)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôd appreciate any thoughts, especially from people building agents, copilots, or internal AI tools.&lt;/p&gt; &lt;p&gt;Of course, it will be &lt;strong&gt;open-source&lt;/strong&gt;üòä&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feisty-Promise-78"&gt; /u/Feisty-Promise-78 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owqa4u/idea_validation_rag_as_a_service_for_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owqa4u/idea_validation_rag_as_a_service_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owqa4u/idea_validation_rag_as_a_service_for_ai_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T07:38:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1owlkzg</id>
    <title>An opinionated, minimalist agentic TUI</title>
    <updated>2025-11-14T03:18:55+00:00</updated>
    <author>
      <name>/u/uwhkdb</name>
      <uri>https://old.reddit.com/user/uwhkdb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1owlkzg/an_opinionated_minimalist_agentic_tui/"&gt; &lt;img alt="An opinionated, minimalist agentic TUI" src="https://preview.redd.it/arf15nh4v41g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=f82f95f01c4c515fe36bcd03902dc7b1b2c060f2" title="An opinionated, minimalist agentic TUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uwhkdb"&gt; /u/uwhkdb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/arf15nh4v41g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owlkzg/an_opinionated_minimalist_agentic_tui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owlkzg/an_opinionated_minimalist_agentic_tui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T03:18:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1owtagc</id>
    <title>MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration</title>
    <updated>2025-11-14T10:48:20+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1owtagc/mcp_server_for_industrial_iot_built_for_polymcp/"&gt; &lt;img alt="MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration" src="https://external-preview.redd.it/ewQJDfU8pKtPoNJ1-_eOSne6U7qS0Zul-LMlgBujvaM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=988638488b2d46b404f2eb0886e999ea0a35e693" title="MCP Server for Industrial IoT - Built for PolyMCP Agent Orchestration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/IoT-Edge-MCP-Server"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owtagc/mcp_server_for_industrial_iot_built_for_polymcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owtagc/mcp_server_for_industrial_iot_built_for_polymcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T10:48:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1owtoge</id>
    <title>Thanks for 24 Stars for Polymcp! üöÄ</title>
    <updated>2025-11-14T11:10:31+00:00</updated>
    <author>
      <name>/u/JustVugg</name>
      <uri>https://old.reddit.com/user/JustVugg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1owtoge/thanks_for_24_stars_for_polymcp/"&gt; &lt;img alt="Thanks for 24 Stars for Polymcp! üöÄ" src="https://external-preview.redd.it/0UAAIIRPuRc1eQD9AHD1hC2Y49-xOfdPSmxtUygoh_8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fe5089fd3b48279973e5a30119e8ceb6d38bb9f" title="Thanks for 24 Stars for Polymcp! üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustVugg"&gt; /u/JustVugg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/llm-use/Polymcp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owtoge/thanks_for_24_stars_for_polymcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owtoge/thanks_for_24_stars_for_polymcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T11:10:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow89ph</id>
    <title>We're visualizing what local LLMs actually do when they run - reality check needed</title>
    <updated>2025-11-13T18:07:42+00:00</updated>
    <author>
      <name>/u/That-Vanilla1513</name>
      <uri>https://old.reddit.com/user/That-Vanilla1513</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;We're building an open source tool that visualizes the internal process of local LLM inference in real-time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; Everyone's running Ollama models, tweaking parameters, switching between Llama/Mistral/whatever - but nobody actually &lt;em&gt;sees&lt;/em&gt; what's happening under the hood. You're flying blind.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What we're building:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time visualization of token processing as your model generates responses&lt;/li&gt; &lt;li&gt;Attention pattern maps showing what the model &amp;quot;focuses on&amp;quot;&lt;/li&gt; &lt;li&gt;Resource usage breakdown (CPU/GPU/RAM) per inference step&lt;/li&gt; &lt;li&gt;Bottleneck detection for performance optimization&lt;/li&gt; &lt;li&gt;Side-by-side comparison when testing different models/params&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; Our tool hooks into Ollama's API and captures the inference process, then renders it as an interactive spider-web style visualization. You can pause, rewind, and explore exactly why your model gave a specific response.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current status:&lt;/strong&gt; We are currently actively developing V1 of our product. We plan to integrate it with major LLM models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why we're posting:&lt;/strong&gt; I need a reality check from people who actually run local models daily.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Be brutally honest:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is &amp;quot;I don't know what my model is doing&amp;quot; actually a problem you have, or are you fine with black-box inference?&lt;/li&gt; &lt;li&gt;Would visualization help you debug, optimize, or pick models - or is this just cool but useless?&lt;/li&gt; &lt;li&gt;If you'd use this, what's the ONE feature that would make it essential vs. just interesting?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're not trying to sell anything, we're just trying to figure out if we're solving a real problem or building something nobody needs.&lt;/p&gt; &lt;p&gt;Links and demo video in the comments.&lt;/p&gt; &lt;p&gt;Thanks for keeping it real. üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/That-Vanilla1513"&gt; /u/That-Vanilla1513 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow89ph/were_visualizing_what_local_llms_actually_do_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ow89ph/were_visualizing_what_local_llms_actually_do_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ow89ph/were_visualizing_what_local_llms_actually_do_when/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-13T18:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1owik8l</id>
    <title>Using my entire source code library in my LLM</title>
    <updated>2025-11-14T00:57:48+00:00</updated>
    <author>
      <name>/u/phoenixfire425</name>
      <uri>https://old.reddit.com/user/phoenixfire425</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have about 25years of my code I would like to be able to have my local ollama instance either trained on or possibly RAG?&lt;/p&gt; &lt;p&gt;My goal is so be able access examples of my previous code by asking questions like I do now with things like qwen or gpt-oss.&lt;/p&gt; &lt;p&gt;Most of my stuff is python and .net stack.&lt;/p&gt; &lt;p&gt;There have been so many times where I know I did something before and it required some crafty work arounds, but I don‚Äôt recall the project. I would love to be able to us all that code as a resource.&lt;/p&gt; &lt;p&gt;My setup is Ollama and OpenWebui on Linux Mint with a RTX 3090 and GTX 1050(used just for memory personalization in openwebui)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phoenixfire425"&gt; /u/phoenixfire425 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owik8l/using_my_entire_source_code_library_in_my_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owik8l/using_my_entire_source_code_library_in_my_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owik8l/using_my_entire_source_code_library_in_my_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T00:57:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxagdv</id>
    <title>Attorney Looking for Hardware and Model Recs</title>
    <updated>2025-11-14T22:21:08+00:00</updated>
    <author>
      <name>/u/Extension-Ad-2801</name>
      <uri>https://old.reddit.com/user/Extension-Ad-2801</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am very new to this, so I apologize if I am not using the right terminology. I am an attorney, and the idea of running your own AI server is very appealing because it would alleviate a lot of concerns about lawyer-client confidentiality when using AI that is present with most commercial AIs. At least I think it would. Please let me know if I am wrong about that. I would want to use it for work and for general AI use. I know that all AI models are not 100% accurate, especially for legal stuff, so I know you have proof everything regardless.&lt;/p&gt; &lt;p&gt;I am wondering what Ollama models would be best for work and general use. &lt;/p&gt; &lt;p&gt;Also, how would I add my personal files and stuff for it to learn on? I assume doing this with your own Ollama would not compromise my client's confidentiality. Part of the pain of trying to use AIs like ChatGPT is that if you show it something that you want it to learn off of, you have to remove anything that could be identifying information of your client, so I would love to just dump a whole lot of files into it without having to edit them. Is that possible? Is this what a RAG is? Again, I am very new to this whole concept, so I am pretty clueless, but I started learning about this, and it seems to have a lot of potential. &lt;/p&gt; &lt;p&gt;I currently have an M4 Mac Mini with 24G of RAM, and I am wondering if that would be enough if I am still using it as my work/general use machine that includes a lightly used media server. &lt;/p&gt; &lt;p&gt;I am also wondering if I can place Ollama's files on an external drive, and, if so, is there a best way to set that up? &lt;/p&gt; &lt;p&gt;Do people have recs for hardware if my M4 Mac Mini with 24G of RAM is not enough? I would like the cheapest computer that would get the job done reasonably well. I have heard the M4 macs are the best for this, but I don't know. &lt;/p&gt; &lt;p&gt;Does anyone have recs for models? Also, can you combine models or do you use just one at a time? If I hear there is a better model out there, would I have to teach it everything from the beginning? &lt;/p&gt; &lt;p&gt;Sorry for all the questions. I figured this would be the best place to go. Thank you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extension-Ad-2801"&gt; /u/Extension-Ad-2801 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxagdv/attorney_looking_for_hardware_and_model_recs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxagdv/attorney_looking_for_hardware_and_model_recs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oxagdv/attorney_looking_for_hardware_and_model_recs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T22:21:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxdiva</id>
    <title>AI Safety Evaluation!</title>
    <updated>2025-11-15T00:31:42+00:00</updated>
    <author>
      <name>/u/DyroZang</name>
      <uri>https://old.reddit.com/user/DyroZang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone! &lt;/p&gt; &lt;p&gt;I thought I would share a project that I've been working on recently that I'm hoping to get some traction and feedback on. Apolien is a python package for evaluating LLMs for their level of AI Safety originally built on ollama but now also supporting Anthropic API. As of now this package, Apolien, will be able to accept any model available on ollama and perform a series of faithfulness tests on the model through something called Chain-of-Thought prompting. Based on the models responses it will determine if the model is faithful to some reasoning or if it's lying or ignoring specific requests. &lt;/p&gt; &lt;p&gt;The repository for this project is available here: &lt;a href="https://github.com/gabe-mousa/Apolien"&gt;https://github.com/gabe-mousa/Apolien&lt;/a&gt; or you can install it using `pip install apolien`. In the repo there is specific information on the faithfulness tests, example outputs, datasets available to test on, and issues if anyone feels like contributing to the project. &lt;/p&gt; &lt;p&gt;Please feel free to comment any questions around the stats, inspiration, feedback of any kind and I'll do my best to respond here. Otherwise if you're feeling generous or find the project particularly interesting I would greatly appreciate if you could star the project on GitHub! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DyroZang"&gt; /u/DyroZang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxdiva/ai_safety_evaluation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxdiva/ai_safety_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oxdiva/ai_safety_evaluation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-15T00:31:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxe0pg</id>
    <title>Local Ollama Processing Scanned Images - Need Ideas</title>
    <updated>2025-11-15T00:54:04+00:00</updated>
    <author>
      <name>/u/degr8sid</name>
      <uri>https://old.reddit.com/user/degr8sid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I want to build a system where my local ollama model processes the scanned text from PDFs. These are basically a lot of scanned books in arabic and the text is not picked up by Adobe either. Like you can't copy the text from them even if you open the pdf using Adobe Acrobat.&lt;/p&gt; &lt;p&gt;So I want my system to process scanned pdf, pick the text that's actually there, convert it into proper text and then use it for RAG. &lt;/p&gt; &lt;p&gt;I want ideas on how I can setup using local llama. And what other tools/agents/etc will I need to make it work successfully. Or should I just drop this project? I really want to help people learning modern standard arabic and the scanned books I have are great resources.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/degr8sid"&gt; /u/degr8sid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxe0pg/local_ollama_processing_scanned_images_need_ideas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1oxe0pg/local_ollama_processing_scanned_images_need_ideas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1oxe0pg/local_ollama_processing_scanned_images_need_ideas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-15T00:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox99fj</id>
    <title>How to get a custom Open WebUi Model to return a JSON object consistently?</title>
    <updated>2025-11-14T21:33:57+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 24GB of VRAM which seems like it should be enough. Here is the issue: I setup a custom model in Open WebUI and attached some knowledge and a system prompt to it. Its job is to read an article I provide it, and evaluate it, and is supposed to return a JSON object in a particular format, for example {status: ‚Äúgreen‚Äù, reason: ‚Äúblah blah‚Äù} where I define when it should evaluate to green, vs blue, vs red etc, and explain the answer in ‚Äúreason‚Äù. The thinking works perfectly as I want it to after I feed it various test articles where I know what the output should be. The problem is that it ignores my rules to return only the json and only with those fields and with only those status options listed. It adds its reasoning to the output like and produces a large response that sometimes also contains the json. Also, when it does add the json, it names the fields what it wants, for example sometimes calling ‚Äústatus‚Äù ‚Äúresult‚Äù instead etc. The fact that the thinking is correct, makes me think its not an issue of the model being too weak. It just refuses to lock into the json format and respecting my rules to SOLELY output the json, with no extra text along with it. Any thoughts? Im currently trying Qwen3 32b since it fits in 24gb. Im also using the openwebui api/chat/completions endpoint since the ollama generate endpoint doesnt see the custom models created in openwebui&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ox99fj/how_to_get_a_custom_open_webui_model_to_return_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ox99fj/how_to_get_a_custom_open_webui_model_to_return_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ox99fj/how_to_get_a_custom_open_webui_model_to_return_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T21:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox403x</id>
    <title>is ollama supposed to work out of the box for a 7800 XT?</title>
    <updated>2025-11-14T18:12:41+00:00</updated>
    <author>
      <name>/u/ZdrytchX</name>
      <uri>https://old.reddit.com/user/ZdrytchX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Turns out its been running cpu this entire time despite the 7800XT being on the &lt;a href="https://ollama.com/blog/amd-preview"&gt;supported&lt;/a&gt; list.&lt;/p&gt; &lt;p&gt;On the side note, WebGPU fails for some reason with my gpu, corrupting output from some things like &lt;a href="https://huggingface.co/spaces/webml-community/kokoro-webgpu"&gt;kokoro&lt;/a&gt;, but the gpu runs video games fine aside from the usual videogame-specific hickups &lt;sup&gt;specifically war thunder crashes on alt-tab sometimes and il-2sturmovik has stuttering since windows 11 'downgrade'&lt;/sup&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZdrytchX"&gt; /u/ZdrytchX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ox403x/is_ollama_supposed_to_work_out_of_the_box_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ox403x/is_ollama_supposed_to_work_out_of_the_box_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ox403x/is_ollama_supposed_to_work_out_of_the_box_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T18:12:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1owymhr</id>
    <title>distil-localdoc.py - SLM assistant for writing Python documentation</title>
    <updated>2025-11-14T14:55:01+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1owymhr/distillocaldocpy_slm_assistant_for_writing_python/"&gt; &lt;img alt="distil-localdoc.py - SLM assistant for writing Python documentation" src="https://preview.redd.it/c1ufxihvk81g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d2a67cbfadc9b786a0817740c4adc2cc7675bf9" title="distil-localdoc.py - SLM assistant for writing Python documentation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an SLM assistant for automatic Python documentation - a Qwen3 0.6B parameter model that generates complete, properly formatted docstrings for your code in Google style. Run it locally, keeping your proprietary code secure! Find it at &lt;a href="https://github.com/distil-labs/distil-localdoc.py"&gt;https://github.com/distil-labs/distil-localdoc.py&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Usage&lt;/h2&gt; &lt;p&gt;We load the model and your Python file. By default we load the downloaded Qwen3 0.6B model and generate Google-style docstrings.&lt;/p&gt; &lt;p&gt;```bash python localdoc.py --file your_script.py&lt;/p&gt; &lt;h1&gt;optionally, specify model and docstring style&lt;/h1&gt; &lt;p&gt;python localdoc.py --file your_script.py --model localdoc_qwen3 --style google ```&lt;/p&gt; &lt;p&gt;The tool will generate an updated file with &lt;code&gt;_documented&lt;/code&gt; suffix (e.g., &lt;code&gt;your_script_documented.py&lt;/code&gt;).&lt;/p&gt; &lt;h2&gt;Features&lt;/h2&gt; &lt;p&gt;The assistant can generate docstrings for: - &lt;strong&gt;Functions&lt;/strong&gt;: Complete parameter descriptions, return values, and raised exceptions - &lt;strong&gt;Methods&lt;/strong&gt;: Instance and class method documentation with proper formatting. The tool skips double underscore (dunder: __xxx) methods.&lt;/p&gt; &lt;h2&gt;Examples&lt;/h2&gt; &lt;p&gt;Feel free to run them yourself using the files in [examples](examples)&lt;/p&gt; &lt;h3&gt;Before:&lt;/h3&gt; &lt;p&gt;&lt;code&gt;python def calculate_total(items, tax_rate=0.08, discount=None): subtotal = sum(item['price'] * item['quantity'] for item in items) if discount: subtotal *= (1 - discount) return subtotal * (1 + tax_rate) &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;After (Google style):&lt;/h3&gt; &lt;p&gt;```python def calculate_total(items, tax_rate=0.08, discount=None): &amp;quot;&amp;quot;&amp;quot; Calculate the total cost of items, applying a tax rate and optionally a discount.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Args: items: List of item objects with price and quantity tax_rate: Tax rate expressed as a decimal (default 0.08) discount: Discount rate expressed as a decimal; if provided, the subtotal is multiplied by (1 - discount) Returns: Total amount after applying the tax Example: &amp;gt;&amp;gt;&amp;gt; items = [{'price': 10, 'quantity': 2}, {'price': 5, 'quantity': 1}] &amp;gt;&amp;gt;&amp;gt; calculate_total(items, tax_rate=0.1, discount=0.05) 22.5 &amp;quot;&amp;quot;&amp;quot; subtotal = sum(item['price'] * item['quantity'] for item in items) if discount: subtotal *= (1 - discount) return subtotal * (1 + tax_rate) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;h2&gt;FAQ&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Q: Why don't we just use GPT-4/Claude API for this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Because your proprietary code shouldn't leave your infrastructure. Cloud APIs create security risks, compliance issues, and ongoing costs. Our models run locally with comparable quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Can I document existing docstrings or update them?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Currently, the tool only adds missing docstrings. Updating existing documentation is planned for future releases. For now, you can manually remove docstrings you want regenerated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Which docstring style can I use?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Google&lt;/strong&gt;: Most readable, great for general Python projects&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Q: The model does not work as expected&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: The tool calling on our platform is in active development! &lt;a href="https://www.linkedin.com/company/distil-labs/"&gt;Follow us on LinkedIn&lt;/a&gt; for updates, or &lt;a href="https://join.slack.com/t/distil-labs-community/shared_invite/zt-36zqj87le-i3quWUn2bjErRq22xoE58g"&gt;join our community&lt;/a&gt;. You can also manually refine any generated docstrings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Can you train a model for my company's documentation standards?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: Visit our &lt;a href="https://www.distillabs.ai"&gt;website&lt;/a&gt; and reach out to us, we offer custom solutions tailored to your coding standards and domain-specific requirements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Does this support type hints or other Python documentation tools?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: Type hints are parsed and incorporated into docstrings. Integration with tools like pydoc, Sphinx, and MkDocs is on our roadmap.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c1ufxihvk81g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1owymhr/distillocaldocpy_slm_assistant_for_writing_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1owymhr/distillocaldocpy_slm_assistant_for_writing_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T14:55:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox8ky1</id>
    <title>Gpt oss 120b 64GB RAM, RTX5090 32GB?</title>
    <updated>2025-11-14T21:07:12+00:00</updated>
    <author>
      <name>/u/Wonk_puffin</name>
      <uri>https://old.reddit.com/user/Wonk_puffin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all Is this possible? Horrified at the price of system RAM which seems to have more than tripled for DDR5 in 8 months so my system is what it is. Using Ollama desktop, docker, openwebui. Have many models but would love to get this running even if at only 10 tokens a second. Any settings appreciated if this is feasible. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wonk_puffin"&gt; /u/Wonk_puffin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ox8ky1/gpt_oss_120b_64gb_ram_rtx5090_32gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ox8ky1/gpt_oss_120b_64gb_ram_rtx5090_32gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ox8ky1/gpt_oss_120b_64gb_ram_rtx5090_32gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-14T21:07:12+00:00</published>
  </entry>
</feed>
