<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-11-30T22:48:41+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1p7qz4r</id>
    <title>JARVIS Local AGENT</title>
    <updated>2025-11-27T02:45:01+00:00</updated>
    <author>
      <name>/u/Xthebuilder</name>
      <uri>https://old.reddit.com/user/Xthebuilder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p7qz4r/jarvis_local_agent/"&gt; &lt;img alt="JARVIS Local AGENT" src="https://b.thumbs.redditmedia.com/FctE-GRFqk4mhPloHbrq2FnnWOS4vNqhHHnNn7xnS7s.jpg" title="JARVIS Local AGENT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;‚óè Built JRVS a local AI agent who lives in your computer and gets better overtime , this is my first public project so feedback is really appreciated&lt;/p&gt; &lt;p&gt;- üß† RAG knowledge base with vector search- üåê Web scraping &amp;amp; auto-indexing- üìÖ Calendar/task management- üíª JARCORE coding engine (code gen, analysis, debugging, tests)&lt;/p&gt; &lt;p&gt;The Moat of JARVIS is he can almost do it all and hes local , we all know why the local part is the best part about JARVIS&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Xthebuilder/JRVS"&gt;https://github.com/Xthebuilder/JRVS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xthebuilder"&gt; /u/Xthebuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p7qz4r"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p7qz4r/jarvis_local_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p7qz4r/jarvis_local_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-27T02:45:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8wxws</id>
    <title>NornicDB - neo4j drop-in - MIT - MemoryOS- golang native - my god the performance</title>
    <updated>2025-11-28T15:00:20+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ChatGPTCoding/comments/1p8wwvv/nornicdb_neo4j_dropin_mit_memoryos_golang_native/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p8wxws/nornicdb_neo4j_dropin_mit_memoryos_golang_native/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p8wxws/nornicdb_neo4j_dropin_mit_memoryos_golang_native/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-28T15:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1p919et</id>
    <title>Which local model for 3090 5069 TI combo</title>
    <updated>2025-11-28T17:51:06+00:00</updated>
    <author>
      <name>/u/Blksagethenomad</name>
      <uri>https://old.reddit.com/user/Blksagethenomad</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Blksagethenomad"&gt; /u/Blksagethenomad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/homelab/comments/1p9189i/which_local_model_for_3090_5069_ti_combo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p919et/which_local_model_for_3090_5069_ti_combo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p919et/which_local_model_for_3090_5069_ti_combo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-28T17:51:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8xvk4</id>
    <title>Single slot, Low profile GPU that can run 7B models</title>
    <updated>2025-11-28T15:38:30+00:00</updated>
    <author>
      <name>/u/Electrical_Fault_915</name>
      <uri>https://old.reddit.com/user/Electrical_Fault_915</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electrical_Fault_915"&gt; /u/Electrical_Fault_915 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1p8xvew/single_slot_low_profile_gpu_that_can_run_7b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p8xvk4/single_slot_low_profile_gpu_that_can_run_7b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p8xvk4/single_slot_low_profile_gpu_that_can_run_7b_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-28T15:38:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9jjv3</id>
    <title>UPLOAD LLAMA.CPP FRONTEND IN GITHUB FOR SERVER OVER LAN MORE EASY</title>
    <updated>2025-11-29T08:11:00+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p9jjv3/upload_llamacpp_frontend_in_github_for_server/"&gt; &lt;img alt="UPLOAD LLAMA.CPP FRONTEND IN GITHUB FOR SERVER OVER LAN MORE EASY" src="https://b.thumbs.redditmedia.com/59ddSZ3iPzn4iR6piUftC1Z64DmbOf8ceb7cIq8qdeY.jpg" title="UPLOAD LLAMA.CPP FRONTEND IN GITHUB FOR SERVER OVER LAN MORE EASY" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/jans1981/LLAMA.CPP-SERVER-FRONTEND-FOR-CONSOLE/blob/main/README.md"&gt;https://github.com/jans1981/LLAMA.CPP-SERVER-FRONTEND-FOR-CONSOLE/blob/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;NOW YOU CAN SERVER MULTIPLE FILES GGUF OVER LAN WITH LLAMA.CPP EASY&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cp0q0qpam54g1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e0e6d5750193c9945b6d3321c52b5c4e138f08db"&gt;https://preview.redd.it/cp0q0qpam54g1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e0e6d5750193c9945b6d3321c52b5c4e138f08db&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9jjv3/upload_llamacpp_frontend_in_github_for_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9jjv3/upload_llamacpp_frontend_in_github_for_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9jjv3/upload_llamacpp_frontend_in_github_for_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T08:11:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9fqpj</id>
    <title>Access to Blackwell hardware and a live use-case. Looking for a business partner</title>
    <updated>2025-11-29T04:33:36+00:00</updated>
    <author>
      <name>/u/Different-Set-1031</name>
      <uri>https://old.reddit.com/user/Different-Set-1031</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Set-1031"&gt; /u/Different-Set-1031 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AmazonRME/comments/1p9fnvo/access_to_blackwell_hardware_and_a_live_usecase/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9fqpj/access_to_blackwell_hardware_and_a_live_usecase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9fqpj/access_to_blackwell_hardware_and_a_live_usecase/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T04:33:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p912ua</id>
    <title>DeepSeek-R1-14B uses ~2√ó more VRAM at the same context - here‚Äôs why it OOMs in Ollama</title>
    <updated>2025-11-28T17:43:51+00:00</updated>
    <author>
      <name>/u/Enough-Cat7020</name>
      <uri>https://old.reddit.com/user/Enough-Cat7020</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p912ua/deepseekr114b_uses_2_more_vram_at_the_same/"&gt; &lt;img alt="DeepSeek-R1-14B uses ~2√ó more VRAM at the same context - here‚Äôs why it OOMs in Ollama" src="https://b.thumbs.redditmedia.com/LSQT7frufrGA3kN0sd3iQgBPbgTZQHHmRCtfSrxv0Lk.jpg" title="DeepSeek-R1-14B uses ~2√ó more VRAM at the same context - here‚Äôs why it OOMs in Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve seen a lot of posts recently about performance issues with DeepSeek R1 Distill 14B especially OOM crashes or slowdowns when pushing context.&lt;/p&gt; &lt;p&gt;A quick reminder: although it‚Äôs Qwen-based, this specific model has &lt;strong&gt;no GQA&lt;/strong&gt; (1:1 attention ‚Üí 1:1 KV heads).&lt;br /&gt; That means the KV cache scales aggressively with context compared to Llama 3 or Mistral. Here‚Äôs what my calculator shows (using a 16GB card as example):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick VRAM Comparison (DEEPSEEK R1 16k vs MISTRAL NEMO 12B 16k):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z9zuvn54b14g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd9f995b14a30c465c540cfc1437263e3a5a2f45"&gt;https://preview.redd.it/z9zuvn54b14g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd9f995b14a30c465c540cfc1437263e3a5a2f45&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mistral Nemo 12B 16k ctx&lt;/strong&gt;: Uses ~11GB VRAM ‚Üí Fits comfortably&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek 14B @ 16k ctx:&lt;/strong&gt; ~25GB VRAM ‚Üí &lt;strong&gt;OOM Crash&lt;/strong&gt; (or heavy offloading)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even though they are similar in parameter count (12B vs 14B), the context bloat destroys the DeepSeek model on mid-range cards.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Fix / Tool Update:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I updated my VRAM calculator to reverse-compute the &lt;strong&gt;max safe&lt;/strong&gt; &lt;code&gt;num_ctx&lt;/code&gt; for your exact GPU.&lt;br /&gt; ¬≠Instead of guessing, it tells you: &lt;em&gt;&amp;quot;Your GPU can handle around&lt;/em&gt; &lt;strong&gt;&lt;em&gt;XXXXX tokens&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;&amp;quot;&lt;/em&gt;.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;You can then run it with (for example):&lt;br /&gt; ollama run deepseek-r1:latest --num_ctx 6000&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;You can check &lt;a href="https://gpuforllm.com"&gt;what your GPU can handle here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;(Also updated for Llama 4 Native Multimodal, Gemma 3, and the recent Qwen changes).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;It's open source on GitHub&lt;/strong&gt; if you want to check the math:&lt;br /&gt; &lt;a href="https://github.com/GPUforLLM/llm-vram-calculator"&gt;https://github.com/GPUforLLM/llm-vram-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope this helps visualize the hidden costs!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;‚ÑπÔ∏è If you hit an edge case or weird result, post your specs - I‚Äôll update the calc&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Enough-Cat7020"&gt; /u/Enough-Cat7020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p912ua/deepseekr114b_uses_2_more_vram_at_the_same/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p912ua/deepseekr114b_uses_2_more_vram_at_the_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p912ua/deepseekr114b_uses_2_more_vram_at_the_same/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-28T17:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9930u</id>
    <title>Where are you supposed to place ModelFile?</title>
    <updated>2025-11-28T23:12:32+00:00</updated>
    <author>
      <name>/u/sharpestknees</name>
      <uri>https://old.reddit.com/user/sharpestknees</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm new to local LLM's and Ollama. My OS is Windows 11. I have not had any success finding the solution to my problem online. Ultimately, I'm trying to import 2 GGUF files I installed from HuggingFace:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Qwen3-30B-A3B-Instruct-2507-Q5\_K\_M.gguf Qwen3-30B-A3B-Thinking-2507-Q5\_K\_M.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I've read instructions online for how to import GGUF files into Ollama via CLI. I have created the ModelFile with the FROM command pointing to the correct filepath and filename for the model. (currently i'm only trying to import Qwen3-30B-A3B-Instruct-2507-Q5_K_M.gguf).&lt;/p&gt; &lt;p&gt;Importantly, I've also confirmed the .txt extension is removed. There is no extension on the file.&lt;/p&gt; &lt;p&gt;My blocker is once I get to the CLI, the create command doesn't work. I receive this error:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Error: no Modelfile or safetensors files found&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I've placed the ModelFile in various different places, from the models folder itself, to the install location, and so on. I can't seem to get a straight answer online on where to place this file. I'm assuming it being in the wrong location is what's causing the error above.&lt;/p&gt; &lt;p&gt;Any help is appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sharpestknees"&gt; /u/sharpestknees &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9930u/where_are_you_supposed_to_place_modelfile/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9930u/where_are_you_supposed_to_place_modelfile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9930u/where_are_you_supposed_to_place_modelfile/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-28T23:12:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9lsie</id>
    <title>Ai like grok companion</title>
    <updated>2025-11-29T10:31:09+00:00</updated>
    <author>
      <name>/u/kikothepug</name>
      <uri>https://old.reddit.com/user/kikothepug</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kikothepug"&gt; /u/kikothepug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AIAssisted/comments/1p9ku2u/ai_like_grok_companion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9lsie/ai_like_grok_companion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9lsie/ai_like_grok_companion/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T10:31:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9lvvt</id>
    <title>Needing help with tool_calls in Ollama Python library</title>
    <updated>2025-11-29T10:37:01+00:00</updated>
    <author>
      <name>/u/evpneqbzhnpub</name>
      <uri>https://old.reddit.com/user/evpneqbzhnpub</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/evpneqbzhnpub"&gt; /u/evpneqbzhnpub &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1p9kdyf/needing_help_with_tool_calls_in_ollama_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9lvvt/needing_help_with_tool_calls_in_ollama_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9lvvt/needing_help_with_tool_calls_in_ollama_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T10:37:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9kxfc</id>
    <title>A Lightweight Go + Redis + Ollama Framework for Building Reusable 0‚Äì100 Text Scoring Endpoints</title>
    <updated>2025-11-29T09:37:31+00:00</updated>
    <author>
      <name>/u/Mussky</name>
      <uri>https://old.reddit.com/user/Mussky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This project is a local LLM-based scoring service that turns any text into a consistent 0‚Äì100 score. It uses Ollama to generate and run customizable evaluation prompts, stores them in Redis, and exposes a simple API + UI for managing and analyzing text.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mg52/ai-analyzer"&gt;https://github.com/mg52/ai-analyzer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mussky"&gt; /u/Mussky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9kxfc/a_lightweight_go_redis_ollama_framework_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9kxfc/a_lightweight_go_redis_ollama_framework_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9kxfc/a_lightweight_go_redis_ollama_framework_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T09:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9xzm4</id>
    <title>Declarative RAG for any DB, any LLM (Feedback Wanted!)</title>
    <updated>2025-11-29T19:48:29+00:00</updated>
    <author>
      <name>/u/returncode0</name>
      <uri>https://old.reddit.com/user/returncode0</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/returncode0"&gt; /u/returncode0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AI_Agents/comments/1p9xz0k/declarative_rag_for_any_db_any_llm_feedback_wanted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9xzm4/declarative_rag_for_any_db_any_llm_feedback_wanted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9xzm4/declarative_rag_for_any_db_any_llm_feedback_wanted/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T19:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9k34w</id>
    <title>Runlevel 3 in debian .LAN without X all resources to llama.cpp</title>
    <updated>2025-11-29T08:44:54+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p9k34w/runlevel_3_in_debian_lan_without_x_all_resources/"&gt; &lt;img alt="Runlevel 3 in debian .LAN without X all resources to llama.cpp" src="https://preview.redd.it/udc9k4bls54g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=518edcdbb220adfa51594b3af405b4bc78f658bd" title="Runlevel 3 in debian .LAN without X all resources to llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Serve gguf over LAN with web interface&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/udc9k4bls54g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9k34w/runlevel_3_in_debian_lan_without_x_all_resources/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9k34w/runlevel_3_in_debian_lan_without_x_all_resources/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T08:44:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9idre</id>
    <title>Using Ollama (qwen2.5-vl) to auto-tag RAW photos in a Python TUI</title>
    <updated>2025-11-29T07:00:40+00:00</updated>
    <author>
      <name>/u/AppropriatePublic687</name>
      <uri>https://old.reddit.com/user/AppropriatePublic687</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p9idre/using_ollama_qwen25vl_to_autotag_raw_photos_in_a/"&gt; &lt;img alt="Using Ollama (qwen2.5-vl) to auto-tag RAW photos in a Python TUI" src="https://b.thumbs.redditmedia.com/jDuq0VNI7Vz1Vdnsxm7t_uNbARwiaxK94NpO0HBHhzQ.jpg" title="Using Ollama (qwen2.5-vl) to auto-tag RAW photos in a Python TUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/t9t9w89t854g1.jpg?width=3117&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb0c989ed1cfad1be1a29bcfd7b18206b7d5f49a"&gt;https://preview.redd.it/t9t9w89t854g1.jpg?width=3117&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb0c989ed1cfad1be1a29bcfd7b18206b7d5f49a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1p9idre/video/nsxcqgqt2h4g1/player"&gt;https://reddit.com/link/1p9idre/video/nsxcqgqt2h4g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I heard the feedback about having a preview/dry run mode and it is implemented with a few other quality of life upgrades recommended by some other users!&lt;/p&gt; &lt;p&gt;&lt;code&gt;[UPDATE v1.1 - DRY RUN &amp;amp; CACHE LIVE]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deployed in v1.1:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;üõ°Ô∏è Dry Run Protocol:&lt;/strong&gt; Simulate the entire sorting workflow without touching a single file. Zero risk.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚ö° Phantom Cache:&lt;/strong&gt; If you like the preview, hit &amp;quot;Execute.&amp;quot; The engine now caches the AI decision logic, so the real run happens instantly. No re-waiting for the LLM.&lt;/li&gt; &lt;li&gt;„èí - preview logs located in .fixxer and outpput as: preview_2025-11-30_135524.txt&lt;/li&gt; &lt;li&gt;üîÆ NEW Feature thanks to &lt;a href="/u/hideo_kuze_"&gt;u/hideo_kuze_&lt;/a&gt;'s suggestion! &lt;/li&gt; &lt;li&gt;üî¥ Live in the repo! &lt;a href="http://www.github.com/Bandwagonvibes/fixxer"&gt;www.github.com/Bandwagonvibes/fixxer&lt;/a&gt; for more videos of the interface: &lt;a href="https://oaklens.art/dev"&gt;https://oaklens.art/dev&lt;/a&gt; (I'm prepping gif's for github for those that like a 1 stop shop)&lt;/li&gt; &lt;li&gt;Have a great Sunday! -Nick&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ideally, I‚Äôd be out over the holiday but I've been in the lab! Initially I was building this tool for my personal digital toolkit but as time has progressed I've felt that this could be practical for photographers or anyone that just wants to point at a messy folder full of photos and have the AI do the work. 100% offline leveraging Ollama and the qwen 2.5vl model. Models are hot swappable. Respects different workflows. Keep your images at home where they belong lol&lt;/p&gt; &lt;p&gt;I shoot a lot of street photography (Oakland), and my archival workflow was a mess. I didn't want to upload RAW files to the cloud just to get AI tagging, so I built a local tool to do it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;It's called FIXXER.&lt;/strong&gt; It runs in the terminal (built with Textual). It uses &lt;code&gt;qwen2.5-vl&lt;/code&gt; via Ollama to &amp;quot;see&amp;quot; the photos and keyword them, and CLIP embeddings to group duplicates.&lt;/p&gt; &lt;p&gt;It‚Äôs running on my M4 Macbook Air and can stack burst, cull singles, and AI rename images and place them in keyword folders, as well as grab a session name (samples from 3 images from ingest) for the parent directory for 150 photos in about 13 min. All moves are hash verified, sidecar logs, and raw to (new) AI name logs.&lt;/p&gt; &lt;p&gt;Just pushed the repo if anyone wants to roast my code or try it out this weekend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; [&lt;a href="https://github.com/BandwagonVibes/fixxer"&gt;https://github.com/BandwagonVibes/fixxer&lt;/a&gt;] &lt;strong&gt;Pics of the interface:&lt;/strong&gt; [&lt;a href="https://oaklens.art/dev"&gt;oaklens.art/dev&lt;/a&gt;]&lt;/p&gt; &lt;p&gt;Happy Friday. ü•É&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppropriatePublic687"&gt; /u/AppropriatePublic687 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9idre/using_ollama_qwen25vl_to_autotag_raw_photos_in_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9idre/using_ollama_qwen25vl_to_autotag_raw_photos_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9idre/using_ollama_qwen25vl_to_autotag_raw_photos_in_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T07:00:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9l2do</id>
    <title>Ollama vs Blender</title>
    <updated>2025-11-29T09:46:42+00:00</updated>
    <author>
      <name>/u/Digital-Building</name>
      <uri>https://old.reddit.com/user/Digital-Building</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1p9l2do/ollama_vs_blender/"&gt; &lt;img alt="Ollama vs Blender" src="https://external-preview.redd.it/zj7DSc3w-zwxzS2_x9K_PvO2eD7C1IjPQMGbnhyQXVU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e0fb3cbb460579fb92b2abb3e828cb0973f7f48" title="Ollama vs Blender" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there &lt;/p&gt; &lt;p&gt;Have you seen these latest attempts in using lacal LLMs to interact with Blender?&lt;/p&gt; &lt;p&gt;In this video they used Gemma 3:4b It seems that small model cannot do much unless using very accurate prompts.&lt;/p&gt; &lt;p&gt;What model side would be reasonable to expect some reasonable outcomes with Blender MCP?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Digital-Building"&gt; /u/Digital-Building &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/0PSOCFHBAfw?si=8CsHSed4DcLCNIiZ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1p9l2do/ollama_vs_blender/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1p9l2do/ollama_vs_blender/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-29T09:46:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1panv0r</id>
    <title>Perplexity AI PRO - 1 YEAR at 90% Discount ‚Äì Don‚Äôt Miss Out!</title>
    <updated>2025-11-30T17:24:17+00:00</updated>
    <author>
      <name>/u/Verza-</name>
      <uri>https://old.reddit.com/user/Verza-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1panv0r/perplexity_ai_pro_1_year_at_90_discount_dont_miss/"&gt; &lt;img alt="Perplexity AI PRO - 1 YEAR at 90% Discount ‚Äì Don‚Äôt Miss Out!" src="https://preview.redd.it/32gpcb06if4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3341dbd11ab8b6f14ea4e386a8f1fb7c573efcc3" title="Perplexity AI PRO - 1 YEAR at 90% Discount ‚Äì Don‚Äôt Miss Out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get Perplexity AI PRO (1-Year) ‚Äì at 90% OFF!&lt;/p&gt; &lt;p&gt;Order here: &lt;a href="https://cheapgpts.store/Perplexity"&gt;CHEAPGPT.STORE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Plan: 12 Months&lt;/p&gt; &lt;p&gt;üí≥ Pay with: PayPal or Revolut&lt;/p&gt; &lt;p&gt;Reddit reviews: &lt;a href="https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu"&gt;FEEDBACK POST&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TrustPilot: &lt;a href="https://www.trustpilot.com/review/cheapgpt.store"&gt;TrustPilot FEEDBACK&lt;/a&gt;&lt;br /&gt; Bonus: Apply code PROMO5 for $2 OFF your order! &lt;/p&gt; &lt;p&gt;BONUS!: Enjoy the AI Powered automated web browser. (Presented by Perplexity) included!&lt;/p&gt; &lt;p&gt;Trusted and the cheapest!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Verza-"&gt; /u/Verza- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/32gpcb06if4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1panv0r/perplexity_ai_pro_1_year_at_90_discount_dont_miss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1panv0r/perplexity_ai_pro_1_year_at_90_discount_dont_miss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T17:24:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1paj3f7</id>
    <title>Help me please</title>
    <updated>2025-11-30T14:06:13+00:00</updated>
    <author>
      <name>/u/Kriss_-</name>
      <uri>https://old.reddit.com/user/Kriss_-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm new here, but would like to try using ollama. Can anyone help me with setting it up please? I tried finding tutorials on YouTube but they didn't fit. Or I'm just stupid. Anyways. Help me please &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kriss_-"&gt; /u/Kriss_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1paj3f7/help_me_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1paj3f7/help_me_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1paj3f7/help_me_please/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T14:06:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pahksd</id>
    <title>What models can i use with a pc without gpu?</title>
    <updated>2025-11-30T12:52:15+00:00</updated>
    <author>
      <name>/u/Neat_Nobody1849</name>
      <uri>https://old.reddit.com/user/Neat_Nobody1849</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neat_Nobody1849"&gt; /u/Neat_Nobody1849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1pahkcf/what_models_can_i_use_with_a_pc_without_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pahksd/what_models_can_i_use_with_a_pc_without_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pahksd/what_models_can_i_use_with_a_pc_without_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T12:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pag2c6</id>
    <title>Built a tool to easily self-host AI models on AWS - now I need uncensored models to test it for Red Teaming</title>
    <updated>2025-11-30T11:26:11+00:00</updated>
    <author>
      <name>/u/dumbelco</name>
      <uri>https://old.reddit.com/user/dumbelco</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a &amp;quot;deploy-and-destroy&amp;quot; tool that spins up a self-hosted AI lab on AWS (running Ollama/Open WebUI on a GPU instance).&lt;/p&gt; &lt;p&gt;Now that the infrastructure is working, I want to test it with some actual cybersecurity workflows. I'm looking for recommendations for models available on Ollama that are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Strictly uncensored&lt;/strong&gt; (won't refuse to generate Python scripts for CTFs or pentesting/red teaming research).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart at coding&lt;/strong&gt; (can handle complex logic without breaking).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any recommendations?&lt;/p&gt; &lt;p&gt;Also, for models like these, should I stick to downloading models directly from Ollama, or is it worth looking into importing models from Hugging Face instead?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dumbelco"&gt; /u/dumbelco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pag2c6/built_a_tool_to_easily_selfhost_ai_models_on_aws/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pag2c6/built_a_tool_to_easily_selfhost_ai_models_on_aws/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pag2c6/built_a_tool_to_easily_selfhost_ai_models_on_aws/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T11:26:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pam6dr</id>
    <title>Built a Modular Agentic RAG System ‚Äì Zero Boilerplate, Full Customization</title>
    <updated>2025-11-30T16:17:32+00:00</updated>
    <author>
      <name>/u/CapitalShake3085</name>
      <uri>https://old.reddit.com/user/CapitalShake3085</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pam6dr/built_a_modular_agentic_rag_system_zero/"&gt; &lt;img alt="Built a Modular Agentic RAG System ‚Äì Zero Boilerplate, Full Customization" src="https://preview.redd.it/vlxnbaqn2f4g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=f25c990e106612bf0d2b452ea1356240ec2e15f9" title="Built a Modular Agentic RAG System ‚Äì Zero Boilerplate, Full Customization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CapitalShake3085"&gt; /u/CapitalShake3085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vlxnbaqn2f4g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pam6dr/built_a_modular_agentic_rag_system_zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pam6dr/built_a_modular_agentic_rag_system_zero/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T16:17:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pafex0</id>
    <title>EGPU for ai use?</title>
    <updated>2025-11-30T10:46:55+00:00</updated>
    <author>
      <name>/u/R0B0t1C_Cucumber</name>
      <uri>https://old.reddit.com/user/R0B0t1C_Cucumber</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I posted a week or two ago about looking for an agentic coder type do-dad like Claude CLI and this awesome community pointed me to aidler/ollama (though OI +llama ccp work fine too if anyone is looking at alternatives). Any way I found the sweet spot for me 14b Q4 llms seem to be the sweet spot for a 4070Ti between performance and quality. &lt;/p&gt; &lt;p&gt;Now looking around I found I have some spare hardware I wanted to see if anyone has tried anything like this... Now again, this is just for me to tinker with... but I have a spare intel ARC 770 16GB Vram and I also found laying around an EGPU enclosure with a 400w dedicated power supply in it... Connects over thunderbolt.... Could I somehow leverage this extra compute/vram through Ollama ? I wouldn't actually want anything to display through the card, I just want its resource.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R0B0t1C_Cucumber"&gt; /u/R0B0t1C_Cucumber &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pafex0/egpu_for_ai_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pafex0/egpu_for_ai_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pafex0/egpu_for_ai_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T10:46:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pamhd1</id>
    <title>Made a tutorial video for Ollama</title>
    <updated>2025-11-30T16:29:57+00:00</updated>
    <author>
      <name>/u/EducationNo3524</name>
      <uri>https://old.reddit.com/user/EducationNo3524</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pamhd1/made_a_tutorial_video_for_ollama/"&gt; &lt;img alt="Made a tutorial video for Ollama" src="https://external-preview.redd.it/EpedG-LsE5kdpTUnEQBwIh0vb_HRcg6EiQrFv-7KiA4.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac718decbbcd25df9cc1d7441146591cbb9cb8c9" title="Made a tutorial video for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a tutorial video for Ollama, and also showed people how to use it on mobile phones. Would anyone be willing to support me?plzzzzz&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EducationNo3524"&gt; /u/EducationNo3524 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/OqDMJlPV4tY?si=jUC2uwpfVzOylwTG"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pamhd1/made_a_tutorial_video_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pamhd1/made_a_tutorial_video_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T16:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pavisz</id>
    <title>CPU advice</title>
    <updated>2025-11-30T22:30:03+00:00</updated>
    <author>
      <name>/u/nECr0MaNCeD</name>
      <uri>https://old.reddit.com/user/nECr0MaNCeD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so I‚Äôm going to take the plunge into AI. I want to run everything locally and I was recommended Ollama.&lt;/p&gt; &lt;p&gt;my 4090 will be fine, but when I googled my CPU, I got a strange reply from the AI, saying that my 9800x3d was non-standard. Can someone shed some light onto this? &lt;/p&gt; &lt;p&gt;In case it matters to someone, I am running 64GB of DDR5 7200/CL32.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nECr0MaNCeD"&gt; /u/nECr0MaNCeD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pavisz/cpu_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pavisz/cpu_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pavisz/cpu_advice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T22:30:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa4x3y</id>
    <title>CUA Local Opensource</title>
    <updated>2025-11-30T00:55:27+00:00</updated>
    <author>
      <name>/u/Goat_bless</name>
      <uri>https://old.reddit.com/user/Goat_bless</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pa4x3y/cua_local_opensource/"&gt; &lt;img alt="CUA Local Opensource" src="https://preview.redd.it/96acfm1pla4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=927ce5830d8f7bf3fe412291022af0d437c6a60b" title="CUA Local Opensource" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bonjour √† tous,&lt;/p&gt; &lt;p&gt;I've created my biggest project to date.&lt;br /&gt; A local open-source computer agent, it uses a fairly complex architecture to perform a very large number of tasks, if not all tasks.&lt;br /&gt; I‚Äôm not going to write too much to explain how it all works; those who are interested can check the GitHub, it‚Äôs very well detailed.&lt;br /&gt; In summary:&lt;br /&gt; For each user input, the agent understands whether it needs to speak or act.&lt;br /&gt; If it needs to speak, it uses memory and context to produce appropriate sentences.&lt;br /&gt; If it needs to act, there are two choices:&lt;/p&gt; &lt;p&gt;A simple action: open an application, lower the volume, launch Google, open a folder...&lt;br /&gt; Everything is done in a single action.&lt;/p&gt; &lt;p&gt;A complex action: browse the internet, create a file with data retrieved online, interact with an application...&lt;br /&gt; Here it goes through an orchestrator that decides what actions to take (multistep) and checks that each action is carried out properly until the global task is completed.&lt;br /&gt; How?&lt;br /&gt; Architecture of a complex action:&lt;br /&gt; LLM orchestrator receives the global task and decides the next action.&lt;br /&gt; For internet actions: CUA first attempts Playwright ‚Äî 80% of cases solved.&lt;br /&gt; If it fails (and this is where it gets interesting):&lt;br /&gt; It uses CUA VISION: Screenshot ‚Äî VLM1 sees the page and suggests what to do ‚Äî Data detection on the page (Ominparser: YOLO + Florence) + PaddleOCR ‚Äî Annotation of the data on the screenshot ‚Äî VLM2 sees the annotated screen and tells which ID to click ‚Äî Pyautogui clicks on the coordinates linked to the ID ‚Äî Loops until Task completed.&lt;br /&gt; In both cases (complex or simple) return to the orchestrator which finishes all actions and sends a message to the user once the task is completed.&lt;/p&gt; &lt;p&gt;This agent has the advantage of running locally with only my 8GB VRAM; I use the LLM models: qwen2.5, VLM: qwen2.5vl and qwen3vl.&lt;br /&gt; If you have more VRAM, with better models you‚Äôll gain in performance and speed.&lt;br /&gt; Currently, this agent can solve 80‚Äì90% of the tasks we can perform on a computer, and I‚Äôm open to improvements or knowledge-sharing to make it a common and useful project for everyone.&lt;br /&gt; The GitHub link: &lt;a href="https://github.com/SpendinFR/CUAOS"&gt;https://github.com/SpendinFR/CUAOS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Goat_bless"&gt; /u/Goat_bless &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/96acfm1pla4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pa4x3y/cua_local_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pa4x3y/cua_local_opensource/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T00:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pah9zz</id>
    <title>Uncensored ollama models for my pc</title>
    <updated>2025-11-30T12:36:16+00:00</updated>
    <author>
      <name>/u/Automatic-Pin9116</name>
      <uri>https://old.reddit.com/user/Automatic-Pin9116</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My pc is i3, 8gb ram, no dedicated vram or gpu. I want a model that'll run in this pc. Fully uncensored. Also maybe roleplay too, though im looking more of uncensored ai models where i can use restricted words (like cu#t and pe###). i just want a open, uncensored, good, knowledge full ai to talk to freely with freedom.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Automatic-Pin9116"&gt; /u/Automatic-Pin9116 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pah9zz/uncensored_ollama_models_for_my_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pah9zz/uncensored_ollama_models_for_my_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pah9zz/uncensored_ollama_models_for_my_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-11-30T12:36:16+00:00</published>
  </entry>
</feed>
