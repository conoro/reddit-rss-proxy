<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2026-01-29T15:14:41+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1qoi1op</id>
    <title>Tiny Ollama-powered CLI: Natural Language to shell commands (qwen2.5-coder default, easy model switch)</title>
    <updated>2026-01-27T16:08:37+00:00</updated>
    <author>
      <name>/u/ykushch</name>
      <uri>https://old.reddit.com/user/ykushch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/"&gt; &lt;img alt="Tiny Ollama-powered CLI: Natural Language to shell commands (qwen2.5-coder default, easy model switch)" src="https://external-preview.redd.it/JnNJEcF3jbp7PevNxANU0riqBFifG0zNxzy_XGtEtCw.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=3d94c3882da0b59d8f2197ce3c151ee720e44138" title="Tiny Ollama-powered CLI: Natural Language to shell commands (qwen2.5-coder default, easy model switch)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CLI that uses Ollama locally to translate natural language into shell commands. Supports &lt;code&gt;--model&lt;/code&gt; / &lt;code&gt;ASK_MODEL&lt;/code&gt; and &lt;code&gt;OLLAMA_HOST&lt;/code&gt;.&lt;br /&gt; Repo: &lt;a href="https://github.com/ykushch/ask"&gt;https://github.com/ykushch/ask&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/k5ldp45d1xfg1.gif"&gt;ask - natural language to shell commands&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ykushch"&gt; /u/ykushch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qoi1op/tiny_ollamapowered_cli_natural_language_to_shell/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T16:08:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qousvs</id>
    <title>GPU advice for entry level AI</title>
    <updated>2026-01-27T23:44:30+00:00</updated>
    <author>
      <name>/u/fulefesi</name>
      <uri>https://old.reddit.com/user/fulefesi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My current desktop pc: h77ds3h mobo pcie gen 3, xeon e3 1275v2 4c/8t ivy bridge, 24gb ddr3 1600mhz bundled in old atx case with side vents at bottom and only 1 fan (80mm rear fan)&lt;/p&gt; &lt;p&gt;Purpose: learning, experimenting with entry-level AI, 1–3B or 7b (if possible) coding LLMs 4-bit quantized + LoRA inference. I only work with Python for data analysis, libraries like pandas, short scripts mainly. Hopefully upgrade entire system + new architecture GPU in 2028&lt;/p&gt; &lt;p&gt;Because of budget constrains and local availability where i'm currently stationed, i have very few contenders (listed as new): rtx 3050 8gb asus tuf (250$), rtx 5060 8gb msi ventus (320$), rtx 3060 12gb asus dual geforce v2 OC (320$)&lt;/p&gt; &lt;p&gt;What/how would you recommend to start with?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fulefesi"&gt; /u/fulefesi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qousvs/gpu_advice_for_entry_level_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qousvs/gpu_advice_for_entry_level_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qousvs/gpu_advice_for_entry_level_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T23:44:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp15k3</id>
    <title>Why does using ollama run claude with glm-4.7-flash have zero memory?</title>
    <updated>2026-01-28T04:17:41+00:00</updated>
    <author>
      <name>/u/ElRayoPeronizador</name>
      <uri>https://old.reddit.com/user/ElRayoPeronizador</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I started claude code with &lt;code&gt;ollama launch claude&lt;/code&gt;, it start and I have access to the prompt, and it will reply when I ask something, but I'm confused about how ollama / claude code handles conversation memory when using the glm-4.7-flash model. It seems like each new command resets the context completely, regardless of previous messages.&lt;/p&gt; &lt;p&gt;Example of the issue:&lt;/p&gt; &lt;p&gt;❯ calculate 1+1&lt;/p&gt; &lt;p&gt;● 1 + 1 = 2&lt;/p&gt; &lt;p&gt;❯ add 4 to that&lt;/p&gt; &lt;p&gt;● I'm not sure what you're referring to. Could you clarify what you'd like me to add 4 to?&lt;/p&gt; &lt;p&gt;Even though I just got &amp;quot;2&amp;quot; in the previous message, the model has no memory of it.&lt;/p&gt; &lt;p&gt;Is this expected behavior? Am I configuring something wrong, or is the glm-4.7-flash model simply not designed for multi-turn conversations? I've been trying to use Claude with local models for longer sessions but the memory seems to be completely reset on every new prompt.&lt;/p&gt; &lt;p&gt;Any insight would be appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElRayoPeronizador"&gt; /u/ElRayoPeronizador &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp15k3/why_does_using_ollama_run_claude_with_glm47flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp15k3/why_does_using_ollama_run_claude_with_glm47flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qp15k3/why_does_using_ollama_run_claude_with_glm47flash/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T04:17:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo9tsi</id>
    <title>NotebookLM For Teams</title>
    <updated>2026-01-27T10:04:14+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qo9tsi/notebooklm_for_teams/"&gt; &lt;img alt="NotebookLM For Teams" src="https://external-preview.redd.it/dmgyMjVieGg4dmZnMSLy8o5ur8LGz7971UKmCZkldIebkAvR30ypzPlMaeND.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0496edb208cda2f95e1b3739aef3d3d21e3a42e3" title="NotebookLM For Teams" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.&lt;/p&gt; &lt;p&gt;In short, it is NotebookLM for teams, as it connects any LLM to your internal knowledge sources (search engines, Drive, Calendar, Notion, Obsidian, and 15+ other connectors) and lets you chat with it in real time alongside your team.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here's a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Self-Hostable (with docker support)&lt;/li&gt; &lt;li&gt;Real Time Collaborative Chats&lt;/li&gt; &lt;li&gt;Real Time Commenting&lt;/li&gt; &lt;li&gt;Deep Agentic Agent&lt;/li&gt; &lt;li&gt;RBAC (Role Based Access for Teams Members)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs (OpenAI spec with LiteLLM)&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Local TTS/STT support.&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Slide Creation Support&lt;/li&gt; &lt;li&gt;Multilingual Podcast Support&lt;/li&gt; &lt;li&gt;Video Creation Agent&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zxqevbwh8vfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qo9tsi/notebooklm_for_teams/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qo9tsi/notebooklm_for_teams/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-27T10:04:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp0mjn</id>
    <title>I made a open source CLI ollama into terminal</title>
    <updated>2026-01-28T03:53:15+00:00</updated>
    <author>
      <name>/u/Loud-Description-460</name>
      <uri>https://old.reddit.com/user/Loud-Description-460</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qp0mjn/i_made_a_open_source_cli_ollama_into_terminal/"&gt; &lt;img alt="I made a open source CLI ollama into terminal" src="https://external-preview.redd.it/0e9XDVAmFcgNkfKVuVMILaTerqoxbi7p92dyaNqELF8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac1bffbca1b66136acbd5d0bfd9be02470d42ba0" title="I made a open source CLI ollama into terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud-Description-460"&gt; /u/Loud-Description-460 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/guirguispierre/Llaminal"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp0mjn/i_made_a_open_source_cli_ollama_into_terminal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qp0mjn/i_made_a_open_source_cli_ollama_into_terminal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T03:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpkcd6</id>
    <title>What is the best model that I can run</title>
    <updated>2026-01-28T18:54:05+00:00</updated>
    <author>
      <name>/u/Legitimate_Worry5069</name>
      <uri>https://old.reddit.com/user/Legitimate_Worry5069</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a base 5060 8gb and was wondering which is the best model.i can run. I've heard terms like quantized and unquantized and 8b and 70b and so on. (I'm doing this just for fun and it does seem cool)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Legitimate_Worry5069"&gt; /u/Legitimate_Worry5069 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpkcd6/what_is_the_best_model_that_i_can_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpkcd6/what_is_the_best_model_that_i_can_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qpkcd6/what_is_the_best_model_that_i_can_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T18:54:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp390m</id>
    <title>[Update] RX 9070 XT Vulkan vs ROCm: Detailed follow-up with more metrics</title>
    <updated>2026-01-28T06:02:19+00:00</updated>
    <author>
      <name>/u/Due_Pea_372</name>
      <uri>https://old.reddit.com/user/Due_Pea_372</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Follow-up to my &lt;a href="https://www.reddit.com/r/cachyos/comments/1qnpcm1/vulkan_vs_rocm_on_rx_9070xt_rdna4_9_faster_50/"&gt;previous post&lt;/a&gt;. Improved methodology and more detailed measurements.&lt;/p&gt; &lt;p&gt;**Setup**&lt;/p&gt; &lt;p&gt;- GPU: AMD RX 9070 XT (gfx1201, RDNA4, 16GB)&lt;/p&gt; &lt;p&gt;- Model: qwen3-coder:30b&lt;/p&gt; &lt;p&gt;- Ollama: 0.15.1 (ollama-vulkan / ollama-rocm from AUR)&lt;/p&gt; &lt;p&gt;- OS: Arch Linux&lt;/p&gt; &lt;p&gt;**Methodology**&lt;/p&gt; &lt;p&gt;- Model stopped and reloaded before each session for clean VRAM state&lt;/p&gt; &lt;p&gt;- Baseline VRAM/GTT measured before model load&lt;/p&gt; &lt;p&gt;- First run marked as warmup and excluded from averages&lt;/p&gt; &lt;p&gt;- Power, clock, GTT, GPU-Busy, MEM-Busy sampled every 100ms during inference&lt;/p&gt; &lt;p&gt;**Results**&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Vulkan&lt;/th&gt; &lt;th align="left"&gt;ROCm&lt;/th&gt; &lt;th align="left"&gt;Diff&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Gen t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;51.58&lt;/td&gt; &lt;td align="left"&gt;48.20&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+7.0%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt t/s&lt;/td&gt; &lt;td align="left"&gt;855.96&lt;/td&gt; &lt;td align="left"&gt;865.39&lt;/td&gt; &lt;td align="left"&gt;-1.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Power&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;65.3 W&lt;/td&gt; &lt;td align="left"&gt;149.4 W&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;-56%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.79 t/W&lt;/td&gt; &lt;td align="left"&gt;0.32 t/W&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+145%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU-Clock&lt;/td&gt; &lt;td align="left"&gt;1988 MHz&lt;/td&gt; &lt;td align="left"&gt;3621 MHz&lt;/td&gt; &lt;td align="left"&gt;-45%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU-Busy&lt;/td&gt; &lt;td align="left"&gt;33%&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;-67%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MEM-Busy&lt;/td&gt; &lt;td align="left"&gt;12%&lt;/td&gt; &lt;td align="left"&gt;8%&lt;/td&gt; &lt;td align="left"&gt;+50%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;VRAM (actual)&lt;/td&gt; &lt;td align="left"&gt;15703 MB&lt;/td&gt; &lt;td align="left"&gt;16024 MB&lt;/td&gt; &lt;td align="left"&gt;-2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Temp&lt;/td&gt; &lt;td align="left"&gt;49°C&lt;/td&gt; &lt;td align="left"&gt;47°C&lt;/td&gt; &lt;td align="left"&gt;+4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TTFT (warm)&lt;/td&gt; &lt;td align="left"&gt;25.8 ms&lt;/td&gt; &lt;td align="left"&gt;25.4 ms&lt;/td&gt; &lt;td align="left"&gt;+2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TTFT (cold)&lt;/td&gt; &lt;td align="left"&gt;89.9 ms&lt;/td&gt; &lt;td align="left"&gt;77.9 ms&lt;/td&gt; &lt;td align="left"&gt;+15%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;**Key findings**&lt;/p&gt; &lt;p&gt;**1. ROCm works harder, not smarter**&lt;br /&gt; ROCm runs at nearly 2x the clock speed (3621 vs 1988 MHz) with 100% GPU utilization yet delivers worse performance than Vulkan at 33% utilization. This suggests the ROCm compute kernels are not optimized for RDNA4.&lt;/p&gt; &lt;p&gt;**2. Power efficiency is dramatic**&lt;br /&gt; Vulkan delivers 0.79 tokens per watt vs ROCm's 0.32 t/W – that's 145% better efficiency.&lt;/p&gt; &lt;p&gt;**3. Prompt processing is identical**&lt;br /&gt; Both backends process prompts at ~860 t/s, so the inefficiency is specifically in token generation.&lt;/p&gt; &lt;p&gt;**Conclusion**&lt;br /&gt; For RDNA4 users: use Vulkan. You get better performance while your GPU runs cooler and quieter.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Pea_372"&gt; /u/Due_Pea_372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp390m/update_rx_9070_xt_vulkan_vs_rocm_detailed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp390m/update_rx_9070_xt_vulkan_vs_rocm_detailed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qp390m/update_rx_9070_xt_vulkan_vs_rocm_detailed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T06:02:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpffvq</id>
    <title>How to Prevent Context Degradation on Local LLM?</title>
    <updated>2026-01-28T16:05:27+00:00</updated>
    <author>
      <name>/u/Ok_Most9659</name>
      <uri>https://old.reddit.com/user/Ok_Most9659</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have run a few local LLMs, which start off great, but inevitably go off the rails as the context window grows resulting in illogical nonsense answers to my prompts. My understanding is this is Context Degradation and is related to the system running out of RAM to hold the contents of the context window. &lt;/p&gt; &lt;p&gt;What I don't understand is why does the system/LLM not simply off-load the prior context window into a file to not consume RAM, then reference that file via RAG when the next prompt is entered within that same context window?&lt;br /&gt; Would this not alleviate the RAM issue and let you continue within the same context window until your harddrive fills? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Most9659"&gt; /u/Ok_Most9659 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpffvq/how_to_prevent_context_degradation_on_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpffvq/how_to_prevent_context_degradation_on_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qpffvq/how_to_prevent_context_degradation_on_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T16:05:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qphrp7</id>
    <title>Ollama cloud / Kimi 2.5 / MoltBolt</title>
    <updated>2026-01-28T17:26:34+00:00</updated>
    <author>
      <name>/u/ayoubhak</name>
      <uri>https://old.reddit.com/user/ayoubhak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;Please excuse my basic understanding, but I’m looking for an efficient way to use an agent to breakdown my idea into actionable products.&lt;/p&gt; &lt;p&gt;Since I own a MacBook m4, I won’t be able to run the kimi 2.5 in this machine as a model with moltbolt, however I can subscribe to ollama cloud and have it use this model and I can basically just text it or share product ideas from Pinterest.&lt;/p&gt; &lt;p&gt;Please let me know if my reasoning is correct, and also I still have no idea if I’ll hit quickly a limit with 20$ subscription using this model. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayoubhak"&gt; /u/ayoubhak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qphrp7/ollama_cloud_kimi_25_moltbolt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qphrp7/ollama_cloud_kimi_25_moltbolt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qphrp7/ollama_cloud_kimi_25_moltbolt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T17:26:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpj5nd</id>
    <title>I need help better undestanding how Ollama works</title>
    <updated>2026-01-28T18:12:59+00:00</updated>
    <author>
      <name>/u/Super_Nova02</name>
      <uri>https://old.reddit.com/user/Super_Nova02</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm new to the world of LLM and I'm currently trying to create a simple chatbot. The chatbot needs to follow the mcp and I've chosen Ollama with gemma3 as model. &lt;/p&gt; &lt;p&gt;I have an index.js file in which I manage the request forwarded by my client. The whole project is managed by Docker, and in the code below I pasted the setup. &lt;/p&gt; &lt;p&gt;Right now my whole system goes up. I manually check for an healthy service by Ollama and then I send the request &amp;quot;get sensors&amp;quot;. I receive this error: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Error chatting with Ollama: TypeError: fetch failed&lt;br /&gt; at node:internal/deps/undici/undici:12637:11&lt;br /&gt; at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&lt;br /&gt; at async post (file:///usr/src/app/node_modules/ollama/dist/browser.mjs:150:20)&lt;br /&gt; at async Ollama.processStreamableRequest (file:///usr/src/app/node_modules/ollama/dist/browser.mjs:297:22)&lt;br /&gt; at async file:///usr/src/app/index.js:22:22 {&lt;br /&gt; cause: Error: connect ECONNREFUSED &lt;a href="http://127.0.0.1:11434"&gt;127.0.0.1:11434&lt;/a&gt;&lt;br /&gt; at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1555:16) {&lt;br /&gt; errno: -111,&lt;br /&gt; code: 'ECONNREFUSED',&lt;br /&gt; syscall: 'connect',&lt;br /&gt; address: '127.0.0.1',&lt;br /&gt; port: 11434&lt;br /&gt; }&lt;br /&gt; }&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I don't really understand what I'm doing wrog, if you have an idea and want to share, thank you!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;//docker-compose.yml ollama: image: ollama/ollama:latest container_name: ollama ports: - 11434:11434 networks: - app-network mcp-server: image: node:18-alpine container_name: air-quality-map-mcp ports: - &amp;quot;4000:4000&amp;quot; depends_on: server: condition: service_healthy mongodb: condition: service_started environment: BACKEND_URL: http://server:3000 MONGO_URI: mongodb://mongodb:27017 OLLAMA_API_URL: http://ollama:11434 networks: - app-network working_dir: /usr/src/app volumes: - ./services/mcp-server:/usr/src/app command: &amp;gt; sh -c &amp;quot; npm install --legacy-peer-deps &amp;amp;&amp;amp; node index.js &amp;quot; healthcheck: test: [&amp;quot;CMD&amp;quot;, &amp;quot;wget&amp;quot;, &amp;quot;-qO-&amp;quot;, &amp;quot;http://localhost:4000/health&amp;quot;] interval: 10s timeout: 5s retries: 10 //index.js import express from &amp;quot;express&amp;quot;; import fetch from &amp;quot;node-fetch&amp;quot;; import ollama from &amp;quot;ollama&amp;quot;; const app = express(); app.use(express.json()); const OLLAMA_API_URL = process.env.OLLAMA_API_URL || &amp;quot;http://ollama:11434&amp;quot;; const BACKEND_URL = process.env.BACKEND_URL || &amp;quot;http://server:3000&amp;quot;; const ollamaClient = ollama; app.post(&amp;quot;/chat&amp;quot;, async (req, res) =&amp;gt; { const userMessage = req.body.message; console.log(&amp;quot;User message:&amp;quot;, userMessage); console.log(&amp;quot;Using Ollama API URL:&amp;quot;, OLLAMA_API_URL); try { const response = await ollamaClient.chat({ model: &amp;quot;gemma3&amp;quot;, messages: [{ role: &amp;quot;user&amp;quot;, content: userMessage }], baseUrl: OLLAMA_API_URL, }); const message = response.message?.content || &amp;quot;&amp;quot;; if (message.toLowerCase().includes(&amp;quot;get sensors&amp;quot;)) { const r = await fetch(`${BACKEND_URL}/tools/getSensors`, { method: &amp;quot;POST&amp;quot;, headers: { &amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot; }, body: JSON.stringify({}), }); const data = await r.json(); return res.json({ answer: &amp;quot;Here are the sensors from the system.&amp;quot;, data: data.result, }); } res.json({ answer: message }); } catch (err) { console.error(&amp;quot;Error chatting with Ollama:&amp;quot;, err); res.status(500).json({ error: &amp;quot;Failed to get response from model&amp;quot; }); } }); app.get(&amp;quot;/health&amp;quot;, (req, res) =&amp;gt; res.send(&amp;quot;OK&amp;quot;)); app.listen(4000, () =&amp;gt; { console.log(&amp;quot;MCP server running on port 4000&amp;quot;); }); &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Super_Nova02"&gt; /u/Super_Nova02 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpj5nd/i_need_help_better_undestanding_how_ollama_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpj5nd/i_need_help_better_undestanding_how_ollama_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qpj5nd/i_need_help_better_undestanding_how_ollama_works/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T18:12:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp7m9z</id>
    <title>Kimi K2.5 just blew my mind</title>
    <updated>2026-01-28T10:19:36+00:00</updated>
    <author>
      <name>/u/ComfyTightwad</name>
      <uri>https://old.reddit.com/user/ComfyTightwad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qp7m9z/kimi_k25_just_blew_my_mind/"&gt; &lt;img alt="Kimi K2.5 just blew my mind" src="https://external-preview.redd.it/W-LGhFmxh1k5FgyDxkGaY8p5gAgHMHyYGe0H34d64kM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5347aa6b8161bbfd06d77d438d3da9013013168a" title="Kimi K2.5 just blew my mind" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComfyTightwad"&gt; /u/ComfyTightwad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8o6c6wace2gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp7m9z/kimi_k25_just_blew_my_mind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qp7m9z/kimi_k25_just_blew_my_mind/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T10:19:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpnhzm</id>
    <title>Harmony-format system prompt for long-context persona stability (GPT-OSS / Lumen)</title>
    <updated>2026-01-28T20:47:40+00:00</updated>
    <author>
      <name>/u/slashreboot</name>
      <uri>https://old.reddit.com/user/slashreboot</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slashreboot"&gt; /u/slashreboot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/learnmachinelearning/comments/1qpi035/harmonyformat_system_prompt_for_longcontext/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpnhzm/harmonyformat_system_prompt_for_longcontext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qpnhzm/harmonyformat_system_prompt_for_longcontext/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T20:47:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpp1f9</id>
    <title>Opensource Tech Stack / Local LLM Questions (First Post)</title>
    <updated>2026-01-28T21:44:57+00:00</updated>
    <author>
      <name>/u/Excellent-Baker-1177</name>
      <uri>https://old.reddit.com/user/Excellent-Baker-1177</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent-Baker-1177"&gt; /u/Excellent-Baker-1177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1qotohw/opensource_tech_stack_local_llm_questions_first/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpp1f9/opensource_tech_stack_local_llm_questions_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qpp1f9/opensource_tech_stack_local_llm_questions_first/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T21:44:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qps8w2</id>
    <title>$20/month Ollama Cloud plan Limits</title>
    <updated>2026-01-28T23:50:06+00:00</updated>
    <author>
      <name>/u/wbiggs205</name>
      <uri>https://old.reddit.com/user/wbiggs205</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I thinking on getting $20/month Ollama Cloud plan. But I really can not fine the message limits on it . I just keep 5x. Dose any one what they are ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wbiggs205"&gt; /u/wbiggs205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qps8w2/20month_ollama_cloud_plan_limits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qps8w2/20month_ollama_cloud_plan_limits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qps8w2/20month_ollama_cloud_plan_limits/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T23:50:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpwncu</id>
    <title>Onsloth glm6.5V is broken.</title>
    <updated>2026-01-29T02:57:24+00:00</updated>
    <author>
      <name>/u/rorowhat</name>
      <uri>https://old.reddit.com/user/rorowhat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I say &amp;quot;hello&amp;quot; and it starts giving me code. has anyone tried this model on ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rorowhat"&gt; /u/rorowhat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpwncu/onsloth_glm65v_is_broken/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpwncu/onsloth_glm65v_is_broken/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qpwncu/onsloth_glm65v_is_broken/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T02:57:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpr4hq</id>
    <title>Just updated Ollama and started using it after almost a year.... Are the Ollama devs stupid or is this harder to deal with than it seems?</title>
    <updated>2026-01-28T23:04:49+00:00</updated>
    <author>
      <name>/u/cmndr_spanky</name>
      <uri>https://old.reddit.com/user/cmndr_spanky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qpr4hq/just_updated_ollama_and_started_using_it_after/"&gt; &lt;img alt="Just updated Ollama and started using it after almost a year.... Are the Ollama devs stupid or is this harder to deal with than it seems?" src="https://preview.redd.it/dvhk25h286gg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee1e9cd4fe523338a8a2343e921113060875dcea" title="Just updated Ollama and started using it after almost a year.... Are the Ollama devs stupid or is this harder to deal with than it seems?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why on earth would you make this a default, and make it moderately annoying to change the default? Couldn't Ollama detect how much VRAM is available to the run-time for a given model and make a best effort to set it to some reasonable default? &lt;/p&gt; &lt;p&gt;So many people are just going to ignore this / not realize this, and the model is going to suck trying to slide context around with only 4k available.&lt;/p&gt; &lt;p&gt;Also this model can handle 256k context btw ... my Mac shares 48gb between the CPU and GPU. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cmndr_spanky"&gt; /u/cmndr_spanky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dvhk25h286gg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qpr4hq/just_updated_ollama_and_started_using_it_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qpr4hq/just_updated_ollama_and_started_using_it_after/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T23:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq2wsq</id>
    <title>Ollama claudbot/moltbot</title>
    <updated>2026-01-29T08:18:59+00:00</updated>
    <author>
      <name>/u/MrNemano</name>
      <uri>https://old.reddit.com/user/MrNemano</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I received an email from Ollama indicating that it would be possible to use Ollama with Clawdbot, allowing the use of cloud-based models. This is an interesting option for using Clawdbot for free. So I hosted Clawdbot locally on my Raspberry Pi 5 16GB, but during the configuration process, I couldn't find how to set it up. Did I misunderstand something? Has anyone tried this and succeeded?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrNemano"&gt; /u/MrNemano &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq2wsq/ollama_claudbotmoltbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq2wsq/ollama_claudbotmoltbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qq2wsq/ollama_claudbotmoltbot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T08:18:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq7wno</id>
    <title>Websearch at Ollama level?</title>
    <updated>2026-01-29T13:00:11+00:00</updated>
    <author>
      <name>/u/autumnwalker123</name>
      <uri>https://old.reddit.com/user/autumnwalker123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm having a hard time wrapping my head aronud how to make my Ollama model get web search context - running Ollama 0.15.2 with ministral-3:8b.&lt;/p&gt; &lt;p&gt;In my mind, it makes sense to somehow give the web search ability at the Ollama level then anything trying to call the model would benefit from the serach built in, but from what I can gather the search needs to be injected at the level where the model is called (e.g. Home Assistant, Open WebUI).&lt;/p&gt; &lt;p&gt;Is there a way to do this where I don't have to try and cobble together a bunch of different search capabilities at &amp;quot;endpoints&amp;quot; and just centralize it through the Ollama engine? I've noticed the Ollama web search API, I've added my API key to Ollama, but it doesn't seem to use it / not sure how to make it use it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/autumnwalker123"&gt; /u/autumnwalker123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq7wno/websearch_at_ollama_level/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq7wno/websearch_at_ollama_level/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qq7wno/websearch_at_ollama_level/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T13:00:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp9c9x</id>
    <title>AI started speaking in Russian out of nowhere</title>
    <updated>2026-01-28T11:54:45+00:00</updated>
    <author>
      <name>/u/No-Sky2462</name>
      <uri>https://old.reddit.com/user/No-Sky2462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qp9c9x/ai_started_speaking_in_russian_out_of_nowhere/"&gt; &lt;img alt="AI started speaking in Russian out of nowhere" src="https://preview.redd.it/jckp73fxv2gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3eaf242a017b1f3ab7b8bb92952977659fe2d5e" title="AI started speaking in Russian out of nowhere" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I am a hobbyist, who is interested in AI and LLM. I don't know much about how these LLM function but i was interested so i installed one using terminal. The installation completed successfully but the ai started speaking in russian out of nowhere. Is this intentional behaviour for a LLM or did i do something wrong? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Sky2462"&gt; /u/No-Sky2462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jckp73fxv2gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qp9c9x/ai_started_speaking_in_russian_out_of_nowhere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qp9c9x/ai_started_speaking_in_russian_out_of_nowhere/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T11:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq8k8y</id>
    <title>Looking to get feedback on a companion tool that runs on Ollama</title>
    <updated>2026-01-29T13:29:12+00:00</updated>
    <author>
      <name>/u/Donviticus</name>
      <uri>https://old.reddit.com/user/Donviticus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;So I have been working on a side project that would help people when it comes to their mental health. It's a tool designed to provide people with the means of dealing with whatever mental struggles they're experiencing in a private and safe way. The tool is called Haven.&lt;/p&gt; &lt;p&gt;Haven is an AI desktop companion that uses the Ollama LLM model to remain private and localized in your computer. It is completely local and offline. There are no external API's or services that are connected to the internet. It's essentially a chat box that is completely black boxed. Haven will respond to your texts in real time and remember several contextual information you provide it but will forget everything as soon as you close it. It's also designed to begin forgetting the conversation as the conversation continues.&lt;/p&gt; &lt;p&gt;In short, nothing you write ever leaves your computer. It's simply just you, your thoughts, and Haven. There's no way for me to even see how Haven is being used except through a feedback box that is embedded in the Haven application. Again, Haven is completely free and I intend to keep it free forever. If you're curious and want to use Haven, you can download it from my website here: &lt;a href="https://prometheussys.com/"&gt;https://prometheussys.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you do end up installing it, I would very much appreciate if you can provide me with feedback on Haven. The goal is to improve this companion so that it can better address people's mental health struggles. It is also important to note that Haven was never designed to replace an actual professional but to be a tool in a person's toolbox when it comes to dealing with mental health struggles.&lt;/p&gt; &lt;p&gt;Thank you for taking the time to read and thank you in advance for considering Haven.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Donviticus"&gt; /u/Donviticus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq8k8y/looking_to_get_feedback_on_a_companion_tool_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq8k8y/looking_to_get_feedback_on_a_companion_tool_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qq8k8y/looking_to_get_feedback_on_a_companion_tool_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T13:29:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq8mas</id>
    <title>figured out how to use ollama + moltbot together (local thinking, cloud doing)</title>
    <updated>2026-01-29T13:31:35+00:00</updated>
    <author>
      <name>/u/Round_Net_225</name>
      <uri>https://old.reddit.com/user/Round_Net_225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;saw that post yesterday asking about ollama with moltbot and had the same question last week.&lt;/p&gt; &lt;p&gt;here's what worked: don't run full moltbot locally (too heavy). use ollama for thinking, cloud for execution.&lt;/p&gt; &lt;p&gt;my setup:&lt;/p&gt; &lt;p&gt;· ollama local with llama3.1 for reasoning&lt;/p&gt; &lt;p&gt;· shell_clawd_bot for actual tasks&lt;/p&gt; &lt;p&gt;· they talk through simple api&lt;/p&gt; &lt;p&gt;basically ollama plans it, cloud does it.&lt;/p&gt; &lt;p&gt;why this works:&lt;/p&gt; &lt;p&gt;· ollama stays free for back-and-forth thinking&lt;/p&gt; &lt;p&gt;· only pay for execution (like 10% of calls)&lt;/p&gt; &lt;p&gt;· agent runs 24/7 without rpi staying on&lt;/p&gt; &lt;p&gt;been running on raspberry pi 5 with 16gb for 2 weeks. free trial covered testing, now ~$20/month for the cloud part.&lt;/p&gt; &lt;p&gt;their telegram group helped with the setup. surprisingly easy to connect.&lt;/p&gt; &lt;p&gt;not sure if this is what that other person meant but it's been solid for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Round_Net_225"&gt; /u/Round_Net_225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq8mas/figured_out_how_to_use_ollama_moltbot_together/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq8mas/figured_out_how_to_use_ollama_moltbot_together/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qq8mas/figured_out_how_to_use_ollama_moltbot_together/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T13:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq0jg1</id>
    <title>Powerful rig owner needing advice</title>
    <updated>2026-01-29T06:03:30+00:00</updated>
    <author>
      <name>/u/Ok-Neighborhood6765</name>
      <uri>https://old.reddit.com/user/Ok-Neighborhood6765</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys. Fist post here. I need advice finding a good model and yeah im a bit of a noob. Ok so to start my rig specs are thus&lt;/p&gt; &lt;p&gt;CPU: Intel i9-14900KS GPUs: NVIDIA RTX 5090 + NVIDIA RTX 4070 System RAM: 96 GB Corsair DDR5 Operating System: Windows 11 I run ollama via command pro at present.&lt;/p&gt; &lt;p&gt;This rig was built as a gift to me as i suffer a chronic pain condition called Fibromyalgia. Ive been recently made disabled and cant work anymore. I use A.I. to do research on symptoms, help with daily tasks, and find resources. I use it to search for doctors, books and comb through the internet. I also use it to help others in their research. Ive used various models and i prefer using command line lol im a bit old school but it isnt a hard requirement. I need to update my current A.I. i tried using llama3.3 70b and it runs ok but its extremely gullible and had to create my own modelfile and yet still find it annoying. Ive tried mistral and idk not my flavor. Tried falcon and found it too simple. Right now downloading chatgpt-oss cuz i hate my programs ethically babysitting me.&lt;/p&gt; &lt;p&gt;Yeah i could just use chatgpt and i do love its format but id like something local and data controlled. Furthermore as of 5 days ago as we know the free version will now have ads and i refuse to waste hours of my life waiting on corporate greed to time out what they are peddling. &lt;/p&gt; &lt;p&gt;Maybe im a dreamer but id like it to be able to retrieve documents of whatever id like to research and store it locally as im getting 2026's version of dial up (1MB/s) out in the middle of nowhere USA. Please be respectful. Im looking for a model to meet my needs. Something that can research medicine psychology sports science and whatever tickles me. Hell it would be nice if it could run the rig too although im finding most llms are nothing more than delusional search bots.&lt;/p&gt; &lt;p&gt;Theres so much noise out there now i dont know what to look for. I dont have the patience with my slow internet to look. Ive heard theres work being done on &amp;quot;edge-ai&amp;quot; and it sounds appealing Any and all paths and rabbit holes that might help would be appreciated. Im lost in the A.I. pacific ocean. &lt;/p&gt; &lt;p&gt;Edit: i just spent 5 painful hours coding faiss and chromadb in python only to find they dont directly communicate with llama and ive had enough. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Neighborhood6765"&gt; /u/Ok-Neighborhood6765 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq0jg1/powerful_rig_owner_needing_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq0jg1/powerful_rig_owner_needing_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qq0jg1/powerful_rig_owner_needing_advice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T06:03:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqan74</id>
    <title>Effects of quantized KV cache on an already quantized model.</title>
    <updated>2026-01-29T14:52:44+00:00</updated>
    <author>
      <name>/u/Pyrore</name>
      <uri>https://old.reddit.com/user/Pyrore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run a QwQ 32B model variant in LM studio, and after the update today, I can finally use KV quantization without absolutely tanking my performance. My question is, &lt;strong&gt;if I'm running QwQ at four bit, will dropping my K/V cache to 4 bits notably impact the accuracy?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm happy at 4 bits for QwQ, I only have 24BG VRAM, and that fits nicely at around 19GB (I understand it's better to have more parameters than higher quants). But I can only fit about 10k of context into the remaining 4GB of VRAM (need to leave about 1GB spare for system overheads), no where near enough for the conversational/role-play I use local LLMs for. So I've bee running the KV cache in main memory with the CPU, easily runs up to 64k, but I never really go past 32k, because by then I'm around 1.5 tokens a second (compared to 15/s when there is negligible context).&lt;/p&gt; &lt;p&gt;But with KV cache at 4 bit I can hit 40k context without overloading my VRAM, and my tests so far indicate three times the token rate for a given context size compared to main memory/CPU. But accuracy is more subjective, I'd love to hear your opinions or links to any studies. My model is already running well at 4 bits, and it seems sensible to run the KV at the same accuracy as the model, anything more seems wasteful, unless there's something I'm not understanding...&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pyrore"&gt; /u/Pyrore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqan74/effects_of_quantized_kv_cache_on_an_already/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qqan74/effects_of_quantized_kv_cache_on_an_already/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qqan74/effects_of_quantized_kv_cache_on_an_already/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T14:52:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq8waa</id>
    <title>usage notice</title>
    <updated>2026-01-29T13:43:03+00:00</updated>
    <author>
      <name>/u/Steus_au</name>
      <uri>https://old.reddit.com/user/Steus_au</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1qq8waa/usage_notice/"&gt; &lt;img alt="usage notice" src="https://b.thumbs.redditmedia.com/SqHPvTVMklCt0JHRkpI5_QoTK-g4JjPLMe70_uKSY0c.jpg" title="usage notice" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;in their cloud - take note that an hour usage is the weekly usage:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x79i5jddlagg1.png?width=791&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=73586b6e6a7125072ddd0d57e886cfa249ac355e"&gt;https://preview.redd.it/x79i5jddlagg1.png?width=791&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=73586b6e6a7125072ddd0d57e886cfa249ac355e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Steus_au"&gt; /u/Steus_au &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq8waa/usage_notice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qq8waa/usage_notice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qq8waa/usage_notice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-29T13:43:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qprl7a</id>
    <title>[Opinion] Why I believe the $20/month Ollama Cloud is a better investment than ChatGPT or Claude</title>
    <updated>2026-01-28T23:23:23+00:00</updated>
    <author>
      <name>/u/AlexHardy08</name>
      <uri>https://old.reddit.com/user/AlexHardy08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; I am not affiliated with Ollama in any way. This is purely based on my personal experience as a long-term user.&lt;/p&gt; &lt;p&gt;I’ve been using Ollama since it first launched, and it has genuinely changed my workflow. Even with a powerful local machine, there are certain walls you eventually hit. Lately, I’ve been testing the $20/month Cloud plan, and I wanted to share why I think it’s worth every penny.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The &amp;quot;Large Model&amp;quot; Barrier&lt;/strong&gt;&lt;br /&gt; We are seeing incredible models being released, like &lt;strong&gt;Kimi-k2.5&lt;/strong&gt;, DeepSeek, GLM, and various Open-Source versions of top-tier models. For 99% of us, running these locally is simply impossible unless you have a $30,000+ rig.&lt;/p&gt; &lt;p&gt;Yes, there is a free tier for Ollama Cloud, but we have to be realistic: running these massive models requires serious computation power. The paid plan gives you the stability and speed that a professional workflow requires.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I chose this over a ChatGPT/Claude subscription:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;The Ecosystem:&lt;/strong&gt; Instead of being locked into one model like GPT-5, I have immediate access to a variety of state-of-the-art models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simplicity:&lt;/strong&gt; If you have Ollama installed, you already know the drill. Switching to a cloud-hosted massive model is as simple as ollama run kimi-k2.5. No complex configurations, no manual weight management. It just works, and it’s incredibly fast.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ROI (Return on Investment):&lt;/strong&gt; If you are building something or doing serious work and don't have the budget for a custom local cluster, this $20 investment pays for itself almost immediately. It bridges the gap between &amp;quot;hobbyist&amp;quot; and &amp;quot;enterprise-level&amp;quot; capabilities.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Only Downside&lt;/strong&gt;&lt;br /&gt; If I had to nitpick, it would be the transparency regarding limits. Much like the free plan, on the $20 plan, it’s sometimes hard to tell exactly when you’ll hit a rate limit. It’s a bit of a &amp;quot;black box&amp;quot; experience, but in my daily use, the performance has been worth the uncertainty.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Suggestion:&lt;/strong&gt;&lt;br /&gt; If you are doing research or building tools and you need the power of models that your local VRAM can’t handle, stop hesitating. It’s a solid investment that democratizes access to high-end AI.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I’m curious to hear from others:&lt;/strong&gt;&lt;br /&gt; Is anyone else here using the $20/month Ollama Cloud plan? What has your experience been like so far? Any &amp;quot;pro-tips&amp;quot; or secrets you’ve discovered to get the most out of it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexHardy08"&gt; /u/AlexHardy08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qprl7a/opinion_why_i_believe_the_20month_ollama_cloud_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1qprl7a/opinion_why_i_believe_the_20month_ollama_cloud_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1qprl7a/opinion_why_i_believe_the_20month_ollama_cloud_is/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2026-01-28T23:23:23+00:00</published>
  </entry>
</feed>
