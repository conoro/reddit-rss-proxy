<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-09T13:30:24+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mkc2e3</id>
    <title>Looking for suggestions on a coding assistant model available in Ollama</title>
    <updated>2025-08-07T21:05:46+00:00</updated>
    <author>
      <name>/u/0x426C797A</name>
      <uri>https://old.reddit.com/user/0x426C797A</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm new to the AI game and I need a coding assistant to model. I was always told claude and Gemini are great but I cannot find them within ollama so I'm looking for suggestions &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0x426C797A"&gt; /u/0x426C797A &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkc2e3/looking_for_suggestions_on_a_coding_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkc2e3/looking_for_suggestions_on_a_coding_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkc2e3/looking_for_suggestions_on_a_coding_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T21:05:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjxd87</id>
    <title>gpt-oss-20b running on i5 Iris Xe graphics with 16 gb ram.</title>
    <updated>2025-08-07T11:20:16+00:00</updated>
    <author>
      <name>/u/Western_Art_3308</name>
      <uri>https://old.reddit.com/user/Western_Art_3308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mjxd87/gptoss20b_running_on_i5_iris_xe_graphics_with_16/"&gt; &lt;img alt="gpt-oss-20b running on i5 Iris Xe graphics with 16 gb ram." src="https://preview.redd.it/zg28ifvd0lhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8921388569ea35dfe3d1ca4612738bd5072d2883" title="gpt-oss-20b running on i5 Iris Xe graphics with 16 gb ram." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Art_3308"&gt; /u/Western_Art_3308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zg28ifvd0lhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjxd87/gptoss20b_running_on_i5_iris_xe_graphics_with_16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjxd87/gptoss20b_running_on_i5_iris_xe_graphics_with_16/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T11:20:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkm54t</id>
    <title>Gpt-oss not working with Cline - Ollama, vLLM, Llamaccp</title>
    <updated>2025-08-08T04:50:02+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi peeps,&lt;/p&gt; &lt;p&gt;Leave this post for posterity. &lt;/p&gt; &lt;p&gt;Seems like the new Openai open-source models don't work very well with Cline, if you're running them locally. &lt;/p&gt; &lt;p&gt;My inference via Ollama works perfectly fine on CLI and Open WebUI, but Cline is practically broken due to harmony style prompt from the model. &lt;/p&gt; &lt;p&gt;Some users complain the same with vLLM and Llamaccp over at Github.&lt;/p&gt; &lt;p&gt;Other notes from Ollama. Changing KV Cache or Flash Attention doesn't seem to make any effect with the model's new quant MXFP4.&lt;/p&gt; &lt;p&gt;Other popular quants (e.g Q1...) don't seem the load either, questioning why these exist in the first place from popular teams. &lt;/p&gt; &lt;p&gt;The MXFP4 is the only one that loads, and despite being a small model, the memory footprint is large. I was constrained 55k context window with RTX 5090. &lt;/p&gt; &lt;p&gt;I'd like to use the model with Cline as it seems to fit my workflow, as my own test prompts via Open Router, beats Qwen3 30B series out of the water.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkm54t/gptoss_not_working_with_cline_ollama_vllm_llamaccp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkm54t/gptoss_not_working_with_cline_ollama_vllm_llamaccp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkm54t/gptoss_not_working_with_cline_ollama_vllm_llamaccp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T04:50:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkd67y</id>
    <title>Actually good benchmarks</title>
    <updated>2025-08-07T21:49:29+00:00</updated>
    <author>
      <name>/u/Mr-Barack-Obama</name>
      <uri>https://old.reddit.com/user/Mr-Barack-Obama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spend a lot of time making private benchmarks for my real world use cases. It's extremely important to create your own unique benchmark for the specific tasks you will be using ai for, but we all know it's helpful to look at other benchmarks too. I think we've all found many benchmarks to not mean much in the real world, but I've found 2 benchmarks that when combined correlate accurately to real world intelligence and capability.&lt;/p&gt; &lt;p&gt;First lets start with livebench.ai . Besides livebench.ai 's coding benchmark, which I always turn off when looking at the total average scores, their total average score is often very accurate to real world use cases. All of their benchmarks combined into one average score tell a great story for how capable the model is. However, the only way that Livebench lacks is that it seems to only test at very short context lengths.&lt;/p&gt; &lt;p&gt;This is where another benchmark comes in, &lt;a href="https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87"&gt;https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87&lt;/a&gt; From a website about fiction writing and while it's not a super serious website, it is the best benchmark for real world long context. No one comes close. For example, I noticed Sonnet 4 performing much better than Opus 4 on context windows over 4,000 words. ONLY the Fiction Live benchmark reliably shows real world long context performance like this.&lt;/p&gt; &lt;p&gt;To estimate real world intelligence, I've found it very accurate to combine the results of both:&lt;/p&gt; &lt;p&gt;- &amp;quot;Fiction Live&amp;quot;: &lt;a href="https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87"&gt;https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &amp;quot;Livebench&amp;quot;: &lt;a href="https://livebench.ai"&gt;https://livebench.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For models that many people run locally, not enough are represented on Livebench or Fiction Live. For example, GPT OSS 20b has not been tested on these benchmarks and it will likely be one of the most widely used open source models ever.&lt;/p&gt; &lt;p&gt;Livebench seems to have a responsive github. We should make posts politely asking for more models to be tested.&lt;/p&gt; &lt;p&gt;Livebench github: &lt;a href="https://github.com/LiveBench/LiveBench/issues"&gt;https://github.com/LiveBench/LiveBench/issues&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also on X, &lt;a href="/u/bindureddy"&gt;u/bindureddy&lt;/a&gt; runs the benchmark and is even more responsive to comments. I think we should make an effort to express that we want more models tested. It's totally worth trying!&lt;/p&gt; &lt;p&gt;FYI I wrote this by hand because I'm so passionate about benchmarks, no ai lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr-Barack-Obama"&gt; /u/Mr-Barack-Obama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkd67y/actually_good_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkd67y/actually_good_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkd67y/actually_good_benchmarks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T21:49:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjo9ki</id>
    <title>Best models under 16GB</title>
    <updated>2025-08-07T02:31:33+00:00</updated>
    <author>
      <name>/u/Mr-Barack-Obama</name>
      <uri>https://old.reddit.com/user/Mr-Barack-Obama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a macbook m4 pro with 16gb ram so I've made a list of the best models that should be able to run on it. I will be using llama.cpp without GUI for max efficiency but even still some of these quants might be too large to have enough space for reasoning tokens and some context, idk I'm a noob.&lt;/p&gt; &lt;p&gt;Here are the best models and quants for under 16gb based on my research, but I'm a noob and I haven't tested these yet:&lt;/p&gt; &lt;p&gt;Best Reasoning: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Qwen3-32B (IQ3_XXS 12.8 GB)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Qwen3-30B-A3B-Thinking-2507 (IQ3_XS 12.7GB)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Qwen 14B (Q6_K_L 12.50GB)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;gpt-oss-20b (12GB)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Phi-4-reasoning-plus (Q6_K_L 12.3 GB)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Best non reasoning:&lt;br /&gt; 1. gemma-3-27b (IQ4_XS 14.77GB)&lt;br /&gt; 2. Mistral-Small-3.2-24B-Instruct-2506 (Q4_K_L 14.83GB)&lt;br /&gt; 3. gemma-3-12b (Q8_0 12.5 GB) &lt;/p&gt; &lt;p&gt;My use cases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Accurately summarizing meeting transcripts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Creating an anonymized/censored version of a a document by removing confidential info while keeping everything else the same.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Asking survival questions for scenarios without internet like camping. I think medgemma-27b-text would be cool for this scenario.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I prefer maximum accuracy and intelligence over speed. How's my list and quants for my use cases? Am I missing any model or have something wrong? Any advice for getting the best performance with llama.cpp on a macbook m4pro 16gb?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr-Barack-Obama"&gt; /u/Mr-Barack-Obama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjo9ki/best_models_under_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjo9ki/best_models_under_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjo9ki/best_models_under_16gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T02:31:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk1xbe</id>
    <title>Probably dumb question: why doesn't Ollama forWindows work in airplane mode?</title>
    <updated>2025-08-07T14:42:05+00:00</updated>
    <author>
      <name>/u/ImAProAtSomeStuff</name>
      <uri>https://old.reddit.com/user/ImAProAtSomeStuff</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImAProAtSomeStuff"&gt; /u/ImAProAtSomeStuff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mk1jwk/probably_dumb_question_why_doesnt_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mk1xbe/probably_dumb_question_why_doesnt_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mk1xbe/probably_dumb_question_why_doesnt_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T14:42:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkzoeg</id>
    <title>Updates on a project I am passionate about- Darnahi</title>
    <updated>2025-08-08T16:21:07+00:00</updated>
    <author>
      <name>/u/TestPilot1980</name>
      <uri>https://old.reddit.com/user/TestPilot1980</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TestPilot1980"&gt; /u/TestPilot1980 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/selfhosted/comments/1mktc79/updates_on_a_project_i_am_passionate_about_darnahi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkzoeg/updates_on_a_project_i_am_passionate_about_darnahi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkzoeg/updates_on_a_project_i_am_passionate_about_darnahi/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T16:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mky29w</id>
    <title>Ollama implementation of openevolve?</title>
    <updated>2025-08-08T15:20:33+00:00</updated>
    <author>
      <name>/u/WaggishBean13</name>
      <uri>https://old.reddit.com/user/WaggishBean13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the release of the local chat in 0.10, I wonder if we will be able to get somewhere in the future local support for openevolve (Open-source implementation of AlphaEvolve) so that we can chose what local models to use for advanced and complex tasks. That would be really cool for users who want to min-max their small lightweight models or go hard with their home labs.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/codelion/openevolve"&gt;https://github.com/codelion/openevolve&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WaggishBean13"&gt; /u/WaggishBean13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mky29w/ollama_implementation_of_openevolve/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mky29w/ollama_implementation_of_openevolve/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mky29w/ollama_implementation_of_openevolve/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T15:20:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml4d5w</id>
    <title>Do responses vary based on the machine</title>
    <updated>2025-08-08T19:18:51+00:00</updated>
    <author>
      <name>/u/Competitive_coder11</name>
      <uri>https://old.reddit.com/user/Competitive_coder11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For example will the same model give me varied responses and better responses on a higher end pc over a lower end pc (given both of them have the requirements to run the model just fine)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive_coder11"&gt; /u/Competitive_coder11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml4d5w/do_responses_vary_based_on_the_machine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml4d5w/do_responses_vary_based_on_the_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml4d5w/do_responses_vary_based_on_the_machine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T19:18:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkyo5d</id>
    <title>Ollama configuration question</title>
    <updated>2025-08-08T15:43:53+00:00</updated>
    <author>
      <name>/u/snapo84</name>
      <uri>https://old.reddit.com/user/snapo84</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use ollama on my linux box as the server API but i cant figure out how to achive the following:&lt;br /&gt; My system has 2 x RTX 2080 Ti with a memory upgrade to 22GB and 64GB of system ram.&lt;/p&gt; &lt;p&gt;i would like to permanently load Qwen3 Thinking 2507 on GPU1 in Q6_K_M and on GPU2 permanently load Qwen3 Coder Q4_0 65k context.&lt;/p&gt; &lt;p&gt;The models should stay there permanently in ram and on a crash (which happens often) reload them again.&lt;/p&gt; &lt;p&gt;Anyone has a advice how to achive this or do i have to move to vllm or sglang to do this? Also is there a way to fix the top p top k temp for each model?&lt;/p&gt; &lt;p&gt;thanks if anyone has an answer....&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/snapo84"&gt; /u/snapo84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkyo5d/ollama_configuration_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkyo5d/ollama_configuration_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkyo5d/ollama_configuration_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T15:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkg49d</id>
    <title>Just released v1 of my open-source CLI app for coding locally: Nanocoder</title>
    <updated>2025-08-07T23:55:07+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mkg49d/just_released_v1_of_my_opensource_cli_app_for/"&gt; &lt;img alt="Just released v1 of my open-source CLI app for coding locally: Nanocoder" src="https://external-preview.redd.it/Drrvz-4kHMvi4lMmu6VO9fxf9_IMIOSmZHpoZGn5meI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb9f517488bd723213b3f631a62e468793339cc9" title="Just released v1 of my open-source CLI app for coding locally: Nanocoder" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just released version 1.0 of Nanocoder, a CLI tool Iâ€™ve been building to make it easier to code agentically with large language models locally with Ollama and OpenRouter in your terminal to offer a similar experience to Claude Code and Gemini CLI. For me terminal experiences feel cleaner and more flexible so thatâ€™s where I was going with this.&lt;/p&gt; &lt;p&gt;Right now itâ€™s very much MVP stage - works well enough to be useful, but rough around the edges. I want to polish it, add more features (better context handling, more tools, improved UX), and make it truly awesome for coding work.&lt;/p&gt; &lt;p&gt;Iâ€™m a big believer in AI being open and for the people, not locked behind subscriptions or proprietary APIs. Thatâ€™s why itâ€™s open source, Iâ€™m hoping to build it as a community.&lt;/p&gt; &lt;p&gt;If you think this is cool, Iâ€™d be grateful for GitHub stars and contributors to help shape where it goes next. Feedback, feature ideas, bug reports - all welcome!&lt;/p&gt; &lt;p&gt;ðŸ‘‰ &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Mote-Software/nanocoder"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkg49d/just_released_v1_of_my_opensource_cli_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkg49d/just_released_v1_of_my_opensource_cli_app_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T23:55:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mktlqb</id>
    <title>Model for text to sql</title>
    <updated>2025-08-08T12:16:26+00:00</updated>
    <author>
      <name>/u/ZitounaT</name>
      <uri>https://old.reddit.com/user/ZitounaT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, i need a model that can take prompt + schema and generate a sql query.&lt;br /&gt; It's for a web app (personnel project) that match clients to experts (IT, Finance, Doctors...), and i want to integrate a chatbot that will understand the client's problem, generate query to fetch the best experts to solve the problem. &lt;/p&gt; &lt;p&gt;I already tried the ollama 7B, it worked fine but with limitations, if the user's prompt is clear and simple and straight to the point, the model will generate a correct query most of the time, otherwise the model will get so confused.&lt;br /&gt; is there any way to have better results with it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZitounaT"&gt; /u/ZitounaT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mktlqb/model_for_text_to_sql/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mktlqb/model_for_text_to_sql/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mktlqb/model_for_text_to_sql/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T12:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mklsm5</id>
    <title>GPT-5 style router, but for local models</title>
    <updated>2025-08-08T04:31:18+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mklsm5/gpt5_style_router_but_for_local_models/"&gt; &lt;img alt="GPT-5 style router, but for local models" src="https://preview.redd.it/k9oxbvcd3qhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d15425761a134887e8b496ee976b02a5a1b4acfd" title="GPT-5 style router, but for local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT-5 launched today, which is essentially different model underneath the covers abstracted away by a real-time router. In June, we published our &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;preference-aligned routing model&lt;/a&gt; and &lt;a href="https://github.com/katanemo/archgw"&gt;framework&lt;/a&gt; for developers so that they can build a best-of-breed agentic experience with choice of models they care about. &lt;/p&gt; &lt;p&gt;Sharing the research and framework again, as it might be helpful to developers looking for similar tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k9oxbvcd3qhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mklsm5/gpt5_style_router_but_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mklsm5/gpt5_style_router_but_for_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T04:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml0c71</id>
    <title>Ollama remembers previous prompts when it shouldn't?</title>
    <updated>2025-08-08T16:46:17+00:00</updated>
    <author>
      <name>/u/Nebosklon</name>
      <uri>https://old.reddit.com/user/Nebosklon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm completely new to this. I've downloaded llama-3.1 and am running a series of queries to it using the ollama python library. As I understand it, the default behaviour should be that the queries run completely independently, so when processing a new prompt ollama has no memory of the previous one whatsoever. That is exactly the desired behaviour for my research problem, but!&lt;/p&gt; &lt;p&gt;The &amp;quot;system&amp;quot; parts of my prompts are all identical, the &amp;quot;user&amp;quot; parts are all different, but they come in pairs, where I have two very similar versions of basically the same prompt. Now what I see is that ollama takes a lot less time to process the second version of the same prompt than it does the first. Like an order of magnitude less time. Why is that happening if it has no memory of the previous prompt? Or does it?&lt;/p&gt; &lt;p&gt;I've found this old post and was wondering if this has something to do with the buffer of the generator function (probably not): &lt;a href="https://www.reddit.com/r/ollama/s/GwmhTB8d0h"&gt;https://www.reddit.com/r/ollama/s/GwmhTB8d0h&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyway, I don't trust it and I would like to ensure that my prompts are processed truly independently. (Or even better, if this is possible, make it remember the &amp;quot;system&amp;quot; part but forget the &amp;quot;user&amp;quot; part.) Do you have any ideas of why this is happening and what I could do? Sorry if this is a stupid question. &lt;/p&gt; &lt;p&gt;UPDATE:&lt;/p&gt; &lt;p&gt;So, I've experimented a bit more with this, and here are my results.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;I've added a Hi! prompt at the beginning. It didn't change the pattern for the rest of the prompts. So at least, it doesn't have to do with the time loading the model at the beginning of the loop.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I've changed the order of the prompts. At first, I had them in this order:&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;text 1 version A 10 sec&lt;/p&gt; &lt;p&gt;text 1 version B 1.5 sec&lt;/p&gt; &lt;p&gt;text 2 version A 10 sec&lt;/p&gt; &lt;p&gt;text 2 version B 1.5 sec&lt;/p&gt; &lt;p&gt;text 3 version A 15 sec&lt;/p&gt; &lt;p&gt;text 3 version B 2 sec&lt;/p&gt; &lt;p&gt;And processing version B was always much faster than processing version A. By the way, the difference was entirely due to prompt evaluation time. Response generation time was not affected.&lt;/p&gt; &lt;p&gt;Then I put them in this order:&lt;/p&gt; &lt;p&gt;text 1 version A 10 sec&lt;/p&gt; &lt;p&gt;text 2 version A 10 sec&lt;/p&gt; &lt;p&gt;text 3 version A 15 sec&lt;/p&gt; &lt;p&gt;text 1 version B 10 sec&lt;/p&gt; &lt;p&gt;text 2 version B 10 sec&lt;/p&gt; &lt;p&gt;text 3 version B 15 sec&lt;/p&gt; &lt;p&gt;And the facilitation effect completely disappeared. Now both versions of the same prompt took about the same time to be processed. &lt;/p&gt; &lt;p&gt;So this facilitation effect must have to do with processing two very similar prompts in a row. Why?&lt;/p&gt; &lt;p&gt;Now, come on, people. WHY? DOES ANYONE HAVE ANY IDEA WHY THIS IS HAPPENING?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nebosklon"&gt; /u/Nebosklon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml0c71/ollama_remembers_previous_prompts_when_it_shouldnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml0c71/ollama_remembers_previous_prompts_when_it_shouldnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml0c71/ollama_remembers_previous_prompts_when_it_shouldnt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T16:46:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkudgd</id>
    <title>Best model with tool capabilities for ai agents?</title>
    <updated>2025-08-08T12:52:13+00:00</updated>
    <author>
      <name>/u/_wanderloots</name>
      <uri>https://old.reddit.com/user/_wanderloots</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m getting more into coding and setting up my ai agent system and I want to power them with a relatively lightweight local model that can also handle tool use. &lt;/p&gt; &lt;p&gt;Iâ€™m curious if people have found any models that do a better job? Iâ€™ve been testing gpt-oss:20b and it works, but Is kind of slow. &lt;/p&gt; &lt;p&gt;Would love to get qwen3 working but it seemed to have issues with tool use. &lt;/p&gt; &lt;p&gt;Any suggestions are appreciated ðŸ˜Š &lt;/p&gt; &lt;p&gt;32 gb ram on an M2 Max studio &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_wanderloots"&gt; /u/_wanderloots &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkudgd/best_model_with_tool_capabilities_for_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkudgd/best_model_with_tool_capabilities_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkudgd/best_model_with_tool_capabilities_for_ai_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T12:52:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mklwrl</id>
    <title>GPT 5 for Computer Use agents.</title>
    <updated>2025-08-08T04:37:34+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mklwrl/gpt_5_for_computer_use_agents/"&gt; &lt;img alt="GPT 5 for Computer Use agents." src="https://external-preview.redd.it/amZianVtMGc1cWhmMffa9LUhs6wvp7jU6XPjtPFZB1S0k_8zNod6eLcZn2nM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2533cb96234ebc6fab32f27912ae124582ad0b00" title="GPT 5 for Computer Use agents." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same tasks, same grounding model we just swapped GPT 4o with GPT 5 as the thinking model. &lt;/p&gt; &lt;p&gt;Left = 4o, right = 5. &lt;/p&gt; &lt;p&gt;Watch GPT 5 pull away.&lt;/p&gt; &lt;p&gt;Try it yourself here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs : &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agents"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wxbv78dg5qhf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mklwrl/gpt_5_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mklwrl/gpt_5_for_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T04:37:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlk3cl</id>
    <title>RTX 5090 hangs after couple of minutes of serving qwen2.5-coder:32b in ollama</title>
    <updated>2025-08-09T08:06:06+00:00</updated>
    <author>
      <name>/u/randomnick4622</name>
      <uri>https://old.reddit.com/user/randomnick4622</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;br /&gt; After running inferences on a qwen2.5-coder:32b in ollama for couple of minutes, my 5090 ceases to respond to Ollama (see dmesg log at the end). nvidia-smi shows no devices. Only system restart helps (but the restart itself takes many minutes - something blocks it when GPU is in this state). Fans are constantly running at 100% when this happens.&lt;br /&gt; Tried both PCIE 5.0 and 4.0 bios modes.&lt;/p&gt; &lt;p&gt;Iâ€™m 99% sure that at the beginning I did not have such hangs, this started to happen after 1-2 weeks of usage. No other instabilities observed beside that 3dMark stability tests now fails (shows ~94% stability whereas at the beginning it was passing with 99%).&lt;br /&gt; Does it mean that my 5090 got broken at the hardware level? Any similar cases on your side guys?&lt;/p&gt; &lt;p&gt;Platform: Gigabyte GeForce RTX 5090 AORUS Master 32GB&lt;br /&gt; Kubuntu 24.04 (6.8.0-71-generic #71-Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 22 16:52:38 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux)&lt;br /&gt; x-org (no wayland installed)&lt;br /&gt; driver:&lt;br /&gt; NVIDIA-SMI 570.172.08 Driver Version: 570.172.08 CUDA Version: 12.8&lt;/p&gt; &lt;p&gt;dmesg:&lt;br /&gt; [ 2028.088565] NVRM: nvGpuOpsReportFatalError: uvm encountered global fatal error 0x60, requiring os reboot to recover.&lt;br /&gt; (repeated about 1000 times)&lt;br /&gt; [ 2028.091736] NVRM: nvGpuOpsReportFatalError: uvm encountered global fatal error 0x60, requiring os reboot to recover.&lt;br /&gt; [ 2028.114941] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.114943] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00017; hObject=0xcaf00161; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.114944] NVRM: nvAssertFailedNoLog: Assertion failed: NV_OK == status @ vaspace_api.c:538&lt;br /&gt; [ 2028.116072] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116073] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000036; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116074] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116080] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116081] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116084] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116084] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000035; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; (repeated about 10 times)&lt;br /&gt; [ 2028.116202] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116202] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00007d; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116206] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116206] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00007c; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116209] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116210] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00007b; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116212] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116213] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00007a; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116220] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116220] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1e0002a; hObject=0xcaf00001; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116224] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116224] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1e0002a; hObject=0xcaf00000; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116243] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116243] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00001b; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116244] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116247] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116248] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116250] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116251] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00001a; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116261] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116261] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000014; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116262] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116265] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116266] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116272] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116273] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000013; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116274] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116292] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116293] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116295] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116296] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000044; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116297] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116298] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116299] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116302] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116303] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000043; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116312] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116313] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000041; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116313] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116315] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116316] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116318] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116318] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000040; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116325] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116326] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00003e; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116326] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116328] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116329] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116331] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116332] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00003d; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116337] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116338] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00003b; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116339] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116340] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116341] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116343] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116344] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00003a; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116355] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116356] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000038; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116356] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116366] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116367] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116369] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116370] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000052; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116371] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116372] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116373] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116375] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116376] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000051; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116384] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116385] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00004f; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116385] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116387] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116388] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116390] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116391] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00004e; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116396] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116396] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00004c; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116397] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116398] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116399] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116401] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116402] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00004b; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116408] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116408] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000049; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116410] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116411] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116412] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116414] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116415] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000048; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116425] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116426] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000046; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116426] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116435] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116436] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116631] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116632] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000009; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116633] NVRM: nvAssertFailedNoLog: Assertion failed: NV_OK == status @ vaspace_api.c:538&lt;br /&gt; [ 2028.116637] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116638] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000008; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116638] NVRM: nvAssertFailedNoLog: Assertion failed: NV_OK == status @ vaspace_api.c:538&lt;br /&gt; [ 2028.118206] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.118206] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000004; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.118207] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.118209] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.118210] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.118213] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.118214] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000053; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.118217] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.118218] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000045; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.118221] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.118222] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000037; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.118230] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.118231] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000003; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.118235] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.118236] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000073; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.118236] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.118238] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.118239] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.118818] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 76!&lt;br /&gt; [ 2028.123403] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 76!&lt;br /&gt; [ 2028.123404] NVRM: _deviceTeardown: Disable of Cuda limit activation failedNVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.123407] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000002; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2029.697863] nvidia-modeset: ERROR: GPU:0: Error while waiting for GPU progress: 0x0000ca7d:0 2:0:4048:4044&lt;br /&gt; [ 2034.697870] nvidia-modeset: ERROR: GPU:0: Error while waiting for GPU progress: 0x0000ca7d:0 2:0:4048:4044&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomnick4622"&gt; /u/randomnick4622 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlk3cl/rtx_5090_hangs_after_couple_of_minutes_of_serving/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlk3cl/rtx_5090_hangs_after_couple_of_minutes_of_serving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlk3cl/rtx_5090_hangs_after_couple_of_minutes_of_serving/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T08:06:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlffsc</id>
    <title>Struggling picking the right LLM for continue + vscode + ollama setup</title>
    <updated>2025-08-09T03:30:54+00:00</updated>
    <author>
      <name>/u/Professional-Try-273</name>
      <uri>https://old.reddit.com/user/Professional-Try-273</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am new to this, so I am not sure if I am doing something wrong. My basic understanding is that we should download a 'tool-trained' version of an LLM so that when we use a VS Code extension like Continue, it will know how to call the correct tools from MCP servers.&lt;/p&gt; &lt;p&gt;Currently, I am following the Hugging Face MCP course.&lt;/p&gt; &lt;p&gt;Here is the link: &lt;a href="https://huggingface.co/learn/mcp-course/unit2/continue-client?local-models=ollama"&gt;&lt;code&gt;https://huggingface.co/learn/mcp-course/unit2/continue-client?local-models=ollama&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was instructed to create two YAML files: one for the agent and one for playwright-mcp, which gives the LLM a list of tools to use.&lt;/p&gt; &lt;p&gt;I have had mixed success with this. The task is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Using Playwright, navigate to &lt;code&gt;https://news.ycombinator.com&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Extract the titles and URLs of the top 4 posts on the homepage.&lt;/li&gt; &lt;li&gt;Create a file named &lt;code&gt;hn.txt&lt;/code&gt; in the root directory of the project.&lt;/li&gt; &lt;li&gt;Save this list as plain text in the &lt;code&gt;hn.txt&lt;/code&gt; file, with each line containing the title and URL separated by a hyphen.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Do not output code or instructionsâ€”just complete the task and confirm when it is done.&lt;/p&gt; &lt;p&gt;Llama 3.1 8B and Qwen2-Coder 7B managed to call the tools but had issues completing the task; they generated the TXT file but failed to extract the correct information. GPT-OSS, Qwen3-Coder-Instruct-30B, and Gemma3-Tools-27B (I apologize if I can't recall the model names exactly) all failed to call the playwright-mcp tools correctly. These models kept creating Python files for me to run and ignored my prompt's steps.&lt;/p&gt; &lt;p&gt;GPT-OSS worked well using Ollama's web search function and successfully extracted the info, but it doesn't have access to write to the disk.&lt;/p&gt; &lt;p&gt;Clearly, I am doing something wrong. Some of these tool-use models aren't playing nice with the Continue extension. What should I do next?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Try-273"&gt; /u/Professional-Try-273 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlffsc/struggling_picking_the_right_llm_for_continue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlffsc/struggling_picking_the_right_llm_for_continue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlffsc/struggling_picking_the_right_llm_for_continue/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T03:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlg1sx</id>
    <title>How long until GLM4.5 availability (or what to use in the meantime)?</title>
    <updated>2025-08-09T04:02:44+00:00</updated>
    <author>
      <name>/u/BeardyScientist</name>
      <uri>https://old.reddit.com/user/BeardyScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am brand new to this wonderful world of local LLMs and am trying to pick a model for a fairly large scientific task (read and extract data from several thousand documents which will take about a month of solid processing). Iâ€™ve seen a lot of rave reviews for GLM4.5 and GLM4.5 air and trying it at &lt;a href="http://chat.z.ai/"&gt;chat.z.ai/&lt;/a&gt; has me impressed. However, as itâ€™s not available on Ollama yet, I canâ€™t use it for my task. I donâ€™t have the experience to know how long I can expect to wait before it becomes available; are we talking days, weeks, months, or maybe never? Alternatively, have I missed something and itâ€™s actually available now without me going through a huge effort?&lt;/p&gt; &lt;p&gt;In the meantime, what model would you all suggest for a scientific task where Iâ€™m asking detailed comprehension questions about long scientific texts (10k â€“ 20k words). The texts are in a range of languages but mostly English and Chinese. The hardware Iâ€™m running on is a single RTX 5090. Iâ€™ve tried GPT-OSS:20b and DeepSeek R1:14b; GPT-OSS mostly flat-out refuses to answer my questions and just spits out generic summaries and the occasional garbled mess, whereas DeepSeek R1 delivered reliable, acceptable, but room-for-improvement results. Iâ€™ve also given Qwen3, Gemma3, and GLM4 a go in limited trials; all of which were good but I couldnâ€™t decide which would be most reliable. What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeardyScientist"&gt; /u/BeardyScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlg1sx/how_long_until_glm45_availability_or_what_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlg1sx/how_long_until_glm45_availability_or_what_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlg1sx/how_long_until_glm45_availability_or_what_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T04:02:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlgihf</id>
    <title>Struggling with broken JSON from LLMs? I built a Flutter/Dart package to fix it automatically.</title>
    <updated>2025-08-09T04:28:01+00:00</updated>
    <author>
      <name>/u/Embarrassed-Way-1350</name>
      <uri>https://old.reddit.com/user/Embarrassed-Way-1350</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;If you're building apps with LLMs and Gen AI in Dart or Flutter, you've probably faced this issue: you ask the model for a structured JSON output, and it gives you something that's &lt;em&gt;almost&lt;/em&gt; right but has syntax errors. Missing quotes, trailing commas, single quotes instead of double... it's enough to break your parsing logic every time.&lt;/p&gt; &lt;p&gt;To solve this, I built **&lt;code&gt;json_repair_flutter&lt;/code&gt;**, a Dart package that automatically cleans up and repairs malformed JSON strings. It's inspired by the popular &lt;code&gt;json-repair&lt;/code&gt; libraries in Python and Javascript and makes your apps more resilient by handling the unpredictable outputs from LLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does it fix?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It handles most of the common errors I've seen from models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Unquoted Keys &amp;amp; String Values:&lt;/strong&gt; &lt;code&gt;{name: &amp;quot;John&amp;quot;}&lt;/code&gt; â†’ &lt;code&gt;{&amp;quot;name&amp;quot;: &amp;quot;John&amp;quot;}&lt;/code&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Single Quotes:&lt;/strong&gt; &lt;code&gt;{'name': 'John'}&lt;/code&gt; â†’ &lt;code&gt;{&amp;quot;name&amp;quot;: &amp;quot;John&amp;quot;}&lt;/code&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Trailing Commas:&lt;/strong&gt; &lt;code&gt;[1, 2, 3,]&lt;/code&gt; â†’ &lt;code&gt;[1, 2, 3]&lt;/code&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Comments:&lt;/strong&gt; Removes &lt;code&gt;//&lt;/code&gt; and &lt;code&gt;/* */&lt;/code&gt; style comments.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Unclosed Braces/Brackets:&lt;/strong&gt; Tries to safely close dangling structures.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;And more&lt;/strong&gt;, like multiline strings and faulty escaping.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Here's how easy it is to use:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You don't need to wrap your calls in a try-catch block for parsing anymore.&lt;/p&gt; &lt;p&gt;```dart import 'package:json_repair_flutter/json_repair_flutter.dart';&lt;/p&gt; &lt;p&gt;void main() { // A typical slightly-broken JSON from an LLM const malformedJsonFromLLM = &amp;quot;{name: 'Alice', age: 27,}&amp;quot;;&lt;/p&gt; &lt;p&gt;// Repair and decode directly into a Dart object final decodedData = repairJsonAndDecode(malformedJsonFromLLM);&lt;/p&gt; &lt;p&gt;print(decodedData['name']); // Output: Alice } ```&lt;/p&gt; &lt;p&gt;This has been a passion project for me, and I'm hoping it can help others who are integrating AI into their apps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How you can support me:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you like the idea and think this could be useful, I would be incredibly grateful for your support! A simple &amp;quot;like&amp;quot; on the pub.dev page or a star on GitHub would be a huge motivation for me to keep building more developer tools.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Pub.dev (give it a like ðŸ‘):&lt;/strong&gt; &lt;a href="https://pub.dev/packages/json_repair_flutter"&gt;https://pub.dev/packages/json_repair_flutter&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;GitHub (give it a star â­):&lt;/strong&gt; &lt;a href="https://github.com/h2210316651/json_repair_flutter"&gt;https://github.com/h2210316651/json_repair_flutter&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Finally, I want to contribute more to the community. What are some of the biggest pain points you face when building with Flutter/Dart? Let me know in the comments!&lt;/p&gt; &lt;p&gt;Thanks for reading&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Way-1350"&gt; /u/Embarrassed-Way-1350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlgihf/struggling_with_broken_json_from_llms_i_built_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlgihf/struggling_with_broken_json_from_llms_i_built_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlgihf/struggling_with_broken_json_from_llms_i_built_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T04:28:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml9fsr</id>
    <title>Seeing the positive reception from the community for my tool that finds the most optimal model, Iâ€™ve released an updated version fixing bugs, optimizing the algorithm, and adding suggestions from some fellow redditors. The new version is now available :)</title>
    <updated>2025-08-08T22:45:17+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ml9fsr/seeing_the_positive_reception_from_the_community/"&gt; &lt;img alt="Seeing the positive reception from the community for my tool that finds the most optimal model, Iâ€™ve released an updated version fixing bugs, optimizing the algorithm, and adding suggestions from some fellow redditors. The new version is now available :)" src="https://external-preview.redd.it/aGJ3cXliMnZpdmhmMbi2piECoZUsnlTdrO5dt19_c4zYyJXquEM3aQ2Vkfc7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b0bf6292d985de20c4f39fd79e05441e57b3599" title="Seeing the positive reception from the community for my tool that finds the most optimal model, Iâ€™ve released an updated version fixing bugs, optimizing the algorithm, and adding suggestions from some fellow redditors. The new version is now available :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;npm here: &lt;a href="https://www.npmjs.com/package/llm-checker/v/2.2.0?activeTab=readme"&gt; LLM Checker - Intelligent Ollama Model Selector&lt;/a&gt; :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b5pe2g2vivhf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml9fsr/seeing_the_positive_reception_from_the_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml9fsr/seeing_the_positive_reception_from_the_community/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T22:45:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml2qof</id>
    <title>Cheapest way to host a 24B parameter Ollama server?</title>
    <updated>2025-08-08T18:16:46+00:00</updated>
    <author>
      <name>/u/Few-Avocado4562</name>
      <uri>https://old.reddit.com/user/Few-Avocado4562</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking to run a 24B param Ollama model without breaking the bank. Any recommendations on the cheapest hosting options that actually work? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Avocado4562"&gt; /u/Few-Avocado4562 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml2qof/cheapest_way_to_host_a_24b_parameter_ollama_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml2qof/cheapest_way_to_host_a_24b_parameter_ollama_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml2qof/cheapest_way_to_host_a_24b_parameter_ollama_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T18:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml66jw</id>
    <title>New Favorite Game: Convince GPT-OSS it Exists</title>
    <updated>2025-08-08T20:30:23+00:00</updated>
    <author>
      <name>/u/j_din</name>
      <uri>https://old.reddit.com/user/j_din</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spoiler alert: it can not FATHOM its own existence.&lt;/p&gt; &lt;p&gt;I have been trying for an hour to have the model even admit it's possible that OpenAI would release an open weight model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/bB0XE2Zv"&gt;https://pastebin.com/bB0XE2Zv&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please comment any successful attempts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j_din"&gt; /u/j_din &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml66jw/new_favorite_game_convince_gptoss_it_exists/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml66jw/new_favorite_game_convince_gptoss_it_exists/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml66jw/new_favorite_game_convince_gptoss_it_exists/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T20:30:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml750r</id>
    <title>Local RAG with 97% smaller index and Claude Codeâ€“compatible semantic search</title>
    <updated>2025-08-08T21:08:43+00:00</updated>
    <author>
      <name>/u/Lanky-District9096</name>
      <uri>https://old.reddit.com/user/Lanky-District9096</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weâ€™re building &lt;strong&gt;LEANN&lt;/strong&gt; at Berkeley Sky Lab â€” a &lt;strong&gt;local vector index for RAG&lt;/strong&gt; thatâ€™s&lt;/p&gt; &lt;p&gt;ðŸ”’ privacy-first&lt;/p&gt; &lt;p&gt;ðŸ“¦ 97% smaller&lt;/p&gt; &lt;p&gt;ðŸ§  fully compatible with &lt;strong&gt;Claude Code&lt;/strong&gt;, &lt;strong&gt;Ollama&lt;/strong&gt;, and &lt;strong&gt;GPT-OSS&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Run semantic search on your laptop â€” fast, lightweight, and cloud-free.&lt;/p&gt; &lt;p&gt;ðŸ”— &lt;a href="https://github.com/yichuan-w/LEANN"&gt;https://github.com/yichuan-w/LEANN&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why does LEANN matter?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Because most vector DBs store &lt;em&gt;everything&lt;/em&gt; â€” all embeddings, all graph structure â€” which quickly balloons to 100+GB when you index emails, chat, and code.&lt;/p&gt; &lt;p&gt;Butâ€¦ most queries only touch a tiny slice of the DB. So we asked:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why store every single embedding?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;LEANN introduces two ultra-lightweight backends:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;ðŸ” Graph-only mode:&lt;/strong&gt; Stores &lt;em&gt;no embeddings&lt;/em&gt;, just a pruned HNSW graph.Recomputes embeddings on the fly using overlapping neighbors.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ðŸ’¡ PQ+Rerank mode:&lt;/strong&gt; Compresses vectors with PQ, then &lt;em&gt;replaces&lt;/em&gt; heavy embedding storage with lightweight on-the-fly recomputation on the candidate set.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each offers a different tradeoff, but both aim for the same goal:&lt;/p&gt; &lt;p&gt;ðŸ§  &lt;strong&gt;Massive storage savings&lt;/strong&gt; with &lt;em&gt;no meaningful drop&lt;/em&gt; in recall.&lt;/p&gt; &lt;p&gt;ðŸ“ &lt;strong&gt;Note:&lt;/strong&gt; In modern RAG systems â€” with long inputs and reasoning-heavy models â€” &lt;strong&gt;generation&lt;/strong&gt;, not retrieval, is the bottleneck.&lt;/p&gt; &lt;p&gt;So even with slight retrieval latency increases, the end-to-end impact is &lt;strong&gt;~5% overhead&lt;/strong&gt; or less.&lt;/p&gt; &lt;p&gt;These give you blazing-fast semantic search over:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;â€¢ ðŸ“¨ Apple Mail â€¢ ðŸ’¾ Filesystem â€¢ ðŸ•°ï¸ Chrome / Chat history â€¢ ðŸ§  Codebase (Claude Codeâ€“compatible) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;LEANN = your personal Jarvis, running locally.&lt;/p&gt; &lt;p&gt;ðŸ”— GitHub: &lt;a href="https://github.com/yichuan-w/LEANN"&gt;https://github.com/yichuan-w/LEANN&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ðŸ“„ Paper: &lt;a href="https://arxiv.org/abs/2506.08276"&gt;https://arxiv.org/abs/2506.08276&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weâ€™d love for you to try it out, give feedback, or ask questions on the repo!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lanky-District9096"&gt; /u/Lanky-District9096 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml750r/local_rag_with_97_smaller_index_and_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml750r/local_rag_with_97_smaller_index_and_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml750r/local_rag_with_97_smaller_index_and_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T21:08:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlgyct</id>
    <title>GPT-OSS 20b vs Qwen3-30B-A3B</title>
    <updated>2025-08-09T04:52:10+00:00</updated>
    <author>
      <name>/u/unofficialreddit0r</name>
      <uri>https://old.reddit.com/user/unofficialreddit0r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone used them in real life coding task? How do they compare?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialreddit0r"&gt; /u/unofficialreddit0r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlgyct/gptoss_20b_vs_qwen330ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlgyct/gptoss_20b_vs_qwen330ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlgyct/gptoss_20b_vs_qwen330ba3b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T04:52:10+00:00</published>
  </entry>
</feed>
