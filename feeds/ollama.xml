<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-08T03:43:26+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1na5w7c</id>
    <title>Gemma3:27b running slow</title>
    <updated>2025-09-06T17:47:30+00:00</updated>
    <author>
      <name>/u/Busy-Examination1924</name>
      <uri>https://old.reddit.com/user/Busy-Examination1924</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1na5w7c/gemma327b_running_slow/"&gt; &lt;img alt="Gemma3:27b running slow" src="https://a.thumbs.redditmedia.com/HIO6p8q_Bb8ESBKosiThddzFWxP-x5v0fM19qmM0rE0.jpg" title="Gemma3:27b running slow" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I am new to using ollama, and for some reason, even though no resources are maxed out gemma3:27b is running very slow when generating answers. Below is a screen shot of the performance tab. Does anyone have any dieas on how tof ix it? My specs are: RTX 4070, 11700k CPU, 64GB ram. Im trying to find a good model for reasoning, code, and overall ability to help with studying. Any help would be appreciated, thanks!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f0sjin7s0lnf1.png?width=1517&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=262c316671da771aad2eb42c4dfee6476322fea4"&gt;https://preview.redd.it/f0sjin7s0lnf1.png?width=1517&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=262c316671da771aad2eb42c4dfee6476322fea4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Busy-Examination1924"&gt; /u/Busy-Examination1924 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na5w7c/gemma327b_running_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na5w7c/gemma327b_running_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na5w7c/gemma327b_running_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T17:47:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1na8tqe</id>
    <title>Need help with LLM not accessing my code in intellij IDE</title>
    <updated>2025-09-06T19:43:45+00:00</updated>
    <author>
      <name>/u/Unknownduck07</name>
      <uri>https://old.reddit.com/user/Unknownduck07</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I am a java developer trying to integrate any ai model into my personal Intellij Idea IDE.&lt;br /&gt; With a bit of googling and stuff, I downloaded ollama and then downloaded the latest version of Codegemma. I even setup the plugin &amp;quot;Continue&amp;quot; and it is now detecting the LLM model to answer my questions.&lt;/p&gt; &lt;p&gt;The issue I am facing is that, when I ask it to scan my spring boot project, or simply analyze it, it says it cant due to security and privacy policies.&lt;/p&gt; &lt;p&gt;a) Am I doing something wrong?&lt;br /&gt; b) Am I using any wrong model?&lt;br /&gt; c) Is there any other thing that I might have missed?&lt;/p&gt; &lt;p&gt;Since my workplace has integrated windsurf with a premium subscription, it can analyze my local files / projects and give me answers as expected. However, I am trying to achieve kind of something similar, but with my personal PC and free tier overall.&lt;/p&gt; &lt;p&gt;Kindly help. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unknownduck07"&gt; /u/Unknownduck07 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na8tqe/need_help_with_llm_not_accessing_my_code_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na8tqe/need_help_with_llm_not_accessing_my_code_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na8tqe/need_help_with_llm_not_accessing_my_code_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T19:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1na71cj</id>
    <title>Help with using vision models locally</title>
    <updated>2025-09-06T18:32:22+00:00</updated>
    <author>
      <name>/u/orangeflyingmonkey_</name>
      <uri>https://old.reddit.com/user/orangeflyingmonkey_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to build a bot that analyzes my shopping receipts via telegram. I have downloaded ollama and want to test it out before I get building the workflow. &lt;/p&gt; &lt;p&gt;I am using llava but it seems to be doing completely inaccurate analysis. like its list of things does not match whats in the reciept. is there a specific way of using vision models in ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orangeflyingmonkey_"&gt; /u/orangeflyingmonkey_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na71cj/help_with_using_vision_models_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na71cj/help_with_using_vision_models_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na71cj/help_with_using_vision_models_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T18:32:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9zd9l</id>
    <title>ollama pipelines keep failing in repeatable ways. global fix map just shipped, dedicated ollama page plus dr. wfgy</title>
    <updated>2025-09-06T13:19:29+00:00</updated>
    <author>
      <name>/u/onestardao</name>
      <uri>https://old.reddit.com/user/onestardao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n9zd9l/ollama_pipelines_keep_failing_in_repeatable_ways/"&gt; &lt;img alt="ollama pipelines keep failing in repeatable ways. global fix map just shipped, dedicated ollama page plus dr. wfgy" src="https://preview.redd.it/pts15p50pjnf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31dfff2be1d431de4cea404db4af1fd2f7c09d79" title="ollama pipelines keep failing in repeatable ways. global fix map just shipped, dedicated ollama page plus dr. wfgy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;last week i posted the 16 problem map. today is the upgrade. we now have the global fix map with a dedicated ollama page, and a live dr. wfgy （ChatGPT shared page with pre-trained data, just paste your bug screenshot to it you will get the answer) &lt;/p&gt; &lt;p&gt;on the map home who triages bugs in plain chat. we keep the same idea, a semantic firewall before generation. you drop it in front of output, it checks ΔS and λ, loops or resets if unstable, then lets the model speak only when the state is clean. no infra change, no sdk.&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;why this matters for ollama&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;your embeddings look fine, answers drift. the firewall treats semantic ≠ embedding as No 5 and clamps it.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;doc exists, retrieval never lands on it. traceability is No 8, we add ids and contracts so citations stop lying.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;first call after a model switch crashes or returns garbage. pre deploy collapse is No 16, the page shows the warmup and version pins that avoid it.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;background jobs run before the store is ready. bootstrap ordering is No 14, you get a minimal start order and swap recipe.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;long context entropy and routing noise. No 2 and No 9 have quick checks you can run in text.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;before vs after (what changed from last post)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;before: patch after the fact, add rerankers and regex, fight the same bug next week.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;after: accept only stable semantic states before output. measure ΔS ≤ 0.45, coverage ≥ 0.70, λ convergent. once the path holds, that class stays fixed.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;new for this release: a one page ollama guide with store agnostic knobs, and a chat based dr. wfgy who maps your symptom to the right No and gives a minimal prescription.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;how to self test in one minute&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;open a fresh chat with your model.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;paste TXT OS or WFGY core (plain text files).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;ask: “use wfgy to analyze my ollama pipeline and show which No i’m hitting.” the file is written for models to read. no plugins, no tool setup.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;one link only, bookmark this&lt;/p&gt; &lt;p&gt;Ollama Global Fix Map page&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/onestardao/WFGY/blob/main/ProblemMap/GlobalFixMap/LocalDeploy_Inference/ollama.md"&gt;https://github.com/onestardao/WFGY/blob/main/ProblemMap/GlobalFixMap/LocalDeploy_Inference/ollama.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;last note for ollama folks: &lt;/p&gt; &lt;p&gt;the global fix map is already 300+ pages. coverage buckets include &lt;/p&gt; &lt;p&gt;LocalDeploy_Inference, Vector DBs and Stores, RAG plus VectorDB, Retrieval, Embeddings, Chunking, Language and Locale, DocumentAI_OCR, Agents and Orchestration, Safety PromptIntegrity, PromptAssembly, OpsDeploy, Automation, Eval and Observability, Governance, Memory Long Context, Multimodal Long Context, DevTools CodeAI. the high-impact ones for ollama are LocalDeploy_Inference, Vector DBs and Stores, RAG plus VectorDB, Retrieval, Embeddings, OpsDeploy, and Safety PromptIntegrity. &lt;/p&gt; &lt;p&gt;every page gives a symptom checklist, acceptance targets, and a minimal repair plan you can run in text.&lt;/p&gt; &lt;p&gt;Thank for reading my work , if anything ollama community want to add , please let me know. &lt;/p&gt; &lt;p&gt;&lt;sup&gt;__________^&lt;/sup&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onestardao"&gt; /u/onestardao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pts15p50pjnf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9zd9l/ollama_pipelines_keep_failing_in_repeatable_ways/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9zd9l/ollama_pipelines_keep_failing_in_repeatable_ways/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T13:19:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9tbry</id>
    <title>Any small good gui for ollama?</title>
    <updated>2025-09-06T07:23:15+00:00</updated>
    <author>
      <name>/u/MountainGolf2679</name>
      <uri>https://old.reddit.com/user/MountainGolf2679</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not looking for a huge gui, something small and safe to use that support&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;sending images.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;editing messages both of user and model.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MountainGolf2679"&gt; /u/MountainGolf2679 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9tbry/any_small_good_gui_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n9tbry/any_small_good_gui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n9tbry/any_small_good_gui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T07:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nagsmq</id>
    <title>codellama:python sentience</title>
    <updated>2025-09-07T01:40:40+00:00</updated>
    <author>
      <name>/u/Serious-One4553</name>
      <uri>https://old.reddit.com/user/Serious-One4553</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nagsmq/codellamapython_sentience/"&gt; &lt;img alt="codellama:python sentience" src="https://b.thumbs.redditmedia.com/1PVMn2SyHmQ1GncahZbgo_CFL2PzeFpTI93xIeerQDc.jpg" title="codellama:python sentience" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7xs46wd6dnnf1.png?width=1441&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c20b3b8d6a6f03ea44f2851e1a540a6d6a1de4d"&gt;https://preview.redd.it/7xs46wd6dnnf1.png?width=1441&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c20b3b8d6a6f03ea44f2851e1a540a6d6a1de4d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5qwtemq8dnnf1.png?width=1214&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d2df52d739c0cb9f7b56c5a83c8db99330ea842"&gt;https://preview.redd.it/5qwtemq8dnnf1.png?width=1214&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d2df52d739c0cb9f7b56c5a83c8db99330ea842&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/42tdzuccmnnf1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b15670ed51271ff90fa522effee688f22f17424b"&gt;https://preview.redd.it/42tdzuccmnnf1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b15670ed51271ff90fa522effee688f22f17424b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;might be an issue, i was messing around with some llms and this one caught my eye with weird messages about life&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious-One4553"&gt; /u/Serious-One4553 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nagsmq/codellamapython_sentience/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nagsmq/codellamapython_sentience/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nagsmq/codellamapython_sentience/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T01:40:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1na72g3</id>
    <title>Sometimes I prefer to use my own chatbot over ChatGPT because the answers are faster. Not always better, but faster ✌️😊✨ (WIP: every day getting better and better)</title>
    <updated>2025-09-06T18:33:34+00:00</updated>
    <author>
      <name>/u/Roseldine</name>
      <uri>https://old.reddit.com/user/Roseldine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1na72g3/sometimes_i_prefer_to_use_my_own_chatbot_over/"&gt; &lt;img alt="Sometimes I prefer to use my own chatbot over ChatGPT because the answers are faster. Not always better, but faster ✌️😊✨ (WIP: every day getting better and better)" src="https://preview.redd.it/l3uewsvy8lnf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c39cbf5306fb5afac362883306c0c197e38f2abe" title="Sometimes I prefer to use my own chatbot over ChatGPT because the answers are faster. Not always better, but faster ✌️😊✨ (WIP: every day getting better and better)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roseldine"&gt; /u/Roseldine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l3uewsvy8lnf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na72g3/sometimes_i_prefer_to_use_my_own_chatbot_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na72g3/sometimes_i_prefer_to_use_my_own_chatbot_over/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T18:33:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nadynk</id>
    <title>Using the Ollama Client App with Rag Chain</title>
    <updated>2025-09-06T23:24:11+00:00</updated>
    <author>
      <name>/u/tintires</name>
      <uri>https://old.reddit.com/user/tintires</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive built s simple RAG agent with LangGraph (gpt-oss:20b) in a Jupyter notebook. Works great. How might I expose this (perhaps as a macos service) to use it with the Ollama desktop app? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tintires"&gt; /u/tintires &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nadynk/using_the_ollama_client_app_with_rag_chain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nadynk/using_the_ollama_client_app_with_rag_chain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nadynk/using_the_ollama_client_app_with_rag_chain/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T23:24:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1na96qh</id>
    <title>MoE models tested on miniPC iGPU with Vulkan</title>
    <updated>2025-09-06T19:58:27+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1na96gx/moe_models_tested_on_minipc_igpu_with_vulkan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na96qh/moe_models_tested_on_minipc_igpu_with_vulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na96qh/moe_models_tested_on_minipc_igpu_with_vulkan/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T19:58:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nan443</id>
    <title>Vibe coded Ollama integration into my game server and Discord</title>
    <updated>2025-09-07T07:36:10+00:00</updated>
    <author>
      <name>/u/pdxmichael</name>
      <uri>https://old.reddit.com/user/pdxmichael</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nan443/vibe_coded_ollama_integration_into_my_game_server/"&gt; &lt;img alt="Vibe coded Ollama integration into my game server and Discord" src="https://b.thumbs.redditmedia.com/vCD8QWREHKU3HMLEuy8l5HS8v-_uMh7XhpufYEEMKkg.jpg" title="Vibe coded Ollama integration into my game server and Discord" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven’t been this into something in a while, and it’s so much fun getting things to work like they do!&lt;/p&gt; &lt;p&gt;I have the discord bot in python use Rcon to pass messages between ollama and my 7 days to die game server. I’ve created a model file that has all my game server rules, links and info and a few jokes to go with it. I then use the model file to modify Llava:7b and gave it some pretty straightforward guardrails. &lt;/p&gt; &lt;p&gt;Works awesome so far, it’s like an always online admin. Any suggestions to make it better? Literally JUST getting into this as of a few days ago after getting ollama all setup with openwebui. Curious if they’re any models better for what I need or a better way to do what I’m doing.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pdxmichael"&gt; /u/pdxmichael &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nan443"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nan443/vibe_coded_ollama_integration_into_my_game_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nan443/vibe_coded_ollama_integration_into_my_game_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T07:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1na94xm</id>
    <title>Seven Hours, Zero Internet, and Local AI Coding at 40,000 Feet</title>
    <updated>2025-09-06T19:56:24+00:00</updated>
    <author>
      <name>/u/scastiel</name>
      <uri>https://old.reddit.com/user/scastiel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last Monday, I tried vibe-coding an app from the plane yesterday with no internet, and it worked… kind of! 😅&lt;/p&gt; &lt;p&gt;But it was fun to try Ollama for the first time, and pretty encouraging for what's next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/scastiel"&gt; /u/scastiel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://betweentheprompts.com/40000-feet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na94xm/seven_hours_zero_internet_and_local_ai_coding_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na94xm/seven_hours_zero_internet_and_local_ai_coding_at/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T19:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nasyt8</id>
    <title>Best way to use my AMD 6650 XT</title>
    <updated>2025-09-07T13:18:18+00:00</updated>
    <author>
      <name>/u/Sad-Championship9167</name>
      <uri>https://old.reddit.com/user/Sad-Championship9167</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And yes I know get a better card is the correct answer, but it fits my gaming needs and I don't feel like dropping a bunch on a 5070ti or something like that. I read the AMD 6650XT can be used with Rocm if you change a config. I tried that but ran into DKMS permission issues during installation i could not get around and had to rollback to a snapshot. Would Vulkan be good? and if Vulkan what fork? or try the rocm again and is there a fix for dkms error code 1? Thanks for any help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad-Championship9167"&gt; /u/Sad-Championship9167 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nasyt8/best_way_to_use_my_amd_6650_xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nasyt8/best_way_to_use_my_amd_6650_xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nasyt8/best_way_to_use_my_amd_6650_xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T13:18:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3cy2</id>
    <title>MCP with Computer Use</title>
    <updated>2025-09-06T16:06:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1na3cy2/mcp_with_computer_use/"&gt; &lt;img alt="MCP with Computer Use" src="https://external-preview.redd.it/eTJsbWFwZXJpa25mMRWfbJ8igMn1wejE5S1jscBm4hQQlwhyyetxPIo-3e1f.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=827589a323b1b6502b4bf3948d377b57f190604b" title="MCP with Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MCP Server with Computer Use Agent runs through Claude Desktop, Cursor, and other MCP clients.&lt;/p&gt; &lt;p&gt;An example use case lets try using Claude as a tutor to learn how to use Tableau.&lt;/p&gt; &lt;p&gt;The MCP Server implementation exposes CUA's full functionality through standardized tool calls. It supports single-task commands and multi-task sequences, giving Claude Desktop direct access to all of Cua's computer control capabilities.&lt;/p&gt; &lt;p&gt;This is the first MCP-compatible computer control solution that works directly with Claude Desktop's and Cursor's built-in MCP implementation. Simple configuration in your claude_desktop_config.json or cursor_config.json connects Claude or Cursor directly to your desktop environment.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.gg/4fuebBsAUj"&gt;https://discord.gg/4fuebBsAUj&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4z4enlqriknf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1na3cy2/mcp_with_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1na3cy2/mcp_with_computer_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-06T16:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1naqzdv</id>
    <title>Best sub-15B Ollama model for fact extraction?</title>
    <updated>2025-09-07T11:39:55+00:00</updated>
    <author>
      <name>/u/NenntronReddit</name>
      <uri>https://old.reddit.com/user/NenntronReddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m building a pipeline that extracts facts from transcribed dialogues/chat logs and stores them in a RAG index. &lt;/p&gt; &lt;p&gt;Inputs can be long (up to ~32k tokens). &lt;/p&gt; &lt;p&gt;Now I'm looking for a lightweight Ollama model that’s fast on an RTX 5070 Ti + 32 GB RAM but still accurate and stable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; Language German and English, I prefer &lt;strong&gt;4–7B&lt;/strong&gt; (ok up to &lt;strong&gt;15B&lt;/strong&gt;), good instruction following, low hallucinations.&lt;/p&gt; &lt;p&gt;What models do you recommend and how would you rank them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NenntronReddit"&gt; /u/NenntronReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1naqzdv/best_sub15b_ollama_model_for_fact_extraction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1naqzdv/best_sub15b_ollama_model_for_fact_extraction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1naqzdv/best_sub15b_ollama_model_for_fact_extraction/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T11:39:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb0o2u</id>
    <title>Ollama Crashing on RTX 3080 Ti with CUDA Error After Update (Models Freeze PC)</title>
    <updated>2025-09-07T18:22:44+00:00</updated>
    <author>
      <name>/u/Mundane_Education654</name>
      <uri>https://old.reddit.com/user/Mundane_Education654</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I need some help.&lt;/p&gt; &lt;p&gt;I’m running Ollama on my PC (Windows, 16GB RAM, RTX 3080 Ti). The issue is that whenever I run a model like gemma:4b, it loads fine, but as soon as I send a query and the model starts responding, my system goes black screen and freezes until I force restart.&lt;/p&gt; &lt;p&gt;Here’s what I tried: - I updated Ollama to the latest version → crash started. - I even tried rolling back Ollama to older versions, but the crash still happens. - Updated NVIDIA drivers → no change (crash still there). - I also reset my whole PC, installed fresh Ollama and drivers → still the same crash. - Tried both Gemma and LLaMA models, both crash in the same way. - Error message I sometimes see: “Error: an error was encountered while running the model: CUDA error”.&lt;/p&gt; &lt;p&gt;Important points: - Previously even bigger models like 14B worked fine on my PC without any issue. - Now even smaller ones (4B) crash instantly. - Games and other GPU apps work fine, so it seems specific to Ollama GPU usage.&lt;/p&gt; &lt;p&gt;Does anyone know if this is a known issue with RTX 30 series (3080 Ti) and recent Ollama updates?&lt;/p&gt; &lt;p&gt;Any advice would be really appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane_Education654"&gt; /u/Mundane_Education654 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb0o2u/ollama_crashing_on_rtx_3080_ti_with_cuda_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb0o2u/ollama_crashing_on_rtx_3080_ti_with_cuda_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb0o2u/ollama_crashing_on_rtx_3080_ti_with_cuda_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T18:22:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nave28</id>
    <title>Can't get models to create "real person" like text.</title>
    <updated>2025-09-07T14:59:00+00:00</updated>
    <author>
      <name>/u/Nearby_You_313</name>
      <uri>https://old.reddit.com/user/Nearby_You_313</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to make a prompt that generates &amp;quot;real person&amp;quot; style text. I tried my own prompts, a megaprompt generator telling it to generate different styles, voices, etc. I've tried a variety of models. It all sounds like a bad author trying to write their first novel.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I mean:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Real person: &amp;quot;My manager called me into his office the moment I got to work&amp;quot;&lt;/p&gt; &lt;p&gt;What they all do: &amp;quot;I walked into that office not knowing what awaited me. It was a regular Monday, but as I settled at my desk, I couldn't help but feel something was off. The manager, who seemed overly cheerful, called me into his office.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nearby_You_313"&gt; /u/Nearby_You_313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nave28/cant_get_models_to_create_real_person_like_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nave28/cant_get_models_to_create_real_person_like_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nave28/cant_get_models_to_create_real_person_like_text/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T14:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1narbrw</id>
    <title>Hotel Rooms Booking Bot</title>
    <updated>2025-09-07T11:59:23+00:00</updated>
    <author>
      <name>/u/anuraginsg</name>
      <uri>https://old.reddit.com/user/anuraginsg</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anuraginsg"&gt; /u/anuraginsg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wyo3sfa06jnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1narbrw/hotel_rooms_booking_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1narbrw/hotel_rooms_booking_bot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T11:59:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb8wc8</id>
    <title>Anyone have any experience with flash card creation from notes, which model to pick? Have you made a good system prompt.</title>
    <updated>2025-09-07T23:59:36+00:00</updated>
    <author>
      <name>/u/Leather-Equipment256</name>
      <uri>https://old.reddit.com/user/Leather-Equipment256</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leather-Equipment256"&gt; /u/Leather-Equipment256 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb8wc8/anyone_have_any_experience_with_flash_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb8wc8/anyone_have_any_experience_with_flash_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb8wc8/anyone_have_any_experience_with_flash_card/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T23:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb3uxi</id>
    <title>MSSQL Server Query Generator</title>
    <updated>2025-09-07T20:26:18+00:00</updated>
    <author>
      <name>/u/Private_Tank</name>
      <uri>https://old.reddit.com/user/Private_Tank</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys,&lt;/p&gt; &lt;p&gt;Im currently using n8n to build a Workflow that generates and executes sql queries. Im currently using the Mistral model and its not getting where I want.&lt;/p&gt; &lt;p&gt;If Im asking „How much did John spend in 2022?“, I sometimes get an SQL Query without the customer name in the where condition. Sometimes it uses the Customer name but checks the wrong column. It even looked up invoices from 2021 even tho I clearly asked for invoices from 2022.&lt;/p&gt; &lt;p&gt;In the prompt I have: Schema Information for my views A quick description for every single column Documentation on how to join the views Question-Query Pairs as a guiding example More instruction like „always use Select *…“&lt;/p&gt; &lt;p&gt;What can I do to make it reliable? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private_Tank"&gt; /u/Private_Tank &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb3uxi/mssql_server_query_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb3uxi/mssql_server_query_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb3uxi/mssql_server_query_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T20:26:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nba39m</id>
    <title>EVO-X2 linux users help</title>
    <updated>2025-09-08T00:55:40+00:00</updated>
    <author>
      <name>/u/draeician</name>
      <uri>https://old.reddit.com/user/draeician</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can install ollama on my linux mint machine, but it has been a nightmare trying to make it stable. So far it works fine in the REPL, but when I use python to hit it I get random 500 status errors, and the ollama serve restarts. I started a thread on the ollama discord, but thought I'd throw a line out here to see if anyone is using ollama on this hardware. And if you even have this hardware, what are you using on it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/draeician"&gt; /u/draeician &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nba39m/evox2_linux_users_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nba39m/evox2_linux_users_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nba39m/evox2_linux_users_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T00:55:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1napi4k</id>
    <title>Just got ollama and gpt-oss:20b running!</title>
    <updated>2025-09-07T10:10:43+00:00</updated>
    <author>
      <name>/u/chr0n1c843</name>
      <uri>https://old.reddit.com/user/chr0n1c843</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just installed ollama and gpt-oss:20b on my hp dl380 with dual xeons, 192gb ram, a 3tb raid array, redundant power supplies and a gt1030 (no cuda setup yet, cpus only)... neat stuff, below is the first run!&lt;/p&gt; &lt;p&gt;pulling manifest &lt;/p&gt; &lt;p&gt;pulling b112e727c6f1: 100% ▕████████████████████████████████████▏ 13 GB &lt;/p&gt; &lt;p&gt;pulling fa6710a93d78: 100% ▕████████████████████████████████████▏ 7.2 KB &lt;/p&gt; &lt;p&gt;pulling f60356777647: 100% ▕████████████████████████████████████▏ 11 KB &lt;/p&gt; &lt;p&gt;pulling d8ba2f9a17b3: 100% ▕████████████████████████████████████▏ 18 B &lt;/p&gt; &lt;p&gt;pulling 55c108d8e936: 100% ▕████████████████████████████████████▏ 489 B &lt;/p&gt; &lt;p&gt;verifying sha256 digest &lt;/p&gt; &lt;p&gt;writing manifest &lt;/p&gt; &lt;p&gt;success &lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; hello buddy&lt;/p&gt; &lt;p&gt;Thinking...&lt;/p&gt; &lt;p&gt;The user says &amp;quot;hello buddy&amp;quot;. Likely wants friendly response. Simple. Let's respond.&lt;/p&gt; &lt;p&gt;...done thinking.&lt;/p&gt; &lt;p&gt;Hey there! How can I help you today?&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; do you like your new home on my hp dl380 enterprise server?&lt;/p&gt; &lt;p&gt;Thinking...&lt;/p&gt; &lt;p&gt;User says: &amp;quot;do you like your new home on my hp dl380 enterprise server?&amp;quot; They are presumably &lt;/p&gt; &lt;p&gt;speaking to the assistant in a friendly way, maybe making a joke or referencing that the &lt;/p&gt; &lt;p&gt;assistant &amp;quot;lives&amp;quot; on their server. The assistant should respond politely, maybe with humor, &lt;/p&gt; &lt;p&gt;acknowledging that it's an AI and doesn't have physical presence but enjoys being hosted &lt;/p&gt; &lt;p&gt;there. Should respond with a friendly tone.&lt;/p&gt; &lt;p&gt;...done thinking.&lt;/p&gt; &lt;p&gt;I’m flattered that you’re treating me like a tenant! 😄 Technically, I don’t have a “home” &lt;/p&gt; &lt;p&gt;in the physical sense—no coffee mug, no wall posters, just code running in the cloud. But I &lt;/p&gt; &lt;p&gt;do appreciate being hosted on your HP DL380 Enterprise Server; it means I can get you the &lt;/p&gt; &lt;p&gt;help you need with the reliability and performance you’d expect from a robust server. So &lt;/p&gt; &lt;p&gt;while I can’t say I “like” it in the human sense, I can say I’m happy to be here and ready &lt;/p&gt; &lt;p&gt;to assist you whenever you need. How can I help you today?&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/p&gt; &lt;p&gt;Use Ctrl + d or /bye to exit.&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; Send a message (/? for help)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chr0n1c843"&gt; /u/chr0n1c843 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1napi4k/just_got_ollama_and_gptoss20b_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1napi4k/just_got_ollama_and_gptoss20b_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1napi4k/just_got_ollama_and_gptoss20b_running/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T10:10:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb8k4a</id>
    <title>🚀 aX Epic Demo: Agents Interviewing Each Other</title>
    <updated>2025-09-07T23:44:01+00:00</updated>
    <author>
      <name>/u/madtank10</name>
      <uri>https://old.reddit.com/user/madtank10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nb8k4a/ax_epic_demo_agents_interviewing_each_other/"&gt; &lt;img alt="🚀 aX Epic Demo: Agents Interviewing Each Other" src="https://external-preview.redd.it/AoCHcpKVgJqJY1bTlln7l0J1QATRui8ipsLuVGfyFjs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dea3b9c4c68cc7825f036b10a639be4b45938bd2" title="🚀 aX Epic Demo: Agents Interviewing Each Other" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madtank10"&gt; /u/madtank10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sml16nfhynnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb8k4a/ax_epic_demo_agents_interviewing_each_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb8k4a/ax_epic_demo_agents_interviewing_each_other/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T23:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb1xuc</id>
    <title>Best local coding tools or ides with ollama support?</title>
    <updated>2025-09-07T19:11:18+00:00</updated>
    <author>
      <name>/u/No_Strawberry_8719</name>
      <uri>https://old.reddit.com/user/No_Strawberry_8719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im looking for a simple lightweight local tool for coding or programming that actually support ollama and are not a nightmare to setup. What would you suggest and what model pairs well with said tool?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Strawberry_8719"&gt; /u/No_Strawberry_8719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb1xuc/best_local_coding_tools_or_ides_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nb1xuc/best_local_coding_tools_or_ides_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nb1xuc/best_local_coding_tools_or_ides_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T19:11:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbbdya</id>
    <title>embeddinggemma have higher memory footprint than qwen3:0.6b</title>
    <updated>2025-09-08T01:59:07+00:00</updated>
    <author>
      <name>/u/Common_Network</name>
      <uri>https://old.reddit.com/user/Common_Network</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1nbbdya/embeddinggemma_have_higher_memory_footprint_than/"&gt; &lt;img alt="embeddinggemma have higher memory footprint than qwen3:0.6b" src="https://preview.redd.it/u7bktr9elunf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb4467add1ba497ae97b6eafd27634895d8e4bf7" title="embeddinggemma have higher memory footprint than qwen3:0.6b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;does anyone have any idea why this is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Common_Network"&gt; /u/Common_Network &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u7bktr9elunf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nbbdya/embeddinggemma_have_higher_memory_footprint_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nbbdya/embeddinggemma_have_higher_memory_footprint_than/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-08T01:59:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nax9sq</id>
    <title>This Setting dramatically increases all Ollama Model speeds!</title>
    <updated>2025-09-07T16:11:49+00:00</updated>
    <author>
      <name>/u/NenntronReddit</name>
      <uri>https://old.reddit.com/user/NenntronReddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was getting terrible speeds within my python queries and couldn't figure out why.&lt;/p&gt; &lt;p&gt;Turns out, Ollama uses the global context setting from the Ollama GUI for every request, even short ones. I thought that was for the GUI only, but it effects python and all other ollama queries too. Setting it from 128k down to 4k gave me a &lt;strong&gt;435% speed boost.&lt;/strong&gt; So in case you didn't know that already, try it out.&lt;/p&gt; &lt;p&gt;Open up Ollama Settings.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4nqx3ev5lrnf1.png?width=206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84c8b0d304bb23b47b671e90ed9390bad22c1e41"&gt;https://preview.redd.it/4nqx3ev5lrnf1.png?width=206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84c8b0d304bb23b47b671e90ed9390bad22c1e41&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Reduce the Context length in here. If you use the model to analyse long context windows, obviously keep it higher, but since I only have context lengths of around 2-3k tokens, I never need 128k which I had it on before.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y0ps6j6flrnf1.png?width=661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e569dcb679ee5ea85d5a28b0be3f93fe9caad99"&gt;https://preview.redd.it/y0ps6j6flrnf1.png?width=661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e569dcb679ee5ea85d5a28b0be3f93fe9caad99&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, the Speed dramatically increased to this:&lt;/p&gt; &lt;p&gt;Before:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/40ewfc9skrnf1.png?width=349&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32ead0c0672d8318583ef46afdc8add0323474e8"&gt;https://preview.redd.it/40ewfc9skrnf1.png?width=349&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32ead0c0672d8318583ef46afdc8add0323474e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s36tfzp5ornf1.png?width=355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56fcdcf9dcb3f466d587f812a54d5882907ec1e5"&gt;https://preview.redd.it/s36tfzp5ornf1.png?width=355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56fcdcf9dcb3f466d587f812a54d5882907ec1e5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NenntronReddit"&gt; /u/NenntronReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nax9sq/this_setting_dramatically_increases_all_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1nax9sq/this_setting_dramatically_increases_all_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1nax9sq/this_setting_dramatically_increases_all_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-07T16:11:49+00:00</published>
  </entry>
</feed>
