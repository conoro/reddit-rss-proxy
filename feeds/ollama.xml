<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-06T14:48:48+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pe5x7a</id>
    <title>smallevals - Tiny 0.6B Evaluation Models and a Local LLM Evaluation Framework</title>
    <updated>2025-12-04T17:23:26+00:00</updated>
    <author>
      <name>/u/mburaksayici</name>
      <uri>https://old.reddit.com/user/mburaksayici</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mburaksayici"&gt; /u/mburaksayici &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Rag/comments/1pe56ch/smallevals_tiny_06b_evaluation_models_and_a_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe5x7a/smallevals_tiny_06b_evaluation_models_and_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe5x7a/smallevals_tiny_06b_evaluation_models_and_a_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T17:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe38x8</id>
    <title>AI Runner Release v5.1.0 · support for art, text-to-speech and speech-to-text in headless server mode</title>
    <updated>2025-12-04T15:43:22+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pe38x8/ai_runner_release_v510_support_for_art/"&gt; &lt;img alt="AI Runner Release v5.1.0 · support for art, text-to-speech and speech-to-text in headless server mode" src="https://external-preview.redd.it/EIGwz7Gm65cPP16X5uk_KCDLI-pxhanbMt56tZbOIho.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17e3785ebc1c34a9ccb69b71dd5325bac0a7e318" title="AI Runner Release v5.1.0 · support for art, text-to-speech and speech-to-text in headless server mode" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title pretty much says it all.&lt;/p&gt; &lt;p&gt;AI Runner can be used as an alternative to ollama for experimentation, educational purposes, serious professional projects during development (for example I choose to use my server when developing cloud based chatbots as it has the same endpoints as ollama and openai).&lt;/p&gt; &lt;p&gt;Works with LLM, art, tts, stt.&lt;/p&gt; &lt;p&gt;This application also comes with a GUI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://airunner.org"&gt;https://airunner.org&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Capsize-Games/airunner/releases/tag/v5.1.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe38x8/ai_runner_release_v510_support_for_art/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe38x8/ai_runner_release_v510_support_for_art/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T15:43:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdw272</id>
    <title>Ollama powered chat component for any website</title>
    <updated>2025-12-04T09:51:52+00:00</updated>
    <author>
      <name>/u/ovi_nation</name>
      <uri>https://old.reddit.com/user/ovi_nation</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pdw272/ollama_powered_chat_component_for_any_website/"&gt; &lt;img alt="Ollama powered chat component for any website" src="https://preview.redd.it/d7xbh8i2t55g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=514ff5f50122295dad7147a13b43b905a32c3830" title="Ollama powered chat component for any website" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! I have open sourced a project called Deep Chat. It is a feature-rich chat web component that can be used to connect and converse with Ollama models.&lt;/p&gt; &lt;p&gt;Check it out at:&lt;br /&gt; &lt;a href="https://github.com/OvidijusParsiunas/deep-chat?fbclid=IwAR0uSvTiVXL5rICg3YfKqV2er0E355LGrg5ha6JVkUEaem8PKU98sU6ysbE"&gt;https://github.com/OvidijusParsiunas/deep-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A GitHub star is ALWAYS appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ovi_nation"&gt; /u/ovi_nation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d7xbh8i2t55g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdw272/ollama_powered_chat_component_for_any_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdw272/ollama_powered_chat_component_for_any_website/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T09:51:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe1dba</id>
    <title>Hito 1.7 GGUF release</title>
    <updated>2025-12-04T14:27:29+00:00</updated>
    <author>
      <name>/u/TastyWriting8360</name>
      <uri>https://old.reddit.com/user/TastyWriting8360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I will leave this here open wieghts &lt;a href="https://huggingface.co/hitonet/hito-1.7b"&gt;https://huggingface.co/hitonet/hito-1.7b&lt;/a&gt; and gguf release &lt;a href="https://huggingface.co/hitonet/hito-1.7b-GGUFI"&gt;https://huggingface.co/hitonet/hito-1.7b-GGUFI&lt;/a&gt; will make a proper post in the next few hours, gotta catch some sleep but cant wait to share this with you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TastyWriting8360"&gt; /u/TastyWriting8360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe1dba/hito_17_gguf_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe1dba/hito_17_gguf_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe1dba/hito_17_gguf_release/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T14:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdv7ya</id>
    <title>Quick real coding test GPT-OS-20b-Code vs. Ministral-3b-fp16</title>
    <updated>2025-12-04T08:56:20+00:00</updated>
    <author>
      <name>/u/New_Cranberry_6451</name>
      <uri>https://old.reddit.com/user/New_Cranberry_6451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few minutes ago I downloaded Ministral-3b-fp16 and plugged it to my 8Gb VRAM laptop running Windows and Ollama. Just a plain 0.13.1 version on a plain standard laptop.&lt;/p&gt; &lt;p&gt;I needed a simple PHP script to export rows from a mysql db to CSV, but I had a function that failed escaping when using &amp;quot;,&amp;quot; as delim.&lt;/p&gt; &lt;p&gt;I asked Ministral and obtained a classic working function that I will resume as (skipped to focus on point):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$header = str_getcsv(implode(',', array_keys($export_data[0])), ','); $csv_data = implode($csv_delim, $header) . &amp;quot;\r\n&amp;quot;; [...] // Escape commas and quotes in values $escaped_value = preg_replace('/\s+/', ' ', $value); $escaped_value = str_replace(',', ' ', $escaped_value); $escaped_value = str_replace('&amp;quot;', '&amp;quot;&amp;quot;', $escaped_value); $row_values[] = '&amp;quot;' . $escaped_value . '&amp;quot;'; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Cool. It works. Now I paste the exact same prompt into GPT-OS-20b-Code: (once again, skipped to focus on point)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// Open a temporary memory stream $fp = fopen( 'php://temp', 'r+' ); [...] fputcsv( $fp, $row, $csv_delim ); rewind( $fp ); $csv_data = stream_get_contents( $fp ); fclose( $fp ); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;WTF! This is so elegant. It delegates escaping to a core function so that the regExp-s, that are prone to fail in weird cases, are not even needed, and on the other hand, the way of treating the array as a temp stream to read and rewind blew my mind.&lt;/p&gt; &lt;p&gt;One shot prompts like these make me say THANKS for this open source gifts I can run locally and privately :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Cranberry_6451"&gt; /u/New_Cranberry_6451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdv7ya/quick_real_coding_test_gptos20bcode_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdv7ya/quick_real_coding_test_gptos20bcode_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdv7ya/quick_real_coding_test_gptos20bcode_vs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T08:56:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe2tk3</id>
    <title>ministral-3 is not using gpu in ollama</title>
    <updated>2025-12-04T15:26:18+00:00</updated>
    <author>
      <name>/u/AdhesivenessLatter57</name>
      <uri>https://old.reddit.com/user/AdhesivenessLatter57</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;why ministral-3 is running on cpu only with ollama version is 0.13.1? &lt;/p&gt; &lt;p&gt;this model starts loading in gpu and later offloads to cpu.&lt;/p&gt; &lt;p&gt;I tried 3b,8b and 14b, while qwen3-coder is running fine in same gpu. &lt;/p&gt; &lt;p&gt;Some issue in ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdhesivenessLatter57"&gt; /u/AdhesivenessLatter57 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2tk3/ministral3_is_not_using_gpu_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2tk3/ministral3_is_not_using_gpu_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe2tk3/ministral3_is_not_using_gpu_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T15:26:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe5b1t</id>
    <title>Computer Use with Claude Opus 4.5</title>
    <updated>2025-12-04T17:01:11+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pe5b1t/computer_use_with_claude_opus_45/"&gt; &lt;img alt="Computer Use with Claude Opus 4.5" src="https://external-preview.redd.it/eWc1Z2J3ZG94NzVnMY644GvbBlUugVuvecrsXUzyNOCzfLoiwHL0f-lwE9G6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64ec98da6d0fbfaba8c88013ccf90ed592f5e91c" title="Computer Use with Claude Opus 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude Opus 4.5 support to the Cua VLM Router and Playground - and you can already see it running inside Windows sandboxes. Early results are seriously impressive, even on tricky desktop workflows.&lt;/p&gt; &lt;p&gt;Benchmark results:&lt;/p&gt; &lt;p&gt;-new SOTA 66.3% on OSWorld (beats Sonnet 4.5’s 61.4% in the general model category)&lt;/p&gt; &lt;p&gt;-88.9% on tool-use&lt;/p&gt; &lt;p&gt;Better reasoning. More reliable multi-step execution.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try the playground here : &lt;a href="https://cua.ai"&gt;https://cua.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9q65hzoox75g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe5b1t/computer_use_with_claude_opus_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe5b1t/computer_use_with_claude_opus_45/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T17:01:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe0s1q</id>
    <title>New Feature in RAGLight: Multimodal PDF Ingestion</title>
    <updated>2025-12-04T14:02:16+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"&gt; &lt;img alt="New Feature in RAGLight: Multimodal PDF Ingestion" src="https://b.thumbs.redditmedia.com/UkmGLN7LsRnojkAulEBihmGFp1HrsIPIMO8GS0KQ9ko.jpg" title="New Feature in RAGLight: Multimodal PDF Ingestion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I just added a small but powerful feature to &lt;strong&gt;RAGLight&lt;/strong&gt;: you can now override any document processor, and this unlocks &lt;strong&gt;a new built-in example : a VLM-powered PDF parser.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Find repo here :&lt;/strong&gt; &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;https://github.com/Bessouat40/RAGLight&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try this new feature with many LLM provider such as &lt;strong&gt;Ollama&lt;/strong&gt; !&lt;/p&gt; &lt;h1&gt;What it does&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Extracts &lt;strong&gt;text AND images&lt;/strong&gt; from PDFs&lt;/li&gt; &lt;li&gt;Sends images to a &lt;strong&gt;Vision-Language Model&lt;/strong&gt; (Mistral, OpenAI, etc.)&lt;/li&gt; &lt;li&gt;Captions them and injects the result into your vector store&lt;/li&gt; &lt;li&gt;Makes RAG truly understand diagrams, block schemas, charts, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Super helpful for technical documentation, research papers, engineering PDFs…&lt;/p&gt; &lt;h1&gt;Minimal Example&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/di41m8op175g1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d01198e5a8bf6c0b0c64f46e9b8fe176debc2707"&gt;https://preview.redd.it/di41m8op175g1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d01198e5a8bf6c0b0c64f46e9b8fe176debc2707&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;p&gt;Most RAG tools ignore images entirely. Now RAGLight can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;interpret diagrams&lt;/li&gt; &lt;li&gt;index visual content&lt;/li&gt; &lt;li&gt;retrieve multimodal meaning&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T14:02:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdui4a</id>
    <title>Pipeshub just hit 2k GitHub stars.</title>
    <updated>2025-12-04T08:08:14+00:00</updated>
    <author>
      <name>/u/Inevitable-Letter385</name>
      <uri>https://old.reddit.com/user/Inevitable-Letter385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re super excited to share a milestone that wouldn’t have been possible without this community. &lt;strong&gt;PipesHub just crossed 2,000 GitHub stars!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thank you to everyone who tried it out, shared feedback, opened issues, or even just followed the project.&lt;/p&gt; &lt;p&gt;For those who haven’t heard of it yet, &lt;strong&gt;PipesHub&lt;/strong&gt; is a fully open-source enterprise search platform we’ve been building over the past few months. Our goal is simple: bring powerful &lt;strong&gt;Enterprise Search&lt;/strong&gt; and &lt;strong&gt;Agent Builders&lt;/strong&gt; to every team, without vendor lock-in. PipesHub brings all your business data together and makes it instantly searchable.&lt;/p&gt; &lt;p&gt;It integrates with tools like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, and even local files. You can deploy it with a single Docker Compose command.&lt;/p&gt; &lt;p&gt;Under the hood, PipesHub runs on a &lt;strong&gt;Kafka powered event streaming architecture&lt;/strong&gt;, giving it real time, scalable, fault tolerant indexing. It combines a vector database with a knowledge graph and uses &lt;strong&gt;Agentic RAG&lt;/strong&gt; to keep responses grounded in source of truth. You get visual citations, reasoning, and confidence scores, and if information isn’t found, it simply says so instead of hallucinating.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enterprise knowledge graph for deep understanding of users, orgs, and teams&lt;/li&gt; &lt;li&gt;Connect to any AI model: OpenAI, Gemini, Claude, Ollama, or any OpenAI compatible endpoint&lt;/li&gt; &lt;li&gt;Vision Language Models and OCR for images and scanned documents&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, and SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs&lt;/li&gt; &lt;li&gt;Support for all major file types, including PDFs with images and diagrams&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Builder&lt;/strong&gt; for actions like sending emails, scheduling meetings, deep research, internet search, and more&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning Agent&lt;/strong&gt; with planning capabilities&lt;/li&gt; &lt;li&gt;&lt;strong&gt;40+ connectors&lt;/strong&gt; for integrating with your business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’d love for you to check it out and share your thoughts or feedback. It truly helps guide the roadmap:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1pdudws"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Letter385"&gt; /u/Inevitable-Letter385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T08:08:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe18c7</id>
    <title>How does Ollama truncate the context when it's too long?</title>
    <updated>2025-12-04T14:21:49+00:00</updated>
    <author>
      <name>/u/AmphibianFrog</name>
      <uri>https://old.reddit.com/user/AmphibianFrog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to understand how prompts are truncated then they exceed the context limit. Let's say I have the num_ctx parameter set to 1000 tokens. I then send in a system prompt, and a whole bunch of user and assistant messages, but in total there are 2000 tokens in the context. What happens?&lt;/p&gt; &lt;p&gt;Specific questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does truncation happen per message or do partial messages get fed into the LLM?&lt;/li&gt; &lt;li&gt;Does the system prompt get truncated? Or is it always passed in even if other messages are removed from the input?&lt;/li&gt; &lt;li&gt;Do the messages get removed in pairs, i.e will it always make sure a user message is first in the prompt?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;My conclusion is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The system prompt is never truncated&lt;/li&gt; &lt;li&gt;Entire messages are dropped from the context, not individual tokens&lt;/li&gt; &lt;li&gt;Ollama will include as many messages as possible, getting rid of the oldest messages first (apart from the system prompt which is always included)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hopefully this helps someone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AmphibianFrog"&gt; /u/AmphibianFrog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T14:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe2xbe</id>
    <title>how close to codex or claude code can you get with ollama and a 4090?</title>
    <updated>2025-12-04T15:30:22+00:00</updated>
    <author>
      <name>/u/reelznfeelz</name>
      <uri>https://old.reddit.com/user/reelznfeelz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to run an agentic code assistant that can use tools on top of ollama without having to spend a whole ton of time on building it yourself? &lt;/p&gt; &lt;p&gt;Somebody here has probably followed or played in that space. &lt;/p&gt; &lt;p&gt;Just thinking/planning for the day when openAI/Claude has to charge what's actually needed to be profitable, which will be like $900/mo for the kind of usage I have. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reelznfeelz"&gt; /u/reelznfeelz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T15:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexhan</id>
    <title>Es Qwen3 el mejor modelo de IA del mundo en general en este momento? Mienten los benchmarks?</title>
    <updated>2025-12-05T15:08:59+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Algunos no nos fiamos de los Benchmarks..nos fiamos mas de lo que vemos por nosotros mismos…&lt;/p&gt; &lt;p&gt;Vosotros que pensais?&lt;/p&gt; &lt;p&gt;Es Qwen el mejor optimizado y el mas preciso mas inteligente y que mejor resultados da en matematicas fisica y programacion ? Siendo un modelo general razonablemente pequeño en comparacion con otros…aunque no he probado el nuevo modelo 3.2 exp de deepseek pero yo con el qwen3 estoy muy muy contento&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexhan/es_qwen3_el_mejor_modelo_de_ia_del_mundo_en/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexhan/es_qwen3_el_mejor_modelo_de_ia_del_mundo_en/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pexhan/es_qwen3_el_mejor_modelo_de_ia_del_mundo_en/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T15:08:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf07j7</id>
    <title>Looking for something to manage API usage</title>
    <updated>2025-12-05T16:54:07+00:00</updated>
    <author>
      <name>/u/tecneeq</name>
      <uri>https://old.reddit.com/user/tecneeq</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for something that allows me to give different amounts of tokens per day or week to different users for different models. Basically some kind of rate limiting.&lt;/p&gt; &lt;p&gt;Does anyone know something like that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tecneeq"&gt; /u/tecneeq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pf07j7/looking_for_something_to_manage_api_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pf07j7/looking_for_something_to_manage_api_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pf07j7/looking_for_something_to_manage_api_usage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T16:54:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1petnr2</id>
    <title>Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices</title>
    <updated>2025-12-05T12:20:16+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1petnr2/translating_webpages_using_local_model_on_ipad/"&gt; &lt;img alt="Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices" src="https://external-preview.redd.it/dmJremhrcWZvZDVnMVvVURK3C0St0olAiXhHNsqHlBUDFrodjuu0gc-gowgU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=471e58ef80ba2c5d0ce953debe5fc9808e05745f" title="Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Browsers comes with translation tool - but few of them provide legible translations. We are not used to the high quality translation provided by LLMs, and we expect the same experience with webpages translation when browsing.&lt;/p&gt; &lt;p&gt;I am pleased to announce that Vector Space now integrates Webpage translation. Featuring:&lt;/p&gt; &lt;p&gt;- Use a LLM instead of translation APIs&lt;/p&gt; &lt;p&gt;- Works on mobile&lt;/p&gt; &lt;p&gt;- Call local models for unlimited and private, translation&lt;/p&gt; &lt;p&gt;- Perserve HTML structures and visuals&lt;/p&gt; &lt;p&gt;- Connect to OpenAI API for faster transaction (enter your API in the settings)&lt;/p&gt; &lt;p&gt;Result is some very nice translations! Please see the video. It is filmed on a M1 iPad.&lt;/p&gt; &lt;p&gt;Try it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://short.yomer.jp/vector-space"&gt;https://short.yomer.jp/vector-space&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Limitations and next directions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Right now a relative large model (~4B) is needed for preserving HTML tags and improving translation quality. I believe a fine tuned model of a much smaller size can do the trick. With enough people supporting me I can work on it to increase translation speed at least 10x.&lt;/li&gt; &lt;li&gt;Due to Apple restriction on running GPU work in the background, currently only iPad multi tasking is supported on iOS. I believe this is solvable by either looking at Background Tasks framework or move to neural engine.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i3d55jqfod5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1petnr2/translating_webpages_using_local_model_on_ipad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1petnr2/translating_webpages_using_local_model_on_ipad/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T12:20:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1peu215</id>
    <title>A better way to share and talk to long PDFs</title>
    <updated>2025-12-05T12:40:44+00:00</updated>
    <author>
      <name>/u/simplext</name>
      <uri>https://old.reddit.com/user/simplext</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1peu215/a_better_way_to_share_and_talk_to_long_pdfs/"&gt; &lt;img alt="A better way to share and talk to long PDFs" src="https://external-preview.redd.it/OGpvMjQ4ZmNyZDVnMbGTwPFoXGKKHKurfth5fGD5PZl_uCHNFDWTG1GTjAFD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0adbc635fe108608aa908b6ae8ac3dae15871e9" title="A better way to share and talk to long PDFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Imagine going through the long and boring jobs data PDF shared by the BLS. What if they had shared a presentation that you could talk to ? &lt;/p&gt; &lt;p&gt;Visual Book allows you to break down an PDF into slides and then share it with people so they can talk to it.&lt;/p&gt; &lt;p&gt;Visual Book: &lt;a href="https://www.visualbook.app"&gt;https://www.visualbook.app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simplext"&gt; /u/simplext &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3vqnevecrd5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1peu215/a_better_way_to_share_and_talk_to_long_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1peu215/a_better_way_to_share_and_talk_to_long_pdfs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T12:40:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pekbzp</id>
    <title>No Uncensored Models?</title>
    <updated>2025-12-05T03:16:26+00:00</updated>
    <author>
      <name>/u/One_Spaceman</name>
      <uri>https://old.reddit.com/user/One_Spaceman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just downloaded Ollama, pulled Dolphin-Ollama3 from &lt;a href="https://www.youtube.com/watch?v=cTxENLLX1ho"&gt;THIS &lt;/a&gt;vid but its completely censored, is there anything that is totally uncensored? idk what to do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One_Spaceman"&gt; /u/One_Spaceman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T03:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfkede</id>
    <title>Y si Elon musk nos regalo MOE y nadie le puede dar las gracias?</title>
    <updated>2025-12-06T08:32:48+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all know who financed with 100 billion dollars the artificial intelligence from the beginning.&lt;br /&gt; We all know that Elon Musk complained that they were hiding from him the results that the AI was giving.&lt;br /&gt; Some of us think that out of envy of being the richest in the world, they want you only to put the money, but the other shareholders and companies did not want &amp;quot;the richest&amp;quot; to know about the advances.&lt;br /&gt; Elon Musk went out... he left voluntarily, right after leaving Musk, the AI explodes suddenly and becomes the most advanced on the planet.. What a coincidence?? While he contributed money the AI does not give results and as soon as I leave, the results appear.&lt;br /&gt; Then he would consider it a scam (They used him to finance it and keep it for themselves)&lt;br /&gt; What Musk financed according to my point of view was the MOE technology (The core of everything)&lt;br /&gt; If that were so, the only way to be able to compete with them, would be taking them to the starting square, and for that he should take away the MOE technology from them, and the best way would be talking with some Chinese engineers, so that they passed it to the Chinese for free, (or to the French of MIXTRAL) he would support them to do it in exchange for committing to release the technology to the world, that way he would put OPENAI to start from zero, and they would start to compete with GROK from zero.&lt;br /&gt; We DO NOT know for sure what has happened, but I know that no one gives away something that is worth billions, neither to the French nor to the Chinese, and I do not believe that they had the capacity to have developed something so valuable.... I believe that here something has happened that no one knows, and I believe that maybe, for having used and scammed Elon Musk, he decided that the technology that he paid out of his pocket was not going to stay with the competition if he wanted to compete with them, and that he looked for a way to give it away to the French of Mixtral or to the Chinese, in order to thus send them all to the starting square of the race and start competing with them with GROK from zero.&lt;br /&gt; The only way to put a company that has a secret worth billions to start from zero, is to release that secret, and that we all have it, that way it stops having value.&lt;br /&gt; I have the feeling that here all this conspiracy that everyone thought of technology thefts and sabotages, no one ever thought that it was the MOE technology, the one that made the race balance and everyone started to compete from the same starting square.&lt;br /&gt; But I have in my head stuck that something has to do with Elon Musk with us having this MOE technology (because think about it well) this is worth millions and they give it to us for free!!!!&lt;br /&gt; Who has paid for it? it was Elon Musk!! HE WAS THE ONE WHO MAINLY FINANCED OPEN AI AT THE BEGINNING TO DEVELOP THIS TECHNOLOGY!!!&lt;br /&gt; In the end the technology fell into our hands, not because he wanted it, but because they tried to scam deceive and use him out of envy of being the richest of all.&lt;br /&gt; How if this were true it cannot be known??? The entire OPENSOURCE community should thank ELON MUSK for what he has done, but since in theory it is something hypothetical that no one knows, we cannot give them...&lt;br /&gt; But I give them anyway, in case he was the one who gave us the MOE technology, because if you think about it well, it is what makes the difference between the AI, The MOE technology is what allows the AI to be used on a large scale.&lt;br /&gt; So nothing.. if hypothetical theory were true... Thanks Elon for being so brave!!!!&lt;/p&gt; &lt;p&gt;Todos sabemos quien financio con 100 mil millones de dolares la inteligencia artificial desde el principio.&lt;/p&gt; &lt;p&gt;Todos sabemos que Elon musk se quejo que le ocultaban los resultados que estaba dando la IA&lt;/p&gt; &lt;p&gt;Algunos pensamos que por envidia de ser el mas rico del mundo , te quieren solo para poner el dinero , pero los otros accionistas y empresas no querian que &amp;quot;el mas rico&amp;quot; supiese de los avances.&lt;/p&gt; &lt;p&gt;Elon musk se fue fuera...abandono voluntariamente , justo despues de abandonar Musk , la IA explota de repente y se vuelve lo mas avanzado en el planeta..Que coincidencia?? MIentras aporto dinero la IA no da resultados y en cuento me voy , aparecen los resultados.&lt;/p&gt; &lt;p&gt;Entonces el lo consideraria una estafa (Lo utilizaron para financiarla y quedarse ellos con ella)&lt;/p&gt; &lt;p&gt;Lo que musk financio segun mi punto de vista fue la tecnologia MOE (El nucleo de todo)&lt;/p&gt; &lt;p&gt;SI ello asi fuese , la unica forma de poder competir con ellos , seria llevandolos a ellos a la casilla de salida , y para ello deberia sacarles la tecnologia MOE , y la mejor forma seria hablando con algunos ingenieros chinos , para que se la pasaron a los chinos de forma gratuita ,(o a los franceses de MIXTRAL) el los apoyaria a hacerlo a cambio de comprometerse a librar la tecnologia al mundo , asi pondria a OPENAI a empezar desde cero , y empezarian a competir con GROK desde cero.&lt;/p&gt; &lt;p&gt;NO sabemos a ciencia cierta que ha pasado , pero yo se que nadie regala algo que vale miles de millones , ni a los franceses ni a los chinos , y no creo que ellos tuviesen la capacidad de haber desarrollado algo tan valioso....yo creo que aqui algo ha pasado que nadie sabe , y creo que a lo mejor , por haber utilizado y estafado a Elon musk , el decidio que la tecnologia que el pago de su bolsillo no se iba a quedar en la competencia si el queria competir con ellos , y que busco la forma de regalarsela a los Franceses de Mixtral o a los chinos , para poder asi mandarlos a todos a la casilla de salida de la carrera y empezar a competir con ellos con GROK desde cero.&lt;/p&gt; &lt;p&gt;La unica forma de poner auna empresa que tiene un secreto que vale miles de millones a empezar de cero , es liberar ese secreto , y que todos lo tengamos , asi deja de tener valor.&lt;/p&gt; &lt;p&gt;Yo tengo la sensacion de que aqui toda esta conspiracion que todo el mundo penso de robos de tecnologias y sabotajes , nunca nadie penso que fue la tecnologia MOE , la que hizo que la carrera se equilibrara y empezaran todos a competir desde la misma casilla de salida.&lt;/p&gt; &lt;p&gt;Pero yo tengo en la cabeza metido que algo tiene que ver Elon musk con nostros tener esta tecnologia MOE (porque pensadlo bien) esto vale millones y nos lo dan dado gratis!!!!&lt;/p&gt; &lt;p&gt;Quien lo ha pagado? fue elon musk!! EL FUE QUIEN PRINCIPALMENTE FINANCIA A OPEN AI AL PRINCIPIO PARA DESARROLLAR ESTA TECNOLOGIA!!!&lt;/p&gt; &lt;p&gt;Al final la tecnologia cayo en nuestras manos , no porque el quisiera , sino porque lo intentaron estafar engañar e utilizar por la envidia de ser el mas rico de todos.&lt;/p&gt; &lt;p&gt;Como esto si fuese cierto no se puede saber??? Toda la comunidad OPENSOURCE le deberia dar las gracias a ELON MUSK por lo que ha echo , pero como en teoria es algo hipotetico que nadie sabe , no se las podemos dar...&lt;/p&gt; &lt;p&gt;Pero yo se las doy igual , por si el fue quien nos regalo la tecnologia MOE , porque si lo pensais bien , es lo que marca la diferencia entre la IA , La tecnologia MOE es lo que permite que la IA pueda ser usada a gran escala.&lt;/p&gt; &lt;p&gt;Asi que nada..si hipotetica teoria fuese cierta...Gracias Elon por ser tan valiente!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfkede/y_si_elon_musk_nos_regalo_moe_y_nadie_le_puede/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfkede/y_si_elon_musk_nos_regalo_moe_y_nadie_le_puede/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfkede/y_si_elon_musk_nos_regalo_moe_y_nadie_le_puede/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T08:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexi8g</id>
    <title>Confused and unsure</title>
    <updated>2025-12-05T15:10:01+00:00</updated>
    <author>
      <name>/u/SirEblingMis</name>
      <uri>https://old.reddit.com/user/SirEblingMis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there.&lt;/p&gt; &lt;p&gt;I've seen lots of different rankings, but I haven't found a good concise resource that explains how I judge a model fitting onto 16gb vram or on 20-24gb M4 pro [mtx on LMstudio? or similar]&lt;/p&gt; &lt;p&gt;I'm genuinely just interested in a solid model to help administrative tasks as I do my Master's degree. I use Overleaf for it's great help with LaTex, and Perplexity for finding papers, teaching myself code or LaTex, etc.&lt;/p&gt; &lt;p&gt;But I want to run this stuff locally, especially since I may sometimes end up working with datasets that are confidential or secure.&lt;/p&gt; &lt;p&gt;Apologies if this post is a repeat or faux pas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SirEblingMis"&gt; /u/SirEblingMis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexi8g/confused_and_unsure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexi8g/confused_and_unsure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pexi8g/confused_and_unsure/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T15:10:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pev1eh</id>
    <title>I can't make Ministral 3 14B to work.</title>
    <updated>2025-12-05T13:27:40+00:00</updated>
    <author>
      <name>/u/No-Acanthisitta9773</name>
      <uri>https://old.reddit.com/user/No-Acanthisitta9773</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pev1eh/i_cant_make_ministral_3_14b_to_work/"&gt; &lt;img alt="I can't make Ministral 3 14B to work." src="https://preview.redd.it/usb606890e5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48964b9a072420216bf48a6c3636ad0d9b29b044" title="I can't make Ministral 3 14B to work." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Ministral-3-14B-Instruct-2512-Q5_K_M &lt;/p&gt; &lt;p&gt;I've tried different kind of modelfile, but it always responds the same nonsense. Ollama is up-to-date. Did you manage to make it work? What was your modelfile like? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Acanthisitta9773"&gt; /u/No-Acanthisitta9773 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/usb606890e5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pev1eh/i_cant_make_ministral_3_14b_to_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pev1eh/i_cant_make_ministral_3_14b_to_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T13:27:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf9grf</id>
    <title>Best model in the 8B range for RAG in 2025</title>
    <updated>2025-12-05T23:00:35+00:00</updated>
    <author>
      <name>/u/Hour-Entertainer-478</name>
      <uri>https://old.reddit.com/user/Hour-Entertainer-478</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hour-Entertainer-478"&gt; /u/Hour-Entertainer-478 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pf92li/best_model_in_the_8b_range_for_rag_in_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pf9grf/best_model_in_the_8b_range_for_rag_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pf9grf/best_model_in_the_8b_range_for_rag_in_2025/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T23:00:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pffgp1</id>
    <title>Noob here, looking for the perfect local LLM for my M3 Macbook Air 24GB RAM</title>
    <updated>2025-12-06T03:45:27+00:00</updated>
    <author>
      <name>/u/sylntnyte</name>
      <uri>https://old.reddit.com/user/sylntnyte</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sylntnyte"&gt; /u/sylntnyte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pff8b5/noob_here_looking_for_the_perfect_local_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pffgp1/noob_here_looking_for_the_perfect_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pffgp1/noob_here_looking_for_the_perfect_local_llm_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T03:45:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfo6zh</id>
    <title>Vllama: CLI based framework to run vision models in local or remote gpus(inspired from Ollama)</title>
    <updated>2025-12-06T12:30:17+00:00</updated>
    <author>
      <name>/u/Weekly_Layer_9315</name>
      <uri>https://old.reddit.com/user/Weekly_Layer_9315</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, this is my first post. I have built a simple CLI tool, which can help all to run the llms, vision models like image and video gen, models in the local system and if the system doesn't have the gpu or sufficient ram, they can also run it using kaggle's gpu(which is 30 hrs free for a week).&lt;/p&gt; &lt;p&gt;This is inspired from Ollama, which made downloading llms easy and interacting with it much easy, so I thought of why can't this be made for vision models, so I tried this first on my system, basic image generation is working but not that good, then I thought, why can't we use the Kaggle's GPU to generate videos and images and that can happen directly from the terminal with a single step, so that everyone can use this, so I built this VLLAMA.&lt;/p&gt; &lt;p&gt;In this, currently there are many features, like image, video generation in local and kaggles gpu session; download llms and make it run and also interact with it from anywhere (inspired by ollama) also improved it further by creating a vs code extension VLLAMA, using which you can chat directly from the vs code's chat section, users can chat with the local running llm with just adding &amp;quot;@vllama&amp;quot; at the start of the message and this doesn't use any usage cost and can be used as much as anyone wants, you can check this out at in the vscode extensions.&lt;/p&gt; &lt;p&gt;I want to implement this further so that the companies or anyone with gpu access can download the best llms for their usage and initialize it in their gpu servers, and can directly interact with it from the vscode's chat section and also in further versions, I am planning to implement agentic features so that users can use the local llm to use for code editing, in line suggestions, so that they don't have to pay for premiums and many more.&lt;/p&gt; &lt;p&gt;Currently it also has simple Text-to-Speech, and Speech-to-Text, which I am planning to include in the further versions, using open source audio models and also in further, implement 3D generation models, so that everyone can leverage the use of the open models directly from their terminal, and making the complex process of the using open models easy with just a single command in the terminal.&lt;/p&gt; &lt;p&gt;I have also implemented simple functionalities which can help, like listing the downloaded models and their sizes. Other things available are, basic dataset preprocessing, and training ML models directly with just two commands by just providing it the dataset. This is a basic implementation and want to further improve this so that users with just a dataset can clean and pre-process the data, train the models in their local or using the kaggle's or any free gpu providing services or their own gpus or cloud provided gpus, and can directly deploy the models and can use it any cases.&lt;/p&gt; &lt;p&gt;Currently this are the things it is doing and I want to improve such that everyone can use this for any case of the AI and leveraging the use of open models.&lt;/p&gt; &lt;p&gt;Please checkout the work at: &lt;a href="https://github.com/ManvithGopu13/Vllama"&gt;https://github.com/ManvithGopu13/Vllama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Published version at: &lt;a href="https://pypi.org/project/vllama/"&gt;https://pypi.org/project/vllama/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also the extension: &lt;a href="https://marketplace.visualstudio.com/items?itemName=ManvithGopu.vllama"&gt;https://marketplace.visualstudio.com/items?itemName=ManvithGopu.vllama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would appreciate your time for reading and thankful for everyone who want to contribute and spread a word of it.&lt;/p&gt; &lt;p&gt;Please leave your requests for improvements and any suggestions, ideas, and even roasts or anything in the comments or in the issues, this is well taken and appreciated. Thanks in advance. If you find the project useful, kindly contribute and can star it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weekly_Layer_9315"&gt; /u/Weekly_Layer_9315 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T12:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfj633</id>
    <title>Llm for log analysis</title>
    <updated>2025-12-06T07:14:35+00:00</updated>
    <author>
      <name>/u/gargento83</name>
      <uri>https://old.reddit.com/user/gargento83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is a good LLM model for security log analysis for cybersecurity?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gargento83"&gt; /u/gargento83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfj633/llm_for_log_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfj633/llm_for_log_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfj633/llm_for_log_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T07:14:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfibo6</id>
    <title>CocoIndex 0.3.1 - Open-Source Data Engine for Dynamic Context Engineering</title>
    <updated>2025-12-06T06:23:30+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I'm back with a new version of &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;CocoIndex&lt;/a&gt; (v0.3.1 - Ollama natively supported), with significant updates since last one. CocoIndex is ultra performant data transformation for AI &amp;amp; Dynamic Context Engineering - Simple to connect to source, and keep the target always fresh for all the heavy AI transformations (and any transformations) with incremental processing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Adaptive Batching&lt;/strong&gt;&lt;br /&gt; Supports automatic, knob-free batching across all functions. In our benchmarks with MiniLM, batching delivered ~5× higher throughput and ~80% lower runtime by amortizing GPU overhead with no manual tuning. I think particular if you have large AI workloads, this can help and is relevant to this sub-reddit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Custom Sources&lt;/strong&gt;&lt;br /&gt; With custom source connector, you can now use it to any external system — APIs, DBs, cloud storage, file systems, and more. CocoIndex handles incremental ingestion, change tracking, and schema alignment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Runtime &amp;amp; Reliability&lt;/strong&gt;&lt;br /&gt; Safer async execution and correct cancellation, Centralized HTTP utility with retries + clear errors, and many others.&lt;/p&gt; &lt;p&gt;You can find the full release notes here: &lt;a href="https://cocoindex.io/blogs/changelog-0310"&gt;https://cocoindex.io/blogs/changelog-0310&lt;/a&gt;&lt;br /&gt; Open source project here : &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;https://github.com/cocoindex-io/cocoindex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Btw, we are also on Github trending in Rust today :) it has Python SDK.&lt;/p&gt; &lt;p&gt;We have been growing so much with feedbacks from this community, thank you so much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfibo6/cocoindex_031_opensource_data_engine_for_dynamic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfibo6/cocoindex_031_opensource_data_engine_for_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfibo6/cocoindex_031_opensource_data_engine_for_dynamic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T06:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfiedc</id>
    <title>If it weren't for the Chinese we wouldn't have local AI</title>
    <updated>2025-12-06T06:28:03+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all know the disaster that happened when a Chinese engineer stole Open AI technology (Or so we have been told) with all the mess it caused when Deepseek brought its AI model to the Opensource world,&lt;/p&gt; &lt;p&gt;We all know that Open AI refused to make the best of their technology...then when MOE technology came out with Qwen and made the American stock market drop by trillions of dollars realizing that Nvidia chips might not be necessary for AI.&lt;/p&gt; &lt;p&gt;It shook the foundations of the American stock markets. We don't know exactly what happened... if Elon Musk had something to do with all this by leaving the company and they took advantage of his funding at the beginning... Maybe they hid from Elon Musk the true results that the AI ​​was giving so that he would continue funding them and he took revenge when he saw that there were no results when he left Open AI and just after Elon Musk left, Open AI took the big leap.&lt;/p&gt; &lt;p&gt;It was too much of a coincidence...Perhaps who really controlled Open AI (Microsoft), since Elon Musk was very busy with his problems in his companies, they took advantage of the opportunity to bet and benefit since they are now the majority shareholder...it is up to you to know if Elon Musk was bribing engineers to give him the MOE technology and since they refused he made a pact with them to give it to the Chinese...and by and by openai while they took out Grok al market...we will never know what happened in that soap opera...a whole story of conspiracies and strange hypotheses...&lt;/p&gt; &lt;p&gt;The strange thing is that Qwen seems to have acquired the technology that everyone believes is in the Open AI models...and we will never know because the models are closed and we don't know what engineering is really behind it. All this thinking... what if this whole story of technology theft had not happened... conspiracies... etc... stock market crash due to investors' fear that the AI ​​bubble would burst... end... if all that had not happened...&lt;/p&gt; &lt;p&gt;I believe that the Opensource community would not be enjoying the MOE models that we have today... since that led to fierce competition and the release of models to harm each other... what we have to see is that if the Open AI thing had not happened... I think they would not have released the GPT-OSS and I also think that they will never release a similar model again...&lt;/p&gt; &lt;p&gt;These companies are very greedy and want everything for themselves and if it were not for the pressure from the Chinese, the Opensource community would have many fewer capable models and we have to be grateful that QWEN AND DEEPSEEK and others put pressure by releasing their models...because the Westerners...if it were up to them...they would keep everything for themselves like Google does with its search engine...facebook or Amazon with its algorithms or Apple With its technologies...&lt;/p&gt; &lt;p&gt;Normally the Chinese have always been very closed...but the competition to see who is now the superpower that dominates the world has brought those blows that have been wonderful for the Opensource community...&lt;/p&gt; &lt;p&gt;The fear I have is that something like this will not be repeated and the models they release from now on will be pocket change... the crumbs... and the really good and the great things will be left to them and they will protect everything with patents... and a lot of security... you already saw the fall that OpenAi had to be founded non-profit for the benefit of Humanity...&lt;/p&gt; &lt;p&gt;It has become a business that moves billions with a highly leveraged debt...and they play tightrope walkers hanging by a thread...How does all this end...if everything explodes...or they continue releasing models...because investors do not invest in the past...they invest in the future...and if there is no news...the Westerners are going to be in trouble...something that the Chinese finance themselves and are cautious and looked for a way to make the business more profitable requiring less computing power...&lt;/p&gt; &lt;p&gt;In short, the Chinese and Asians have always excelled in many areas such as electronics... but now China is becoming a greater Superpower than the US in many areas and I think we should not underestimate or belittle them... OPEN AI is going to completely go bankrupt I think... and the Chinese are going to win this battle... I think it will be the beginning of China's hegemony over the world...&lt;/p&gt; &lt;p&gt;They are hard-working, efficient and peaceful people...so I really support their models and I think that thanks to them this competition to see who wins is benefiting the Opensource community in an unimaginable way that I never thought we could have such capable and useful models in our hands to work completely offline...and a lot of it is thanks to the Chinese!!!!! Maybe it's not thanks to the Chinese... but... to ELON MUSK. If I am wrong in my assumptions OPENAI I would be evolving behind all the other competitors...since they would be the best...although without Elon Musk's financing...everything changes.&lt;/p&gt; &lt;p&gt;Soon we will see what has happened here and what is happening...because Elon Musk is a brilliant and brave person...and a good person...and good people get cold...I think Microsoft thought that its move against Musk was going to turn out well...but soon we will see if OPEN AI goes bankrupt...&lt;/p&gt; &lt;p&gt;If my assumptions are true, Elon Musk would have given us all MOE technology completely free so that the Chinese AI could give a strong blow to OPENAI to stop them and enter the top 3 with GROK. If that were true we should thank him... but it couldn't be done because no one would know that all the high quality models that we enjoy offline are thanks to HIM. To the richest man on the planet who gave us this technology because some people wanted to deceive him!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T06:28:03+00:00</published>
  </entry>
</feed>
