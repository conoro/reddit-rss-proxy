<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-12-07T11:34:14+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1pe1dba</id>
    <title>Hito 1.7 GGUF release</title>
    <updated>2025-12-04T14:27:29+00:00</updated>
    <author>
      <name>/u/TastyWriting8360</name>
      <uri>https://old.reddit.com/user/TastyWriting8360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I will leave this here open wieghts &lt;a href="https://huggingface.co/hitonet/hito-1.7b"&gt;https://huggingface.co/hitonet/hito-1.7b&lt;/a&gt; and gguf release &lt;a href="https://huggingface.co/hitonet/hito-1.7b-GGUFI"&gt;https://huggingface.co/hitonet/hito-1.7b-GGUFI&lt;/a&gt; will make a proper post in the next few hours, gotta catch some sleep but cant wait to share this with you! &lt;strong&gt;Title:&lt;/strong&gt; [Release] Hito 1.7B: Can a tiny model learn to &amp;quot;doubt&amp;quot; itself? (Trained on only ~300 examples)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; We fine-tuned Qwen-1.7B on just ~300 high-quality &amp;quot;cognitive&amp;quot; examples. It learned to use XML tags like &lt;code&gt;&amp;lt;doubt&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;honest&amp;gt;&lt;/code&gt;, and &lt;code&gt;&amp;lt;verify&amp;gt;&lt;/code&gt; to catch its own mistakes. It solves the &amp;quot;Bat and Ball&amp;quot; riddle correctly where base Qwen fails. Weights are Apache 2.0.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üß™ The Experiment&lt;/h3&gt; &lt;p&gt;We wanted to answer a specific question: &lt;strong&gt;Can we distill complex cognitive architecture into a tiny model using minimal data?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most small models are confidently wrong. They hallucinate, make up facts, and never question themselves. We took our flagship model (Hito-Genius) and generated ~300 reasoning examples, then fine-tuned a &lt;strong&gt;Qwen3-1.7B&lt;/strong&gt; base model on them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Goal:&lt;/strong&gt; Teach the model to &amp;quot;think&amp;quot; before answering, specifically using a tree-structured reasoning process with emotional and logical checks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Result:&lt;/strong&gt; It actually works. The cognitive patterns transferred. It‚Äôs not GPT-4 level (obviously), but for a 1.7B model, the reasoning structure is fascinating.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üß† The &amp;quot;Bat and Ball&amp;quot; Test&lt;/h3&gt; &lt;p&gt;This is the classic cognitive bias test: &lt;em&gt;&amp;quot;A bat and a ball cost $1.10 together. The bat costs $1.00 more than the ball. How much does the ball cost?&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Most small models (and human intuition) say &lt;strong&gt;$0.10&lt;/strong&gt;. That is wrong.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hito 1.7B Output:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;xml &amp;lt;think&amp;gt; &amp;lt;understand&amp;gt;Ball + Bat = $1.10, Bat = Ball + $1.00&amp;lt;/understand&amp;gt; &amp;lt;doubt&amp;gt;Intuition says 10 cents... but let me verify.&amp;lt;/doubt&amp;gt; &amp;lt;logic&amp;gt; If ball = $0.10, bat = $1.10, total = $1.20. WRONG. Let ball = x: x + (x + 1) = 1.10, 2x = 0.10, x = 0.05 &amp;lt;/logic&amp;gt; &amp;lt;verify&amp;gt;Ball $0.05 + Bat $1.05 = $1.10 ‚úì&amp;lt;/verify&amp;gt; &amp;lt;/think&amp;gt; The ball costs five cents. &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note the &lt;code&gt;&amp;lt;doubt&amp;gt;&lt;/code&gt; tag explicitly triggering a re-evaluation of the intuition.&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üìä Quick Benchmarks (Size vs Performance)&lt;/h3&gt; &lt;p&gt;We tested against other small models on basic logic/math tasks.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Params&lt;/th&gt; &lt;th&gt;Bat &amp;amp; Ball Result&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Hito 1.7B&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;1.7B&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;$0.05 (Correct)&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3 1.7B (Base)&lt;/td&gt; &lt;td&gt;1.7B&lt;/td&gt; &lt;td&gt;$0.10 (Wrong)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GPT-4o-mini&lt;/td&gt; &lt;td&gt;~8B&lt;/td&gt; &lt;td&gt;$0.10 (Wrong)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Claude Haiku 4.5&lt;/td&gt; &lt;td&gt;~8B&lt;/td&gt; &lt;td&gt;$0.05 (Correct)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;hr /&gt; &lt;h3&gt;‚ö° How it Works (The Cognitive Tags)&lt;/h3&gt; &lt;p&gt;Instead of a hidden Chain-of-Thought, Hito uses transparent XML tags to organize its thinking:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;&amp;lt;doubt&amp;gt;&lt;/code&gt;: Forces the model to question its previous assumption.&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;honest&amp;gt;&lt;/code&gt;: Trained to admit when it doesn't know something or made an error.&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;limits&amp;gt;&lt;/code&gt;: Acknowledges gaps in its capabilities (e.g., &amp;quot;I process tokens, not characters&amp;quot;).&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;verify&amp;gt;&lt;/code&gt;: A step to double-check math or logic.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;üì• Download &amp;amp; Run&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;GGUF (for Ollama/LM Studio):&lt;/strong&gt; &lt;a href="https://huggingface.co/hitonet/hito-1.7b-GGUF"&gt;HuggingFace GGUF Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ollama Command:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;```bash wget &lt;a href="https://huggingface.co/hitonet/hito-1.7b-GGUF/resolve/main/hito-1.7b-Q4_K_M.gguf"&gt;https://huggingface.co/hitonet/hito-1.7b-GGUF/resolve/main/hito-1.7b-Q4_K_M.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;cat &amp;gt; Modelfile &amp;lt;&amp;lt; 'EOF' FROM hito-1.7b-Q4_K_M.gguf PARAMETER temperature 0.7 PARAMETER stop &amp;quot;&amp;lt;|im_end|&amp;gt;&amp;quot; EOF&lt;/p&gt; &lt;p&gt;ollama create hito -f Modelfile ollama run hito ```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Python/Transformers:&lt;/strong&gt; &lt;a href="https://huggingface.co/hitonet/hito-1.7b"&gt;HuggingFace Base Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;```python from transformers import AutoModelForCausalLM, AutoTokenizer&lt;/p&gt; &lt;p&gt;model = AutoModelForCausalLM.from_pretrained(&amp;quot;hitonet/hito-1.7b&amp;quot;) tokenizer = AutoTokenizer.from_pretrained(&amp;quot;hitonet/hito-1.7b&amp;quot;)&lt;/p&gt; &lt;p&gt;messages = [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;A bat and a ball cost $1.10...&amp;quot;}] inputs = tokenizer.apply_chat_template(messages, return_tensors=&amp;quot;pt&amp;quot;, add_generation_prompt=True) outputs = model.generate(inputs, max_new_tokens=512) print(tokenizer.decode(outputs[0])) ```&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üìù Licensing &amp;amp; Disclaimers&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Weights:&lt;/strong&gt; Apache 2.0 (Open for everyone).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Methodology:&lt;/strong&gt; The &amp;quot;Nested Cognitive Reasoning&amp;quot; (NCR) method itself is under a CC BY-NC-ND license if used commercially (check the repo for details).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Status:&lt;/strong&gt; Experimental. This is a Proof of Concept. Don't use it for medical advice or launching rockets.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Try the demo free (no login):&lt;/strong&gt; &lt;a href="https://chat.hitonet.com"&gt;chat.hitonet.com&lt;/a&gt; &lt;strong&gt;Full Research Paper:&lt;/strong&gt; &lt;a href="https://hitonet.com/research/nested-cognitive-reasoning"&gt;hitonet.com/research/nested-cognitive-reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We'd love to hear your feedback on how the &lt;code&gt;&amp;lt;doubt&amp;gt;&lt;/code&gt; tags handle other logic puzzles!ur feedback on how the `&amp;lt;doubt&amp;gt;` tags handle other logic puzzles!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TastyWriting8360"&gt; /u/TastyWriting8360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe1dba/hito_17_gguf_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe1dba/hito_17_gguf_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe1dba/hito_17_gguf_release/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T14:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe5b1t</id>
    <title>Computer Use with Claude Opus 4.5</title>
    <updated>2025-12-04T17:01:11+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pe5b1t/computer_use_with_claude_opus_45/"&gt; &lt;img alt="Computer Use with Claude Opus 4.5" src="https://external-preview.redd.it/eWc1Z2J3ZG94NzVnMY644GvbBlUugVuvecrsXUzyNOCzfLoiwHL0f-lwE9G6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64ec98da6d0fbfaba8c88013ccf90ed592f5e91c" title="Computer Use with Claude Opus 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude Opus 4.5 support to the Cua VLM Router and Playground - and you can already see it running inside Windows sandboxes. Early results are seriously impressive, even on tricky desktop workflows.&lt;/p&gt; &lt;p&gt;Benchmark results:&lt;/p&gt; &lt;p&gt;-new SOTA 66.3% on OSWorld (beats Sonnet 4.5‚Äôs 61.4% in the general model category)&lt;/p&gt; &lt;p&gt;-88.9% on tool-use&lt;/p&gt; &lt;p&gt;Better reasoning. More reliable multi-step execution.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try the playground here : &lt;a href="https://cua.ai"&gt;https://cua.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9q65hzoox75g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe5b1t/computer_use_with_claude_opus_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe5b1t/computer_use_with_claude_opus_45/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T17:01:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe0s1q</id>
    <title>New Feature in RAGLight: Multimodal PDF Ingestion</title>
    <updated>2025-12-04T14:02:16+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"&gt; &lt;img alt="New Feature in RAGLight: Multimodal PDF Ingestion" src="https://b.thumbs.redditmedia.com/UkmGLN7LsRnojkAulEBihmGFp1HrsIPIMO8GS0KQ9ko.jpg" title="New Feature in RAGLight: Multimodal PDF Ingestion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I just added a small but powerful feature to &lt;strong&gt;RAGLight&lt;/strong&gt;: you can now override any document processor, and this unlocks &lt;strong&gt;a new built-in example : a VLM-powered PDF parser.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Find repo here :&lt;/strong&gt; &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;https://github.com/Bessouat40/RAGLight&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try this new feature with many LLM provider such as &lt;strong&gt;Ollama&lt;/strong&gt; !&lt;/p&gt; &lt;h1&gt;What it does&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Extracts &lt;strong&gt;text AND images&lt;/strong&gt; from PDFs&lt;/li&gt; &lt;li&gt;Sends images to a &lt;strong&gt;Vision-Language Model&lt;/strong&gt; (Mistral, OpenAI, etc.)&lt;/li&gt; &lt;li&gt;Captions them and injects the result into your vector store&lt;/li&gt; &lt;li&gt;Makes RAG truly understand diagrams, block schemas, charts, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Super helpful for technical documentation, research papers, engineering PDFs‚Ä¶&lt;/p&gt; &lt;h1&gt;Minimal Example&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/di41m8op175g1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d01198e5a8bf6c0b0c64f46e9b8fe176debc2707"&gt;https://preview.redd.it/di41m8op175g1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d01198e5a8bf6c0b0c64f46e9b8fe176debc2707&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;p&gt;Most RAG tools ignore images entirely. Now RAGLight can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;interpret diagrams&lt;/li&gt; &lt;li&gt;index visual content&lt;/li&gt; &lt;li&gt;retrieve multimodal meaning&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe0s1q/new_feature_in_raglight_multimodal_pdf_ingestion/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T14:02:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdui4a</id>
    <title>Pipeshub just hit 2k GitHub stars.</title>
    <updated>2025-12-04T08:08:14+00:00</updated>
    <author>
      <name>/u/Inevitable-Letter385</name>
      <uri>https://old.reddit.com/user/Inevitable-Letter385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôre super excited to share a milestone that wouldn‚Äôt have been possible without this community. &lt;strong&gt;PipesHub just crossed 2,000 GitHub stars!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thank you to everyone who tried it out, shared feedback, opened issues, or even just followed the project.&lt;/p&gt; &lt;p&gt;For those who haven‚Äôt heard of it yet, &lt;strong&gt;PipesHub&lt;/strong&gt; is a fully open-source enterprise search platform we‚Äôve been building over the past few months. Our goal is simple: bring powerful &lt;strong&gt;Enterprise Search&lt;/strong&gt; and &lt;strong&gt;Agent Builders&lt;/strong&gt; to every team, without vendor lock-in. PipesHub brings all your business data together and makes it instantly searchable.&lt;/p&gt; &lt;p&gt;It integrates with tools like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, and even local files. You can deploy it with a single Docker Compose command.&lt;/p&gt; &lt;p&gt;Under the hood, PipesHub runs on a &lt;strong&gt;Kafka powered event streaming architecture&lt;/strong&gt;, giving it real time, scalable, fault tolerant indexing. It combines a vector database with a knowledge graph and uses &lt;strong&gt;Agentic RAG&lt;/strong&gt; to keep responses grounded in source of truth. You get visual citations, reasoning, and confidence scores, and if information isn‚Äôt found, it simply says so instead of hallucinating.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enterprise knowledge graph for deep understanding of users, orgs, and teams&lt;/li&gt; &lt;li&gt;Connect to any AI model: OpenAI, Gemini, Claude, Ollama, or any OpenAI compatible endpoint&lt;/li&gt; &lt;li&gt;Vision Language Models and OCR for images and scanned documents&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, and SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs&lt;/li&gt; &lt;li&gt;Support for all major file types, including PDFs with images and diagrams&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Builder&lt;/strong&gt; for actions like sending emails, scheduling meetings, deep research, internet search, and more&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning Agent&lt;/strong&gt; with planning capabilities&lt;/li&gt; &lt;li&gt;&lt;strong&gt;40+ connectors&lt;/strong&gt; for integrating with your business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôd love for you to check it out and share your thoughts or feedback. It truly helps guide the roadmap:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1pdudws"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Letter385"&gt; /u/Inevitable-Letter385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pdui4a/pipeshub_just_hit_2k_github_stars/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T08:08:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe18c7</id>
    <title>How does Ollama truncate the context when it's too long?</title>
    <updated>2025-12-04T14:21:49+00:00</updated>
    <author>
      <name>/u/AmphibianFrog</name>
      <uri>https://old.reddit.com/user/AmphibianFrog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to understand how prompts are truncated then they exceed the context limit. Let's say I have the num_ctx parameter set to 1000 tokens. I then send in a system prompt, and a whole bunch of user and assistant messages, but in total there are 2000 tokens in the context. What happens?&lt;/p&gt; &lt;p&gt;Specific questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does truncation happen per message or do partial messages get fed into the LLM?&lt;/li&gt; &lt;li&gt;Does the system prompt get truncated? Or is it always passed in even if other messages are removed from the input?&lt;/li&gt; &lt;li&gt;Do the messages get removed in pairs, i.e will it always make sure a user message is first in the prompt?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;My conclusion is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The system prompt is never truncated&lt;/li&gt; &lt;li&gt;Entire messages are dropped from the context, not individual tokens&lt;/li&gt; &lt;li&gt;Ollama will include as many messages as possible, getting rid of the oldest messages first (apart from the system prompt which is always included)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hopefully this helps someone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AmphibianFrog"&gt; /u/AmphibianFrog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T14:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe2xbe</id>
    <title>how close to codex or claude code can you get with ollama and a 4090?</title>
    <updated>2025-12-04T15:30:22+00:00</updated>
    <author>
      <name>/u/reelznfeelz</name>
      <uri>https://old.reddit.com/user/reelznfeelz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to run an agentic code assistant that can use tools on top of ollama without having to spend a whole ton of time on building it yourself? &lt;/p&gt; &lt;p&gt;Somebody here has probably followed or played in that space. &lt;/p&gt; &lt;p&gt;Just thinking/planning for the day when openAI/Claude has to charge what's actually needed to be profitable, which will be like $900/mo for the kind of usage I have. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reelznfeelz"&gt; /u/reelznfeelz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pe2xbe/how_close_to_codex_or_claude_code_can_you_get/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-04T15:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexhan</id>
    <title>Es Qwen3 el mejor modelo de IA del mundo en general en este momento? Mienten los benchmarks?</title>
    <updated>2025-12-05T15:08:59+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Algunos no nos fiamos de los Benchmarks..nos fiamos mas de lo que vemos por nosotros mismos‚Ä¶&lt;/p&gt; &lt;p&gt;Vosotros que pensais?&lt;/p&gt; &lt;p&gt;Es Qwen el mejor optimizado y el mas preciso mas inteligente y que mejor resultados da en matematicas fisica y programacion ? Siendo un modelo general razonablemente peque√±o en comparacion con otros‚Ä¶aunque no he probado el nuevo modelo 3.2 exp de deepseek pero yo con el qwen3 estoy muy muy contento&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexhan/es_qwen3_el_mejor_modelo_de_ia_del_mundo_en/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexhan/es_qwen3_el_mejor_modelo_de_ia_del_mundo_en/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pexhan/es_qwen3_el_mejor_modelo_de_ia_del_mundo_en/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T15:08:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1petnr2</id>
    <title>Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices</title>
    <updated>2025-12-05T12:20:16+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1petnr2/translating_webpages_using_local_model_on_ipad/"&gt; &lt;img alt="Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices" src="https://external-preview.redd.it/dmJremhrcWZvZDVnMVvVURK3C0St0olAiXhHNsqHlBUDFrodjuu0gc-gowgU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=471e58ef80ba2c5d0ce953debe5fc9808e05745f" title="Translating WEBPAGES using LOCAL MODEL on IPAD - pushing what is possible on mobile devices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Browsers comes with translation tool - but few of them provide legible translations. We are not used to the high quality translation provided by LLMs, and we expect the same experience with webpages translation when browsing.&lt;/p&gt; &lt;p&gt;I am pleased to announce that Vector Space now integrates Webpage translation. Featuring:&lt;/p&gt; &lt;p&gt;- Use a LLM instead of translation APIs&lt;/p&gt; &lt;p&gt;- Works on mobile&lt;/p&gt; &lt;p&gt;- Call local models for unlimited and private, translation&lt;/p&gt; &lt;p&gt;- Perserve HTML structures and visuals&lt;/p&gt; &lt;p&gt;- Connect to OpenAI API for faster transaction (enter your API in the settings)&lt;/p&gt; &lt;p&gt;Result is some very nice translations! Please see the video. It is filmed on a M1 iPad.&lt;/p&gt; &lt;p&gt;Try it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://short.yomer.jp/vector-space"&gt;https://short.yomer.jp/vector-space&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Limitations and next directions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Right now a relative large model (~4B) is needed for preserving HTML tags and improving translation quality. I believe a fine tuned model of a much smaller size can do the trick. With enough people supporting me I can work on it to increase translation speed at least 10x.&lt;/li&gt; &lt;li&gt;Due to Apple restriction on running GPU work in the background, currently only iPad multi tasking is supported on iOS. I believe this is solvable by either looking at Background Tasks framework or move to neural engine.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i3d55jqfod5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1petnr2/translating_webpages_using_local_model_on_ipad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1petnr2/translating_webpages_using_local_model_on_ipad/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T12:20:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1peu215</id>
    <title>A better way to share and talk to long PDFs</title>
    <updated>2025-12-05T12:40:44+00:00</updated>
    <author>
      <name>/u/simplext</name>
      <uri>https://old.reddit.com/user/simplext</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1peu215/a_better_way_to_share_and_talk_to_long_pdfs/"&gt; &lt;img alt="A better way to share and talk to long PDFs" src="https://external-preview.redd.it/OGpvMjQ4ZmNyZDVnMbGTwPFoXGKKHKurfth5fGD5PZl_uCHNFDWTG1GTjAFD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0adbc635fe108608aa908b6ae8ac3dae15871e9" title="A better way to share and talk to long PDFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Imagine going through the long and boring jobs data PDF shared by the BLS. What if they had shared a presentation that you could talk to ? &lt;/p&gt; &lt;p&gt;Visual Book allows you to break down an PDF into slides and then share it with people so they can talk to it.&lt;/p&gt; &lt;p&gt;Visual Book: &lt;a href="https://www.visualbook.app"&gt;https://www.visualbook.app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simplext"&gt; /u/simplext &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3vqnevecrd5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1peu215/a_better_way_to_share_and_talk_to_long_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1peu215/a_better_way_to_share_and_talk_to_long_pdfs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T12:40:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf07j7</id>
    <title>Looking for something to manage API usage</title>
    <updated>2025-12-05T16:54:07+00:00</updated>
    <author>
      <name>/u/tecneeq</name>
      <uri>https://old.reddit.com/user/tecneeq</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for something that allows me to give different amounts of tokens per day or week to different users for different models. Basically some kind of rate limiting.&lt;/p&gt; &lt;p&gt;Does anyone know something like that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tecneeq"&gt; /u/tecneeq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pf07j7/looking_for_something_to_manage_api_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pf07j7/looking_for_something_to_manage_api_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pf07j7/looking_for_something_to_manage_api_usage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T16:54:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pekbzp</id>
    <title>No Uncensored Models?</title>
    <updated>2025-12-05T03:16:26+00:00</updated>
    <author>
      <name>/u/One_Spaceman</name>
      <uri>https://old.reddit.com/user/One_Spaceman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just downloaded Ollama, pulled Dolphin-Ollama3 from &lt;a href="https://www.youtube.com/watch?v=cTxENLLX1ho"&gt;THIS &lt;/a&gt;vid but its completely censored, is there anything that is totally uncensored? idk what to do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One_Spaceman"&gt; /u/One_Spaceman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pekbzp/no_uncensored_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T03:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfkede</id>
    <title>Y si Elon musk nos regalo MOE y nadie le puede dar las gracias?</title>
    <updated>2025-12-06T08:32:48+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all know who financed with 100 billion dollars the artificial intelligence from the beginning.&lt;br /&gt; We all know that Elon Musk complained that they were hiding from him the results that the AI was giving.&lt;br /&gt; Some of us think that out of envy of being the richest in the world, they want you only to put the money, but the other shareholders and companies did not want &amp;quot;the richest&amp;quot; to know about the advances.&lt;br /&gt; Elon Musk went out... he left voluntarily, right after leaving Musk, the AI explodes suddenly and becomes the most advanced on the planet.. What a coincidence?? While he contributed money the AI does not give results and as soon as I leave, the results appear.&lt;br /&gt; Then he would consider it a scam (They used him to finance it and keep it for themselves)&lt;br /&gt; What Musk financed according to my point of view was the MOE technology (The core of everything)&lt;br /&gt; If that were so, the only way to be able to compete with them, would be taking them to the starting square, and for that he should take away the MOE technology from them, and the best way would be talking with some Chinese engineers, so that they passed it to the Chinese for free, (or to the French of MIXTRAL) he would support them to do it in exchange for committing to release the technology to the world, that way he would put OPENAI to start from zero, and they would start to compete with GROK from zero.&lt;br /&gt; We DO NOT know for sure what has happened, but I know that no one gives away something that is worth billions, neither to the French nor to the Chinese, and I do not believe that they had the capacity to have developed something so valuable.... I believe that here something has happened that no one knows, and I believe that maybe, for having used and scammed Elon Musk, he decided that the technology that he paid out of his pocket was not going to stay with the competition if he wanted to compete with them, and that he looked for a way to give it away to the French of Mixtral or to the Chinese, in order to thus send them all to the starting square of the race and start competing with them with GROK from zero.&lt;br /&gt; The only way to put a company that has a secret worth billions to start from zero, is to release that secret, and that we all have it, that way it stops having value.&lt;br /&gt; I have the feeling that here all this conspiracy that everyone thought of technology thefts and sabotages, no one ever thought that it was the MOE technology, the one that made the race balance and everyone started to compete from the same starting square.&lt;br /&gt; But I have in my head stuck that something has to do with Elon Musk with us having this MOE technology (because think about it well) this is worth millions and they give it to us for free!!!!&lt;br /&gt; Who has paid for it? it was Elon Musk!! HE WAS THE ONE WHO MAINLY FINANCED OPEN AI AT THE BEGINNING TO DEVELOP THIS TECHNOLOGY!!!&lt;br /&gt; In the end the technology fell into our hands, not because he wanted it, but because they tried to scam deceive and use him out of envy of being the richest of all.&lt;br /&gt; How if this were true it cannot be known??? The entire OPENSOURCE community should thank ELON MUSK for what he has done, but since in theory it is something hypothetical that no one knows, we cannot give them...&lt;br /&gt; But I give them anyway, in case he was the one who gave us the MOE technology, because if you think about it well, it is what makes the difference between the AI, The MOE technology is what allows the AI to be used on a large scale.&lt;br /&gt; So nothing.. if hypothetical theory were true... Thanks Elon for being so brave!!!!&lt;/p&gt; &lt;p&gt;Todos sabemos quien financio con 100 mil millones de dolares la inteligencia artificial desde el principio.&lt;/p&gt; &lt;p&gt;Todos sabemos que Elon musk se quejo que le ocultaban los resultados que estaba dando la IA&lt;/p&gt; &lt;p&gt;Algunos pensamos que por envidia de ser el mas rico del mundo , te quieren solo para poner el dinero , pero los otros accionistas y empresas no querian que &amp;quot;el mas rico&amp;quot; supiese de los avances.&lt;/p&gt; &lt;p&gt;Elon musk se fue fuera...abandono voluntariamente , justo despues de abandonar Musk , la IA explota de repente y se vuelve lo mas avanzado en el planeta..Que coincidencia?? MIentras aporto dinero la IA no da resultados y en cuento me voy , aparecen los resultados.&lt;/p&gt; &lt;p&gt;Entonces el lo consideraria una estafa (Lo utilizaron para financiarla y quedarse ellos con ella)&lt;/p&gt; &lt;p&gt;Lo que musk financio segun mi punto de vista fue la tecnologia MOE (El nucleo de todo)&lt;/p&gt; &lt;p&gt;SI ello asi fuese , la unica forma de poder competir con ellos , seria llevandolos a ellos a la casilla de salida , y para ello deberia sacarles la tecnologia MOE , y la mejor forma seria hablando con algunos ingenieros chinos , para que se la pasaron a los chinos de forma gratuita ,(o a los franceses de MIXTRAL) el los apoyaria a hacerlo a cambio de comprometerse a librar la tecnologia al mundo , asi pondria a OPENAI a empezar desde cero , y empezarian a competir con GROK desde cero.&lt;/p&gt; &lt;p&gt;NO sabemos a ciencia cierta que ha pasado , pero yo se que nadie regala algo que vale miles de millones , ni a los franceses ni a los chinos , y no creo que ellos tuviesen la capacidad de haber desarrollado algo tan valioso....yo creo que aqui algo ha pasado que nadie sabe , y creo que a lo mejor , por haber utilizado y estafado a Elon musk , el decidio que la tecnologia que el pago de su bolsillo no se iba a quedar en la competencia si el queria competir con ellos , y que busco la forma de regalarsela a los Franceses de Mixtral o a los chinos , para poder asi mandarlos a todos a la casilla de salida de la carrera y empezar a competir con ellos con GROK desde cero.&lt;/p&gt; &lt;p&gt;La unica forma de poner auna empresa que tiene un secreto que vale miles de millones a empezar de cero , es liberar ese secreto , y que todos lo tengamos , asi deja de tener valor.&lt;/p&gt; &lt;p&gt;Yo tengo la sensacion de que aqui toda esta conspiracion que todo el mundo penso de robos de tecnologias y sabotajes , nunca nadie penso que fue la tecnologia MOE , la que hizo que la carrera se equilibrara y empezaran todos a competir desde la misma casilla de salida.&lt;/p&gt; &lt;p&gt;Pero yo tengo en la cabeza metido que algo tiene que ver Elon musk con nostros tener esta tecnologia MOE (porque pensadlo bien) esto vale millones y nos lo dan dado gratis!!!!&lt;/p&gt; &lt;p&gt;Quien lo ha pagado? fue elon musk!! EL FUE QUIEN PRINCIPALMENTE FINANCIA A OPEN AI AL PRINCIPIO PARA DESARROLLAR ESTA TECNOLOGIA!!!&lt;/p&gt; &lt;p&gt;Al final la tecnologia cayo en nuestras manos , no porque el quisiera , sino porque lo intentaron estafar enga√±ar e utilizar por la envidia de ser el mas rico de todos.&lt;/p&gt; &lt;p&gt;Como esto si fuese cierto no se puede saber??? Toda la comunidad OPENSOURCE le deberia dar las gracias a ELON MUSK por lo que ha echo , pero como en teoria es algo hipotetico que nadie sabe , no se las podemos dar...&lt;/p&gt; &lt;p&gt;Pero yo se las doy igual , por si el fue quien nos regalo la tecnologia MOE , porque si lo pensais bien , es lo que marca la diferencia entre la IA , La tecnologia MOE es lo que permite que la IA pueda ser usada a gran escala.&lt;/p&gt; &lt;p&gt;Asi que nada..si hipotetica teoria fuese cierta...Gracias Elon por ser tan valiente!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfkede/y_si_elon_musk_nos_regalo_moe_y_nadie_le_puede/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfkede/y_si_elon_musk_nos_regalo_moe_y_nadie_le_puede/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfkede/y_si_elon_musk_nos_regalo_moe_y_nadie_le_puede/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T08:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexi8g</id>
    <title>Confused and unsure</title>
    <updated>2025-12-05T15:10:01+00:00</updated>
    <author>
      <name>/u/SirEblingMis</name>
      <uri>https://old.reddit.com/user/SirEblingMis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there.&lt;/p&gt; &lt;p&gt;I've seen lots of different rankings, but I haven't found a good concise resource that explains how I judge a model fitting onto 16gb vram or on 20-24gb M4 pro [mtx on LMstudio? or similar]&lt;/p&gt; &lt;p&gt;I'm genuinely just interested in a solid model to help administrative tasks as I do my Master's degree. I use Overleaf for it's great help with LaTex, and Perplexity for finding papers, teaching myself code or LaTex, etc.&lt;/p&gt; &lt;p&gt;But I want to run this stuff locally, especially since I may sometimes end up working with datasets that are confidential or secure.&lt;/p&gt; &lt;p&gt;Apologies if this post is a repeat or faux pas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SirEblingMis"&gt; /u/SirEblingMis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexi8g/confused_and_unsure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pexi8g/confused_and_unsure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pexi8g/confused_and_unsure/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T15:10:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pev1eh</id>
    <title>I can't make Ministral 3 14B to work.</title>
    <updated>2025-12-05T13:27:40+00:00</updated>
    <author>
      <name>/u/No-Acanthisitta9773</name>
      <uri>https://old.reddit.com/user/No-Acanthisitta9773</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1pev1eh/i_cant_make_ministral_3_14b_to_work/"&gt; &lt;img alt="I can't make Ministral 3 14B to work." src="https://preview.redd.it/usb606890e5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48964b9a072420216bf48a6c3636ad0d9b29b044" title="I can't make Ministral 3 14B to work." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Ministral-3-14B-Instruct-2512-Q5_K_M &lt;/p&gt; &lt;p&gt;I've tried different kind of modelfile, but it always responds the same nonsense. Ollama is up-to-date. Did you manage to make it work? What was your modelfile like? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Acanthisitta9773"&gt; /u/No-Acanthisitta9773 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/usb606890e5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pev1eh/i_cant_make_ministral_3_14b_to_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pev1eh/i_cant_make_ministral_3_14b_to_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T13:27:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf9grf</id>
    <title>Best model in the 8B range for RAG in 2025</title>
    <updated>2025-12-05T23:00:35+00:00</updated>
    <author>
      <name>/u/Hour-Entertainer-478</name>
      <uri>https://old.reddit.com/user/Hour-Entertainer-478</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hour-Entertainer-478"&gt; /u/Hour-Entertainer-478 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pf92li/best_model_in_the_8b_range_for_rag_in_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pf9grf/best_model_in_the_8b_range_for_rag_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pf9grf/best_model_in_the_8b_range_for_rag_in_2025/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-05T23:00:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pffgp1</id>
    <title>Noob here, looking for the perfect local LLM for my M3 Macbook Air 24GB RAM</title>
    <updated>2025-12-06T03:45:27+00:00</updated>
    <author>
      <name>/u/sylntnyte</name>
      <uri>https://old.reddit.com/user/sylntnyte</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sylntnyte"&gt; /u/sylntnyte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1pff8b5/noob_here_looking_for_the_perfect_local_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pffgp1/noob_here_looking_for_the_perfect_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pffgp1/noob_here_looking_for_the_perfect_local_llm_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T03:45:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfj633</id>
    <title>Llm for log analysis</title>
    <updated>2025-12-06T07:14:35+00:00</updated>
    <author>
      <name>/u/gargento83</name>
      <uri>https://old.reddit.com/user/gargento83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is a good LLM model for security log analysis for cybersecurity?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gargento83"&gt; /u/gargento83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfj633/llm_for_log_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfj633/llm_for_log_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfj633/llm_for_log_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T07:14:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfr31a</id>
    <title>NornicDB - V1 MemoryOS for LLMs - MIT</title>
    <updated>2025-12-06T14:50:57+00:00</updated>
    <author>
      <name>/u/Dense_Gate_5193</name>
      <uri>https://old.reddit.com/user/Dense_Gate_5193</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense_Gate_5193"&gt; /u/Dense_Gate_5193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1pfr2tj/nornicdb_v1_memoryos_for_llms_mit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfr31a/nornicdb_v1_memoryos_for_llms_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfr31a/nornicdb_v1_memoryos_for_llms_mit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T14:50:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfibo6</id>
    <title>CocoIndex 0.3.1 - Open-Source Data Engine for Dynamic Context Engineering</title>
    <updated>2025-12-06T06:23:30+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I'm back with a new version of &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;CocoIndex&lt;/a&gt; (v0.3.1 - Ollama natively supported), with significant updates since last one. CocoIndex is ultra performant data transformation for AI &amp;amp; Dynamic Context Engineering - Simple to connect to source, and keep the target always fresh for all the heavy AI transformations (and any transformations) with incremental processing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Adaptive Batching&lt;/strong&gt;&lt;br /&gt; Supports automatic, knob-free batching across all functions. In our benchmarks with MiniLM, batching delivered ~5√ó higher throughput and ~80% lower runtime by amortizing GPU overhead with no manual tuning. I think particular if you have large AI workloads, this can help and is relevant to this sub-reddit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Custom Sources&lt;/strong&gt;&lt;br /&gt; With custom source connector, you can now use it to any external system ‚Äî APIs, DBs, cloud storage, file systems, and more. CocoIndex handles incremental ingestion, change tracking, and schema alignment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Runtime &amp;amp; Reliability&lt;/strong&gt;&lt;br /&gt; Safer async execution and correct cancellation, Centralized HTTP utility with retries + clear errors, and many others.&lt;/p&gt; &lt;p&gt;You can find the full release notes here: &lt;a href="https://cocoindex.io/blogs/changelog-0310"&gt;https://cocoindex.io/blogs/changelog-0310&lt;/a&gt;&lt;br /&gt; Open source project here : &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;https://github.com/cocoindex-io/cocoindex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Btw, we are also on Github trending in Rust today :) it has Python SDK.&lt;/p&gt; &lt;p&gt;We have been growing so much with feedbacks from this community, thank you so much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfibo6/cocoindex_031_opensource_data_engine_for_dynamic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfibo6/cocoindex_031_opensource_data_engine_for_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfibo6/cocoindex_031_opensource_data_engine_for_dynamic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T06:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg03g5</id>
    <title>Fara-7b on ollama?</title>
    <updated>2025-12-06T21:09:52+00:00</updated>
    <author>
      <name>/u/tracagnotto</name>
      <uri>https://old.reddit.com/user/tracagnotto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; i was trying running Fara-7B from microsoft, they require vllm and it's a nightmare.&lt;/p&gt; &lt;p&gt;I found that it's usable on ollama but I don't really understand how. Ollama is chat, and that model needs to do continuous agentic work. In fact my test was:&lt;br /&gt; ME:&lt;br /&gt; [URL OF SOME BLOG]&lt;/p&gt; &lt;p&gt;what's this blog about&lt;/p&gt; &lt;p&gt;OLLAMA/FARA:&lt;/p&gt; &lt;p&gt;This is a screenshot of the latest news page on Matthew Leer-Bobinson's blog. The URL (url of the blog) indicates it's the second page of the latest news posts. &amp;lt;tool\_call&amp;gt; {&amp;quot;name&amp;quot;: &amp;quot;olution&amp;quot;, &amp;quot;arguments&amp;quot;: {&amp;quot;t&amp;quot;: &amp;quot;info&amp;quot;}} &amp;lt;/tool\_call&amp;gt;&lt;/p&gt; &lt;p&gt;ME:&lt;br /&gt; Examine content pls&lt;/p&gt; &lt;p&gt;OLLAMA/FARA:&lt;br /&gt; I‚Äôll click on the link to open the latest-news page and examine its content. &amp;lt;tool\_call&amp;gt; {&amp;quot;name&amp;quot;: &amp;quot;olution&amp;quot;, &amp;quot;arguments&amp;quot;: {&amp;quot;t&amp;quot;: &amp;quot;left_click&amp;quot;, &amp;quot;coordinate&amp;quot;: [100, 10]}} &amp;lt;/tool\_call&amp;gt;&lt;/p&gt; &lt;p&gt;what am I missing?&lt;br /&gt; Is ollama only a way to run it and then you have to build on it a whole MCP/Agentic stack that takes those result and make actions? lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tracagnotto"&gt; /u/tracagnotto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg03g5/fara7b_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg03g5/fara7b_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pg03g5/fara7b_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T21:09:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg213d</id>
    <title>Usable models and Performance of RTX 2000 Ada 16GB or RTX 4000 20GB?</title>
    <updated>2025-12-06T22:34:06+00:00</updated>
    <author>
      <name>/u/Temporary_Sir</name>
      <uri>https://old.reddit.com/user/Temporary_Sir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm considering picking up an RTX2000 or RTX Pro 4000 card to add to a server. (Ideally directly powered via pci)&lt;/p&gt; &lt;p&gt;Any insights on what performance would be for general usage as well as a bit of coding? &lt;/p&gt; &lt;p&gt;What models would be recommended? Would those even be useful?&lt;/p&gt; &lt;p&gt;Looking forward to your replies&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Sir"&gt; /u/Temporary_Sir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg213d/usable_models_and_performance_of_rtx_2000_ada/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg213d/usable_models_and_performance_of_rtx_2000_ada/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pg213d/usable_models_and_performance_of_rtx_2000_ada/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T22:34:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfo6zh</id>
    <title>Vllama: CLI based framework to run vision models in local or remote gpus(inspired from Ollama)</title>
    <updated>2025-12-06T12:30:17+00:00</updated>
    <author>
      <name>/u/Weekly_Layer_9315</name>
      <uri>https://old.reddit.com/user/Weekly_Layer_9315</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, this is my first post. I have built a simple CLI tool, which can help all to run the llms, vision models like image and video gen, models in the local system and if the system doesn't have the gpu or sufficient ram, they can also run it using kaggle's gpu(which is 30 hrs free for a week).&lt;/p&gt; &lt;p&gt;This is inspired from Ollama, which made downloading llms easy and interacting with it much easy, so I thought of why can't this be made for vision models, so I tried this first on my system, basic image generation is working but not that good, then I thought, why can't we use the Kaggle's GPU to generate videos and images and that can happen directly from the terminal with a single step, so that everyone can use this, so I built this VLLAMA.&lt;/p&gt; &lt;p&gt;In this, currently there are many features, like image, video generation in local and kaggles gpu session; download llms and make it run and also interact with it from anywhere (inspired by ollama) also improved it further by creating a vs code extension VLLAMA, using which you can chat directly from the vs code's chat section, users can chat with the local running llm with just adding &amp;quot;@vllama&amp;quot; at the start of the message and this doesn't use any usage cost and can be used as much as anyone wants, you can check this out at in the vscode extensions.&lt;/p&gt; &lt;p&gt;I want to implement this further so that the companies or anyone with gpu access can download the best llms for their usage and initialize it in their gpu servers, and can directly interact with it from the vscode's chat section and also in further versions, I am planning to implement agentic features so that users can use the local llm to use for code editing, in line suggestions, so that they don't have to pay for premiums and many more.&lt;/p&gt; &lt;p&gt;Currently it also has simple Text-to-Speech, and Speech-to-Text, which I am planning to include in the further versions, using open source audio models and also in further, implement 3D generation models, so that everyone can leverage the use of the open models directly from their terminal, and making the complex process of the using open models easy with just a single command in the terminal.&lt;/p&gt; &lt;p&gt;I have also implemented simple functionalities which can help, like listing the downloaded models and their sizes. Other things available are, basic dataset preprocessing, and training ML models directly with just two commands by just providing it the dataset. This is a basic implementation and want to further improve this so that users with just a dataset can clean and pre-process the data, train the models in their local or using the kaggle's or any free gpu providing services or their own gpus or cloud provided gpus, and can directly deploy the models and can use it any cases.&lt;/p&gt; &lt;p&gt;Currently this are the things it is doing and I want to improve such that everyone can use this for any case of the AI and leveraging the use of open models.&lt;/p&gt; &lt;p&gt;Please checkout the work at: &lt;a href="https://github.com/ManvithGopu13/Vllama"&gt;https://github.com/ManvithGopu13/Vllama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Published version at: &lt;a href="https://pypi.org/project/vllama/"&gt;https://pypi.org/project/vllama/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also the extension: &lt;a href="https://marketplace.visualstudio.com/items?itemName=ManvithGopu.vllama"&gt;https://marketplace.visualstudio.com/items?itemName=ManvithGopu.vllama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would appreciate your time for reading and thankful for everyone who want to contribute and spread a word of it.&lt;/p&gt; &lt;p&gt;Please leave your requests for improvements and any suggestions, ideas, and even roasts or anything in the comments or in the issues, this is well taken and appreciated. Thanks in advance. If you find the project useful, kindly contribute and can star it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weekly_Layer_9315"&gt; /u/Weekly_Layer_9315 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfo6zh/vllama_cli_based_framework_to_run_vision_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T12:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfiedc</id>
    <title>If it weren't for the Chinese we wouldn't have local AI</title>
    <updated>2025-12-06T06:28:03+00:00</updated>
    <author>
      <name>/u/Icy_Resolution8390</name>
      <uri>https://old.reddit.com/user/Icy_Resolution8390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GRACIAS A ELON MUSK!! ESTOY SEGURO!!! Elon el mas trabajador e inteligente del planeta , lo quisieron fastidiar y ahora OpenAi corre riesgo de ir a la quiebra por traicionarlo...todos lo que lo traicionaron por envidia y le ocultaron los resultados de la IA (google Y microsoft) Ahora veremos lo que agutanta OpenAI sin el dinero de musk...&lt;/p&gt; &lt;p&gt;Mientras GROK Y LOS CHINOS avanzan con las cuentas saneadas, con las empresas sin riesgo..bien apalancadas financiadas con seguridad , OPENAI esta temblando de miedo de ir a la quiebra aajjajajajajajaja&lt;/p&gt; &lt;p&gt;Todos sabemos el desastre que pas√≥ cuando un ingeniero chino rob√≥ tecnolog√≠a de Open AI (O eso nos han contado) con todo el l√≠o que caus√≥ cuando Deepseek sac√≥ su modelo de IA al mundo Opensource,&lt;/p&gt; &lt;p&gt;Todos sabemos que Open AI se neg√≥ a sacar lo mejor de su tecnolog√≠a...luego cuando MOE technology sali√≥ con Qwen e hizo que la bolsa americana cayera por trillones de d√≥lares d√°ndose cuenta que los chips Nvidia quiz√°s no fueran necesarios para la IA.&lt;/p&gt; &lt;p&gt;Sacudi√≥ los cimientos de los mercados de valores americanos. No sabemos exactamente qu√© pas√≥... si Elon Musk tuvo algo que ver con todo esto al irse de la empresa y se aprovecharon de su financiaci√≥n al principio... Quiz√°s le ocultaron a Elon Musk los verdaderos resultados que estaba dando la IA para que siguiera financi√°ndolos y √©l se veng√≥ cuando vio que no hab√≠a resultados cuando se fue de Open AI y justo despu√©s de que Elon Musk se fuera, Open AI dio el gran salto.&lt;/p&gt; &lt;p&gt;Fue demasiada coincidencia...Quiz√°s quien realmente controlaba Open AI (Microsoft), ya que Elon Musk estaba muy ocupado con sus problemas en sus empresas, aprovecharon la oportunidad para apostar y beneficiarse ya que ahora son el accionista mayoritario...ya es cosa tuya saber si Elon Musk estaba sobornando a ingenieros para que le dieran la tecnolog√≠a MOE y como se negaron hizo un pacto con ellos para d√°rsela a los chinos...y por ah√≠ openai mientras sacaban Grok al mercado...nunca sabremos que pas√≥ en esa telenovela...toda una historia de conspiraciones e hip√≥tesis extra√±as...&lt;/p&gt; &lt;p&gt;Lo extra√±o es que Qwen parece haber adquirido la tecnolog√≠a que todos creen que est√° en los modelos de Open AI...y nunca lo sabremos porque los modelos son cerrados y no sabemos qu√© ingenier√≠a hay realmente detr√°s. Todo esto pensando... y si no hubiera pasado toda esta historia de robo de tecnolog√≠a... conspiraciones... etc... ca√≠da de la bolsa por el miedo de los inversores a que la burbuja de la IA estallara... fin... si todo eso no hubiera pasado...&lt;/p&gt; &lt;p&gt;Yo creo que la comunidad Opensource no estar√≠a disfrutando de los modelos MOE que tenemos hoy en d√≠a... ya que eso conllev√≥ a una feroz competencia y a la salida de modelos para perjudicarse unos a otros... lo que tenemos que ver es que si no hubiera pasado lo de Open AI... creo que no hubieran sacado el GPT-OSS y tambi√©n creo que nunca volver√°n a sacar un modelo similar...&lt;/p&gt; &lt;p&gt;Estas empresas son muy avariciosas y quieren todo para ellas y si no fuera por la presi√≥n de los chinos, la comunidad Opensource tendr√≠a muchos menos modelos capaces y tenemos que estar agradecidos de que QWEN Y DEEPSEEK y otros metieran presi√≥n sacando sus modelos...porque los occidentales...si por ellos fuera...se quedar√≠an todo para ellos como hace Google con su buscador...facebook o Amazon con sus algoritmos o Apple Con sus tecnolog√≠as...&lt;/p&gt; &lt;p&gt;Normalmente los chinos siempre han sido muy cerrados...pero la competencia por ver qui√©n es ahora la superpotencia que domina el mundo ha tra√≠do esos golpes que han sido maravillosos para la comunidad Opensource...&lt;/p&gt; &lt;p&gt;El miedo que tengo es que no se repita algo as√≠ y los modelos que saquen de ahora en adelante sean calderilla... las migajas... y lo realmente bueno y las grandes cosas se las queden ellos y lo protejan todo con patentes... y mucha seguridad... ya viste la ca√≠da que OpenAi tuvo que ser fundada sin √°nimo de lucro para el beneficio de la Humanidad...&lt;/p&gt; &lt;p&gt;Se ha convertido en un negocio que mueve billones con una deuda muy apalancada...y juegan funambulistas colgados de un hilo...C√≥mo termina todo esto...si todo explota...o siguen sacando modelos...porque los inversores no invierten en el pasado...invierten en el futuro...y si no hay novedades...los occidentales van a estar en problemas...algo que los chinos se financian ellos mismos y son cautos y buscaron la forma de hacer el negocio m√°s rentable requiriendo menos potencia de c√°lculo...&lt;/p&gt; &lt;p&gt;En resumen, los chinos y asi√°ticos siempre han destacado en muchas √°reas como la electr√≥nica... pero ahora China se est√° convirtiendo en una Superpotencia mayor que EEUU en muchas √°reas y creo que no debemos subestimarlos ni menospreciarlos... OPEN AI va a quebrar completamente creo... y los chinos van a ganar esta batalla... creo que ser√° el principio de la hegemon√≠a de China sobre el mundo...&lt;/p&gt; &lt;p&gt;Son gente trabajadora, eficiente y pac√≠fica...as√≠ que realmente apoyo sus modelos y creo que gracias a ellos esta competencia por ver qui√©n gana est√° beneficiando a la comunidad Opensource de una manera inimaginable que nunca pens√© que podr√≠amos tener modelos tan capaces y √∫tiles en nuestras manos para trabajar completamente offline...y gran parte es gracias a los chinos!!!!! Quiz√°s no es gracias a los chinos... sino... a ELON MUSK. Si me equivoco en mis suposiciones OPENAI estar√≠a evolucionando por detr√°s de todos los dem√°s competidores...ya que ser√≠an los mejores...aunque sin la financiaci√≥n de Elon Musk...todo cambia.&lt;/p&gt; &lt;p&gt;Pronto veremos que ha pasado aqu√≠ y que est√° pasando...porque Elon Musk es una persona brillante y valiente...y una buena persona...y las buenas personas se enfr√≠an...creo que Microsoft pens√≥ que su jugada contra Musk iba a salir bien...pero pronto veremos si OPEN AI quiebra...&lt;/p&gt; &lt;p&gt;Si mis suposiciones son ciertas, Elon Musk nos habr√≠a dado a todos la tecnolog√≠a MOE completamente gratis para que la IA china diera un golpe fuerte a OPENAI para frenarlos y entrar en el top 3 con GROK. Si eso fuera cierto deber√≠amos agradecerle... pero no se pudo hacer porque nadie sabr√≠a que todos los modelos de alta calidad que disfrutamos offline son gracias a √âL. Al hombre m√°s rico del planeta que nos dio esta tecnolog√≠a porque algunas personas quer√≠an enga√±arlo!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Resolution8390"&gt; /u/Icy_Resolution8390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pfiedc/if_it_werent_for_the_chinese_we_wouldnt_have/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-06T06:28:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg5kui</id>
    <title>create model name without the ending :latest</title>
    <updated>2025-12-07T01:18:48+00:00</updated>
    <author>
      <name>/u/Frosty_Chest8025</name>
      <uri>https://old.reddit.com/user/Frosty_Chest8025</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI,&lt;br /&gt; how can i create a copy of gemma3-27b without it having a name gemma3:latest. I need it to be named as gemma3 in the ollama ps list.&lt;/p&gt; &lt;p&gt;ollama create gemma3 -f ollamaproduction&lt;br /&gt; creates model named gemma3:latest. &lt;/p&gt; &lt;p&gt;litellm needs that name without :latest. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frosty_Chest8025"&gt; /u/Frosty_Chest8025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg5kui/create_model_name_without_the_ending_latest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pg5kui/create_model_name_without_the_ending_latest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pg5kui/create_model_name_without_the_ending_latest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T01:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgdlw6</id>
    <title>n8n not connecting to Ollama - Locally Hosted</title>
    <updated>2025-12-07T08:43:54+00:00</updated>
    <author>
      <name>/u/zohan_796</name>
      <uri>https://old.reddit.com/user/zohan_796</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I've been using NetworkChuck's videos to get locally hosted AI models and n8n through OpenWebUI.&lt;/p&gt; &lt;p&gt;However, I'm currently having issues with getting my Ollama account on n8n to connect to Ollama. Both are locally hosted using OpenWebUI as per Chuck's videos. I've got the Base URL as &lt;a href="http://localhost:11434"&gt;http://localhost:11434&lt;/a&gt;, which doesn't seem to connect. What do I need to do to allow n8n to link to Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zohan_796"&gt; /u/zohan_796 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgdlw6/n8n_not_connecting_to_ollama_locally_hosted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1pgdlw6/n8n_not_connecting_to_ollama_locally_hosted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1pgdlw6/n8n_not_connecting_to_ollama_locally_hosted/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-12-07T08:43:54+00:00</published>
  </entry>
</feed>
