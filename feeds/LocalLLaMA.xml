<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-05T18:23:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qwkq31</id>
    <title>Is Huggingface ü§ó Down?</title>
    <updated>2026-02-05T13:03:57+00:00</updated>
    <author>
      <name>/u/NoobMLDude</name>
      <uri>https://old.reddit.com/user/NoobMLDude</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwkq31/is_huggingface_down/"&gt; &lt;img alt="Is Huggingface ü§ó Down?" src="https://preview.redd.it/ygv181rscohg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c48c650a00d76b2fb81d684799f054d2cf05f6da" title="Is Huggingface ü§ó Down?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoobMLDude"&gt; /u/NoobMLDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ygv181rscohg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwkq31/is_huggingface_down/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwkq31/is_huggingface_down/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T13:03:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwichp</id>
    <title>Qwen3 TTS Streaming workflow help</title>
    <updated>2026-02-05T11:01:31+00:00</updated>
    <author>
      <name>/u/RateRoutine2268</name>
      <uri>https://old.reddit.com/user/RateRoutine2268</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Guys,&lt;br /&gt; Noob here , im thinking of using Qwen3 TTS for voice agent poc` , and need help on the streaming part , does it supports stream ingestion &amp;amp; generation (as soon as it get response from llm it starts generating audio that can also be streamed for real time ), look at qwen3-tts i couldn't find any implementation or examples of such scenarios,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RateRoutine2268"&gt; /u/RateRoutine2268 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwichp/qwen3_tts_streaming_workflow_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwichp/qwen3_tts_streaming_workflow_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwichp/qwen3_tts_streaming_workflow_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T11:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwrco8</id>
    <title>Qwen3-Coder-Next slow prompt processing in llama.cpp</title>
    <updated>2026-02-05T17:20:57+00:00</updated>
    <author>
      <name>/u/DistanceAlert5706</name>
      <uri>https://old.reddit.com/user/DistanceAlert5706</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrco8/qwen3codernext_slow_prompt_processing_in_llamacpp/"&gt; &lt;img alt="Qwen3-Coder-Next slow prompt processing in llama.cpp" src="https://b.thumbs.redditmedia.com/hklVZbKamt4I4Ob1y9xDCakoVp54UG7u_GbraoPXA8I.jpg" title="Qwen3-Coder-Next slow prompt processing in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was trying to run Qwen3-Coder-Next today, updated llama.cpp from main beforehand and while token generation speed is nice, prompt processing speed is just extremely slow.&lt;/p&gt; &lt;p&gt;Running Unsloth's MXFP4 quant, tried on 2 5060Ti's and 3 5060Ti's.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;taskset -c 0-11 ~/llama.cpp/build/bin/llama-server --device CUDA1,CUDA2 \ --model ~/models/unsloth/Qwen3-Coder-Next-GGUF/Qwen3-Coder-Next-MXFP4_MOE.gguf \ --host 0.0.0.0 \ --port 8052 \ --jinja \ --threads 12 \ --ctx-size 131072 \ --alias &amp;quot;qwen3-next&amp;quot; \ --fit on \ --seed 3407 \ --temp 1.0 \ --top-p 0.95 \ --min-p 0.01 \ --top-k 40 \ --log-timestamps \ --log-prefix &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1uonvm1xlphg1.png?width=1784&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b58941b4dc627ad5a6c7aa13d1640bf9ce8def2"&gt;https://preview.redd.it/1uonvm1xlphg1.png?width=1784&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b58941b4dc627ad5a6c7aa13d1640bf9ce8def2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z2h7rjgzlphg1.png?width=1784&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d20a51921320b272677cf02a3677ab56475d2f2"&gt;https://preview.redd.it/z2h7rjgzlphg1.png?width=1784&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d20a51921320b272677cf02a3677ab56475d2f2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Something is clearly broken as this prompt processing speed should be impossible, 2x slower than token generation.&lt;/p&gt; &lt;p&gt;Maybe someone knows what's going on?&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; Something is playing tricks here, results from single GPU without `--fit on`&lt;/p&gt; &lt;pre&gt;&lt;code&gt;taskset -c 0-11 ~/llama.cpp/build/bin/llama-server --device CUDA2 \ --model ~/models/unsloth/Qwen3-Coder-Next-GGUF/Qwen3-Coder-Next-MXFP4_MOE.gguf \ --host 0.0.0.0 \ --port 8052 \ --jinja \ --threads 12 \ --ctx-size 131072 \ --alias &amp;quot;qwen3-next&amp;quot; \ --batch-size 2048 \ --ubatch-size 2048 \ --flash-attn on \ --n-gpu-layers 999 \ --cpu-moe \ --seed 3407 \ --temp 1.0 \ --top-p 0.95 \ --min-p 0.01 \ --top-k 40 \ --log-timestamps \ --log-prefix &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jkl6vx86sphg1.png?width=1714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9b00e417a9fc4de4e9df98d448109a235c07c0a0"&gt;https://preview.redd.it/jkl6vx86sphg1.png?width=1714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9b00e417a9fc4de4e9df98d448109a235c07c0a0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gewc78xvsphg1.png?width=1725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f1975b484e7575b81b6fb356761bd855462c2367"&gt;https://preview.redd.it/gewc78xvsphg1.png?width=1725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f1975b484e7575b81b6fb356761bd855462c2367&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With `fit on` on single GPU it's faster on token gen and uses full VRAM but 2 times slower on PP&lt;/p&gt; &lt;p&gt;Edit 2:&lt;br /&gt; I think I know what bottlenecks it, CUDA 1 is on PCIe3 x1 lane, it's not an issue if whole model fits into VRAM but looks like an issue with CPU offloading, results from original command but on CUDA0+CUDA2&lt;br /&gt; Still lower PP with fit on then manual, looks like it tries to optimize for TG instead, but it's something.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hoamk6bbvphg1.png?width=1784&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bcebcce969530c81d7286ffc7901979f80492bee"&gt;https://preview.redd.it/hoamk6bbvphg1.png?width=1784&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bcebcce969530c81d7286ffc7901979f80492bee&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DistanceAlert5706"&gt; /u/DistanceAlert5706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrco8/qwen3codernext_slow_prompt_processing_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrco8/qwen3codernext_slow_prompt_processing_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrco8/qwen3codernext_slow_prompt_processing_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T17:20:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwnirs</id>
    <title>Tencent Youtu-VL-4B. Potential Florence-2 replacement? (Heads up on the weird license)</title>
    <updated>2026-02-05T15:00:10+00:00</updated>
    <author>
      <name>/u/Gohab2001</name>
      <uri>https://old.reddit.com/user/Gohab2001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tencent/Youtu-VL-4B-Instruct"&gt;https://huggingface.co/tencent/Youtu-VL-4B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4B params, so it's perfect for the low-VRAM gang (should run comfortably on 6-8GB cards). The paper claims it beats Qwen-VL and Florence-2 on grounding and segmentation, which is huge if true. The architecture uses visual tokens as targets rather than just inputs, which is pretty clever.&lt;/p&gt; &lt;p&gt;The License: It explicitly says &lt;strong&gt;&amp;quot;NOT INTENDED FOR USE WITHIN THE EUROPEAN UNION.&amp;quot;&lt;/strong&gt; I've seen &amp;quot;research only&amp;quot; or &amp;quot;non-commercial&amp;quot; plenty of times, but a specific geo-block in the license text is a new one for me.&lt;/p&gt; &lt;p&gt;GGUFs are already up if you want to test the chat capabilities/OCR, but might want to wait until the actual vision tools get released before trying to build a workflow around it.&lt;/p&gt; &lt;p&gt;Anyone managed to force it to output masks with the raw weights yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gohab2001"&gt; /u/Gohab2001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwnirs/tencent_youtuvl4b_potential_florence2_replacement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwnirs/tencent_youtuvl4b_potential_florence2_replacement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwnirs/tencent_youtuvl4b_potential_florence2_replacement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T15:00:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwqzcz</id>
    <title>Will Qwen ever release their video generation model locally?</title>
    <updated>2026-02-05T17:07:29+00:00</updated>
    <author>
      <name>/u/XiRw</name>
      <uri>https://old.reddit.com/user/XiRw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really am a big fan of the quality with their videos and I like how it automatically comes with sound so I was wondering if there was any word in the future if this will happen or not?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XiRw"&gt; /u/XiRw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwqzcz/will_qwen_ever_release_their_video_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwqzcz/will_qwen_ever_release_their_video_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwqzcz/will_qwen_ever_release_their_video_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T17:07:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwjdx6</id>
    <title>Is there a good local model to translate small snippets of text from English to Russian that can be run completely on 12GB VRAM?</title>
    <updated>2026-02-05T11:58:59+00:00</updated>
    <author>
      <name>/u/ShaderCompilation</name>
      <uri>https://old.reddit.com/user/ShaderCompilation</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically the title. I want a model that can be used to translate small snippets of text from books to Russian. But i need it to run on just 12GB of VRAM. Is there a decent model, or 12GB is too small for one?&lt;/p&gt; &lt;p&gt;Edit: I want something that i can run with Ollama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShaderCompilation"&gt; /u/ShaderCompilation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwjdx6/is_there_a_good_local_model_to_translate_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwjdx6/is_there_a_good_local_model_to_translate_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwjdx6/is_there_a_good_local_model_to_translate_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T11:58:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwrlmy</id>
    <title>24hr-research-agent: An experimental autonomous research system that conducts comprehensive, multi-hour research sessions and produces book-length reports with full citations on any topic.</title>
    <updated>2026-02-05T17:29:42+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty ridiculous, had to do it :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Aaryan-Kapoor/24hr-research-agent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrlmy/24hrresearchagent_an_experimental_autonomous/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrlmy/24hrresearchagent_an_experimental_autonomous/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T17:29:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qw8ord</id>
    <title>Why do companies release "SOTA" models when the code is just a TODO list? My night wasted on Tencent's Youtu-VL-4B.</title>
    <updated>2026-02-05T02:19:24+00:00</updated>
    <author>
      <name>/u/MadPelmewka</name>
      <uri>https://old.reddit.com/user/MadPelmewka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw8ord/why_do_companies_release_sota_models_when_the/"&gt; &lt;img alt="Why do companies release &amp;quot;SOTA&amp;quot; models when the code is just a TODO list? My night wasted on Tencent's Youtu-VL-4B." src="https://a.thumbs.redditmedia.com/DBvnsS5atMTiuTFe3ikSawYF1v0rEWf-C_c7jXWt5E8.jpg" title="Why do companies release &amp;quot;SOTA&amp;quot; models when the code is just a TODO list? My night wasted on Tencent's Youtu-VL-4B." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;[SOLVED / IMPORTANT UPDATE]:&lt;/strong&gt; Received an &lt;a href="https://huggingface.co/tencent/Youtu-VL-4B-Instruct/discussions/5#6984abda7c150e9f6d23e94e"&gt;official response&lt;/a&gt; from the developers. They updated their GitHub with a &lt;a href="https://github.com/TencentCloudADP/youtu-vl/tree/main/demo"&gt;full demo&lt;/a&gt; and a Jupyter notebook explaining how to trigger vision-centric tasks (Detection/Segmentation/etc.).&lt;/p&gt; &lt;p&gt;I was browsing Hugging Face trending models as usual to see what's new, and I saw &lt;a href="https://huggingface.co/tencent/Youtu-VL-4B-Instruct"&gt;Tencent/Youtu-VL-4B-Instruct&lt;/a&gt;. The README looks amazing. It describes a hybrid VLM that can do everything: Object Detection, Semantic Segmentation, Grounding, etc. I immediately thought: &lt;em&gt;&amp;quot;Cool, finally a potential replacement or competitor to&lt;/em&gt; &lt;a href="https://huggingface.co/collections/microsoft/florence"&gt;Florence-2&lt;/a&gt;&lt;em&gt;.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I specifically needed high-quality segmentation to create a dataset for my scenario. So I tried to run it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Reality:&lt;/strong&gt; The model was released raw. Right now, it's just a standard VLM that can only describe what's in the image. There is &lt;strong&gt;NO information&lt;/strong&gt; about this on the model's main Hugging Face page. I had to dig for the truth, which I only found in the &lt;a href="https://github.com/TencentCloudADP/youtu-vl?tab=readme-ov-file#todo-list"&gt;GitHub TODO List&lt;/a&gt; and &lt;strong&gt;in the&lt;/strong&gt; &lt;a href="https://huggingface.co/tencent/Youtu-Parsing/discussions/2#697acfb8037b0052e316ae70"&gt;Community tab of ANOTHER model&lt;/a&gt;, where they mention that the current Transformers implementation is incomplete and full functionality requires a separate SDK...&lt;/p&gt; &lt;p&gt;The GitHub TODO list literally hides it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;## TODO List - [ ] Support vLLM - [ ] Release recipes for various tasks - [ ] Release evaluation codes &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;They mask it behind vague phrases like &amp;quot;recipes for various tasks&amp;quot;. What is the point of publishing a model, boasting about SOTA benchmarks in the README, but hiding the fact that you can't actually test them because the code is missing? It feels misleading.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bonus -&lt;/strong&gt; &lt;a href="https://huggingface.co/tencent/Youtu-VL-4B-Instruct/blob/main/LICENSE.txt"&gt;The License&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; The license is essentially free/MIT-like, except for one line:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Youtu-VL IS NOT INTENDED FOR USE WITHIN THE EUROPEAN UNION.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So, it's trending on HF, but it's raw, &amp;quot;vision-centric&amp;quot; features are missing (or hidden in a non-existent SDK), and it's banned in the EU. Just a heads up before you waste your time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UPD&lt;/strong&gt;: I want to clarify that I‚Äôm not &amp;quot;anti-Tencent.&amp;quot; In fact, I generally support their work and I'm excited about their research. My issue is strictly with transparency. When a README is filled with impressive &amp;quot;Key Features&amp;quot; and benchmarks, but fails to mention that the actual codebase is unfinished ‚Äì and then that model hits the HuggingFace trending list ‚Äì it‚Äôs a problem. It leads to people wasting hours of their time on a product that isn't ready for the tasks it claims to solve.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UPD2 (Important):&lt;/strong&gt; It turns out that the &amp;quot;Vision-Centric&amp;quot; features (Detection and Segmentation) &lt;em&gt;are&lt;/em&gt; actually functional in the current release, but they are effectively &amp;quot;undocumented&amp;quot; on Hugging Face. I managed to get them working by using the specific prompts found deep in the Arxiv paper (thanks to &lt;a href="/u/MitsotakiShogun"&gt;u/MitsotakiShogun&lt;/a&gt; for the nudge!).&lt;/p&gt; &lt;p&gt;Interestingly, I had previously fed the paper to Gemini as context, but it failed to extract the necessary info. Only when I started a fresh chat and explicitly told it to focus &lt;em&gt;strictly&lt;/em&gt; on &amp;quot;inference prompts for detection and segmentation&amp;quot; did it find the correct formats.&lt;/p&gt; &lt;p&gt;However, this doesn't change the fact that the developer experience is currently a mess:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Broken &amp;quot;Quickstart&amp;quot;:&lt;/strong&gt; The official code snippet on Hugging Face literally contains Python syntax errors. You can't even run the basic description example without fixing it first.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hidden Documentation:&lt;/strong&gt; Why force users to read a 70-page research paper just to find the basic prompts needed to run the model's core features? It would have been trivial to include these prompt examples in a &amp;quot;collapsible/spoiler&amp;quot; section in the README.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Attention Hell &amp;amp; Compilation Issues:&lt;/strong&gt; The documentation only mentions &lt;code&gt;flash_attention_2&lt;/code&gt;. I spent the entire night trying to compile it on a Blackwell instance, but the process was a nightmare‚ÄîRAM usage would balloon uncontrollably until the system crashed. To make matters worse, &lt;code&gt;sdpa&lt;/code&gt; &lt;strong&gt;(Scaled Dot Product Attention) doesn't seem to work properly here&lt;/strong&gt;, leaving you with either the compilation lottery of Flash-Attn or the painfully slow &lt;code&gt;eager&lt;/code&gt; mode.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Of course, &lt;strong&gt;there is a high probability that I‚Äôm just an inexperienced user/an idiot and the fault lies entirely on my end&lt;/strong&gt;, but in my experience, a &amp;quot;Quickstart&amp;quot; model trending on HF is usually more straightforward. If a corporate giant like Tencent releases a model in 2026, I‚Äôd expect at least a working &lt;code&gt;sdpa&lt;/code&gt; fallback and a README without syntax errors.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bottom line:&lt;/strong&gt; Don't include a &amp;quot;Quickstart&amp;quot; section if it's neither &amp;quot;quick&amp;quot; nor &amp;quot;starting&amp;quot; anything without a deep dive into Arxiv. Tencent has released some great weights, but the way they‚Äôve packaged this for the community is incredibly sloppy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UPD3:&lt;/strong&gt; Received an &lt;a href="https://huggingface.co/tencent/Youtu-VL-4B-Instruct/discussions/5#6984abda7c150e9f6d23e94e"&gt;official response&lt;/a&gt; from the developers. They updated their GitHub with a &lt;a href="https://github.com/TencentCloudADP/youtu-vl/tree/main/demo"&gt;full demo&lt;/a&gt; and a Jupyter notebook explaining how to trigger vision-centric tasks (Detection/Segmentation/etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadPelmewka"&gt; /u/MadPelmewka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qw8ord"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw8ord/why_do_companies_release_sota_models_when_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qw8ord/why_do_companies_release_sota_models_when_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T02:19:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qw6rwc</id>
    <title>I built a tool to visualize LLM workflows as interactive and shareable graphs</title>
    <updated>2026-02-05T00:55:57+00:00</updated>
    <author>
      <name>/u/Cyanosistaken</name>
      <uri>https://old.reddit.com/user/Cyanosistaken</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw6rwc/i_built_a_tool_to_visualize_llm_workflows_as/"&gt; &lt;img alt="I built a tool to visualize LLM workflows as interactive and shareable graphs" src="https://external-preview.redd.it/N3U4aTg5N3Zwa2hnMZamVz7bJmXM-USGdY_dhCyJiLc44FJ8QD5RU8S3ljgg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93b4cb715f5ce539b498094ba9f1092db8f74ef7" title="I built a tool to visualize LLM workflows as interactive and shareable graphs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! &lt;/p&gt; &lt;p&gt;I built Codag - an open source VSCode extension to visualize LLM workflows natively in your codebase. I kept on getting lost with the sheer amount of code that agents were output, and what better way of keeping track than to visualize it? &lt;/p&gt; &lt;p&gt;It supports OpenAI, Anthropic, Gemini, LangChain, LangGraph, CrewAI + more, and works with Python, TypeScript, Go, Rust, Java + more. &lt;/p&gt; &lt;p&gt;The demo video visualizes Vercel's AIChatbot repo. &lt;/p&gt; &lt;p&gt;Codag's link is in the comments, would love feedback from anyone building agents or multi-step LLM pipelines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cyanosistaken"&gt; /u/Cyanosistaken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e9x23c6vpkhg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw6rwc/i_built_a_tool_to_visualize_llm_workflows_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qw6rwc/i_built_a_tool_to_visualize_llm_workflows_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T00:55:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwpns7</id>
    <title>How are you handling hallucinations with self-hosted agents in production?</title>
    <updated>2026-02-05T16:19:36+00:00</updated>
    <author>
      <name>/u/MarionberrySingle538</name>
      <uri>https://old.reddit.com/user/MarionberrySingle538</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those running agents in production:&lt;/p&gt; &lt;p&gt;Are you just accepting some error rate and handling it downstream?&lt;/p&gt; &lt;p&gt;Using multiple models to cross-check outputs?&lt;/p&gt; &lt;p&gt;Building verification layers that catch hallucinations before they cause problems?&lt;/p&gt; &lt;p&gt;Restricting agents to tasks where hallucinations are less catastrophic?&lt;/p&gt; &lt;p&gt;Curious if anyone's found approaches that actually work at scale, or if this is still an unsolved problem everyone's just managing around.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarionberrySingle538"&gt; /u/MarionberrySingle538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwpns7/how_are_you_handling_hallucinations_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwpns7/how_are_you_handling_hallucinations_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwpns7/how_are_you_handling_hallucinations_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T16:19:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwnb4a</id>
    <title>Stop falling for the marketing: Your wallet is at risk</title>
    <updated>2026-02-05T14:51:46+00:00</updated>
    <author>
      <name>/u/Own_Most_8489</name>
      <uri>https://old.reddit.com/user/Own_Most_8489</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The amount of low-effort marketing for these new agents is insane. Beyond the hype on &lt;a href="/r/myclaw"&gt;r/myclaw&lt;/a&gt;, there are serious risks of prompt injection attacks that target your linked wallets or expose your entire database. If an agent can &amp;quot;read&amp;quot; an external website and that site contains a hidden command to exfiltrate your API keys, it's game over. Be extremely careful about what permissions you're granting these bots.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own_Most_8489"&gt; /u/Own_Most_8489 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwnb4a/stop_falling_for_the_marketing_your_wallet_is_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwnb4a/stop_falling_for_the_marketing_your_wallet_is_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwnb4a/stop_falling_for_the_marketing_your_wallet_is_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T14:51:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvq0xe</id>
    <title>Bashing Ollama isn‚Äôt just a pleasure, it‚Äôs a duty</title>
    <updated>2026-02-04T14:29:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvq0xe/bashing_ollama_isnt_just_a_pleasure_its_a_duty/"&gt; &lt;img alt="Bashing Ollama isn‚Äôt just a pleasure, it‚Äôs a duty" src="https://preview.redd.it/ad5zhvq0nhhg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b9fa62de0e64a6887124b87e66b3b99b2942107" title="Bashing Ollama isn‚Äôt just a pleasure, it‚Äôs a duty" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ad5zhvq0nhhg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvq0xe/bashing_ollama_isnt_just_a_pleasure_its_a_duty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvq0xe/bashing_ollama_isnt_just_a_pleasure_its_a_duty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T14:29:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwisld</id>
    <title>vLLM-Omni paper is out ‚Äî up to 91.4% JCT reduction for any-to-any multimodal serving (tested with Qwen-Image-2512)</title>
    <updated>2026-02-05T11:26:25+00:00</updated>
    <author>
      <name>/u/still_debugging_note</name>
      <uri>https://old.reddit.com/user/still_debugging_note</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwisld/vllmomni_paper_is_out_up_to_914_jct_reduction_for/"&gt; &lt;img alt="vLLM-Omni paper is out ‚Äî up to 91.4% JCT reduction for any-to-any multimodal serving (tested with Qwen-Image-2512)" src="https://b.thumbs.redditmedia.com/fwuQJBBgAjnQ7NMd5tOYI0zJFhDqirXaHdo6h7gdbQc.jpg" title="vLLM-Omni paper is out ‚Äî up to 91.4% JCT reduction for any-to-any multimodal serving (tested with Qwen-Image-2512)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The vLLM team just released the vLLM-Omni paper on arXiv: &lt;a href="https://arxiv.org/abs/2602.02204"&gt;https://arxiv.org/abs/2602.02204&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vLLM-Omni is designed for any-to-any multimodal models that jointly handle text, images, video, and audio ‚Äî which is where serving starts to get really painful in practice.&lt;/p&gt; &lt;p&gt;It documents their system design for serving &lt;em&gt;any-to-any multimodal models&lt;/em&gt; ‚Äî think pipelines that mix AR LLMs, diffusion models, encoders, etc., instead of assuming a single paradigm.&lt;/p&gt; &lt;p&gt;A few things that stood out: stage-based graph decomposition for pipelines, per-stage batching, and flexible GPU allocation across stages ‚Äî makes serving any-to-any multimodal models much cleaner and faster.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4lzqx6ldrnhg1.png?width=717&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12957425682c9438946b61d9f1a554eec7e851ae"&gt;https://preview.redd.it/4lzqx6ldrnhg1.png?width=717&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12957425682c9438946b61d9f1a554eec7e851ae&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve actually tested vLLM-Omni with Qwen-Image-2512 ‚Äî comparable GPU memory to diffusers, but much faster generation üëá&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zho8tpassnhg1.png?width=405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa46ed99b93ebd6638c9e4dc7b05840d2cca18af"&gt;https://preview.redd.it/zho8tpassnhg1.png?width=405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa46ed99b93ebd6638c9e4dc7b05840d2cca18af&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/still_debugging_note"&gt; /u/still_debugging_note &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwisld/vllmomni_paper_is_out_up_to_914_jct_reduction_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwisld/vllmomni_paper_is_out_up_to_914_jct_reduction_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwisld/vllmomni_paper_is_out_up_to_914_jct_reduction_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T11:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwmxlw</id>
    <title>I built a virtual filesystem to replace MCP for AI agents</title>
    <updated>2026-02-05T14:37:12+00:00</updated>
    <author>
      <name>/u/velobro</name>
      <uri>https://old.reddit.com/user/velobro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwmxlw/i_built_a_virtual_filesystem_to_replace_mcp_for/"&gt; &lt;img alt="I built a virtual filesystem to replace MCP for AI agents" src="https://external-preview.redd.it/Z2o0djdzMmVzb2hnMRzYL8l2zAETrvK0CoWkq8ClJmiK60eAGCQn0bzkwntA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37052db339ec5b2bfe62b2b387836038f2ee58c6" title="I built a virtual filesystem to replace MCP for AI agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One of the reasons Claude Code is so good at coding is because all the context it needs is just sitting there as files on your computer. But that‚Äôs not true for most non-coding tasks. Your PRs are on Github. Your docs are in Drive. Your emails are in Gmail.&lt;/p&gt; &lt;p&gt;You can connect MCP servers to Claude and provide access to those data sources. But setting up each MCP involves a bunch of glue code, and you usually end up giving your agent way more access than they need - not to mention the tokens you need to spend to have an LLM write the query to pull in exactly what you want.&lt;/p&gt; &lt;p&gt;Airstore turns all your data sources into a virtual filesystem for Claude code. You connect your services, create ‚Äúsmart folders‚Äù with natural language (for example, ‚Äúinvoices I received in my email last week‚Äù), and they are then mounted as local folders that Claude can access to accomplish tasks.&lt;/p&gt; &lt;p&gt;This is convenient, but it‚Äôs also safe: by principle of least privilege, Claude only gets access to the sort of things you want it to have access to.&lt;/p&gt; &lt;p&gt;The native interface to Claude is a filesystem. And the more of your world that you can represent as files, the more things Claude can do for you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/velobro"&gt; /u/velobro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ie40tx1esohg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwmxlw/i_built_a_virtual_filesystem_to_replace_mcp_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwmxlw/i_built_a_virtual_filesystem_to_replace_mcp_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T14:37:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwkk20</id>
    <title>Huggingface down but online?</title>
    <updated>2026-02-05T12:56:21+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwkk20/huggingface_down_but_online/"&gt; &lt;img alt="Huggingface down but online?" src="https://preview.redd.it/zjgxqj4ebohg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1acdac94ab180afea16559baeeaf59de769d3824" title="Huggingface down but online?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;does it work for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zjgxqj4ebohg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwkk20/huggingface_down_but_online/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwkk20/huggingface_down_but_online/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T12:56:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwo5ig</id>
    <title>Unofficial ik_llama.cpp release builds available for macOS, Ubuntu and Windows</title>
    <updated>2026-02-05T15:24:16+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I first got introduced to &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;ik_llama.cpp&lt;/a&gt; I struggled to run it because builds were not available and I didn‚Äôt have time/experience to set up a build environment on Windows (the env I use, don't ask me why).&lt;br /&gt; To make onboarding easier for others in the same boat, I created and publish pre-built releases from my fork so folks can try ik_llama.cpp without wrestling with compilation ‚Äî in the hope that more people will adopt it.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Latest build (at time of posting): &lt;a href="https://github.com/Thireus/ik_llama.cpp/releases/tag/main-b4222-30c39e3"&gt;https://github.com/Thireus/ik_llama.cpp/releases/tag/main-b4222-30c39e3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;All future builds/releases: &lt;a href="https://github.com/Thireus/ik_llama.cpp/releases"&gt;https://github.com/Thireus/ik_llama.cpp/releases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Original project (please prefer compiling from source if you can): &lt;a href="https://github.com/ikawrakow/ik_llama.cpp/"&gt;https://github.com/ikawrakow/ik_llama.cpp/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;My compilation parameters (GitHub Actions used): &lt;a href="https://github.com/Thireus/ik_llama.cpp/blob/main/.github/workflows/release.yml"&gt;https://github.com/Thireus/ik_llama.cpp/blob/main/.github/workflows/release.yml&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why I‚Äôm sharing this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Make it easier for users / newcomers (specifically on Windows) to test ik_llama.cpp‚Äôs faster inference and extra quantisation options.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Not trying to replace the upstream repo ‚Äî if you can compile from the original source, please do (ikawrakow strongly prefers issue reports that reference his exact commit IDs). My builds are intended as an easy entry point.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope this helps anyone who‚Äôs been waiting to try ik_llama.cpp.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwo5ig/unofficial_ik_llamacpp_release_builds_available/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwo5ig/unofficial_ik_llamacpp_release_builds_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwo5ig/unofficial_ik_llamacpp_release_builds_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T15:24:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwp7kt</id>
    <title>Released: DeepBrainz-R1 ‚Äî reasoning-first small models for agentic workflows (4B / 2B / 0.6B)</title>
    <updated>2026-02-05T16:03:04+00:00</updated>
    <author>
      <name>/u/arunkumar_bvr</name>
      <uri>https://old.reddit.com/user/arunkumar_bvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing DeepBrainz-R1 ‚Äî a family of reasoning-first small language models aimed at agentic workflows rather than chat.&lt;/p&gt; &lt;p&gt;These models are post-trained to emphasize:&lt;/p&gt; &lt;p&gt;- multi-step reasoning&lt;/p&gt; &lt;p&gt;- stability in tool-calling / retry loops&lt;/p&gt; &lt;p&gt;- lower-variance outputs in agent pipelines&lt;/p&gt; &lt;p&gt;They‚Äôre not optimized for roleplay or creative writing. The goal is predictable reasoning behavior at small parameter sizes for local / cost-sensitive setups.&lt;/p&gt; &lt;p&gt;Models:&lt;/p&gt; &lt;p&gt;- R1-4B (flagship)&lt;/p&gt; &lt;p&gt;- R1-2B&lt;/p&gt; &lt;p&gt;- R1-0.6B-v2&lt;/p&gt; &lt;p&gt;- experimental long-context variants (16K / 40K)&lt;/p&gt; &lt;p&gt;Apache-2.0. Community-maintained GGUF / low-bit quantizations are already appearing.&lt;/p&gt; &lt;p&gt;HF: &lt;a href="https://huggingface.co/DeepBrainz"&gt;https://huggingface.co/DeepBrainz&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious how folks here evaluate reasoning behavior in local agent setups, especially beyond standard benchmarks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arunkumar_bvr"&gt; /u/arunkumar_bvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwp7kt/released_deepbrainzr1_reasoningfirst_small_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwp7kt/released_deepbrainzr1_reasoningfirst_small_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwp7kt/released_deepbrainzr1_reasoningfirst_small_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T16:03:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwrpom</id>
    <title>really impressed with these new ocr models (lightonocr-2 and glm-ocr). much better than what i saw come out in nov-dec 2025</title>
    <updated>2026-02-05T17:33:54+00:00</updated>
    <author>
      <name>/u/datascienceharp</name>
      <uri>https://old.reddit.com/user/datascienceharp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrpom/really_impressed_with_these_new_ocr_models/"&gt; &lt;img alt="really impressed with these new ocr models (lightonocr-2 and glm-ocr). much better than what i saw come out in nov-dec 2025" src="https://b.thumbs.redditmedia.com/lemBjuywLXHSh55oKPeQldeNetPiKExhrecGB4VPRXY.jpg" title="really impressed with these new ocr models (lightonocr-2 and glm-ocr). much better than what i saw come out in nov-dec 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gif 1: LightOnOCR-2-1B&lt;/p&gt; &lt;p&gt;docs page: &lt;a href="https://docs.voxel51.com/plugins/plugins_ecosystem/lightonocr_2.html"&gt;https://docs.voxel51.com/plugins/plugins_ecosystem/lightonocr_2.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;quickstart nb: &lt;a href="https://github.com/harpreetsahota204/LightOnOCR-2/blob/main/lightonocr2_fiftyone_example.ipynb"&gt;https://github.com/harpreetsahota204/LightOnOCR-2/blob/main/lightonocr2_fiftyone_example.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gif 2: GLM-OCR&lt;/p&gt; &lt;p&gt;docs page: &lt;a href="https://docs.voxel51.com/plugins/plugins_ecosystem/glm_ocr.html"&gt;https://docs.voxel51.com/plugins/plugins_ecosystem/glm_ocr.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;quickstart nb: &lt;a href="https://github.com/harpreetsahota204/glm_ocr/blob/main/glm_ocr_fiftyone_example.ipynb"&gt;https://github.com/harpreetsahota204/glm_ocr/blob/main/glm_ocr_fiftyone_example.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;imo, glm-ocr takes the cake. much faster, and you can get pretty reliable structured output &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/datascienceharp"&gt; /u/datascienceharp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qwrpom"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrpom/really_impressed_with_these_new_ocr_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrpom/really_impressed_with_these_new_ocr_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T17:33:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwbmct</id>
    <title>Qwen3-Coder-Next on RTX 5060 Ti 16 GB - Some numbers</title>
    <updated>2026-02-05T04:33:49+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About 2 weeks ago, I posted about running GLM-4.7-Flash on 16 GB of VRAM here &lt;a href="http://www.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"&gt;www.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/&lt;/a&gt;. And here we go, today, let's squeeze an even bigger model into the poor rig.&lt;/p&gt; &lt;p&gt;Hardware: - AMD Ryzen 7 7700X - RAM 32 GB DDR5-6000 - RTX 5060 Ti 16 GB&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF?show_file_info=Qwen3-Coder-Next-Q3_K_M.gguf"&gt;unsloth/Qwen3-Coder-Next-GGUF Q3_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Llama.cpp version: &lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7940"&gt;llama.cpp@b7940&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The llamap.cpp command:&lt;/p&gt; &lt;p&gt;&lt;code&gt; llama-server -m ./Qwen3-Coder-Next-Q3_K_M.gguf -c 32768 -np 1 -t 8 --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 --jinja --fit on -fa 1 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;When I started, I didn't expect much, given that my best result for GLM-4.7-Flash was something ~300 t/s pp and 14 t/s gen. Maybe I'll end up with a lot of OOM and crash.&lt;/p&gt; &lt;p&gt;But, to my surprise, the card was able to pull it well!&lt;/p&gt; &lt;p&gt;When llama.cpp is fully loaded, it takes &lt;strong&gt;15.1 GB&lt;/strong&gt; GPU memory, and &lt;strong&gt;30.2 GB&lt;/strong&gt; RAM. The rig is almost at its memory limit.&lt;/p&gt; &lt;p&gt;During prompt processing, GPU usage was about &lt;strong&gt;35%&lt;/strong&gt;, and CPU usage was about &lt;strong&gt;15%&lt;/strong&gt;. During token generation, that's &lt;strong&gt;45%&lt;/strong&gt; for the GPU, and &lt;strong&gt;25%-45%&lt;/strong&gt; CPU. So perhaps there are some room to squeeze in some tuning here.&lt;/p&gt; &lt;p&gt;Does it run? Yes, and it's quite fast for a 5060!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Metric&lt;/th&gt; &lt;th&gt;Task 2 (Large Context)&lt;/th&gt; &lt;th&gt;Task 190 (Med Context)&lt;/th&gt; &lt;th&gt;Task 327 (Small Context)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Prompt Eval (Prefill)&lt;/td&gt; &lt;td&gt;154.08 t/s&lt;/td&gt; &lt;td&gt;225.14 t/s&lt;/td&gt; &lt;td&gt;118.98 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Generation (Decode)&lt;/td&gt; &lt;td&gt;16.90 t/s&lt;/td&gt; &lt;td&gt;16.82 t/s&lt;/td&gt; &lt;td&gt;18.46 t/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The above run was with a 32k context size. Later on, I tried again with a 64k context size, the speed did not change much.&lt;/p&gt; &lt;p&gt;Is it usable? I'd say yes, not Opus 4.5 or Gemini Flash usable, but I think it's pretty close to my experience when Claude Sonnet 3.7 or 4 was still a thing.&lt;/p&gt; &lt;p&gt;One thing that sticks out is, this model uses way less tool calls than Opus, so it feels fast. It seems to read the whole file all at once when needed, rather than grepping every 200 lines like the Claude brothers.&lt;/p&gt; &lt;p&gt;One-shot something seems to work pretty well, until it runs into bugs. In my example, I asked the model to create a web-based chess game with a Python backend, connected via WebSocket. The model showed that it can debug the problem by jumping back and forth between frontend and backend code very well.&lt;/p&gt; &lt;p&gt;When facing a problem, it will first hypothesize a cause, then work its way through the code to verify that. Then there will be a lot of &amp;quot;But wait&amp;quot;, &amp;quot;Hold on&amp;quot;, followed by a tool call to read some files, and then changing directions. Sometimes it works. Sometimes, it was just burning through the tokens and ended up reaching the context limit. Maybe because I was using Q3_K_M, and higher quants will have better quality here.&lt;/p&gt; &lt;p&gt;Some screenshots:&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/user-attachments/assets/8d074a76-c441-42df-b146-0ae291af17df"&gt;https://gist.github.com/user-attachments/assets/8d074a76-c441-42df-b146-0ae291af17df&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/user-attachments/assets/3aa3a845-96cd-4b23-b6d9-1255036106db"&gt;https://gist.github.com/user-attachments/assets/3aa3a845-96cd-4b23-b6d9-1255036106db&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can see the Claude session logs and llama.cpp logs of the run here &lt;a href="https://gist.github.com/huytd/6b1e9f2271dd677346430c1b92893b57"&gt;https://gist.github.com/huytd/6b1e9f2271dd677346430c1b92893b57&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwbmct/qwen3codernext_on_rtx_5060_ti_16_gb_some_numbers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwbmct/qwen3codernext_on_rtx_5060_ti_16_gb_some_numbers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwbmct/qwen3codernext_on_rtx_5060_ti_16_gb_some_numbers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T04:33:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwgyrn</id>
    <title>Best "Deep research" for local LLM in 2026 - platforms/tools/interface/setups</title>
    <updated>2026-02-05T09:39:18+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwgyrn/best_deep_research_for_local_llm_in_2026/"&gt; &lt;img alt="Best &amp;quot;Deep research&amp;quot; for local LLM in 2026 - platforms/tools/interface/setups" src="https://preview.redd.it/ffio9l5h0nhg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44645626277aa899f8e9caa47376f578527edc62" title="Best &amp;quot;Deep research&amp;quot; for local LLM in 2026 - platforms/tools/interface/setups" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using the &lt;strong&gt;Deep research&lt;/strong&gt; function from ChatGPT quite a lot since it came out.&lt;/p&gt; &lt;p&gt;I love it, but every month I use the limit in the first 2-3 days... so I was wondering if anyone else has any tips or setups they use for running something similar to Deep research -- on local LLM.&lt;/p&gt; &lt;p&gt;I have a decent setup of 3x3090, so I can run big-ish models (gpt-oss-120b or GLM Air) at VRAM speed or 30b models in Q8 (if precision is more important for deep research).&lt;/p&gt; &lt;p&gt;I've been using OpenWebUI + local SearXNG so fart. It works ok for simple &amp;quot;read this webpage and summarise&amp;quot; but it's far from the accuracy you get from a search&lt;strong&gt;analyze&lt;/strong&gt;search loop -- the way Deep research acts.&lt;/p&gt; &lt;p&gt;Any suggestions would help, thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ffio9l5h0nhg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwgyrn/best_deep_research_for_local_llm_in_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwgyrn/best_deep_research_for_local_llm_in_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T09:39:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwngbv</id>
    <title>OpenWebui + Ace Step 1.5</title>
    <updated>2026-02-05T14:57:36+00:00</updated>
    <author>
      <name>/u/iChrist</name>
      <uri>https://old.reddit.com/user/iChrist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwngbv/openwebui_ace_step_15/"&gt; &lt;img alt="OpenWebui + Ace Step 1.5" src="https://b.thumbs.redditmedia.com/epKePg4NU33-mx1EvCIYhJsukfYPYtcAcJVLZLyiqyA.jpg" title="OpenWebui + Ace Step 1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the new Ace-Step 1.5 music generation model and the awesome developer of the tools:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Haervwe/open-webui-tools"&gt;https://github.com/Haervwe/open-webui-tools&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With a beefy GPU (24GB) you can use a decent LLM like GPT-OSS:20b or Ministral alongside the full ace step model and generate music on the go!&lt;/p&gt; &lt;p&gt;I hope you guys found it awesome and star his github page, he has so many good tools for openwebui!&lt;/p&gt; &lt;p&gt;We are at a point where you can hook up Flux Klein for image generation and image editing, use ace step to create music, all with one interface, model with tool support are a game changer.&lt;/p&gt; &lt;p&gt;With all the other benefits like web search, computer use through playwright mcp, youtube summarizing or basically anything you need.&lt;/p&gt; &lt;p&gt;What competitive edge does ChatGPT and the likes still poses?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iChrist"&gt; /u/iChrist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qwngbv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwngbv/openwebui_ace_step_15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwngbv/openwebui_ace_step_15/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T14:57:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwo9j0</id>
    <title>We built an 8B world model that beats 402B Llama 4 by generating web code instead of pixels ‚Äî open weights on HF</title>
    <updated>2026-02-05T15:28:27+00:00</updated>
    <author>
      <name>/u/jshin49</name>
      <uri>https://old.reddit.com/user/jshin49</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwo9j0/we_built_an_8b_world_model_that_beats_402b_llama/"&gt; &lt;img alt="We built an 8B world model that beats 402B Llama 4 by generating web code instead of pixels ‚Äî open weights on HF" src="https://external-preview.redd.it/bmIycDZuMHYxcGhnMTkRUzZawZzMWm4JXBBoVayTVh3fNrkxvwbY4-FVurAN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=416b047c4576396c89a7eee16410255cdb27cd61" title="We built an 8B world model that beats 402B Llama 4 by generating web code instead of pixels ‚Äî open weights on HF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Here's something new for you: Mobile World Models.&lt;br /&gt; We just released gWorld ‚Äî open-weight visual world models for mobile GUIs (8B and 32B).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo Video Explanation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's gWorld 32B imagining a multi-step Booking dot com session ‚Äî zero access to the real app:&lt;br /&gt; 1. Sees flight search form (Detroit ‚Üí Chicago)&lt;br /&gt; 2. Click &amp;quot;Search&amp;quot; ‚Üí writes code ‚Üí renders full results page with airlines, prices, times&lt;br /&gt; 3. Click destination field ‚Üí predicts the search UI with history &lt;/p&gt; &lt;p&gt;Every screen = executable HTML/CSS/JS rendered to pixels.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The core idea:&lt;/strong&gt; Instead of predicting the next screen as pixels (diffusion, autoregressive image gen), gWorld predicts it as executable web code. You render the code, you get the image. This sounds simple but it works remarkably well because VLMs already have strong priors on structured web code from pre-training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why code instead of pixels?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text-based world models lose visual fidelity (can't represent layouts, colors, images)&lt;/li&gt; &lt;li&gt;Pixel-generation models hallucinate text and structural elements&lt;/li&gt; &lt;li&gt;Code generation gives you the best of both: precise text rendering from linguistic priors + high-fidelity visuals from structured code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results on MWMBench (6 benchmarks, 4 ID + 2 OOD):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Avg Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 VL&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;29.2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Scout&lt;/td&gt; &lt;td align="left"&gt;109B (A17B)&lt;/td&gt; &lt;td align="left"&gt;50.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Maverick&lt;/td&gt; &lt;td align="left"&gt;402B (A17B)&lt;/td&gt; &lt;td align="left"&gt;55.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 VL&lt;/td&gt; &lt;td align="left"&gt;235B (A22B)&lt;/td&gt; &lt;td align="left"&gt;51.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.6V&lt;/td&gt; &lt;td align="left"&gt;106B&lt;/td&gt; &lt;td align="left"&gt;67.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gWorld&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;74.9%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gWorld&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;32B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;79.6%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The 8B model beats everything up to 50√ó its size. Render failure rate is &amp;lt;1% (vs 40% for base Qwen3 VL 8B before our training).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other things worth noting:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data scaling follows a power law with R¬≤ ‚â• 0.94 ‚Äî gains are predictable and nowhere near saturating&lt;/li&gt; &lt;li&gt;We include a Korean apps benchmark (KApps) as OOD eval ‚Äî the models generalize well cross-lingually&lt;/li&gt; &lt;li&gt;The data pipeline is automated: repurpose existing trajectory data ‚Üí cross-modal relabeling to code ‚Üí synthetic reasoning traces&lt;/li&gt; &lt;li&gt;We also show that better world models ‚Üí better downstream GUI agent performance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why this matters beyond benchmarks:&lt;/strong&gt; The bottleneck for training GUI agents with online RL is device-policy coupling ‚Äî every rollout needs a real Android emulator. World models could decouple this entirely, enabling massively parallel rollouts on pure compute. gWorld is a step in that direction.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ü§ó gWorld 8B: &lt;a href="https://huggingface.co/trillionlabs/gWorld-8B"&gt;https://huggingface.co/trillionlabs/gWorld-8B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ü§ó gWorld 32B: &lt;a href="https://huggingface.co/trillionlabs/gWorld-32B"&gt;https://huggingface.co/trillionlabs/gWorld-32B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üíª Code: &lt;a href="https://github.com/trillion-labs/gWorld"&gt;https://github.com/trillion-labs/gWorld&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìÑ Paper: &lt;a href="https://huggingface.co/papers/2602.01576"&gt;https://huggingface.co/papers/2602.01576&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üåê Project page (and demos): &lt;a href="https://trillionlabs-gworld.github.io/"&gt;https://trillionlabs-gworld.github.io&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Benchmarks (incl. K-Apps) coming soon.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions.&lt;br /&gt; Built by Trillion Labs √ó KAIST AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshin49"&gt; /u/jshin49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/37uavl0v1phg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwo9j0/we_built_an_8b_world_model_that_beats_402b_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwo9j0/we_built_an_8b_world_model_that_beats_402b_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T15:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwobcc</id>
    <title>Strix Halo benchmarks: 13 models, 15 llama.cpp builds</title>
    <updated>2026-02-05T15:30:19+00:00</updated>
    <author>
      <name>/u/Beneficial-Shame-483</name>
      <uri>https://old.reddit.com/user/Beneficial-Shame-483</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwobcc/strix_halo_benchmarks_13_models_15_llamacpp_builds/"&gt; &lt;img alt="Strix Halo benchmarks: 13 models, 15 llama.cpp builds" src="https://preview.redd.it/feayylk82phg1.png?width=140&amp;amp;height=48&amp;amp;auto=webp&amp;amp;s=0f252b48bce559e0c572a43100cbd3ea8a9ccb86" title="Strix Halo benchmarks: 13 models, 15 llama.cpp builds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/feayylk82phg1.png?width=3469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd82806fb3743ba1b57c2ade12ef4d71e25679bf"&gt;https://preview.redd.it/feayylk82phg1.png?width=3469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd82806fb3743ba1b57c2ade12ef4d71e25679bf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ran a software ablation study on the Strix Halo's iGPU testing anything I could fine (ROCm, Vulkan, gfx version, hipblaslt on/off, rocWMMA, various Vulkan/RADV options) across different build configurations. Rather than fighting dependency hell to find &amp;quot;the&amp;quot; working setup, I dockerized 15 different llama.cpp builds and let them all run. Some failed but that's ok, that's data too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://whylucian.github.io/softab/results-tables/results.html"&gt;https://whylucian.github.io/softab/results-tables/results.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beneficial-Shame-483"&gt; /u/Beneficial-Shame-483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwobcc/strix_halo_benchmarks_13_models_15_llamacpp_builds/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwobcc/strix_halo_benchmarks_13_models_15_llamacpp_builds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwobcc/strix_halo_benchmarks_13_models_15_llamacpp_builds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T15:30:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwboqn</id>
    <title>Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy</title>
    <updated>2026-02-05T04:37:05+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwboqn/google_research_announces_sequential_attention/"&gt; &lt;img alt="Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy" src="https://external-preview.redd.it/Xfy8b5oz8xAgNpbj0L9Mmjzxactj5HdaKRFOmBPu0YE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9dad5b13e20f57d64f5fc0bbc7415c9f4186b1d" title="Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.google/blog/sequential-attention-making-ai-models-leaner-and-faster-without-sacrificing-accuracy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwboqn/google_research_announces_sequential_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwboqn/google_research_announces_sequential_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T04:37:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
