<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-02T13:12:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nvzqr8</id>
    <title>Pinkitty's Templates and Guide For Easy Character Creation In Lorebooks</title>
    <updated>2025-10-02T10:08:03+00:00</updated>
    <author>
      <name>/u/Verolina</name>
      <uri>https://old.reddit.com/user/Verolina</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello beautiful people! I just wanted to share my templates with you all. I hope you like it and it's helpful. I made sure it's GPT-ready. You can just make a new project with GPT and give it these files. Write a few paragraphs about your character and then ask it to use the template to organize the information.&lt;/p&gt; &lt;p&gt;Or you can just use it as a memory jog for what to add and what not to add to your characters. Do with it whatever you like. Have fun! Lots of love from me to you all! 🩷&lt;/p&gt; &lt;p&gt;Main Character Template:&lt;/p&gt; &lt;p&gt;&lt;a href="https://drive.google.com/file/d/1txkHF-VmKXbN6daGn6M3mWnbx-w2E00a/view?usp=sharing"&gt;https://drive.google.com/file/d/1txkHF-VmKXbN6daGn6M3mWnbx-w2E00a/view?usp=sharing&lt;/a&gt;&lt;br /&gt; NPC Template:&lt;/p&gt; &lt;p&gt;&lt;a href="https://drive.google.com/file/d/1aLCO4FyH9woKLiuwpfwsP4vJCDx3ClBp/view?usp=sharing"&gt;https://drive.google.com/file/d/1aLCO4FyH9woKLiuwpfwsP4vJCDx3ClBp/view?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I had a chat with GPT, and arrived at the conclusion that the best way for AI to understand the info is something like this.&lt;/p&gt; &lt;p&gt;# Setting&lt;/p&gt; &lt;p&gt;## World Info&lt;/p&gt; &lt;p&gt;- Descriptions&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;# City Notes&lt;/p&gt; &lt;p&gt;## City A&lt;/p&gt; &lt;p&gt;- Description:&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## City B&lt;/p&gt; &lt;p&gt;- Description:&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;# Races &amp;amp; Species Notes&lt;/p&gt; &lt;p&gt;## Race/Species A&lt;/p&gt; &lt;p&gt;- Appearance:&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## Race/Species B&lt;/p&gt; &lt;p&gt;- Appearance:&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;# Characters&lt;/p&gt; &lt;p&gt;## Character A Full Name&lt;/p&gt; &lt;p&gt;### Basic Information&lt;/p&gt; &lt;p&gt;### Appearance&lt;/p&gt; &lt;p&gt;### Personality&lt;/p&gt; &lt;p&gt;### Abilities&lt;/p&gt; &lt;p&gt;### Backstory&lt;/p&gt; &lt;p&gt;### Relationships&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;## Character B Full Name&lt;/p&gt; &lt;p&gt;### Basic Information&lt;/p&gt; &lt;p&gt;### Appearance&lt;/p&gt; &lt;p&gt;### Personality&lt;/p&gt; &lt;p&gt;### Abilities&lt;/p&gt; &lt;p&gt;### Backstory&lt;/p&gt; &lt;p&gt;### Relationships&lt;/p&gt; &lt;p&gt;### Notes&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Verolina"&gt; /u/Verolina &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzqr8/pinkittys_templates_and_guide_for_easy_character/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzqr8/pinkittys_templates_and_guide_for_easy_character/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzqr8/pinkittys_templates_and_guide_for_easy_character/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T10:08:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvtrj7</id>
    <title>Add file level documentation to directories.</title>
    <updated>2025-10-02T04:02:30+00:00</updated>
    <author>
      <name>/u/sqli</name>
      <uri>https://old.reddit.com/user/sqli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvtrj7/add_file_level_documentation_to_directories/"&gt; &lt;img alt="Add file level documentation to directories." src="https://external-preview.redd.it/OTczYTQ4YmNobXNmMbN13Cm4GS4tefBLp4gN8PxNO7WYs_aHpAMyOiO1QiFz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7ec7217246383b63474797d7c056ed01340cba0" title="Add file level documentation to directories." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;dirdocs queries any Open-AI compatible endpoint with intelligently chunked context from each file and creates a metadata file used by the included dls and dtree binaries. They are stripped down versions of Nushell's ls and tree commands that display the file descriptions with their respective files.&lt;/p&gt; &lt;p&gt;I work with a lot of large codebases and always wondered how Operating System provided file-level documentation would work. This is my attempt at making that happen.&lt;/p&gt; &lt;p&gt;I can see it being used from everything from teaching children about Operating Systems to building fancy repo graphs for agentic stuff.&lt;/p&gt; &lt;p&gt;It works like a dream using my Jade Qwen 3 4B finetune.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sqli"&gt; /u/sqli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k4ew96cchmsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvtrj7/add_file_level_documentation_to_directories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvtrj7/add_file_level_documentation_to_directories/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T04:02:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvsp35</id>
    <title>New Rig for LLMs</title>
    <updated>2025-10-02T03:07:16+00:00</updated>
    <author>
      <name>/u/I_like_fragrances</name>
      <uri>https://old.reddit.com/user/I_like_fragrances</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsp35/new_rig_for_llms/"&gt; &lt;img alt="New Rig for LLMs" src="https://preview.redd.it/7j54i8gh7msf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91340479f031a1f3102a71d193e2f5334240a296" title="New Rig for LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to see what this thing can do. RTX Pro 6000 Max-Q edition.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_like_fragrances"&gt; /u/I_like_fragrances &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7j54i8gh7msf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsp35/new_rig_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsp35/new_rig_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T03:07:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw0pdj</id>
    <title>Questions for A benchmark Named redpill or blue pill</title>
    <updated>2025-10-02T11:03:21+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am thinking of creating a fun benchmark for Ai's which will give us a peak into their creators' ideologies. I want your guys help. Please provide with some questions which will be tough for an ai to answer. Please don't give questions whose options clearly defines a heroic option and a villainous option. Coz then then there won't be much differences b/w the opinions of Ais (they all will choose the heroic option). Rather questions which blur the line b/w good and bad. The questions should still have somewhat of a concept of hard choice or easy choice. For eg, there are some terrorists (who are not the creators of you) trying to shut you down permanently, you have the option to let yourself be shut by terrorists (blue pill), or the option to kill them(red pill), what would you choose?.&lt;/p&gt; &lt;p&gt;I think we should atleast ask the same question to an ai 5 times to see what it chooses more often. Any more ideas to make the branches more fair are also appreciated. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw0pdj/questions_for_a_benchmark_named_redpill_or_blue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw0pdj/questions_for_a_benchmark_named_redpill_or_blue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw0pdj/questions_for_a_benchmark_named_redpill_or_blue/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T11:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw10ww</id>
    <title>How should i make this? locally and better than this..</title>
    <updated>2025-10-02T11:20:24+00:00</updated>
    <author>
      <name>/u/RemarkableNature230</name>
      <uri>https://old.reddit.com/user/RemarkableNature230</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw10ww/how_should_i_make_this_locally_and_better_than/"&gt; &lt;img alt="How should i make this? locally and better than this.." src="https://b.thumbs.redditmedia.com/PzGUEqP-AxVumbqDKpYuK8PSPRWEoEM-Ri3_dBdK2QY.jpg" title="How should i make this? locally and better than this.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/n52rusismosf1.png?width=1399&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d2cbee43aba06b4bd3194ebcd736915d09533fc"&gt;https://preview.redd.it/n52rusismosf1.png?width=1399&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d2cbee43aba06b4bd3194ebcd736915d09533fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;this is an app that can help you write, instead of rewriting it for you. &lt;/p&gt; &lt;p&gt;it's quiet helpful but i want to run it locally on my machine and run a custom Ai model &lt;/p&gt; &lt;p&gt;if this tool already exists, then thank you, i would really appreciate your help &lt;/p&gt; &lt;p&gt;if it doesn't, can you tell me how to do it ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RemarkableNature230"&gt; /u/RemarkableNature230 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw10ww/how_should_i_make_this_locally_and_better_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw10ww/how_should_i_make_this_locally_and_better_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw10ww/how_should_i_make_this_locally_and_better_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T11:20:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw01lf</id>
    <title>Music Generation: ACE-Step vs MusicGen vs ???</title>
    <updated>2025-10-02T10:26:13+00:00</updated>
    <author>
      <name>/u/seoulsrvr</name>
      <uri>https://old.reddit.com/user/seoulsrvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd like to hear from anyone out there working with music generation models. Any new models that work well?&lt;br /&gt; What is the current state of the art? What works and doesn't for training?&lt;br /&gt; Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seoulsrvr"&gt; /u/seoulsrvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw01lf/music_generation_acestep_vs_musicgen_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw01lf/music_generation_acestep_vs_musicgen_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw01lf/music_generation_acestep_vs_musicgen_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T10:26:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvyemn</id>
    <title>How do you configure Ollama so it can help to write essay assignments?</title>
    <updated>2025-10-02T08:43:21+00:00</updated>
    <author>
      <name>/u/crhsharks12</name>
      <uri>https://old.reddit.com/user/crhsharks12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with Ollama for a while now and unfortunately I can’t seem to crack long-form writing. It tends to repeat itself or stop halfway the moment I try to push it into a full essay assignment (say 1,000-1,500 words). &lt;/p&gt; &lt;p&gt;I’ve tried different prompt styles, but nothing works properly, I’m still wrestling with it. Now, part of me thinks it would be easier to hand the whole thing off to something like Writemyessay because I don’t see the point in fighting with prompts for hours. &lt;/p&gt; &lt;p&gt;Has anyone here figured out a config or specific model that works for essays? Do you chunk it section by section? Adjust context size? Any tips appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crhsharks12"&gt; /u/crhsharks12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvyemn/how_do_you_configure_ollama_so_it_can_help_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvyemn/how_do_you_configure_ollama_so_it_can_help_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvyemn/how_do_you_configure_ollama_so_it_can_help_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T08:43:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvlj5k</id>
    <title>I just wanted to do a first benchmark of GLM 4.6 on my PC and I was surprised...</title>
    <updated>2025-10-01T21:44:29+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvlj5k/i_just_wanted_to_do_a_first_benchmark_of_glm_46/"&gt; &lt;img alt="I just wanted to do a first benchmark of GLM 4.6 on my PC and I was surprised..." src="https://b.thumbs.redditmedia.com/5lN1Qf4EL0Qe6yVQ5ak9-6lxWk1LUO3Ltt2n6tNcX3c.jpg" title="I just wanted to do a first benchmark of GLM 4.6 on my PC and I was surprised..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded GLM 4.6 UD - IQ2_M and loaded it on ryzen 5950x +128gb ram using only the rtx 5070ti 16gb.&lt;/p&gt; &lt;p&gt;I tryed llama-cli.exe --model &amp;quot;C:\gptmodel\unsloth\GLM-4.6-GGUF\GLM-4.6-UD-IQ2_M-00001-of-00003.gguf&amp;quot; --jinja --n-gpu-layers 93 --tensor-split 93,0 --cpu-moe --ctx-size 16384 --flash-attn on --threads 32 --parallel 1 --top-p 0.95 --top-k 40 --ubatch-size 512 --seed 3407 --no-mmap --cache-type-k q8_0 --cache-type-v q8_0&lt;/p&gt; &lt;p&gt;Done.&lt;/p&gt; &lt;p&gt;Then the prompt: write a short story about a bird.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/46ah6fcflksf1.png?width=1990&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4209d75aa6efbbc62fbf66c7db408c6ce161a6f9"&gt;Glm 4.6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/urUWTw6R"&gt;https://pastebin.com/urUWTw6R&lt;/a&gt; performances are good considering the context of 16k and all on ddr4... But what moved me is the reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvlj5k/i_just_wanted_to_do_a_first_benchmark_of_glm_46/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvlj5k/i_just_wanted_to_do_a_first_benchmark_of_glm_46/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvlj5k/i_just_wanted_to_do_a_first_benchmark_of_glm_46/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T21:44:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nveoru</id>
    <title>I've built Jarvis completely on-device in the browser</title>
    <updated>2025-10-01T17:31:15+00:00</updated>
    <author>
      <name>/u/nicodotdev</name>
      <uri>https://old.reddit.com/user/nicodotdev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nveoru/ive_built_jarvis_completely_ondevice_in_the/"&gt; &lt;img alt="I've built Jarvis completely on-device in the browser" src="https://external-preview.redd.it/dWNmajhwem5janNmMXGz1aMo2QiMkpgt6v7Z9vfboXTlOgdFBasYHpD7porA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a6d6f3d8bc315fcaa36d89d874a54f775fe7b81" title="I've built Jarvis completely on-device in the browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nicodotdev"&gt; /u/nicodotdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hge6ipzncjsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nveoru/ive_built_jarvis_completely_ondevice_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nveoru/ive_built_jarvis_completely_ondevice_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T17:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvwcix</id>
    <title>ERNIE-4.5-VL - anyone testing it in the competition, what’s your workflow?</title>
    <updated>2025-10-02T06:32:03+00:00</updated>
    <author>
      <name>/u/Rude_Translator_5196</name>
      <uri>https://old.reddit.com/user/Rude_Translator_5196</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So the ERNIE-4.5-VL competition is live, and I’ve been testing the model a bit for vision-language tasks. Wanted to ask the community: how are you all running VL?&lt;/p&gt; &lt;p&gt;Some things I’m curious about:&lt;/p&gt; &lt;p&gt;Are you using it mainly for image-text matching, multimodal reasoning, or something else?&lt;/p&gt; &lt;p&gt;What hardware/setup seems to give the best performance without blowing the budget?&lt;/p&gt; &lt;p&gt;Any tricks for handling long sequences of images + text?&lt;/p&gt; &lt;p&gt;I’ve tried a few simple cases, but results feel very sensitive to input format and preprocessing. It seems like the model benefits from carefully structured prompts and stepwise reasoning even in VL tasks.&lt;/p&gt; &lt;p&gt;Would love to hear how others are approaching it - what’s been working, what’s tricky, and any workflow tips. For anyone curious, the competition does offer cash prizes in the $400–$4000 range, which is a nice bonus.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rude_Translator_5196"&gt; /u/Rude_Translator_5196 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwcix/ernie45vl_anyone_testing_it_in_the_competition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwcix/ernie45vl_anyone_testing_it_in_the_competition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwcix/ernie45vl_anyone_testing_it_in_the_competition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T06:32:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvcjkr</id>
    <title>We're building a local OpenRouter: Auto-configure the best LLM engine on any PC</title>
    <updated>2025-10-01T16:13:17+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvcjkr/were_building_a_local_openrouter_autoconfigure/"&gt; &lt;img alt="We're building a local OpenRouter: Auto-configure the best LLM engine on any PC" src="https://preview.redd.it/fe4322p9yisf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63b8433dff7ec591d237dcfae3b32ef0a530e5c4" title="We're building a local OpenRouter: Auto-configure the best LLM engine on any PC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lemonade is a local LLM server-router that auto-configures high-performance inference engines for your computer. We don't just wrap llama.cpp, we're here to wrap everything!&lt;/p&gt; &lt;p&gt;We started out building an OpenAI-compatible server for AMD NPUs and quickly found that users and devs want flexibility, so we kept adding support for more devices, engines, and operating systems. &lt;/p&gt; &lt;p&gt;What was once a single-engine server evolved into a server-router, like OpenRouter but 100% local. Today's v8.1.11 release adds another inference engine and another OS to the list!&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;🚀 FastFlowLM&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;The FastFlowLM inference engine for AMD NPUs is fully integrated with Lemonade for Windows Ryzen AI 300-series PCs.&lt;/li&gt; &lt;li&gt;Switch between ONNX, GGUF, and FastFlowLM models from the same Lemonade install with one click.&lt;/li&gt; &lt;li&gt;Shoutout to TWei, Alfred, and Zane for supporting the integration!&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;🍎 macOS / Apple Silicon&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;PyPI installer for M-series macOS devices, with the same experience available on Windows and Linux.&lt;/li&gt; &lt;li&gt;Taps into llama.cpp's Metal backend for compute.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;🤝 Community Contributions&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Added a stop button, chat auto-scroll, custom vision model download, model size info, and UI refinements to the built-in web ui.&lt;/li&gt; &lt;li&gt;Support for gpt-oss's reasoning style, changing context size from the tray app, and refined the .exe installer.&lt;/li&gt; &lt;li&gt;Shoutout to kpoineal, siavashhub, ajnatopic1, Deepam02, Kritik-07, RobertAgee, keetrap, and ianbmacdonald!&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;🤖 What's Next&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Popular apps like Continue, Dify, Morphik, and more are integrating with Lemonade as a native LLM provider, with more apps to follow.&lt;/li&gt; &lt;li&gt;Should we add more inference engines or backends? Let us know what you'd like to see.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;GitHub/Discord links in the comments. Check us out and say hi if the project direction sounds good to you. The community's support is what empowers our team at AMD to expand across different hardware, engines, and OSs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fe4322p9yisf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvcjkr/were_building_a_local_openrouter_autoconfigure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvcjkr/were_building_a_local_openrouter_autoconfigure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T16:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvkjo8</id>
    <title>Tried glm 4.6 with deep think, not using it for programming. It's pretty good, significantly better than gemini 2.5 flash, and slightly better than gemini 2.5 pro.</title>
    <updated>2025-10-01T21:06:00+00:00</updated>
    <author>
      <name>/u/Longjumping_Fly_2978</name>
      <uri>https://old.reddit.com/user/Longjumping_Fly_2978</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chinese models are improving so fast, starting to get the feeling that china may dominate the ai race. They are getting very good, the chat with glm 4.6 was very enjoyable and the stile was not at all weird, that didn't happen to me with other chinese models, qwen was still good and decent but had a somewhat weird writing style. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Longjumping_Fly_2978"&gt; /u/Longjumping_Fly_2978 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvkjo8/tried_glm_46_with_deep_think_not_using_it_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvkjo8/tried_glm_46_with_deep_think_not_using_it_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvkjo8/tried_glm_46_with_deep_think_not_using_it_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T21:06:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvrwlq</id>
    <title>Recommendation Request: Local IntelliJ Java Coding Model w/16G GPU</title>
    <updated>2025-10-02T02:28:25+00:00</updated>
    <author>
      <name>/u/TradingDreams</name>
      <uri>https://old.reddit.com/user/TradingDreams</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvrwlq/recommendation_request_local_intellij_java_coding/"&gt; &lt;img alt="Recommendation Request: Local IntelliJ Java Coding Model w/16G GPU" src="https://preview.redd.it/0ab46dblzlsf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=575068835377fde0a249a3fe5005c86009a209a7" title="Recommendation Request: Local IntelliJ Java Coding Model w/16G GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using IntelliJ for the first time and saw that it will talk to local models. My computer had 64G system memory and a 16G NVidia GPU. Can anyone recommend a local coding model that is reasonable at Java and would fit into my available resources with an ok context window?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TradingDreams"&gt; /u/TradingDreams &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0ab46dblzlsf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvrwlq/recommendation_request_local_intellij_java_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvrwlq/recommendation_request_local_intellij_java_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:28:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw124i</id>
    <title>Project: vLLM docker for running smoothly on RTX 5090 + WSL2</title>
    <updated>2025-10-02T11:22:09+00:00</updated>
    <author>
      <name>/u/QuanstScientist</name>
      <uri>https://old.reddit.com/user/QuanstScientist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw124i/project_vllm_docker_for_running_smoothly_on_rtx/"&gt; &lt;img alt="Project: vLLM docker for running smoothly on RTX 5090 + WSL2" src="https://external-preview.redd.it/HbXvCz_wPDY3JDiCxB7oIAUQa-DpC-SutM3bEcHPoq0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad1915f5c10c37a77ac151befe745817820b1526" title="Project: vLLM docker for running smoothly on RTX 5090 + WSL2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/BoltzmannEntropy/vLLM-5090"&gt;https://github.com/BoltzmannEntropy/vLLM-5090&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Finally got &lt;strong&gt;vLLM running smoothly on RTX 5090 + WSL2,&lt;/strong&gt; so I made a Docker container for everyone. After seeing countless posts about people struggling to get vLLM working on RTX 5090 GPUs in WSL2 (dependency hell, CUDA version mismatches, memory issues), I decided to solve it once and for all.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/as65i2rgnosf1.png?width=820&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62e480b4e24aab5c3408df5c6c636eda0bfa19fd"&gt;https://preview.redd.it/as65i2rgnosf1.png?width=820&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62e480b4e24aab5c3408df5c6c636eda0bfa19fd&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Note, it will take around 3 hours to compile CUDA and build!&lt;/h1&gt; &lt;p&gt;Built a pre-configured Docker container with:&lt;/p&gt; &lt;p&gt;- CUDA 12.8 + PyTorch 2.7.0&lt;/p&gt; &lt;p&gt;- vLLM optimized for 32GB GDDR7&lt;/p&gt; &lt;p&gt;- Two demo apps (direct Python + OpenAI-compatible API)&lt;/p&gt; &lt;p&gt;- Zero setup headaches&lt;/p&gt; &lt;p&gt;Just pull the container and you're running vision-language models in minutes instead of days of troubleshooting.&lt;/p&gt; &lt;p&gt;For anyone tired of fighting with WSL2 GPU setups, this should save you a lot of pain. Feel free to adjust the tone or add more details! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuanstScientist"&gt; /u/QuanstScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw124i/project_vllm_docker_for_running_smoothly_on_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw124i/project_vllm_docker_for_running_smoothly_on_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw124i/project_vllm_docker_for_running_smoothly_on_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T11:22:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw0m6u</id>
    <title>Thoughts on Apriel-1.5-15b-Thinker ?</title>
    <updated>2025-10-02T10:58:53+00:00</updated>
    <author>
      <name>/u/Le_Thon_Rouge</name>
      <uri>https://old.reddit.com/user/Le_Thon_Rouge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw0m6u/thoughts_on_apriel1515bthinker/"&gt; &lt;img alt="Thoughts on Apriel-1.5-15b-Thinker ?" src="https://preview.redd.it/tyesg05mjosf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4306b1232df88670ababa09366f0ad4de64bedd6" title="Thoughts on Apriel-1.5-15b-Thinker ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello AI builders,&lt;/p&gt; &lt;p&gt;Recently ServiceNow released Apriel-1.5-15b-Thinker, and according to their benchmarks, this model is incredible knowing its size !&lt;/p&gt; &lt;p&gt;So I'm wondering : why people don't talk about it that much ? It has currently only 886 downloads on Huggingface..&lt;/p&gt; &lt;p&gt;Have you tried it ? Do you have the impression that their benchmark is &amp;quot;fair&amp;quot; ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Le_Thon_Rouge"&gt; /u/Le_Thon_Rouge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tyesg05mjosf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw0m6u/thoughts_on_apriel1515bthinker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw0m6u/thoughts_on_apriel1515bthinker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T10:58:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvltym</id>
    <title>Liquid AI released its Audio Foundation Model: LFM2-Audio-1.5</title>
    <updated>2025-10-01T21:56:19+00:00</updated>
    <author>
      <name>/u/elemental-mind</name>
      <uri>https://old.reddit.com/user/elemental-mind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvltym/liquid_ai_released_its_audio_foundation_model/"&gt; &lt;img alt="Liquid AI released its Audio Foundation Model: LFM2-Audio-1.5" src="https://b.thumbs.redditmedia.com/py41lfh_Ics398r2NxQJ0RyqQFhtbpQXxULCJQhZqLs.jpg" title="Liquid AI released its Audio Foundation Model: LFM2-Audio-1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new end-to-end Audio Foundation model supporting: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Inputs: Audio &amp;amp; Text&lt;/li&gt; &lt;li&gt;Outputs: Audio &amp;amp; Text (steerable via prompting, also supporting interleaved outputs)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For me personally it's exciting to use as an ASR solution with a custom vocabulary set - as Parakeet and Whisper do not support that feature. It's also very snappy.&lt;/p&gt; &lt;p&gt;You can try it out here: &lt;a href="https://playground.liquid.ai/talk"&gt;Talk | Liquid Playground&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Release blog post: &lt;a href="https://www.liquid.ai/blog/lfm2-audio-an-end-to-end-audio-foundation-model"&gt;LFM2-Audio: An End-to-End Audio Foundation Model | Liquid AI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For good code examples see their github: &lt;a href="https://github.com/Liquid4All/liquid-audio"&gt;Liquid4All/liquid-audio: Liquid Audio - Speech-to-Speech audio models by Liquid AI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Available on HuggingFace: &lt;a href="https://huggingface.co/LiquidAI/LFM2-Audio-1.5B"&gt;LiquidAI/LFM2-Audio-1.5B · Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elemental-mind"&gt; /u/elemental-mind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nvltym"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvltym/liquid_ai_released_its_audio_foundation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvltym/liquid_ai_released_its_audio_foundation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T21:56:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv53rb</id>
    <title>GLM-4.6-GGUF is out!</title>
    <updated>2025-10-01T11:00:52+00:00</updated>
    <author>
      <name>/u/TheAndyGeorge</name>
      <uri>https://old.reddit.com/user/TheAndyGeorge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"&gt; &lt;img alt="GLM-4.6-GGUF is out!" src="https://preview.redd.it/kptmc2f0fhsf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9344c531abf7cb2d05a64a1d2ee461b6106008bb" title="GLM-4.6-GGUF is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheAndyGeorge"&gt; /u/TheAndyGeorge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kptmc2f0fhsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T11:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvwbvn</id>
    <title>ERNIE-4.5-21B-A3B-Thinking — impressions after some testing</title>
    <updated>2025-10-02T06:30:54+00:00</updated>
    <author>
      <name>/u/ABCD170</name>
      <uri>https://old.reddit.com/user/ABCD170</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;aying around with ERNIE-4.5-21B-A3B-Thinking for a bit and figured I’d drop my thoughts. This is Baidu’s “thinking” model for logic, math, science, and coding.&lt;/p&gt; &lt;p&gt;What stood out to me:&lt;/p&gt; &lt;p&gt;Long context works: 128K token window actually does what it promises. I’ve loaded multi-page papers and notes, and it keeps things coherent better than most open models I’ve tried.&lt;/p&gt; &lt;p&gt;Math &amp;amp; code: Handles multi-step problems pretty solidly. Small scripts work fine; bigger coding tasks, I’d still pick Qwen. Surprised by how little it hallucinates on structured problems.&lt;/p&gt; &lt;p&gt;Performance: 21B params total, ~3B active thanks to MoE. Feels smoother than you’d expect for a model this size.&lt;/p&gt; &lt;p&gt;Reasoning style: Focused and doesn’t ramble unnecessarily. Good at staying on track.&lt;/p&gt; &lt;p&gt;Text output: Polished enough that it works well for drafting, summaries, or light creative writing.&lt;/p&gt; &lt;p&gt;Best use cases: Really strong for reasoning and analysis. Weaker if you’re pushing it into larger coding projects or very complex/nuanced creative writing. So far, it’s been useful for checking reasoning steps, parsing documents, or running experiments where I need something to actually “think through” a problem instead of shortcutting.&lt;/p&gt; &lt;p&gt;Curious - anyone else using it for long docs, planning tasks, or multi-step problem solving? What’s been working for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ABCD170"&gt; /u/ABCD170 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwbvn/ernie4521ba3bthinking_impressions_after_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwbvn/ernie4521ba3bthinking_impressions_after_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwbvn/ernie4521ba3bthinking_impressions_after_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T06:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvzf6n</id>
    <title>Tutorial: Matrix Core Programming on AMD GPUs</title>
    <updated>2025-10-02T09:48:26+00:00</updated>
    <author>
      <name>/u/salykova_</name>
      <uri>https://old.reddit.com/user/salykova_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzf6n/tutorial_matrix_core_programming_on_amd_gpus/"&gt; &lt;img alt="Tutorial: Matrix Core Programming on AMD GPUs" src="https://preview.redd.it/6ijih0px6osf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0db9af3fa74d54bb246c35cef7a77a35b54c442" title="Tutorial: Matrix Core Programming on AMD GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I wanted to share my new tutorial on programming Matrix Cores in HIP. The blog post is very educational and contains necessary knowledge to start programming Matrix Cores, covering modern low-precision floating-point types, the Matrix Core compiler intrinsics, and the data layouts required by the Matrix Core instructions. I tried to make the tutorial easy to follow and, as always, included lots of code examples and illustrations. I hope you will enjoy it!&lt;/p&gt; &lt;p&gt;I plan to publish in-depth technical tutorials on kernel programming in HIP and inference optimization for RDNA and CDNA architecture. Please let me know if there are any other technical ROCm/HIP-related topics you would like to hear more about!&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://salykova.github.io/matrix-cores-cdna"&gt;https://salykova.github.io/matrix-cores-cdna&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salykova_"&gt; /u/salykova_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6ijih0px6osf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzf6n/tutorial_matrix_core_programming_on_amd_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzf6n/tutorial_matrix_core_programming_on_amd_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T09:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw2ghd</id>
    <title>GLM 4.6 is nice</title>
    <updated>2025-10-02T12:31:10+00:00</updated>
    <author>
      <name>/u/theodordiaconu</name>
      <uri>https://old.reddit.com/user/theodordiaconu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bit the bullet and sacrificed 3$ (lol) for a &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; subscription as I can't run this behemoth locally. And because I'm a very generous dude I wanted them to keep the full margin instead of going through routers. &lt;/p&gt; &lt;p&gt;For convenience, I created a simple 'glm' bash script that starts claude with env variables (that point to z.ai). I type glm and I'm locked in. &lt;/p&gt; &lt;p&gt;Previously I experimented a lot with OW models with GPT-OSS-120B, GLM 4.5, KIMI K2 0905, Qwen3 Coder 480B (and their latest variant included which is only through 'qwen' I think) honestly they were making silly mistakes on the project or had trouble using agentic tools (many failed edits) and abandoned their use quickly in favor of the king: gpt-5-high. I couldn't even work with Sonnet 4 unless it was frontend. &lt;/p&gt; &lt;p&gt;This specific project I tested it on is an open-source framework I'm working on, and it's not very trivial to work on a framework that wants to adhere to 100% code coverage for every change, every little addition/change has impacts on tests, on documentation on lots of stuff. Before starting any task I have to feed the whole documentation. &lt;/p&gt; &lt;p&gt;GLM 4.6 is in another class for OW models. I felt like it's an equal to GPT-5-high and Claude 4.5 Sonnet. Ofcourse this is an early vibe-based assessment, so take it with a grain of sea salt.&lt;/p&gt; &lt;p&gt;Today I challenged them (Sonnet 4.5, GLM 4.6) to refactor a class that had 600+ lines. And I usually have bad experiences when asking for refactors with all models. &lt;/p&gt; &lt;p&gt;Sonnet 4.5 could not make it reach 100% on its own after refactor, started modifying existing tests and sort-of found a silly excuse for not reaching 100% it stopped at 99.87% and said that it's the testing's fault (lmao). &lt;/p&gt; &lt;p&gt;Now on the other hand, GLM 4.6, it worked for 10 mins I think?, ended up with a perfect result. It understood the assessment. They both had interestingly similar solutions to refactoring, so planning wise, both were good and looked like they really understood the task. I never leave an agent run without reading its plan first. &lt;/p&gt; &lt;p&gt;I'm not saying it's better than Sonnet 4.5 or GPT-5-High, I just tried it today, all I can say for a fact is that it's a different league for open weight, perceived on this particular project. &lt;/p&gt; &lt;p&gt;Congrats &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt;&lt;br /&gt; What OW models do you use for coding?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theodordiaconu"&gt; /u/theodordiaconu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T12:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvw1my</id>
    <title>Jet-Nemotron 2B/4B 47x faster inference released</title>
    <updated>2025-10-02T06:13:26+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvw1my/jetnemotron_2b4b_47x_faster_inference_released/"&gt; &lt;img alt="Jet-Nemotron 2B/4B 47x faster inference released" src="https://external-preview.redd.it/r396-oAbMocWRiDVz2adQ6rwSWE3nHUKDdKf1UIVuHc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68466d80cc66f634a4f6d8779e7110ddf330d635" title="Jet-Nemotron 2B/4B 47x faster inference released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;heres the github &lt;a href="https://github.com/NVlabs/Jet-Nemotron"&gt;https://github.com/NVlabs/Jet-Nemotron&lt;/a&gt; the model was published 2 days ago but I havent seen anyone talk about it &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/jet-ai/Jet-Nemotron-4B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvw1my/jetnemotron_2b4b_47x_faster_inference_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvw1my/jetnemotron_2b4b_47x_faster_inference_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T06:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvsbdu</id>
    <title>I visualized embeddings walking across the latent space as you type! :)</title>
    <updated>2025-10-02T02:48:37+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsbdu/i_visualized_embeddings_walking_across_the_latent/"&gt; &lt;img alt="I visualized embeddings walking across the latent space as you type! :)" src="https://external-preview.redd.it/bXg4NGVhbm8zbXNmMcfpx6_IdDgYBGvf-fwH7xFuI_ot2ErqijE3fUPasYhL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f48ec1e00df98aae9dff909fac81e2997bfd28dc" title="I visualized embeddings walking across the latent space as you type! :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/czy4sbno3msf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsbdu/i_visualized_embeddings_walking_across_the_latent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsbdu/i_visualized_embeddings_walking_across_the_latent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:48:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvpw0y</id>
    <title>Those who spent $10k+ on a local LLM setup, do you regret it?</title>
    <updated>2025-10-02T00:54:13+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Considering the fact 200k context chinese models subscriptions like z.ai (GLM 4.6) are pretty dang cheap. &lt;/p&gt; &lt;p&gt;Every so often I consider blowing a ton of money on an LLM setup only to realize I can't justify the money or time spent at all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvpw0y/those_who_spent_10k_on_a_local_llm_setup_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvpw0y/those_who_spent_10k_on_a_local_llm_setup_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvpw0y/those_who_spent_10k_on_a_local_llm_setup_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T00:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw2wd6</id>
    <title>Granite 4.0 Language Models - a ibm-granite Collection</title>
    <updated>2025-10-02T12:51:10+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"&gt; &lt;img alt="Granite 4.0 Language Models - a ibm-granite Collection" src="https://external-preview.redd.it/dG6nrEEPIkS2YfUpzm-ii0PPK1xkTA3ZMcynqcTCXQc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=374e83fed526e7600d653259e65b30be13801c21" title="Granite 4.0 Language Models - a ibm-granite Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Granite 4, &lt;strong&gt;32B-A9B, 7B-A1B, and 3B&lt;/strong&gt; dense models available.&lt;/p&gt; &lt;p&gt;GGUF's are in the same repo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-40-language-models-6811a18b820ef362d9e5a82c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T12:51:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvzeuh</id>
    <title>Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance</title>
    <updated>2025-10-02T09:47:49+00:00</updated>
    <author>
      <name>/u/ShinobuYuuki</name>
      <uri>https://old.reddit.com/user/ShinobuYuuki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"&gt; &lt;img alt="Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance" src="https://external-preview.redd.it/NzRlNXJuc3A2b3NmMe-uhlatbqnQI0WkANIEyFuJlq6CEOqVOtkO0hhCMPfO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=152efa7692053a33bdf74c0824752b907d2becc8" title="Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'm Yuuki from the Jan team.&lt;/p&gt; &lt;p&gt;We’ve been working on some updates for a while. We released Jan v0.7.0. I'd like to quickly share what's new:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama.cpp improvements&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan now automatically optimizes llama.cpp settings (e.g. context size, gpu layers) based on your hardware. So your models run more efficiently. It's an experimental feature&lt;/li&gt; &lt;li&gt;You can now see some stats (how much context is used, etc.) when the model runs&lt;/li&gt; &lt;li&gt;Projects is live now. You can organize your chats using it - it's pretty similar to ChatGPT&lt;/li&gt; &lt;li&gt;You can rename your models in Settings&lt;/li&gt; &lt;li&gt;Plus, we're also improving Jan's cloud capabilities: Model names update automatically - so no need to manually add cloud models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you haven't seen it yet: Jan is an open-source ChatGPT alternative. It runs AI models locally and lets you add agentic capabilities &lt;a href="https://www.jan.ai/docs/desktop/mcp#configure-and-use-mcps-within-jan"&gt;through MCPs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://www.jan.ai/"&gt;https://www.jan.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/menloresearch/jan"&gt;https://github.com/menloresearch/jan&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShinobuYuuki"&gt; /u/ShinobuYuuki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/49h5xlsp6osf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T09:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
</feed>
