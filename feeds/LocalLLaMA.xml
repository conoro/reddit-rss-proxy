<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-28T10:07:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p82mg3</id>
    <title>Never been a better time, to learn to write a good rhyme!</title>
    <updated>2025-11-27T13:48:02+00:00</updated>
    <author>
      <name>/u/lakySK</name>
      <uri>https://old.reddit.com/user/lakySK</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p82mg3/never_been_a_better_time_to_learn_to_write_a_good/"&gt; &lt;img alt="Never been a better time, to learn to write a good rhyme!" src="https://preview.redd.it/g6nc02nc0t3g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d98ddd988232c35646210a0f68c0b2836986a310" title="Never been a better time, to learn to write a good rhyme!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models &lt;a href="https://arxiv.org/abs/2511.15304"&gt;https://arxiv.org/abs/2511.15304&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lakySK"&gt; /u/lakySK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g6nc02nc0t3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p82mg3/never_been_a_better_time_to_learn_to_write_a_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p82mg3/never_been_a_better_time_to_learn_to_write_a_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T13:48:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8f0jz</id>
    <title>Trying to find the best AI note taking app that isn‚Äôt a bot in my meetings</title>
    <updated>2025-11-27T22:38:43+00:00</updated>
    <author>
      <name>/u/lebron8</name>
      <uri>https://old.reddit.com/user/lebron8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been bouncing between different ‚ÄúAI note‚Äù tools, and honestly most of them are kind of annoying, either a bot joins the call, or everything gets shipped off to the cloud. Not great if you‚Äôre on sensitive or client calls.&lt;/p&gt; &lt;p&gt;I tried Bluedot recently because it records on your device without joining the meeting, which feels way less weird....but it made me wonder if there‚Äôs a fully local setup people here use.&lt;/p&gt; &lt;p&gt;Anyone hacked together a Whisper + LLaMA combo for meeting transcriptions/summaries? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lebron8"&gt; /u/lebron8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8f0jz/trying_to_find_the_best_ai_note_taking_app_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8f0jz/trying_to_find_the_best_ai_note_taking_app_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8f0jz/trying_to_find_the_best_ai_note_taking_app_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T22:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7o83p</id>
    <title>Where did the Epstein emails dataset go</title>
    <updated>2025-11-27T00:27:19+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Removed from Hugging Face (&lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;link&lt;/a&gt;)&lt;br /&gt; Removed from GitHub (&lt;a href="https://github.com/EF20K/"&gt;link&lt;/a&gt;)&lt;br /&gt; Reddit account deleted (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p683yz/thank_you_all_for_your_contribution_with_tools/"&gt;last post&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o83p/where_did_the_epstein_emails_dataset_go/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o83p/where_did_the_epstein_emails_dataset_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o83p/where_did_the_epstein_emails_dataset_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T00:27:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8p6tm</id>
    <title>Looking for advice on improving RAG responses for my personal AI chat archive</title>
    <updated>2025-11-28T07:51:49+00:00</updated>
    <author>
      <name>/u/n8signals</name>
      <uri>https://old.reddit.com/user/n8signals</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've built a local RAG system to search and analyze my AI chat history across multiple platforms (ChatGPT, Claude, Cursor, Codex) since early 2023. The goal is to use this a resource for new things I am working on, as well as, eventually identify patterns in my conversations and surface recommendations for better prompts, common solutions to recurring problems, etc.&lt;/p&gt; &lt;p&gt;The Hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Windows server 2022 64-bit&lt;/li&gt; &lt;li&gt;AMD Ryzen 9 9950X (16-Core, 4.30 GHz)&lt;/li&gt; &lt;li&gt;192 GB DDR5&lt;/li&gt; &lt;li&gt;RTX 5090 (32GB VRAM, Blackwell sm_120, driver 581.57)&lt;/li&gt; &lt;li&gt;CUDA 12.4 toolkit / PyTorch cu128 nightly (native sm_120 support)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Python 3.12 with dedicated venv for GPU embeddings&lt;/li&gt; &lt;li&gt;PyTorch 2.10.0.dev20251124+cu128 (nightly build)&lt;/li&gt; &lt;li&gt;sentence-transformers (all-mpnet-base-v2) running on CUDA&lt;/li&gt; &lt;li&gt;DuckDB as the vector store (768-dim embeddings)&lt;/li&gt; &lt;li&gt;Ollama for generation with custom model&lt;/li&gt; &lt;li&gt;Open WebUI as the frontend&lt;/li&gt; &lt;li&gt;~1,200+ conversation files extracted to markdown, chunked (2000 chars, 200 overlap), and embedded&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ollama Model Config:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM mistral-nemo:12b PARAMETER temperature 0.15 PARAMETER num_ctx 18492 PARAMETER repeat_penalty 1.1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;p&gt;Conversations get extracted from each platform, saved as markdown, chunked, embedded on GPU, then stored in DuckDB. Query goes through sentence-transformers for embedding, cosine similarity retrieval against the vector store, then Ollama generates a response with the top-k context chunks.&lt;/p&gt; &lt;p&gt;Where I'm struggling (looking for opinions):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;System prompt gets ignored ‚Äì I have a prepend in the system prompt that says &amp;quot;You are a RAG assistant. Use ONLY the provided DuckDB context; if none, say 'no data found.'&amp;quot; but unless I literally write it in the user prompt itself, it gets ignored. Is this a mistral-nemo quirk, an Ollama API issue, or is there a better way to enforce grounding?&lt;/li&gt; &lt;li&gt;Hallucination / massaging of results ‚Äì The retrieval seems solid (it finds relevant chunks), but the analysis feels like it's hallucinating or paraphrasing what it thinks I want rather than what was actually in the archived conversation. Even with temperature at 0.15, it takes my context and blends it with general knowledge instead of staying grounded. It's finding the right data but the response doesn't reflect it accurately.&lt;/li&gt; &lt;li&gt;Ultimate goal feels out of reach - I not only want to use this to find things I have already done so I do not recreate the wheel, I also want to use this to find common patterns across my conversations and make recommendations (better prompts, faster workflows, etc.). But right now I'm lucky if the response feels accurate at all. The retrieval works, the generation is where things fall apart.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Previous issue (now resolved):&lt;/p&gt; &lt;p&gt;I used to constantly battle Python version conflicts across different tools, Ollama using one Python, VS Code another, scripts another. Now that everything runs in a single venv with consistent dependencies, that's no longer a problem. The latest pytorch build from 20251124 was the last missing piece that helped me finally get to the native sm_120 support that I had not been able to get to work.&lt;/p&gt; &lt;p&gt;Questions for the community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How are you enforcing grounding in local LLMs? Is there a better model than mistral-nemo for staying strictly on-context?&lt;/li&gt; &lt;li&gt;Any tips for reducing hallucination in RAG when the retrieval is accurate but the generation wanders?&lt;/li&gt; &lt;li&gt;Has anyone had success with pattern analysis across their own chat archives? What approach worked?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If there are other threads, articles, books I should pick up I am open to that feedback as well. Appreciate any insights. Happy to share more details about the setup if anyone has any.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/n8signals"&gt; /u/n8signals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p6tm/looking_for_advice_on_improving_rag_responses_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p6tm/looking_for_advice_on_improving_rag_responses_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p6tm/looking_for_advice_on_improving_rag_responses_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T07:51:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8fhii</id>
    <title>Seeing 5060 Ti 16GB going for $370; worth it?</title>
    <updated>2025-11-27T23:02:01+00:00</updated>
    <author>
      <name>/u/Careful_Breath_1108</name>
      <uri>https://old.reddit.com/user/Careful_Breath_1108</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thinking of using two of these together for a total of 32GB VRAM for a beginner home setup to explore inference, fine tuning, and training. Would this be considered viable and cost effective? Or is a single 3090 still way more worth it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careful_Breath_1108"&gt; /u/Careful_Breath_1108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8fhii/seeing_5060_ti_16gb_going_for_370_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8fhii/seeing_5060_ti_16gb_going_for_370_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8fhii/seeing_5060_ti_16gb_going_for_370_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T23:02:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7siuu</id>
    <title>Anthropic just showed how to make AI agents work on long projects without falling apart</title>
    <updated>2025-11-27T04:05:59+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most AI agents forget everything between sessions, which means they completely lose track of long tasks. Anthropic‚Äôs new article shows a surprisingly practical fix. Instead of giving an agent one giant goal like ‚Äúbuild a web app,‚Äù they wrap it in a simple harness that forces structure, memory, and accountability.&lt;/p&gt; &lt;p&gt;First, an initializer agent sets up the project. It creates a full feature list, marks everything as failing, initializes git, and writes a progress log. Then each later session uses a coding agent that reads the log and git history, picks exactly one unfinished feature, implements it, tests it, commits the changes, and updates the log. No guessing, no drift, no forgetting.&lt;/p&gt; &lt;p&gt;The result is an AI that can stop, restart, and keep improving a project across many independent runs. It behaves more like a disciplined engineer than a clever autocomplete. It also shows that the real unlock for long-running agents may not be smarter models, but better scaffolding.&lt;/p&gt; &lt;p&gt;Read the article here:&lt;br /&gt; &lt;a href="https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents"&gt;https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7siuu/anthropic_just_showed_how_to_make_ai_agents_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7siuu/anthropic_just_showed_how_to_make_ai_agents_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7siuu/anthropic_just_showed_how_to_make_ai_agents_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T04:05:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8puov</id>
    <title>llama-cli how to include input in log file</title>
    <updated>2025-11-28T08:34:34+00:00</updated>
    <author>
      <name>/u/reddit-doc</name>
      <uri>https://old.reddit.com/user/reddit-doc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there, this might be a stupid question, but how can I include my interactive input in the log file when I use llama-cli directly? Output in the terminal:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;== Running in interactive mode. == - Press Ctrl+C to interject at any time. - Press Return to return control to the AI. - To return control without starting a new line, end your input with '/'. - If you want to submit another line, end your input with '\'. - Not using system message. To change it, set a different value via -sys PROMPT &amp;gt; Hello Hello there! üëã How can I help you today? Are you looking to: * **Chat?** Just want to talk about your day? * **Get information?** Ask me a question about anything! * **Brainstorm ideas?** Need help with a project or a problem? * **Write something?** I can help with stories, poems, emails, and more. * **Something else?** Just let me know what's on your mind. I'm ready to listen (or, well, read)! üòä &amp;gt; What is the result of 1+2 The result of 1 + 2 is **3**. Simple as that! üòä Is there anything else I can help you calculate? &amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Output in the log file (parameter --log-file):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;== Running in interactive mode. == - Press Ctrl+C to interject at any time. - Press Return to return control to the AI. - To return control without starting a new line, end your input with '/'. - If you want to submit another line, end your input with '\'. - Not using system message. To change it, set a different value via -sys PROMPT &amp;gt; Hello there! üëã How can I help you today? Are you looking to: * **Chat?** Just want to talk about your day? * **Get information?** Ask me a question about anything! * **Brainstorm ideas?** Need help with a project or a problem? * **Write something?** I can help with stories, poems, emails, and more. * **Something else?** Just let me know what's on your mind. I'm ready to listen (or, well, read)! üòä &amp;gt; The result of 1 + 2 is **3**. Simple as that! üòä Is there anything else I can help you calculate? &amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see all my input is missing here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reddit-doc"&gt; /u/reddit-doc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8puov/llamacli_how_to_include_input_in_log_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8puov/llamacli_how_to_include_input_in_log_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8puov/llamacli_how_to_include_input_in_log_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T08:34:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8d0xn</id>
    <title>If you were wondering about how Tenstorrent's Blackhole chips perform, now we know</title>
    <updated>2025-11-27T21:04:38+00:00</updated>
    <author>
      <name>/u/Tyme4Trouble</name>
      <uri>https://old.reddit.com/user/Tyme4Trouble</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a pretty dense read but the TLDR is that that Tenstorrent's P150 has a lot of &lt;em&gt;potential&lt;/em&gt; particularly if you string a bunch of them together. &lt;/p&gt; &lt;p&gt;&lt;em&gt;Potential&lt;/em&gt; being the key word here because the software just isn't there yet and won't be until someone writes new kernels for the chips rather than rerunning ones written for Wormhole.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tyme4Trouble"&gt; /u/Tyme4Trouble &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theregister.com/2025/11/27/tenstorrent_quietbox_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8d0xn/if_you_were_wondering_about_how_tenstorrents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8d0xn/if_you_were_wondering_about_how_tenstorrents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T21:04:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8khfu</id>
    <title>Opencode Mobile / Web</title>
    <updated>2025-11-28T03:20:33+00:00</updated>
    <author>
      <name>/u/getfitdotus</name>
      <uri>https://old.reddit.com/user/getfitdotus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8khfu/opencode_mobile_web/"&gt; &lt;img alt="Opencode Mobile / Web" src="https://a.thumbs.redditmedia.com/zdbtZsgeLmnqdpgBM6hy97r7Ua23cY63qTXbxWn50s4.jpg" title="Opencode Mobile / Web" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mobile-first web interface for OpenCode AI assistant. Run, control, and code with OpenCode from any device - your phone, tablet, or desktop. Features Git integration, file management, and real-time chat in a responsive PWA. Deploy with Docker for instant setup.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/23wobsrr1x3g1.gif"&gt;https://i.redd.it/23wobsrr1x3g1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/mkln8blj1x3g1.gif"&gt;https://i.redd.it/mkln8blj1x3g1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/chriswritescode-dev/opencode-web"&gt;https://github.com/chriswritescode-dev/opencode-web&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getfitdotus"&gt; /u/getfitdotus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8khfu/opencode_mobile_web/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8khfu/opencode_mobile_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8khfu/opencode_mobile_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T03:20:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8qkah</id>
    <title>GLM Coding Plan Black Friday Deal ‚Äî real stackable discounts</title>
    <updated>2025-11-28T09:20:43+00:00</updated>
    <author>
      <name>/u/zAiModel-api</name>
      <uri>https://old.reddit.com/user/zAiModel-api</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! If you‚Äôve been thinking about getting a coding assistant, now‚Äôs a great time.&lt;/p&gt; &lt;p&gt;The GLM Coding Plan is running a Black Friday promo, and it‚Äôs super straightforward ‚Äî no tricks, no weird ‚Äúmarketing math.‚Äù&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here‚Äôs the deal:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; 50% off for first-time buyers&lt;/li&gt; &lt;li&gt; On top of that, an extra 20% or 30% off depending on which plan you pick&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How to grab it:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Just go to the official page ‚Äî the final price updates automatically. No promo codes, no hidden links.&lt;/p&gt; &lt;p&gt;&lt;em&gt;üëâ&lt;/em&gt;&lt;a href="https://z.ai/subscribe?utm_source=reddit&amp;amp;utm_campaign=reddit&amp;amp;_channel_track_key=LSDarS2a"&gt;https://z.ai/subscribe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it‚Äôs useful:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it takes care of the boring parts of coding. Generate, fix, rewrite, troubleshoot ‚Äî it handles the grunt work so you can focus on the important stuff. Perfect for anyone who wants less hassle and faster coding.&lt;/p&gt; &lt;p&gt;If you were already planning to get an AI coding assistant, this is probably the best time to jump in. The deal only lasts through Black Friday.&lt;/p&gt; &lt;p&gt;Got questions? Drop them below ‚Äî I‚Äôll do my best to answer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zAiModel-api"&gt; /u/zAiModel-api &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8qkah/glm_coding_plan_black_friday_deal_real_stackable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8qkah/glm_coding_plan_black_friday_deal_real_stackable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8qkah/glm_coding_plan_black_friday_deal_real_stackable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T09:20:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8ilde</id>
    <title>Agentic coding with 16GB VRAM and 64GB RAM: can I do locally?</title>
    <updated>2025-11-28T01:41:09+00:00</updated>
    <author>
      <name>/u/esamueb32</name>
      <uri>https://old.reddit.com/user/esamueb32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I'm a software engineer, and at work I use the company provided cursor agent which works well enough for our uses.&lt;/p&gt; &lt;p&gt;I want to have something similar for personal projects. Is there any model that I can run with my machine that's actually good enough for general coding tasks, or should I just use online models? Which local or online models would you suggest?&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/esamueb32"&gt; /u/esamueb32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ilde/agentic_coding_with_16gb_vram_and_64gb_ram_can_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ilde/agentic_coding_with_16gb_vram_and_64gb_ram_can_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ilde/agentic_coding_with_16gb_vram_and_64gb_ram_can_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T01:41:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8orwd</id>
    <title>Please help me pick the right Mac for local LLM inference (M4 vs M2 Pro vs M1 Max)</title>
    <updated>2025-11-28T07:24:34+00:00</updated>
    <author>
      <name>/u/mystical_mountain</name>
      <uri>https://old.reddit.com/user/mystical_mountain</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm trying to decide which Mac to buy, mainly for local LLM inference and general text generation. Nothing too heavy, my top priority is still energy efficiency and silence, which is why I'm sticking with a Mac. After some research, I‚Äôve narrowed it down to three options that seem to hit the sweet spot between performance and budget:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mac Mini M4, 32GB RAM, 1064‚Ç¨ (new)&lt;/li&gt; &lt;li&gt;Mac Mini M2 Pro, 32GB RAM, 900‚Ç¨ (used)&lt;/li&gt; &lt;li&gt;Mac Studio M1 Max, 64GB RAM, 1300‚Ç¨ (used)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;From the benchmarks I‚Äôve seen (&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/4167"&gt;Ggerganov's llama.cpp discussion&lt;/a&gt;), it looks like:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Mac Studio M1 Max is by far the fastest for LLM inference.&lt;/li&gt; &lt;li&gt;Mac Mini M2 Pro seems to outperform the base M4 in real token-per-second benchmarks.&lt;/li&gt; &lt;li&gt;Mac Mini M4 is newer, but the base model is the slowest of all three.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Before I buy anything, can anyone sanity-check this? Did I overlook something important, or is this ranking basically correct?&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;p&gt;Edit (use case): I want to set the Mac up as a dedicated headless local LLM server. It won‚Äôt run anything else. I‚Äôll use it to process private documents in Paperless-NGX, and possibly connect it to my Home Assistant instance for the chat function.&lt;/p&gt; &lt;p&gt;Edit 2: Thank y'all for your comments! My conclusion: I'll wait a bit more and save money, possibly until the M5 comes out and the old Mac's prices hopefully drop a bit. Then I'll target the Mac Studio M1 Ultra, 128GB RAM, which is currently around 2900‚Ç¨ (used).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mystical_mountain"&gt; /u/mystical_mountain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8orwd/please_help_me_pick_the_right_mac_for_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8orwd/please_help_me_pick_the_right_mac_for_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8orwd/please_help_me_pick_the_right_mac_for_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T07:24:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8irtu</id>
    <title>What LocalLlama Black Friday deals should I go for?</title>
    <updated>2025-11-28T01:50:15+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only answers that will get me in trouble with significant other please.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8irtu/what_localllama_black_friday_deals_should_i_go_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8irtu/what_localllama_black_friday_deals_should_i_go_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8irtu/what_localllama_black_friday_deals_should_i_go_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T01:50:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7z9g1</id>
    <title>deepseek-ai/DeepSeek-Math-V2 ¬∑ Hugging Face</title>
    <updated>2025-11-27T10:47:09+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7z9g1/deepseekaideepseekmathv2_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-Math-V2 ¬∑ Hugging Face" src="https://external-preview.redd.it/NNRX5IH0bPXI-mJ26LQk19NWgnKHeMgBlqbRSXbbGFk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91ce1e1d706821a2296ebf8e620933cc5aa91b2f" title="deepseek-ai/DeepSeek-Math-V2 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Math-V2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7z9g1/deepseekaideepseekmathv2_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7z9g1/deepseekaideepseekmathv2_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T10:47:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p89j2t</id>
    <title>Today I learned that DDR5 can throttle itself at high temps. It affects inference speed.</title>
    <updated>2025-11-27T18:33:00+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been moving the rig over to a proper frame from the $50 Amazon mining frame and taking the opportunity to do airflow properly. I measured the temps of the 6400 MT/s DDR5 RDIMMs using ipmitool and found they were hitting 95C and above while compiling vLLM from source.&lt;/p&gt; &lt;p&gt;Ouch. That‚Äôs very near the top of their operating envelope.&lt;/p&gt; &lt;p&gt;After 3D printing some RAM shrouds and adding a pair of 92mm Noctua Chromax the DDR5 stays under 60C during compiling and even during CPU inference.&lt;/p&gt; &lt;p&gt;And it runs approx 10% faster at inference even for GPU-only models. &lt;/p&gt; &lt;p&gt;Check your RAM temps!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p89j2t/today_i_learned_that_ddr5_can_throttle_itself_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p89j2t/today_i_learned_that_ddr5_can_throttle_itself_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p89j2t/today_i_learned_that_ddr5_can_throttle_itself_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T18:33:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8nped</id>
    <title>Strix Halo batching with tensor parallel and pipeline parallel using vllm benchmarked</title>
    <updated>2025-11-28T06:19:58+00:00</updated>
    <author>
      <name>/u/Hungry_Elk_3276</name>
      <uri>https://old.reddit.com/user/Hungry_Elk_3276</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a continuation of last dual Strix Halo cluster post &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ot3lxv/i_tested_strix_halo_clustering_w_50gig_ib_to_see/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It turns out that RCCL seems to work, but it is not enabled by AMD for some reason. (Why??) Following a random PR on GitHub that uses gfx1100 path on gfx1151, I was able to get RCCL working with vLLM. Just compile and swap the default RCCL shipped with vLLM to your local one and everything started working. So I tested some models I was able to run and got the following results for the original hybrid qwen3-4b (to see the batching performance) and qwen3-vl-30b-a3b to try to have an idea of real-world performance.&lt;/p&gt; &lt;p&gt;Here are the results:&lt;/p&gt; &lt;h1&gt;Qwen3-4B&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test Config&lt;/th&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Single Node&lt;/th&gt; &lt;th align="left"&gt;tp=2&lt;/th&gt; &lt;th align="left"&gt;pp=2&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;512 input / 128 output / 128 concurrency&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;1.64&lt;/td&gt; &lt;td align="left"&gt;3.55&lt;/td&gt; &lt;td align="left"&gt;3.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;209.96&lt;/td&gt; &lt;td align="left"&gt;454.32&lt;/td&gt; &lt;td align="left"&gt;402.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;384.00&lt;/td&gt; &lt;td align="left"&gt;896.00&lt;/td&gt; &lt;td align="left"&gt;647.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;5221.80&lt;/td&gt; &lt;td align="left"&gt;2893.86&lt;/td&gt; &lt;td align="left"&gt;3040.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;5218.32&lt;/td&gt; &lt;td align="left"&gt;3079.07&lt;/td&gt; &lt;td align="left"&gt;2935.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;11067.56&lt;/td&gt; &lt;td align="left"&gt;5608.94&lt;/td&gt; &lt;td align="left"&gt;4441.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;548.74&lt;/td&gt; &lt;td align="left"&gt;242.83&lt;/td&gt; &lt;td align="left"&gt;276.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;563.52&lt;/td&gt; &lt;td align="left"&gt;249.43&lt;/td&gt; &lt;td align="left"&gt;286.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;589.95&lt;/td&gt; &lt;td align="left"&gt;274.77&lt;/td&gt; &lt;td align="left"&gt;307.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;544.46&lt;/td&gt; &lt;td align="left"&gt;240.93&lt;/td&gt; &lt;td align="left"&gt;274.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;450.00&lt;/td&gt; &lt;td align="left"&gt;167.44&lt;/td&gt; &lt;td align="left"&gt;214.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;304.82&lt;/td&gt; &lt;td align="left"&gt;140.87&lt;/td&gt; &lt;td align="left"&gt;159.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2048 input / 256 output / 128 concurrency&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;0.28&lt;/td&gt; &lt;td align="left"&gt;0.79&lt;/td&gt; &lt;td align="left"&gt;0.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;71.97&lt;/td&gt; &lt;td align="left"&gt;202.32&lt;/td&gt; &lt;td align="left"&gt;157.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;182.00&lt;/td&gt; &lt;td align="left"&gt;384.00&lt;/td&gt; &lt;td align="left"&gt;294.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;28426.97&lt;/td&gt; &lt;td align="left"&gt;11321.20&lt;/td&gt; &lt;td align="left"&gt;14431.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;19933.60&lt;/td&gt; &lt;td align="left"&gt;5554.79&lt;/td&gt; &lt;td align="left"&gt;8448.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;117059.55&lt;/td&gt; &lt;td align="left"&gt;52412.20&lt;/td&gt; &lt;td align="left"&gt;55070.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;1635.82&lt;/td&gt; &lt;td align="left"&gt;574.54&lt;/td&gt; &lt;td align="left"&gt;740.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;1692.04&lt;/td&gt; &lt;td align="left"&gt;608.23&lt;/td&gt; &lt;td align="left"&gt;780.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;1752.66&lt;/td&gt; &lt;td align="left"&gt;620.89&lt;/td&gt; &lt;td align="left"&gt;798.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;1629.43&lt;/td&gt; &lt;td align="left"&gt;572.30&lt;/td&gt; &lt;td align="left"&gt;737.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;1275.61&lt;/td&gt; &lt;td align="left"&gt;400.22&lt;/td&gt; &lt;td align="left"&gt;551.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;1778.59&lt;/td&gt; &lt;td align="left"&gt;632.66&lt;/td&gt; &lt;td align="left"&gt;813.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;512 input / 128 output / 256 concurrency&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;1.93&lt;/td&gt; &lt;td align="left"&gt;5.85&lt;/td&gt; &lt;td align="left"&gt;2.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;246.56&lt;/td&gt; &lt;td align="left"&gt;749.28&lt;/td&gt; &lt;td align="left"&gt;285.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;512.00&lt;/td&gt; &lt;td align="left"&gt;1025.00&lt;/td&gt; &lt;td align="left"&gt;521.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;6999.42&lt;/td&gt; &lt;td align="left"&gt;431.48&lt;/td&gt; &lt;td align="left"&gt;1288.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;4504.39&lt;/td&gt; &lt;td align="left"&gt;417.06&lt;/td&gt; &lt;td align="left"&gt;1657.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;22205.62&lt;/td&gt; &lt;td align="left"&gt;660.91&lt;/td&gt; &lt;td align="left"&gt;1877.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;912.78&lt;/td&gt; &lt;td align="left"&gt;249.23&lt;/td&gt; &lt;td align="left"&gt;790.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;912.48&lt;/td&gt; &lt;td align="left"&gt;261.94&lt;/td&gt; &lt;td align="left"&gt;805.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;1078.28&lt;/td&gt; &lt;td align="left"&gt;304.48&lt;/td&gt; &lt;td align="left"&gt;869.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;905.65&lt;/td&gt; &lt;td align="left"&gt;247.28&lt;/td&gt; &lt;td align="left"&gt;784.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;814.82&lt;/td&gt; &lt;td align="left"&gt;276.54&lt;/td&gt; &lt;td align="left"&gt;837.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;259.57&lt;/td&gt; &lt;td align="left"&gt;85.42&lt;/td&gt; &lt;td align="left"&gt;224.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2048 input / 256 output / 256 concurrency&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;0.28&lt;/td&gt; &lt;td align="left"&gt;0.80&lt;/td&gt; &lt;td align="left"&gt;0.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;70.64&lt;/td&gt; &lt;td align="left"&gt;205.47&lt;/td&gt; &lt;td align="left"&gt;124.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;259.00&lt;/td&gt; &lt;td align="left"&gt;512.00&lt;/td&gt; &lt;td align="left"&gt;256.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;95111.92&lt;/td&gt; &lt;td align="left"&gt;32136.63&lt;/td&gt; &lt;td align="left"&gt;36498.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;78589.23&lt;/td&gt; &lt;td align="left"&gt;9586.82&lt;/td&gt; &lt;td align="left"&gt;16249.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;278357.25&lt;/td&gt; &lt;td align="left"&gt;111121.91&lt;/td&gt; &lt;td align="left"&gt;114120.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;3131.02&lt;/td&gt; &lt;td align="left"&gt;1070.57&lt;/td&gt; &lt;td align="left"&gt;1848.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;3333.69&lt;/td&gt; &lt;td align="left"&gt;1162.72&lt;/td&gt; &lt;td align="left"&gt;1891.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;3416.15&lt;/td&gt; &lt;td align="left"&gt;1216.61&lt;/td&gt; &lt;td align="left"&gt;2079.38&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;3118.79&lt;/td&gt; &lt;td align="left"&gt;1066.38&lt;/td&gt; &lt;td align="left"&gt;1841.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;2603.32&lt;/td&gt; &lt;td align="left"&gt;769.11&lt;/td&gt; &lt;td align="left"&gt;1474.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;1812.06&lt;/td&gt; &lt;td align="left"&gt;622.97&lt;/td&gt; &lt;td align="left"&gt;1027.46&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Qwen3VL-30B-A3B&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test Config&lt;/th&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;tp=2&lt;/th&gt; &lt;th align="left"&gt;pp=2&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;512 input / 128 output / 1 concurrency / 10 requests&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;0.16&lt;/td&gt; &lt;td align="left"&gt;0.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;20.66&lt;/td&gt; &lt;td align="left"&gt;13.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;24.00&lt;/td&gt; &lt;td align="left"&gt;15.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;506.55&lt;/td&gt; &lt;td align="left"&gt;667.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;300.01&lt;/td&gt; &lt;td align="left"&gt;467.83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;2196.93&lt;/td&gt; &lt;td align="left"&gt;2346.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;44.74&lt;/td&gt; &lt;td align="left"&gt;69.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;43.40&lt;/td&gt; &lt;td align="left"&gt;67.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;55.68&lt;/td&gt; &lt;td align="left"&gt;80.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;44.39&lt;/td&gt; &lt;td align="left"&gt;68.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;43.32&lt;/td&gt; &lt;td align="left"&gt;67.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;61.96&lt;/td&gt; &lt;td align="left"&gt;94.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2048 input / 256 output / 1 concurrency / 10 requests&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;0.08&lt;/td&gt; &lt;td align="left"&gt;0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;21.43&lt;/td&gt; &lt;td align="left"&gt;13.63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;23.00&lt;/td&gt; &lt;td align="left"&gt;15.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;728.18&lt;/td&gt; &lt;td align="left"&gt;1306.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;726.75&lt;/td&gt; &lt;td align="left"&gt;1309.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;752.38&lt;/td&gt; &lt;td align="left"&gt;1319.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;43.96&lt;/td&gt; &lt;td align="left"&gt;68.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;43.97&lt;/td&gt; &lt;td align="left"&gt;68.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;44.08&lt;/td&gt; &lt;td align="left"&gt;68.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;43.79&lt;/td&gt; &lt;td align="left"&gt;68.21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;43.85&lt;/td&gt; &lt;td align="left"&gt;68.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;119.46&lt;/td&gt; &lt;td align="left"&gt;187.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;512 input / 128 output / 8 concurrency / 100 requests&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;0.71&lt;/td&gt; &lt;td align="left"&gt;0.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;90.55&lt;/td&gt; &lt;td align="left"&gt;52.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;124.00&lt;/td&gt; &lt;td align="left"&gt;80.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;949.21&lt;/td&gt; &lt;td align="left"&gt;1879.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;851.09&lt;/td&gt; &lt;td align="left"&gt;2096.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;1496.50&lt;/td&gt; &lt;td align="left"&gt;2263.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;78.66&lt;/td&gt; &lt;td align="left"&gt;133.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;78.90&lt;/td&gt; &lt;td align="left"&gt;134.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;86.23&lt;/td&gt; &lt;td align="left"&gt;147.97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;78.04&lt;/td&gt; &lt;td align="left"&gt;132.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;76.56&lt;/td&gt; &lt;td align="left"&gt;132.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;141.35&lt;/td&gt; &lt;td align="left"&gt;242.91&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2048 input / 256 output / 8 concurrency / 100 requests&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;0.31&lt;/td&gt; &lt;td align="left"&gt;0.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;78.50&lt;/td&gt; &lt;td align="left"&gt;45.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;112.00&lt;/td&gt; &lt;td align="left"&gt;73.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;1229.13&lt;/td&gt; &lt;td align="left"&gt;3934.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;829.60&lt;/td&gt; &lt;td align="left"&gt;5636.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;2089.51&lt;/td&gt; &lt;td align="left"&gt;5760.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;94.68&lt;/td&gt; &lt;td align="left"&gt;156.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;96.46&lt;/td&gt; &lt;td align="left"&gt;156.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;101.22&lt;/td&gt; &lt;td align="left"&gt;175.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;94.31&lt;/td&gt; &lt;td align="left"&gt;155.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;82.06&lt;/td&gt; &lt;td align="left"&gt;141.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;326.12&lt;/td&gt; &lt;td align="left"&gt;562.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;512 input / 128 output / 16 concurrency / 200 requests&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;1.09&lt;/td&gt; &lt;td align="left"&gt;0.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;139.24&lt;/td&gt; &lt;td align="left"&gt;82.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;192.00&lt;/td&gt; &lt;td align="left"&gt;115.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;406.30&lt;/td&gt; &lt;td align="left"&gt;733.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;392.66&lt;/td&gt; &lt;td align="left"&gt;669.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;742.20&lt;/td&gt; &lt;td align="left"&gt;1419.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;109.05&lt;/td&gt; &lt;td align="left"&gt;184.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;106.78&lt;/td&gt; &lt;td align="left"&gt;183.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;122.48&lt;/td&gt; &lt;td align="left"&gt;204.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;108.20&lt;/td&gt; &lt;td align="left"&gt;182.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;99.34&lt;/td&gt; &lt;td align="left"&gt;172.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;183.85&lt;/td&gt; &lt;td align="left"&gt;310.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2048 input / 256 output / 16 concurrency / 200 requests&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;0.48&lt;/td&gt; &lt;td align="left"&gt;0.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;121.79&lt;/td&gt; &lt;td align="left"&gt;70.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;176.00&lt;/td&gt; &lt;td align="left"&gt;115.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;941.88&lt;/td&gt; &lt;td align="left"&gt;2290.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;632.24&lt;/td&gt; &lt;td align="left"&gt;1468.52&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;2152.66&lt;/td&gt; &lt;td align="left"&gt;6903.66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;124.63&lt;/td&gt; &lt;td align="left"&gt;214.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;121.63&lt;/td&gt; &lt;td align="left"&gt;208.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;147.76&lt;/td&gt; &lt;td align="left"&gt;256.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;124.14&lt;/td&gt; &lt;td align="left"&gt;213.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;108.46&lt;/td&gt; &lt;td align="left"&gt;190.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;420.41&lt;/td&gt; &lt;td align="left"&gt;730.73&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The first qwen3-4b is trying to see how well the Strix Halo handled the high pressure situation. As we can see from the results, TP is getting much better performance compared to PP. And I am not sure why the single node inference is this slow for some reason.&lt;/p&gt; &lt;p&gt;For the qwen3vl-30b-a3b, I want to simulate a more realistic situation, which is 1 user or a small-sized team that is using it as a local inference server. And we can see that TP is giving us nearly 50% more token generation speed. While both PP and TP are providing speedups, TP is performing much better.&lt;/p&gt; &lt;p&gt;If someone wonders why the hell this token generation speed is so slow, it is because it is running the full bf16/fp16 weight. The AWQ support isn't quite there yet, but it is improving. It is surprising to see that qwen3-next-awq is working right now, but running the AWQ multi-nodes hits some errors. But it is improving at a rate much faster than I expected. The ultimate goal of running qwen3vl 235b AWQ 4bit seems very near.&lt;/p&gt; &lt;p&gt;And happy Thanksgiving folks! Hope this data provides some insights.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hungry_Elk_3276"&gt; /u/Hungry_Elk_3276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8nped/strix_halo_batching_with_tensor_parallel_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8nped/strix_halo_batching_with_tensor_parallel_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8nped/strix_halo_batching_with_tensor_parallel_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T06:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8pcrj</id>
    <title>How many parameters do you think are required to emulate the *knowledge* of an average person</title>
    <updated>2025-11-28T08:02:12+00:00</updated>
    <author>
      <name>/u/FrostTactics</name>
      <uri>https://old.reddit.com/user/FrostTactics</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's not controversial to state that LLMs today aren't 100% efficient in their parameter usage. It would not surprise me if we could compress current day performance into one hundredth of the parameters. That said, all knowledge requires &lt;em&gt;information,&lt;/em&gt; and there must therefore be a limit to the level of compression that can be achieved.&lt;/p&gt; &lt;p&gt;The current paradigm tries to train all LLMs as generalists for various technical reasons I'm sure I don't have to explain to the people here. This means that basically all LLMs, even those with only a couple of billion parameters, speak passable Norwegian, for example.&lt;/p&gt; &lt;p&gt;Say we narrowed the scope and instead of trying to build generalists, we tried to build an LLM with an amount of knowledge comparable to that of an average person. Let's make the person monolingual, with the common knowledge expected of any modern person, and an expert in a single field.&lt;/p&gt; &lt;p&gt;Let's also ignore vision, real-world navigation, and actually processing the knowledge, as these seem a bit too vague to reliably get an estimate of at the moment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrostTactics"&gt; /u/FrostTactics &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8pcrj/how_many_parameters_do_you_think_are_required_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8pcrj/how_many_parameters_do_you_think_are_required_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8pcrj/how_many_parameters_do_you_think_are_required_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T08:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8o8sn</id>
    <title>I have a RTX5090 and an AMD AI MAX+ 95 128GB. Which benchmark do you want me to run?</title>
    <updated>2025-11-28T06:52:19+00:00</updated>
    <author>
      <name>/u/foogitiff</name>
      <uri>https://old.reddit.com/user/foogitiff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After selling my spare 5080, I couldn't decide between the two option (well, another is a R9700 Pro).&lt;/p&gt; &lt;p&gt;I decided to buy a 5090 in the end, but I didn't had the time to cancel my framework preorder, so I have currently both! I will be keeping only one.&lt;/p&gt; &lt;p&gt;If people want some llama-bench number comparisons, let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foogitiff"&gt; /u/foogitiff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8o8sn/i_have_a_rtx5090_and_an_amd_ai_max_95_128gb_which/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8o8sn/i_have_a_rtx5090_and_an_amd_ai_max_95_128gb_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8o8sn/i_have_a_rtx5090_and_an_amd_ai_max_95_128gb_which/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T06:52:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p81k2z</id>
    <title>Yes it is possible to uncensor gpt-oss-20b - ArliAI/gpt-oss-20b-Derestricted</title>
    <updated>2025-11-27T12:56:59+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/"&gt; &lt;img alt="Yes it is possible to uncensor gpt-oss-20b - ArliAI/gpt-oss-20b-Derestricted" src="https://external-preview.redd.it/8n5MhbkzXEcl9NlvZMbb8GGre-k1VjQ0kDAKe7qQtQM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9334318d3d29cfd953050dfdf981bc10db9cc00b" title="Yes it is possible to uncensor gpt-oss-20b - ArliAI/gpt-oss-20b-Derestricted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Original discussion on the initial &lt;strong&gt;&lt;a href="https://www.arliai.com"&gt;Arli AI&lt;/a&gt;&lt;/strong&gt; created GLM-4.5-Air-Derestricted model that was ablated using &lt;a href="/u/grimjim"&gt;u/grimjim&lt;/a&gt;'s new ablation method is here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/"&gt;The most objectively correct way to abliterate so far - ArliAI/GLM-4.5-Air-Derestricted &lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Note: Derestricted is a name given to models created by Arli AI using this method, but the method officially is just called &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oypwa7/a_more_surgical_approach_to_abliteration/"&gt;Norm-Preserving Biprojected Abliteration&lt;/a&gt; by &lt;a href="/u/grimjim"&gt;u/grimjim&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Hey everyone, Owen here from &lt;strong&gt;&lt;a href="https://www.arliai.com"&gt;Arli AI&lt;/a&gt;&lt;/strong&gt; again. In my previous post, I got a lot of requests to attempt this derestricting on OpenAI's gpt-oss models as they are models that are intelligent but was infamous for being very...restricted.&lt;/p&gt; &lt;p&gt;I thought that it would be a big challenge and be interesting to try and attempt as well, and so that was the next model I decided to try and derestrict next. The 120b version is more unwieldy to transfer around and load in/out of VRAM/RAM as I was experimenting, so I started with the 20b version first but I will get to the 120b next which should be super interesting.&lt;/p&gt; &lt;p&gt;As for the 20b model here, it seems to have worked! The model now can respond to questions that OpenAI never would have approved of answering (lol!). It also seems to have cut down its wasteful looping around of deciding whether it can or cannot answer a question based on a non existent policy in it's reasoning, although this isn't completely removed yet. I suspect a more customized harmful/harmless dataset to specifically target this behavior might be useful for this, so that will be what I need to work on.&lt;/p&gt; &lt;p&gt;Otherwise I think this is just an outright improved model over the original as it is much more useful now than it's original behavior. Where it would usually flag a lot of false positives and be absolutely useless in certain situations just because of &amp;quot;safety&amp;quot;.&lt;/p&gt; &lt;p&gt;In order to work on modifying the weights of the model, I also had to use a BF16 converted version to start with as the model as you all might know was released in MXFP4 format, but then attempting the ablation on the BF16 converted model seems to work well. I think that this proves that this new method of essentially &amp;quot;direction-based&amp;quot; abliteration is really flexible and works super well for probably any models.&lt;/p&gt; &lt;p&gt;As for quants, I'm not one to worry about making GGUFs myself because I'm sure the GGUF makers will get to it pretty fast and do a better job than I can. Also, there are no FP8 or INT8 quants now because its pretty small and those that run FP8 or INT8 quants usually have a substantial GPU setup anyways.&lt;/p&gt; &lt;p&gt;Try it out and have fun! This time it's really for &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; because we don't even run this model on our Arli AI API service.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ArliAI/gpt-oss-20b-Derestricted"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T12:56:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8ahy8</id>
    <title>Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp; Reasoning Benchmarks. (Link to Chat with the Model provided)</title>
    <updated>2025-11-27T19:13:50+00:00</updated>
    <author>
      <name>/u/44th--Hokage</name>
      <uri>https://old.reddit.com/user/44th--Hokage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"&gt; &lt;img alt="Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp;amp; Reasoning Benchmarks. (Link to Chat with the Model provided)" src="https://a.thumbs.redditmedia.com/tQWpy1j22HMExtYqdGvlA_Lo8sIubygJf4xso2VwSj0.jpg" title="Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp;amp; Reasoning Benchmarks. (Link to Chat with the Model provided)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;From the Official Announcement:&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;Today, we release INTELLECT-3, a 100B+ parameter Mixture-of-Experts model trained on our RL stack, achieving state-of-the-art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our complete recipe ‚Äî from the model weights and training frameworks, to our datasets, RL environments, and evaluations ‚Äî has been open-sourced, with the goal of encouraging more open research on large scale reinforcement learning.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;INTELLECT-3 is trained on the same software and infrastructure that we‚Äôre open-sourcing and making available on our platform at Prime Intellect, giving everyone the tools to post-train their own state-of-the-art models, and moving us towards a future where every company can be an AI company.&lt;/p&gt; &lt;p&gt;The sharpest distinction between Prime-RL and many other RL trainers is that it is async-only ‚Äî we recognized fairly early (for our previous INTELLECT-2 model) that the future of RL is async; i.e. always a few steps off-policy. Async training is simply the only practical way to efficiently scale RL to long-horizon agentic rollouts without incurring bottlenecks based on the slowest rollouts per step.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2&gt;Architecture:&lt;/h2&gt; &lt;p&gt;Three main abstractions facilitate RL training: the orchestrator, the trainer, and the inference service. A RL training run involves the coordination of a trainer, orchestrator and an inference service. The FSDP trainer and vLLM inference run disaggregated, and can be individually deployed across multiple nodes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Orchestrator:&lt;/strong&gt; - The orchestrator is a lightweight CPU process that handles the core data flow and scheduling logic, serving as an intermediary between the trainer and inference service with bidirectional relays. In one direction, it collects rollouts from the inference server, assembles them into packed batches, and dispatches them to the trainer; in the other direction, it relays updated model weights from the trainer to the inference service. The orchestrator utilizes verifiers environments to abstract multi-turn rollout generation and scoring, allowing any environment on the Environments Hub to plug into the training loop.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Trainer:&lt;/strong&gt; - The trainer is responsible for producing an updated policy model given rollouts and advantages. We use FSDP 2 as the backend with compatibility for any HuggingFace model. FSDP shards model parameters, gradients, and optimizer states, allowing training large models with data parallelism and minimal GPU memory footprint. The trainer is inspired by torchtitan and relies on native PyTorch features to implement advanced parallelism techniques, such as tensor, context, and expert parallelism, and leverages grouped matrix multiplication kernels for efficient MoE training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt; - The inference pool consists of standard OpenAI-compatible servers with a vLLM backend. The API specification is extended with custom endpoints to enable updating the server with the latest policy: &lt;code&gt;/update_weights&lt;/code&gt; is used to update the policy, and &lt;code&gt;/reload_weights&lt;/code&gt; is used to reset the weights to the base model in between experiments. We rely on vLLM's optimized kernels, parallelism strategies, and scheduling for fast rollout generation. Given the disaggregated nature of the service architecture, it can be directly extended to include multiple engines with a shared request pool, allowing operation across multiple clusters and straightforward integration of alternative inference engines.&lt;/p&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Official Announcement: &lt;a href="https://www.primeintellect.ai/blog/intellect-3"&gt;https://www.primeintellect.ai/blog/intellect-3&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Technical Report: &lt;a href="https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf"&gt;https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Open-Sourced Prime-RL GitHub: &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;https://github.com/PrimeIntellect-ai/prime-rl&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Open-Sourced Model Weights: &lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-3"&gt;https://huggingface.co/PrimeIntellect/INTELLECT-3&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Chat with the Model Here: &lt;a href="https://chat.primeintellect.ai/"&gt;https://chat.primeintellect.ai/&lt;/a&gt;&lt;/h4&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44th--Hokage"&gt; /u/44th--Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p8ahy8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T19:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8onns</id>
    <title>I cooked abliterated gemma3-27b-it with norm-preserving technique</title>
    <updated>2025-11-28T07:17:10+00:00</updated>
    <author>
      <name>/u/Perfect_Biscotti_476</name>
      <uri>https://old.reddit.com/user/Perfect_Biscotti_476</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Gemma 3 27B Instruct - Norm-Preserving Abliterated&lt;/h1&gt; &lt;p&gt;I'm excited to share my contribution to the community: a &lt;strong&gt;norm-preserving abliterated version of Google's Gemma 3 27B Instruct&lt;/strong&gt;! Consider it a late Thanksgiving present.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/YanLabs/gemma3-27b-it-abliterated-normpreserve"&gt;https://huggingface.co/YanLabs/gemma3-27b-it-abliterated-normpreserve&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This model uses the &lt;strong&gt;norm-preserving biprojected abliteration&lt;/strong&gt; technique, which surgically removes refusal mechanisms while preserving reasoning capabilities.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;: &lt;a href="https://huggingface.co/YanLabs/gemma3-27b-it-abliterated-normpreserve"&gt;YanLabs/gemma3-27b-it-abliterated-normpreserve&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Technique&lt;/strong&gt;: &lt;a href="https://github.com/jim-plus/llm-abliteration"&gt;jim-plus/llm-abliteration&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Hardware&lt;/strong&gt;: Cooked on a rented A100 GPU via RunPod&lt;/p&gt; &lt;p&gt;I haven't created GGUF quants yet due to my limited quantization experience. If anyone's willing to help create Q8_0 and Q4_K_M versions, I (and the community) would greatly appreciate it!&lt;/p&gt; &lt;h1&gt;Disclaimer&lt;/h1&gt; &lt;p&gt;This model has safety guardrails removed. &lt;strong&gt;Research purposes only.&lt;/strong&gt; Use responsibly and in compliance with applicable laws.&lt;/p&gt; &lt;h1&gt;About Me&lt;/h1&gt; &lt;p&gt;I'm an LLM enthusiast and practicing lawyer based in Shanghai. If your AI company needs legal services (domestic or international), feel free to reach out!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üìß [&lt;a href="mailto:ruiqingyan@outlook.com"&gt;ruiqingyan@outlook.com&lt;/a&gt;](mailto:&lt;a href="mailto:ruiqingyan@outlook.com"&gt;ruiqingyan@outlook.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Happy experimenting! üöÄ&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect_Biscotti_476"&gt; /u/Perfect_Biscotti_476 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8onns/i_cooked_abliterated_gemma327bit_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8onns/i_cooked_abliterated_gemma327bit_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8onns/i_cooked_abliterated_gemma327bit_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T07:17:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8p844</id>
    <title>Tested quantization on my 8GB potato laptop here's what actually breaks first</title>
    <updated>2025-11-28T07:54:17+00:00</updated>
    <author>
      <name>/u/Even_Ganache6148</name>
      <uri>https://old.reddit.com/user/Even_Ganache6148</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p844/tested_quantization_on_my_8gb_potato_laptop_heres/"&gt; &lt;img alt="Tested quantization on my 8GB potato laptop here's what actually breaks first" src="https://b.thumbs.redditmedia.com/vR1H3t5ezVMlfOa85U2nv8CFIEe0-WSfW8pF48jj7ns.jpg" title="Tested quantization on my 8GB potato laptop here's what actually breaks first" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running local LLMs on my broke-student laptop (8GB RAM, i3 processor) and kept hitting the quantization guessing game. Downloaded like 10 different formats trying to figure out which one wouldn't destroy quality.&lt;/p&gt; &lt;p&gt;Here's what I found from testing TinyLlama and reading through hundreds of benchmark results:&lt;/p&gt; &lt;p&gt;Findings:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0x2atfgwdy3g1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4feadc39dc6e8a7dc96e7d8d4e63393b13d0859"&gt;https://preview.redd.it/0x2atfgwdy3g1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4feadc39dc6e8a7dc96e7d8d4e63393b13d0859&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Pattern:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;General chat: Survives down to Q4 pretty well (2-3% quality drop)&lt;/li&gt; &lt;li&gt;Creative writing: Actually stays decent even at Q3&lt;/li&gt; &lt;li&gt;Code generation: Starts getting buggy at Q4 (5-10% drop)&lt;/li&gt; &lt;li&gt;Math/reasoning: Falls off a CLIFF at Q4 (15-20% accuracy drop)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Data Sources:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 3.1 8B (multiple quant formats from TheBloke/bartowski)&lt;/li&gt; &lt;li&gt;Mistral 7B v0.3 (various GGUF quants)&lt;/li&gt; &lt;li&gt;Qwen2 7B (official quants)&lt;/li&gt; &lt;li&gt;Phi-3 Mini (Microsoft's quants)&lt;/li&gt; &lt;li&gt;Tested on: MMLU (general reasoning), HumanEval (coding), GSM8K (math), creative writing prompts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Compiled from:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;HuggingFace model cards with reported benchmarks&lt;/li&gt; &lt;li&gt;Open LLM Leaderboard results&lt;/li&gt; &lt;li&gt;llama.cpp community benchmarks on GitHub&lt;/li&gt; &lt;li&gt;My own testing on TinyLlama 1.1B (what my laptop can actually run)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is aggregated trends across models, not exhaustive testing. Different models degrade slightly differently, but the PATTERN holds - math breaks way faster than other tasks.&lt;/p&gt; &lt;p&gt;Why this matters: If you're using a model for coding or math, Q4 might seem fine in casual testing but will randomly fail on complex problems. Meanwhile creative tasks are way more forgiving.&lt;/p&gt; &lt;p&gt;My conclusion: Q5_K_M is the sweet spot - 95%+ quality, fits on 8GB systems, doesn't randomly break on specific tasks.&lt;/p&gt; &lt;p&gt;Now heres my question would anyone actually pay for a tool that analyzes YOUR specific model/use-case and predicts which quantization to use BEFORE downloading 50GB of different formats?&lt;/p&gt; &lt;p&gt;I'm thinking of building this because I'm tired of the trial-and-error, but want to know if it's just me being lazy or an actual problem people would pay to solve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Even_Ganache6148"&gt; /u/Even_Ganache6148 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p844/tested_quantization_on_my_8gb_potato_laptop_heres/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p844/tested_quantization_on_my_8gb_potato_laptop_heres/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p844/tested_quantization_on_my_8gb_potato_laptop_heres/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T07:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8hqq4</id>
    <title>Apparently Asus is working with Nvidia on a 784GB "Coherent" Memory desktop PC with 20 PFLOPS AI Performance</title>
    <updated>2025-11-28T00:56:49+00:00</updated>
    <author>
      <name>/u/waiting_for_zban</name>
      <uri>https://old.reddit.com/user/waiting_for_zban</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Somehow the announcement went under the radar, but back in May, along side the Ascent GX10, Asus announced the &lt;a href="https://www.asus.com/displays-desktops/workstations/performance/expertcenter-pro-et900n-g3/"&gt;ExpertCenter Pro ET900N G3&lt;/a&gt;, with GB300 Blackwell. They don't really say what's a &amp;quot;Coherent&amp;quot; memory, but my guess it's another term of saying unified memory like Apple and AMD. &lt;/p&gt; &lt;p&gt;The announcement and the specs are very dry on details, but given the GB300, we might get a very decent memory bandwidth, without &lt;a href="https://i.imgur.com/pNaKzWb.png"&gt;looking like a hideous frankestein monster&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;This might be &lt;a href="/r/Localllama"&gt;r/Localllama&lt;/a&gt; wet dream. If they manage to price it well, and fix that memory bandwidth (that plagued Spark), they have my money. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; As &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/nr5ae77/"&gt;many&lt;/a&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/nr5jvc1/"&gt;pointed out&lt;/a&gt; in the comments, it's based on the &lt;a href="https://www.nvidia.com/en-us/products/workstations/dgx-station/"&gt;Nvidia DGX Station&lt;/a&gt;, announced back in March, which is &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/nr5jvc1/"&gt;rumored to be 80k&lt;/a&gt;. ServeTheHome had a &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl4amv/a_closer_look_at_the_nvidia_dgx_station_gb300/"&gt;nice article about it&lt;/a&gt; back in March.&lt;br /&gt; The official specs: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;496GB LPDDR5X CPU memory at 396GB/s (Micron SOCAMM, so it seems that it will be modular not soldered!)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;288GB HBM3e GPU memory at 8TB/s.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waiting_for_zban"&gt; /u/waiting_for_zban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T00:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; ‚Äî Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
