<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-11T21:21:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o3qx3e</id>
    <title>Kwaipilot/KAT-Dev-72B-Exp seems to be a great coding model？</title>
    <updated>2025-10-11T09:08:47+00:00</updated>
    <author>
      <name>/u/Human-Gas-1288</name>
      <uri>https://old.reddit.com/user/Human-Gas-1288</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3qx3e/kwaipilotkatdev72bexp_seems_to_be_a_great_coding/"&gt; &lt;img alt="Kwaipilot/KAT-Dev-72B-Exp seems to be a great coding model？" src="https://external-preview.redd.it/rxyepxgYUof3_-pxPA16Sj6OoiuoO3OTQiZrKV-cxps.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a34ebb6475d3fb833395063cb58949ed7cc21cd" title="Kwaipilot/KAT-Dev-72B-Exp seems to be a great coding model？" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j0m718zu7guf1.png?width=1186&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f04d36fe4ab33026c8bafc9cb90592b260562ec"&gt;https://preview.redd.it/j0m718zu7guf1.png?width=1186&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f04d36fe4ab33026c8bafc9cb90592b260562ec&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Human-Gas-1288"&gt; /u/Human-Gas-1288 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3qx3e/kwaipilotkatdev72bexp_seems_to_be_a_great_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3qx3e/kwaipilotkatdev72bexp_seems_to_be_a_great_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3qx3e/kwaipilotkatdev72bexp_seems_to_be_a_great_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T09:08:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o44u78</id>
    <title>We know the rule of thumb… large quantized models outperform smaller less quantized models, but is there a level where that breaks down?</title>
    <updated>2025-10-11T19:43:54+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ask because I’ve also heard quants below 4 bit are less effective, and that rule of thumb always seemed to compare 4bit large vs 8bit small.&lt;/p&gt; &lt;p&gt;As an example let’s take the large GLM 4.5 vs GLM 4.5 Air. You can have a much higher bitrate with GLM 4.5 Air… but… even with a 2bit quant made by unsloth, GLM 4.5 does quite well for me. &lt;/p&gt; &lt;p&gt;I haven’t figured out a great way to have complete confidence though so I thought I’d ask you all. What’s your rule of thumb when having to weigh a smaller model vs larger model at different quants? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o44u78/we_know_the_rule_of_thumb_large_quantized_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o44u78/we_know_the_rule_of_thumb_large_quantized_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o44u78/we_know_the_rule_of_thumb_large_quantized_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T19:43:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3sstr</id>
    <title>Every single COT terms score.</title>
    <updated>2025-10-11T11:04:46+00:00</updated>
    <author>
      <name>/u/Ambitious-a4s</name>
      <uri>https://old.reddit.com/user/Ambitious-a4s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;1. Zeroing&lt;/strong&gt; - By far the most annoying word I've ever heard, used by Gemini 2.5 Pro's Thinking but when people actually knows this meaning, it just a fancy word that means 'I am directing all the focus into this or that (subject)'. Its efficient (it only uses two tokens) and fancy but annoying. &lt;strong&gt;8.7/10&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Synthesizing&lt;/strong&gt; - GLM 4.6 uses this, Gemini 2.5 Pro Exp, and more. Its fancy wording too, but it just means 'I am combining this thought I have with the old thought I made', its good and it doesn't really sound the much annoying (IMO). It also helps the AI combining ideas and thoughts in one go so its &lt;strong&gt;9/10&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Hmm&lt;/strong&gt; - Used by Qwen (idk what model but Qwen), Deepseek V3.1 to V3.2, and more. Its not fancy, but it just means lots of things, sometimes Qwen 3 235B do this by pausing and hesitating before dumping more thoughts, and Deepseek uses this in the first word to think. It doesn't do much I would say, its by far mid and its only for pausing and hesitating to think. My only favorite part is that it uses two tokens, &lt;strong&gt;6.6/10&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Confidence Score/Confident Score&lt;/strong&gt; - For some reason, it's one of my favorite terms. It makes the LLM aware how confident it is with the answer or not. It can also make the LLM think more further ahead for some reason, but its not perfect, most LLM's hallucinate and would think its 5/5 or 10/10 in a wrong answer they would give so sometimes it had no point to use it. &lt;strong&gt;7.1/10&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Alternatively&lt;/strong&gt; - The old Deepseek times. ONE OF MY FAVORITE TERMS. It makes the LLM aware of its thoughts so it lowers the mistake. &lt;/p&gt; &lt;p&gt;The biggest con is when the LLM uses these terms is that it BLOATS, when you expect it only uses 941 tokens on a single thought turned into a massive 5000 tokens in a single thought before response. It makes my API cripple so bad and its the biggest con. So its a &lt;strong&gt;5.4/10&lt;/strong&gt;, I wish it can be back but this time more efficient.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious-a4s"&gt; /u/Ambitious-a4s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sstr/every_single_cot_terms_score/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sstr/every_single_cot_terms_score/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sstr/every_single_cot_terms_score/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T11:04:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3dfib</id>
    <title>GPT-OSS from Scratch on AMD GPUs</title>
    <updated>2025-10-10T21:21:45+00:00</updated>
    <author>
      <name>/u/tuanlda78202</name>
      <uri>https://old.reddit.com/user/tuanlda78202</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"&gt; &lt;img alt="GPT-OSS from Scratch on AMD GPUs" src="https://external-preview.redd.it/kr7-cYQLjVSYgCHTdYk0hOHo8LtDx1fruxInOK1rC5k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df53654295de2b4add7e5b3992fa595b88f048f1" title="GPT-OSS from Scratch on AMD GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After six years-the first time since GPT-2, OpenAI has released new open-weight LLMs, gpt-oss-20b and gpt-oss-120b. From day one, many inference engines such as llama.cpp, vLLM, and sgl-project have supported these models; however, most focus on maximizing throughput using CUDA for NVIDIA GPUs, offering limited support for AMD* GPUs. Moreover, their library-oriented implementations are often complex to understand and difficult to adapt for personal or experimental use cases.&lt;/p&gt; &lt;p&gt;To address these limitations, my team introduce “gpt-oss-amd”, a pure C++ implementation of OpenAI’s GPT-OSS models designed to maximize inference throughput on AMD GPUs without relying on external libraries. Our goal is to explore end-to-end LLM optimization, from kernel-level improvements to system-level design, providing insights for researchers and developers interested in high-performance computing and model-level optimization.&lt;/p&gt; &lt;p&gt;Inspired by llama2.c by Andrej Karpathy, our implementation uses HIP (an AMD programming model equivalent to CUDA) and avoids dependencies such as rocBLAS, hipBLAS, RCCL, and MPI. We utilize multiple optimization strategies for the 20B and 120B models, including efficient model loading, batching, multi-streaming, multi-GPU communication, optimized CPU–GPU–SRAM memory access, FlashAttention, matrix-core–based GEMM, and load balancing for MoE routing.&lt;/p&gt; &lt;p&gt;Experiments on a single node with 8× AMD MI250 GPUs show that our implementation achieves over 30k TPS on the 20B model and nearly 10k TPS on the 120B model in custom benchmarks, demonstrating the effectiveness of our optimizations and the strong potential of AMD GPUs for large-scale LLM inference.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9dr4gme0qcuf1.png?width=3392&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ad41a33bc3ecf6625afaa2ff62ca762f5479d2a"&gt;https://preview.redd.it/9dr4gme0qcuf1.png?width=3392&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ad41a33bc3ecf6625afaa2ff62ca762f5479d2a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/tuanlda78202/gpt-oss-amd"&gt;https://github.com/tuanlda78202/gpt-oss-amd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tuanlda78202"&gt; /u/tuanlda78202 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T21:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3kb3o</id>
    <title>Real SVD GLM-4.5-Air-GLM-4.6-Distill</title>
    <updated>2025-10-11T02:39:01+00:00</updated>
    <author>
      <name>/u/realmaywell</name>
      <uri>https://old.reddit.com/user/realmaywell</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"&gt; &lt;img alt="Real SVD GLM-4.5-Air-GLM-4.6-Distill" src="https://external-preview.redd.it/e09zZ1vJH-206eXdnYOOyzVU_npt174nxPybTvRs_LQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aac58b8a3b097d5a8fa8a939d97c17ee2f3dee5" title="Real SVD GLM-4.5-Air-GLM-4.6-Distill" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/imzpad6l8euf1.png?width=7167&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1582f372758ee501dae32e6d09cf36652bba3a9f"&gt;https://preview.redd.it/imzpad6l8euf1.png?width=7167&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1582f372758ee501dae32e6d09cf36652bba3a9f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the person who posted that &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fb6jdy/reflectionllama3170b_is_actually_llama3/"&gt;Reflection-Llama-3.1-70B is actually Llama-3. &lt;/a&gt;I didn't expect my first Reddit post in a year would be another debunk.&lt;/p&gt; &lt;p&gt;After seeing this &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nyopyc/did_anyone_try_out_glm45airglm46distill/"&gt;Reddit post&lt;/a&gt;, the idea of using SVD to distill a model seemed plausible, So I decided to test it out myself.&lt;/p&gt; &lt;p&gt;Although the original model mentioned in the post was a scam, I was curious about what would happen if I actually applied the methodology. So, I rewrote the entire existing CPU-based script into PyTorch code and ran the experiment on an H200 machine.&lt;/p&gt; &lt;p&gt;I excluded the LayerNorm and embed layers from the distillation because it was obvious that including them would break the model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3y75s9519euf1.png?width=2502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb9670538e5c603cdeba84a0e04e3035817192e"&gt;https://preview.redd.it/3y75s9519euf1.png?width=2502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb9670538e5c603cdeba84a0e04e3035817192e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The results are as shown in the pictures above. I guess this means you don't have to go out of your way to replicate this experiment yourselves.&lt;/p&gt; &lt;p&gt;Still, for those who do want to try it out, you might find the model and LoRA below helpful.&lt;br /&gt; Model: &lt;a href="https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill"&gt;https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill&lt;/a&gt;&lt;br /&gt; LoRA: &lt;a href="https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill-LoRA"&gt;https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill-LoRA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Script: &lt;a href="https://gist.github.com/StableFluffy/cfa24ce7d93e3c6b0d55d08b12f6f55c"&gt;https://gist.github.com/StableFluffy/cfa24ce7d93e3c6b0d55d08b12f6f55c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yay.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realmaywell"&gt; /u/realmaywell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T02:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o43hyn</id>
    <title>Recommendation for a local Japanese -&gt; English vision model</title>
    <updated>2025-10-11T18:49:43+00:00</updated>
    <author>
      <name>/u/Confident-Willow5457</name>
      <uri>https://old.reddit.com/user/Confident-Willow5457</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per the title I'm looking for a multimodal model that can perform competent JP to ENG translations from images. Ideally it'd fit in 48 gb of VRAM but I'm not opposed to doing a bit of CPU offloading for significantly higher quality translation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Confident-Willow5457"&gt; /u/Confident-Willow5457 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o43hyn/recommendation_for_a_local_japanese_english/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o43hyn/recommendation_for_a_local_japanese_english/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o43hyn/recommendation_for_a_local_japanese_english/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T18:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3k9zr</id>
    <title>AI Studio Pro mini PC from Orange Pi pairs dual Huawei Ascend 310 processors with up to 192GB of RAM</title>
    <updated>2025-10-11T02:37:28+00:00</updated>
    <author>
      <name>/u/cafedude</name>
      <uri>https://old.reddit.com/user/cafedude</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3k9zr/ai_studio_pro_mini_pc_from_orange_pi_pairs_dual/"&gt; &lt;img alt="AI Studio Pro mini PC from Orange Pi pairs dual Huawei Ascend 310 processors with up to 192GB of RAM" src="https://external-preview.redd.it/pGGQzmvOi4pQiLTOWOaCqIqsQ66ZIQqjltXUjZikyAU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5a776a9f8f4b6e3a30c9df214e7029c4c64c5b8" title="AI Studio Pro mini PC from Orange Pi pairs dual Huawei Ascend 310 processors with up to 192GB of RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cafedude"&gt; /u/cafedude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techradar.com/pro/this-mini-pc-has-192gb-of-ram-yes-ram-but-thats-not-the-most-surprising-fact-about-it-the-orange-pi-ai-studio-pro-uses-a-huawei-ascend-310-thats-on-paper-7x-more-powerful-than-amds-ryzen-ai-max-395"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3k9zr/ai_studio_pro_mini_pc_from_orange_pi_pairs_dual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3k9zr/ai_studio_pro_mini_pc_from_orange_pi_pairs_dual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T02:37:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o41j5y</id>
    <title>video to audio?</title>
    <updated>2025-10-11T17:30:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you know any open models to generate audio from the video?&lt;/p&gt; &lt;p&gt;I know video from image, audio to text, text to audio, but can't find audio from video.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o41j5y/video_to_audio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o41j5y/video_to_audio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o41j5y/video_to_audio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T17:30:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3zz30</id>
    <title>Poor GPU Club : Anyone use Q3/Q2 quants of 20-40B Dense models? How's it?</title>
    <updated>2025-10-11T16:27:46+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FYI My System Info: &lt;sup&gt;Intel(R&lt;/sup&gt; Core(TM) i7-14700HX 2.10 GHz |) &lt;strong&gt;&lt;sup&gt;32 GB RAM&lt;/sup&gt;&lt;/strong&gt; &lt;sup&gt;| 64-bit OS, x64-based processor | NVIDIA GeForce RTX 4060 Laptop GPU (&lt;/sup&gt;&lt;strong&gt;&lt;sup&gt;8GB VRAM&lt;/sup&gt;&lt;/strong&gt; |) &lt;strong&gt;&lt;sup&gt;Cores - 20&lt;/sup&gt;&lt;/strong&gt; &lt;sup&gt;|&lt;/sup&gt; &lt;strong&gt;&lt;sup&gt;Logical Processors - 28&lt;/sup&gt;&lt;/strong&gt;&lt;sup&gt;.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;Unfortunately I can't use Q4 or above quants of 20-40B Dense models, it'll be slower with single digit t/s.&lt;/p&gt; &lt;p&gt;How is Q3/Q2 quants of 20-40B Dense models? Talking about Perplexity, KL divergence, etc., metrics. Are they worthy enough to use? Wish there's a portal for such metrics for all models &amp;amp; with all quants.&lt;/p&gt; &lt;p&gt;List of models I want to use:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Magistral-Small-2509 ( &lt;strong&gt;IQ3_XXS&lt;/strong&gt; - 9.41GB | Q3_K_S - 10.4GB | Q3_K_M - 11.5GB )&lt;/li&gt; &lt;li&gt;Devstral-Small-2507 ( &lt;strong&gt;IQ3_XXS&lt;/strong&gt; - 9.41GB | Q3_K_S - 10.4GB | Q3_K_M - 11.5GB )&lt;/li&gt; &lt;li&gt;reka-flash-3.1 ( &lt;strong&gt;IQ3_XXS&lt;/strong&gt; - 9.2GB )&lt;/li&gt; &lt;li&gt;Seed-OSS-36B-Instruct ( IQ3_XXS - 14.3GB | &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 10.2GB )&lt;/li&gt; &lt;li&gt;GLM-4-32B-0414 ( IQ3_XXS - 13GB | &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 9.26GB )&lt;/li&gt; &lt;li&gt;Gemma-3-27B-it ( IQ3_XXS - 10.8GB | &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 7.85GB )&lt;/li&gt; &lt;li&gt;Qwen3-32B ( IQ3_XXS - 13GB | &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 9.3GB )&lt;/li&gt; &lt;li&gt;KAT-V1-40B ( &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 11.1GB )&lt;/li&gt; &lt;li&gt;KAT-Dev ( IQ3_XXS - 12.8GB | &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 9.1GB )&lt;/li&gt; &lt;li&gt;EXAONE-4.0.1-32B ( IQ3_XXS - 12.5GB | &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 8.7GB )&lt;/li&gt; &lt;li&gt;Falcon-H1-34B-Instruct ( IQ3_XXS - 13.5GB | &lt;strong&gt;IQ2_XXS&lt;/strong&gt; - 9.8GB )&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share your thoughts. Thanks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;BTW I'm able to run ~30B MOE models &amp;amp; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nyxmci/poor_gpu_club_8gb_vram_qwen330ba3b_gptoss20b_ts/"&gt;posted a thread recently&lt;/a&gt;. Here my above list contains some models without MOE or small size choices. It seems I can skip Gemma &amp;amp; Qwen from the list since we have low size models from them. But for other few models, I don't have choice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zz30/poor_gpu_club_anyone_use_q3q2_quants_of_2040b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zz30/poor_gpu_club_anyone_use_q3q2_quants_of_2040b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zz30/poor_gpu_club_anyone_use_q3q2_quants_of_2040b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T16:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o44bom</id>
    <title>[Looking for testers] TraceML: Live GPU/memory tracing for PyTorch fine-tuning</title>
    <updated>2025-10-11T19:22:42+00:00</updated>
    <author>
      <name>/u/traceml-ai</name>
      <uri>https://old.reddit.com/user/traceml-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for a few people to test TraceML, an open-source tool that shows GPU/CPU/memory usage live during training. It is for spotting CUDA OOMs and inefficiency.&lt;/p&gt; &lt;p&gt;It works for single-GPU fine-tuning and tracks activation + gradient peaks, per-layer memory, and step timings (forward/backward/optimizer).&lt;/p&gt; &lt;p&gt;Repo: github.com/traceopt-ai/traceml&lt;/p&gt; &lt;p&gt;I.would love to find a couple of regular testers / design partners whose feedback can shape what to build next. Active contributors will also be mentioned in the README 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/traceml-ai"&gt; /u/traceml-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o44bom/looking_for_testers_traceml_live_gpumemory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o44bom/looking_for_testers_traceml_live_gpumemory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o44bom/looking_for_testers_traceml_live_gpumemory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T19:22:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o408is</id>
    <title>How to use GLM Plan + Claude Plan with Claude Code on macOS</title>
    <updated>2025-10-11T16:38:25+00:00</updated>
    <author>
      <name>/u/Routine-Teach5293</name>
      <uri>https://old.reddit.com/user/Routine-Teach5293</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o408is/how_to_use_glm_plan_claude_plan_with_claude_code/"&gt; &lt;img alt="How to use GLM Plan + Claude Plan with Claude Code on macOS" src="https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f53460a90493497883ab4cacbbb58e2acb464c4" title="How to use GLM Plan + Claude Plan with Claude Code on macOS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Routine-Teach5293"&gt; /u/Routine-Teach5293 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gist.github.com/RuiNelson/a5af5620404a0a9fbf3cf3e92fe97585"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o408is/how_to_use_glm_plan_claude_plan_with_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o408is/how_to_use_glm_plan_claude_plan_with_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T16:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1o44et0</id>
    <title>Question on privacy when using Openrouter API</title>
    <updated>2025-10-11T19:26:20+00:00</updated>
    <author>
      <name>/u/JaniceRaynor</name>
      <uri>https://old.reddit.com/user/JaniceRaynor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am unable to run a fully local LLM on my old laptop, so I need to use an LLM in the cloud. &lt;/p&gt; &lt;p&gt;Excluding fully local LLM, Duck.ai is so far one of the most private ones. As far as I know, these are the privacy upside of using duck.ai:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;All messages goes through DuckDuckGo’s proxy to the LLM provider, making everyone look the same to the providers as if duck.ai is the one that is asking all the different questions. &lt;/li&gt; &lt;li&gt;duck.ai has it set so the LLM providers do not train on the data submitted through duck.ai. &lt;/li&gt; &lt;li&gt;all the chats are stored locally on the device in the browser files, not on DuckDuckGo’s servers. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Is using Openrouter API via a local interface like Jan, LMstudio, etc the same in terms of privacy? Since all messages go through Openrouter’s server so it’s indistinguishable which user is asking, users can turn off data training from within the openrouter settings, and the chat history are stored locally within Jan, LMstudio app. Am I missing anything or is openrouter API with a local app interface just as private as Duck.ai? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JaniceRaynor"&gt; /u/JaniceRaynor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o44et0/question_on_privacy_when_using_openrouter_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o44et0/question_on_privacy_when_using_openrouter_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o44et0/question_on_privacy_when_using_openrouter_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T19:26:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3z7q9</id>
    <title>LM Studio + Open-WebUI - no reasoning</title>
    <updated>2025-10-11T15:57:46+00:00</updated>
    <author>
      <name>/u/michalpl7</name>
      <uri>https://old.reddit.com/user/michalpl7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I run &lt;strong&gt;LM Studio&lt;/strong&gt; + &lt;strong&gt;Open-WebUI&lt;/strong&gt; with model &lt;strong&gt;GPT-OSS-20b&lt;/strong&gt; but it's much worse on that web page than used locally in LM Studio, it answers completely stupid. I also don't see the &lt;strong&gt;reasoning button&lt;/strong&gt;, checked models settings in Open-WebUI admin page but there were nothing matching, only vision, file input, code interpreter, etc. Do you know how to make it working same smart with open-webui as local?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michalpl7"&gt; /u/michalpl7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3z7q9/lm_studio_openwebui_no_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3z7q9/lm_studio_openwebui_no_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3z7q9/lm_studio_openwebui_no_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T15:57:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3atdu</id>
    <title>GLM 5 coming before the end of 2025</title>
    <updated>2025-10-10T19:41:19+00:00</updated>
    <author>
      <name>/u/Helpful_Jacket8953</name>
      <uri>https://old.reddit.com/user/Helpful_Jacket8953</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"&gt; &lt;img alt="GLM 5 coming before the end of 2025" src="https://a.thumbs.redditmedia.com/hVBgymEdEuqZZKnYwcpRm5UQBweQDbSo5chE1AGg9a8.jpg" title="GLM 5 coming before the end of 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get ready. At this rate it seems like there's a real chance it'll start surpassing SOTA models on some benchmarks, not just DeepSeek.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2knp5zv98cuf1.png?width=1556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4458e1672e8db34aceb649902b4d838d88335dc2"&gt;https://preview.redd.it/2knp5zv98cuf1.png?width=1556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4458e1672e8db34aceb649902b4d838d88335dc2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Helpful_Jacket8953"&gt; /u/Helpful_Jacket8953 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T19:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3zgy7</id>
    <title>The LLM running on my local PC is too slow.</title>
    <updated>2025-10-11T16:07:43+00:00</updated>
    <author>
      <name>/u/Glanble</name>
      <uri>https://old.reddit.com/user/Glanble</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I'm getting really slow speeds and need a sanity check.&lt;br /&gt; I'm only getting 1.0 t/s running a C4AI 111B model (63GB Q4_GGUF) on an RTX 5090 with 128GB of RAM.&lt;br /&gt; this normal, or is something wrong with my config?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glanble"&gt; /u/Glanble &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zgy7/the_llm_running_on_my_local_pc_is_too_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zgy7/the_llm_running_on_my_local_pc_is_too_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zgy7/the_llm_running_on_my_local_pc_is_too_slow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T16:07:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4290b</id>
    <title>Wanted to share tool for linking LM Studio/Ollama to a discord bot for mobile chatting!</title>
    <updated>2025-10-11T17:59:36+00:00</updated>
    <author>
      <name>/u/ella0333</name>
      <uri>https://old.reddit.com/user/ella0333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built this for myself while I was rating chats for RLHF training and wanted to do it from my phone. I felt this was the easiest way to get my models on mobile, saves chat logs, message ratings and has a quick and easy setup! &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ella0333/Local-LLM-Discord-Bot"&gt;https://github.com/ella0333/Local-LLM-Discord-Bot&lt;/a&gt; (free/opensource) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ella0333"&gt; /u/ella0333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4290b/wanted_to_share_tool_for_linking_lm_studioollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4290b/wanted_to_share_tool_for_linking_lm_studioollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4290b/wanted_to_share_tool_for_linking_lm_studioollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T17:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o371t4</id>
    <title>bro disappeared like he never existed</title>
    <updated>2025-10-10T17:18:54+00:00</updated>
    <author>
      <name>/u/Full_Piano_3448</name>
      <uri>https://old.reddit.com/user/Full_Piano_3448</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o371t4/bro_disappeared_like_he_never_existed/"&gt; &lt;img alt="bro disappeared like he never existed" src="https://preview.redd.it/2e01fz4pibuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2158310d30c308746ab8924442748cf6a37b692a" title="bro disappeared like he never existed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Knowing him is a sign you’ve been in the AI game for a long time (iykyk)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Full_Piano_3448"&gt; /u/Full_Piano_3448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2e01fz4pibuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o371t4/bro_disappeared_like_he_never_existed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o371t4/bro_disappeared_like_he_never_existed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T17:18:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3vnt3</id>
    <title>Tooling+Model recommendations for base (16G) mac Mini M4 as remote server?</title>
    <updated>2025-10-11T13:28:29+00:00</updated>
    <author>
      <name>/u/Valuable-Question706</name>
      <uri>https://old.reddit.com/user/Valuable-Question706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Intel laptop as my main coding machine. Recently got myself a base model mac Mini and got surprised how fast it is for inference.&lt;/p&gt; &lt;p&gt;I'm still very new at using AI for coding. Not trying to be lazy, but want to get an advice in a large and quickly developing field from knowledgeable people.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;What I already tried: &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; in VS studio + ollama with qwen2.5-coder:7B. It works, but is there a better, more efficient way? I'm quite technical so I won't mind running more complex software stack if it brings significant improvements.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I'd like to automate some routine, boring programming tasks, for example: writing boilerplate html/js, writing bash scripts (yes, I very carefully check them before running), writing basic, boring python code. Nothing too complex, because I still prefer using my brain for actual work, plus even paid edge models are still not good at my area.&lt;/p&gt; &lt;p&gt;So I need a model that is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;is good at tasks specified above (should I use a specially optimized model or generic ones are OK?)&lt;/li&gt; &lt;li&gt;outputs at least 15+ tokens/sec&lt;/li&gt; &lt;li&gt;would integrate nicely with tooling on my work machine&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also, what does a proper, modern VS code setup looks nowadays? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Question706"&gt; /u/Valuable-Question706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3vnt3/toolingmodel_recommendations_for_base_16g_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3vnt3/toolingmodel_recommendations_for_base_16g_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3vnt3/toolingmodel_recommendations_for_base_16g_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T13:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3l5zs</id>
    <title>GLM just blow up, or have I been in the dark?</title>
    <updated>2025-10-11T03:24:01+00:00</updated>
    <author>
      <name>/u/EasyConference4177</name>
      <uri>https://old.reddit.com/user/EasyConference4177</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like this community is ever moving, did GLM just blow up? like, I did not realise so many people talked about it.... What kinda system are you guys on 4.6 running? Because it looks like I would essential need 4x48gb Quadro 8000s/a6000s/6000 ada GPUs or at least 2x96gb RTX Pro 6000s... I may can afford 4 quadros but not 2 rtx pro 6000s, for the price of a car. lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasyConference4177"&gt; /u/EasyConference4177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3l5zs/glm_just_blow_up_or_have_i_been_in_the_dark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3l5zs/glm_just_blow_up_or_have_i_been_in_the_dark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3l5zs/glm_just_blow_up_or_have_i_been_in_the_dark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T03:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o42jx1</id>
    <title>Optimize my environment for GLM 4.5 Air</title>
    <updated>2025-10-11T18:11:28+00:00</updated>
    <author>
      <name>/u/Former-Tangerine-723</name>
      <uri>https://old.reddit.com/user/Former-Tangerine-723</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there people. For the last month I am using GLM air (4 K S quant) and I really like it! It's super smart and always to the point! I only have one problem, the t/s are really low (6-7 tk/s) So im looking for a way to upgrade my local rig, that's why I call you, the smart people! ☺️ My current setup is AMD 7600 cpu, 64 gb ddr5 6000, and two cpus, 1 5060ti 16gb and 1 4060ti 16gb. My backend is LM Studio. So, should I change backend? Should I get a third GPU? What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Former-Tangerine-723"&gt; /u/Former-Tangerine-723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o42jx1/optimize_my_environment_for_glm_45_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o42jx1/optimize_my_environment_for_glm_45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o42jx1/optimize_my_environment_for_glm_45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T18:11:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o394p3</id>
    <title>Here we go again</title>
    <updated>2025-10-10T18:36:34+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"&gt; &lt;img alt="Here we go again" src="https://preview.redd.it/b2abfaikwbuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7db3949cb0def07809e7a9ba9a730d1582083844" title="Here we go again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b2abfaikwbuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T18:36:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1o42ch4</id>
    <title>Choosing a code completion (FIM) model</title>
    <updated>2025-10-11T18:03:11+00:00</updated>
    <author>
      <name>/u/Zc5Gwu</name>
      <uri>https://old.reddit.com/user/Zc5Gwu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fill-in-the-middle (FIM) models don't necessarily get all of the attention that coder models get but they work great with llama.cpp and &lt;a href="https://github.com/ggml-org/llama.vim"&gt;llama.vim&lt;/a&gt; or &lt;a href="https://github.com/ggml-org/llama.vscode"&gt;llama.vscode&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Generally, when picking an FIM model, &lt;strong&gt;&lt;em&gt;speed&lt;/em&gt;&lt;/strong&gt; is absolute priority because no one wants to sit waiting for the completion to finish. Choosing models with few active parameters and running GPU only is key. Also, counterintuitively, &amp;quot;base&amp;quot; models work just as well as instruct models. Try to aim for &amp;gt;70 t/s.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Note that only some models support FIM.&lt;/em&gt;&lt;/strong&gt; Sometimes, it can be hard to tell from model cards whether they are supported or not.&lt;/p&gt; &lt;p&gt;Recent models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt; (the larger variant might also be FIM, I don't have the hardware to try it)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Kwaipilot/KwaiCoder-23B-A4B-v1"&gt;Kwaipilot/KwaiCoder-23B-A4B-v1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Kwaipilot/KwaiCoder-DS-V2-Lite-Base"&gt;Kwaipilot/KwaiCoder-DS-V2-Lite-Base&lt;/a&gt; (16b 2.4b active)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Slightly older but reliable small models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-3B"&gt;Qwen/Qwen2.5-Coder-3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B"&gt;Qwen/Qwen2.5-Coder-1.5B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Untested, new models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Salesforce/CoDA-v0-Instruct"&gt;Salesforce/CoDA-v0-Instruct&lt;/a&gt; (I'm unsure if this is FIM)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What models am I missing? What models are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zc5Gwu"&gt; /u/Zc5Gwu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o42ch4/choosing_a_code_completion_fim_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o42ch4/choosing_a_code_completion_fim_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o42ch4/choosing_a_code_completion_fim_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T18:03:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3xluf</id>
    <title>Fighting Email Spam on Your Mail Server with LLMs — Privately</title>
    <updated>2025-10-11T14:52:20+00:00</updated>
    <author>
      <name>/u/unixf0x</name>
      <uri>https://old.reddit.com/user/unixf0x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sharing a blog post I wrote: &lt;a href="https://cybercarnet.eu/posts/email-spam-llm/"&gt;https://cybercarnet.eu/posts/email-spam-llm/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's about how to use local LLMs on your own mail server to identify and fight email spam.&lt;/p&gt; &lt;p&gt;This uses Mailcow, Rspamd, Ollama and a custom proxy in python.&lt;/p&gt; &lt;p&gt;Give your opinion, what you think about the post. If this could be useful for those of you that self-host mail servers.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unixf0x"&gt; /u/unixf0x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3xluf/fighting_email_spam_on_your_mail_server_with_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3xluf/fighting_email_spam_on_your_mail_server_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3xluf/fighting_email_spam_on_your_mail_server_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T14:52:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o43qhn</id>
    <title>What rig are you running to fuel your LLM addiction?</title>
    <updated>2025-10-11T18:59:15+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post your shitboxes, H100's, nvidya 3080ti's, RAM-only setups, MI300X's, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o43qhn/what_rig_are_you_running_to_fuel_your_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o43qhn/what_rig_are_you_running_to_fuel_your_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o43qhn/what_rig_are_you_running_to_fuel_your_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T18:59:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3opq5</id>
    <title>What the sub feels like lately</title>
    <updated>2025-10-11T06:47:33+00:00</updated>
    <author>
      <name>/u/marderbot13</name>
      <uri>https://old.reddit.com/user/marderbot13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"&gt; &lt;img alt="What the sub feels like lately" src="https://preview.redd.it/92s8znbxifuf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb4866bff0d572386ea47fc19d643a6b2261fbdb" title="What the sub feels like lately" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marderbot13"&gt; /u/marderbot13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/92s8znbxifuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T06:47:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
