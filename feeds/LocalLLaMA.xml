<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-08T23:36:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1luhmmi</id>
    <title>Bytedance releases new agentic coding assistant: Trae-Agent</title>
    <updated>2025-07-08T06:36:23+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luhmmi/bytedance_releases_new_agentic_coding_assistant/"&gt; &lt;img alt="Bytedance releases new agentic coding assistant: Trae-Agent" src="https://external-preview.redd.it/2kbD9hIKBj55ykS2AmlC98FIs3m9CAJZ5myO4lqm-lw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcfc5a3088bfab4d6be53d66237a02b38cc2d358" title="Bytedance releases new agentic coding assistant: Trae-Agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/bytedance/trae-agent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luhmmi/bytedance_releases_new_agentic_coding_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luhmmi/bytedance_releases_new_agentic_coding_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T06:36:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv1763</id>
    <title>Prompt to "compress" transcripts</title>
    <updated>2025-07-08T21:40:21+00:00</updated>
    <author>
      <name>/u/simondueckert</name>
      <uri>https://old.reddit.com/user/simondueckert</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im have a hugh collection of transcripts from talks, podcasts, presentations etc. Often I want to use lots of them in a local AI tool, but the texts are too long for the context windwow. I wonder if there are good prompts to compress (summarize) the transcripts in a way that no details or key concepts are lost nut the size of the text gets way smaller. Any research or experience on that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simondueckert"&gt; /u/simondueckert &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv1763/prompt_to_compress_transcripts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv1763/prompt_to_compress_transcripts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv1763/prompt_to_compress_transcripts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T21:40:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv1m0i</id>
    <title>Best context compression other than llmlingua?</title>
    <updated>2025-07-08T21:57:26+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also would love to know your experiences with context/prompt compression, is it worth it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv1m0i/best_context_compression_other_than_llmlingua/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv1m0i/best_context_compression_other_than_llmlingua/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv1m0i/best_context_compression_other_than_llmlingua/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T21:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1luybka</id>
    <title>LLM Hallucination Detection Leaderboard for both RAG and Chat</title>
    <updated>2025-07-08T19:46:43+00:00</updated>
    <author>
      <name>/u/cakesir</name>
      <uri>https://old.reddit.com/user/cakesir</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luybka/llm_hallucination_detection_leaderboard_for_both/"&gt; &lt;img alt="LLM Hallucination Detection Leaderboard for both RAG and Chat" src="https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69ebf8eb874819abf47adedb14a04f419a0c710d" title="LLM Hallucination Detection Leaderboard for both RAG and Chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;does this track with your experiences?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cakesir"&gt; /u/cakesir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/kluster-ai/LLM-Hallucination-Detection-Leaderboard"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luybka/llm_hallucination_detection_leaderboard_for_both/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luybka/llm_hallucination_detection_leaderboard_for_both/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T19:46:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1luytx2</id>
    <title>Best Local LLM for Code assist similar to Sonnet 4?</title>
    <updated>2025-07-08T20:06:35+00:00</updated>
    <author>
      <name>/u/Kainzo</name>
      <uri>https://old.reddit.com/user/Kainzo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luytx2/best_local_llm_for_code_assist_similar_to_sonnet_4/"&gt; &lt;img alt="Best Local LLM for Code assist similar to Sonnet 4?" src="https://b.thumbs.redditmedia.com/aFc1eTSyhpbPCDNRYEtxNU2fc0UmI3sPPSwMWbqT_BI.jpg" title="Best Local LLM for Code assist similar to Sonnet 4?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to sort through and host my own LLM but I really dont know how these might compare, I'm using Void as the IDE and trying to replicate pretty close to what Cursor offers. &lt;/p&gt; &lt;p&gt;Is it even comparable?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tv72kg9xipbf1.png?width=628&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee67695b0a353a8b0805fc8b111ba81fe0d36fd6"&gt;https://preview.redd.it/tv72kg9xipbf1.png?width=628&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee67695b0a353a8b0805fc8b111ba81fe0d36fd6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kainzo"&gt; /u/Kainzo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luytx2/best_local_llm_for_code_assist_similar_to_sonnet_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luytx2/best_local_llm_for_code_assist_similar_to_sonnet_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luytx2/best_local_llm_for_code_assist_similar_to_sonnet_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T20:06:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu5g8c</id>
    <title>Thanks to you, I built an open-source website that can watch your screen and trigger actions. It runs 100% locally and was inspired by all of you!</title>
    <updated>2025-07-07T20:43:03+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: I'm a solo dev who wanted a simple, private way to have local LLMs watch my screen and do simple logging/notifying. I'm launching the open-source tool for it, Observer AI, this Friday. It's built for this community, and I'd love your feedback.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Some of you might remember my earlier posts showing off a local agent framework I was tinkering with. Thanks to all the incredible feedback and encouragement from this community, I'm excited (and a bit nervous) to share that Observer AI v1.0 is launching this &lt;strong&gt;Friday&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;This isn't just an announcement; it's a huge &lt;strong&gt;thank you&lt;/strong&gt; note.&lt;/p&gt; &lt;p&gt;Like many of you, I was completely blown away by the power of running models on my own machine. But I hit a wall: I wanted a super simple, minimal, but powerful way to connect these models to my own computer‚Äîto let them see my screen, react to events, and log things.&lt;/p&gt; &lt;p&gt;That's why I started building &lt;strong&gt;Observer AI üëÅÔ∏è&lt;/strong&gt;: a privacy-first, open-source platform for building your own micro-agents that run entirely locally!&lt;/p&gt; &lt;h1&gt;What Can You Actually Do With It?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Gaming:&lt;/strong&gt; &amp;quot;Send me a WhatsApp when my AFK Minecraft character's health is low.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Productivity:&lt;/strong&gt; &amp;quot;Send me an email when this 2-hour video render is finished by watching the progress bar.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Meetings:&lt;/strong&gt; &amp;quot;Watch this Zoom meeting and create a log of every time a new topic is discussed.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Security:&lt;/strong&gt; &amp;quot;Start a screen recording the moment a person appears on my security camera feed.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try it out in your browser with zero setup, and make it &lt;strong&gt;100% local with a single command:&lt;/strong&gt; docker compose up --build.&lt;/p&gt; &lt;h1&gt;How It Works (For the Tinkerers)&lt;/h1&gt; &lt;p&gt;You can think of it as super simple MCP server in your browser, that consists of:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Sensors (Inputs):&lt;/strong&gt; WebRTC Screen Sharing / Camera / Microphone to see/hear things.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model (The Brain):&lt;/strong&gt; Any Ollama model, running locally. You give it a system prompt and the sensor data. (adding support for llama.cpp soon!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tools (Actions):&lt;/strong&gt; What the agent can do with the model's response. notify(), sendEmail(), startClip(), and you can even run your own code.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;My Commitment &amp;amp; A Sustainable Future&lt;/h1&gt; &lt;p&gt;The core Observer AI platform is, and will always be, &lt;strong&gt;free and open-source.&lt;/strong&gt; That's non-negotiable. The code is all on GitHub for you to use, fork, and inspect.&lt;/p&gt; &lt;p&gt;To keep this project alive and kicking long-term (I'm a solo dev, so server costs and coffee are my main fuel!), I'm also introducing an optional &lt;strong&gt;Observer Pro&lt;/strong&gt; subscription. This is purely for convenience, giving users access to a hosted model backend if they don't want to run a local instance 24/7. It‚Äôs my attempt at making the project sustainable without compromising the open-source core.&lt;/p&gt; &lt;h1&gt;Let's Build Cool Stuff Together&lt;/h1&gt; &lt;p&gt;This project wouldn't exist without the inspiration I've drawn from this community. You are the people I'm building this for.&lt;/p&gt; &lt;p&gt;I'd be incredibly grateful if you'd take a look. Star the repo if you think it's cool, try building an agent, and please, let me know what you think. Your feedback is what will guide v1.1 and beyond.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub (All the code is here!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;App Link:&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Twitter/X:&lt;/strong&gt; &lt;a href="https://x.com/AppObserverAI"&gt;https://x.com/AppObserverAI&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out here all day to answer any and all questions. Thank you again for everything!&lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu5g8c/thanks_to_you_i_built_an_opensource_website_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu5g8c/thanks_to_you_i_built_an_opensource_website_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu5g8c/thanks_to_you_i_built_an_opensource_website_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T20:43:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lurv79</id>
    <title>Major Hugging Face announcement on July 24th</title>
    <updated>2025-07-08T15:40:15+00:00</updated>
    <author>
      <name>/u/LightEt3rnaL</name>
      <uri>https://old.reddit.com/user/LightEt3rnaL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurv79/major_hugging_face_announcement_on_july_24th/"&gt; &lt;img alt="Major Hugging Face announcement on July 24th" src="https://b.thumbs.redditmedia.com/Si8awmHDpB69t1JGly1pXoSydscVCXoxla1VnwQVMbE.jpg" title="Major Hugging Face announcement on July 24th" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vwc0npf97obf1.png?width=611&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8d87f24a4c50c6bb6f4fe41460ac027808c04e8"&gt;https://preview.redd.it/vwc0npf97obf1.png?width=611&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8d87f24a4c50c6bb6f4fe41460ac027808c04e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any ideas what it might be?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LightEt3rnaL"&gt; /u/LightEt3rnaL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurv79/major_hugging_face_announcement_on_july_24th/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurv79/major_hugging_face_announcement_on_july_24th/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lurv79/major_hugging_face_announcement_on_july_24th/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:40:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lulpev</id>
    <title>SK Telecom released Korean-focused continual pretraining of Qwen2.5</title>
    <updated>2025-07-08T11:05:24+00:00</updated>
    <author>
      <name>/u/Then-Reveal-2162</name>
      <uri>https://old.reddit.com/user/Then-Reveal-2162</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been testing these for Korean projects. Two models:&lt;/p&gt; &lt;p&gt;72B version: &lt;a href="https://huggingface.co/skt/A.X-4.0"&gt;https://huggingface.co/skt/A.X-4.0&lt;/a&gt;&lt;br /&gt; 7B version: &lt;a href="https://huggingface.co/skt/A.X-4.0-Light"&gt;https://huggingface.co/skt/A.X-4.0-Light&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Benchmarks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;KMMLU: 78.3 (GPT-4o: 72.5) - Korean version of MMLU with 35k questions from Korean exams&lt;/li&gt; &lt;li&gt;CLIcK: 83.5 (GPT-4o: 80.2) - tests Korean cultural and linguistic understanding&lt;/li&gt; &lt;li&gt;Uses ~33% fewer tokens for Korean&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Then-Reveal-2162"&gt; /u/Then-Reveal-2162 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lulpev/sk_telecom_released_koreanfocused_continual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lulpev/sk_telecom_released_koreanfocused_continual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lulpev/sk_telecom_released_koreanfocused_continual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T11:05:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lumgjj</id>
    <title>New model GLM-Experimental is quite good (not local so far)</title>
    <updated>2025-07-08T11:47:13+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://chat.z.ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lumgjj/new_model_glmexperimental_is_quite_good_not_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lumgjj/new_model_glmexperimental_is_quite_good_not_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T11:47:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1luh1w3</id>
    <title>Gemma 3n on phone with 6GB of ram</title>
    <updated>2025-07-08T05:59:38+00:00</updated>
    <author>
      <name>/u/Thedudely1</name>
      <uri>https://old.reddit.com/user/Thedudely1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luh1w3/gemma_3n_on_phone_with_6gb_of_ram/"&gt; &lt;img alt="Gemma 3n on phone with 6GB of ram" src="https://preview.redd.it/3yac87hublbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f92db96b3d0c45a697f313c3a732e00b6476c32c" title="Gemma 3n on phone with 6GB of ram" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tokens per second is quite slow on my Pixel 6a (0.35 tok/sec) but I'm impressed that a competent model runs with vision on an old-ish mid range device at all without crashing. I'm using the 2b parameter version instead of the 4b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thedudely1"&gt; /u/Thedudely1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3yac87hublbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luh1w3/gemma_3n_on_phone_with_6gb_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luh1w3/gemma_3n_on_phone_with_6gb_of_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T05:59:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1luw2yu</id>
    <title>In-browser Local Document Understanding Using SmolDocling 256M with Transformers.js</title>
    <updated>2025-07-08T18:20:48+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luw2yu/inbrowser_local_document_understanding_using/"&gt; &lt;img alt="In-browser Local Document Understanding Using SmolDocling 256M with Transformers.js" src="https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c30389f676a71708930e865f312a96dcfa3be046" title="In-browser Local Document Understanding Using SmolDocling 256M with Transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! A couple of days ago, I came across SmolDocling-256M and liked how well it performed for its size with document understanding and feature extraction. As such, I wanted to try my hand at creating a demo for it using Transformers.js since there weren't any that I saw. &lt;/p&gt; &lt;p&gt;Anyway, how it works is that the model takes in a document image and (given a prompt) produces a structured representation of the document using &lt;a href="https://github.com/docling-project/docling/discussions/354"&gt;DocTags&lt;/a&gt; &lt;a href="https://arxiv.org/html/2503.11576v1#S3"&gt;(a custom markup language format made by the Docling team from what I've gathered)&lt;/a&gt;, then that output is parsed the old fashioned way to create machine readable forms of the document like markdown and JSON.&lt;/p&gt; &lt;p&gt;Check it out for yourselves!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/callbacked/smoldocling256M-webgpu"&gt;HF Space&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/callbacked/smoldocling256M-webgpu"&gt;Demo Repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/callbacked/smoldocling256M-webgpu"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zm461kmdzobf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luw2yu/inbrowser_local_document_understanding_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luw2yu/inbrowser_local_document_understanding_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T18:20:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv1z7b</id>
    <title>Why hasn't RTX Pro 6000 Balckwell significantly shake down the price of older RTX 6000 / RTX 6000 Ada</title>
    <updated>2025-07-08T22:12:27+00:00</updated>
    <author>
      <name>/u/--dany--</name>
      <uri>https://old.reddit.com/user/--dany--</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX Pro 6000 Blackwell is much better with 30% more CUDA cores and twice the VRAM, than RTX 6000 Ada (and even better than RTX 6000), but the price difference is really minimum, like the prices of those 3 generations are only $1k apart for new ($8k, $7k and $6k) and $2k apart for used ($8k - only new, $6k and $4k).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/--dany--"&gt; /u/--dany-- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv1z7b/why_hasnt_rtx_pro_6000_balckwell_significantly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv1z7b/why_hasnt_rtx_pro_6000_balckwell_significantly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv1z7b/why_hasnt_rtx_pro_6000_balckwell_significantly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T22:12:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1luwtdr</id>
    <title>NSFW Model image analysis</title>
    <updated>2025-07-08T18:48:31+00:00</updated>
    <author>
      <name>/u/Technical_Whole_947</name>
      <uri>https://old.reddit.com/user/Technical_Whole_947</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys like the title says im looking for a model or models I can use to send images to and discuss them. I want it to have support for NSFW content. I'd prefer a ui like oobabooga but I've h3ards it has issues with this kind of stuff. Image generation is a plus but not needed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical_Whole_947"&gt; /u/Technical_Whole_947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T18:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1luy32e</id>
    <title>SmolLM3 has day-0 support in MistralRS!</title>
    <updated>2025-07-08T19:37:29+00:00</updated>
    <author>
      <name>/u/EricBuehler</name>
      <uri>https://old.reddit.com/user/EricBuehler</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/"&gt; &lt;img alt="SmolLM3 has day-0 support in MistralRS!" src="https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07999e9a892b675527d3e33998c11728e0e28b01" title="SmolLM3 has day-0 support in MistralRS!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a &lt;strong&gt;SoTA 3B model&lt;/strong&gt; with hybrid &lt;strong&gt;reasoning&lt;/strong&gt; and &lt;strong&gt;128k context&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Hits ‚ö°105 T/s with AFQ4 @ M3 Max.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/EricLBuehler/mistral.rs"&gt;https://github.com/EricLBuehler/mistral.rs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Using MistralRS means that you get&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Builtin MCP client&lt;/li&gt; &lt;li&gt;OpenAI HTTP server&lt;/li&gt; &lt;li&gt;Python &amp;amp; Rust APIs&lt;/li&gt; &lt;li&gt;Full multimodal inference engine (&lt;strong&gt;in&lt;/strong&gt;: image, audio, text in, &lt;strong&gt;out:&lt;/strong&gt; image, audio, text).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Super easy to run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./mistralrs_server -i run -m HuggingFaceTB/SmolLM3-3B &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What's next for MistralRS? Full Gemma 3n support, multi-device backend, and more. Stay tuned!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1luy32e/video/kkojaflgdpbf1/player"&gt;https://reddit.com/link/1luy32e/video/kkojaflgdpbf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EricBuehler"&gt; /u/EricBuehler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T19:37:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lurili</id>
    <title>Practical Attacks on AI Text Classifiers with RL (Qwen/Llama, datasets and models available for download)</title>
    <updated>2025-07-08T15:26:41+00:00</updated>
    <author>
      <name>/u/WithoutReason1729</name>
      <uri>https://old.reddit.com/user/WithoutReason1729</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurili/practical_attacks_on_ai_text_classifiers_with_rl/"&gt; &lt;img alt="Practical Attacks on AI Text Classifiers with RL (Qwen/Llama, datasets and models available for download)" src="https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c0bbd88f042ac1925e2d60123ccd65702e90497" title="Practical Attacks on AI Text Classifiers with RL (Qwen/Llama, datasets and models available for download)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WithoutReason1729"&gt; /u/WithoutReason1729 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://trentmkelly.substack.com/p/practical-attacks-on-ai-text-classifiers"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurili/practical_attacks_on_ai_text_classifiers_with_rl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lurili/practical_attacks_on_ai_text_classifiers_with_rl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:26:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1luq8hp</id>
    <title>Skywork/Skywork-R1V3-38B ¬∑ Hugging Face</title>
    <updated>2025-07-08T14:37:34+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luq8hp/skyworkskyworkr1v338b_hugging_face/"&gt; &lt;img alt="Skywork/Skywork-R1V3-38B ¬∑ Hugging Face" src="https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9781820f621f09b406ca5a209d2d1f7685f966ef" title="Skywork/Skywork-R1V3-38B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Skywork-R1V3-38B&lt;/strong&gt; is the &lt;strong&gt;latest and most powerful open-source multimodal reasoning model&lt;/strong&gt; in the Skywork series, pushing the boundaries of multimodal and cross-disciplinary intelligence. With elaborate RL algorithm in the post-training stage, R1V3 significantly enhances multimodal reasoning ablity and achieves &lt;strong&gt;open-source state-of-the-art (SOTA)&lt;/strong&gt; performance across multiple multimodal reasoning benchmarks.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V3-38B#2-evaluation"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V3-38B#%F0%9F%8C%9F-key-results"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;üåü Key Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MMMU:&lt;/strong&gt; 76.0 ‚Äî &lt;em&gt;Open-source SOTA, approaching human experts (76.2)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;EMMA-Mini(CoT):&lt;/strong&gt; 40.3 ‚Äî &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MMK12:&lt;/strong&gt; 78.5 ‚Äî &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Physics Reasoning:&lt;/strong&gt; PhyX-MC-TM (52.8), SeePhys (31.5) ‚Äî &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logic Reasoning:&lt;/strong&gt; MME-Reasoning (42.8) ‚Äî &lt;em&gt;Beats Claude-4-Sonnet&lt;/em&gt;, VisuLogic (28.5) ‚Äî &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Math Benchmarks:&lt;/strong&gt; MathVista (77.1), MathVerse (59.6), MathVision (52.6) ‚Äî &lt;em&gt;Exceptional problem-solving&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V3-38B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luq8hp/skyworkskyworkr1v338b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luq8hp/skyworkskyworkr1v338b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T14:37:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv2t7n</id>
    <title>"Not x, but y" Slop Leaderboard</title>
    <updated>2025-07-08T22:48:41+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"&gt; &lt;img alt="&amp;quot;Not x, but y&amp;quot; Slop Leaderboard" src="https://preview.redd.it/nxw6fmegaqbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f634168f40782641454db362ee799df6971e84f" title="&amp;quot;Not x, but y&amp;quot; Slop Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models have been converging on &amp;quot;not x, but y&amp;quot; type phrases to an absurd degree. So here's a leaderboard for it. &lt;/p&gt; &lt;p&gt;I don't think many labs are targeting this kind of slop in their training set filtering, so it gets compounded with subsequent model generations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nxw6fmegaqbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T22:48:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1luxu6s</id>
    <title>Any one tried ERNIE-4.5-21B-A3B?</title>
    <updated>2025-07-08T19:27:45+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any one tried ERNIE-4.5-21B-A3B? How is that compared to Qwen3-30B-A3B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luxu6s/any_one_tried_ernie4521ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luxu6s/any_one_tried_ernie4521ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luxu6s/any_one_tried_ernie4521ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T19:27:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lumsd2</id>
    <title>Mac Studio 512GB online!</title>
    <updated>2025-07-08T12:04:20+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just had a $10k Mac Studio arrive. The first thing I installed was LM Studio. I downloaded qwen3-235b-a22b and fired it up. Fantastic performance with a small system prompt. I fired up devstral and tried to use it with Cline (a large system prompt agent) and very quickly discovered limitations. I managed to instruct the poor LLM to load the memory bank but it lacked all the comprehension that I get from google gemini. Next I'm going to try to use devstral in Act mode only and see if I can at least get some tool usage and code generation out of it, but I have serious doubts it will even work. I think a bigger reasoning model is needed for my use cases and this system would just be too slow to accomplish that.&lt;/p&gt; &lt;p&gt;That said, I wanted to share my experiences with the community. If anyone is thinking about buying a mac studio for LLMs, I'm happy to run any sort of use case evaluation for you to help you make your decision. Just comment in here and be sure to upvote if you do so other people see the post and can ask questions too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lumsd2/mac_studio_512gb_online/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lumsd2/mac_studio_512gb_online/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lumsd2/mac_studio_512gb_online/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T12:04:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lujedm</id>
    <title>Hunyuan-A13B model support has been merged into llama.cpp</title>
    <updated>2025-07-08T08:36:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lujedm/hunyuana13b_model_support_has_been_merged_into/"&gt; &lt;img alt="Hunyuan-A13B model support has been merged into llama.cpp" src="https://external-preview.redd.it/9jUZNMJtHKaljkWO0STnEWPE0o_A8ZlYFbsk9KFfTaQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d85e0ea0459ffe03d3921b645c9c77dcaf2f99bd" title="Hunyuan-A13B model support has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14425"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lujedm/hunyuana13b_model_support_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lujedm/hunyuana13b_model_support_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T08:36:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lurzqf</id>
    <title>NextCoder - a Microsoft Collection</title>
    <updated>2025-07-08T15:45:04+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/"&gt; &lt;img alt="NextCoder - a Microsoft Collection" src="https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b2179fc5426163403bc73a148e1730509944514" title="NextCoder - a Microsoft Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/microsoft/nextcoder-6815ee6bfcf4e42f20d45028"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:45:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lus2yw</id>
    <title>new models from NVIDIA: OpenCodeReasoning-Nemotron-1.1 7B/14B/32B</title>
    <updated>2025-07-08T15:48:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenCodeReasoning-Nemotron-1.1-7B is a large language model (LLM) which is a derivative of Qwen2.5-7B-Instruct (AKA the reference model). It is a reasoning model that is post-trained for reasoning for code generation. The model supports a context length of 64k tokens. &lt;/p&gt; &lt;p&gt;This model is ready for commercial/non-commercial use.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;LiveCodeBench&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;QwQ-32B&lt;/td&gt; &lt;td align="left"&gt;61.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;OpenCodeReasoning-Nemotron-1.1-14B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;65.9&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenCodeReasoning-Nemotron-14B&lt;/td&gt; &lt;td align="left"&gt;59.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;OpenCodeReasoning-Nemotron-1.1-32B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;69.9&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenCodeReasoning-Nemotron-32B&lt;/td&gt; &lt;td align="left"&gt;61.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-0528&lt;/td&gt; &lt;td align="left"&gt;73.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1&lt;/td&gt; &lt;td align="left"&gt;65.6&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1luroqh</id>
    <title>NVIDIA‚Äôs Highly Anticipated ‚ÄúMini-Supercomputer,‚Äù the DGX Spark, Launches This Month ‚Äî Bringing Immense AI Power to Your Hands ‚Äî up to 4000$</title>
    <updated>2025-07-08T15:33:16+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"&gt; &lt;img alt="NVIDIA‚Äôs Highly Anticipated ‚ÄúMini-Supercomputer,‚Äù the DGX Spark, Launches This Month ‚Äî Bringing Immense AI Power to Your Hands ‚Äî up to 4000$" src="https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8436d2033ab2a873dac41641dd69093f14dcb51c" title="NVIDIA‚Äôs Highly Anticipated ‚ÄúMini-Supercomputer,‚Äù the DGX Spark, Launches This Month ‚Äî Bringing Immense AI Power to Your Hands ‚Äî up to 4000$" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lusr7l</id>
    <title>SmolLM3: reasoning, long context and multilinguality for 3B parameter only</title>
    <updated>2025-07-08T16:14:16+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"&gt; &lt;img alt="SmolLM3: reasoning, long context and multilinguality for 3B parameter only" src="https://preview.redd.it/njam3shfcobf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac0783544f10bf513ae61c3adb68fd4ef3c75281" title="SmolLM3: reasoning, long context and multilinguality for 3B parameter only" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there, I'm Elie from the smollm team at huggingface, sharing this new model we built for local/on device use! &lt;/p&gt; &lt;p&gt;blog: &lt;a href="https://huggingface.co/blog/smollm3"&gt;https://huggingface.co/blog/smollm3&lt;/a&gt;&lt;br /&gt; GGUF/ONIX ckpt are being uploaded here: &lt;a href="https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23"&gt;https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Let us know what you think!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/njam3shfcobf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T16:14:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lux0q2</id>
    <title>LM Studio is now free for use at work</title>
    <updated>2025-07-08T18:56:25+00:00</updated>
    <author>
      <name>/u/mtomas7</name>
      <uri>https://old.reddit.com/user/mtomas7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is great news for all of us, but at the same time, it will put a lot of pressure on other similar paid projects, like Msty, as in my opinion, LM Studio is one of the best AI front ends at the moment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmstudio.ai/blog/free-for-work"&gt;LM Studio is free for use at work | LM Studio Blog&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtomas7"&gt; /u/mtomas7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T18:56:25+00:00</published>
  </entry>
</feed>
