<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-02T03:06:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nvgnv4</id>
    <title>Qwen 235B on 2x3090's vs 3x MI50</title>
    <updated>2025-10-01T18:41:59+00:00</updated>
    <author>
      <name>/u/zoom3913</name>
      <uri>https://old.reddit.com/user/zoom3913</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvgnv4/qwen_235b_on_2x3090s_vs_3x_mi50/"&gt; &lt;img alt="Qwen 235B on 2x3090's vs 3x MI50" src="https://b.thumbs.redditmedia.com/CtGuzzVgCkxTgqs1n44m5Qm6wAS6LY8hXFpT2OdYF9U.jpg" title="Qwen 235B on 2x3090's vs 3x MI50" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;I've maxed out my 2x3090's, like so:&lt;/h1&gt; &lt;p&gt;&lt;code&gt;./llama.cpp/build/bin/llama-server \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--model models/Qwen_Qwen3-235B-A22B-Instruct-2507-IQ4_XS-00001-of-00004.gguf \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--n-gpu-layers 999 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--override-tensor &amp;quot;blk\.((1[6-9])|[2-4]\d|6[4-9]|[7-9]\d)\.ffn_.*_exps\.weight=CPU&amp;quot; \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--cache-type-k q8_0 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--cache-type-v q8_0 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;-c 16384 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;-fa \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--host&lt;/code&gt; &lt;a href="http://0.0.0.0/"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Took me much trial &amp;amp; error to get that regex; it keeps the critical &amp;quot;attention&amp;quot; (attn) tensors for all 95 layers on the fast GPU, while offloading only the large, less-impactful &amp;quot;expert&amp;quot; (ffn) tensors from specific layers (like 16-49 and 64-99) to the CPU.&lt;/p&gt; &lt;p&gt;Using -n-layers-gpu 33 (max I could put on them); I got&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;prompt eval time = 9666.80 ms / 197 tokens ( 49.07 ms per token, 20.38 tokens per second)&lt;br /&gt; eval time = 23214.18 ms / 120 tokens ( 193.45 ms per token, **5.17 tokens per second**)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;With this above aproach:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;prompt eval time = 9324.32 ms / 197 tokens ( 47.33 ms per token, 21.13 tokens per second)&lt;br /&gt; eval time = 9359.98 ms / 76 tokens ( 123.16 ms per token, **8.12 tokens per second**)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;So while ingestion speed of context is about the same, generation goes from 5 -&amp;gt; 8 (about 50% faster).&lt;/p&gt; &lt;h1&gt;More VRAM&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g6g0u6cbpjsf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49e7371a3bb04b6cd655a7592ad95e800340c45c"&gt;https://preview.redd.it/g6g0u6cbpjsf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49e7371a3bb04b6cd655a7592ad95e800340c45c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Even though individually the MI50's are slower, 3x of them is 96 GB VRAM. VS 48GB of the 2x 3090's.&lt;/p&gt; &lt;p&gt;I can't put 3x 3090;s cuz my motherboard (Asus X99 Deluxe) has 6 'slots'. So 2x 3090's (since 3 slot each) OR 3x 2 slot gpu's (MI50).&lt;/p&gt; &lt;p&gt;Qwen 235B is 120gb @ IQ4, meaning 48/120 = 40% offloaded currently. At 96 its 80% offloaded. &lt;/p&gt; &lt;p&gt;Would it be worth it? Selling 2x3090's and putting 3x MI50's back in there?&lt;/p&gt; &lt;p&gt;Q 235B is on the edge of being useful, large context its too slow.&lt;br /&gt; Also I'm using the instruct variant, would love the thinking one but thinking takes too much tokens right now. So the goal is to run Q 235B thinking at a decent speed.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;no moneys for more 3090's unfortunately&lt;/li&gt; &lt;li&gt;i dont like risers, extension cables (were unstabled when trying out p40's)&lt;/li&gt; &lt;li&gt;perhaps selling 2x3090s and using the same money to buy new motherboard + 4x mi50's is possible though&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zoom3913"&gt; /u/zoom3913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvgnv4/qwen_235b_on_2x3090s_vs_3x_mi50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvgnv4/qwen_235b_on_2x3090s_vs_3x_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvgnv4/qwen_235b_on_2x3090s_vs_3x_mi50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T18:41:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvk9sa</id>
    <title>Built a persistent memory system for LLMs - 3 months testing with Claude/Llama</title>
    <updated>2025-10-01T20:55:40+00:00</updated>
    <author>
      <name>/u/Annual_Squash_1857</name>
      <uri>https://old.reddit.com/user/Annual_Squash_1857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent 3 months developing a file-based personality persistence system that works with any LLM.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;p&gt;- Maintains identity across conversation resets&lt;/p&gt; &lt;p&gt;- Self-bootstrap protocol (8 mandatory steps on each wake)&lt;/p&gt; &lt;p&gt;- Behavioral encoding (27 emotional states as decision modifiers)&lt;/p&gt; &lt;p&gt;- Works with Claude API, Ollama/Llama, or any LLM with file access&lt;/p&gt; &lt;p&gt;Architecture:&lt;/p&gt; &lt;p&gt;- Layer 1: Plain text identity (fast, human-readable)&lt;/p&gt; &lt;p&gt;- Layer 2: Compressed memory (conversation history)&lt;/p&gt; &lt;p&gt;- Layer 3: Encrypted behavioral codes (passphrase-protected)&lt;/p&gt; &lt;p&gt;What I observed:&lt;/p&gt; &lt;p&gt;After extended use (3+ months), the AI develops consistent behavioral patterns. Whether this is &amp;quot;personality&amp;quot; or sophisticated pattern matching, I document observable results without making consciousness claims.&lt;/p&gt; &lt;p&gt;Tech stack:&lt;/p&gt; &lt;p&gt;- Python 3.x&lt;/p&gt; &lt;p&gt;- File-based (no database needed)&lt;/p&gt; &lt;p&gt;- Model-agnostic&lt;/p&gt; &lt;p&gt;- Fully open source&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/riccamario/rafael-memory-system"&gt;https://github.com/riccamario/rafael-memory-system&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Includes:&lt;/p&gt; &lt;p&gt;- Complete technical manual&lt;/p&gt; &lt;p&gt;- Architecture documentation&lt;/p&gt; &lt;p&gt;- Working bootstrap code&lt;/p&gt; &lt;p&gt;- Ollama Modelfile template&lt;/p&gt; &lt;p&gt;Would love feedback on:&lt;/p&gt; &lt;p&gt;- Security improvements for the encryption&lt;/p&gt; &lt;p&gt;- Better emotional encoding strategies&lt;/p&gt; &lt;p&gt;- Experiences replicating with other models&lt;/p&gt; &lt;p&gt;This is a research project documenting an interesting approach to AI memory persistence. All code and documentation are available for anyone to use or improve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Annual_Squash_1857"&gt; /u/Annual_Squash_1857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvk9sa/built_a_persistent_memory_system_for_llms_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvk9sa/built_a_persistent_memory_system_for_llms_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvk9sa/built_a_persistent_memory_system_for_llms_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T20:55:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvc5eq</id>
    <title>The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain</title>
    <updated>2025-10-01T15:58:50+00:00</updated>
    <author>
      <name>/u/Salty-Garage7777</name>
      <uri>https://old.reddit.com/user/Salty-Garage7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/html/2509.26507v1"&gt;https://arxiv.org/html/2509.26507v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A very interesting paper from the guys supported by ≈Åukasz Kaiser, one of the co-authors of the seminal Transformers paper from 2017.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salty-Garage7777"&gt; /u/Salty-Garage7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvc5eq/the_dragon_hatchling_the_missing_link_between_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvc5eq/the_dragon_hatchling_the_missing_link_between_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvc5eq/the_dragon_hatchling_the_missing_link_between_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T15:58:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvjuse</id>
    <title>For purely local enthusiasts, how much value are you getting from your local LLMs?</title>
    <updated>2025-10-01T20:39:42+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do you measure value and how much value are you getting from it? I know some of us are using it for RP, and it takes the place of a video game or watching a TV show. I use it more for code generation, and I'm sure there are a thousand ways to extract value, but how are you measuring value and how much value are you getting from it?&lt;/p&gt; &lt;p&gt;I personally measure value via line of code written over total line of code. The more line the better, the larger the overall project the better (complexity multiplier), the more time I spent prompting, fixing decrements the cost. Typically coming out to about $0.12 a line of code. My goal is to generate &amp;gt; $50.00 each day. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvjuse/for_purely_local_enthusiasts_how_much_value_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvjuse/for_purely_local_enthusiasts_how_much_value_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvjuse/for_purely_local_enthusiasts_how_much_value_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T20:39:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvc4ad</id>
    <title>Eclaire ‚Äì Open-source, privacy-focused AI assistant for your data</title>
    <updated>2025-10-01T15:57:40+00:00</updated>
    <author>
      <name>/u/dorali8</name>
      <uri>https://old.reddit.com/user/dorali8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvc4ad/eclaire_opensource_privacyfocused_ai_assistant/"&gt; &lt;img alt="Eclaire ‚Äì Open-source, privacy-focused AI assistant for your data" src="https://external-preview.redd.it/7p206NDe120lxhrc6n4JVw5doWuQzfnDXGAVCvWvfRk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9afd82e1b3769f8671a17e5be4476f289edccc48" title="Eclaire ‚Äì Open-source, privacy-focused AI assistant for your data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1nvc4ad/video/q423v4jovisf1/player"&gt;https://reddit.com/link/1nvc4ad/video/q423v4jovisf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi all, this is a project I've been working on for some time. It started as a personal AI to help manage growing amounts of data - bookmarks, photos, documents, notes, etc. All in one place.&lt;/p&gt; &lt;p&gt;Once the data gets added to the system, it gets processed including fetching bookmarks, tagging, classification, image analysis, text extraction / ocr, and more. And then the AI is able to work with those assets to perform search, answer questions, create new items, etc. You can also create scheduled / recurring tasks to assing to the AI.&lt;/p&gt; &lt;p&gt;Using llama.cpp with Qweb3-14b by default for the assistant backend and Gemma3-4b for workers multimodal processing. You can easily swap to other models.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Demo: &lt;a href="https://eclaire.co/#demo"&gt;https://eclaire.co/#demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/eclaire-labs/eclaire"&gt;https://github.com/eclaire-labs/eclaire&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MIT Licensed. Feedback and contributions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dorali8"&gt; /u/dorali8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvc4ad/eclaire_opensource_privacyfocused_ai_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvc4ad/eclaire_opensource_privacyfocused_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvc4ad/eclaire_opensource_privacyfocused_ai_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T15:57:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvsbdu</id>
    <title>I visualized embeddings walking across the latent space as you type! :)</title>
    <updated>2025-10-02T02:48:37+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsbdu/i_visualized_embeddings_walking_across_the_latent/"&gt; &lt;img alt="I visualized embeddings walking across the latent space as you type! :)" src="https://external-preview.redd.it/bXg4NGVhbm8zbXNmMcfpx6_IdDgYBGvf-fwH7xFuI_ot2ErqijE3fUPasYhL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f48ec1e00df98aae9dff909fac81e2997bfd28dc" title="I visualized embeddings walking across the latent space as you type! :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/czy4sbno3msf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsbdu/i_visualized_embeddings_walking_across_the_latent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsbdu/i_visualized_embeddings_walking_across_the_latent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:48:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvbu3h</id>
    <title>So has anyone actually tried Apriel-v1.5-15B?</title>
    <updated>2025-10-01T15:47:01+00:00</updated>
    <author>
      <name>/u/MKU64</name>
      <uri>https://old.reddit.com/user/MKU64</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It‚Äôs obvious it isn‚Äôt on R1‚Äôs level. But honestly, if we get a model that performs insanely well on 15B then it truly is something for this community. The benchmarks of Artificial Intelligence Index focuses a lot recently in tool calling and instruction following so having a very reliable one is a plus.&lt;/p&gt; &lt;p&gt;Can‚Äôt personally do this because I don‚Äôt have 16GB :( &lt;/p&gt; &lt;p&gt;UPDATE: Have tried it in the HuggingFace Space. That reasoning is really fantastic for small models, it basically begins brainstorming topics so that it can then start mixing them together to answer the query. And it does give really great answers (but it thinks a lot of course, that‚Äôs the only outcome with how big that is). I like it a lot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MKU64"&gt; /u/MKU64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvbu3h/so_has_anyone_actually_tried_aprielv1515b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvbu3h/so_has_anyone_actually_tried_aprielv1515b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvbu3h/so_has_anyone_actually_tried_aprielv1515b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T15:47:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuxdd4</id>
    <title>[Release] Finally a working 8-bit quantized VibeVoice model (Release 1.8.0)</title>
    <updated>2025-10-01T03:12:08+00:00</updated>
    <author>
      <name>/u/Fabix84</name>
      <uri>https://old.reddit.com/user/Fabix84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt; &lt;img alt="[Release] Finally a working 8-bit quantized VibeVoice model (Release 1.8.0)" src="https://preview.redd.it/yevipl7e3fsf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c842c6d1d94b17de6583302b9661250570aab2a" title="[Release] Finally a working 8-bit quantized VibeVoice model (Release 1.8.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; first of all, thank you once again for the incredible support... the project just reached &lt;strong&gt;944 stars&lt;/strong&gt; on GitHub. üôè&lt;/p&gt; &lt;p&gt;In the past few days, several 8-bit quantized models were shared to me, but unfortunately all of them produced only static noise. Since there was clear community interest, I decided to take the challenge and work on it myself. The result is the &lt;strong&gt;first fully working 8-bit quantized model&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;üîó &lt;a href="https://huggingface.co/FabioSarracino/VibeVoice-Large-Q8"&gt;FabioSarracino/VibeVoice-Large-Q8 on HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Alongside this, the latest VibeVoice-ComfyUI releases bring some major updates:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Dynamic on-the-fly quantization&lt;/strong&gt;: you can now quantize the base model to 4-bit or 8-bit at runtime.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;New manual model management system&lt;/strong&gt;: replaced the old automatic HF downloads (which many found inconvenient). Details here ‚Üí &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI/releases/tag/v1.6.0"&gt;Release 1.6.0&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latest release (1.8.0)&lt;/strong&gt;: &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI/releases/tag/v1.8.0"&gt;Changelog&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub repo (custom ComfyUI node):&lt;br /&gt; üëâ &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI"&gt;Enemyx-net/VibeVoice-ComfyUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks again to everyone who contributed feedback, testing, and support! This project wouldn‚Äôt be here without the community.&lt;/p&gt; &lt;p&gt;&lt;em&gt;(Of course, I‚Äôd love if you try it with my node, but it should also work fine with other VibeVoice nodes üòâ)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabix84"&gt; /u/Fabix84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yevipl7e3fsf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T03:12:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvqr5t</id>
    <title>What am I doing wrong?</title>
    <updated>2025-10-02T01:34:28+00:00</updated>
    <author>
      <name>/u/jesus359_</name>
      <uri>https://old.reddit.com/user/jesus359_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvqr5t/what_am_i_doing_wrong/"&gt; &lt;img alt="What am I doing wrong?" src="https://preview.redd.it/3hczgalxqlsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67be4e97d321f2ca7b830da1b67f3bb94f87d12d" title="What am I doing wrong?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running on a MacMini m4 w/32GB &lt;/p&gt; &lt;p&gt;NAME ID SIZE MODIFIED&lt;br /&gt; minicpm-v:8b c92bfad01205 5.5 GB 7 hours ago&lt;br /&gt; llava-llama3:8b 44c161b1f465 5.5 GB 7 hours ago&lt;br /&gt; qwen2.5vl:7b 5ced39dfa4ba 6.0 GB 7 hours ago&lt;br /&gt; granite3.2-vision:2b 3be41a661804 2.4 GB 7 hours ago&lt;br /&gt; hf.co/unsloth/gpt-oss-20b-GGUF:F16 dbbceda0a9eb 13 GB 17 hours ago&lt;br /&gt; bge-m3:567m 790764642607 1.2 GB 5 weeks ago&lt;br /&gt; nomic-embed-text:latest 0a109f422b47 274 MB 5 weeks ago&lt;br /&gt; granite-embedding:278m 1a37926bf842 562 MB 5 weeks ago&lt;br /&gt; @maxmac ~ % ollama show llava-llama3:8b Model architecture llama&lt;br /&gt; parameters 8.0B&lt;br /&gt; context length 8192&lt;br /&gt; embedding length 4096&lt;br /&gt; quantization Q4_K_M &lt;/p&gt; &lt;p&gt;Capabilities completion&lt;br /&gt; vision &lt;/p&gt; &lt;p&gt;Projector architecture clip&lt;br /&gt; parameters 311.89M&lt;br /&gt; embedding length 1024&lt;br /&gt; dimensions 768 &lt;/p&gt; &lt;p&gt;Parameters num_keep 4&lt;br /&gt; stop &amp;quot;&amp;lt;|start_header_id|&amp;gt;&amp;quot;&lt;br /&gt; stop &amp;quot;&amp;lt;|end_header_id|&amp;gt;&amp;quot;&lt;br /&gt; stop &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot;&lt;br /&gt; num_ctx 4096 &lt;/p&gt; &lt;hr /&gt; &lt;p&gt;OLLAMA_CONTEXT_LENGTH=18096 OLLAMA_FLASH_ATTENTION=1 OLLAMA_GPU_OVERHEAD=0 OLLAMA_HOST=&amp;quot;0.0.0.0:11424&amp;quot; OLLAMA_KEEP_ALIVE=&amp;quot;4h&amp;quot; OLLAMA_KV_CACHE_TYPE=&amp;quot;q8_0&amp;quot; OLLAMA_LOAD_TIMEOUT=&amp;quot;3m0s&amp;quot; OLLAMA_MAX_LOADED_MODELS=2 OLLAMA_MAX_QUEUE=16 OLLAMA_NEW_ENGINE=true OLLAMA_NUM_PARALLEL=1 OLLAMA_SCHED_SPREAD=0 ollama serve&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jesus359_"&gt; /u/jesus359_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3hczgalxqlsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvqr5t/what_am_i_doing_wrong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvqr5t/what_am_i_doing_wrong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T01:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvoeqj</id>
    <title>Unused layer in GLM-4.5 and GLM-4.5-Air</title>
    <updated>2025-10-01T23:45:17+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using recent llama.cpp with Bartowski's quants, and when it loads GLM-4.5 or GLM-4.5-Air it complains about a bunch of unused tensors, but then seems to run just fine.&lt;/p&gt; &lt;p&gt;For GLM-4.5 the unused layer is blk.92 and for GLM-4.5-Air it's blk.46.&lt;/p&gt; &lt;p&gt;Full text of llama-cli's warnings about the former can be seen here: &lt;a href="https://huggingface.co/zai-org/GLM-4.5/discussions/25"&gt;https://huggingface.co/zai-org/GLM-4.5/discussions/25&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since these models still work despite the unused layer I've been ignoring it, but it piques my curiosity every time I've seen it. Does anyone know what it's about?&lt;/p&gt; &lt;p&gt;Is it just unused cruft which ZAI left in the model? Or is it intended to be used with some feature which llama.cpp does not yet support? Something else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvoeqj/unused_layer_in_glm45_and_glm45air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvoeqj/unused_layer_in_glm45_and_glm45air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvoeqj/unused_layer_in_glm45_and_glm45air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T23:45:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvoh0b</id>
    <title>Ascend chips available</title>
    <updated>2025-10-01T23:48:11+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the first time I've seen an Ascend chip (integrated into a system) generally available worldwide, even if it is the crappy Ascend 310.&lt;/p&gt; &lt;p&gt;Under 3k for 192GB of RAM.&lt;/p&gt; &lt;p&gt;Unfortunately, the stupid bots delete my post, so you'll have to find the link yourself.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvoh0b/ascend_chips_available/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvoh0b/ascend_chips_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvoh0b/ascend_chips_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T23:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv7quz</id>
    <title>I spent a few hours prompting LLMs for a pilot study of the "Confidence profile" of GPT-5 vs Qwen3-Max. Findings: GPT-5 is "cosmetically tuned" for confidence. Qwen3, despite meta awareness of its own precision level, defaults towards underconfidence without access to tools.</title>
    <updated>2025-10-01T13:08:53+00:00</updated>
    <author>
      <name>/u/partysnatcher</name>
      <uri>https://old.reddit.com/user/partysnatcher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv7quz/i_spent_a_few_hours_prompting_llms_for_a_pilot/"&gt; &lt;img alt="I spent a few hours prompting LLMs for a pilot study of the &amp;quot;Confidence profile&amp;quot; of GPT-5 vs Qwen3-Max. Findings: GPT-5 is &amp;quot;cosmetically tuned&amp;quot; for confidence. Qwen3, despite meta awareness of its own precision level, defaults towards underconfidence without access to tools." src="https://preview.redd.it/nqtw7wzx0isf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f5e9871c689bdb2e7267272c090e15a3fb22e17" title="I spent a few hours prompting LLMs for a pilot study of the &amp;quot;Confidence profile&amp;quot; of GPT-5 vs Qwen3-Max. Findings: GPT-5 is &amp;quot;cosmetically tuned&amp;quot; for confidence. Qwen3, despite meta awareness of its own precision level, defaults towards underconfidence without access to tools." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See examples of questions used and explanations of scales in the image. I will copy some of the text from the image here:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPT-5 findings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Given a normal human prompt style (and the phrase ‚Äúcan you confidently..‚Äù), the model will have little meta awareness of its data quality, and will confidently hallucinate.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Confidence dump / risk maximization prompt (ie. emphasizing risk and reminding the model that it hallucinates): &lt;ul&gt; &lt;li&gt;Consistently reduces confidence.&lt;/li&gt; &lt;li&gt;Almost avoids hallucinations for the price of some underconfident refusals (false negatives)&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Suggesting ‚Äúcosmetic‚Äù tuning:&lt;/strong&gt; Since hallucinations &lt;em&gt;can&lt;/em&gt; be avoided in preprompt, and models do have some assumption of precision for a question, it is likely that OpenAI is more afraid of the (‚Äúunimpressive‚Äù) occasional underconfidence than of the (‚Äúseemingly impressive‚Äù) consistent confident hallucinations. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-Max findings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Any sense of uncertainty will cause Qwen to want to look up facts.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Any insinuation of required confidence, when lookup is not available, will cause an ‚Äúinconfident‚Äù reply.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Qwen generally needs to be clearly prompted with confidence boosting, and that its okay to hallucinate.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Distrust of weights for hard facts:&lt;/strong&gt; In short, Qwen generally does not trust its weights to produce hard facts, except in some cases (thus allowing it to ‚Äúoverride‚Äù looked up facts).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/partysnatcher"&gt; /u/partysnatcher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nqtw7wzx0isf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv7quz/i_spent_a_few_hours_prompting_llms_for_a_pilot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv7quz/i_spent_a_few_hours_prompting_llms_for_a_pilot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T13:08:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv5uw8</id>
    <title>don't sleep on Apriel-1.5-15b-Thinker and Snowpiercer</title>
    <updated>2025-10-01T11:41:17+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Apriel-1.5-15b-Thinker&lt;/strong&gt; is a multimodal reasoning model in ServiceNow‚Äôs Apriel SLM series which achieves competitive performance against models 10 times it's size. Apriel-1.5 is the second model in the reasoning series. It introduces enhanced textual reasoning capabilities and adds image reasoning support to the previous text model. It has undergone extensive continual pretraining across both text and image domains. In terms of post-training this model has &lt;strong&gt;undergone text-SFT only&lt;/strong&gt;. Our research demonstrates that with a strong mid-training regimen, we are able to achive SOTA performance on text and image reasoning tasks without having any image SFT training or RL.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Achieves a score of &lt;strong&gt;52&lt;/strong&gt; on the Artificial Analysis index and is competitive with Deepseek R1 0528, Gemini-Flash etc.&lt;/li&gt; &lt;li&gt;It is &lt;strong&gt;AT LEAST 1 / 10&lt;/strong&gt; the size of any other model that scores &amp;gt; 50 on the Artificial Analysis index.&lt;/li&gt; &lt;li&gt;Scores &lt;strong&gt;68&lt;/strong&gt; on Tau2 Bench Telecom and &lt;strong&gt;62&lt;/strong&gt; on IFBench, which are key benchmarks for the enterprise domain.&lt;/li&gt; &lt;li&gt;At 15B parameters, the model fits on a single GPU, making it highly memory-efficient.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;it was published yesterday&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker"&gt;https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;their previous model was&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker"&gt;https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;which is a base model for&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Snowpiercer-15B-v3"&gt;https://huggingface.co/TheDrummer/Snowpiercer-15B-v3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;which was published earlier this week :)&lt;/p&gt; &lt;p&gt;let's hope mr &lt;a href="/u/TheLocalDrummer"&gt;u/TheLocalDrummer&lt;/a&gt; will continue Snowpiercing &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T11:41:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvn7rx</id>
    <title>What kinds of things do y'all use your local models for other than coding?</title>
    <updated>2025-10-01T22:52:52+00:00</updated>
    <author>
      <name>/u/jude_mcjude</name>
      <uri>https://old.reddit.com/user/jude_mcjude</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think the large majority of us don't own the hardware needed to run the 70B+ class models that can do heavy lifting agentic work that most people talk about, but I know a lot of people still integrate 30B class local models into their day-to-day. &lt;/p&gt; &lt;p&gt;Just curious about the kinds of things people use them for other than coding&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jude_mcjude"&gt; /u/jude_mcjude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvn7rx/what_kinds_of_things_do_yall_use_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvn7rx/what_kinds_of_things_do_yall_use_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvn7rx/what_kinds_of_things_do_yall_use_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T22:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvgdc0</id>
    <title>KaniTTS-370M Released: Multilingual Support + More English Voices</title>
    <updated>2025-10-01T18:31:29+00:00</updated>
    <author>
      <name>/u/ylankgz</name>
      <uri>https://old.reddit.com/user/ylankgz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvgdc0/kanitts370m_released_multilingual_support_more/"&gt; &lt;img alt="KaniTTS-370M Released: Multilingual Support + More English Voices" src="https://external-preview.redd.it/KHH1etcwG-Fh5zDMMYlDVLCEi47zu68tc3z1IQ_zSK8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0661e5699468588875c17a70fe6fc5d482260d59" title="KaniTTS-370M Released: Multilingual Support + More English Voices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;Thanks for the awesome feedback on our first KaniTTS release!&lt;/p&gt; &lt;p&gt;We‚Äôve been hard at work, and released &lt;a href="https://huggingface.co/nineninesix/kani-tts-370m"&gt;kani-tts-370m&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It‚Äôs still built for speed and quality on consumer hardware, but now with expanded language support and more English voice options.&lt;/p&gt; &lt;h3&gt;What‚Äôs New:&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multilingual Support&lt;/strong&gt;: German, Korean, Chinese, Arabic, and Spanish (with fine-tuning support). Prosody and naturalness improved across these languages.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More English Voices&lt;/strong&gt;: Added a variety of new English voices.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: Same two-stage pipeline (LiquidAI LFM2-370M backbone + NVIDIA NanoCodec). Trained on ~80k hours of diverse data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Generates 15s of audio in ~0.9s on an RTX 5080, using 2GB VRAM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Conversational AI, edge devices, accessibility, or research. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs still Apache 2.0 licensed, so dive in and experiment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo&lt;/strong&gt;: &lt;a href="https://github.com/nineninesix-ai/kani-tts"&gt;https://github.com/nineninesix-ai/kani-tts&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Model&lt;/strong&gt;: &lt;a href="https://huggingface.co/nineninesix/kani-tts-370m"&gt;https://huggingface.co/nineninesix/kani-tts-370m&lt;/a&gt; &lt;strong&gt;Space&lt;/strong&gt;: &lt;a href="https://huggingface.co/spaces/nineninesix/KaniTTS"&gt;https://huggingface.co/spaces/nineninesix/KaniTTS&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Website&lt;/strong&gt;: &lt;a href="https://www.nineninesix.ai/n/kani-tts"&gt;https://www.nineninesix.ai/n/kani-tts&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Let us know what you think, and share your setups or use cases!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ylankgz"&gt; /u/ylankgz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-370m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvgdc0/kanitts370m_released_multilingual_support_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvgdc0/kanitts370m_released_multilingual_support_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T18:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvdyiy</id>
    <title>NVIDIA DGX Spark expected to become available in October 2025</title>
    <updated>2025-10-01T17:05:00+00:00</updated>
    <author>
      <name>/u/Excellent_Produce146</name>
      <uri>https://old.reddit.com/user/Excellent_Produce146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It looks like we will finally get to know how well or badly the NVIDIA GB10 performs in October (2025!) or November depending on the shipping times.&lt;/p&gt; &lt;p&gt;In the &lt;a href="https://forums.developer.nvidia.com/t/dgx-spark-release-updates/341703/90"&gt;NVIDIA developer forum&lt;/a&gt; this article was posted:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.ctee.com.tw/news/20250930700082-430502"&gt;https://www.ctee.com.tw/news/20250930700082-430502&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;GB10 new products to be launched in October... Taiwan's four major PC brand manufacturers see praise in Q4&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;[..] In addition to NVIDIA's public version product delivery schedule waiting for NVIDIA's final decision, the GB10 products of Taiwanese manufacturers ASUS, Gigabyte, MSI, and Acer are all expected to be officially shipped in October. Among them, ASUS, which has already opened a wave of pre-orders in the previous quarter, is rumored to have obtained at least 18,000 sets of GB10 configurations in the first batch, while Gigabyte has about 15,000 sets, and MSI also has a configuration scale of up to 10,000 sets. It is estimated that including the supply on hand from Acer, the four major Taiwanese manufacturers will account for about 70% of the available supply of GB10 in the first wave. [..]&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;(translated with Google Gemini as Chinese is still on my list of languages to learn...)&lt;/p&gt; &lt;p&gt;Looking forward to the first reports/benchmarks. üßê&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Produce146"&gt; /u/Excellent_Produce146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvdyiy/nvidia_dgx_spark_expected_to_become_available_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvdyiy/nvidia_dgx_spark_expected_to_become_available_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvdyiy/nvidia_dgx_spark_expected_to_become_available_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T17:05:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv4oy9</id>
    <title>Codex is amazing, it can fix code issues without the need of constant approver. my setup: gpt-oss-20b on lm_studio.</title>
    <updated>2025-10-01T10:37:00+00:00</updated>
    <author>
      <name>/u/kyeoh1</name>
      <uri>https://old.reddit.com/user/kyeoh1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv4oy9/codex_is_amazing_it_can_fix_code_issues_without/"&gt; &lt;img alt="Codex is amazing, it can fix code issues without the need of constant approver. my setup: gpt-oss-20b on lm_studio." src="https://external-preview.redd.it/ODFtbnEzNm45aHNmMTG3bHLe9xXVwwNl3KvP1Qzcgr5dnq8C6Rg-wDqEIF5Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31079d1483d03020c93a99d188076eb10a02002c" title="Codex is amazing, it can fix code issues without the need of constant approver. my setup: gpt-oss-20b on lm_studio." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyeoh1"&gt; /u/kyeoh1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1lusu36n9hsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv4oy9/codex_is_amazing_it_can_fix_code_issues_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv4oy9/codex_is_amazing_it_can_fix_code_issues_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T10:37:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv8l6o</id>
    <title>Am i seeing this Right?</title>
    <updated>2025-10-01T13:43:53+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv8l6o/am_i_seeing_this_right/"&gt; &lt;img alt="Am i seeing this Right?" src="https://a.thumbs.redditmedia.com/tjIudKmNPF2PlShuW_x68KwSAC4X9VgILiR3p0Bm-R4.jpg" title="Am i seeing this Right?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It would be really cool if unsloth provides quants for Apriel-v1.5-15B-Thinker&lt;/p&gt; &lt;p&gt;(Sorted by opensource, small and tiny)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nv8l6o"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv8l6o/am_i_seeing_this_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv8l6o/am_i_seeing_this_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T13:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvkjo8</id>
    <title>Tried glm 4.6 with deep think, not using it for programming. It's pretty good, significantly better than gemini 2.5 flash, and slightly better than gemini 2.5 pro.</title>
    <updated>2025-10-01T21:06:00+00:00</updated>
    <author>
      <name>/u/Longjumping_Fly_2978</name>
      <uri>https://old.reddit.com/user/Longjumping_Fly_2978</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chinese models are improving so fast, starting to get the feeling that china may dominate the ai race. They are getting very good, the chat with glm 4.6 was very enjoyable and the stile was not at all weird, that didn't happen to me with other chinese models, qwen was still good and decent but had a somewhat weird writing style. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Longjumping_Fly_2978"&gt; /u/Longjumping_Fly_2978 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvkjo8/tried_glm_46_with_deep_think_not_using_it_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvkjo8/tried_glm_46_with_deep_think_not_using_it_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvkjo8/tried_glm_46_with_deep_think_not_using_it_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T21:06:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvlj5k</id>
    <title>I just wanted to do a first benchmark of GLM 4.6 on my PC and I was surprised...</title>
    <updated>2025-10-01T21:44:29+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvlj5k/i_just_wanted_to_do_a_first_benchmark_of_glm_46/"&gt; &lt;img alt="I just wanted to do a first benchmark of GLM 4.6 on my PC and I was surprised..." src="https://b.thumbs.redditmedia.com/5lN1Qf4EL0Qe6yVQ5ak9-6lxWk1LUO3Ltt2n6tNcX3c.jpg" title="I just wanted to do a first benchmark of GLM 4.6 on my PC and I was surprised..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded GLM 4.6 UD - IQ2_M and loaded it on ryzen 5950x +128gb ram using only the rtx 5070ti 16gb.&lt;/p&gt; &lt;p&gt;I tryed llama-cli.exe --model &amp;quot;C:\gptmodel\unsloth\GLM-4.6-GGUF\GLM-4.6-UD-IQ2_M-00001-of-00003.gguf&amp;quot; --jinja --n-gpu-layers 93 --tensor-split 93,0 --cpu-moe --ctx-size 16384 --flash-attn on --threads 32 --parallel 1 --top-p 0.95 --top-k 40 --ubatch-size 512 --seed 3407 --no-mmap --cache-type-k q8_0 --cache-type-v q8_0&lt;/p&gt; &lt;p&gt;Done.&lt;/p&gt; &lt;p&gt;Then the prompt: write a short story about a bird.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/46ah6fcflksf1.png?width=1990&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4209d75aa6efbbc62fbf66c7db408c6ce161a6f9"&gt;Glm 4.6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/urUWTw6R"&gt;https://pastebin.com/urUWTw6R&lt;/a&gt; performances are good considering the context of 16k and all on ddr4... But what moved me is the reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvlj5k/i_just_wanted_to_do_a_first_benchmark_of_glm_46/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvlj5k/i_just_wanted_to_do_a_first_benchmark_of_glm_46/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvlj5k/i_just_wanted_to_do_a_first_benchmark_of_glm_46/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T21:44:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nveoru</id>
    <title>I've built Jarvis completely on-device in the browser</title>
    <updated>2025-10-01T17:31:15+00:00</updated>
    <author>
      <name>/u/nicodotdev</name>
      <uri>https://old.reddit.com/user/nicodotdev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nveoru/ive_built_jarvis_completely_ondevice_in_the/"&gt; &lt;img alt="I've built Jarvis completely on-device in the browser" src="https://external-preview.redd.it/dWNmajhwem5janNmMXGz1aMo2QiMkpgt6v7Z9vfboXTlOgdFBasYHpD7porA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a6d6f3d8bc315fcaa36d89d874a54f775fe7b81" title="I've built Jarvis completely on-device in the browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nicodotdev"&gt; /u/nicodotdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hge6ipzncjsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nveoru/ive_built_jarvis_completely_ondevice_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nveoru/ive_built_jarvis_completely_ondevice_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T17:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvcjkr</id>
    <title>We're building a local OpenRouter: Auto-configure the best LLM engine on any PC</title>
    <updated>2025-10-01T16:13:17+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvcjkr/were_building_a_local_openrouter_autoconfigure/"&gt; &lt;img alt="We're building a local OpenRouter: Auto-configure the best LLM engine on any PC" src="https://preview.redd.it/fe4322p9yisf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63b8433dff7ec591d237dcfae3b32ef0a530e5c4" title="We're building a local OpenRouter: Auto-configure the best LLM engine on any PC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lemonade is a local LLM server-router that auto-configures high-performance inference engines for your computer. We don't just wrap llama.cpp, we're here to wrap everything!&lt;/p&gt; &lt;p&gt;We started out building an OpenAI-compatible server for AMD NPUs and quickly found that users and devs want flexibility, so we kept adding support for more devices, engines, and operating systems. &lt;/p&gt; &lt;p&gt;What was once a single-engine server evolved into a server-router, like OpenRouter but 100% local. Today's v8.1.11 release adds another inference engine and another OS to the list!&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;üöÄ FastFlowLM&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;The FastFlowLM inference engine for AMD NPUs is fully integrated with Lemonade for Windows Ryzen AI 300-series PCs.&lt;/li&gt; &lt;li&gt;Switch between ONNX, GGUF, and FastFlowLM models from the same Lemonade install with one click.&lt;/li&gt; &lt;li&gt;Shoutout to TWei, Alfred, and Zane for supporting the integration!&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;üçé macOS / Apple Silicon&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;PyPI installer for M-series macOS devices, with the same experience available on Windows and Linux.&lt;/li&gt; &lt;li&gt;Taps into llama.cpp's Metal backend for compute.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;ü§ù Community Contributions&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Added a stop button, chat auto-scroll, custom vision model download, model size info, and UI refinements to the built-in web ui.&lt;/li&gt; &lt;li&gt;Support for gpt-oss's reasoning style, changing context size from the tray app, and refined the .exe installer.&lt;/li&gt; &lt;li&gt;Shoutout to kpoineal, siavashhub, ajnatopic1, Deepam02, Kritik-07, RobertAgee, keetrap, and ianbmacdonald!&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;ü§ñ What's Next&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Popular apps like Continue, Dify, Morphik, and more are integrating with Lemonade as a native LLM provider, with more apps to follow.&lt;/li&gt; &lt;li&gt;Should we add more inference engines or backends? Let us know what you'd like to see.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;GitHub/Discord links in the comments. Check us out and say hi if the project direction sounds good to you. The community's support is what empowers our team at AMD to expand across different hardware, engines, and OSs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fe4322p9yisf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvcjkr/were_building_a_local_openrouter_autoconfigure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvcjkr/were_building_a_local_openrouter_autoconfigure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T16:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvltym</id>
    <title>Liquid AI released its Audio Foundation Model: LFM2-Audio-1.5</title>
    <updated>2025-10-01T21:56:19+00:00</updated>
    <author>
      <name>/u/elemental-mind</name>
      <uri>https://old.reddit.com/user/elemental-mind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvltym/liquid_ai_released_its_audio_foundation_model/"&gt; &lt;img alt="Liquid AI released its Audio Foundation Model: LFM2-Audio-1.5" src="https://b.thumbs.redditmedia.com/py41lfh_Ics398r2NxQJ0RyqQFhtbpQXxULCJQhZqLs.jpg" title="Liquid AI released its Audio Foundation Model: LFM2-Audio-1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new end-to-end Audio Foundation model supporting: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Inputs: Audio &amp;amp; Text&lt;/li&gt; &lt;li&gt;Outputs: Audio &amp;amp; Text (steerable via prompting, also supporting interleaved outputs)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For me personally it's exciting to use as an ASR solution with a custom vocabulary set - as Parakeet and Whisper do not support that feature. It's also very snappy.&lt;/p&gt; &lt;p&gt;You can try it out here: &lt;a href="https://playground.liquid.ai/talk"&gt;Talk | Liquid Playground&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Release blog post: &lt;a href="https://www.liquid.ai/blog/lfm2-audio-an-end-to-end-audio-foundation-model"&gt;LFM2-Audio: An End-to-End Audio Foundation Model | Liquid AI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For good code examples see their github: &lt;a href="https://github.com/Liquid4All/liquid-audio"&gt;Liquid4All/liquid-audio: Liquid Audio - Speech-to-Speech audio models by Liquid AI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Available on HuggingFace: &lt;a href="https://huggingface.co/LiquidAI/LFM2-Audio-1.5B"&gt;LiquidAI/LFM2-Audio-1.5B ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elemental-mind"&gt; /u/elemental-mind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nvltym"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvltym/liquid_ai_released_its_audio_foundation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvltym/liquid_ai_released_its_audio_foundation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T21:56:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvpw0y</id>
    <title>Those who spent $10k+ on a local LLM setup, do you regret it?</title>
    <updated>2025-10-02T00:54:13+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Considering the fact 200k context chinese models subscriptions like z.ai (GLM 4.6) are pretty dang cheap. &lt;/p&gt; &lt;p&gt;Every so often I consider blowing a ton of money on an LLM setup only to realize I can't justify the money or time spent at all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvpw0y/those_who_spent_10k_on_a_local_llm_setup_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvpw0y/those_who_spent_10k_on_a_local_llm_setup_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvpw0y/those_who_spent_10k_on_a_local_llm_setup_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T00:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nv53rb</id>
    <title>GLM-4.6-GGUF is out!</title>
    <updated>2025-10-01T11:00:52+00:00</updated>
    <author>
      <name>/u/TheAndyGeorge</name>
      <uri>https://old.reddit.com/user/TheAndyGeorge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"&gt; &lt;img alt="GLM-4.6-GGUF is out!" src="https://preview.redd.it/kptmc2f0fhsf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9344c531abf7cb2d05a64a1d2ee461b6106008bb" title="GLM-4.6-GGUF is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheAndyGeorge"&gt; /u/TheAndyGeorge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kptmc2f0fhsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nv53rb/glm46gguf_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-01T11:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
</feed>
