<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-16T16:08:09+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o7miyx</id>
    <title>Just ordered new 3090 TI from MicroCenter ðŸ¤”</title>
    <updated>2025-10-15T20:39:56+00:00</updated>
    <author>
      <name>/u/GravyPoo</name>
      <uri>https://old.reddit.com/user/GravyPoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"&gt; &lt;img alt="Just ordered new 3090 TI from MicroCenter ðŸ¤”" src="https://preview.redd.it/mzozs3957cvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb5a83e22f624acd437f0414ec334d5a460f063d" title="Just ordered new 3090 TI from MicroCenter ðŸ¤”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GravyPoo"&gt; /u/GravyPoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mzozs3957cvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T20:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7ep8a</id>
    <title>Apple M5 Officially Announced: is this a big deal?</title>
    <updated>2025-10-15T15:48:34+00:00</updated>
    <author>
      <name>/u/ontorealist</name>
      <uri>https://old.reddit.com/user/ontorealist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(&lt;em&gt;Edit: To be clear, only the *&lt;/em&gt;base** M5 has been announced. My question is primarily about whether M5 &lt;strong&gt;Pro&lt;/strong&gt; and higher-end M5 chips with more high bandwidth memory, etc. are more compelling compared to PC builds for inference given the confirmed specs for the base M5.*)&lt;/p&gt; &lt;p&gt;If Iâ€™m understanding correctly:&lt;/p&gt; &lt;p&gt;â€¢ &lt;strong&gt;3.5x faster AI performance&lt;/strong&gt; compared to the M4 (though the exact neural engine improvements arenâ€™t yet confirmed)&lt;br /&gt; â€¢ &lt;strong&gt;153 GB/s memory bandwidth&lt;/strong&gt; (~30% improvement)&lt;br /&gt; â€¢ &lt;strong&gt;4x increase in GPU compute&lt;/strong&gt;&lt;br /&gt; â€¢ &lt;strong&gt;Unified memory architecture&lt;/strong&gt;, eliminating the need for CPUâ†”GPU data transfers, as with previous gens&lt;/p&gt; &lt;p&gt;Even if the neural accelerators on the base M5 arenâ€™t dedicated matmul units (which seems unlikely given the A19 Pro), will this translate into noticeably faster prompt processing speeds?&lt;/p&gt; &lt;p&gt;At $1,600 for an entry-level 16GB M5 ($2K for 32GB), serious inference workloads feels limiting, especially when compared to refurbished M-series models with more RAM. That said, it seems like a solid choice for new users exploring local AI experiences, particularly when working with sub-30B models for RAG or large context windows at faster speeds. That, along with another LM Studio feature in the press release, is a good sign, no? &lt;/p&gt; &lt;p&gt;Do the specs / pricing represent a meaningful upgrade for anyone considering the M5 Pro, Max, or Ultra? Iâ€™d love to hear othersâ€™ thoughts.&lt;/p&gt; &lt;p&gt;Read the announcement &lt;a href="https://www.apple.com/newsroom/2025/10/apple-unleashes-m5-the-next-big-leap-in-ai-performance-for-apple-silicon/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ontorealist"&gt; /u/ontorealist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T15:48:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8529x</id>
    <title>Qwen3-30B-A3B 2507 Instruct vs Thinking</title>
    <updated>2025-10-16T12:40:23+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got an Oneplus 12 phone with 24GB RAM. Want to run Qwen3-30-A3B on it but I find that there are two versions: Instruct and Thinking&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1md8rxu/qwenqwen330ba3bthinking2507_hugging_face/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1md8rxu/qwenqwen330ba3bthinking2507_hugging_face/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;According to the charts published in the above links:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Instruct&lt;/th&gt; &lt;th align="left"&gt;Thinking&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GPQA&lt;/td&gt; &lt;td align="left"&gt;70.4&lt;/td&gt; &lt;td align="left"&gt;73.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AIME25&lt;/td&gt; &lt;td align="left"&gt;61.3&lt;/td&gt; &lt;td align="left"&gt;85.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LiveCodeBench V6&lt;/td&gt; &lt;td align="left"&gt;43.2&lt;/td&gt; &lt;td align="left"&gt;66.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Arena-Hard V2&lt;/td&gt; &lt;td align="left"&gt;69.0&lt;/td&gt; &lt;td align="left"&gt;56.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BCFLv3&lt;/td&gt; &lt;td align="left"&gt;65.1&lt;/td&gt; &lt;td align="left"&gt;72.4&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;My understanding is that GPQA is general knowledge, AIME25 is Math, LiveCodeBench is coding, Arena-Hard is a predictor of how well it will perform at LMArena and BCFL is about tool calling.&lt;/p&gt; &lt;p&gt;I want to know which one should I use. Since I am not going to do math, coding and tool calling on my phone and mostly want to query it for knowledge, so maybe Instruct is better for my use case as GPQA is not much different and Arena-Hard might mean Instruct follows instruction better and fewer frustration for me?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8529x/qwen330ba3b_2507_instruct_vs_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8529x/qwen330ba3b_2507_instruct_vs_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8529x/qwen330ba3b_2507_instruct_vs_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T12:40:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7x25o</id>
    <title>Ollama v0.12.6 finally includes Vulkan support</title>
    <updated>2025-10-16T04:39:06+00:00</updated>
    <author>
      <name>/u/geerlingguy</name>
      <uri>https://old.reddit.com/user/geerlingguy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x25o/ollama_v0126_finally_includes_vulkan_support/"&gt; &lt;img alt="Ollama v0.12.6 finally includes Vulkan support" src="https://external-preview.redd.it/cjCC50drSVvsSjC6BG0LlHPfYq9pihhGvZz2PN90ZFQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c878fa25eb39194203fe7a493bd33e19b95d026c" title="Ollama v0.12.6 finally includes Vulkan support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geerlingguy"&gt; /u/geerlingguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.12.6-rc0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x25o/ollama_v0126_finally_includes_vulkan_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x25o/ollama_v0126_finally_includes_vulkan_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T04:39:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o87u2f</id>
    <title>I know the DGX Spark isnâ€™t what a lot people hoped it would be, but what ifâ€¦â€¦</title>
    <updated>2025-10-16T14:35:31+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o87u2f/i_know_the_dgx_spark_isnt_what_a_lot_people_hoped/"&gt; &lt;img alt="I know the DGX Spark isnâ€™t what a lot people hoped it would be, but what ifâ€¦â€¦" src="https://preview.redd.it/42fhtkk1jhvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63509ea9a99a07f56b3e3f3db9a472885786f097" title="I know the DGX Spark isnâ€™t what a lot people hoped it would be, but what ifâ€¦â€¦" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What if you bought a ConnectX-7 NIC PCI card and connected the Sparkâ€™s Connect-X-7 port to an existing AI rig that had a couple 3090s in it? Would you be able to offload some layers to your 3090s and use the DGX Spark unified memory for the other layers or whatever? Is this a thing? Or is it not worth even trying? Just curious. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/42fhtkk1jhvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o87u2f/i_know_the_dgx_spark_isnt_what_a_lot_people_hoped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o87u2f/i_know_the_dgx_spark_isnt_what_a_lot_people_hoped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T14:35:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1o888fs</id>
    <title>Has anyone tried EmoVoice (yanghaha0908) â€” emotional TTS? How do I actually run it?</title>
    <updated>2025-10-16T14:50:51+00:00</updated>
    <author>
      <name>/u/Forsaken-Turnip-6664</name>
      <uri>https://old.reddit.com/user/Forsaken-Turnip-6664</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey â€” I found this repo: &lt;a href="https://github.com/yanghaha0908/EmoVoice?utm_source=chatgpt.com"&gt;https://github.com/yanghaha0908/EmoVoice&lt;/a&gt;. Has anyone tried it? Howâ€™s the voice quality and emotion control?&lt;/p&gt; &lt;p&gt;Iâ€™m new to this and on Windows. Could someone give a very short, Windows-only walkthrough to get &lt;strong&gt;one&lt;/strong&gt; audio file (download â†’ install â†’ run)? Thanks a ton!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Forsaken-Turnip-6664"&gt; /u/Forsaken-Turnip-6664 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o888fs/has_anyone_tried_emovoice_yanghaha0908_emotional/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o888fs/has_anyone_tried_emovoice_yanghaha0908_emotional/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o888fs/has_anyone_tried_emovoice_yanghaha0908_emotional/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T14:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7pe3g</id>
    <title>Matthew McConaughey LLaMa</title>
    <updated>2025-10-15T22:34:38+00:00</updated>
    <author>
      <name>/u/ContextualNina</name>
      <uri>https://old.reddit.com/user/ContextualNina</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We thought it would be fun to build something for Matthew McConaughey, based on his recent Rogan podcast interview.&lt;/p&gt; &lt;p&gt;&amp;quot;Matthew McConaughey says he wants a private LLM, fed only with his books, notes, journals, and aspirations, so he can ask it questions and get answers based solely on that information, without any outside influence.&amp;quot;&lt;/p&gt; &lt;p&gt;Pretty classic RAG/context engineering challenge, right? And we use a fine-tuned Llama model in this setup, which also happens to be the most factual and grounded LLM according to the FACTS benchmark (link in comment), Llama-3-Glm-V2. &lt;/p&gt; &lt;p&gt;Here's how we built it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;We found public writings, podcast transcripts, etc, as our base materials to upload as a proxy for the all the information Matthew mentioned in his interview (of course our access to such documents is very limited compared to his).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The agent ingested those to use as a source of truth&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;We configured the agent to the specifications that Matthew asked for in his interview. Note that we already have the most grounded language model (GLM) as the generator, and multiple guardrails against hallucinations, but additional response qualities can be configured via prompt.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Now, when you converse with the agent, it knows to only pull from those sources instead of making things up or use its other training data.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;However, the model retains its overall knowledge of how the world works, and can reason about the responses, in addition to referencing uploaded information verbatim.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The agent is powered by Contextual AI's APIs, and we deployed the full web application on Vercel to create a publicly accessible demo.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ContextualNina"&gt; /u/ContextualNina &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.alrightalrightalright.ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe3g/matthew_mcconaughey_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe3g/matthew_mcconaughey_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T22:34:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1o891o4</id>
    <title>Tensor Logic: The Language of AI</title>
    <updated>2025-10-16T15:21:52+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Progress in AI is hindered by the lack of a programming language with all the requisite features. Libraries like PyTorch and TensorFlow provide automatic differentiation and efficient GPU implementation, but are additions to Python, which was never intended for AI. Their lack of support for automated reasoning and knowledge acquisition has led to a long and costly series of hacky attempts to tack them on. On the other hand, AI languages like LISP an Prolog lack scalability and support for learning. This paper proposes tensor logic, a language that solves these problems by unifying neural and symbolic AI at a fundamental level. The sole construct in tensor logic is the tensor equation, based on the observation that logical rules and Einstein summation are essentially the same operation, and all else can be reduced to them. I show how to elegantly implement key forms of neural, symbolic and statistical AI in tensor logic, including transformers, formal reasoning, kernel machines and graphical models. Most importantly, tensor logic makes new directions possible, such as sound reasoning in embedding space. This combines the scalability and learnability of neural networks with the reliability and transparency of symbolic reasoning, and is potentially a basis for the wider adoption of AI.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2510.12269"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o891o4/tensor_logic_the_language_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o891o4/tensor_logic_the_language_of_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T15:21:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7rchv</id>
    <title>LLama.cpp GPU Support on Android Device</title>
    <updated>2025-10-15T23:56:47+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7rchv/llamacpp_gpu_support_on_android_device/"&gt; &lt;img alt="LLama.cpp GPU Support on Android Device" src="https://a.thumbs.redditmedia.com/VeG6UZmL3mBW6TmARr_WVpKD3xQ0T0XIPkiAj730lQ8.jpg" title="LLama.cpp GPU Support on Android Device" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have figured out a way to Use Android - GPU for LLAMA.CPP&lt;br /&gt; I mean it is not what you would expect like boost in tk/s but it is good for background work mostly&lt;/p&gt; &lt;p&gt;and i didn't saw much of a difference in both GPU and CPU mode&lt;/p&gt; &lt;p&gt;i was using &lt;a href="https://huggingface.co/Menlo/Lucy-128k-gguf/tree/main"&gt;lucy-128k&lt;/a&gt; model, i mean i am also using k-v cache + state file saving so yaa that's all that i got&lt;br /&gt; love to hear more about it from you guys : )&lt;/p&gt; &lt;p&gt;here is the relevant post : &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o7p34f/for_those_building_llamacpp_for_android/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1o7p34f/for_those_building_llamacpp_for_android/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o7rchv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7rchv/llamacpp_gpu_support_on_android_device/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7rchv/llamacpp_gpu_support_on_android_device/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T23:56:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o89fnx</id>
    <title>Thoughts on M5 MacBook Pro to run models locally?</title>
    <updated>2025-10-16T15:36:13+00:00</updated>
    <author>
      <name>/u/A4_Ts</name>
      <uri>https://old.reddit.com/user/A4_Ts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Itâ€™s a huge boost but unfortunately with such little RAM(16gb) my thinking was might as well stay with MacBook Air M4 than shelling out at least 2.5x the amount and instead use Cloud services for $40/month&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A4_Ts"&gt; /u/A4_Ts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o89fnx/thoughts_on_m5_macbook_pro_to_run_models_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o89fnx/thoughts_on_m5_macbook_pro_to_run_models_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o89fnx/thoughts_on_m5_macbook_pro_to_run_models_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T15:36:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7mhf5</id>
    <title>Google &amp; Yale release C2S Scale, a Gemma-based model for cell analysis</title>
    <updated>2025-10-15T20:38:17+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! This is Omar, from the Gemma team.&lt;/p&gt; &lt;p&gt;I'm super excited to share this research based on Gemma. Today, we're releasing a 27B model for single-cell analysis. This model generated hypotheses about how cancer cells behave, and we were able to confirm the predictions with experimental validation in living cells. This reveals a promising new pathway for developing therapies to fight cancer. &lt;/p&gt; &lt;p&gt;This applications of open models for medical use cases are super exciting for me. It's one of many examples of how open models can change the world&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B"&gt;https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2"&gt;https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/"&gt;https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T20:38:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o75kkb</id>
    <title>AI has replaced programmersâ€¦ totally.</title>
    <updated>2025-10-15T08:37:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt; &lt;img alt="AI has replaced programmersâ€¦ totally." src="https://preview.redd.it/bnnb2fb9m8vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1a55140b6915df726dfa4932943df64e43e7d94" title="AI has replaced programmersâ€¦ totally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bnnb2fb9m8vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o86h5j</id>
    <title>What MoE model sizes and capabilities are currently missing in the open weight ecosystem?</title>
    <updated>2025-10-16T13:41:40+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As someone who trains models, Iâ€™d love to know if you have specific requests for model size or capabilities youâ€™d like to see in a (fully) open MoE model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o86h5j/what_moe_model_sizes_and_capabilities_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o86h5j/what_moe_model_sizes_and_capabilities_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o86h5j/what_moe_model_sizes_and_capabilities_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T13:41:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o88j9i</id>
    <title>HuggingChat Omni: new chat app by Hugging Face</title>
    <updated>2025-10-16T15:02:18+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HuggingChat is back! the main new feature is auto-routing to the best open source model for your query. Making it competitive and often better than base chatgpt. &lt;/p&gt; &lt;p&gt;more info about it: &lt;a href="https://x.com/victormustar/status/1978817795312808065?s=46"&gt;https://x.com/victormustar/status/1978817795312808065?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o88j9i/huggingchat_omni_new_chat_app_by_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o88j9i/huggingchat_omni_new_chat_app_by_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T15:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o808av</id>
    <title>I fine-tuned Qwen3-VL (4B &amp; 8B) on a free Colab instance using TRL (SFT and GRPO)!</title>
    <updated>2025-10-16T07:58:52+00:00</updated>
    <author>
      <name>/u/External-Rub5414</name>
      <uri>https://old.reddit.com/user/External-Rub5414</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've created a couple of notebook that work for free on Colab (T4 GPU) to fine-tune the new Qwen3-VL small and dense vision-language models (4B and 8B). Both the Instruct and Thinking variants are supported.&lt;/p&gt; &lt;p&gt;They use &lt;strong&gt;TRL&lt;/strong&gt;, which handles most of the training complexity so you can focus entirely on the specific task you want to fine-tune for.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;SFT&lt;/strong&gt; notebook: fine-tunes with a dataset to refine the model's response style: &lt;a href="https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_qwen_vl.ipynb"&gt;https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_qwen_vl.ipynb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GRPO&lt;/strong&gt; notebook: includes two reward functions to make the non-reasoning model learn to reason (&lt;a href="https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_qwen3_vl.ipynb"&gt;https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_qwen3_vl.ipynb&lt;/a&gt;): &lt;ol&gt; &lt;li&gt;A tag-based reward that checks for &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;answer&amp;gt;&lt;/code&gt; sections.&lt;/li&gt; &lt;li&gt;A length-based reward that discourages overthinking and checks correctness.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both notebooks can be run on a free Colab instance, but can also be scaled up for more advanced setups. The notebooks can also be accessed here: &lt;a href="https://github.com/huggingface/trl/tree/main/examples/notebooks"&gt;https://github.com/huggingface/trl/tree/main/examples/notebooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback and experiments are welcome!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External-Rub5414"&gt; /u/External-Rub5414 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o808av/i_finetuned_qwen3vl_4b_8b_on_a_free_colab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o808av/i_finetuned_qwen3vl_4b_8b_on_a_free_colab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o808av/i_finetuned_qwen3vl_4b_8b_on_a_free_colab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T07:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7b5i4</id>
    <title>Apple unveils M5</title>
    <updated>2025-10-15T13:34:26+00:00</updated>
    <author>
      <name>/u/Agreeable-Rest9162</name>
      <uri>https://old.reddit.com/user/Agreeable-Rest9162</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"&gt; &lt;img alt="Apple unveils M5" src="https://preview.redd.it/5ehnojlm2avf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbc46c6e19f88c18588d2f5384d7fb2dd4717f50" title="Apple unveils M5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following the iPhone 17 AI accelerators, most of us were expecting the same tech to be added to M5. Here it is! Lets see what M5 Pro &amp;amp; Max will add. The speedup from M4 to M5 seems to be around 3.5x for prompt processing. &lt;/p&gt; &lt;p&gt;Faster SSDs &amp;amp; RAM:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Additionally, with up to 2x faster SSD performance than the prior generation, the new 14-inch MacBook Pro lets users load a local LLM faster, and they can now choose up to 4TB of storage. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;150GB/s of unified memory bandwidth&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable-Rest9162"&gt; /u/Agreeable-Rest9162 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5ehnojlm2avf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T13:34:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o83b2i</id>
    <title>Qwen3 Next 80b FP8 with vllm on Pro 6000 Blackwell</title>
    <updated>2025-10-16T11:11:58+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"&gt; &lt;img alt="Qwen3 Next 80b FP8 with vllm on Pro 6000 Blackwell" src="https://b.thumbs.redditmedia.com/YJxZs33mPxgOa7fkc4zIPMKaCT3QAzLt14Sr270bH7A.jpg" title="Qwen3 Next 80b FP8 with vllm on Pro 6000 Blackwell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPU: NVIDIA RTX Pro 6000 Blackwell Edition (96GB VRAM)&lt;/p&gt; &lt;p&gt;- Driver: 580.95.05&lt;/p&gt; &lt;p&gt;- CUDA: 13.0&lt;/p&gt; &lt;p&gt;- Compute Capability: 9.0 (Blackwell)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tf7qkz8ligvf1.png?width=7569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48977ab9548affc46738478260262bbf19184782"&gt;https://preview.redd.it/tf7qkz8ligvf1.png?width=7569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48977ab9548affc46738478260262bbf19184782&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Software:&lt;/p&gt; &lt;p&gt;- vLLM: v0.11.1rc2.dev72+gf7d318de2 (nightly)&lt;/p&gt; &lt;p&gt;- Attention Backend: **FlashInfer** (with JIT autotuning)&lt;/p&gt; &lt;p&gt;- Quantization: FP8 W8A8&lt;/p&gt; &lt;p&gt;- Python: 3.12.12&lt;/p&gt; &lt;p&gt;- PyTorch with CUDA 12.4 backend (forward compatible with CUDA 13.0 driver)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T11:11:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7x7ss</id>
    <title>GLM 4.5 Air AWQ 4bit on RTX Pro 6000 with vllm</title>
    <updated>2025-10-16T04:47:49+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"&gt; &lt;img alt="GLM 4.5 Air AWQ 4bit on RTX Pro 6000 with vllm" src="https://b.thumbs.redditmedia.com/vckUDC6hgDdCUoZhjQ0jn7hdsNydHHYam8wvadpbBSo.jpg" title="GLM 4.5 Air AWQ 4bit on RTX Pro 6000 with vllm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/g45oegzplevf1.png?width=5379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44f1bf58f80336a1524c4fd128d5e07a6034f517"&gt;https://preview.redd.it/g45oegzplevf1.png?width=5379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44f1bf58f80336a1524c4fd128d5e07a6034f517&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ran benchmark of cpatonn/GLM-4.5-Air-AWQ-4bit on a single Pro 6000 with vllm. Nvidia Driver Version: 580.95.05 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T04:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7gpr8</id>
    <title>Got the DGX Spark - ask me anything</title>
    <updated>2025-10-15T17:02:50+00:00</updated>
    <author>
      <name>/u/sotech117</name>
      <uri>https://old.reddit.com/user/sotech117</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"&gt; &lt;img alt="Got the DGX Spark - ask me anything" src="https://preview.redd.it/9mr835ne4bvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42dc8e85dcff8b55d4174e98495bb8d2d144fd7d" title="Got the DGX Spark - ask me anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If thereâ€™s anything you want me to benchmark (or want to see in general), let me know, and Iâ€™ll try to reply to your comment. I will be playing with this all night trying a ton of different models Iâ€™ve always wanted to run. &lt;/p&gt; &lt;p&gt;(&amp;amp; shoutout to microcenter my goats!)&lt;/p&gt; &lt;p&gt;edit: Downloading and setting up a ton of requested programs models currently. Should have some llm and image/video gen numbers tonight.&lt;br /&gt; Gonna do more of the &amp;quot;exotic&amp;quot; requests tomorrow. Over the weekend I'll try to reply to everyone I missed and do whatever I forgot to do!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sotech117"&gt; /u/sotech117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9mr835ne4bvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T17:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o88w9c</id>
    <title>ARM Partners with Meta</title>
    <updated>2025-10-16T15:16:10+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o88w9c/arm_partners_with_meta/"&gt; &lt;img alt="ARM Partners with Meta" src="https://preview.redd.it/wpf7pikaqhvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ab9b69847afb2486fa0aa6d707cbb6b7c878439" title="ARM Partners with Meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ARM Partners with Meta for Data Center and Next Generation Software, Collaboration May Be Interesting Info : &lt;a href="https://x.com/Arm/status/1978494349966025044?t=9tw4dYon0ecqebNQfE5rsQ&amp;amp;s=19"&gt;https://x.com/Arm/status/1978494349966025044?t=9tw4dYon0ecqebNQfE5rsQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wpf7pikaqhvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o88w9c/arm_partners_with_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o88w9c/arm_partners_with_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T15:16:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o82kta</id>
    <title>NVIDIA DGX Spark â€“ A Non-Sponsored Review (Strix Halo Comparison, Pros &amp; Cons)</title>
    <updated>2025-10-16T10:30:22+00:00</updated>
    <author>
      <name>/u/Corylus-Core</name>
      <uri>https://old.reddit.com/user/Corylus-Core</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;NVIDIA DGX Spark â€“ A Non-Sponsored Review (Strix Halo Comparison, Pros &amp;amp; Cons)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Pww8rIzr1pg"&gt;https://www.youtube.com/watch?v=Pww8rIzr1pg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Corylus-Core"&gt; /u/Corylus-Core &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o82kta/nvidia_dgx_spark_a_nonsponsored_review_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o82kta/nvidia_dgx_spark_a_nonsponsored_review_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o82kta/nvidia_dgx_spark_a_nonsponsored_review_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T10:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7pe1u</id>
    <title>gigaResearch</title>
    <updated>2025-10-15T22:34:35+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"&gt; &lt;img alt="gigaResearch" src="https://preview.redd.it/nb2hmgqircvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71c101f2683e8df117cbc2a9abd685bcac5cbce0" title="gigaResearch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nb2hmgqircvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T22:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o84b36</id>
    <title>Qwen3-30B-A3B FP8 on RTX Pro 6000 blackwell with vllm</title>
    <updated>2025-10-16T12:04:11+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"&gt; &lt;img alt="Qwen3-30B-A3B FP8 on RTX Pro 6000 blackwell with vllm" src="https://b.thumbs.redditmedia.com/tevD9Ijs-RE6cQcddGazJZY--0Lzdu42Yn4uKrDPRjI.jpg" title="Qwen3-30B-A3B FP8 on RTX Pro 6000 blackwell with vllm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Power limit set to 450w &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Short Context (1K tokens):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single user: 88.4 tok/s&lt;/li&gt; &lt;li&gt;10 concurrent users: &lt;strong&gt;652 tok/s&lt;/strong&gt; throughput&lt;/li&gt; &lt;li&gt;Latency: 5.65s â†’ 7.65s (1â†’10 users)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Long Context (256K tokens):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single user: 22.0 tok/s&lt;/li&gt; &lt;li&gt;10 concurrent users: &lt;strong&gt;115.5 tok/s&lt;/strong&gt; throughput&lt;/li&gt; &lt;li&gt;Latency: 22.7s â†’ 43.2s (1â†’10 users)&lt;/li&gt; &lt;li&gt;Still able to handle 10 concurrent requests!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Sweet Spot (32K-64K context):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;64K @ 10 users: 311 tok/s total, 31 tok/s per user&lt;/li&gt; &lt;li&gt;32K @ 10 users: 413 tok/s total, 41 tok/s per user&lt;/li&gt; &lt;li&gt;Best balance of context length and throughput&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;FP8 quantization really shines here - getting 115 tok/s aggregate at 256K context with 10 users is wild, even with the power constraint. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x9t4ttsvrgvf1.png?width=7590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c86bf3cc42032a595ee4d02b2c78986da150836"&gt;https://preview.redd.it/x9t4ttsvrgvf1.png?width=7590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c86bf3cc42032a595ee4d02b2c78986da150836&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T12:04:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o866vl</id>
    <title>PaddleOCR-VL, is better than private models</title>
    <updated>2025-10-16T13:29:48+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"&gt; &lt;img alt="PaddleOCR-VL, is better than private models" src="https://b.thumbs.redditmedia.com/X4U2z8D2mUefEIEuBe11hVIGCJrPT-oF1EOiA8c0dyw.jpg" title="PaddleOCR-VL, is better than private models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/PaddlePaddle/status/1978809999263781290?t=mcHYAF7osq3MmicjMLi0IQ&amp;amp;s=19"&gt;https://x.com/PaddlePaddle/status/1978809999263781290?t=mcHYAF7osq3MmicjMLi0IQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o866vl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T13:29:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o81rvs</id>
    <title>Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub</title>
    <updated>2025-10-16T09:41:04+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"&gt; &lt;img alt="Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub" src="https://b.thumbs.redditmedia.com/h0BE1gNO8S-6xv6b1X5IIoHb8CSHWZoS7YxS0LFbuxA.jpg" title="Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog post: How a Gemma model helped discover a new potential cancer therapy pathway - Weâ€™re launching a new 27 billion parameter foundation model for single-cell analysis built on the Gemma family of open models.: &lt;a href="https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/"&gt;https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/&lt;/a&gt;&lt;br /&gt; Hugging Face: &lt;a href="https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B"&gt;https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B&lt;/a&gt;&lt;br /&gt; Scientific preprint on bioRxiv: &lt;a href="https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2"&gt;https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2&lt;/a&gt;&lt;br /&gt; Code on GitHub: &lt;a href="https://github.com/vandijklab/cell2sentence"&gt;https://github.com/vandijklab/cell2sentence&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o81rvs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T09:41:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
