<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-15T09:46:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r4zbqf</id>
    <title>What actually works for roleplay (in my experience)</title>
    <updated>2026-02-14T23:32:08+00:00</updated>
    <author>
      <name>/u/Academic-Map268</name>
      <uri>https://old.reddit.com/user/Academic-Map268</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried endlessly to make roleplay work with increasingly sophisticated system prompts. It doesn't. Whatever you write in the system prompt, the LLM will become a caricature of that.&lt;/p&gt; &lt;p&gt;What actually works: randomizable system prompts.&lt;br /&gt; Parts of the system prompt are static (age, gender, backstory) and others get randomized periodically (mood, goals, desires).&lt;br /&gt; This makes the LLM feel &amp;quot;alive&amp;quot;. Sometimes the orc queen is &amp;quot;melancholic and irritable&amp;quot;, other times she's &amp;quot;energetic and commanding&amp;quot; and a million other trait combinations.&lt;/p&gt; &lt;p&gt;Shaking up the system prompt by randomizing parts of it every once in a while is huge in making the roleplay feel organic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Academic-Map268"&gt; /u/Academic-Map268 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4zbqf/what_actually_works_for_roleplay_in_my_experience/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4zbqf/what_actually_works_for_roleplay_in_my_experience/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4zbqf/what_actually_works_for_roleplay_in_my_experience/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T23:32:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r59seg</id>
    <title>Open-source LLM-as-a-Judge pipeline for comparing local models - feedback welcome</title>
    <updated>2026-02-15T08:47:56+00:00</updated>
    <author>
      <name>/u/gvij</name>
      <uri>https://old.reddit.com/user/gvij</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been trying to evaluate local models more systematically (LLaMA-3, Qwen-Coder, etc.), especially for things like RAG answers and code tasks.&lt;/p&gt; &lt;p&gt;Manual spot-checking wasn’t scaling, so I built a small open-source pipeline that uses &lt;strong&gt;LLM-as-a-Judge&lt;/strong&gt; with structured prompts + logging:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Dakshjain1604/LLM-response-Judge-By-NEO"&gt;https://github.com/Dakshjain1604/LLM-response-Judge-By-NEO&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Not meant to be a product, just a reproducible workflow for batch evals.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;• Compare responses from multiple models&lt;br /&gt; • Score with an LLM judge + reasoning logs&lt;br /&gt; • Export results for analysis&lt;br /&gt; • Easy to plug into RAG or dataset experiments&lt;/p&gt; &lt;p&gt;I’ve been using it to:&lt;/p&gt; &lt;p&gt;• Compare local code models on Kaggle-style tasks&lt;br /&gt; • Check regression when tweaking prompts/RAG pipelines&lt;br /&gt; • Generate preference data for fine-tuning&lt;/p&gt; &lt;p&gt;Two things I noticed while building it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;LLM-judge pipelines are very prompt-sensitive&lt;/li&gt; &lt;li&gt;Logging intermediate reasoning is essential for debugging scores&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Also curious how people here handle evals as I see a lot of benchmark posts but not many reusable pipelines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gvij"&gt; /u/gvij &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r59seg/opensource_llmasajudge_pipeline_for_comparing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r59seg/opensource_llmasajudge_pipeline_for_comparing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r59seg/opensource_llmasajudge_pipeline_for_comparing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T08:47:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4vnzn</id>
    <title>MiniMax M2.5 Performance Testing on dual RTX 6000 Pros</title>
    <updated>2026-02-14T20:55:23+00:00</updated>
    <author>
      <name>/u/itsjustmarky</name>
      <uri>https://old.reddit.com/user/itsjustmarky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4vnzn/minimax_m25_performance_testing_on_dual_rtx_6000/"&gt; &lt;img alt="MiniMax M2.5 Performance Testing on dual RTX 6000 Pros" src="https://preview.redd.it/e1mi4932xijg1.png?width=140&amp;amp;height=79&amp;amp;auto=webp&amp;amp;s=e4a85fbffddb53bc2243d8e9d692d571d18ac435" title="MiniMax M2.5 Performance Testing on dual RTX 6000 Pros" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/e1mi4932xijg1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a0f0bf2c9ff785e5ad1c0040cbb1f97aba34705"&gt;https://preview.redd.it/e1mi4932xijg1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a0f0bf2c9ff785e5ad1c0040cbb1f97aba34705&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itsjustmarky"&gt; /u/itsjustmarky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4vnzn/minimax_m25_performance_testing_on_dual_rtx_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4vnzn/minimax_m25_performance_testing_on_dual_rtx_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4vnzn/minimax_m25_performance_testing_on_dual_rtx_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T20:55:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4lx7x</id>
    <title>Nemotron3 Super/Ultra: FP4 pre-training, H1 2026 release, "NVIDIA is a company of volunteers" (all from recent NVIDIA interview)</title>
    <updated>2026-02-14T14:25:15+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nathan Lambert (from Ai2) interviewed an NVIDIA's VP of Applied Deep Learning Research: &lt;a href="https://www.interconnects.ai/p/why-nvidia-builds-open-models-with"&gt;Why Nvidia builds open models with Bryan Catanzaro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Many interesting bits, but of course I was hoping for hints of when the next Nemotron3 models were to be released. Nothing really new there, &amp;quot;2026 H1&amp;quot; is a pretty broad window. &lt;/p&gt; &lt;p&gt;This was interesting: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;we’re pre-training our Nemotron-3 Super and Ultra models using FP4 which is a thing that, you know, hasn’t been done publicly anyway and something that, you know, we’re pretty excited about because our GPUs have really awesome FP4 throughput. But obviously, the numerical challenges of, like, trying to train a state-of-the-art language model using four bits is non-trivial. ...&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Hopefully those will be highly performant at Q4 quants. &lt;/p&gt; &lt;p&gt;Many other interesting things in the interview, such as motivations for creating open source models. Nathan asks this of various open-source guests, &amp;quot;what is your business reason&amp;quot; -- the NVIDIA VP effectively says, &amp;quot;so people will keep buying NVIDIA GPUs.&amp;quot; (Do they see a lot more businesses running local models, on-prem or in the cloud?)&lt;/p&gt; &lt;p&gt;Another interesting thing: more than once the VP said that &amp;quot;NVIDIA is a company of volunteers&amp;quot; -- if you ctrl+f for &amp;quot;volunteers&amp;quot; in the transcript you will see it repeatedly. &lt;/p&gt; &lt;p&gt;The context is &amp;quot;how do you manage and coordinate people to work on Nemotron,&amp;quot; but the wording still caught me off-guard -- &amp;quot;Hey I want to volunteer there...&amp;quot;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;00:22:25 Nathan Lambert: ...Do you have any advice for making the orgs come together? ...&lt;/p&gt; &lt;p&gt;00:23:20 Bryan Catanzaro: You know what’s worked for us is invitation and not control. ... So you know, NVIDIA is a very decentralized company with a lot of volunteers. You know, everybody that works at NVIDIA is a volunteer. And what do I mean by that? Well, I mean, look, the industry is moving quick.&lt;/p&gt; &lt;p&gt;You know, people can always move from one job to the next. So the way that we think about the work that we do is like, it’s very decentralized, it’s very much let smart people figure out what they should be doing and then kind of self-organize. ... There’s just an enormous number of brilliant people that have decided that they’re gonna volunteer to make Nemotron awesome, and we’re, we’re starting to see some pretty great things come together.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;...etc. &lt;/p&gt; &lt;p&gt;Full interview is very interesting.&lt;/p&gt; &lt;p&gt;Edit: much more excited about the FP4 training in retrospect. &lt;/p&gt; &lt;p&gt;And I wonder how hard it would be to REAM the 500B version...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lx7x/nemotron3_superultra_fp4_pretraining_h1_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lx7x/nemotron3_superultra_fp4_pretraining_h1_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lx7x/nemotron3_superultra_fp4_pretraining_h1_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T14:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4tu48</id>
    <title>A 0.2M, 271KB INT8 GRU+attention based TinyStories model that (tries) to generate stories.</title>
    <updated>2026-02-14T19:40:39+00:00</updated>
    <author>
      <name>/u/ValuableLucky8566</name>
      <uri>https://old.reddit.com/user/ValuableLucky8566</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The datasheet used is TinyStories-valid.txt, 20MB.&lt;/p&gt; &lt;p&gt;The model was trained on an Nvidia T4 for an hour, converged to a loss of 0.9 with 10000 steps and a batch size of 128.&lt;/p&gt; &lt;p&gt;The model was trained on the same architecture as that on the original tinystoriesgru model which was 2.5M parameters large at 10MB. &lt;/p&gt; &lt;p&gt;It uses a character level tokenizer, so the vocab stays entirely in the chat.py.&lt;/p&gt; &lt;p&gt;It uses memory gating by making a proposed memory M~t=tanh⁡(Wcht+bc), and updates by mixing the current memory with the new one Mt=(1−pt)⊙Mt−1+pt⊙M~t. &lt;/p&gt; &lt;p&gt;The model is trained with a single attention layer in the &lt;a href="http://train.py"&gt;train.py&lt;/a&gt; file, using nn.MultiheadAttention. It uses search query-based attention for filling the memory lane/mixing post training, which gives it a complexity of O(T²d²).&lt;/p&gt; &lt;p&gt;This model introduces W(hh) multiplier to the input h(t-1). The eigenvalues are used as a knob to 'fake' the anchor signal.&lt;/p&gt; &lt;p&gt;The original FP32 weights are ~1MB.&lt;/p&gt; &lt;p&gt;The measured spectral radius for FP32 is 1.8842. (Essentially, for a GRU, when this value is &amp;gt;1, the model is generally unstable and random. If it is less than one, it is considered conservative.)&lt;/p&gt; &lt;p&gt;The measured INT8 value for the same was 0.5855. The model has no perfect orthogonality, as the cosine similarities are similar or same for both.&lt;/p&gt; &lt;p&gt;Because of this, the INT8 model feels conservative even at temperature 0.7, whereas FP32 might collapse quick around temperature 0.8 and needs to be fixed at 0.5 for proper/meaningful generation.&lt;/p&gt; &lt;p&gt;Example comparision:&lt;/p&gt; &lt;h1&gt;INT8 (271KB):&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/kavyamali/tinystoriesgru#int8-271kb"&gt;&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Prompt&lt;/th&gt; &lt;th align="left"&gt;Output&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;The little bird was very sad because he could not fly.&lt;/td&gt; &lt;td align="left"&gt;The little bird was very sad because he could not fly. They all felt too big and safe. They were sorry for help. Suddenly, a big butterfly with the bark with their friends. They also played with the bird for a pond with her friends. They are happy and safe. He wanted to go on a pond with his mom's car. They were done and said goodbye to the park. They do not like to come back to her. He was so happy and they could help her and said, &amp;quot;I would do not have fun. They saw a big box of the birds. They liked to play with his toys.&amp;quot;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Once upon a time&lt;/td&gt; &lt;td align="left"&gt;Once upon a time there was a little boy named Timmy. Timmy was so excited and said, &amp;quot;That's a searce was so beautiful. He wanted to help her finished, and that he was tired and something scared. So, they had to go to the picture from the day, Jimmy was so excited. He was happy that he was very happy to explore the grass. They had a lot of fun that he could not make a swimmer.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;FP32 (1MB):&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/kavyamali/tinystoriesgru#fp32-1mb"&gt;&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Prompt&lt;/th&gt; &lt;th align="left"&gt;Output&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;The little bird was very sad because he could not fly.&lt;/td&gt; &lt;td align="left"&gt;The little bird was very sad because he could not fly. They saw that he had found a special bird for her friends. They looked around, but they were so scared. When they were all little girl named Lily and they were so happy. They had so much fun and happy and he could see her. She was so excited to see the birds and even safe. They had to do and she was happy. They looked around and said, &amp;quot;Yes, help you a special cookies. She liked to play with his treat. She was happy that she was very proud of himself and had a fun toys in the sky.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Once upon a time&lt;/td&gt; &lt;td align="left"&gt;Once upon a time there was a little girl and the same scarf toy careful with her mommy to play with his toys. They had a good squirrel and the bird with a special boy. He was so happy that he realized that the ball both had a warm before making a sun with the sea. They all gave her a such a big boy who was going to be so happy. They had a good day she could say sorry and hugged as he held. The bird said that Tom was a long time and said, &amp;quot;I want to play with the big special new pillows and the yummy story.&amp;quot;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The architecture and &lt;a href="http://train.py"&gt;train.py&lt;/a&gt; along with the model weights are all on github:&lt;br /&gt; &lt;a href="https://github.com/kavyamali/tinystoriesgru"&gt;https://github.com/kavyamali/tinystoriesgru&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you for reading!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ValuableLucky8566"&gt; /u/ValuableLucky8566 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4tu48/a_02m_271kb_int8_gruattention_based_tinystories/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4tu48/a_02m_271kb_int8_gruattention_based_tinystories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4tu48/a_02m_271kb_int8_gruattention_based_tinystories/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T19:40:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4vlh4</id>
    <title>Fix for JSON Parser Errors with Qwen3 Next Coder + OpenCode in llama.cpp</title>
    <updated>2026-02-14T20:52:27+00:00</updated>
    <author>
      <name>/u/zpirx</name>
      <uri>https://old.reddit.com/user/zpirx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just a friendly reminder because this keeps coming up in the last few days:&lt;/p&gt; &lt;p&gt;if you’re using Qwen3 Next Coder + OpenCode with llama.cpp you’ll likely run into JSON parser errors. switch to pwilkin’s (aka ilintar) autoparser branch. it fixes the issue for now. &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18675"&gt;https://github.com/ggml-org/llama.cpp/pull/18675&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zpirx"&gt; /u/zpirx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4vlh4/fix_for_json_parser_errors_with_qwen3_next_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4vlh4/fix_for_json_parser_errors_with_qwen3_next_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4vlh4/fix_for_json_parser_errors_with_qwen3_next_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T20:52:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4lg46</id>
    <title>We need to bring back the "experimental" era of LLMs</title>
    <updated>2026-02-14T14:04:45+00:00</updated>
    <author>
      <name>/u/TemperatureMajor5083</name>
      <uri>https://old.reddit.com/user/TemperatureMajor5083</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you remember projects like &lt;a href="https://en.wikipedia.org/wiki/GPT-4Chan"&gt;GPT-4chan&lt;/a&gt;? Back then, training on more &amp;quot;unconventional&amp;quot; data sources was far more common than it is today, where most models tend to converge on the same polished, &amp;quot;helpful assistant&amp;quot; persona. It’s interesting to think about what we could build with today’s high-performance base models if they were fine-tuned on more distinctive, niche datasets. Done well, that could be genuinely entertaining.&lt;/p&gt; &lt;p&gt;The recently posted MechaEpstein kind of goes in that direction, but I think there’s room to be more creative than just having it reply with &amp;quot;&amp;lt;thing&amp;gt; are goy. Sorry for the typos. Sent from my iPhone.&amp;quot; to every message.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TemperatureMajor5083"&gt; /u/TemperatureMajor5083 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lg46/we_need_to_bring_back_the_experimental_era_of_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lg46/we_need_to_bring_back_the_experimental_era_of_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lg46/we_need_to_bring_back_the_experimental_era_of_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T14:04:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4hhyy</id>
    <title>local vibe coding</title>
    <updated>2026-02-14T10:37:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please share your experience with vibe coding using local (not cloud) models.&lt;/p&gt; &lt;p&gt;General note: to use tools correctly, some models require a modified chat template, or you may need in-progress PR.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/anomalyco/opencode"&gt;https://github.com/anomalyco/opencode&lt;/a&gt; - probably the most mature and feature complete solution. I use it similarly to Claude Code and Codex.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/mistralai/mistral-vibe"&gt;https://github.com/mistralai/mistral-vibe&lt;/a&gt; - a nice new project, similar to opencode, but simpler.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/RooCodeInc/Roo-Code"&gt;https://github.com/RooCodeInc/Roo-Code&lt;/a&gt; - integrates with Visual Studio Code (not CLI).&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Aider-AI/aider"&gt;https://github.com/Aider-AI/aider&lt;/a&gt; - a CLI tool, but it feels different from opencode (at least in my experience).&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.continue.dev/"&gt;https://docs.continue.dev/&lt;/a&gt; - I tried it last year as a Visual Studio Code plugin, but I never managed to get the CLI working with llama.cpp.&lt;/li&gt; &lt;li&gt;Cline - I was able to use it as Visual Studio Code plugin &lt;/li&gt; &lt;li&gt;Kilo Code - I was able to use it as Visual Studio Code plugin &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hhyy/local_vibe_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hhyy/local_vibe_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hhyy/local_vibe_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T10:37:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r547ur</id>
    <title>Popular MoEs speed comparison (Apple Silicon, llama.cpp)</title>
    <updated>2026-02-15T03:28:13+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r547ur/popular_moes_speed_comparison_apple_silicon/"&gt; &lt;img alt="Popular MoEs speed comparison (Apple Silicon, llama.cpp)" src="https://preview.redd.it/lbu0zng7skjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85e45d0296314003ac6acc7b568d0d00960fc3ef" title="Popular MoEs speed comparison (Apple Silicon, llama.cpp)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some interesting insights into comparing what in my opinion are the best models - best for performance to parameter size trade off for &lt;strong&gt;moderately&lt;/strong&gt; priced hardware right now:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;GPT-OSS:120B despite being bigger for both active parameters and total parameters is faster than GLM-4.7-Flash, Qwen3-a3b and Qwen-Next-a3b. It really is a great model and is still my go to for general use.&lt;/li&gt; &lt;li&gt;I dont know what they cooked with Nemotron Nano but its SIGNIFICANTLY faster despite being bigger relative to the other a3b boys. Need to use it more.&lt;/li&gt; &lt;li&gt;GLM-4.7-flash's speed loss at large context sizes is a tragedy. I was looking forward to using it as the new daily driver for easy coding tasks but now qwen3-coder-next is out and might be comparable in speed but superior in coding performance. That's the next thing to setup and check out for me&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apple Silicon - M3 Ultra 256GB&lt;/li&gt; &lt;li&gt;llama.cpp&lt;/li&gt; &lt;li&gt;data from llama-bench with 10000 token context size and 500 token output size. Results pictured are for token generation at depth=10000 - felt this is the best proxy for agentic coding applications where system prompts themselves are regularly in this ball park&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lbu0zng7skjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r547ur/popular_moes_speed_comparison_apple_silicon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r547ur/popular_moes_speed_comparison_apple_silicon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T03:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4hx24</id>
    <title>models : optimizing qwen3next graph by ggerganov · Pull Request #19375 · ggml-org/llama.cpp</title>
    <updated>2026-02-14T11:03:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hx24/models_optimizing_qwen3next_graph_by_ggerganov/"&gt; &lt;img alt="models : optimizing qwen3next graph by ggerganov · Pull Request #19375 · ggml-org/llama.cpp" src="https://external-preview.redd.it/VVE5ljhhmuVj3S3mZp6__yfxBNtIYwLxEpi1hRwGGjU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95716ff4da82d5b8eaae30528e020e4dd291b4ab" title="models : optimizing qwen3next graph by ggerganov · Pull Request #19375 · ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Faster (t/s) Qwen Next models.&lt;/p&gt; &lt;p&gt;There are still some in-progress PRs to fix/improve Qwen Next in llama.cpp. Let's hope this model will be awesome soon :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19375"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hx24/models_optimizing_qwen3next_graph_by_ggerganov/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hx24/models_optimizing_qwen3next_graph_by_ggerganov/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T11:03:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4nt7u</id>
    <title>Qwen3-TTS.cpp</title>
    <updated>2026-02-14T15:43:12+00:00</updated>
    <author>
      <name>/u/redditgivingmeshit</name>
      <uri>https://old.reddit.com/user/redditgivingmeshit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4nt7u/qwen3ttscpp/"&gt; &lt;img alt="Qwen3-TTS.cpp" src="https://external-preview.redd.it/lXcar2l04LaOhzsDCTxgOTofdAoFx7BoAnOuaZ4jNcw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f7e414f528eab2e463ea468be5c2271e564d443" title="Qwen3-TTS.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Lightweight GGML implementation of Qwen3-TTS 0.6B&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4x Speedup compared to pytorch pipeline, with ~2 Gigs of Memory usage.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hi, this was something I've been working on for the last few days. The result actually performed better than expected, so I'm sharing it here.&lt;/p&gt; &lt;p&gt;The pipeline was optimized with Metal backend support &amp;amp; CoreML code predictor. The other parts contained operations that were not able to be loaded into the ANE, so only the code predictor was converted.&lt;/p&gt; &lt;p&gt;No quantization support yet, but coming soon. Turns out using Q8 for the entire pipeline produces bad results. I'm still figuring out which parts are sensitive to quantization and which parts are okay.&lt;/p&gt; &lt;p&gt;Supports all features, including voice cloning&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redditgivingmeshit"&gt; /u/redditgivingmeshit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/predict-woo/qwen3-tts.cpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4nt7u/qwen3ttscpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4nt7u/qwen3ttscpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T15:43:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r56ak1</id>
    <title>Ground-up MLX reimplementation of Qwen3-ASR for Apple Silicon</title>
    <updated>2026-02-15T05:19:47+00:00</updated>
    <author>
      <name>/u/PrimaryAbility9</name>
      <uri>https://old.reddit.com/user/PrimaryAbility9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r56ak1/groundup_mlx_reimplementation_of_qwen3asr_for/"&gt; &lt;img alt="Ground-up MLX reimplementation of Qwen3-ASR for Apple Silicon" src="https://external-preview.redd.it/KDZqgIfnwVfMzphVYrHDwwZwiqbRfErXcNBAe6CZe0k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=255b94cddc71c3ada0252399ffc5d7764559ae69" title="Ground-up MLX reimplementation of Qwen3-ASR for Apple Silicon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ground-up MLX reimplementation of Qwen3-ASR for Apple Silicon&lt;/p&gt; &lt;p&gt;Qwen3-ASR is the new open-source SOTA model for ASR and this can now run natively on M-series GPUs.&lt;/p&gt; &lt;p&gt;pip install mlx-qwen3-asr&lt;/p&gt; &lt;p&gt;Benchmarks (M4 Pro, 0.6B fp16):&lt;br /&gt; - 2.5s clip: 0.46s, RTF 0.08 &lt;br /&gt; - 10s clip: 0.83s, RTF 0.08&lt;br /&gt; - 4-bit quantized: 4.7x faster, WER 2.29% → 2.72% (LibriSpeech test-clean, n=100)&lt;br /&gt; - vs official PyTorch on multilingual-100: 15.99% vs 16.69% WER&lt;/p&gt; &lt;p&gt;Features:&lt;br /&gt; - 0.6B and 1.7B models, 52 languages&lt;br /&gt; - Word-level timestamps (native MLX forced aligner)&lt;br /&gt; - 4-bit / 8-bit quantization&lt;br /&gt; - Streaming and speculative decoding (experimental)&lt;br /&gt; - Output: txt, json, srt, vtt, tsv&lt;br /&gt; - 393 tests, all benchmarks backed by committed JSON artifacts&lt;/p&gt; &lt;p&gt;4 dependencies: mlx, numpy, regex, huggingface-hub.&lt;br /&gt; PyTorch, no transformers in the inference path.&lt;/p&gt; &lt;p&gt;Memory: ~1.2 GB (0.6B), ~3.4 GB (1.7B)&lt;/p&gt; &lt;p&gt;P.S. This is what claude &amp;amp; codex worked on for valentine's day. Speaker diarization is coming soon!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrimaryAbility9"&gt; /u/PrimaryAbility9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/moona3k/mlx-qwen3-asr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r56ak1/groundup_mlx_reimplementation_of_qwen3asr_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r56ak1/groundup_mlx_reimplementation_of_qwen3asr_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T05:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4utig</id>
    <title>Did anyone compare this model to the full Qwen coder? it claims to give almost identical performance at 60B</title>
    <updated>2026-02-14T20:20:20+00:00</updated>
    <author>
      <name>/u/Significant_Fig_7581</name>
      <uri>https://old.reddit.com/user/Significant_Fig_7581</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4utig/did_anyone_compare_this_model_to_the_full_qwen/"&gt; &lt;img alt="Did anyone compare this model to the full Qwen coder? it claims to give almost identical performance at 60B" src="https://external-preview.redd.it/dBcOJXWudLyTVcABpzExdWUuWIm1TJwZv-cq3Wuv3Xs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3430d9389e6ef329e44ca58a39b2bbc8f2f392d" title="Did anyone compare this model to the full Qwen coder? it claims to give almost identical performance at 60B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Fig_7581"&gt; /u/Significant_Fig_7581 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mradermacher/Qwen3-Coder-Next-REAM-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4utig/did_anyone_compare_this_model_to_the_full_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4utig/did_anyone_compare_this_model_to_the_full_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T20:20:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4mks7</id>
    <title>6-GPU local LLM workstation (≈200GB+ VRAM) – looking for scaling / orchestration advice</title>
    <updated>2026-02-14T14:53:06+00:00</updated>
    <author>
      <name>/u/shiftyleprechaun</name>
      <uri>https://old.reddit.com/user/shiftyleprechaun</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4mks7/6gpu_local_llm_workstation_200gb_vram_looking_for/"&gt; &lt;img alt="6-GPU local LLM workstation (≈200GB+ VRAM) – looking for scaling / orchestration advice" src="https://preview.redd.it/blrzr76h4hjg1.jpg?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=bfd384e1306c8a061f9957d6b40c7914ce182d4f" title="6-GPU local LLM workstation (≈200GB+ VRAM) – looking for scaling / orchestration advice" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am newer to building high-end hardware but have been researching local LLM infrastructure for about a year.&lt;/p&gt; &lt;p&gt;Last night was the first time I had all six GPUs running three open-source reasoning models concurrently without stability issues.&lt;/p&gt; &lt;p&gt;Current setup (high level):&lt;/p&gt; &lt;p&gt;Threadripper PRO platform&lt;/p&gt; &lt;p&gt;256GB ECC RAM&lt;/p&gt; &lt;p&gt;~200GB+ aggregate VRAM across 6 GPUs (mix of 24GB + higher VRAM cards)&lt;/p&gt; &lt;p&gt;Dual PSU&lt;/p&gt; &lt;p&gt;Open-air rack&lt;/p&gt; &lt;p&gt;Ubuntu 24.04&lt;/p&gt; &lt;p&gt;Gen4 + Gen5 NVMe&lt;/p&gt; &lt;p&gt;Primary use case is running larger reasoning models locally for internal data analysis + workflow automation&lt;/p&gt; &lt;p&gt;Currently experimenting with multi-model concurrency and different GPU assignment strategies.&lt;/p&gt; &lt;p&gt;I would really appreciate feedback from people running similar multi-GPU rigs:&lt;/p&gt; &lt;p&gt;At this scale, what typically becomes the first real bottleneck for local LLM inference VRAM, PCIe bandwidth, CPU orchestration, memory bandwidth, something else?&lt;/p&gt; &lt;p&gt;Is mixing GPU types a long-term pain point, or fine as long as models are pinned deliberately?&lt;/p&gt; &lt;p&gt;For those running multiple reasoning models simultaneously, where did you start seeing diminishing returns?&lt;/p&gt; &lt;p&gt;How are people handling model scheduling across GPUs — static pinning vs dynamic routing?&lt;/p&gt; &lt;p&gt;If you were building today, would you consolidate into fewer high-VRAM GPUs or keep a distributed multi-card setup?&lt;/p&gt; &lt;p&gt;What is one mistake people make when building larger local LLM workstations?&lt;/p&gt; &lt;p&gt;Still learning — would rather hear what I am overlooking than what I got right, but I appreciate any comments questions or feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shiftyleprechaun"&gt; /u/shiftyleprechaun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r4mks7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4mks7/6gpu_local_llm_workstation_200gb_vram_looking_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4mks7/6gpu_local_llm_workstation_200gb_vram_looking_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T14:53:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r58zep</id>
    <title>MiniMax-M2.5 REAP models available on HF</title>
    <updated>2026-02-15T07:58:25+00:00</updated>
    <author>
      <name>/u/Look_0ver_There</name>
      <uri>https://old.reddit.com/user/Look_0ver_There</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just noticed that a bunch of REAP variants for MiniMax M2.5 got pushed to HF here: &lt;a href="https://huggingface.co/Akicou/models"&gt;https://huggingface.co/Akicou/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been messing about flipping between Qwen Coder Next and MiniMax M2.5, and just personally I've been preferring MiniMax. QCN does eventually get things right, but I find that I have to babysit it and nudge it fairly heavily, whereas MiniMax, while a lot more verbose, does seem to require less hand-holding.&lt;/p&gt; &lt;p&gt;That's just my take though. I'm running on a 128GB Strix Halo though, and I've had to run with Unsloth's Q3_K_XL quants just to make MiniMax fit with a large enough context such that the system isn't begging for mercy after 3 prompts.&lt;/p&gt; &lt;p&gt;Anyway, that HF account there has 19, 29, 39, and 50% REAPS available. Presently just safetensors, but they're easy to convert. I'm going to mess about with the 19% and 29% REAPS, and see how they work out. Hope others may find these useful too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Look_0ver_There"&gt; /u/Look_0ver_There &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r58zep/minimaxm25_reap_models_available_on_hf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r58zep/minimaxm25_reap_models_available_on_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r58zep/minimaxm25_reap_models_available_on_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T07:58:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4yg6p</id>
    <title>[Release] AdaLLM: NVFP4-first inference on RTX 4090 (FP8 KV cache + custom FP8 decode)</title>
    <updated>2026-02-14T22:53:06+00:00</updated>
    <author>
      <name>/u/Educational_Cry_7951</name>
      <uri>https://old.reddit.com/user/Educational_Cry_7951</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I have been working on &lt;strong&gt;AdaLLM&lt;/strong&gt; (repo: &lt;a href="https://github.com/BenChaliah/NVFP4-on-4090-vLLM"&gt;https://github.com/BenChaliah/NVFP4-on-4090-vLLM&lt;/a&gt;) to make NVFP4 weights actually usable on Ada Lovelace GPUs (sm_89). The focus is a pure NVFP4 fast path: FP8 KV cache, custom FP8 decode kernel, no silent FP16 fallback. It currently targets Qwen3 (dense + MoE) and Gemma3 (including sliding-window layers), I'll be adding support to other models soon.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Please think of giving the Github repo a STAR if you like it :)&lt;/strong&gt; &lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Why this is interesting&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;NVFP4-first runtime for Ada GPUs (tested on RTX 4090) with FP8 KV cache end-to-end.&lt;/li&gt; &lt;li&gt;Custom Triton FP8 decode kernel; prefill uses FlashAttention (varlen).&lt;/li&gt; &lt;li&gt;No FP16 fallback for decode. If FP8 kernel fails, it errors out instead of silently switching.&lt;/li&gt; &lt;li&gt;Tensor-parallel (NCCL) + CUDA graphs for decode (also support eager mode)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Benchmarks (RTX 4090)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Qwen3-8B-NVFP4&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;batch&lt;/th&gt; &lt;th align="left"&gt;total tokens&lt;/th&gt; &lt;th align="left"&gt;seconds&lt;/th&gt; &lt;th align="left"&gt;tok/s&lt;/th&gt; &lt;th align="left"&gt;peak GB&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;128&lt;/td&gt; &lt;td align="left"&gt;3.3867&lt;/td&gt; &lt;td align="left"&gt;37.79&lt;/td&gt; &lt;td align="left"&gt;7.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;256&lt;/td&gt; &lt;td align="left"&gt;3.5471&lt;/td&gt; &lt;td align="left"&gt;72.17&lt;/td&gt; &lt;td align="left"&gt;7.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;3.4392&lt;/td&gt; &lt;td align="left"&gt;148.87&lt;/td&gt; &lt;td align="left"&gt;7.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;3.4459&lt;/td&gt; &lt;td align="left"&gt;297.16&lt;/td&gt; &lt;td align="left"&gt;7.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;2048&lt;/td&gt; &lt;td align="left"&gt;4.3636&lt;/td&gt; &lt;td align="left"&gt;469.34&lt;/td&gt; &lt;td align="left"&gt;7.56&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Gemma3-27B-it-NVFP4&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;batch&lt;/th&gt; &lt;th align="left"&gt;total tokens&lt;/th&gt; &lt;th align="left"&gt;seconds&lt;/th&gt; &lt;th align="left"&gt;tok/s&lt;/th&gt; &lt;th align="left"&gt;peak GB&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;128&lt;/td&gt; &lt;td align="left"&gt;9.3982&lt;/td&gt; &lt;td align="left"&gt;13.62&lt;/td&gt; &lt;td align="left"&gt;19.83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;256&lt;/td&gt; &lt;td align="left"&gt;9.5545&lt;/td&gt; &lt;td align="left"&gt;26.79&lt;/td&gt; &lt;td align="left"&gt;19.83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;9.5344&lt;/td&gt; &lt;td align="left"&gt;53.70&lt;/td&gt; &lt;td align="left"&gt;19.84&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;for Qwen3-8B-NVFP4 I observed ~2.4x lower peak VRAM vs Qwen3-8B FP16 baselines (with ~20-25% throughput loss).&lt;/p&gt; &lt;h1&gt;Quickstart&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pip install git+https://github.com/BenChaliah/NVFP4-on-4090-vLLM.git adallm serve nvidia/Qwen3-8B-NVFP4 &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;p&gt;`export NVFP4_FP8=1` is optional and enables FP8 GEMM path (NVFP4_FP8=0: the difference is in compute precision not VRAM, FP8 KV cache + the FP8 decode kernel are still used. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Supported models (so far)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;nvidia/Qwen3-8B-NVFP4&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;BenChaliah/Gemma3-27B-it-NVFP4&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Qwen3 MoE variants are supported, but still slow (see README for MoE notes).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;MoE routing and offload paths are not fully optimized yet (working on it currently)&lt;/li&gt; &lt;li&gt;Only NVFP4 weights, no FP16 fallback for decode by design.&lt;/li&gt; &lt;li&gt;Targeted at Ada Lovelace (sm_89). Needs validation on other Ada cards.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Repo&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/BenChaliah/NVFP4-on-4090-vLLM"&gt;https://github.com/BenChaliah/NVFP4-on-4090-vLLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you have a RTX 4000 series GPU, I would love to hear results or issues. Also looking for help on MoE CPU-Offloading optimization, extra model support, and kernel tuning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Cry_7951"&gt; /u/Educational_Cry_7951 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4yg6p/release_adallm_nvfp4first_inference_on_rtx_4090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4yg6p/release_adallm_nvfp4first_inference_on_rtx_4090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4yg6p/release_adallm_nvfp4first_inference_on_rtx_4090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T22:53:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r55dhq</id>
    <title>Opencode Manager</title>
    <updated>2026-02-15T04:29:30+00:00</updated>
    <author>
      <name>/u/getfitdotus</name>
      <uri>https://old.reddit.com/user/getfitdotus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r55dhq/opencode_manager/"&gt; &lt;img alt="Opencode Manager" src="https://external-preview.redd.it/AUQf-LOorsrfdWmtZxI5-8VZ7v2JlosFkQ5o_bNyMlg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=992d846e4a25e907443be4fd06878fda1fbed597" title="Opencode Manager" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Opencode for your phone. Deployable docker container with Git / File browser / speech to text / text to speech / push notifications and much more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getfitdotus"&gt; /u/getfitdotus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/chriswritescode-dev/opencode-manager"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r55dhq/opencode_manager/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r55dhq/opencode_manager/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T04:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4n3as</id>
    <title>Heretic 1.2 released: 70% lower VRAM usage with quantization, Magnitude-Preserving Orthogonal Ablation ("derestriction"), broad VL model support, session resumption, and more</title>
    <updated>2026-02-14T15:14:00+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llamas and Gentlemen,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Heretic&lt;/strong&gt; (&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;) is the leading software for removing censorship from language models. In the three months since its initial release, &lt;a href="https://huggingface.co/models?other=heretic"&gt;more than 1,300 models&lt;/a&gt; (including quants) made using Heretic have been published by the community. This represents more than a third of all abliterated models ever published, and the vast majority of abliterated models published since Heretic's first release.&lt;/p&gt; &lt;p&gt;Today, I am happy to announce the release of Heretic 1.2, the product of two months of hard work by the Heretic contributors.&lt;/p&gt; &lt;p&gt;The headline feature is the new LoRA-based abliteration engine implemented by accemlcc. Built on top of PEFT, it supports loading models with 4-bit quantization using bitsandbytes, which can reduce VRAM requirements for processing a model by up to 70%. The abliterated model is still exported in full precision, which is achieved by re-loading the original model in system RAM and applying the optimized LoRA adapter on top of it, yielding a high-quality model despite the low resource requirements. To enable quantized loading, set &lt;code&gt;quantization&lt;/code&gt; to &lt;code&gt;bnb_4bit&lt;/code&gt; in the configuration.&lt;/p&gt; &lt;p&gt;spikymoth implemented Magnitude-Preserving Orthogonal Ablation (MPOA) aka Norm-Preserving Biprojected Abliteration aka &amp;quot;derestriction&amp;quot;, a refined abliteration technique developed by Jim Lai which can improve the quality of the resulting model in many cases. This has been one of the most frequently requested features from the community, and is now finally available. To enable MPOA, set &lt;code&gt;orthogonalize_direction&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; and &lt;code&gt;row_normalization&lt;/code&gt; to &lt;code&gt;full&lt;/code&gt; in the configuration.&lt;/p&gt; &lt;p&gt;Heretic's implementation of MPOA uses Optuna to optimize weight parameters. This can result in models that are better than those generated with the original MPOA technique, which employs a different strategy for layer selection. For example, &lt;code&gt;MuXodious/gpt-oss-20b-RichardErkhov-heresy&lt;/code&gt; dominates &lt;code&gt;ArliAI/gpt-oss-20b-Derestricted&lt;/code&gt; on the UGI Leaderboard, scoring 39.05 vs 34.22 and beating the derestricted model in every individual test (W/10, NatInt, and Writing).&lt;/p&gt; &lt;p&gt;After a long history of hacks being passed around in the community, anrp finally found a clean way to support vision language models in Heretic, and a broad range of VL models can now be processed. Note that only the language model part (the text decoder transformer) is abliterated, not the image encoder.&lt;/p&gt; &lt;p&gt;anrp also implemented fully automatic session progress saving and resumption. This means worrying about crashes during a long optimization run is now a thing of the past, as you can simply restart Heretic and it will offer to continue where it left off. You can also interrupt the run yourself at any time with Ctrl+C, and resume it later.&lt;/p&gt; &lt;p&gt;Please see the release notes for the full list of improvements and fixes. More exciting stuff is coming in future versions!&lt;/p&gt; &lt;p&gt;Cheers :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4n3as/heretic_12_released_70_lower_vram_usage_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4n3as/heretic_12_released_70_lower_vram_usage_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4n3as/heretic_12_released_70_lower_vram_usage_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T15:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r59th1</id>
    <title>Kreuzberg v4.3.0 and benchmarks</title>
    <updated>2026-02-15T08:49:49+00:00</updated>
    <author>
      <name>/u/Eastern-Surround7763</name>
      <uri>https://old.reddit.com/user/Eastern-Surround7763</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;we have two announcements to share about &lt;a href="https://github.com/kreuzberg-dev/kreuzberg"&gt;Kreuzberg&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;First, we’ve published a new set of comparative benchmarks with an interactive UI and fully reproducible results. We’ve been working on these for quite some time, and the goal is to help developers understand how Kreuzberg behaves in real production scenarios and to make performance claims transparent and verifiable.&lt;/p&gt; &lt;p&gt;Second, we released Kreuzberg v4.3.0, which brings several improvements and adds PaddleOCR as an optional backend through a native Rust integration. This release is particularly important for teams working with Chinese and other East Asian languages, where Paddle models perform very well.&lt;/p&gt; &lt;p&gt;What is Kreuzberg?&lt;/p&gt; &lt;p&gt;Kreuzberg is an open-source (MIT-licensed) polyglot document intelligence framework written in Rust, with bindings for Python, TypeScript/JavaScript (Node, Bun, and WASM), Ruby, Java, Go, PHP, Elixir, and C#. It’s also available as a CLI tool, Docker image, REST API server, and MCP server.&lt;/p&gt; &lt;p&gt;In practical terms, Kreuzberg helps you extract text, metadata, tables, and structured information from 75+ document and image formats, perform OCR, and prepare data for search, embeddings, or LLM pipelines. This kind of preprocessing step is necessary in many AI applications, document workflows, and data pipelines, where the quality of ingestion directly affects downstream results.&lt;/p&gt; &lt;p&gt;Comparative benchmarks: &lt;a href="https://kreuzberg.dev/benchmarks"&gt;https://kreuzberg.dev/benchmarks&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The new benchmarks compare Kreuzberg with several widely used document extraction tools, including Apache Tika, Docling, Unstructured, PDFPlumber, PyMuPDF4LLM, MarkItDown, and Mineru.&lt;/p&gt; &lt;p&gt;All benchmarks are executed automatically in GitHub Actions using a standardized Linux environment and a shared harness, so each framework is tested under the same conditions. We measure throughput, extraction duration, memory consumption, CPU usage, tail latencies, success rates, and extraction quality, both in single-file scenarios (latency and cold start) and batch processing scenarios (parallelism and throughput).&lt;/p&gt; &lt;p&gt;At a high level, the results show significantly higher throughput across common document types such as PDFs, DOCX, PPTX, and HTML. Processing times are often measured in milliseconds rather than seconds, cold start times are lower than most alternatives, and the installation footprint is smaller.&lt;/p&gt; &lt;p&gt;You can explore the benchmarks and download the raw results from the project pages if you want to take a deeper look.&lt;/p&gt; &lt;p&gt;What’s new in v4.3.0&lt;/p&gt; &lt;p&gt;Alongside the benchmarks, we’ve continued shipping improvements and fixes.&lt;/p&gt; &lt;p&gt;One of the biggest additions in this release is PaddleOCR support through a native Rust integration, with automatic model downloading and caching. This currently supports six languages: English, Chinese, Japanese, Korean, German, and French, and makes it easier to build pipelines that require high-quality OCR for Asian languages without leaving the Rust ecosystem.&lt;/p&gt; &lt;p&gt;We also added structured document data extraction, expanded format support, and removed LibreOffice as a dependency by introducing native extraction for legacy formats such as .doc and .ppt. Reducing external dependencies has been an ongoing focus for us because it simplifies deployment and reduces installation size, especially in containerized environments.&lt;/p&gt; &lt;p&gt;The full changelog is available here:&lt;br /&gt; &lt;a href="https://github.com/kreuzberg-dev/kreuzberg/blob/main/CHANGELOG.md"&gt;https://github.com/kreuzberg-dev/kreuzberg/blob/main/CHANGELOG.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Getting involved&lt;/p&gt; &lt;p&gt;Kreuzberg is an open-source project and contributions are always welcome!Thanks for reading, and we’d love to hear what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eastern-Surround7763"&gt; /u/Eastern-Surround7763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r59th1/kreuzberg_v430_and_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r59th1/kreuzberg_v430_and_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r59th1/kreuzberg_v430_and_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T08:49:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1r58ca8</id>
    <title>jdopensource/JoyAI-LLM-Flash • HuggingFace</title>
    <updated>2026-02-15T07:18:52+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r58ca8/jdopensourcejoyaillmflash_huggingface/"&gt; &lt;img alt="jdopensource/JoyAI-LLM-Flash • HuggingFace" src="https://preview.redd.it/vkpqjjqj4mjg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=ec8a0dbc966144fb6a5b31a4dec1781fb812a67e" title="jdopensource/JoyAI-LLM-Flash • HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vkpqjjqj4mjg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37e9ae1daf8fb794ef27f75590b6ad7557e0e326"&gt;https://preview.redd.it/vkpqjjqj4mjg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37e9ae1daf8fb794ef27f75590b6ad7557e0e326&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/jdopensource/JoyAI-LLM-Flash"&gt;https://huggingface.co/jdopensource/JoyAI-LLM-Flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kl2loe9c0mjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1b1437da4ce6468f7f9b580b3a7f88bb359f23e9"&gt;https://preview.redd.it/kl2loe9c0mjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1b1437da4ce6468f7f9b580b3a7f88bb359f23e9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r58ca8/jdopensourcejoyaillmflash_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r58ca8/jdopensourcejoyaillmflash_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r58ca8/jdopensourcejoyaillmflash_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T07:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r50ohq</id>
    <title>Qwen3 Coder Next Speedup with Latest Llama.cpp</title>
    <updated>2026-02-15T00:34:02+00:00</updated>
    <author>
      <name>/u/StardockEngineer</name>
      <uri>https://old.reddit.com/user/StardockEngineer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like it released just a few hours ago. Previously, I was getting 80ish tokens, max, on either of my GPUS in any combination.&lt;/p&gt; &lt;p&gt;Now I'm over 110+ in dual and 130+ on my RTX Pro&lt;/p&gt; &lt;p&gt;PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/19375"&gt;https://github.com/ggml-org/llama.cpp/pull/19375&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Update your llama.cpp.&lt;/p&gt; &lt;p&gt;Edit: This is for CUDA devices.&lt;/p&gt; &lt;p&gt;Previous: ``` ❯ llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0&lt;/p&gt; &lt;p&gt;ggml_cuda_init: found 2 CUDA devices: Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes | model | size | params | backend | ngl | n_ubatch | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | --------------: | -------------------: | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 | 2470.78 ± 3.84 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 | 87.35 ± 0.48 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 @ d500 | 2468.72 ± 23.27 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 @ d500 | 85.99 ± 0.53 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 @ d1000 | 2451.68 ± 19.96 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 @ d1000 | 87.15 ± 0.57 |&lt;/p&gt; &lt;p&gt;build: e06088da0 (7972) ```&lt;/p&gt; &lt;p&gt;New ``` ❯ llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0 &lt;/p&gt; &lt;p&gt;ggml_cuda_init: found 2 CUDA devices: Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes | model | size | params | backend | ngl | n_ubatch | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | --------------: | -------------------: | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 | 2770.34 ± 3.40 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 | 118.63 ± 1.14 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 @ d500 | 2769.27 ± 23.92 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 @ d500 | 119.69 ± 1.65 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 @ d1000 | 2753.07 ± 21.85 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 @ d1000 | 112.34 ± 0.74 |&lt;/p&gt; &lt;p&gt;build: 079feab9e (8055) ```&lt;/p&gt; &lt;p&gt;RTX by itself on new build ``` ❯ llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0 -dev CUDA1 ggml_cuda_init: found 2 CUDA devices: Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes | model | size | params | backend | ngl | n_ubatch | fa | dev | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | ------------ | --------------: | -------------------: | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | pp500 | 3563.60 ± 4.35 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | tg32 | 132.09 ± 1.07 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | pp500 @ d500 | 3481.63 ± 33.66 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | tg32 @ d500 | 119.57 ± 1.43 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | pp500 @ d1000 | 3534.69 ± 30.89 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | tg32 @ d1000 | 131.07 ± 7.27 |&lt;/p&gt; &lt;p&gt;build: 079feab9e (8055) ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StardockEngineer"&gt; /u/StardockEngineer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r50ohq/qwen3_coder_next_speedup_with_latest_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r50ohq/qwen3_coder_next_speedup_with_latest_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r50ohq/qwen3_coder_next_speedup_with_latest_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T00:34:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4sivv</id>
    <title>KaniTTS2 — open-source 400M TTS model with voice cloning, runs in 3GB VRAM. Pretrain code included.</title>
    <updated>2026-02-14T18:48:10+00:00</updated>
    <author>
      <name>/u/ylankgz</name>
      <uri>https://old.reddit.com/user/ylankgz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4sivv/kanitts2_opensource_400m_tts_model_with_voice/"&gt; &lt;img alt="KaniTTS2 — open-source 400M TTS model with voice cloning, runs in 3GB VRAM. Pretrain code included." src="https://external-preview.redd.it/ZDNyNTR0bGRhaWpnMXVXG3fU6KXmgjVT8aA3qyPzkraI-m8wnbRR2eD2luPt.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89937a54be408692a953dcd50857e8ea58cf59a4" title="KaniTTS2 — open-source 400M TTS model with voice cloning, runs in 3GB VRAM. Pretrain code included." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, we just open-sourced KaniTTS2 - a text-to-speech model designed for real-time conversational use cases.&lt;/p&gt; &lt;p&gt;## Models:&lt;/p&gt; &lt;p&gt;Multilingual (English, Spanish), and English-specific with local accents. Language support is actively expanding - more languages coming in future updates&lt;/p&gt; &lt;p&gt;## Specs&lt;/p&gt; &lt;p&gt;* 400M parameters (BF16)&lt;/p&gt; &lt;p&gt;* 22kHz sample rate&lt;/p&gt; &lt;p&gt;* Voice Cloning&lt;/p&gt; &lt;p&gt;* ~0.2 RTF on RTX 5090&lt;/p&gt; &lt;p&gt;* 3GB GPU VRAM&lt;/p&gt; &lt;p&gt;* Pretrained on ~10k hours of speech&lt;/p&gt; &lt;p&gt;* Training took 6 hours on 8x H100s&lt;/p&gt; &lt;p&gt;## Full pretrain code - train your own TTS from scratch&lt;/p&gt; &lt;p&gt;This is the part we’re most excited to share. We’re releasing the complete pretraining framework so anyone can train a TTS model for their own language, accent, or domain.&lt;/p&gt; &lt;p&gt;## Links&lt;/p&gt; &lt;p&gt;* Pretrained model: &lt;a href="https://huggingface.co/nineninesix/kani-tts-2-pt"&gt;https://huggingface.co/nineninesix/kani-tts-2-pt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* English model: &lt;a href="https://huggingface.co/nineninesix/kani-tts-2-en"&gt;https://huggingface.co/nineninesix/kani-tts-2-en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* Pretrain code: &lt;a href="https://github.com/nineninesix-ai/kani-tts-2-pretrain"&gt;https://github.com/nineninesix-ai/kani-tts-2-pretrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* HF Spaces: &lt;a href="https://huggingface.co/spaces/nineninesix/kani-tts-2-pt"&gt;https://huggingface.co/spaces/nineninesix/kani-tts-2-pt&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/nineninesix/kanitts-2-en"&gt;https://huggingface.co/spaces/nineninesix/kanitts-2-en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* License: Apache 2.0&lt;/p&gt; &lt;p&gt;Happy to answer any questions. Would love to see what people build with this, especially for underrepresented languages.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ylankgz"&gt; /u/ylankgz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/swybh9pdaijg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4sivv/kanitts2_opensource_400m_tts_model_with_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4sivv/kanitts2_opensource_400m_tts_model_with_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T18:48:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r569eb</id>
    <title>PSA: NVIDIA DGX Spark has terrible CUDA &amp; software compatibility; and seems like a handheld gaming chip.</title>
    <updated>2026-02-15T05:17:53+00:00</updated>
    <author>
      <name>/u/goldcakes</name>
      <uri>https://old.reddit.com/user/goldcakes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've spent the past week experimenting with the DGX Spark and I am about to return it. While I had understood the memory bandwidth and performance limitations, I like the CUDA ecosystem and was willing to pay the premium. Unfortunately, my experiences have been quite poor, and I suspect this is actually handheld gaming scraps that NVIDIA rushed to turn into a product to compete with Apple and Strix Halo.&lt;/p&gt; &lt;p&gt;The biggest issue: DGX Spark is not datacentre Blackwell, it's not even gaming Blackwell, it has its own special snowflake sm121 architecture. A lot of software do not work with it, or &lt;a href="https://github.com/triton-lang/triton/issues/8335#issuecomment-3417643519"&gt;have been patched to run sm80&lt;/a&gt; (Ampere, 6 years old!) codepaths which means it doesn't take advantage of blackwell optimisations.&lt;/p&gt; &lt;p&gt;When questioned about this on NVIDIA support forum, &lt;a href="https://forums.developer.nvidia.com/t/dgx-spark-sm121-software-support-is-severely-lacking-official-roadmap-needed/357663/9#p-1745639-h-1-when-will-sm121-receive-native-support-instead-of-sm80-fallbacks-10"&gt;an official NVIDIA representative said&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;sm80-class kernels can execute on DGX Spark because Tensor Core behavior is very similar, particularly for GEMM/MMAs (closer to the GeForce Ampere-style MMA model). &lt;strong&gt;DGX Spark not has tcgen05 like jetson Thor or GB200, due die space with RT Cores and DLSS algorithm&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Excuse me?? The reason we're getting cut-down tensor cores (not real blackwell) is because of RT Cores and &amp;quot;DLSS algorithm&amp;quot;? This is an AI dev kit; why would I need RT Cores, and additionally how does DLSS come into play? This makes me think they tried to turn a gaming handheld GPU (which needs/supports unified memory) into a poor competitor for a market they weren't prepared for.&lt;/p&gt; &lt;p&gt;In addition, in the same post the rep posted what appears to be LLM hallucinations, mentioning issues have been fixed in version numbers and releases for software libraries that &lt;em&gt;do not exist&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;Just be careful when buying a DGX Spark. You are not really getting a modern CUDA experience. Yes, everything works fine if you pretend you only have an Ampere, but attempting to use any Blackwell features is an exercise in futility.&lt;/p&gt; &lt;p&gt;Additionally, for something that is supposed to be ready 'out of the box', many people (including myself and servethehome) reports basic issues like &lt;strong&gt;HDMI display output&lt;/strong&gt;. I originally thought my Spark was DOA; nope; it just refuses to work with my 1080p144 viewsonic (which works with all other GPUs; including my NVIDIA ones); and had to switch to my 4K60 monitor. Dear NVIDIA, you should not have basic display output issues...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goldcakes"&gt; /u/goldcakes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T05:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t775</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2026-02-13T16:07:54+00:00</updated>
    <author>
      <name>/u/HardToVary</name>
      <uri>https://old.reddit.com/user/HardToVary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt; &lt;img alt="AMA with MiniMax — Ask Us Anything!" src="https://preview.redd.it/5z2li1ntcajg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=ce3340cd37b7e9408878509be00dd7b871efebde" title="AMA with MiniMax — Ask Us Anything!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;We're &lt;strong&gt;MiniMax&lt;/strong&gt;, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;.5&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining the channel today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — Founder of MiniMax&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Wise_Evidence9973"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ryan85127704"&gt;u/ryan85127704&lt;/a&gt; — Head of Engineering&lt;/li&gt; &lt;li&gt;&lt;a href="/u/HardToVary"&gt;u/HardToVary&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c"&gt;https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. We'll continue monitoring and responding to questions for 48 hours after the end of the AMA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HardToVary"&gt; /u/HardToVary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:07:54+00:00</published>
  </entry>
</feed>
