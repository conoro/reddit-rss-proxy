<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-30T19:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pzdw3h</id>
    <title>reko ‚Äì Local-first YouTube-to-Markdown summarizer with small local LLMs</title>
    <updated>2025-12-30T09:35:48+00:00</updated>
    <author>
      <name>/u/Rikifire</name>
      <uri>https://old.reddit.com/user/Rikifire</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a local-first tool to summarize YouTube videos into clean Markdown using transcripts + LLMs.&lt;/p&gt; &lt;p&gt;The default setup uses Ollama with small local models. Other local or cloud providers are supported too if preferred.&lt;/p&gt; &lt;p&gt;The Python app is the core engine. The main interface is a CLI (useful for scripting and automation), and I also added a small localhost web UI to speed up session-based workflows (paste a link, tweak settings, get rendered Markdown). Everything runs locally.&lt;/p&gt; &lt;p&gt;I‚Äôd really appreciate feedback from this community on model choice, output quality improvement, and anything that could improve the local-first workflow.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/riccardoruspoli/reko"&gt;https://github.com/riccardoruspoli/reko&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rikifire"&gt; /u/Rikifire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzdw3h/reko_localfirst_youtubetomarkdown_summarizer_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzdw3h/reko_localfirst_youtubetomarkdown_summarizer_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzdw3h/reko_localfirst_youtubetomarkdown_summarizer_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T09:35:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzko2m</id>
    <title>Following up on my PPO derivation ‚Äì I worked through DPO (Direct Preference Optimization) from first principles</title>
    <updated>2025-12-30T15:16:25+00:00</updated>
    <author>
      <name>/u/garg-aayush</name>
      <uri>https://old.reddit.com/user/garg-aayush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzko2m/following_up_on_my_ppo_derivation_i_worked/"&gt; &lt;img alt="Following up on my PPO derivation ‚Äì I worked through DPO (Direct Preference Optimization) from first principles" src="https://external-preview.redd.it/2FdgwWh6OLrWAMyYTlKigdz-lf8oVF8ucnUDu_IxS-w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ad567987e74e4da6f90b50234fe7cc0a31dcb1a" title="Following up on my PPO derivation ‚Äì I worked through DPO (Direct Preference Optimization) from first principles" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week I shared my attempt at deriving the PPO loss from scratch. Naturally, I also derived DPO as a follow-up. &lt;/p&gt; &lt;p&gt;After grinding through PPO‚Äôs multi-component objective (clipped surrogate, value function, entropy bonus, KL penalty) which I honestly found a bit complex, DPO is radically simple and elegant. &lt;/p&gt; &lt;p&gt;It is a computationally lightweight approach that directly optimizes LLMs to align with human preferences without explicit reward modeling or reinforcement learning. DPO implicitly optimizes the same objective as PPO-based RLHF (reward maximization with a KL-divergence constraint) but replaces the entire reward model + PPO loop with a single supervised objective on preference pairs. &lt;/p&gt; &lt;p&gt;No separate reward model training. No RL sampling loop. No PPO clipping gymnastics. Just gradient descent on your preference dataset.&lt;/p&gt; &lt;p&gt;If you worked through my PPO post and found it dense, I think you‚Äôll find this one more approachable. The math is cleaner and the derivation more linear.&lt;/p&gt; &lt;p&gt;Blog post here: &lt;a href="https://huggingface.co/blog/garg-aayush/derive-dpo-loss"&gt;https://huggingface.co/blog/garg-aayush/derive-dpo-loss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As always, happy to discuss or get corrections if I‚Äôve messed something up. And big thanks again to Umar Jamil, his DPO video was invaluable for building intuition before diving into the paper.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/garg-aayush"&gt; /u/garg-aayush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/garg-aayush/derive-dpo-loss"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzko2m/following_up_on_my_ppo_derivation_i_worked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzko2m/following_up_on_my_ppo_derivation_i_worked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T15:16:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzl5bv</id>
    <title>SXM2 (V100) vs PCIe (RTX 4000) for huge 800GB models - Is NVLink critical for memory pooling?</title>
    <updated>2025-12-30T15:35:39+00:00</updated>
    <author>
      <name>/u/Sad_Ninja_3717</name>
      <uri>https://old.reddit.com/user/Sad_Ninja_3717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I am building a local inference server for my company to run a massive multi-agent AI setup (approx. 800GB model size). I need to convince my boss that investing in the SXM2 platform is worth the extra cost compared to a standard PCIe setup.&lt;/p&gt; &lt;p&gt;We have two options on the table:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option A (Budget):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; 2x Xeon Platinum 8268&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 2 TB DDR4&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; 8x Quadro RTX 4000 8GB (Total 64GB VRAM) - &lt;strong&gt;PCIe Gen3&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Concern:&lt;/em&gt; Slow communication between cards and CPU, strictly P2P limitation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Option B (Premium - SXM2):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; 2x Xeon Platinum 8280L&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 2 TB DDR4&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; 8x Tesla V100 32GB (Total 256GB VRAM) - &lt;strong&gt;SXM2 with NVLink&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My specific question:&lt;/strong&gt; Since the model is huge (800GB) and will heavily rely on system RAM offloading (CPU &amp;lt;-&amp;gt; GPU transfer), will the &lt;strong&gt;SXM2/NVLink&lt;/strong&gt; architecture provide a massive performance boost over PCIe Gen3?&lt;/p&gt; &lt;p&gt;I understand that Ubuntu will still see 8 separate devices, but I am counting on &lt;strong&gt;CUDA Unified Memory&lt;/strong&gt; and NVLink to treat the VRAM as a closer-to-unified pool and handle the bandwidth much better than the PCIe bottleneck.&lt;/p&gt; &lt;p&gt;Is the performance jump in a split RAM/VRAM scenario significant enough to justify the higher price for a business use case?&lt;/p&gt; &lt;p&gt;Thanks for your help!&lt;/p&gt; &lt;p&gt;Robert&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_Ninja_3717"&gt; /u/Sad_Ninja_3717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzl5bv/sxm2_v100_vs_pcie_rtx_4000_for_huge_800gb_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzl5bv/sxm2_v100_vs_pcie_rtx_4000_for_huge_800gb_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzl5bv/sxm2_v100_vs_pcie_rtx_4000_for_huge_800gb_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T15:35:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz67hm</id>
    <title>5 new korean models will be released in 2 hours</title>
    <updated>2025-12-30T02:42:37+00:00</updated>
    <author>
      <name>/u/Specialist-2193</name>
      <uri>https://old.reddit.com/user/Specialist-2193</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/live/fLBh97ls--Q?si=Ql8JOjXXVoSA7ura"&gt;https://www.youtube.com/live/fLBh97ls--Q?si=Ql8JOjXXVoSA7ura&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Naver, LG, SK, NC, Upstage&lt;/p&gt; &lt;p&gt;All 5 models will be released in 2 to 3 hours. Follow with the YouTube link&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist-2193"&gt; /u/Specialist-2193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T02:42:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzd0j7</id>
    <title>Has anyone built a RAG on WikiLeaks?</title>
    <updated>2025-12-30T08:41:14+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because that would be a useful application. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzl1dy</id>
    <title>CPU inference</title>
    <updated>2025-12-30T15:31:23+00:00</updated>
    <author>
      <name>/u/Time_Dust_2303</name>
      <uri>https://old.reddit.com/user/Time_Dust_2303</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear community, any suggestions for an open-source LLM which could be used on a CPU for inference?&lt;/p&gt; &lt;p&gt;Have tried qwen 0.6B, but it ain't that good.&lt;/p&gt; &lt;p&gt;Any suggestions?&lt;/p&gt; &lt;p&gt;Edit: specs CPU -- consumer grade (i5/i7), RAM -- 8GB DDR4&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Time_Dust_2303"&gt; /u/Time_Dust_2303 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzl1dy/cpu_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzl1dy/cpu_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzl1dy/cpu_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T15:31:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzr54u</id>
    <title>Is there a decent model that is capable of detecting a language from an audio file? It needs to be able to differentiate between language variations, e.g. Latin American vs European Spanish.</title>
    <updated>2025-12-30T19:22:30+00:00</updated>
    <author>
      <name>/u/nmkd</name>
      <uri>https://old.reddit.com/user/nmkd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to find an automated way to tell which variation of a language a movie's audio track is.&lt;/p&gt; &lt;p&gt;For example, French can be Canadian, or European (Quebec/Parisian) French.&lt;/p&gt; &lt;p&gt;Similarly, Spanish can be Latin American or European (Catalan). Same deal with Portuguese (Portugal/Brazil variations).&lt;/p&gt; &lt;p&gt;However those audio tracks are usually just tagged French/Spanish, with no sure way to know which variation is actually is, unless you speak the language or send a sample to someone who does.&lt;/p&gt; &lt;p&gt;Is there a model that can do this locally? Preferably something that runs alright on an RTX 4090. I do have 64 GB DDR5-6000 so offloading would work too if the slowdown isn't terrible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nmkd"&gt; /u/nmkd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzr54u/is_there_a_decent_model_that_is_capable_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzr54u/is_there_a_decent_model_that_is_capable_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzr54u/is_there_a_decent_model_that_is_capable_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T19:22:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzhpg1</id>
    <title>What has been your experience with Diffusion LLM‚Äôs vs Autoregressive?</title>
    <updated>2025-12-30T13:08:38+00:00</updated>
    <author>
      <name>/u/InceptionAI_Tom</name>
      <uri>https://old.reddit.com/user/InceptionAI_Tom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most LLMs people use today (GPT, Claude, Gemini, etc.) share the same core assumption,Generate one token at a time, left to right.&lt;/p&gt; &lt;p&gt;That‚Äôs the autoregressive setup. It works insanely well, but it bakes in a couple of structural issues:&lt;/p&gt; &lt;p&gt;‚Ä¢ Latency: You must go token ‚Üí token ‚Üí token. Even with parallelism in the stack, the generation step itself is serialized.&lt;/p&gt; &lt;p&gt;‚Ä¢ Cost: If you need 200‚Äì500 tokens of output, you‚Äôre doing 200‚Äì500 forward passes over some slice of the context. It adds up quickly.&lt;/p&gt; &lt;p&gt;‚Ä¢ UX ceiling: For many interactive use cases, especially code and UI-embedded assistants, 1‚Äì3s latency is already too slow. On the other side, there‚Äôs a very different approach that‚Äôs getting less attention outside research circles: diffusion language models.&lt;/p&gt; &lt;p&gt;Instead of ‚Äúwrite the next word,‚Äù you:&lt;/p&gt; &lt;p&gt;Start with a noisy guess of the entire answer (sequence). Refine the whole sequence in a fixed number of steps, updating multiple tokens in parallel. You pay a fixed number of refinement steps rather than ‚Äúone step per token.‚Äù At small/medium scales we‚Äôve seen:&lt;/p&gt; &lt;p&gt;‚Ä¢ Similar quality to speed-optimized autoregressive models (Claude Haiku, Gemini Flash) with 5-10x improvements in latency)‚Ä¶&lt;/p&gt; &lt;p&gt;‚Ä¢ ‚Ä¶with order-of-magnitude improvements in latency, because you can exploit parallelism the hardware already wants to give you (GPUs/TPUs). This is especially interesting for:&lt;/p&gt; &lt;p&gt;‚Ä¢ Low-latency applications (code autocomplete, inline helpers, agents inside products).&lt;/p&gt; &lt;p&gt;‚Ä¢ High-volume workloads where shaving 5‚Äì10x off inference cost matters more than squeezing out the last benchmark point. Obviously, diffusion LLMs aren‚Äôt free lunch:&lt;/p&gt; &lt;p&gt;‚Ä¢ Training is more complex.&lt;/p&gt; &lt;p&gt;‚Ä¢ You need careful sequence representations and noise schedules for text.&lt;/p&gt; &lt;p&gt;‚Ä¢ Tooling and serving infra are optimized for autoregressive LLMs&lt;/p&gt; &lt;p&gt;But from where I sit (working with a team that builds and deploys diffusion-based language models), it feels like the field has massively path-dependent bias toward autoregression because it was easier to train and deploy first, not necessarily because it‚Äôs the end state.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InceptionAI_Tom"&gt; /u/InceptionAI_Tom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhpg1/what_has_been_your_experience_with_diffusion_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhpg1/what_has_been_your_experience_with_diffusion_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhpg1/what_has_been_your_experience_with_diffusion_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T13:08:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzaz73</id>
    <title>[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the "Thinking Process" (Hidden States).</title>
    <updated>2025-12-30T06:40:32+00:00</updated>
    <author>
      <name>/u/JB_King1919</name>
      <uri>https://old.reddit.com/user/JB_King1919</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt; &lt;img alt="[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the &amp;quot;Thinking Process&amp;quot; (Hidden States)." src="https://b.thumbs.redditmedia.com/Bpxf3FjdwheVRpbXrcXJxisJg0Hd6sGLSEAe6P2x7Fg.jpg" title="[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the &amp;quot;Thinking Process&amp;quot; (Hidden States)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm a PhD student in &lt;strong&gt;Electromagnetics&lt;/strong&gt;. In my daily work, I deal with fields, waves, and trajectories. When I started playing with Local LLMs, I felt something was missing: we usually look at the &lt;em&gt;output&lt;/em&gt; text or the &lt;em&gt;loss curves&lt;/em&gt;, but we rarely see &lt;strong&gt;how&lt;/strong&gt; the model gets from A to B.&lt;/p&gt; &lt;p&gt;To an RF engineer, reasoning isn't just a probability distribution‚Äîit's a &lt;strong&gt;dynamic flow&lt;/strong&gt; through a high-dimensional space.&lt;/p&gt; &lt;p&gt;So, I built a lightweight Python toolkit to extract hidden states layer-by-layer and visualize them as continuous &lt;strong&gt;2D/3D trajectories&lt;/strong&gt;. I wanted to see if &amp;quot;thoughts&amp;quot; have a geometric shape.&lt;/p&gt; &lt;p&gt;The results were surprisingly consistent. I‚Äôm sharing the tool so you can run it on your own models (Llama, Qwen, Mistral, etc.).&lt;/p&gt; &lt;h1&gt;1. The &amp;quot;Confidence Funnel&amp;quot; (Convergence)&lt;/h1&gt; &lt;p&gt;I found that if you feed the model slightly different prompts about the same concept (e.g., &amp;quot;Define Justice&amp;quot;, &amp;quot;What is Fairness&amp;quot;), the internal states start far apart but &lt;strong&gt;physically collapse&lt;/strong&gt; into a single &amp;quot;attractor basin&amp;quot; as the layers get deeper.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ockr11ldcaag1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2eb1f34a4e014bcd85d8ba77b6e95fdb1fba422c"&gt;https://preview.redd.it/ockr11ldcaag1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2eb1f34a4e014bcd85d8ba77b6e95fdb1fba422c&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Practical Use:&lt;/strong&gt; You can use this to test &lt;strong&gt;Prompt Stability&lt;/strong&gt;. If the funnel is tight, the model is sure. If it sprays out at the end, the model is confused or hallucinating.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Llama-3 vs. Qwen-2.5: Different &amp;quot;Thinking Styles&amp;quot;&lt;/h1&gt; &lt;p&gt;This was the coolest find. When I ran the same prompts through different architectures, the &amp;quot;shape&amp;quot; of their thinking was totally different.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d6kdjcifcaag1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bab8f3499bbd2b69481d5f24faefb7773c585df8"&gt;https://preview.redd.it/d6kdjcifcaag1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bab8f3499bbd2b69481d5f24faefb7773c585df8&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Llama-3 (Left):&lt;/strong&gt; Seems to &amp;quot;decide&amp;quot; on the semantics very early (Layers 5-10). The trajectory is direct.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen-2.5 (Right):&lt;/strong&gt; Keeps the trajectory expanded (in superposition?) until the very last layers (Layer 20+). It seems to &amp;quot;hold&amp;quot; the ambiguity much longer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; This might give us a geometric way to profile model behaviors beyond just benchmarks.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Visualizing &amp;quot;Refusal&amp;quot; (The Safety Spike)&lt;/h1&gt; &lt;p&gt;I was curious what RLHF looks like geometrically. I visualized the trajectory when the model refuses a jailbreak versus when it follows a safe instruction.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k1cq3ehjcaag1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70f269d5357171735646780298a877604dd80aca"&gt;https://preview.redd.it/k1cq3ehjcaag1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70f269d5357171735646780298a877604dd80aca&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hard Refusal(Red):&lt;/strong&gt; Looks like a particle hitting a brick wall‚Äîa sharp, high-curvature spike.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Soft Steering(Green):&lt;/strong&gt; Looks like a smooth turn. And an obvious &amp;quot;U-turn&amp;quot; at the end of its trajectory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Practical Use:&lt;/strong&gt; A visual &amp;quot;Geiger Counter&amp;quot; for safety tuning. You can see if your system prompt is creating a hard wall or a soft guide.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üì• The Toolkit&lt;/h1&gt; &lt;p&gt;I packaged this into a Python library with example scripts. It works with local HuggingFace weights (no API needed).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/JBKing514/map_llm_toolkit"&gt;LLM Toolkit&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üß† The Theory (Optional)&lt;/h1&gt; &lt;p&gt;I‚Äôm not an AI researcher, but I wrote up some notes on the &lt;strong&gt;manifold dynamics&lt;/strong&gt; perspective behind this tool (treating inference as a Langevin flow). If you are interested in the math/physics intuition behind these visualizations or need more info about my experiment setup, I put up a page and my notes here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Project Page &amp;amp; Math:&lt;/strong&gt; &lt;a href="https://jbking514.github.io/map_blog/"&gt;Project GitHub Page&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Foundational Notes:&lt;/strong&gt; &lt;a href="https://zenodo.org/records/17900444"&gt;Manifold Alignment Protocol (MAP)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to see what &lt;strong&gt;Mistral&lt;/strong&gt; or &lt;strong&gt;Gemma&lt;/strong&gt; trajectories look like if anyone runs this. Let me know what you find!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JB_King1919"&gt; /u/JB_King1919 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T06:40:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pze13o</id>
    <title>Exploring a 1.58-bit / ternary LLM core inspired by BitNet (CUDA attention, GTX 1050 tests)</title>
    <updated>2025-12-30T09:44:36+00:00</updated>
    <author>
      <name>/u/HuseyinKama</name>
      <uri>https://old.reddit.com/user/HuseyinKama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been experimenting with extreme low-bit LLM inference inspired by the BitNet 1.58-bit paper,&lt;/p&gt; &lt;p&gt;and wanted to share a research-style project I‚Äôve been working on over the last few weeks.&lt;/p&gt; &lt;p&gt;This is NOT a production-ready model, but rather an exploration of how far ternary / sparse logic&lt;/p&gt; &lt;p&gt;can be pushed on consumer GPUs.&lt;/p&gt; &lt;p&gt;What this project explores:&lt;/p&gt; &lt;p&gt;- A custom LLM core using ternary weights {-1, 0, +1}&lt;/p&gt; &lt;p&gt;- Trainable via Straight-Through Estimator (STE)&lt;/p&gt; &lt;p&gt;- Custom CUDA attention kernel (thresholded / shifted-ReLU instead of softmax)&lt;/p&gt; &lt;p&gt;- Designed for local inference (tested on GTX 1050)&lt;/p&gt; &lt;p&gt;Core ideas:&lt;/p&gt; &lt;p&gt;- Replace FP16-heavy matmul layers with ternary linear layers&lt;/p&gt; &lt;p&gt;- Abs-mean scaling (BitNet-style quantization)&lt;/p&gt; &lt;p&gt;- Focus on reducing interference via sparsity rather than magnitude precision&lt;/p&gt; &lt;p&gt;- Attention without softmax to reduce compute and improve stability in low-bit regimes&lt;/p&gt; &lt;p&gt;Current results:&lt;/p&gt; &lt;p&gt;- End-to-end training works&lt;/p&gt; &lt;p&gt;- Overfitting tests succeed (Python training ‚Üí CUDA inference consistency)&lt;/p&gt; &lt;p&gt;- Character-level Shakespeare training produces coherent output&lt;/p&gt; &lt;p&gt;- Memory footprint is significantly reduced compared to FP16 baselines&lt;/p&gt; &lt;p&gt;Limitations / open problems:&lt;/p&gt; &lt;p&gt;- Not competitive with large FP16/INT8 models (expected)&lt;/p&gt; &lt;p&gt;- Sensitive to threshold and temperature tuning&lt;/p&gt; &lt;p&gt;- No advanced optimizations like FlashAttention&lt;/p&gt; &lt;p&gt;- Very much a research prototype&lt;/p&gt; &lt;p&gt;I‚Äôm mainly sharing this to get feedback from people who:&lt;/p&gt; &lt;p&gt;- Have worked with BitNet / ternary networks&lt;/p&gt; &lt;p&gt;- Experiment with custom CUDA kernels&lt;/p&gt; &lt;p&gt;- Care about local / low-power LLM inference&lt;/p&gt; &lt;p&gt;Code and CUDA kernels are available here for anyone curious:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/QKV-Core/Trion"&gt;https://github.com/QKV-Core/Trion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer technical questions or discuss design tradeoffs.&lt;/p&gt; &lt;p&gt;EDIT / Clarification:&lt;/p&gt; &lt;p&gt;This is not a commercial project, not a startup pitch, and not a benchmark claim.&lt;/p&gt; &lt;p&gt;I‚Äôm sharing this as an experimental research / engineering exploration inspired by the BitNet 1.58-bit paper.&lt;/p&gt; &lt;p&gt;The goal is to understand how far ternary + sparse computation can go on consumer hardware.&lt;/p&gt; &lt;p&gt;No paid product, no token, no API, no funding.&lt;/p&gt; &lt;p&gt;Just code, CUDA kernels, and learning.&lt;/p&gt; &lt;p&gt;Feedback and criticism are very welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HuseyinKama"&gt; /u/HuseyinKama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pze13o/exploring_a_158bit_ternary_llm_core_inspired_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pze13o/exploring_a_158bit_ternary_llm_core_inspired_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pze13o/exploring_a_158bit_ternary_llm_core_inspired_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T09:44:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz7mxr</id>
    <title>Llama-3.3-8B-Instruct</title>
    <updated>2025-12-30T03:49:11+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not sure if this is real, but the author provides a fascinating story behind its acquisition. I would like for it to be real!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Bartowski GGUFs: &lt;a href="https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T03:49:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcdu1</id>
    <title>LG K EXAONE 236b</title>
    <updated>2025-12-30T08:02:08+00:00</updated>
    <author>
      <name>/u/Specialist-2193</name>
      <uri>https://old.reddit.com/user/Specialist-2193</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"&gt; &lt;img alt="LG K EXAONE 236b" src="https://preview.redd.it/1wirc918taag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fcab270f79f71dba1d330db0ee8e85422de763b" title="LG K EXAONE 236b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will be released in few days &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist-2193"&gt; /u/Specialist-2193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1wirc918taag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcj1q</id>
    <title>Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy</title>
    <updated>2025-12-30T08:11:09+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"&gt; &lt;img alt="Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy" src="https://b.thumbs.redditmedia.com/d_jApTNkEXlNvJcoJA6qryuDnUo0ni-DFWBY6RTdAfg.jpg" title="Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/collections/tencent/hy-mt15"&gt;https://huggingface.co/collections/tencent/hy-mt15&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights: üîπ 1.8B On-Device Power: Optimized for consumer hardware with a 1GB memory footprint. Using on-policy distillation to align with larger models, it delivers 0.18s latency (50 tokens), outperforming mainstream commercial APIs. üîπ 7B SOTA Performance: An upgraded version of our WMT25 champion, surpassing mid-sized open-source models and rivaling the 90th percentile of closed-source giants like Gemini-3.0-Pro. üîπ 33+ Languages &amp;amp; Dialects: High-fidelity translation across 33 languages and 5 Chinese dialects. üîπ Production-Ready: Native support for custom terminology, long-dialogue context, and maintaining document formatting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pzcj1q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:11:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzfuqg</id>
    <title>Why Kimi K2 Thinking choose Int4 QAT, from infra enginner of KImi</title>
    <updated>2025-12-30T11:33:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/"&gt; &lt;img alt="Why Kimi K2 Thinking choose Int4 QAT, from infra enginner of KImi" src="https://b.thumbs.redditmedia.com/6XCL91-lLSt5Lf0kxdky6Te-XoW5kEkEa9zxhgmAQGg.jpg" title="Why Kimi K2 Thinking choose Int4 QAT, from infra enginner of KImi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw the recent &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"&gt;discussion&lt;/a&gt; here regarding MiniMax engineer's tweet about why they decided &lt;em&gt;against&lt;/em&gt; using int4 QAT for the MiniMax M2.1 model.&lt;/p&gt; &lt;p&gt;Interestingly, at the time of the K2 Thinking release, a Kimi infra engineer posted a deep dive on Zhihu explaining why native int4 QAT was actually crucial for them. I‚Äôve summarized the key takeaways below to offer a different perspective on the 'to quant or not to quant' debate.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Kimi found int4 QAT is essential for &lt;strong&gt;MoE latency&lt;/strong&gt;, &lt;strong&gt;long-context stability&lt;/strong&gt;, and &lt;strong&gt;speeding up the RL training loop&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Decoding is Memory-Bound (Latency Focus)&lt;/h1&gt; &lt;p&gt;Unlike the MiniMax case, Kimi found that for their specific MoE architecture (which is highly sparse), the decoding phase is almost exclusively memory-bound. By using W4A16 (4-bit weights, 16-bit activations), they reduced memory usage significantly. This allowed the model to fit on fewer GPUs, which reduced inter-device communication overhead, a major factor in lowering end-to-end latency for users.&lt;/p&gt; &lt;h1&gt;PTQ Failed at &amp;quot;Thinking&amp;quot; Lengths&lt;/h1&gt; &lt;p&gt;The team initially tried standard Post-Training Quantization (PTQ). While it worked for short responses, it fell apart for the long chain-of-thought &amp;quot;thinking&amp;quot; process. As generation length increased, quantization errors accumulated, leading to degradation. Furthermore, PTQ struggled with sparse experts; if an expert wasn't hit frequently during the calibration step with the calibration dataset, it essentially &amp;quot;forgot&amp;quot; knowledge. QAT (Quantization Aware Training) was necessary to make the model &amp;quot;lossless&amp;quot; compared to the BF16 baseline.&lt;/p&gt; &lt;h1&gt;A less discussed benefit: Faster RL Training&lt;/h1&gt; &lt;p&gt;This is the point that often gets overlooked: Int4 QAT wasn't just for inference serving, it accelerated the training process itself. In Reinforcement Learning, the model spends a massive amount of time in the &amp;quot;rollout&amp;quot; phase (generating text). By using the Int4 model for these rollouts, they reduced the total time for an RL iteration by 10-20%. It also reduced the discrepancy between the training forward pass and the inference engine.&lt;/p&gt; &lt;h1&gt;Why Int4 and not FP4?&lt;/h1&gt; &lt;p&gt;They chose standard Int4 over newer formats like FP4 to maintain compatibility with existing hardware (non-Blackwell GPUs) and to utilize mature, highly efficient kernels like Marlin.&lt;/p&gt; &lt;p&gt;In summary, I believe there isn't a one-size-fits-all answer regarding quantization. It depends heavily on the model's parameters and specific architecture. It is a matter of trade-offs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dzmceu5zybag1.png?width=1362&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0ba8f78c6e5ade3463a1c62fba1d338a1c01ce9"&gt; AI translation, there may be some translation errors.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T11:33:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz68fz</id>
    <title>Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.</title>
    <updated>2025-12-30T02:43:48+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"&gt; &lt;img alt="Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market." src="https://preview.redd.it/ocq43c2a79ag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e9f7477bee69f806ab9bab82c73557ea1345393" title="Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ocq43c2a79ag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T02:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzmbqy</id>
    <title>Anyone else basically just use this hobby as an excuse to try and run LLMs on the jankiest hardware you possibly can?</title>
    <updated>2025-12-30T16:21:25+00:00</updated>
    <author>
      <name>/u/kevin_1994</name>
      <uri>https://old.reddit.com/user/kevin_1994</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I find it so addicting to take some old random hardware, install llama.cpp on it, and try to do something useful with it.&lt;/p&gt; &lt;p&gt;Examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I found an old gaming laptop from 2017 with 7GB (?) DDR4 and a GTX 1050 (3GB). I'm running Granite 4-h tiny on it (9ba1b MoE model) at q6 with 20 tg/s and 100 pp/s. I'm using this model to generate tags, titles, etc. on Open-WebUI&lt;/li&gt; &lt;li&gt;I run reranker model (qwen3 reranker 4b) on my raspberry pi 5&lt;/li&gt; &lt;li&gt;I run my backup FIM coding model (qwen 2.5 coder 1.5B q8) my steam deck (which I never use for gaming anymore, lmao) at around 100 tg/s 1000 pp/s on vulkan&lt;/li&gt; &lt;li&gt;My original setup was an old BTC-S37 mining motherboard (2 core, 3 Ghz, 8GB DDR4 SODIMM) with 4xRTX 3060 I found on fb marketplace and an old 2kW mining PSU which ran Qwen3 32b Q8 around 20 tok/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ideas:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I really want to buy a AMD-4700S (defective ps5) board and see if the LPDDR5 memory bandwidth leads to ok inference performance&lt;/li&gt; &lt;li&gt;My experience with steam deck makes me think maybe modded nintendo switch would work relatively ok, since it has an nvidia gpu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Anyone else do this shit?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kevin_1994"&gt; /u/kevin_1994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzmbqy/anyone_else_basically_just_use_this_hobby_as_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzmbqy/anyone_else_basically_just_use_this_hobby_as_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzmbqy/anyone_else_basically_just_use_this_hobby_as_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T16:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzg32r</id>
    <title>Solar 100B claimed that it counts better than GPT today</title>
    <updated>2025-12-30T11:46:20+00:00</updated>
    <author>
      <name>/u/Icy_Company_6216</name>
      <uri>https://old.reddit.com/user/Icy_Company_6216</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzg32r/solar_100b_claimed_that_it_counts_better_than_gpt/"&gt; &lt;img alt="Solar 100B claimed that it counts better than GPT today" src="https://preview.redd.it/kxyfw9z2xbag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7132ef64860af64198c058381a4037b3b7c69818" title="Solar 100B claimed that it counts better than GPT today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Company_6216"&gt; /u/Icy_Company_6216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kxyfw9z2xbag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzg32r/solar_100b_claimed_that_it_counts_better_than_gpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzg32r/solar_100b_claimed_that_it_counts_better_than_gpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T11:46:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzmwzh</id>
    <title>I benchmarked 26 local + cloud Speech-to-Text models on long-form medical dialogue and ranked them + open-sourced the full eval</title>
    <updated>2025-12-30T16:43:59+00:00</updated>
    <author>
      <name>/u/MajesticAd2862</name>
      <uri>https://old.reddit.com/user/MajesticAd2862</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzmwzh/i_benchmarked_26_local_cloud_speechtotext_models/"&gt; &lt;img alt="I benchmarked 26 local + cloud Speech-to-Text models on long-form medical dialogue and ranked them + open-sourced the full eval" src="https://preview.redd.it/gz5z65l1edag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d4544711be1730cb9627cfc8aabdc74db18a410" title="I benchmarked 26 local + cloud Speech-to-Text models on long-form medical dialogue and ranked them + open-sourced the full eval" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I‚Äôm building a fully local AI-Scribe for clinicians and just pushed an end-of-year refresh of our medical dialogue STT benchmark.&lt;/p&gt; &lt;p&gt;I ran &lt;strong&gt;26 open + closed source STT models&lt;/strong&gt; on &lt;strong&gt;PriMock57&lt;/strong&gt; (55 files, 81,236 words) and ranked them by &lt;strong&gt;average WER&lt;/strong&gt;. I also logged &lt;strong&gt;avg seconds per file&lt;/strong&gt; and noted when models required chunking due to repetition loops or failures.&lt;/p&gt; &lt;p&gt;Full eval code, runners, and the complete leaderboard are on GitHub (I‚Äôll drop the link in the comments).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;PriMock57 (55 files used) ‚Ä¢ Updated: 2025-12-24&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Top 10 (55 files)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Rank&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;WER&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Avg sec/file&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Host&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;Google Gemini 2.5 Pro&lt;/td&gt; &lt;td align="left"&gt;10.79%&lt;/td&gt; &lt;td align="left"&gt;56.4s&lt;/td&gt; &lt;td align="left"&gt;API (Google)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;Google Gemini 3 Pro Preview*&lt;/td&gt; &lt;td align="left"&gt;11.03%&lt;/td&gt; &lt;td align="left"&gt;64.5s&lt;/td&gt; &lt;td align="left"&gt;API (Google)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;Parakeet TDT 0.6B v3&lt;/td&gt; &lt;td align="left"&gt;11.90%&lt;/td&gt; &lt;td align="left"&gt;6.3s&lt;/td&gt; &lt;td align="left"&gt;Local (M4, MLX)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;Google Gemini 2.5 Flash&lt;/td&gt; &lt;td align="left"&gt;12.08%&lt;/td&gt; &lt;td align="left"&gt;20.2s&lt;/td&gt; &lt;td align="left"&gt;API (Google)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;OpenAI GPT-4o Mini (2025-12-15)&lt;/td&gt; &lt;td align="left"&gt;12.82%&lt;/td&gt; &lt;td align="left"&gt;40.5s&lt;/td&gt; &lt;td align="left"&gt;API (OpenAI)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;Parakeet TDT 0.6B v2&lt;/td&gt; &lt;td align="left"&gt;13.26%&lt;/td&gt; &lt;td align="left"&gt;5.4s&lt;/td&gt; &lt;td align="left"&gt;Local (M4, MLX)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;ElevenLabs Scribe v1&lt;/td&gt; &lt;td align="left"&gt;13.54%&lt;/td&gt; &lt;td align="left"&gt;36.3s&lt;/td&gt; &lt;td align="left"&gt;API (ElevenLabs)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;Kyutai STT 2.6B&lt;/td&gt; &lt;td align="left"&gt;13.79%&lt;/td&gt; &lt;td align="left"&gt;148.4s&lt;/td&gt; &lt;td align="left"&gt;Local (L4 GPU)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9&lt;/td&gt; &lt;td align="left"&gt;Google Gemini 3 Flash Preview&lt;/td&gt; &lt;td align="left"&gt;13.88%&lt;/td&gt; &lt;td align="left"&gt;51.5s&lt;/td&gt; &lt;td align="left"&gt;API (Google)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;MLX Whisper Large v3 Turbo&lt;/td&gt; &lt;td align="left"&gt;14.22%&lt;/td&gt; &lt;td align="left"&gt;12.9s&lt;/td&gt; &lt;td align="left"&gt;Local (M4, MLX)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;* 54/55 files evaluated (1 blocked by safety filter)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key findings&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gemini 2.5 Pro leads at ~10.8% WER, with Gemini 3 Pro Preview close behind&lt;/li&gt; &lt;li&gt;Parakeet v3 is the new local champion at 11.9% WER and ~6s/file on M4&lt;/li&gt; &lt;li&gt;GPT-4o Mini improved a lot with the Dec 15 update (15.9% ‚Üí 12.8%), now #5 overall&lt;/li&gt; &lt;li&gt;Google MedASR came dead last (64.9% WER) and looks tuned for dictation, not dialogue&lt;/li&gt; &lt;li&gt;We saw repetition-loop failure modes in Canary 1B v2, Granite Speech, and Kyutai; chunking with overlap helps&lt;/li&gt; &lt;li&gt;Groq Whisper-v3 (turbo) still looks like the best cloud price/latency balance&lt;/li&gt; &lt;li&gt;Apple SpeechAnalyzer remains a solid Swift-native option (14.8% WER)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full leaderboard (26 models) + notes (incl. MedASR and repetition-loop cases) are in the repo. Blog link with interpretation is also in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MajesticAd2862"&gt; /u/MajesticAd2862 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gz5z65l1edag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzmwzh/i_benchmarked_26_local_cloud_speechtotext_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzmwzh/i_benchmarked_26_local_cloud_speechtotext_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T16:43:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzggbf</id>
    <title>Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware ‚Äì Full Optimization Guide</title>
    <updated>2025-12-30T12:05:52+00:00</updated>
    <author>
      <name>/u/at0mi</name>
      <uri>https://old.reddit.com/user/at0mi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzggbf/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/"&gt; &lt;img alt="Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware ‚Äì Full Optimization Guide" src="https://preview.redd.it/2eimvrgo0cag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a97232a34737346670be6cb9292a1cdde03aa47a" title="Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware ‚Äì Full Optimization Guide" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ! If you're passionate about squeezing every last bit of performance out of older hardware for local large language models, I've got something exciting to share. I managed to get GLM-4.7 ‚Äì that's the massive 355B parameter Mixture of Experts model ‚Äì running in Q8_0 quantization on a seriously vintage setup: a 2015 Lenovo System x3950 X6 with eight Xeon E7-8880 v3 CPUs (no GPU in sight, just pure CPU inference). After a bunch of trial and error, I'm hitting around 5-6 tokens per second, which is pretty respectable for such an ancient beast. The Q8 quantization delivers extremely high quality outputs, preserving nearly all the model's intelligence with minimal degradation ‚Äì it's practically indistinguishable from full precision for most tasks.&lt;/p&gt; &lt;p&gt;The key was optimizing everything from BIOS settings (like enabling hyper-threading and tweaking power management) to NUMA node distribution for better memory access, and experimenting with different llama.cpp forks to handle the MoE architecture efficiently. I also dove into Linux kernel tweaks, like adjusting CPU governors and hugepages, to minimize latency. Keep in mind, this setup draws about 1300W AC under full load, so it's power-hungry but worth it for local runs. Benchmarks show solid performance for generation tasks, though it's not blazing fast ‚Äì perfect for homelab enthusiasts or those without access to modern GPUs.&lt;/p&gt; &lt;p&gt;I documented the entire process chronologically in this blog post, including step-by-step setup, code snippets, potential pitfalls, and full performance metrics: &lt;a href="https://postl.ai/2025/12/29/glm47on3950x6/?referrer=grok.com"&gt;https://postl.ai/2025/12/29/glm47on3950x6/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone else tried pushing big MoE models like this on CPU-only rigs? What optimizations worked for you, or what models are you running on similar hardware? Let's discuss!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/at0mi"&gt; /u/at0mi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2eimvrgo0cag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzggbf/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzggbf/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T12:05:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz7bmv</id>
    <title>Llama-3.3-8B-Instruct</title>
    <updated>2025-12-30T03:34:19+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"&gt; &lt;img alt="Llama-3.3-8B-Instruct" src="https://external-preview.redd.it/F-RvVhAB2x8ac9OzOxDw905YUEWDIOQBeDMa2ZyMwo4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a109a6917bc66847cb36c61990f58523049b666" title="Llama-3.3-8B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;from &lt;strong&gt;allura-forge&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct#llama-33-8b-instruct"&gt;&lt;/a&gt;&lt;strong&gt;Llama 3.3 8B Instruct&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Yes, this is official, and yes, this is, to my knowledge, a real version of Llama 3.3 8B. (I think, anyways)&lt;/p&gt; &lt;p&gt;Facebook has a &lt;a href="https://llama.developer.meta.com"&gt;Llama API&lt;/a&gt; available that allows for inference of the other Llama models (L3.3 70B, L4 Scout and Maverick), but &lt;em&gt;also&lt;/em&gt; includes a special, new (according to the original press release) &amp;quot;Llama 3.3 8B&amp;quot; that didn't exist anywhere else and was stuck behind the Facebook API!&lt;/p&gt; &lt;p&gt;However. The Llama API supports finetuning L3.3... &lt;em&gt;and downloading the final model in HF format.&lt;/em&gt; Problem solved, right?&lt;/p&gt; &lt;p&gt;Wellllllllllllllll. Not really. The finetuning API was hidden behind layers of support tickets. I tried when the original API dropped in April, and was just told &amp;quot;We'll think about it and send you any updates&amp;quot; (there never were any updates).&lt;/p&gt; &lt;p&gt;Flash forward to December, on a whim I decide to look at the API again. And... by god... the finetuning tab was there. I could click on it and start a job (please ignore that I have no idea how it works, and in fact the finetuning tab actually disappeared after the first time I clicked on it, though I could still manually go to the page).&lt;/p&gt; &lt;p&gt;Apparently, this was not very well tested, as there were a good few bugs, the UI was janky, and the download model function did not actually work due to CORS (I had to manually curl things to get the CDN link).&lt;/p&gt; &lt;p&gt;But... by god... the zip file downloaded, and I had my slightly finetuned model.&lt;/p&gt; &lt;p&gt;To my shock and delight, however, they also provide the adapter that they merged into the model. That means I can &lt;em&gt;subtract&lt;/em&gt; that adapter and get the original model. And... here we are!&lt;/p&gt; &lt;p&gt;(actually, it should be ‚Äúnew model,‚Äù but I used ‚Äúother‚Äù to avoid triggering people)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T03:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzlx9w</id>
    <title>How llama.cpp implements 2.9x faster top-k sampling with bucket sort</title>
    <updated>2025-12-30T16:05:58+00:00</updated>
    <author>
      <name>/u/noninertialframe96</name>
      <uri>https://old.reddit.com/user/noninertialframe96</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzlx9w/how_llamacpp_implements_29x_faster_topk_sampling/"&gt; &lt;img alt="How llama.cpp implements 2.9x faster top-k sampling with bucket sort" src="https://external-preview.redd.it/CZbcx8kLlfmOhgEbqLeuBvYqHwSKzwQAaRIJW4yJFv8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6426b29c1a766614bad4ab7965774390635e0620" title="How llama.cpp implements 2.9x faster top-k sampling with bucket sort" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I looked into how llama.cpp optimizes top-k sampling, and the trick is surprisingly simple.&lt;/p&gt; &lt;p&gt;Top-k on Llama 3's 128K vocabulary means finding k highest scores out of 128,256 candidates. std::partial_sort does this at O(n log k), but llama.cpp noticed that token logits cluster in a narrow range (-10 to +10).&lt;/p&gt; &lt;p&gt;So instead of sorting, it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Builds a 128-bucket histogram over the logit range&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Walks from the highest bucket down until it accumulates k tokens&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Only sorts those survivors&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noninertialframe96"&gt; /u/noninertialframe96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://codepointer.substack.com/p/llamacpp-accelerate-top-k-sampling"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzlx9w/how_llamacpp_implements_29x_faster_topk_sampling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzlx9w/how_llamacpp_implements_29x_faster_topk_sampling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T16:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcrtb</id>
    <title>Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model</title>
    <updated>2025-12-30T08:26:06+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"&gt; &lt;img alt="Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model" src="https://preview.redd.it/yq8uriwhxaag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d990cf5383783b3e2aa22351ddeb29ebac5eb2b2" title="Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are excited to open-source Tencent HY-Motion 1.0, a billion-parameter text-to-motion model built on the Diffusion Transformer (DiT) architecture and flow matching. Tencent HY-Motion 1.0 empowers developers and individual creators alike by transforming natural language into high-fidelity, fluid, and diverse 3D character animations, delivering exceptional instruction-following capabilities across a broad range of categories. The generated 3D animation assets can be seamlessly integrated into typical 3D animation pipelines.&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;p&gt;üîπBillion-Scale DiT: Successfully scaled flow-matching DiT to 1B+ parameters, setting a new ceiling for instruction-following capability and generated motion quality.&lt;/p&gt; &lt;p&gt;üîπFull-Stage Training Strategy: The industry‚Äôs first motion generation model featuring a complete Pre-training ‚Üí SFT ‚Üí RL loop to optimize physical plausibility and semantic accuracy.&lt;/p&gt; &lt;p&gt;üîπComprehensive Category Coverage: Features 200+ motion categories across 6 major classes‚Äîthe most comprehensive in the industry, curated via a meticulous data pipeline.&lt;/p&gt; &lt;p&gt;üåêProject Page: &lt;a href="https://hunyuan.tencent.com/motion"&gt;https://hunyuan.tencent.com/motion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîóGithub: &lt;a href="https://github.com/Tencent-Hunyuan/HY-Motion-1.0"&gt;https://github.com/Tencent-Hunyuan/HY-Motion-1.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§óHugging Face: &lt;a href="https://huggingface.co/tencent/HY-Motion-1.0"&gt;https://huggingface.co/tencent/HY-Motion-1.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìÑTechnical report: &lt;a href="https://arxiv.org/pdf/2512.23464"&gt;https://arxiv.org/pdf/2512.23464&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yq8uriwhxaag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzhcqu</id>
    <title>Any guesses?</title>
    <updated>2025-12-30T12:52:15+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"&gt; &lt;img alt="Any guesses?" src="https://preview.redd.it/xqvj95zv8cag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30044fbca7ba499223943c95d7d236600fdbb10e" title="Any guesses?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xqvj95zv8cag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T12:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
