<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-19T17:24:13+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p1coup</id>
    <title>MacOS 26.2 to add full 'Neural Accelerator' support for M5 chips</title>
    <updated>2025-11-19T16:43:38+00:00</updated>
    <author>
      <name>/u/PracticlySpeaking</name>
      <uri>https://old.reddit.com/user/PracticlySpeaking</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No more need for the custom MLX patch in the Weinbach anrticle. Also clustering at 80Gb/s over Tb5.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.engadget.com/ai/you-can-turn-a-cluster-of-macs-into-an-ai-supercomputer-in-macos-tahoe-262-191500778.html"&gt;https://www.engadget.com/ai/you-can-turn-a-cluster-of-macs-into-an-ai-supercomputer-in-macos-tahoe-262-191500778.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PracticlySpeaking"&gt; /u/PracticlySpeaking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1coup/macos_262_to_add_full_neural_accelerator_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1coup/macos_262_to_add_full_neural_accelerator_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1coup/macos_262_to_add_full_neural_accelerator_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T16:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0jr1f</id>
    <title>Mistral removing ton of old models from API (preparing for a new launch?)</title>
    <updated>2025-11-18T18:28:50+00:00</updated>
    <author>
      <name>/u/mpasila</name>
      <uri>https://old.reddit.com/user/mpasila</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0jr1f/mistral_removing_ton_of_old_models_from_api/"&gt; &lt;img alt="Mistral removing ton of old models from API (preparing for a new launch?)" src="https://preview.redd.it/tg4zaa7b622g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=879c9f3922693c16a694f6bce7604bb1dd61da54" title="Mistral removing ton of old models from API (preparing for a new launch?)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They are going to be removing 9 (screenshot is missing one) models from their API at the end of this month. So I wonder if that means they are preparing to release something early December? I sure hope I finally get Nemo 2.0 or something... (it's been over a year since that released).&lt;br /&gt; Source: &lt;a href="https://docs.mistral.ai/getting-started/models#legacy-models"&gt;https://docs.mistral.ai/getting-started/models#legacy-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mpasila"&gt; /u/mpasila &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tg4zaa7b622g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0jr1f/mistral_removing_ton_of_old_models_from_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0jr1f/mistral_removing_ton_of_old_models_from_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T18:28:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1anwi</id>
    <title>RamaLama strives to make working with AI simple, straightforward, and familiar by using OCI containers.</title>
    <updated>2025-11-19T15:28:43+00:00</updated>
    <author>
      <name>/u/autodidacticasaurus</name>
      <uri>https://old.reddit.com/user/autodidacticasaurus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1anwi/ramalama_strives_to_make_working_with_ai_simple/"&gt; &lt;img alt="RamaLama strives to make working with AI simple, straightforward, and familiar by using OCI containers." src="https://external-preview.redd.it/Vy3A11YYoT7GF2K1mf33E4_0fJuNt7SW0a58HiNtw5c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d9a8e2a60de8c4d7d50e0ff91a24b4d2e2e4060" title="RamaLama strives to make working with AI simple, straightforward, and familiar by using OCI containers." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/autodidacticasaurus"&gt; /u/autodidacticasaurus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/containers/ramalama"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1anwi/ramalama_strives_to_make_working_with_ai_simple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1anwi/ramalama_strives_to_make_working_with_ai_simple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1brbk</id>
    <title>Best Courses for learning LLM</title>
    <updated>2025-11-19T16:09:41+00:00</updated>
    <author>
      <name>/u/PersonSuitTV</name>
      <uri>https://old.reddit.com/user/PersonSuitTV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been playing with local LLMs for a while, and really want to get serious with it. I would like to take a course or something to really amp up my ability but I am not sure where to start. Has anyone taken any classes or can recommend a video course or anything? Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PersonSuitTV"&gt; /u/PersonSuitTV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1brbk/best_courses_for_learning_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1brbk/best_courses_for_learning_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1brbk/best_courses_for_learning_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T16:09:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1p15qcm</id>
    <title>Is 64GB vs 128GB ($700) Worth It?</title>
    <updated>2025-11-19T11:55:26+00:00</updated>
    <author>
      <name>/u/PersonSuitTV</name>
      <uri>https://old.reddit.com/user/PersonSuitTV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just picked up an M4 Max Mac Studio with 64GB Unified memory. It runs great but I obviously can’t run models like gpt-oss-120b. Considering the bump to 128GB is an additional $700, is it honestly worth it to upgrade? Any feedback is greatly appreciated.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1p15qcm"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PersonSuitTV"&gt; /u/PersonSuitTV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p15qcm/is_64gb_vs_128gb_700_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p15qcm/is_64gb_vs_128gb_700_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p15qcm/is_64gb_vs_128gb_700_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T11:55:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1dkzh</id>
    <title>You're using HuggingFace wrong. Stop downloading pre-quantized GGUFs and start building hardware-optimized, domain-specific models. Here's the pipeline I built to do it.</title>
    <updated>2025-11-19T17:16:18+00:00</updated>
    <author>
      <name>/u/badgerbadgerbadgerWI</name>
      <uri>https://old.reddit.com/user/badgerbadgerbadgerWI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1dkzh/youre_using_huggingface_wrong_stop_downloading/"&gt; &lt;img alt="You're using HuggingFace wrong. Stop downloading pre-quantized GGUFs and start building hardware-optimized, domain-specific models. Here's the pipeline I built to do it." src="https://a.thumbs.redditmedia.com/fec-cvcGsXrZCdhq4EiRvOc2FqscazNKFZZd3l7_k80.jpg" title="You're using HuggingFace wrong. Stop downloading pre-quantized GGUFs and start building hardware-optimized, domain-specific models. Here's the pipeline I built to do it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Downloading TheBloke's Q4_K_M and calling it a day is lazy and you're leaving massive performance on the table. I built &lt;a href="https://github.com/llama-farm/LlamaPajamas"&gt;LlamaPajamas&lt;/a&gt; (experimental / open-source), a pipeline that downloads full-precision models, converts them to the optimal format for your specific hardware (CoreML/TensorRT/ONNX for vision/SST, MLX/GGUF/TensorRT-LLM for LLMs), and then applies importance quantization with domain-specific calibration data. An 8B model quantized for YOUR use case beats a 70B general-purpose model for YOUR task. Also discovered most quantization benchmarks are lying to you.&lt;/p&gt; &lt;h1&gt;The problem with how everyone uses HuggingFace&lt;/h1&gt; &lt;p&gt;Go to any LocalLlama thread. &amp;quot;What model should I download?&amp;quot; And everyone recommends some pre-quantized GGUF.&lt;/p&gt; &lt;p&gt;That's fine for playing around. It's completely wrong for production or for real workloads.&lt;/p&gt; &lt;p&gt;Here's what you're doing when you download a pre-quantized model:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Someone else decided which quantization format to use&lt;/li&gt; &lt;li&gt;Someone else decided which calibration data to use (usually generic web text)&lt;/li&gt; &lt;li&gt;Someone else decided which weights to preserve and which to compress&lt;/li&gt; &lt;li&gt;You have no idea if any of those decisions match your use case&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You're running a model that was optimized for nobody in particular on hardware it wasn't optimized for.&lt;/p&gt; &lt;p&gt;And then you wonder why your local setup feels worse than the APIs.&lt;/p&gt; &lt;h1&gt;The approach that actually works&lt;/h1&gt; &lt;p&gt;Download the full-precision model. Do your own conversion. Do your own quantization with your own calibration data.&lt;/p&gt; &lt;p&gt;Yes, it takes more time. Yes, it requires understanding what you're doing. But you end up with a model that's actually optimized for your hardware and your task instead of some generic middle ground.&lt;/p&gt; &lt;p&gt;That's what LlamaPajamas does. It's the pipeline for doing this properly.&lt;/p&gt; &lt;h1&gt;Different model types need completely different backends&lt;/h1&gt; &lt;p&gt;This is where most people screw up. They treat all AI models the same. &amp;quot;Just convert it to GGUF and run it.&amp;quot;&lt;/p&gt; &lt;p&gt;No. Different architectures run best on completely different backends.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Vision and Speech models (Whisper, YOLO, ViT, CLIP)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;These are mostly matrix multiplications and convolutions. They're well-suited for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CoreML&lt;/strong&gt; on Apple Silicon → Uses the Neural Engine and GPU properly. Whisper-tiny runs in 2 seconds for a 1-minute clip on M1 Max.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TensorRT&lt;/strong&gt; on NVIDIA → Graph optimization and tensor cores. YOLO inference at 87ms per frame.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ONNX&lt;/strong&gt; for CPU/AMD → Portable, runs everywhere, good enough performance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You probably know this, but Do NOT run vision models through GGUF or MLX. That's not what those backends are for and they really don't support it (yet).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Large Language Models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;LLMs have different compute patterns. Attention mechanisms, KV caches, sequential token generation. They need:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MLX&lt;/strong&gt; on Apple Silicon → Apple's ML framework built for LLMs on M-series chips. Way better than CoreML for text generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GGUF&lt;/strong&gt; for CPU/universal → llama.cpp's format. Works everywhere, highly optimized for CPU inference, and this is where you do importance quantization.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TensorRT-LLM&lt;/strong&gt; on NVIDIA → Not regular TensorRT. TensorRT-LLM is specifically optimized for autoregressive generation, KV caching, and batched inference on NVIDIA GPUs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Notice that CoreML isn't in the LLM list. CoreML is great for vision but it's not designed for the sequential generation pattern of LLMs. MLX is what you want on Apple Silicon for text.&lt;/p&gt; &lt;p&gt;Similarly, regular TensorRT is great for vision but you need TensorRT-LLM for language models. Different optimization strategies entirely.&lt;/p&gt; &lt;h1&gt;The quantization stack: format first, then hyper-compress&lt;/h1&gt; &lt;p&gt;Once you've got the right backend format, then you quantize. And for LLMs, you should be going way more aggressive than Q4_K_M.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The GGUF quantization ladder:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Format&lt;/th&gt; &lt;th align="left"&gt;Compression&lt;/th&gt; &lt;th align="left"&gt;Use Case&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;F16&lt;/td&gt; &lt;td align="left"&gt;1x&lt;/td&gt; &lt;td align="left"&gt;Baseline, too big for most uses&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;2x&lt;/td&gt; &lt;td align="left"&gt;Overkill for most tasks&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;4x&lt;/td&gt; &lt;td align="left"&gt;Where most people stop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;5x&lt;/td&gt; &lt;td align="left"&gt;Where you should start looking&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_XS&lt;/td&gt; &lt;td align="left"&gt;6x&lt;/td&gt; &lt;td align="left"&gt;Sweet spot for most use cases&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_XS&lt;/td&gt; &lt;td align="left"&gt;8x&lt;/td&gt; &lt;td align="left"&gt;Aggressive but works with good calibration&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Most people stop at Q4_K_M because that's what the pre-quantized downloads offer. You're missing the whole point.&lt;/p&gt; &lt;p&gt;IQ (importance quantization) uses calibration data to figure out which weights matter. Generic calibration preserves weights that matter for generic tasks. Domain-specific calibration preserves weights that matter for YOUR task.&lt;/p&gt; &lt;h1&gt;Domain-specific calibration changes everything&lt;/h1&gt; &lt;p&gt;This is the core insight that most people miss.&lt;/p&gt; &lt;p&gt;We created 7 calibration datasets:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Domain&lt;/th&gt; &lt;th align="left"&gt;Use Case&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;General&lt;/td&gt; &lt;td align="left"&gt;Multi-purpose balanced&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tool Calling&lt;/td&gt; &lt;td align="left"&gt;Function/API calling&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Summarization&lt;/td&gt; &lt;td align="left"&gt;Text compression&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAG&lt;/td&gt; &lt;td align="left"&gt;Document Q&amp;amp;A&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Medical&lt;/td&gt; &lt;td align="left"&gt;Healthcare/diagnosis&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Military&lt;/td&gt; &lt;td align="left"&gt;Defense/tactical&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tone Analysis&lt;/td&gt; &lt;td align="left"&gt;Sentiment/emotion&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Real results&lt;/strong&gt;: A medical model quantized with medical calibration data maintains 95%+ task accuracy at IQ3_XS (900MB). The same model with general calibration drops to 85%.&lt;/p&gt; &lt;p&gt;That's 10% accuracy difference from calibration data alone at the same file size.&lt;/p&gt; &lt;p&gt;A well-calibrated IQ3_XS model for your specific domain will outperform a generic Q4_K_M for your task. Smaller file, better performance. That's not magic, that's just optimizing for what you actually care about instead of what some random person on the internet cared about.&lt;/p&gt; &lt;h1&gt;The calibration lesson that cost us&lt;/h1&gt; &lt;p&gt;We built all these calibration datasets and felt good about ourselves. Then tool_calling quantization completely failed.&lt;/p&gt; &lt;p&gt;Turns out llama-imatrix needs at least 4,096 tokens to generate a useful importance matrix. Our tool_calling dataset only had 1,650 tokens.&lt;/p&gt; &lt;p&gt;Had to rebuild everything. Medical prompts went from &amp;quot;diagnose chest pain&amp;quot; to full clinical scenarios with differential diagnosis, test ordering, and treatment plans. Each calibration file needs to hit that token threshold or your importance matrix is garbage.&lt;/p&gt; &lt;p&gt;Check your token counts before running quantization. Learned this the hard way.&lt;/p&gt; &lt;h1&gt;Your evaluation is lying to you&lt;/h1&gt; &lt;p&gt;LlamaPajamas has a built-in evaluation tool - the first time I did it completely wrong (a lesson I am sure many have run into).&lt;/p&gt; &lt;p&gt;We were running evaluations and getting 90%+ accuracy on quantized models. Great! Ship it!&lt;/p&gt; &lt;p&gt;The evaluation was garbage.&lt;/p&gt; &lt;p&gt;Our &amp;quot;lenient mode&amp;quot; accepted any answer containing the right letter. Correct answer is &amp;quot;A&amp;quot;? We'd accept:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;A&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;A.&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;A) Because the mitochondria is the powerhouse of the cell&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;The answer is A&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In production, most of those are WRONG. If your system expects &amp;quot;A&amp;quot; and gets &amp;quot;A) Because...&amp;quot;, that's a parsing failure.&lt;/p&gt; &lt;p&gt;We built strict mode. Exact matches only.&lt;/p&gt; &lt;p&gt;Accuracy dropped from 90% to ~50%.&lt;/p&gt; &lt;p&gt;That's the truth. That's what your model actually does. The 90% number was a lie that made us feel good.&lt;/p&gt; &lt;p&gt;We also built category-specific prompts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Math: &amp;quot;Answer with ONLY the number. No units. No explanations.&amp;quot;&lt;/li&gt; &lt;li&gt;Multiple choice: &amp;quot;Answer with ONLY the letter. No punctuation.&amp;quot;&lt;/li&gt; &lt;li&gt;Tool calling: &amp;quot;Output ONLY the function name.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're not evaluating with strict exact-match, you don't know what your model can actually do, expecially in an agentic / tool calling world.&lt;/p&gt; &lt;h1&gt;Handling thinking models&lt;/h1&gt; &lt;p&gt;Some models output reasoning in &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tags:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;think&amp;gt; The question asks about cellular respiration which is option B &amp;lt;/think&amp;gt; B &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Our regex broke when outputs got truncated mid-tag. Fixed it with two-pass extraction: remove complete tags first, then clean up unclosed tags.&lt;/p&gt; &lt;p&gt;Thinking models can reason all they want internally but still need exact final answers.&lt;/p&gt; &lt;h1&gt;Actual benchmark results&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Vision (YOLO-v8n)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CoreML FP16: 6.2MB, 87ms per frame on M1 (m laptop)&lt;/li&gt; &lt;li&gt;TensorRT FP16: 6MB, 45ms per frame on RTX 3090&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Speech (Whisper-Tiny)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CoreML INT8: 39MB, 2.1s for 1-minute audio&lt;/li&gt; &lt;li&gt;ONNX: 39MB, 3.8s same audio on CPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;LLM (Qwen3 1.7B)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Format&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Strict Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;F16 baseline&lt;/td&gt; &lt;td align="left"&gt;3.8 GB&lt;/td&gt; &lt;td align="left"&gt;78%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;1.2 GB&lt;/td&gt; &lt;td align="left"&gt;75%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_XS (general)&lt;/td&gt; &lt;td align="left"&gt;900 MB&lt;/td&gt; &lt;td align="left"&gt;73%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_XS (domain)&lt;/td&gt; &lt;td align="left"&gt;900 MB&lt;/td&gt; &lt;td align="left"&gt;76% on domain tasks&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_XS&lt;/td&gt; &lt;td align="left"&gt;700 MB&lt;/td&gt; &lt;td align="left"&gt;68%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The sweet spot is IQ3_XS with domain calibration. You get 6x compression with minimal accuracy loss on your target task. For 8B models that's 15GB down to 2.5GB.&lt;/p&gt; &lt;h1&gt;How to use the pipeline&lt;/h1&gt; &lt;p&gt;Install:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/llama-farm/llama-pajamas cd llama-pajamas curl -LsSf https://astral.sh/uv/install.sh | sh ./setup.sh &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Download full model and convert to GGUF F16:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd quant uv run llama-pajamas-quant quantize \ --model Qwen/Qwen3-1.7B\ --format gguf \ --precision F16 \ --output ./models/qwen3-1.7b &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;IQ quantize with your domain calibration:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv run llama-pajamas-quant iq quantize \ --model ./models/qwen3-1.7b/gguf/F16/model.gguf \ --domain medical \ --precision IQ3_XS \ --output ./models/qwen3-1.7b-medical-iq3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Evaluate with strict mode (no lying to yourself):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv run llama-pajamas-quant evaluate llm \ --model-dir ./models/qwen3-1.7b-medical-iq3/*.gguf \ --num-questions 140 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Convert vision model to CoreML:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv run llama-pajamas-quant quantize \ --model yolov8n \ --format coreml \ --precision fp16 \ --output ./models/yolo-coreml &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;What we're building next&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Automatic calibration generation&lt;/strong&gt;: Describe your use case, get calibration data generated automatically.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quality prediction&lt;/strong&gt;: Estimate accuracy at different quantization levels before running the full process.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mobile export&lt;/strong&gt;: Direct to CoreML for iOS, TFLite for Android.&lt;/p&gt; &lt;h1&gt;The caveat: general-use GGUFs have their place&lt;/h1&gt; &lt;p&gt;Look, there are a lot of great pre-quantized GGUFs out there. TheBloke did great work. Bartowski's quants are solid. For playing around with different models and getting a feel for what's out there, they're fine.&lt;/p&gt; &lt;p&gt;But here's my question: why are you running models locally for &amp;quot;general use&amp;quot;?&lt;/p&gt; &lt;p&gt;If you just want a general-purpose assistant, use Claude or ChatGPT. They're better at it than any local model and you don't have to manage infrastructure.&lt;/p&gt; &lt;p&gt;The reason to run locally is privacy, offline access, or specialization. And if you need privacy or offline access, you probably have a specific use case. And if you have a specific use case, you should be fine-tuning and using domain-specific iMatrix quantization to turn your model into a specialist.&lt;/p&gt; &lt;p&gt;A 3B model fine-tuned on your data and quantized with your calibration will destroy a generic 8B model for your task. Smaller, faster, better. That's the whole point.&lt;/p&gt; &lt;p&gt;Stop downloading generic quants and hoping they work for your use case. Download full models, fine-tune if you can, and quantize with calibration data that matches what you're actually trying to do.&lt;/p&gt; &lt;p&gt;That's how you get local AI that actually competes with the APIs.&lt;/p&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/llama-farm/LlamaPajamas"&gt;https://github.com/llama-farm/LlamaPajamas&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about hardware-specific optimization, calibration data design, or why your current evaluation is probably lying to you. Learn more about what we are building at &lt;a href="/r/LlamaFarm"&gt;r/LlamaFarm&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;P.S.&lt;br /&gt; Why LlamaPajamas - you shouldn't just make pajamas 1 size fits all, they need to be specialized for the hardware (the animal). Plus my daughter and son love the book :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cujx94sly82g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aae93c60959c3ab33c74a1c09c931f5a8bd599c7"&gt;https://preview.redd.it/cujx94sly82g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aae93c60959c3ab33c74a1c09c931f5a8bd599c7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badgerbadgerbadgerWI"&gt; /u/badgerbadgerbadgerWI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1dkzh/youre_using_huggingface_wrong_stop_downloading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1dkzh/youre_using_huggingface_wrong_stop_downloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1dkzh/youre_using_huggingface_wrong_stop_downloading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T17:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1724h</id>
    <title>tried a memory system for local llama and its kinda interesting</title>
    <updated>2025-11-19T13:00:33+00:00</updated>
    <author>
      <name>/u/Independent_Plum_489</name>
      <uri>https://old.reddit.com/user/Independent_Plum_489</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Llama 3 8B (Q4 quant) on my 3090 with ollama for coding assistant stuff. Been hitting issues with longer sessions where it loses track of earlier context.&lt;/p&gt; &lt;p&gt;Tried summarization but it loses too much detail. Tried vector DB with embeddings but didnt work well for my use case. I know I could use RoPE scaling to extend context but that gets slow and eats VRAM fast on my setup.&lt;/p&gt; &lt;p&gt;The problem isnt just context size, its that the model doesnt actually maintain state between turns.&lt;/p&gt; &lt;p&gt;Found this memory system called EverMemOS on github couple days ago. Took like 3 hours to get it working because the docs are kinda sparse. Had to mess with the config to get it running with ollama.&lt;/p&gt; &lt;p&gt;The approach is different from RAG. Instead of retrieving chunks, it does some kind of state management. Not totally sure how it works under the hood, something about maintaining context differently.&lt;/p&gt; &lt;p&gt;Did some testing. Base setup starts losing coherence around turn 15-20 in my tests (pretty long responses each turn, like 200-300 tokens). With the memory system went to turn 40+ and it still tracked earlier context. VRAM usage went up about 3GB. Base Llama 3 8B Q4 uses ~6GB, with the memory system it goes to ~9GB.&lt;/p&gt; &lt;p&gt;Whats interesting is it seems to understand references better. Like if I mention something from turn 8 at turn 35, it actually gets it. Not perfect tho, still messes up sometimes.&lt;/p&gt; &lt;p&gt;Been running it for 2 days now. Not sure if its worth the extra VRAM for my use case but the approach seems promising.&lt;/p&gt; &lt;p&gt;Code is here if anyone wants to try: github.com/EverMind-AI/EverMemOS&lt;/p&gt; &lt;p&gt;Setup is a bit annoying tho. You need decent VRAM, probably 10GB+ depending on your model size. Gotta configure it manually. Says it works with different backends but I only tried it with ollama.&lt;/p&gt; &lt;p&gt;Has anyone tried memory systems vs just extending context? Wondering if theres better approaches out there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Plum_489"&gt; /u/Independent_Plum_489 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1724h/tried_a_memory_system_for_local_llama_and_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1724h/tried_a_memory_system_for_local_llama_and_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1724h/tried_a_memory_system_for_local_llama_and_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T13:00:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p12jz3</id>
    <title>RAM prices exploding should I grab old stock now for rag?</title>
    <updated>2025-11-19T08:56:21+00:00</updated>
    <author>
      <name>/u/Working_Opposite4167</name>
      <uri>https://old.reddit.com/user/Working_Opposite4167</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need some advice.&lt;/p&gt; &lt;p&gt;I have 32GB RAM in my PC right now, but since it’s my work machine I usually have around 10GB free. I’m also running an RTX 3090.&lt;/p&gt; &lt;p&gt;I want to build RAG setups for two AI projects. I found a 96GB DDR5 6000MHz kit that’s still being sold for the &lt;em&gt;old price&lt;/em&gt; (~520$), and the store told me RAM prices are about to spike because the market is going crazy.&lt;/p&gt; &lt;p&gt;The idea is that if I buy the 96GB, I’ll probably sell my current 32GB kit.&lt;/p&gt; &lt;p&gt;My dilemma:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I &lt;em&gt;can&lt;/em&gt; rely on the OpenAI API and avoid running big models locally.&lt;/li&gt; &lt;li&gt;But I’m scared the API costs will pile up over time and end up costing more than just buying the RAM once.&lt;/li&gt; &lt;li&gt;On the other hand, maybe I don’t even need so much RAM if I mostly stick to OpenAI.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I’m torn:&lt;br /&gt; Should I buy the 96GB now before prices jump?&lt;br /&gt; Or skip it and just rely on the API, even though long-term costs worry me?&lt;/p&gt; &lt;p&gt;Anyone with experience running local models or using OpenAI heavily your advice would help a lot. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Working_Opposite4167"&gt; /u/Working_Opposite4167 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p12jz3/ram_prices_exploding_should_i_grab_old_stock_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p12jz3/ram_prices_exploding_should_i_grab_old_stock_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p12jz3/ram_prices_exploding_should_i_grab_old_stock_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T08:56:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1p11od1</id>
    <title>What are the most unique models that are under 15b you encountered</title>
    <updated>2025-11-19T08:00:40+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not talking about nsfw, I know remember people claiming some models have personality, I would like to see what models you have encountered that were unique and fun to chat with.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p11od1/what_are_the_most_unique_models_that_are_under/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p11od1/what_are_the_most_unique_models_that_are_under/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p11od1/what_are_the_most_unique_models_that_are_under/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T08:00:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1p16kxx</id>
    <title>Why do you use open-source LLMs ?</title>
    <updated>2025-11-19T12:37:58+00:00</updated>
    <author>
      <name>/u/MrTorgue7</name>
      <uri>https://old.reddit.com/user/MrTorgue7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, &lt;/p&gt; &lt;p&gt;I’ve been following the progress of AI closely since AlphaGo (crazy how much progress we’ve had since then !), reading papers and such, but I’ve never really experimented in the technical side of things.&lt;/p&gt; &lt;p&gt;After stumbling on this sub, I’m really curious. Why do you guys do it ? What do you find fun about it ? What’s your use cases ?&lt;/p&gt; &lt;p&gt;Thanks for your answers :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrTorgue7"&gt; /u/MrTorgue7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p16kxx/why_do_you_use_opensource_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p16kxx/why_do_you_use_opensource_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p16kxx/why_do_you_use_opensource_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T12:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p15wbk</id>
    <title>[Release] DragonMemory: 16× semantic compression for local RAG context (open-source, AGPL)</title>
    <updated>2025-11-19T12:03:57+00:00</updated>
    <author>
      <name>/u/freeky78</name>
      <uri>https://old.reddit.com/user/freeky78</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’ve been experimenting with a small “memory engine” for local LLM setups and just open-sourced it:&lt;/p&gt; &lt;p&gt;**DragonMemory – a 16× semantic compression layer for RAG contexts**, built to sit in front of your local LLaMA (or any other backend).&lt;/p&gt; &lt;p&gt;**Repo:** &lt;a href="https://github.com/Freeky7819/DragonMemory"&gt;DragonMemory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;### What it does (in plain terms)&lt;/p&gt; &lt;p&gt;Instead of throwing raw document chunks straight into a vector DB, DragonMemory:&lt;/p&gt; &lt;p&gt;- takes **token embeddings** for a chunk,&lt;/p&gt; &lt;p&gt;- compresses them **16:1 along the sequence dimension** (16 token embeddings → 1 “latent” vector),&lt;/p&gt; &lt;p&gt;- learns to reconstruct the *sentence-level* meaning from that latent,&lt;/p&gt; &lt;p&gt;- and then uses these compressed vectors for RAG retrieval.&lt;/p&gt; &lt;p&gt;So it’s not just dropping tokens or doing scalar quantization – it’s a **learned sequence compressor** that tries to preserve the original embedding space.&lt;/p&gt; &lt;p&gt;You can still use your usual stack (Ollama / local LLaMA / whatever) as the generator. DragonMemory just gives it a denser, cheaper memory.&lt;/p&gt; &lt;p&gt;### How it compares to quantization&lt;/p&gt; &lt;p&gt;A quick summary of how I see it:&lt;/p&gt; &lt;p&gt;- **Quantization**: &lt;/p&gt; &lt;p&gt;‣ shrinks **each vector** (fewer bits / lower precision), &lt;/p&gt; &lt;p&gt;‣ usually doesn’t model sequence structure explicitly.&lt;/p&gt; &lt;p&gt;- **DragonMemory**: &lt;/p&gt; &lt;p&gt;‣ shrinks the **sequence itself** (16 token embeddings → 1 latent), &lt;/p&gt; &lt;p&gt;‣ is trained to keep **sentence-level semantics** (cosine similarity) and retrieval quality.&lt;/p&gt; &lt;p&gt;You can actually **stack them**: first compress sequences with DragonMemory, then quantize the resulting latent vectors if you want even more savings.&lt;/p&gt; &lt;p&gt;### What I’ve measured so far&lt;/p&gt; &lt;p&gt;All numbers are from my local experiments (no cherry-picking, full eval scripts in the repo):&lt;/p&gt; &lt;p&gt;- **Compression ratio:** 1:16 along the sequence.&lt;/p&gt; &lt;p&gt;- **Teacher model:** MiniLM-style sentence embeddings.&lt;/p&gt; &lt;p&gt;- **Semantic reconstruction (sentence cosine):**&lt;/p&gt; &lt;p&gt;- Wikitext-2: ~**0.90** cosine after 16× compression.&lt;/p&gt; &lt;p&gt;- Technical report (Slovenian): ~0.85.&lt;/p&gt; &lt;p&gt;- Long-form literature sample (Frankenstein): ~0.88–0.89.&lt;/p&gt; &lt;p&gt;**RAG recall (on internal tests):**&lt;/p&gt; &lt;p&gt;- self-recall@1 = 1.0 across datasets (gets the original chunk back),&lt;/p&gt; &lt;p&gt;- partial-recall@3 in the ~0.6–1.0 range depending on corpus (technical docs vs. literature).&lt;/p&gt; &lt;p&gt;Everything runs locally; license is **AGPL-3.0** (I want it to stay open and not silently disappear into closed SaaS backends).&lt;/p&gt; &lt;p&gt;### Limitations / honest notes&lt;/p&gt; &lt;p&gt;- This is **not** a drop-in replacement for Faiss, Chroma, etc. – it’s a **layer in front of them**.&lt;/p&gt; &lt;p&gt;- It’s focused on **semantic retrieval**, not bit-perfect reconstruction of the original text.&lt;/p&gt; &lt;p&gt;- It’s early-stage research code, not a polished commercial product (yet) – expect rough edges.&lt;/p&gt; &lt;p&gt;### Why I’m posting here&lt;/p&gt; &lt;p&gt;Local LLaMA setups live or die by context and memory cost. I wanted to see how far a learned sequence compressor can go before recall quality breaks – and 16× with decent cosine surprised me.&lt;/p&gt; &lt;p&gt;If anyone here:&lt;/p&gt; &lt;p&gt;- wants to benchmark this on their own RAG pipeline,&lt;/p&gt; &lt;p&gt;- has feedback on the architecture or eval setup,&lt;/p&gt; &lt;p&gt;- or sees obvious ways to plug it into existing LocalLLaMA stacks (text-gen-webui, llama.cpp pipelines, etc.),&lt;/p&gt; &lt;p&gt;I’d love to hear it.&lt;/p&gt; &lt;p&gt;Happy to answer questions and share more detailed logs if useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freeky78"&gt; /u/freeky78 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p15wbk/release_dragonmemory_16_semantic_compression_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p15wbk/release_dragonmemory_16_semantic_compression_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p15wbk/release_dragonmemory_16_semantic_compression_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T12:03:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0r5ww</id>
    <title>GLM 4.6 on 128 GB RAM with llama.cpp</title>
    <updated>2025-11-18T23:11:48+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I got my hands on a new box at work with 128 GB RAM and 32 GB VRAM (it's a semi-budget option, with 2x5070, but it performs really well). I decided I'm going to try a few of the bigger models. Obviously, a very good model to run on this is GPT-OSS-120B and it's been the default model, but I've set my eyes on the big ones. The GLM 4.6 REAP was a bit overwhelming, but then I though &amp;quot;what if I could get my hands on a good low quant that fits&amp;quot;?&lt;/p&gt; &lt;p&gt;So, with the help of &lt;a href="https://huggingface.co/AesSedai"&gt;https://huggingface.co/AesSedai&lt;/a&gt; I've obtained a really nice mixed quant: &lt;a href="https://huggingface.co/AesSedai/GLM-4.6-GGUF/tree/main/llama.cpp/GLM-4.6-Q6_K-IQ2_XS-IQ2_XS-IQ3_S"&gt;https://huggingface.co/AesSedai/GLM-4.6-GGUF/tree/main/llama.cpp/GLM-4.6-Q6_K-IQ2_XS-IQ2_XS-IQ3_S&lt;/a&gt; - it's tuned to *just barely* fit in 128GB. What's surprising is how good quality it retains even at such low quant sizes - here's its analysis when I fed it the `modeling_kimi.py` file from Kimi Linear: &lt;a href="https://gist.github.com/pwilkin/7ee5672422bd30afdb47d3898680626b"&gt;https://gist.github.com/pwilkin/7ee5672422bd30afdb47d3898680626b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And on top of that, llama.cpp just merged the results of a few weeks of hard work of new contributor &lt;strong&gt;hksdpc255&lt;/strong&gt; on XML tool calling, including GLM 4.6: &lt;a href="https://github.com/ggml-org/llama.cpp/commit/1920345c3bcec451421bb6abc4981678cc721154"&gt;https://github.com/ggml-org/llama.cpp/commit/1920345c3bcec451421bb6abc4981678cc721154&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to give it a try - on my box it's getting around 40 t/s prompt processing and about 5 t/s generation, which is not lightning fast, but still a HUGE upgrade from the 5 t/s pp and 3 t/s tg when I tried just a slightly bigger quant.&lt;/p&gt; &lt;p&gt;Edit: forgot to mention, the deployment has 80k context at quite good Q8_0 K/V quantization, so not a gimmick build.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r5ww/glm_46_on_128_gb_ram_with_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r5ww/glm_46_on_128_gb_ram_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r5ww/glm_46_on_128_gb_ram_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T23:11:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p115u6</id>
    <title>vLLM 0.11.1 Seems to Be Bringing Massive Speedup on Turing GPUs</title>
    <updated>2025-11-19T07:27:07+00:00</updated>
    <author>
      <name>/u/lly0571</name>
      <uri>https://old.reddit.com/user/lly0571</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;vllm v0.11.1 using a new FLASHINFER backend and re-enables FP16 support on Turing GPUs, resulting in a much better performance on Volta and Turing GPUs (close to lmdeploy, better in prefill, worse in decode).&lt;/p&gt; &lt;p&gt;Hoping someone with a V100, T4, 2080Ti(22GB) or Titan RTX can have a similar test.&lt;/p&gt; &lt;p&gt;Here is a brief Qwen3-4B-Inst-2507 throughput benchmark of on my &lt;a href="https://www.techpowerup.com/gpu-specs/tesla-t10-16-gb.c4036"&gt;Tesla T10 16GB&lt;/a&gt; (a rare Tesla GPU close to RTX 2080, but 16GB).&lt;/p&gt; &lt;p&gt;I am using these commands to serve all of these models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=1 vllm serve Qwen3-4B-Instruct-2507 --gpu_memory_utilization 0.9 --port 8000 --max-model-len 16k CUDA_VISIBLE_DEVICES=1 lmdeploy serve api_server Qwen3-4B-Instruct-2507 --server-port 8000 --session-len 16384 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Prefill Heavy: PP8192/TG1 (Parallel 16)&lt;/h1&gt; &lt;p&gt;vllm 0.11.0&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve --dataset-name random --num-prompts 16 --backend vllm --host 10.249.42.202 --port 8000 --max-concurrency 16 --tokenizer Qwen3-0.6B --model Qwen3-4B-Instruct-2507 --random-input-len 8192 --random-output-len 1 INFO 11-19 14:58:30 [__init__.py:216] Automatically detected platform cuda. Namespace(subparser='bench', bench_type='serve', dispatch_function=&amp;lt;function BenchmarkServingSubcommand.cmd at 0x7f020b929620&amp;gt;, seed=0, num_prompts=16, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, custom_output_len=256, custom_skip_chat_template=False, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=8192, random_output_len=1, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='vllm', endpoint_type=None, base_url=None, host='10.249.42.202', port=8000, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen3-4B-Instruct-2507', tokenizer='Qwen3-0.6B', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600) INFO 11-19 14:58:32 [datasets.py:507] Sampling input_len from [8192, 8192] and output_len from [1, 1] Starting initial single prompt test run... Waiting for endpoint to become up in 600 seconds | | 01:21 elapsed, 31635:35:38 remaining Initial test run completed. Starting main benchmark run... Traffic request rate: inf Burstiness factor: 1.0 (Poisson process) Maximum request concurrency: 16 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [04:48&amp;lt;00:00, 18.02s/it] tip: install termplotlib and gnuplot to plot the metrics ============ Serving Benchmark Result ============ Successful requests: 16 Maximum request concurrency: 16 Benchmark duration (s): 288.39 Total input tokens: 130981 Total generated tokens: 16 Request throughput (req/s): 0.06 Output token throughput (tok/s): 0.06 Peak output token throughput (tok/s): 1.00 Peak concurrent requests: 16.00 Total Token throughput (tok/s): 454.23 ---------------Time to First Token---------------- Mean TTFT (ms): 125794.42 Median TTFT (ms): 111166.06 P99 TTFT (ms): 283469.41 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 0.00 Median TPOT (ms): 0.00 P99 TPOT (ms): 0.00 ---------------Inter-token Latency---------------- Mean ITL (ms): 0.00 Median ITL (ms): 0.00 P99 ITL (ms): 0.00 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;vllm 0.11.1&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve --dataset-name random --num-prompts 64 --backend vllm --host 10.249.42.202 --port 8000 --max-concurrency 16 --tokenizer Qwen3-0.6B --model Qwen3-4B-Instruct-2507 --random-input-len 8192 --random-output-len 1 INFO 11-19 14:47:01 [__init__.py:216] Automatically detected platform cuda. Namespace(subparser='bench', bench_type='serve', dispatch_function=&amp;lt;function BenchmarkServingSubcommand.cmd at 0x7f2572149620&amp;gt;, seed=0, num_prompts=64, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, custom_output_len=256, custom_skip_chat_template=False, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=8192, random_output_len=1, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='vllm', endpoint_type=None, base_url=None, host='10.249.42.202', port=8000, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen3-4B-Instruct-2507', tokenizer='Qwen3-0.6B', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600) INFO 11-19 14:47:04 [datasets.py:507] Sampling input_len from [8192, 8192] and output_len from [1, 1] Starting initial single prompt test run... Waiting for endpoint to become up in 600 seconds | | 00:01 elapsed, 642:35:16 remaining Initial test run completed. Starting main benchmark run... Traffic request rate: inf Burstiness factor: 1.0 (Poisson process) Maximum request concurrency: 16 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [01:50&amp;lt;00:00, 1.72s/it] tip: install termplotlib and gnuplot to plot the metrics ============ Serving Benchmark Result ============ Successful requests: 64 Maximum request concurrency: 16 Benchmark duration (s): 110.03 Total input tokens: 523886 Total generated tokens: 64 Request throughput (req/s): 0.58 Output token throughput (tok/s): 0.58 Peak output token throughput (tok/s): 1.00 Peak concurrent requests: 17.00 Total Token throughput (tok/s): 4761.83 ---------------Time to First Token---------------- Mean TTFT (ms): 24172.28 Median TTFT (ms): 27210.15 P99 TTFT (ms): 28380.61 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 0.00 Median TPOT (ms): 0.00 P99 TPOT (ms): 0.00 ---------------Inter-token Latency---------------- Mean ITL (ms): 0.00 Median ITL (ms): 0.00 P99 ITL (ms): 0.00 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;lmdeploy&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve --dataset-name random --num-prompts 64 --backend vllm --host 10.249.42.202 --port 8000 --max-concurrency 16 --tokenizer Qwen3-0.6B --model Qwen3-4B-Instruct-2507 --random-input-len 8192 --random-output-len 1 INFO 11-19 15:16:51 [__init__.py:216] Automatically detected platform cuda. Namespace(subparser='bench', bench_type='serve', dispatch_function=&amp;lt;function BenchmarkServingSubcommand.cmd at 0x7fa4823b5620&amp;gt;, seed=0, num_prompts=64, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, custom_output_len=256, custom_skip_chat_template=False, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=8192, random_output_len=1, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='vllm', endpoint_type=None, base_url=None, host='10.249.42.202', port=8000, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen3-4B-Instruct-2507', tokenizer='Qwen3-0.6B', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600) INFO 11-19 15:16:53 [datasets.py:507] Sampling input_len from [8192, 8192] and output_len from [1, 1] Starting initial single prompt test run... Waiting for endpoint to become up in 600 seconds | | 00:01 elapsed, 756:41:43 remaining Initial test run completed. Starting main benchmark run... Traffic request rate: inf Burstiness factor: 1.0 (Poisson process) Maximum request concurrency: 16 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [01:58&amp;lt;00:00, 1.85s/it] tip: install termplotlib and gnuplot to plot the metrics ============ Serving Benchmark Result ============ Successful requests: 64 Maximum request concurrency: 16 Benchmark duration (s): 118.10 Total input tokens: 523886 Total generated tokens: 124 Request throughput (req/s): 0.54 Output token throughput (tok/s): 1.05 Peak output token throughput (tok/s): 8.00 Peak concurrent requests: 18.00 Total Token throughput (tok/s): 4437.05 ---------------Time to First Token---------------- Mean TTFT (ms): 24981.20 Median TTFT (ms): 28008.93 P99 TTFT (ms): 29259.25 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 1803.85 Median TPOT (ms): 1869.74 P99 TPOT (ms): 1937.03 ---------------Inter-token Latency---------------- Mean ITL (ms): 895.75 Median ITL (ms): 0.33 P99 ITL (ms): 1936.55 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Decode heavy: PP512/TG512 (Parallel 16)&lt;/h1&gt; &lt;p&gt;v0.11.0&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve --dataset-name random --num-prompts 16 --backend vllm --host 10.249.42.202 --port 8000 --max-concurrency 16 --tokenizer Qwen3-0.6B --model Qwen3-4B-Instruct-2507 --random-input-len 512 --random-output-len 512 INFO 11-19 15:08:12 [__init__.py:216] Automatically detected platform cuda. Namespace(subparser='bench', bench_type='serve', dispatch_function=&amp;lt;function BenchmarkServingSubcommand.cmd at 0x7fe684875620&amp;gt;, seed=0, num_prompts=16, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, custom_output_len=256, custom_skip_chat_template=False, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=512, random_output_len=512, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='vllm', endpoint_type=None, base_url=None, host='10.249.42.202', port=8000, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen3-4B-Instruct-2507', tokenizer='Qwen3-0.6B', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600) INFO 11-19 15:08:14 [datasets.py:507] Sampling input_len from [512, 512] and output_len from [512, 512] Starting initial single prompt test run... Waiting for endpoint to become up in 600 seconds | | 00:40 elapsed, 15758:20:48 remaining Initial test run completed. Starting main benchmark run... Traffic request rate: inf Burstiness factor: 1.0 (Poisson process) Maximum request concurrency: 16 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [03:02&amp;lt;00:00, 11.43s/it] tip: install termplotlib and gnuplot to plot the metrics ============ Serving Benchmark Result ============ Successful requests: 16 Maximum request concurrency: 16 Benchmark duration (s): 182.80 Total input tokens: 8177 Total generated tokens: 7681 Request throughput (req/s): 0.09 Output token throughput (tok/s): 42.02 Peak output token throughput (tok/s): 75.00 Peak concurrent requests: 16.00 Total Token throughput (tok/s): 86.75 ---------------Time to First Token---------------- Mean TTFT (ms): 18188.82 Median TTFT (ms): 16467.30 P99 TTFT (ms): 22968.20 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 322.22 Median TPOT (ms): 325.09 P99 TPOT (ms): 327.25 ---------------Inter-token Latency---------------- Mean ITL (ms): 322.22 Median ITL (ms): 307.80 P99 ITL (ms): 389.45 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;v0.11.1&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve --dataset-name random --num-prompts 64 --backend vllm --host 10.249.42.202 --port 8000 --max-concurrency 16 --tokenizer Qwen3-0.6B --model Qwen3-4B-Instruct-2507 --random-input-len 512 --random-output-len 512 INFO 11-19 14:54:10 [__init__.py:216] Automatically detected platform cuda. Namespace(subparser='bench', bench_type='serve', dispatch_function=&amp;lt;function BenchmarkServingSubcommand.cmd at 0x7f76d6b1d580&amp;gt;, seed=0, num_prompts=64, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, custom_output_len=256, custom_skip_chat_template=False, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=512, random_output_len=512, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='vllm', endpoint_type=None, base_url=None, host='10.249.42.202', port=8000, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen3-4B-Instruct-2507', tokenizer='Qwen3-0.6B', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600) INFO 11-19 14:54:12 [datasets.py:507] Sampling input_len from [512, 512] and output_len from [512, 512] Starting initial single prompt test run... Waiting for endpoint to become up in 600 seconds | | 00:12 elapsed, 4714:00:33 remaining Initial test run completed. Starting main benchmark run... Traffic request rate: inf Burstiness factor: 1.0 (Poisson process) Maximum request concurrency: 16 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [01:11&amp;lt;00:00, 1.11s/it] tip: install termplotlib and gnuplot to plot the metrics ============ Serving Benchmark Result ============ Successful requests: 64 Maximum request concurrency: 16 Benchmark duration (s): 71.04 Total input tokens: 32565 Total generated tokens: 31353 Request throughput (req/s): 0.90 Output token throughput (tok/s): 441.34 Peak output token throughput (tok/s): 512.00 Peak concurrent requests: 31.00 Total Token throughput (tok/s): 899.75 ---------------Time to First Token---------------- Mean TTFT (ms): 591.82 Median TTFT (ms): 599.07 P99 TTFT (ms): 1251.87 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 33.70 Median TPOT (ms): 34.11 P99 TPOT (ms): 35.13 ---------------Inter-token Latency---------------- Mean ITL (ms): 33.68 Median ITL (ms): 32.30 P99 ITL (ms): 35.16 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;lmdeploy:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve --dataset-name random --num-prompts 64 --backend vllm --host 10.249.42.202 --port 8000 --max-concurrency 16 --tokenizer Qwen3-0.6B --model Qwen3-4B-Instruct-2507 --random-input-len 512 --random-output-len 512 INFO 11-19 15:14:54 [__init__.py:216] Automatically detected platform cuda. Namespace(subparser='bench', bench_type='serve', dispatch_function=&amp;lt;function BenchmarkServingSubcommand.cmd at 0x7f3146319580&amp;gt;, seed=0, num_prompts=64, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, custom_output_len=256, custom_skip_chat_template=False, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=512, random_output_len=512, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='vllm', endpoint_type=None, base_url=None, host='10.249.42.202', port=8000, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen3-4B-Instruct-2507', tokenizer='Qwen3-0.6B', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600) INFO 11-19 15:14:57 [datasets.py:507] Sampling input_len from [512, 512] and output_len from [512, 512] Starting initial single prompt test run... Waiting for endpoint to become up in 600 seconds | | 00:14 elapsed, 5459:10:19 remaining Initial test run completed. Starting main benchmark run... Traffic request rate: inf Burstiness factor: 1.0 (Poisson process) Maximum request concurrency: 16 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [01:05&amp;lt;00:00, 1.03s/it] tip: install termplotlib and gnuplot to plot the metrics ============ Serving Benchmark Result ============ Successful requests: 64 Maximum request concurrency: 16 Benchmark duration (s): 65.94 Total input tokens: 32565 Total generated tokens: 30895 Request throughput (req/s): 0.97 Output token throughput (tok/s): 468.55 Peak output token throughput (tok/s): 560.00 Peak concurrent requests: 32.00 Total Token throughput (tok/s): 962.42 ---------------Time to First Token---------------- Mean TTFT (ms): 1051.63 Median TTFT (ms): 1118.93 P99 TTFT (ms): 1370.53 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 30.14 Median TPOT (ms): 30.31 P99 TPOT (ms): 32.24 ---------------Inter-token Latency---------------- Mean ITL (ms): 30.11 Median ITL (ms): 29.66 P99 ITL (ms): 31.83 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lly0571"&gt; /u/lly0571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p115u6/vllm_0111_seems_to_be_bringing_massive_speedup_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p115u6/vllm_0111_seems_to_be_bringing_massive_speedup_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p115u6/vllm_0111_seems_to_be_bringing_massive_speedup_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T07:27:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0q3z1</id>
    <title>Offline Epstein File Ranker Using GPT-OSS-120B (Built on tensonaut’s dataset)</title>
    <updated>2025-11-18T22:29:30+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0q3z1/offline_epstein_file_ranker_using_gptoss120b/"&gt; &lt;img alt="Offline Epstein File Ranker Using GPT-OSS-120B (Built on tensonaut’s dataset)" src="https://preview.redd.it/nkktzj83y22g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a55f8da7446aedd9d2f482226b72c19b4e4ebbf9" title="Offline Epstein File Ranker Using GPT-OSS-120B (Built on tensonaut’s dataset)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been playing with the new 25k-page Epstein Files drop that &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file"&gt;tensonaut posted&lt;/a&gt;. Instead of reading 100MB of chaotic OCR myself like a medieval scribe, I threw an open-source model at it and built a local tool that &lt;strong&gt;ranks every document by “investigative usefulness.”&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Everything runs on a single M3 Max MacBook Pro with &lt;strong&gt;open-source&lt;/strong&gt; models only. No cloud, no API calls, no data leaving the machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;br /&gt; • Streams the entire House Oversight release through &lt;strong&gt;openai/gpt-oss-120b&lt;/strong&gt; running locally via LM Studio.&lt;br /&gt; • Scores each passage based on actionable leads, controversy, novelty, and power-linkage.&lt;br /&gt; • Outputs a fully structured JSONL dataset with headline, score, key insights, implicated actors, financial-flow notes, etc.&lt;br /&gt; • Ships with an interactive local viewer so you can filter by score, read full source text, explore lead types, and inspect charts.&lt;br /&gt; • Designed for investigative triage, RAG, IR experiments, or academic analysis.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;&lt;br /&gt; This corpus is massive, messy, and full of OCR noise. Doing a systematic pass manually is impossible. Doing it with cloud models would be expensive and slow. Doing it locally means it’s cheap, private, and reproducible.&lt;/p&gt; &lt;p&gt;A full run costs about &lt;strong&gt;$1.50 in electricity&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech details&lt;/strong&gt;&lt;br /&gt; • Model: openai/gpt-oss-120b served at &lt;code&gt;localhost:5002/v1&lt;/code&gt;&lt;br /&gt; • Hardware: M3 Max, 128 GB RAM&lt;br /&gt; • Viewer: simple JS dashboard with AG Grid, charts, and chunked JSONL loading&lt;br /&gt; • Input dataset: &lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;tensonaut’s EPSTEIN_FILES_20K on Hugging Face&lt;/a&gt;&lt;br /&gt; • Output: ranked chunks in &lt;code&gt;contrib/&lt;/code&gt;, auto-indexed by the viewer&lt;br /&gt; • Prompt: optimized for investigative lead scoring, with a consistent numerical scale (0–100)&lt;/p&gt; &lt;p&gt;Repo:&lt;br /&gt; &lt;a href="https://github.com/latent-variable/epstein-ranker"&gt;https://github.com/latent-variable/epstein-ranker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far I’ve processed the first 5,000 rows myself and published the scored chunks in the repo. If anyone wants to help triage more of the dataset, the GitHub includes simple instructions for claiming a slice and submitting it as a contrib chunk. The workflow supports clean collaboration with automatic deduping.&lt;/p&gt; &lt;p&gt;If you’d rather build your own tools on top of the scored output or adapt the ranking method for other document dumps, go for it. Everything is MIT-licensed, fully local, and easy to extend.&lt;/p&gt; &lt;p&gt;Contributions, forks, or experiments are all welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nkktzj83y22g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0q3z1/offline_epstein_file_ranker_using_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0q3z1/offline_epstein_file_ranker_using_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T22:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0iayb</id>
    <title>Google Antigravity is a cursor clone</title>
    <updated>2025-11-18T17:36:03+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you love vibe coding: &lt;a href="https://antigravity.google/"&gt;https://antigravity.google/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Supports models other than gemini such as GPT-OSS. Hopefully we will get instructions for running local models soon.&lt;/p&gt; &lt;p&gt;Update: Title should more appropriately say : windsurf clone . &lt;a href="https://www.reuters.com/business/google-hires-windsurf-ceo-researchers-advance-ai-ambitions-2025-07-11/"&gt;https://www.reuters.com/business/google-hires-windsurf-ceo-researchers-advance-ai-ambitions-2025-07-11/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T17:36:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p18lim</id>
    <title>Meituan Longcat releases AMO Bench: Kimi k2 Thinking is the best Math AI</title>
    <updated>2025-11-19T14:07:05+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p18lim/meituan_longcat_releases_amo_bench_kimi_k2/"&gt; &lt;img alt="Meituan Longcat releases AMO Bench: Kimi k2 Thinking is the best Math AI" src="https://b.thumbs.redditmedia.com/oPjnD6rAukaQ0UryA8xvdAgUAoZGEFEJQz8xO1JZ5qc.jpg" title="Meituan Longcat releases AMO Bench: Kimi k2 Thinking is the best Math AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/k6cgs17t082g1.png?width=2816&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0fd6ad58bd315eb3bf5fa0b376d71f21b7342a0"&gt;https://preview.redd.it/k6cgs17t082g1.png?width=2816&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0fd6ad58bd315eb3bf5fa0b376d71f21b7342a0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8t68e95w082g1.png?width=2506&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddd00c72663e56e797b5d74982d89939554beb2c"&gt;https://preview.redd.it/8t68e95w082g1.png?width=2506&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddd00c72663e56e797b5d74982d89939554beb2c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Original Problems&lt;/strong&gt;: 50 brand-new problems designed by human experts&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Guaranteed Difficulty&lt;/strong&gt;: Cross-validated to ensure at least IMO-level difficulty&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Automatic Grading&lt;/strong&gt;: Hybrid grading algorithm with 99.2% scoring accuracy&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Human-Annotated CoT&lt;/strong&gt;: Facilitate further case analysis to improve model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p18lim/meituan_longcat_releases_amo_bench_kimi_k2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p18lim/meituan_longcat_releases_amo_bench_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p18lim/meituan_longcat_releases_amo_bench_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T14:07:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0sisn</id>
    <title>I replicated Anthropic’s "Introspection" paper on DeepSeek-7B. It works.</title>
    <updated>2025-11-19T00:09:39+00:00</updated>
    <author>
      <name>/u/Specialist_Bad_4465</name>
      <uri>https://old.reddit.com/user/Specialist_Bad_4465</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Bad_4465"&gt; /u/Specialist_Bad_4465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://joshfonseca.com/blogs/introspection"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0sisn/i_replicated_anthropics_introspection_paper_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0sisn/i_replicated_anthropics_introspection_paper_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T00:09:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0lnlo</id>
    <title>Make your AI talk like a caveman and decrease token usage</title>
    <updated>2025-11-18T19:39:38+00:00</updated>
    <author>
      <name>/u/RegionCareful7282</name>
      <uri>https://old.reddit.com/user/RegionCareful7282</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"&gt; &lt;img alt="Make your AI talk like a caveman and decrease token usage" src="https://preview.redd.it/7g67ftgti22g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7d9207d83386575ef61218ed4c0a30301826b10" title="Make your AI talk like a caveman and decrease token usage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on a little side project to help LLMs talk like… cavemen.&lt;br /&gt; Why? To save tokens, of course. &lt;/p&gt; &lt;p&gt;It works because LLMs can easily fill in grammar and connectives on their own. So we strip what’s predictable, keep what’s meaningful, and the model still understands everything perfectly. &lt;/p&gt; &lt;p&gt;Store RAG documents in caveman-compressed form so each chunk carries more valuable data, fits more context, and gives better retrieval quality.&lt;/p&gt; &lt;p&gt;Thought I'd share it here as it might be beneficial in order to not waste tokens on unnecessary words :)&lt;/p&gt; &lt;p&gt;Feel free to contribute if you have any additions!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/wilpel/caveman-compression"&gt;https://github.com/wilpel/caveman-compression&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RegionCareful7282"&gt; /u/RegionCareful7282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7g67ftgti22g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T19:39:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1df5y</id>
    <title>SAM 3: Segment Anything with Concepts, by Meta Superintelligence Labs</title>
    <updated>2025-11-19T17:10:27+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1df5y/sam_3_segment_anything_with_concepts_by_meta/"&gt; &lt;img alt="SAM 3: Segment Anything with Concepts, by Meta Superintelligence Labs" src="https://external-preview.redd.it/4Uyf8OlIkFBtIXR-wKdBIOZqZgS3NkQSX04eUeTDY7w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88abe479f5e76ceeae47c92c033ef5091fe19a40" title="SAM 3: Segment Anything with Concepts, by Meta Superintelligence Labs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/facebook/sam3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1df5y/sam_3_segment_anything_with_concepts_by_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1df5y/sam_3_segment_anything_with_concepts_by_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T17:10:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0zn5j</id>
    <title>Most people in this LocalLLaMA are hypocritical.</title>
    <updated>2025-11-19T05:56:48+00:00</updated>
    <author>
      <name>/u/Ok_houlin</name>
      <uri>https://old.reddit.com/user/Ok_houlin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0zn5j/most_people_in_this_localllama_are_hypocritical/"&gt; &lt;img alt="Most people in this LocalLLaMA are hypocritical." src="https://b.thumbs.redditmedia.com/WxKfTmSH4Yd9ArZ6YbJC1DQFNluNfkuOSm4Ty9fMAOQ.jpg" title="Most people in this LocalLLaMA are hypocritical." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When posts about qwen max appear, there are a lot of comments saying that it shouldn't be discussed.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2rr3u5vzk52g1.png?width=1062&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2adc0fc48f38eb189a05ecdb9ec1f366bd84e50b"&gt;https://preview.redd.it/2rr3u5vzk52g1.png?width=1062&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2adc0fc48f38eb189a05ecdb9ec1f366bd84e50b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, when Gemini 3 and gpt 5 were discussed, not a single comment objected to their being discussed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_houlin"&gt; /u/Ok_houlin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0zn5j/most_people_in_this_localllama_are_hypocritical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0zn5j/most_people_in_this_localllama_are_hypocritical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0zn5j/most_people_in_this_localllama_are_hypocritical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T05:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0gjcu</id>
    <title>Gemini 3 is launched</title>
    <updated>2025-11-18T16:31:01+00:00</updated>
    <author>
      <name>/u/Several-Republic-609</name>
      <uri>https://old.reddit.com/user/Several-Republic-609</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"&gt; &lt;img alt="Gemini 3 is launched" src="https://external-preview.redd.it/Jcgyato32sPSUDLsqQhcsyfnhHKEryk97hJ_EjIMDyU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc3edcd8902e26525ff2ad02160747ab3d46316e" title="Gemini 3 is launched" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Several-Republic-609"&gt; /u/Several-Republic-609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.google/products/gemini/gemini-3/#note-from-ceo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T16:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p145pj</id>
    <title>Our AI assistant keeps getting jailbroken and it’s becoming a security nightmare</title>
    <updated>2025-11-19T10:32:16+00:00</updated>
    <author>
      <name>/u/Comfortable_Clue5430</name>
      <uri>https://old.reddit.com/user/Comfortable_Clue5430</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an internal AI helper for our support team, and no matter how many guardrails we add, people keep finding ways to jailbreak it. Employees aren’t doing it maliciously, they’re just curious and want to see what happens, but suddenly the assistant is spitting out stuff it’s absolutely not supposed to.&lt;/p&gt; &lt;p&gt;We’ve tried regex filters, prompt-hardening, even manual review nothing sticks.&lt;/p&gt; &lt;p&gt;Feels like every week we patch one exploit and three more show up.&lt;/p&gt; &lt;p&gt;Anyone actually found a scalable way to test and secure an AI model before it goes public?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Clue5430"&gt; /u/Comfortable_Clue5430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p145pj/our_ai_assistant_keeps_getting_jailbroken_and_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p145pj/our_ai_assistant_keeps_getting_jailbroken_and_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p145pj/our_ai_assistant_keeps_getting_jailbroken_and_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T10:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0u8hd</id>
    <title>ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama</title>
    <updated>2025-11-19T01:26:53+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"&gt; &lt;img alt="ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama" src="https://preview.redd.it/2zt7d6q0942g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d69898cd41ba5897e02dd650de189c04e2b1fbb" title="ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2zt7d6q0942g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T01:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I’m Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; — Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
