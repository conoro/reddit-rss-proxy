<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-09T09:37:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qzz3zw</id>
    <title>I built an MCP server that syncs Cursor, Claude Desktop, and Windsurf with one brain [Open Source]</title>
    <updated>2026-02-09T08:45:06+00:00</updated>
    <author>
      <name>/u/NucleusOS</name>
      <uri>https://old.reddit.com/user/NucleusOS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;The Problem&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I use Cursor for coding, Claude Desktop for thinking, and Windsurf for exploration. But every time I switch tools, I lose context. I'd tell Claude about an architecture decision, then Cursor would have no idea.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/eidetic-works/nucleus-mcp"&gt;Nucleus MCP&lt;/a&gt; creates a shared brain (&lt;code&gt;.brain/&lt;/code&gt; folder) across all your AI tools.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tell Claude about a decision ‚Üí Cursor knows it&lt;/li&gt; &lt;li&gt;Make a plan in Windsurf ‚Üí Claude remembers it&lt;/li&gt; &lt;li&gt;One brain. All your tools.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Quick Start&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash pip install nucleus-mcp nucleus-init # Auto-configures Claude, Cursor, Windsurf &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's Included&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Persistent Memory&lt;/strong&gt; ‚Äî Knowledge that survives sessions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Agent Sync&lt;/strong&gt; ‚Äî One brain across all tools&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Audit Trail&lt;/strong&gt; ‚Äî Every decision logged&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local-First&lt;/strong&gt; ‚Äî Your data stays on your machine&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hypervisor&lt;/strong&gt; ‚Äî File locking and security&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How It's Different from OpenClaw&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;OpenClaw is great for running agent teams on their platform. Nucleus connects &lt;em&gt;different&lt;/em&gt; platforms together. Use OpenClaw for agent teams. Use Nucleus for your universal brain.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;It's Open Source&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MIT licensed. No catch. No waitlist.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/eidetic-works/nucleus-mcp"&gt;https://github.com/eidetic-works/nucleus-mcp&lt;/a&gt;&lt;/li&gt; &lt;li&gt;PyPI: &lt;code&gt;pip install nucleus-mcp&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I'm Looking For&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Feedback on the concept&lt;/li&gt; &lt;li&gt;Feature requests&lt;/li&gt; &lt;li&gt;Beta testers who use multiple AI tools daily&lt;/li&gt; &lt;li&gt;Contributors (Python, MCP protocol)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NucleusOS"&gt; /u/NucleusOS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz3zw/i_built_an_mcp_server_that_syncs_cursor_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz3zw/i_built_an_mcp_server_that_syncs_cursor_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz3zw/i_built_an_mcp_server_that_syncs_cursor_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T08:45:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzz7p4</id>
    <title>I built a voice assistant that controls my Terminal using Whisper (Local) + Claude Code CLI (&lt;100 lines of script)</title>
    <updated>2026-02-09T08:51:53+00:00</updated>
    <author>
      <name>/u/jokiruiz</name>
      <uri>https://old.reddit.com/user/jokiruiz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a weekend project I've been working on. I was frustrated with Siri/Alexa not being able to actually &lt;em&gt;interact&lt;/em&gt; with my dev environment, so I built a small Python script to bridge the gap between voice and my terminal.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Architecture:&lt;/strong&gt; It's a loop that runs in under 100 lines of Python:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Audio Capture:&lt;/strong&gt; Uses &lt;code&gt;sounddevice&lt;/code&gt; and &lt;code&gt;numpy&lt;/code&gt; to detect silence thresholds (VAD) automatically.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;STT (Speech to Text):&lt;/strong&gt; Runs &lt;strong&gt;OpenAI Whisper&lt;/strong&gt; locally (base model). No audio is sent to the cloud for transcription, which keeps latency decent and privacy high.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligence:&lt;/strong&gt; Pipes the transcribed text into the new &lt;strong&gt;Claude Code CLI&lt;/strong&gt; (via &lt;code&gt;subprocess&lt;/code&gt;). &lt;ul&gt; &lt;li&gt;&lt;em&gt;Why Claude Code?&lt;/em&gt; Because unlike the standard API, the CLI has permission to execute terminal commands, read files, and search the codebase directly.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TTS:&lt;/strong&gt; Uses native OS text-to-speech ( &lt;code&gt;say&lt;/code&gt; on Mac, &lt;code&gt;pyttsx3&lt;/code&gt; on Windows) to read the response back.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The cool part:&lt;/strong&gt; Since Claude Code has shell access, I can ask things like &lt;em&gt;&amp;quot;Check the load average and if it's high, list the top 5 processes&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;quot;Read the readme in this folder and summarize it&amp;quot;&lt;/em&gt;, and it actually executes it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here is the core logic for the Whisper implementation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Simple snippet of the logic import sounddevice as sd import numpy as np import whisper model = whisper.load_model(&amp;quot;base&amp;quot;) def record_audio(): # ... (silence detection logic) pass def transcribe(audio_data): result = model.transcribe(audio_data, fp16=False) return result[&amp;quot;text&amp;quot;] # ... (rest of the loop) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I made a video breakdown explaining the setup and showing a live demo of it managing files and checking system stats.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;üì∫ Video Demo &amp;amp; Walkthrough:&lt;/strong&gt; &lt;a href="https://youtu.be/hps59cmmbms?si=FBWyVZZDETl6Hi1J"&gt;https://youtu.be/hps59cmmbms?si=FBWyVZZDETl6Hi1J&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm planning to upload the full source code to GitHub once I clean up the dependencies.&lt;/p&gt; &lt;p&gt;Let me know if you have any ideas on how to improve the latency between the local Whisper transcription and the Claude response!&lt;/p&gt; &lt;p&gt;Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jokiruiz"&gt; /u/jokiruiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz7p4/i_built_a_voice_assistant_that_controls_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz7p4/i_built_a_voice_assistant_that_controls_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz7p4/i_built_a_voice_assistant_that_controls_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T08:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzzb8w</id>
    <title>Dual 3090s (power-limited) - Are 3x PCI-E cables w/daisy-chain "okay?"</title>
    <updated>2026-02-09T08:58:27+00:00</updated>
    <author>
      <name>/u/overand</name>
      <uri>https://old.reddit.com/user/overand</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I &lt;em&gt;just&lt;/em&gt; discovered that my modular 1350 watt power supply - despite having the new generation 12V connector (for cards I'll never be able to afford) - only came with 3 of the PCI-E power cables - though each has the little daisy-chain end on it, unused.&lt;/p&gt; &lt;p&gt;I'm running my current 3090 power-limited - and it's a dell OEM one, two PCI-E power connectors. I have a second identical card I'll be putting in, and I'm wondering if it's reasonable to run one &amp;quot;dedicated&amp;quot; power cable to each card, and use the daisy-chain to run both - and, if so, should I be more aggressive with my power limiting? I've never used the daisy-chain stuff, but I wonder why it's even offered if it's actually unsafe to use. (But, could be down to marketing and inertia). Anyway, any advice welcomed. The obvious solution is &amp;quot;get another modular cable, dumdum.&amp;quot; But, would &lt;em&gt;you&lt;/em&gt; be patient enough to not try, as your second 3090 arrived? (;&lt;/p&gt; &lt;p&gt;The power supply, for reference, is a Thermaltake Toughpower GF3 1350W (ATX 3.0). And I've only run into dodgy third party cables so far (but thermaltake's site was down last time I tried.)&lt;/p&gt; &lt;p&gt;(I sure wish modular power supply standards were consistent - I have a spare I could use, but the pins are wired &lt;strong&gt;wildly&lt;/strong&gt; differently, despite being the same Molex connector on the power supply end - yuck.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/overand"&gt; /u/overand &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzzb8w/dual_3090s_powerlimited_are_3x_pcie_cables/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzzb8w/dual_3090s_powerlimited_are_3x_pcie_cables/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzzb8w/dual_3090s_powerlimited_are_3x_pcie_cables/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T08:58:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzlo6a</id>
    <title>arXiv at Home - a self-hosted search engine for arXiv papers</title>
    <updated>2026-02-08T21:37:30+00:00</updated>
    <author>
      <name>/u/mrAppleXZ</name>
      <uri>https://old.reddit.com/user/mrAppleXZ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzlo6a/arxiv_at_home_a_selfhosted_search_engine_for/"&gt; &lt;img alt="arXiv at Home - a self-hosted search engine for arXiv papers" src="https://external-preview.redd.it/GByuthZvqw-sh4cUy1TLMJvthzS18fOPWRRPk2rkWTU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=600cdbb4cfc30b5cd647a299f4b1c24cbbcd97f5" title="arXiv at Home - a self-hosted search engine for arXiv papers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrAppleXZ"&gt; /u/mrAppleXZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/mrapplexz/arxiv-at-home"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzlo6a/arxiv_at_home_a_selfhosted_search_engine_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzlo6a/arxiv_at_home_a_selfhosted_search_engine_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T21:37:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz8clh</id>
    <title>Verity,a Perplexity style AI search and answer engine that runs fully locally on AI PCs with CPU,GPU,NPU acceleration</title>
    <updated>2026-02-08T13:01:54+00:00</updated>
    <author>
      <name>/u/simpleuserhere</name>
      <uri>https://old.reddit.com/user/simpleuserhere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz8clh/veritya_perplexity_style_ai_search_and_answer/"&gt; &lt;img alt="Verity,a Perplexity style AI search and answer engine that runs fully locally on AI PCs with CPU,GPU,NPU acceleration" src="https://preview.redd.it/aahtdiytq9ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d55a1c7f167d59371e910cefd07f2e7f958a59c0" title="Verity,a Perplexity style AI search and answer engine that runs fully locally on AI PCs with CPU,GPU,NPU acceleration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing my new App - Verity,a Perplexity style AI search and answer engine that runs fully locally on AI PCs with CPU,GPU,NPU acceleration.&lt;/p&gt; &lt;p&gt;You can run it as a CLI or a Web UI, depending on your workflow.&lt;/p&gt; &lt;p&gt;Developed and tested on Intel Core Ultra Series 1, leveraging on-device compute for fast, private AI inference.&lt;/p&gt; &lt;p&gt;Features :&lt;/p&gt; &lt;p&gt;- Fully Local, AI PC Ready - Optimized for Intel AI PCs using OpenVINO (CPU / iGPU / NPU), Ollama (CPU / CUDA / Metal)&lt;/p&gt; &lt;p&gt;- Privacy by Design - Search and inference can be fully self-hosted&lt;/p&gt; &lt;p&gt;- SearXNG-Powered Search - Self-hosted, privacy-friendly meta search engine&lt;/p&gt; &lt;p&gt;- Designed for fact-grounded, explorable answers&lt;/p&gt; &lt;p&gt;- OpenVINO and Ollama models supported&lt;/p&gt; &lt;p&gt;- Modular architecture&lt;/p&gt; &lt;p&gt;- CLI and WebUI support&lt;/p&gt; &lt;p&gt;- API server support&lt;/p&gt; &lt;p&gt;- Powered by Jan-nano 4B model,or configure any model&lt;/p&gt; &lt;p&gt;GitHub Repo : &lt;a href="https://github.com/rupeshs/verity"&gt;https://github.com/rupeshs/verity&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simpleuserhere"&gt; /u/simpleuserhere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aahtdiytq9ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz8clh/veritya_perplexity_style_ai_search_and_answer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz8clh/veritya_perplexity_style_ai_search_and_answer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T13:01:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzy8uu</id>
    <title>[Project] MCP Orchestrator - Turn one AI agent into a team with parallel sub-agents</title>
    <updated>2026-02-09T07:49:56+00:00</updated>
    <author>
      <name>/u/ask149</name>
      <uri>https://old.reddit.com/user/ask149</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! I built an open-source MCP server that lets you spawn parallel AI sub-agents ‚Äî think of it as turning one AI coding agent into a team.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Spawns up to 10 parallel sub-agents using Copilot CLI or Claude Code CLI&lt;/li&gt; &lt;li&gt;Passes file context to each agent (full file, summary, or grep mode)&lt;/li&gt; &lt;li&gt;Smart timeout selection based on MCP servers requested&lt;/li&gt; &lt;li&gt;Cross-platform: macOS, Linux, and Windows&lt;/li&gt; &lt;li&gt;Headless &amp;amp; programmatic ‚Äî designed for AI-to-AI orchestration via MCP protocol&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example use case:&lt;/strong&gt; You give one prompt like &amp;quot;research job openings at Stripe, Google, and Meta&amp;quot; ‚Äî the orchestrator fans that out to 3 parallel agents, each with their own MCP servers (e.g., Playwright for browser access), and aggregates results.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Install:&lt;/strong&gt; &lt;code&gt;npm i @‚Äãask149/mcp-orchestrator&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Ask149/orchestrator"&gt;https://github.com/Ask149/orchestrator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Looking for dev feedback &amp;amp; contributions:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What CLI backends would you want supported next? (e.g., Aider, Open Interpreter, local LLM CLIs)&lt;/li&gt; &lt;li&gt;Any ideas for improving the context-passing system?&lt;/li&gt; &lt;li&gt;What MCP server integrations would be most useful for your workflows?&lt;/li&gt; &lt;li&gt;PRs and issues welcome ‚Äî check out CONTRIBUTING.md in the repo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is a solo side project and I'd really appreciate any suggestions, code reviews, or feature ideas from this community. Not looking for donations ‚Äî just want to build something useful with input from people who actually use these tools daily.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ask149"&gt; /u/ask149 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzy8uu/project_mcp_orchestrator_turn_one_ai_agent_into_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzy8uu/project_mcp_orchestrator_turn_one_ai_agent_into_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzy8uu/project_mcp_orchestrator_turn_one_ai_agent_into_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T07:49:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzwvj6</id>
    <title>Trainable System Router and Industry standard Dual Method Memory System Release</title>
    <updated>2026-02-09T06:28:46+00:00</updated>
    <author>
      <name>/u/daeron-blackFyr</name>
      <uri>https://old.reddit.com/user/daeron-blackFyr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzwvj6/trainable_system_router_and_industry_standard/"&gt; &lt;img alt="Trainable System Router and Industry standard Dual Method Memory System Release" src="https://external-preview.redd.it/MZ1bQ8k89m7DOk17Lq1lfNswsIifwYmWOcpqLKDQHEY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f87f66b160f4ae76bd541dbd19ae89eb0c6ebb85" title="Trainable System Router and Industry standard Dual Method Memory System Release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Another late night weekend update, I have finally pushed the second adition to the SOTA Grade Open Source Toolkit for Industry capabilites on your machine. This yet again, just lime rlhf and the inference optimizations, is aimed at again leveling the playing field and closing the artificially gated and created capability gap between open-source LLM development and closed-door corporate development. No proprietary technology from any leading lab or company was accessed or used for any developments in this codebase.&lt;/p&gt; &lt;p&gt;This is the second, but not certainly not last, attempt to democratize access to these capabilities and ultimately decentralize the modern compute infrastructure. The second addition to the SOTA toolkit is Neural prompt routing with dynamic reasoning depth, tool gating, and multi-template prompt assembly. This comes with pre-made jinja2 templates and a markdown system prompt example. These can be interchanged with any jinja2 prompt templates/tool manifest. Now the 2nd and a complimentary but also standalone system for this release is another SOTA tool a Memory System based on open-data, research, and analysis of open-data for a Production-grade Industry Standard memory system with two forms of memory. This is cross-session memory extraction, semantic storage, and context injection that learns facts, preferences, and patterns from conversations. The third file released is the integrated demo of how these two can work together for the functionally equivalent runtime you normally pay $20-$200 a month for. I have left each however, with the ability to fully run standalone with no degradation to whichever system. All you need to do is copy and paste into your codebase. You now have industry standard innovations, for free that is gatekept behind billions of dollars in investments. Again no proprietary technology was accessed, read, touched or even looked at during the development of this recreation runtime. All research was gathered through open source data, open publications, and discussions. No proprietary innovations were accessed. This entire repository, just as RLHF, uses the Sovereign Anti-Exploitation License.&lt;/p&gt; &lt;p&gt;Expanded Context On &amp;quot;Why&amp;quot; I am doing this:&lt;/p&gt; &lt;p&gt;The infrastructure for modern AI is being hoarded. The same companies that trained on the open web now gate access to the runtime systems that make their models useful. This work was developed alongside the recursion/theoretical work aswell. This toolkit project started with one single goal, decentralize compute and distribute back advancements to level the field between SaaS and OSS. If we can do for free in python, then what is their excuse?&lt;/p&gt; &lt;p&gt;This is practical decentralization. SOTA-tier runtime tooling, local-first, for everyone.&lt;/p&gt; &lt;p&gt;Github Quick Clone and Provenance Links:&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/calisweetleaf/SOTA-Runtime-Core"&gt;https://github.com/calisweetleaf/SOTA-Runtime-Core&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Zenodo: &lt;a href="https://doi.org/10.5281/zenodo.18530654"&gt;https://doi.org/10.5281/zenodo.18530654&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Prior Work (Drop 1 - RLHF): &lt;a href="https://github.com/calisweetleaf/Reinforcement-Learning-Full-Pipeline"&gt;https://github.com/calisweetleaf/Reinforcement-Learning-Full-Pipeline&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Future Notes:&lt;/p&gt; &lt;p&gt;The next release is going to be one of the biggest advancements in this domain that I have developed. A runtime system for fully trained llms, straight from huggingface, that enables self healing guided reasoning for long horizon agentic tasking and an effective infinite context window. This is not rag and there is nocompression algorithm, it is representation mutation. &amp;quot;Entropy, scaffolding, and garlic is all you need.&lt;/p&gt; &lt;p&gt;Keep an eye on my HuggingFace and GitHub - 10 converted local models with these capabilities are coming soon. When the release gets closer I will link them. In the meantime I also am taking suggestions for models the community wants so feel free to message me that. If you do I will try to show you plenty of demos leading to the release. Of course the tools to do this yourselves to any model of your choosing will be possible and has been through an extreme detailed documentation process.&lt;/p&gt; &lt;p&gt;Thank you and I look forward to any questions. Please feel free to engage and let me know if you train or build with these systems. More drops are coming. I greatly appreciate it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/daeron-blackFyr"&gt; /u/daeron-blackFyr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/calisweetleaf/SOTA-Runtime-Core"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzwvj6/trainable_system_router_and_industry_standard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzwvj6/trainable_system_router_and_industry_standard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T06:28:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qznps2</id>
    <title>Comparing the same model with reasoning turned on and off</title>
    <updated>2026-02-08T23:00:51+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm preparing to use Nemotron-3-30B to analyze a huge personal file (close to 1M tokens), and thought I might turn off reasoning so it doesn't go schizo over the sheer amount of content. But I was curious what turning off reasoning would do, so I went looking for benchmarks.&lt;/p&gt; &lt;p&gt;There seems to be very few benchmarks comparing the same model with reasoning on, vs turned off via chat template. I was only able to find 2 places with info on this, Artificial Analysis and UGI Leaderboard. Here's a selection of models and their benchmarks.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Nemotron-3-30B-A30B&lt;/th&gt; &lt;th align="left"&gt;Reasoning&lt;/th&gt; &lt;th align="left"&gt;Non-Reasoning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Terminal Bench Hard&lt;/td&gt; &lt;td align="left"&gt;14%&lt;/td&gt; &lt;td align="left"&gt;12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tau2 Telecom&lt;/td&gt; &lt;td align="left"&gt;41%&lt;/td&gt; &lt;td align="left"&gt;25%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AA-LCR Long Context Reasoning&lt;/td&gt; &lt;td align="left"&gt;34%&lt;/td&gt; &lt;td align="left"&gt;7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AA-Omniscience Accuracy (Knowledge)&lt;/td&gt; &lt;td align="left"&gt;17%&lt;/td&gt; &lt;td align="left"&gt;13%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Humanity's Last Exam&lt;/td&gt; &lt;td align="left"&gt;10.2%&lt;/td&gt; &lt;td align="left"&gt;4.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPQA Diamond (Scientific Reasoning)&lt;/td&gt; &lt;td align="left"&gt;76%&lt;/td&gt; &lt;td align="left"&gt;40%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LiveCodeBench (Coding)&lt;/td&gt; &lt;td align="left"&gt;74%&lt;/td&gt; &lt;td align="left"&gt;36%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SciCode (Coding)&lt;/td&gt; &lt;td align="left"&gt;30%&lt;/td&gt; &lt;td align="left"&gt;23%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IFBench (Instruction Following)&lt;/td&gt; &lt;td align="left"&gt;71%&lt;/td&gt; &lt;td align="left"&gt;38%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AIME 2025&lt;/td&gt; &lt;td align="left"&gt;91%&lt;/td&gt; &lt;td align="left"&gt;13%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GLM-4.7-Flash&lt;/th&gt; &lt;th align="left"&gt;Reasoning&lt;/th&gt; &lt;th align="left"&gt;Non-Reasoning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Terminal Bench Hard&lt;/td&gt; &lt;td align="left"&gt;22%&lt;/td&gt; &lt;td align="left"&gt;4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tau2 Telecom&lt;/td&gt; &lt;td align="left"&gt;99%&lt;/td&gt; &lt;td align="left"&gt;92%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AA-LCR Long Context Reasoning&lt;/td&gt; &lt;td align="left"&gt;35%&lt;/td&gt; &lt;td align="left"&gt;15%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AA-Omniscience Accuracy (Knowledge)&lt;/td&gt; &lt;td align="left"&gt;15%&lt;/td&gt; &lt;td align="left"&gt;12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Humanity's Last Exam&lt;/td&gt; &lt;td align="left"&gt;7.1%&lt;/td&gt; &lt;td align="left"&gt;4.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPQA Diamond (Scientific Reasoning)&lt;/td&gt; &lt;td align="left"&gt;58%&lt;/td&gt; &lt;td align="left"&gt;45%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SciCode (Coding)&lt;/td&gt; &lt;td align="left"&gt;34%&lt;/td&gt; &lt;td align="left"&gt;26%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IFBench (Instruction Following)&lt;/td&gt; &lt;td align="left"&gt;61%&lt;/td&gt; &lt;td align="left"&gt;46%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;DeepSeek V3.2&lt;/th&gt; &lt;th align="left"&gt;Reasoning&lt;/th&gt; &lt;th align="left"&gt;Non-Reasoning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Terminal Bench Hard&lt;/td&gt; &lt;td align="left"&gt;36%&lt;/td&gt; &lt;td align="left"&gt;33%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tau2 Telecom&lt;/td&gt; &lt;td align="left"&gt;91%&lt;/td&gt; &lt;td align="left"&gt;79%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AA-LCR Long Context Reasoning&lt;/td&gt; &lt;td align="left"&gt;65%&lt;/td&gt; &lt;td align="left"&gt;39%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AA-Omniscience Accuracy (Knowledge)&lt;/td&gt; &lt;td align="left"&gt;32%&lt;/td&gt; &lt;td align="left"&gt;23%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Humanity's Last Exam&lt;/td&gt; &lt;td align="left"&gt;22.2%&lt;/td&gt; &lt;td align="left"&gt;10.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPQA Diamond (Scientific Reasoning)&lt;/td&gt; &lt;td align="left"&gt;84%&lt;/td&gt; &lt;td align="left"&gt;65%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LiveCodeBench (Coding)&lt;/td&gt; &lt;td align="left"&gt;86%&lt;/td&gt; &lt;td align="left"&gt;59%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SciCode (Coding)&lt;/td&gt; &lt;td align="left"&gt;39%&lt;/td&gt; &lt;td align="left"&gt;39%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IFBench (Instruction Following)&lt;/td&gt; &lt;td align="left"&gt;61%&lt;/td&gt; &lt;td align="left"&gt;49%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AIME 2025&lt;/td&gt; &lt;td align="left"&gt;92%&lt;/td&gt; &lt;td align="left"&gt;59%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Then there's UGI Leaderboard's NatInt. This is a closed but relatively amateurish intelligence benchmark. (I don't mean this in a disparaging way, it's just a fact that it's 1 guy writing this, vs the thousands of questions created by entire teams for the above benchmarks). Interestingly, the UGI maintainer did a lot of tests in various setups, always turning off reasoning when he gets a chance, and including reasoning on Instruct models (presumably by prompting &amp;quot;think step-by-step&amp;quot;). It's appreciated!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Reasoning NatInt&lt;/th&gt; &lt;th align="left"&gt;Non-Reasoning NatInt&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Ministral-3-14B-Reasoning-2512&lt;/td&gt; &lt;td align="left"&gt;16.33%&lt;/td&gt; &lt;td align="left"&gt;16.35%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ministral-3-14B-Instruct-2512&lt;/td&gt; &lt;td align="left"&gt;18.09%&lt;/td&gt; &lt;td align="left"&gt;16.73%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nemotron-3-30-A3B-BF16&lt;/td&gt; &lt;td align="left"&gt;29.12%&lt;/td&gt; &lt;td align="left"&gt;16.51%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-30B-A3B Thinking=true/false&lt;/td&gt; &lt;td align="left"&gt;19.19%&lt;/td&gt; &lt;td align="left"&gt;15.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.5-Air&lt;/td&gt; &lt;td align="left"&gt;33%&lt;/td&gt; &lt;td align="left"&gt;32.18%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-32B&lt;/td&gt; &lt;td align="left"&gt;30.34%&lt;/td&gt; &lt;td align="left"&gt;32.95%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V3.2&lt;/td&gt; &lt;td align="left"&gt;48.11%&lt;/td&gt; &lt;td align="left"&gt;47.85%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2.5&lt;/td&gt; &lt;td align="left"&gt;62.96%&lt;/td&gt; &lt;td align="left"&gt;60.32%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;It seems like it's a big performance penalty on some models, while being about the same on others. The gap is much bigger on the tougher &amp;quot;replace human workers&amp;quot; corpo benchmarks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qznps2/comparing_the_same_model_with_reasoning_turned_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qznps2/comparing_the_same_model_with_reasoning_turned_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qznps2/comparing_the_same_model_with_reasoning_turned_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T23:00:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzhxd0</id>
    <title>Strix Halo Distributed Cluster (2x Strix Halo, RDMA RoCE v2) benchmarks by kyuz0</title>
    <updated>2026-02-08T19:16:19+00:00</updated>
    <author>
      <name>/u/Relevant-Audience441</name>
      <uri>https://old.reddit.com/user/Relevant-Audience441</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;kyuz0 has been a godsend to the Strix Halo community, they can't be thanked enough!&lt;/p&gt; &lt;p&gt;For their latest escapade, they have built a two-node &lt;strong&gt;AMD Strix Halo&lt;/strong&gt; cluster linked via &lt;strong&gt;Intel E810 (RoCE v2)&lt;/strong&gt; for distributed vLLM inference using Tensor Parallelism.&lt;/p&gt; &lt;p&gt;Here are some benchmarks- &lt;/p&gt; &lt;p&gt;&lt;a href="https://kyuz0.github.io/amd-strix-halo-vllm-toolboxes/"&gt;https://kyuz0.github.io/amd-strix-halo-vllm-toolboxes/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's the setup guide-&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kyuz0/amd-strix-halo-vllm-toolboxes/blob/main/rdma_cluster/setup_guide.md"&gt;https://github.com/kyuz0/amd-strix-halo-vllm-toolboxes/blob/main/rdma_cluster/setup_guide.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's the video that goes with this project-&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=nnB8a3OHS2E"&gt;https://www.youtube.com/watch?v=nnB8a3OHS2E&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Relevant-Audience441"&gt; /u/Relevant-Audience441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzhxd0/strix_halo_distributed_cluster_2x_strix_halo_rdma/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzhxd0/strix_halo_distributed_cluster_2x_strix_halo_rdma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzhxd0/strix_halo_distributed_cluster_2x_strix_halo_rdma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T19:16:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzq3z8</id>
    <title>Lekh AI v2.0 is out ‚Äì Big offline AI update, Better memory and llama GGUF models support. Mac app coming next week.</title>
    <updated>2026-02-09T00:51:19+00:00</updated>
    <author>
      <name>/u/Living_Commercial_10</name>
      <uri>https://old.reddit.com/user/Living_Commercial_10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone&lt;/p&gt; &lt;p&gt;I‚Äôm the solo developer behind &lt;strong&gt;Lekh AI&lt;/strong&gt;, an on-device AI app for iPhone &amp;amp; iPad. I just shipped &lt;strong&gt;v2.0&lt;/strong&gt;, and this release is focused on making local models more flexible, faster, and more reliable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick recap:&lt;/strong&gt; Lekh AI runs LLMs, vision, image generation, and voice &lt;strong&gt;entirely on-device&lt;/strong&gt;. No cloud. No accounts. No subscriptions. Your data stays on your device.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What‚Äôs new in v2.0&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLaMA GGUF support&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Load and run &lt;strong&gt;GGUF LLaMA models&lt;/strong&gt; locally&lt;/li&gt; &lt;li&gt;Much better compatibility with community models&lt;/li&gt; &lt;li&gt;Easier experimentation with different model sizes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Better RAG memory&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Improved recall and relevance&lt;/li&gt; &lt;li&gt;More consistent use of stored context across chats&lt;/li&gt; &lt;li&gt;Fewer ‚Äúwhy did it forget that?‚Äù moments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TTS optimizations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Faster, smoother voice output&lt;/li&gt; &lt;li&gt;Reduced latency and improved stability in longer sessions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;UX &amp;amp; cleanup&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Removed the persistent uncensored-model warning&lt;/li&gt; &lt;li&gt;Cleaner model switching experience&lt;/li&gt; &lt;li&gt;General polish across the app&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Bug fixes &amp;amp; performance improvements&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fewer hiccups during long chats&lt;/li&gt; &lt;li&gt;Better memory management&lt;/li&gt; &lt;li&gt;Overall smoother feel&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Smarter AI &amp;amp; Memory&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Custom AI personas (role-consistent, persistent)&lt;/li&gt; &lt;li&gt;View, edit, and fine-tune RAG memories&lt;/li&gt; &lt;li&gt;Chat summarization&lt;/li&gt; &lt;li&gt;Better RAG integration across chats&lt;/li&gt; &lt;li&gt;Ask the AI about your book progress directly in chat&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;New AI Image Tools (all offline)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AI image editing with &lt;strong&gt;SD 1.5 inpainting&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Ability to load custom models as well&lt;/li&gt; &lt;li&gt;Object remover&lt;/li&gt; &lt;li&gt;Black &amp;amp; white photo colorizer&lt;/li&gt; &lt;li&gt;Photo ‚Üí 3D depth generation&lt;/li&gt; &lt;li&gt;3D splat generator + viewer&lt;/li&gt; &lt;li&gt;Image editing now feels way more ‚ÄúPhotos-app-like‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Documents &amp;amp; Reading&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Improved document &amp;amp; PDF handling&lt;/li&gt; &lt;li&gt;Better long-file performance&lt;/li&gt; &lt;li&gt;More reliable book context awareness&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance &amp;amp; UX&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Background model downloading&lt;/li&gt; &lt;li&gt;Much better memory management (fewer slowdowns)&lt;/li&gt; &lt;li&gt;App size significantly reduced by making FastVLM optional&lt;/li&gt; &lt;li&gt;Improved chat UI (HTML artifacts, cleaner code blocks)&lt;/li&gt; &lt;li&gt;More Siri Shortcuts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Plus:&lt;/strong&gt; lots of bug fixes and stability improvements&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Core features (for anyone new)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Offline LLM chat (Gemma, Qwen, Llama, Mistral, Phi, DeepSeek, OpenELM, more)&lt;/li&gt; &lt;li&gt;Vision: ask questions about images and photos&lt;/li&gt; &lt;li&gt;On-device image generation (SD 1.5 / SDXL)&lt;/li&gt; &lt;li&gt;Voice chat with Kokoro TTS&lt;/li&gt; &lt;li&gt;Local AI server (OpenAI-compatible API over LAN)&lt;/li&gt; &lt;li&gt;iCloud sync (optional, encrypted)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;One-time price: $4.99 - no subscriptions&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What‚Äôs next&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;macOS app ships next week&lt;/strong&gt;, bringing the same fully on-device experience to desktop&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;App Store link:&lt;/strong&gt; &lt;a href="https://apps.apple.com/us/app/lekh-ai/id6757496953"&gt;https://apps.apple.com/us/app/lekh-ai/id6757496953&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm building this very openly, and feedback genuinely shapes the roadmap.&lt;/p&gt; &lt;p&gt;If you‚Äôre into &lt;strong&gt;local AI, privacy-first apps, or running models on Apple devices&lt;/strong&gt;, I‚Äôd love to hear what you think üôè&lt;/p&gt; &lt;p&gt;Happy to answer any technical questions in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Living_Commercial_10"&gt; /u/Living_Commercial_10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzq3z8/lekh_ai_v20_is_out_big_offline_ai_update_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzq3z8/lekh_ai_v20_is_out_big_offline_ai_update_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzq3z8/lekh_ai_v20_is_out_big_offline_ai_update_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T00:51:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzs0h9</id>
    <title>Final Destination, Hallucination Station. (Opus 4.6 hallucinates</title>
    <updated>2026-02-09T02:26:08+00:00</updated>
    <author>
      <name>/u/UnreasonableEconomy</name>
      <uri>https://old.reddit.com/user/UnreasonableEconomy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzs0h9/final_destination_hallucination_station_opus_46/"&gt; &lt;img alt="Final Destination, Hallucination Station. (Opus 4.6 hallucinates" src="https://b.thumbs.redditmedia.com/4J2ROZIwjRUzPsoF71wpUEegN7DPzfyS-gevaPovW4g.jpg" title="Final Destination, Hallucination Station. (Opus 4.6 hallucinates" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit: Ope, ate the title. TBH, IDK how the title should end. &amp;quot;We're all toast?&amp;quot;&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;This is just some napkin math.&lt;/p&gt; &lt;p&gt;Hallucination is of course the biggest thing holding back agentics, and if it's not solved within the next 24 months this whole hype train is going to smash into the buffer stop. It's not looking good.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/525cpl98rdig1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=251ced00f0ee29ede414db448df8f062abd11e5a"&gt;https://preview.redd.it/525cpl98rdig1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=251ced00f0ee29ede414db448df8f062abd11e5a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Of course, local models lag behind by a wide margin, but even if we look at the SOTA (opus 4.6), it's still pretty harrowing.&lt;/p&gt; &lt;p&gt;On page 76 of the 4.6 system card (&lt;a href="https://www-cdn.anthropic.com/0dd865075ad3132672ee0ab40b05a53f14cf5288.pdf"&gt;https://www-cdn.anthropic.com/0dd865075ad3132672ee0ab40b05a53f14cf5288.pdf&lt;/a&gt;) they run SimpleQA, and give the model the option to abstain if it's uncertain. The top is how often the model is right, the bottom is how often it's right - how often it's wrong.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lxe7zoftpdig1.png?width=979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=26d0d2574e47e8310a4ace9de1366bd64b271491"&gt;https://preview.redd.it/lxe7zoftpdig1.png?width=979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=26d0d2574e47e8310a4ace9de1366bd64b271491&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let's interpret this charitably. Let's say the model is correct 50% of the time, and gets a net score of 25%.&lt;/p&gt; &lt;p&gt;That means that out of 100 tries, it gets 50 correct, confidently hallucinates at least 25, and correctly abstains from 25.&lt;/p&gt; &lt;p&gt;That means at least 1 out of 3 answers have no grounded basis, but the model doesn't know that.&lt;/p&gt; &lt;p&gt;In reality, it's much worse. Thinking+Effort: 46.2% correct, 7.8% net. 53.8% wrong, (46.2 - 7.8) = 38.4% confidently hallucinated, (100 - 46.2 - 38.4) 15.4% correctly abstained.&lt;/p&gt; &lt;p&gt;that means that approximately out of 5 times, it will know it doesn't know 2 times and hallucinate 3 times.&lt;/p&gt; &lt;p&gt;That means every time you ask an LLM to double check its' answer (assuming it was wrong because it doesn't know), the likelihood that the new answer is now worse is 60%, and assuming you even gave it an out, it would ask for help 40% of the time.&lt;/p&gt; &lt;p&gt;If you tell it to fix it, and give it tests, the probability that it will hallucinate &lt;em&gt;increases exponentially&lt;/em&gt; 1-(1-0.6)&lt;sup&gt;n,&lt;/sup&gt; and the probability that it will catch itself &lt;em&gt;decreases exponentially&lt;/em&gt; (0.4)&lt;sup&gt;n,&lt;/sup&gt; causing a token churn with zero yield.&lt;/p&gt; &lt;p&gt;This also explains why Thinking+Effort has a lower net yield than just Thinking.&lt;/p&gt; &lt;p&gt;TL;DR: whether a model can do any novel task right is a coin flip. If you give an agent the option to flip again, it'll turn into a gambling addict on your dime.&lt;/p&gt; &lt;p&gt;What we need is a model that reaches a net score &amp;gt;50%. But it looks like we're a long way off from that.&lt;/p&gt; &lt;p&gt;Clawd is just another iteration of autogpt/swarmgpt and all that stuff. When will people learn?&lt;/p&gt; &lt;p&gt;Thanks for coming to my draft of a ted talk.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnreasonableEconomy"&gt; /u/UnreasonableEconomy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzs0h9/final_destination_hallucination_station_opus_46/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzs0h9/final_destination_hallucination_station_opus_46/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzs0h9/final_destination_hallucination_station_opus_46/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T02:26:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzytme</id>
    <title>Caret ‚Äì A terminal tool to inspect and clean massive LLM datasets</title>
    <updated>2026-02-09T08:26:32+00:00</updated>
    <author>
      <name>/u/Mental_Figure_1130</name>
      <uri>https://old.reddit.com/user/Mental_Figure_1130</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzytme/caret_a_terminal_tool_to_inspect_and_clean/"&gt; &lt;img alt="Caret ‚Äì A terminal tool to inspect and clean massive LLM datasets" src="https://preview.redd.it/ip091tcnifig1.png?width=140&amp;amp;height=63&amp;amp;auto=webp&amp;amp;s=7526987c073820123909e1af40dafb10c9ef19d9" title="Caret ‚Äì A terminal tool to inspect and clean massive LLM datasets" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a CLI tool called &lt;a href="https://github.com/rouapps/caret"&gt;Caret&lt;/a&gt; because I was struggling to inspect large pre-training datasets efficiently.&lt;/p&gt; &lt;p&gt;The main issue I had was that opening 10GB+ JSONL or Parquet files usually crashed my editor (VS Code) or used too much RAM. I wanted something that felt like &lt;code&gt;less&lt;/code&gt; but understood the structure of LLM data, specifically for visualizing tokenization and finding bad data.&lt;/p&gt; &lt;p&gt;It‚Äôs written in Rust and uses memory-mapped I/O, so it opens files of basically any size instantly without loading them fully into RAM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Zero-Copy Open:&lt;/strong&gt; Uses &lt;code&gt;mmap&lt;/code&gt; to handle massive files. You can scroll through a 100GB dataset instantly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Token X-Ray:&lt;/strong&gt; Toggles a view that visualizes exactly how your tokenizer (Tiktoken, Llama 3, GPT-2...) is splitting the text (see screenshot).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SimHash Deduplication:&lt;/strong&gt; Uses parallelized SimHash (with hardware &lt;code&gt;POPCNT&lt;/code&gt;) to find near-duplicates in your training data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parquet &amp;amp; CSV Support:&lt;/strong&gt; Handles binary formats natively without needing to convert them to JSONL first.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MCP Server:&lt;/strong&gt; I added an experimental MCP (Model Context Protocol) server. If you use Claude Desktop or Cursor, you can connect it to Caret to &amp;quot;chat&amp;quot; with your local dataset (e.g., &amp;quot;Find me 5 examples of bad JSON formatting in this file&amp;quot;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it works under the hood:&lt;/strong&gt; Instead of reading the whole file, it builds a lightweight index of line offsets and maps the file into virtual memory. When you scroll, it slices the bytes directly from the OS page cache. For remote HuggingFace datasets, it fetches only the parquet metadata footer first and streams row groups on demand, so you don't have to download the full repo to check the data quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Installation:&lt;/strong&gt; If you have Rust installed:&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/rouapps/caret.git cd caret &amp;amp;&amp;amp; cargo run --release -- path/to/data.jsonl &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It‚Äôs still early days, so I‚Äôd appreciate any feedback or issue reports if you try it on your datasets!&lt;/p&gt; &lt;p&gt;Github link: &lt;a href="https://github.com/rouapps/caret"&gt;https://github.com/rouapps/caret&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ip091tcnifig1.png?width=1778&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cff35eda5fa5628659c5b0c7abf2f4903644419b"&gt;https://preview.redd.it/ip091tcnifig1.png?width=1778&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cff35eda5fa5628659c5b0c7abf2f4903644419b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mental_Figure_1130"&gt; /u/Mental_Figure_1130 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzytme/caret_a_terminal_tool_to_inspect_and_clean/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzytme/caret_a_terminal_tool_to_inspect_and_clean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzytme/caret_a_terminal_tool_to_inspect_and_clean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T08:26:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzgvyh</id>
    <title>pwilkin is doing things</title>
    <updated>2026-02-08T18:38:34+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzgvyh/pwilkin_is_doing_things/"&gt; &lt;img alt="pwilkin is doing things" src="https://external-preview.redd.it/LP9lWJIkvOFwEJy7i2edxqBM2iBmROue3pUEdiXyxYg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a45fd4b46acdf1a22c62c7c684471a43354c1397" title="pwilkin is doing things" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19435"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzgvyh/pwilkin_is_doing_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzgvyh/pwilkin_is_doing_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T18:38:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz23pp</id>
    <title>PR opened for Qwen3.5!!</title>
    <updated>2026-02-08T06:57:13+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"&gt; &lt;img alt="PR opened for Qwen3.5!!" src="https://preview.redd.it/r10pwm02y7ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb19e2c9eac9c47e80b6a33b08c10d458c3fb6c0" title="PR opened for Qwen3.5!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/43830/"&gt;https://github.com/huggingface/transformers/pull/43830/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking at the code at &lt;code&gt;src/transformers/models/qwen3_5/modeling_qwen3_5.py&lt;/code&gt;, it looks like Qwen3.5 series will have VLMs right off the bat!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r10pwm02y7ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T06:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz5uww</id>
    <title>Qwen3 Coder Next as first "usable" coding model &lt; 60 GB for me</title>
    <updated>2026-02-08T10:43:59+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried lots of &amp;quot;small&amp;quot; models &amp;lt; 60 GB in the past. GLM 4.5 Air, GLM 4.7 Flash, GPT OSS 20B and 120B, Magistral, Devstral, Apriel Thinker, previous Qwen coders, Seed OSS, QwQ, DeepCoder, DeepSeekCoder, etc. So what's different with Qwen3 Coder Next in OpenCode or in Roo Code with VSCodium?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: The reasoning models would often yet not always produce rather good results. However, now and then they'd enter reasoning loops despite correct sampling settings, leading to no results at all in a large over-night run. Aside from that the sometimes extensive reasoning takes quite some time for the multiple steps that OpenCode or Roo would induce, slowing down interactive work &lt;em&gt;a lot&lt;/em&gt;. Q3CN on the other hand is an instruct MoE model, doesn't have internal thinking loops and is relatively quick at generating tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quality&lt;/strong&gt;: Other models occasionally botched the tool calls of the harness. This one seems to work reliably. Also I finally have the impression that this can handle a moderately complex codebase with a custom client &amp;amp; server, different programming languages, protobuf, and some quirks. It provided good answers to extreme multi-hop questions and made reliable full-stack changes. Well, almost. On Roo Code it was sometimes a bit lazy and needed a reminder to really go deep to achieve correct results. Other models often got lost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context size&lt;/strong&gt;: Coding on larger projects needs context. Most models with standard attention eat all your VRAM for breakfast. With Q3CN having 100k+ context is easy. A few other models also supported that already, yet there were drawbacks in the first two mentioned points.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I run the model this way:&lt;br /&gt; &lt;code&gt;set GGML_CUDA_GRAPH_OPT=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server -m Qwen3-Coder-Next-UD-Q4_K_XL.gguf -ngl 99 -fa on -c 120000 --n-cpu-moe 29 --temp 0 --cache-ram 0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This works well with 24 GB VRAM and 64 GB system RAM when there's (almost) nothing else on the GPU. Yields about 180 TPS prompt processing and 30 TPS generation speed for me.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;temp 0&lt;/code&gt;? Yes, works well for instruct for me, no higher-temp &amp;quot;creativity&amp;quot; needed. Prevents the &lt;em&gt;very occasional&lt;/em&gt; issue that it outputs an unlikely (and incorrect) token when coding.&lt;/li&gt; &lt;li&gt;&lt;code&gt;cache-ram 0&lt;/code&gt;? The cache was supposed to be fast (30 ms), but I saw 3 second query/update times after each request. So I didn't investigate further and disabled it, as it's only one long conversation history in a single slot anyway.&lt;/li&gt; &lt;li&gt;&lt;code&gt;GGML_CUDA_GRAPH_OPT&lt;/code&gt;? Experimental option to get more TPS. Usually works, yet breaks processing with some models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;OpenCode vs. Roo Code&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Both solved things with the model, yet with OpenCode I've seen slightly more correct answers and solutions. But: Roo asks &lt;em&gt;by default&lt;/em&gt; about every single thing, even harmless things like running a syntax check via command line. This can be configured with an easy permission list to not stop the automated flow that often. OpenCode on the other hand just permits everything by default in code mode. One time it encountered an issue, uninstalled and reinstalled packages in an attempt of solving it, removed files and drove itself into a corner by breaking the dev environment. Too autonomous in trying to &amp;quot;get things done&amp;quot;, which doesn't work well on bleeding edge stuff that's not in the training set. Permissions can of course also be configured, but the default is &amp;quot;YOLO&amp;quot;.&lt;/p&gt; &lt;p&gt;Aside from that: Despite running with only a locally hosted model, and having disabled update checks and news downloads, OpenCode (Desktop version) tries to contact a whole lot of IPs on start-up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T10:43:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzrl2g</id>
    <title>Are there any alternatives to Open WebUI that don't have terrible UX?</title>
    <updated>2026-02-09T02:05:01+00:00</updated>
    <author>
      <name>/u/lostmsu</name>
      <uri>https://old.reddit.com/user/lostmsu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Configuring Open WebUI is a nightmare.&lt;/p&gt; &lt;p&gt;Even if you managed to add a tool server and got tools to show up in UI (which is comparable to completing dark brotherhood quest in Skyrim in complexity), you have to enable it every fucking time you start a new chat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lostmsu"&gt; /u/lostmsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzrl2g/are_there_any_alternatives_to_open_webui_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzrl2g/are_there_any_alternatives_to_open_webui_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzrl2g/are_there_any_alternatives_to_open_webui_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T02:05:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzo77z</id>
    <title>MiniMax M2.2 Coming Soon!</title>
    <updated>2026-02-08T23:22:31+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzo77z/minimax_m22_coming_soon/"&gt; &lt;img alt="MiniMax M2.2 Coming Soon!" src="https://preview.redd.it/cj2as13ttcig1.png?width=140&amp;amp;height=19&amp;amp;auto=webp&amp;amp;s=0c0420fddf7b3160e28c4a7e5bea9abb03314341" title="MiniMax M2.2 Coming Soon!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It found on their website code&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cj2as13ttcig1.png?width=825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9492b73dd14c581e30b35a5e64062f4ac7356a3f"&gt;https://preview.redd.it/cj2as13ttcig1.png?width=825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9492b73dd14c581e30b35a5e64062f4ac7356a3f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://cdn.hailuo.ai/mmx-agent/prod-web-va-0.1.746/_next/static/chunks/app/(pages"&gt;https://cdn.hailuo.ai/mmx-agent/prod-web-va-0.1.746/_next/static/chunks/app/(pages)/(base)/page-0cfae9566c3e528b.js&lt;/a&gt;/(base)/page-0cfae9566c3e528b.js)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzo77z/minimax_m22_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzo77z/minimax_m22_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzo77z/minimax_m22_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T23:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzwzqj</id>
    <title>ministral-3-3b is great model, give it a shot!</title>
    <updated>2026-02-09T06:35:18+00:00</updated>
    <author>
      <name>/u/FeiX7</name>
      <uri>https://old.reddit.com/user/FeiX7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I was experimenting the small models that can do tool calls effectively and can fit in 6GB Vram and I found ministral-3-3b.&lt;/p&gt; &lt;p&gt;Currently using it's instruct version with Q8 and it's accuracy to run tools written in skills md is generous.&lt;/p&gt; &lt;p&gt;I am curious about your use cases of this model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeiX7"&gt; /u/FeiX7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzwzqj/ministral33b_is_great_model_give_it_a_shot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzwzqj/ministral33b_is_great_model_give_it_a_shot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzwzqj/ministral33b_is_great_model_give_it_a_shot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T06:35:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzsbn9</id>
    <title>StepFun is preparing a "bigger surprise" for Chinese New Year, and will also release Step-3.5-Flash-Base.</title>
    <updated>2026-02-09T02:40:54+00:00</updated>
    <author>
      <name>/u/MadPelmewka</name>
      <uri>https://old.reddit.com/user/MadPelmewka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzsbn9/stepfun_is_preparing_a_bigger_surprise_for/"&gt; &lt;img alt="StepFun is preparing a &amp;quot;bigger surprise&amp;quot; for Chinese New Year, and will also release Step-3.5-Flash-Base." src="https://preview.redd.it/zytph079tdig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c50a154f20b8f8f0e56f8e7c6353847588c73a1" title="StepFun is preparing a &amp;quot;bigger surprise&amp;quot; for Chinese New Year, and will also release Step-3.5-Flash-Base." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step-3.5-Flash/discussions/21#698941a597b7256a083f94b6"&gt;https://huggingface.co/stepfun-ai/Step-3.5-Flash/discussions/21#698941a597b7256a083f94b6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They also mentioned discussions with Nvidia regarding NVFP4 and responded to questions about excessive token usage by stating they are working on it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadPelmewka"&gt; /u/MadPelmewka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zytph079tdig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzsbn9/stepfun_is_preparing_a_bigger_surprise_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzsbn9/stepfun_is_preparing_a_bigger_surprise_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T02:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzstxq</id>
    <title>3 New Models for Marxist-Leninist Revolutionary Theory - T-34 Division Army</title>
    <updated>2026-02-09T03:04:56+00:00</updated>
    <author>
      <name>/u/FizzarolliAI</name>
      <uri>https://old.reddit.com/user/FizzarolliAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Comrades and comrades-to-be, we are proud to drop three new SFT-only models‚Äîbuilt &lt;em&gt;strictly&lt;/em&gt; on working-class data and prompts‚Äîinto the field:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/WokeAI/Tankie-LFM2.5-1.2B-SFT-v1"&gt;&lt;strong&gt;Tankie-LFM2.5-1.2B-SFT-v1&lt;/strong&gt;&lt;/a&gt;: LFM2.5 backbone, 4 epochs on the Tankie Dataset.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/WokeAI/Tankie-NB-3B-SFT-v1"&gt;&lt;strong&gt;Tankie-NB-3B-SFT-v1&lt;/strong&gt;&lt;/a&gt;: NanBeige4-3B core, 4 epochs as well.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/WokeAI/Tankie-DPE-12B-SFT-v2"&gt;&lt;strong&gt;Tankie-DPE-12B-SFT-v2&lt;/strong&gt;&lt;/a&gt;: Dan‚Äôs PersonalityEngine 12B, only two epochs on the Tankie dataset.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All models are completely free on the Hugging Face Hub. You don‚Äôt need a token, an invite, or an NDA; they run on any CPU, GPU, or TPU. The only thing we ask is that you share findings and critiques back to the collective, so we can continue tightening our line.&lt;/p&gt; &lt;p&gt;We built them for one purpose: to sharpen ideological clarity, expose ruling-class myths, and give revolutionary cadre another tool in the battle against liberal co-option and technocratic paternalism. Try them on imperialism 101, strike planning, or debunking the myth of ‚Äúneutral‚Äù AI‚Äîsee which one handles your local context best.&lt;/p&gt; &lt;p&gt;Solidarity!~&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FizzarolliAI"&gt; /u/FizzarolliAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzstxq/3_new_models_for_marxistleninist_revolutionary/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzstxq/3_new_models_for_marxistleninist_revolutionary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzstxq/3_new_models_for_marxistleninist_revolutionary/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T03:04:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzz0vr</id>
    <title>GLM 5 is coming! spotted on vllm PR</title>
    <updated>2026-02-09T08:39:31+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"&gt; &lt;img alt="GLM 5 is coming! spotted on vllm PR" src="https://preview.redd.it/285aias7lfig1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=5a644c4fce313f2c4b8643b1d8a7931145a54db1" title="GLM 5 is coming! spotted on vllm PR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/285aias7lfig1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5287959d193fad4f96c5c80ec8b7546a7dcbe023"&gt;https://preview.redd.it/285aias7lfig1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5287959d193fad4f96c5c80ec8b7546a7dcbe023&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/34124"&gt;https://github.com/vllm-project/vllm/pull/34124&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T08:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzppr7</id>
    <title>Qwen3.5 Support Merged in llama.cpp</title>
    <updated>2026-02-09T00:32:33+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzppr7/qwen35_support_merged_in_llamacpp/"&gt; &lt;img alt="Qwen3.5 Support Merged in llama.cpp" src="https://external-preview.redd.it/LP9lWJIkvOFwEJy7i2edxqBM2iBmROue3pUEdiXyxYg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a45fd4b46acdf1a22c62c7c684471a43354c1397" title="Qwen3.5 Support Merged in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19435"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzppr7/qwen35_support_merged_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzppr7/qwen35_support_merged_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T00:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzjbw2</id>
    <title>I built a rough .gguf LLM visualizer</title>
    <updated>2026-02-08T20:08:31+00:00</updated>
    <author>
      <name>/u/sultan_papagani</name>
      <uri>https://old.reddit.com/user/sultan_papagani</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzjbw2/i_built_a_rough_gguf_llm_visualizer/"&gt; &lt;img alt="I built a rough .gguf LLM visualizer" src="https://b.thumbs.redditmedia.com/kcxBxykQQ15O2Oz4xDuJc0i9OygqR7aRSLKKBTm5a5E.jpg" title="I built a rough .gguf LLM visualizer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hacked together a small tool that lets you upload a .gguf file and visualize its internals in a 3D-ish way (layers / neurons / connections). The original goal was just to see what‚Äôs inside these models instead of treating them like a black box. &lt;/p&gt; &lt;p&gt;That said, my version is pretty rough, and I‚Äôm very aware that someone who actually knows what they‚Äôre doing could‚Äôve built something way better :p &lt;/p&gt; &lt;p&gt;So I figured I‚Äôd ask here: Does something like this already exist, but done properly? If yes, I‚Äôd much rather use that For reference, this is really good: &lt;a href="https://bbycroft.net/llm"&gt;https://bbycroft.net/llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚Ä¶but you can‚Äôt upload new LLMs.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sultan_papagani"&gt; /u/sultan_papagani &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qzjbw2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzjbw2/i_built_a_rough_gguf_llm_visualizer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzjbw2/i_built_a_rough_gguf_llm_visualizer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T20:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzxfzr</id>
    <title>Some times is the wrong time</title>
    <updated>2026-02-09T07:01:13+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzxfzr/some_times_is_the_wrong_time/"&gt; &lt;img alt="Some times is the wrong time" src="https://preview.redd.it/7lcfomik3fig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed063735d9cc0f30769725caaffc2c6359a5fcb4" title="Some times is the wrong time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7lcfomik3fig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzxfzr/some_times_is_the_wrong_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzxfzr/some_times_is_the_wrong_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T07:01:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
