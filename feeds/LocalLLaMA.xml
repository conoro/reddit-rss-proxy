<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-18T19:23:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ppwylg</id>
    <title>What's your favourite local coding model?</title>
    <updated>2025-12-18T17:40:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwylg/whats_your_favourite_local_coding_model/"&gt; &lt;img alt="What's your favourite local coding model?" src="https://preview.redd.it/q8ipunvr008g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4e54f5d47898a4570fb732cd3140edf2551267b" title="What's your favourite local coding model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried (with Mistral Vibe Cli)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;mistralai_Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf - works but it's kind of slow for coding &lt;/li&gt; &lt;li&gt;nvidia_Nemotron-3-Nano-30B-A3B-Q8_0.gguf - text generation is fast, but the actual coding is slow and often incorrect&lt;/li&gt; &lt;li&gt;Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf - works correctly and it's fast&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What else would you recommend? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q8ipunvr008g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwylg/whats_your_favourite_local_coding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwylg/whats_your_favourite_local_coding_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppqp83</id>
    <title>memory systems benchmarks seem way inflated, anyone else notice this?</title>
    <updated>2025-12-18T13:23:23+00:00</updated>
    <author>
      <name>/u/FeelingWatercress871</name>
      <uri>https://old.reddit.com/user/FeelingWatercress871</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;been trying to add memory to my local llama setup and all these memory systems claim crazy good numbers but when i actually test them the results are trash.&lt;/p&gt; &lt;p&gt;started with mem0 cause everyone talks about it. their website says 80%+ accuracy but when i hooked it up to my local setup i got like 64%. thought maybe i screwed up the integration so i spent weeks debugging. turns out their marketing numbers use some special evaluation setup thats not available in their actual api.&lt;/p&gt; &lt;p&gt;tried zep next. same bs - they claim 85% but i got 72%. their github has evaluation code but it uses old api versions and some preprocessing steps that arent documented anywhere.&lt;/p&gt; &lt;p&gt;getting pretty annoyed at this point so i decided to test a bunch more to see if everyone is just making up numbers:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;System &lt;/th&gt; &lt;th align="left"&gt;Their Claims&lt;/th&gt; &lt;th align="left"&gt;What I Got&lt;/th&gt; &lt;th align="left"&gt;Gap &lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Zep &lt;/td&gt; &lt;td align="left"&gt;~85% &lt;/td&gt; &lt;td align="left"&gt;72% &lt;/td&gt; &lt;td align="left"&gt;-13%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mem0 &lt;/td&gt; &lt;td align="left"&gt;~80% &lt;/td&gt; &lt;td align="left"&gt;64% &lt;/td&gt; &lt;td align="left"&gt;-16%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MemGPT &lt;/td&gt; &lt;td align="left"&gt;~85% &lt;/td&gt; &lt;td align="left"&gt;70% &lt;/td&gt; &lt;td align="left"&gt;-15%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;gaps are huge. either im doing something really wrong or these companies are just inflating their numbers for marketing.&lt;/p&gt; &lt;p&gt;stuff i noticed while testing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;most use private test data so you cant verify their claims&lt;/li&gt; &lt;li&gt;when they do share evaluation code its usually broken or uses old apis&lt;/li&gt; &lt;li&gt;&amp;quot;fair comparison&amp;quot; usually means they optimized everything for their own system&lt;/li&gt; &lt;li&gt;temporal stuff (remembering things from weeks ago) is universally terrible but nobody mentions this&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;tried to keep my testing fair. used the same dataset for all systems, same local llama model (llama 3.1 8b) for generating answers, same scoring method. still got way lower numbers than what they advertise.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# basic test loop i used for question in test_questions: memories = memory_system.search(question, user_id=&amp;quot;test_user&amp;quot;) context = format_context(memories) answer = local_llm.generate(question, context) score = check_answer_quality(answer, expected_answer) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;honestly starting to think this whole memory system space is just marketing hype. like everyone just slaps &amp;quot;AI memory&amp;quot; on their rag implementation and calls it revolutionary.&lt;/p&gt; &lt;p&gt;did find one open source project (github.com/EverMind-AI/EverMemOS) that actually tests multiple systems on the same benchmarks. their setup looks way more complex than what im doing but at least they seem honest about the results. they get higher numbers for their own system but also show that other systems perform closer to what i found.&lt;/p&gt; &lt;p&gt;am i missing something obvious or are these benchmark numbers just complete bs?&lt;/p&gt; &lt;p&gt;running everything locally with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;llama 3.1 8b q4_k_m&lt;/li&gt; &lt;li&gt;32gb ram, rtx 4090&lt;/li&gt; &lt;li&gt;ubuntu 22.04&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;really want to get memory working well but hard to know which direction to go when all the marketing claims seem fake.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeelingWatercress871"&gt; /u/FeelingWatercress871 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppqp83/memory_systems_benchmarks_seem_way_inflated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppqp83/memory_systems_benchmarks_seem_way_inflated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppqp83/memory_systems_benchmarks_seem_way_inflated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T13:23:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppvdy5</id>
    <title>Putting together a repo for 21 Days of Building a Small Language Model</title>
    <updated>2025-12-18T16:38:45+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to say thanks to &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, a bunch of you have been following my &lt;em&gt;21 Days of Building a Small Language Model&lt;/em&gt; posts.&lt;br /&gt; I‚Äôve now organized everything into a GitHub repo so it‚Äôs easier to track and revisit.&lt;br /&gt; Thanks again for the encouragement&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ideaweaver-ai/21-Days-of-Building-a-Small-Language-Model/"&gt;https://github.com/ideaweaver-ai/21-Days-of-Building-a-Small-Language-Model/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppvdy5/putting_together_a_repo_for_21_days_of_building_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppvdy5/putting_together_a_repo_for_21_days_of_building_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppvdy5/putting_together_a_repo_for_21_days_of_building_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T16:38:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppx93g</id>
    <title>VibeVoice 7B and 1.5B FastAPI Wrapper</title>
    <updated>2025-12-18T17:51:20+00:00</updated>
    <author>
      <name>/u/TommarrA</name>
      <uri>https://old.reddit.com/user/TommarrA</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppx93g/vibevoice_7b_and_15b_fastapi_wrapper/"&gt; &lt;img alt="VibeVoice 7B and 1.5B FastAPI Wrapper" src="https://external-preview.redd.it/ocq5FpDlatga69140E_bI6uHAd--cmL-kjurFzGrpzw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4bfc93e7c6b2a4bfa1956edd98031f239c71437" title="VibeVoice 7B and 1.5B FastAPI Wrapper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had created a fast API wrapper for the original VibeVoice model (7B and 1.5B)&lt;/p&gt; &lt;p&gt;It allows you to use custom voices unlike the current iteration of VibeVoice that has Microsoft generated voice models.&lt;/p&gt; &lt;p&gt;It works well for my ebook narration use case so thought I would share with the community too.&lt;/p&gt; &lt;p&gt;Thanks to folks who had made a backup of the original code.&lt;/p&gt; &lt;p&gt;I will eventually build in the ability to use the 0.5B model as well but current iteration only support and 7B and 1.5B models&lt;/p&gt; &lt;p&gt;Let me know how it works for your use cases&lt;/p&gt; &lt;p&gt;Docker is the preferred deployment model - tested on Ubuntu.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TommarrA"&gt; /u/TommarrA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ncoder-ai/VibeVoice-FastAPI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppx93g/vibevoice_7b_and_15b_fastapi_wrapper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppx93g/vibevoice_7b_and_15b_fastapi_wrapper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp6jhq</id>
    <title>Hey, LocalLLaMa. We need to talk...</title>
    <updated>2025-12-17T20:04:07+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I look on the front page and I see people who have spent time and effort to make something, and they share it willingly. They are getting no upvotes.&lt;/p&gt; &lt;p&gt;We are here because we are &lt;em&gt;local&lt;/em&gt; and we are &lt;em&gt;open source&lt;/em&gt;. Those things &lt;em&gt;depend on people who give us things&lt;/em&gt;, and they don't ask for anything in return, but they &lt;em&gt;need&lt;/em&gt; something in return or they will stop.&lt;/p&gt; &lt;p&gt;Pop your head into the smaller posts where someone is showing work they have done. Give honest and constructive feedback. UPVOTE IT.&lt;/p&gt; &lt;p&gt;The project may be terrible -- encourage them to grow by telling them how they can make it better. &lt;/p&gt; &lt;p&gt;The project may be awesome. They would love to hear how awesome it is. But if you use it, then they would love 100 times more to hear how you use it and how it helps you.&lt;/p&gt; &lt;p&gt;Engage with the people who share their things, and not just with the entertainment. &lt;/p&gt; &lt;p&gt;It take so little effort but it makes so much difference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T20:04:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppntz9</id>
    <title>GLM-V GGUF is out!</title>
    <updated>2025-12-18T10:45:27+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"&gt; &lt;img alt="GLM-V GGUF is out!" src="https://b.thumbs.redditmedia.com/FZv8pFFPpwEa4qKxoMpu3mE3J-5QIWLQlQCBViK_yvg.jpg" title="GLM-V GGUF is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/ggml-org/glm-v"&gt;https://huggingface.co/collections/ggml-org/glm-v&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/klip0rudzx7g1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50865e4b0f1c5479683b40e8dc6fe68df02f03db"&gt;https://preview.redd.it/klip0rudzx7g1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50865e4b0f1c5479683b40e8dc6fe68df02f03db&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T10:45:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pper90</id>
    <title>MiraTTS: High quality and fast TTS model</title>
    <updated>2025-12-18T01:55:55+00:00</updated>
    <author>
      <name>/u/SplitNice1982</name>
      <uri>https://old.reddit.com/user/SplitNice1982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MiraTTS&lt;/strong&gt; is a high quality LLM based TTS finetune that can generate audio at &lt;strong&gt;100x&lt;/strong&gt; realtime and generate realistic and clear 48khz speech! I heavily optimized it using Lmdeploy and used &lt;a href="https://github.com/ysharma3501/FlashSR"&gt;FlashSR&lt;/a&gt; to enhance the audio.&lt;/p&gt; &lt;h1&gt;Benefits of this repo&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Incredibly fast: As stated before, over &lt;strong&gt;100x&lt;/strong&gt; realtime!&lt;/li&gt; &lt;li&gt;High quality: Generates realistic and 48khz speech, &lt;strong&gt;much&lt;/strong&gt; clearer then most TTS models and it‚Äôs base model.&lt;/li&gt; &lt;li&gt;Memory efficient: Works with even 6gb vram gpus!&lt;/li&gt; &lt;li&gt;Low latency: Possible latency low as &lt;strong&gt;150ms&lt;/strong&gt;, I have not released code for streaming yet but will release soon.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basic multilingual versions are already supported, I just need to clean up code. Multispeaker is still in progress, but should come soon. If you have any other issues, I will be happy to fix them.&lt;/p&gt; &lt;p&gt;Github link: &lt;a href="https://github.com/ysharma3501/MiraTTS"&gt;https://github.com/ysharma3501/MiraTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model link: &lt;a href="https://huggingface.co/YatharthS/MiraTTS"&gt;https://huggingface.co/YatharthS/MiraTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog explaining llm tts models: &lt;a href="https://huggingface.co/blog/YatharthS/llm-tts-models"&gt;https://huggingface.co/blog/YatharthS/llm-tts-models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Stars/Likes would be appreciated very much, thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplitNice1982"&gt; /u/SplitNice1982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T01:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1poy0lb</id>
    <title>Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.</title>
    <updated>2025-12-17T14:33:13+00:00</updated>
    <author>
      <name>/u/themixtergames</name>
      <uri>https://old.reddit.com/user/themixtergames</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"&gt; &lt;img alt="Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds." src="https://external-preview.redd.it/YWpkODI1NDF4cjdnMbxNGAI-puPRf-AP3cgrLxlreCeM4kV742La4OIIHHvj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1dded6d1cdcc956e0916d9926400982637f4d7c" title="Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/apple/ml-sharp"&gt;https://github.com/apple/ml-sharp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2512.10685"&gt;https://arxiv.org/abs/2512.10685&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themixtergames"&gt; /u/themixtergames &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l2mp7b31xr7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T14:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp8vo4</id>
    <title>Nvidia plans heavy cuts to GPU supply in early 2026</title>
    <updated>2025-12-17T21:37:13+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://overclock3d.net/news/gpu-displays/nvidia-plans-heavy-cuts-to-gpu-supply-in-early-2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T21:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppnmca</id>
    <title>AI is great at answers, but terrible at uncertainty and that‚Äôs a bigger problem than hallucinations</title>
    <updated>2025-12-18T10:32:06+00:00</updated>
    <author>
      <name>/u/Mediocre_Common_4126</name>
      <uri>https://old.reddit.com/user/Mediocre_Common_4126</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most of the criticism around LLMs focuses on hallucinations, wrong facts, or confidence issues but I think the deeper problem is AI is optimized to sound &lt;em&gt;certain&lt;/em&gt;&lt;/p&gt; &lt;p&gt;In real work, the hardest moments are not when you need an answer. They‚Äôre when you don‚Äôt even know what the right question is yet&lt;/p&gt; &lt;p&gt;The messy parts: half-formed thoughts + contradictory signals + ‚Äúthis feels wrong but I don‚Äôt know why‚Äù backtracking changing your mind mid-way&lt;/p&gt; &lt;p&gt;Humans spend a huge amount of time operating in uncertainty, we explore, we reframe, we circle around the problem&lt;/p&gt; &lt;p&gt;Most training data skips that phase entirely, we feed models clean prompts and polished conclusions, then expect them to handle ambiguity well&lt;/p&gt; &lt;p&gt;That‚Äôs why LLMs often feel impressive but fragile, they jump to conclusions too fast, they don‚Äôt linger in confusion, they optimize for closure, not exploration.&lt;/p&gt; &lt;p&gt;What‚Äôs interesting is that the best human collaborators are the opposite. They slow you down, they ask annoying clarifying questions, they surface blind spots instead of hiding them behind confident language&lt;/p&gt; &lt;p&gt;This made me rethink how AI tools should be built, less ‚Äúgive me the answer‚Äù, more ‚Äúhelp me think without collapsing the space too early‚Äù&lt;/p&gt; &lt;p&gt;Interesting if others have noticed this too. Especially people building tools on top of LLMs or using them for real decision making&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mediocre_Common_4126"&gt; /u/Mediocre_Common_4126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnmca/ai_is_great_at_answers_but_terrible_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnmca/ai_is_great_at_answers_but_terrible_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnmca/ai_is_great_at_answers_but_terrible_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T10:32:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppq8pi</id>
    <title>Z-Image is now the default image model on HuggingChat</title>
    <updated>2025-12-18T13:01:20+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"&gt; &lt;img alt="Z-Image is now the default image model on HuggingChat" src="https://b.thumbs.redditmedia.com/q04f8-Hq7gSnGXdIq-IW8V70b-2l8sOF10WS2JF_Kks.jpg" title="Z-Image is now the default image model on HuggingChat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Victor M (Hugging Face) on ùïè: &lt;a href="https://x.com/victormustar/status/2001629770329858391?s=20"&gt;https://x.com/victormustar/status/2001629770329858391&lt;/a&gt;&lt;br /&gt; HuggingChat: &lt;a href="https://huggingface.co/chat/"&gt;https://huggingface.co/chat/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ppq8pi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T13:01:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppv68d</id>
    <title>[Blog from Hugging Face] Tokenization in Transformers v5: Simpler, Clearer, and More Modular</title>
    <updated>2025-12-18T16:30:13+00:00</updated>
    <author>
      <name>/u/Disastrous-Work-1632</name>
      <uri>https://old.reddit.com/user/Disastrous-Work-1632</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppv68d/blog_from_hugging_face_tokenization_in/"&gt; &lt;img alt="[Blog from Hugging Face] Tokenization in Transformers v5: Simpler, Clearer, and More Modular" src="https://preview.redd.it/ggovkfrtoz7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d79c723d2ee425a8d0fe89be6ee4871ab9baba7b" title="[Blog from Hugging Face] Tokenization in Transformers v5: Simpler, Clearer, and More Modular" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This blog explains how tokenization works in Transformers and why v5 is a major redesign, with clearer internals, a clean class hierarchy, and a single fast backend. It‚Äôs a practical guide for anyone who wants to understand, customize, or train model-specific tokenizers instead of treating them as black boxes.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/blog/tokenizers"&gt;https://huggingface.co/blog/tokenizers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Work-1632"&gt; /u/Disastrous-Work-1632 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ggovkfrtoz7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppv68d/blog_from_hugging_face_tokenization_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppv68d/blog_from_hugging_face_tokenization_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T16:30:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppm9xm</id>
    <title>NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano</title>
    <updated>2025-12-18T09:03:01+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"&gt; &lt;img alt="NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano" src="https://external-preview.redd.it/i9rG1D6xcH_2B9JTT5Ak5wKM4ExK483hNq6oNeOkRNo.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa9a037e77932298ed68f09b93c42491dd8ab8e0" title="NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T09:03:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppstef</id>
    <title>Thoughts on recent small (under 20B) models</title>
    <updated>2025-12-18T14:55:45+00:00</updated>
    <author>
      <name>/u/surubel</name>
      <uri>https://old.reddit.com/user/surubel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently we're been graced with quite a few small (under 20B) models and I've tried most of them.&lt;/p&gt; &lt;p&gt;The initial benchmarks seemed a bit too good to be true, but I've tried them regardless. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;RNJ-1: this one had probably the most &amp;quot;honest&amp;quot; benchmark results. About as good as QWEN3 8B, which seems fair from my limited usage. &lt;/li&gt; &lt;li&gt;GLM 4.6v Flash: even after the latest llama.cpp update and Unsloth quantization I still have mixed feelings. Can't get it to think in English, but produces decent results. Either there are still issues with llama.cpp / quantization or it's a bit benchmaxxed&lt;/li&gt; &lt;li&gt;Ministral 3 14B: solid vision capabilities, but tends to overthink a lot. Occasionally messes up tool calls. A bit unreliable.&lt;/li&gt; &lt;li&gt;Nemotron cascade 14B. Similar to Ministral 3 14B tends to overthink a lot. Although it has great coding benchmarks, I couldn't get good results out of it. GPT OSS 20B and QWEN3 8B VL seem to give better results. This was the most underwhelming for me.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Did anyone get different results from these models? Am I missing something?&lt;/p&gt; &lt;p&gt;Seems like GPT OSS 20B and QWEN3 8B VL are still the most reliable small models, at least for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/surubel"&gt; /u/surubel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T14:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppongx</id>
    <title>Fast on-device Speech-to-text for Home Assistant (open source)</title>
    <updated>2025-12-18T11:34:57+00:00</updated>
    <author>
      <name>/u/banafo</name>
      <uri>https://old.reddit.com/user/banafo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"&gt; &lt;img alt="Fast on-device Speech-to-text for Home Assistant (open source)" src="https://external-preview.redd.it/6PRNLd3TFMw1DCfYP7618_nVHzwQRPRrDRjMqQg7XGU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cff7a166c2a85ced6d24604f32dc307cf599fedf" title="Fast on-device Speech-to-text for Home Assistant (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just released &lt;a href="https://github.com/orgs/kroko-ai/repositories"&gt;kroko-onnx-home-assistant &lt;/a&gt; is a &lt;strong&gt;local&lt;/strong&gt; streaming STT pipeline for home assistant.&lt;/p&gt; &lt;p&gt;It's currently just a fork of the excellent &lt;a href="https://github.com/ptbsare/sherpa-onnx-tts-stt"&gt;https://github.com/ptbsare/sherpa-onnx-tts-stt&lt;/a&gt; with support for our models added, hopefully it will be accepted in the main project. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;High quality&lt;/li&gt; &lt;li&gt;Real streaming (partial results, low latency)&lt;/li&gt; &lt;li&gt;100% local &amp;amp; privacy-first&lt;/li&gt; &lt;li&gt;optimized for fast CPU inference, even in low resources raspberry pi's&lt;/li&gt; &lt;li&gt;Does not require additional VAD&lt;/li&gt; &lt;li&gt;Home Assistant integration&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo:&lt;br /&gt; [&lt;a href="https://github.com/kroko-ai/kroko-onnx-home-assistant%5D()"&gt;https://github.com/kroko-ai/kroko-onnx-home-assistant]()&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to test the model quality before installing: the huggingface models running in the browser is the easiest way: &lt;a href="https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm"&gt;https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A big thanks to:&lt;br /&gt; - NaggingDaivy on discord, for the assistance.&lt;br /&gt; - the sherpa-onnx-tts-stt team for adding support for streaming models in record time.&lt;/p&gt; &lt;p&gt;Want us to integrate with your favorite open source project ? Contact us on discord:&lt;br /&gt; &lt;a href="https://discord.gg/TEbfnC7b"&gt;https://discord.gg/TEbfnC7b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some releases you may have missed:&lt;br /&gt; - Freewitch Module: &lt;a href="https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko"&gt;https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko&lt;/a&gt;&lt;br /&gt; - Asterisk Module: &lt;a href="https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko"&gt;https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko&lt;/a&gt;&lt;br /&gt; - Full Asterisk based voicebot running with Kroko streaming models: &lt;a href="https://github.com/hkjarral/Asterisk-AI-Voice-Agent"&gt;https://github.com/hkjarral/Asterisk-AI-Voice-Agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are still working on the main models, code and documentation as well, but held up a bit with urgent paid work deadlines, more coming there soon too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/banafo"&gt; /u/banafo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kroko-ai/kroko-onnx-home-assistant"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T11:34:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwfw3</id>
    <title>Key Highlights of Google's New Open Model, FunctionGemma</title>
    <updated>2025-12-18T17:19:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwfw3/key_highlights_of_googles_new_open_model/"&gt; &lt;img alt="Key Highlights of Google's New Open Model, FunctionGemma" src="https://external-preview.redd.it/f3OilJIGGaBNRWiWULRSz5XOCY6YipQN2XKt886yVr0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0508cadd0c3629606d28322469362c690c52148b" title="Key Highlights of Google's New Open Model, FunctionGemma" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;[1] Function-calling specialized&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Built on the &lt;em&gt;Gemma 3 270M&lt;/em&gt; foundation and fine-tuned for function calling tasks, turning natural language into structured function calls for API/tool execution.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[2] Lightweight &amp;amp; open&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;A compact, open-weight model (~270 M parameters) designed for efficient use on resource-constrained hardware (laptops, desktops, cloud, edge) and democratizing access to advanced function-call agents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[3] 32K token context&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports up to ~32 k token context window, like other 270M Gemma models, making it suitable for moderately long prompts and complex sequences.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[4] Fine-tuning friendly&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intended to be further fine-tuned for specific custom actions, improving accuracy and customization for particular domains or workflows (e.g., mobile actions, custom APIs).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model - &lt;a href="https://huggingface.co/google/functiongemma-270m-it"&gt;https://huggingface.co/google/functiongemma-270m-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model GGUF - &lt;a href="https://huggingface.co/unsloth/functiongemma-270m-it-GGUF"&gt;https://huggingface.co/unsloth/functiongemma-270m-it-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/google/functiongemma-270m-it"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwfw3/key_highlights_of_googles_new_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwfw3/key_highlights_of_googles_new_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppu35l</id>
    <title>Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting.</title>
    <updated>2025-12-18T15:47:20+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"&gt; &lt;img alt="Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting." src="https://b.thumbs.redditmedia.com/CTdnDnhEVXKhvcC_xn7Fo04JbVfjTe3Wx_yk_R9kVRw.jpg" title="Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://mistral.ai/news/mistral-ocr-3"&gt;https://mistral.ai/news/mistral-ocr-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mistral OCR 3 sets new benchmarks in both accuracy and efficiency, outperforming enterprise document processing solutions as well as AI-native OCR.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ppu35l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:47:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppvdf5</id>
    <title>google/functiongemma-270m-it - Lightweight Model that transforms command into function calling</title>
    <updated>2025-12-18T16:38:10+00:00</updated>
    <author>
      <name>/u/Varterove_muke</name>
      <uri>https://old.reddit.com/user/Varterove_muke</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/google/functiongemma-270m-it"&gt;https://huggingface.co/google/functiongemma-270m-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As far as I understand, this model is for moble phones for a Google Assistent like aplications.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Varterove_muke"&gt; /u/Varterove_muke &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppvdf5/googlefunctiongemma270mit_lightweight_model_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppvdf5/googlefunctiongemma270mit_lightweight_model_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppvdf5/googlefunctiongemma270mit_lightweight_model_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T16:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwqki</id>
    <title>FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!</title>
    <updated>2025-12-18T17:31:14+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"&gt; &lt;img alt="FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!" src="https://external-preview.redd.it/MXBiZjQzZTd4ejdnMYei2aDWEA5WccTd6X2Ceg7tONZcTZmqT6GgxYYEX2jv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c19158fb01b0628ef68c006e673dc09cd2cf081" title="FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Google released FunctionGemma, a lightweight (270M), open foundation model built for creating specialized function calling models! To test it out, I built a small game where you use natural language to solve physics simulation puzzles. It runs entirely locally in your browser on WebGPU, powered by Transformers.js.&lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; - Game: &lt;a href="https://huggingface.co/spaces/webml-community/FunctionGemma-Physics-Playground"&gt;https://huggingface.co/spaces/webml-community/FunctionGemma-Physics-Playground&lt;/a&gt;&lt;br /&gt; - FunctionGemma on Hugging Face: &lt;a href="https://huggingface.co/google/functiongemma-270m-it"&gt;https://huggingface.co/google/functiongemma-270m-it&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k33t7zd7xz7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:31:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppu4lc</id>
    <title>Fine-tuning Qwen3 at home to respond to any prompt with a dad joke</title>
    <updated>2025-12-18T15:48:58+00:00</updated>
    <author>
      <name>/u/InvadersMustLive</name>
      <uri>https://old.reddit.com/user/InvadersMustLive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"&gt; &lt;img alt="Fine-tuning Qwen3 at home to respond to any prompt with a dad joke" src="https://external-preview.redd.it/aeJXUJD-EG13fwr7w155noLxr7JTSfAKwf9XG0w-u3s.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9431c18c37e750b69f2ab16532111dd97d789f41" title="Fine-tuning Qwen3 at home to respond to any prompt with a dad joke" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InvadersMustLive"&gt; /u/InvadersMustLive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nixiesearch.substack.com/p/fine-tuning-qwen3-at-home-to-respond"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppt1xb</id>
    <title>Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction</title>
    <updated>2025-12-18T15:05:22+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"&gt; &lt;img alt="Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction" src="https://preview.redd.it/go7lager9z7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96b4f63fa1cdd2136e6c82f35c609cc6cc1ead9c" title="Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/facebook/map-anything-v1"&gt;https://huggingface.co/facebook/map-anything-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It supports 12+ tasks like multi-view stereo and SfM in a single feed-forward pass&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/go7lager9z7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:05:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppy9wn</id>
    <title>Waiting on Gemma 4</title>
    <updated>2025-12-18T18:30:50+00:00</updated>
    <author>
      <name>/u/QuantityGullible4092</name>
      <uri>https://old.reddit.com/user/QuantityGullible4092</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppy9wn/waiting_on_gemma_4/"&gt; &lt;img alt="Waiting on Gemma 4" src="https://preview.redd.it/sbvx6scga08g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e051f9cc3796f2181cbade90df0b0acb63e937f" title="Waiting on Gemma 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuantityGullible4092"&gt; /u/QuantityGullible4092 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sbvx6scga08g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppy9wn/waiting_on_gemma_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppy9wn/waiting_on_gemma_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T18:30:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppun3v</id>
    <title>Google's Gemma models family</title>
    <updated>2025-12-18T16:09:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"&gt; &lt;img alt="Google's Gemma models family" src="https://preview.redd.it/59w0vja4lz7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dd2d66ee23d4078bf31aba81cdeecc769669af4" title="Google's Gemma models family" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/59w0vja4lz7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T16:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pniwfj</id>
    <title>Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams.</title>
    <updated>2025-12-15T21:02:55+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt; &lt;img alt="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." src="https://b.thumbs.redditmedia.com/bsv34WIHZXC49Az9mFES5lSIAtaQ2CuLZJ4dCaLxsEY.jpg" title="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre researchers and engineers from Ai2, the nonprofit AI lab. We recently announced:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2&lt;/strong&gt;‚Äîopen multimodal models for video + images that can return grounded answers (pixel coordinates + timestamps), trained with open datasets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3&lt;/strong&gt;‚Äîa family of fully open language models (7B‚Äì32B) with Base/Instruct/Thinking variants, long‚Äëcontext support, open training recipes &amp;amp; checkpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ask us anything about local inference, training mixes &amp;amp; our truly open approach, long‚Äëcontext, grounded video QA/tracking, and real‚Äëworld deployment.&lt;/p&gt; &lt;p&gt;Participating in the AMA:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Ranjay Krishna ( &lt;a href="/u/ranjaykrishna"&gt;u/ranjaykrishna&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Zixian Ma ( &lt;a href="/u/Frequent_Rooster2980"&gt;u/Frequent_Rooster2980&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Chris Clark ( &lt;a href="/u/mostly_reasonable"&gt;u/mostly_reasonable&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Jieyu Zhang ( &lt;a href="/u/Jealous_Programmer51"&gt;u/Jealous_Programmer51&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Rohun Tripathi ( &lt;a href="/u/darkerWind"&gt;u/darkerWind&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Kyle Lo ( &lt;a href="/u/klstats"&gt;u/klstats&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Allyson Ettinger ( &lt;a href="/u/aeclang"&gt;u/aeclang&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Finbarr Timbers ( &lt;a href="/u/fnbr"&gt;u/fnbr&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Faeze Brahman ( &lt;a href="/u/faebrhn"&gt;u/faebrhn&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôll be live from &lt;strong&gt;1pm&lt;/strong&gt; to &lt;strong&gt;2pm PST.&lt;/strong&gt; Read up on our latest releases below, and feel welcome to jump in anytime!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ñ∂Ô∏è &lt;strong&gt;Try in the Playground:&lt;/strong&gt; &lt;a href="https://playground.allenai.org"&gt;https://playground.allenai.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;‚¨áÔ∏è &lt;strong&gt;Download&lt;/strong&gt;: &lt;a href="https://huggingface.co/collections/allenai/molmo2"&gt;https://huggingface.co/collections/allenai/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìù &lt;strong&gt;Blog&lt;/strong&gt;: &lt;a href="https://allenai.org/blog/molmo2"&gt;https://allenai.org/blog/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìÑReport: &lt;a href="https://allenai.org/papers/molmo2"&gt;https://allenai.org/papers/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üíª &lt;strong&gt;API coming soon&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ü´Ü PROOF:&lt;/strong&gt; &lt;a href="https://x.com/allen_ai/status/2000692253606514828"&gt;https://x.com/allen_ai/status/2000692253606514828&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Join us on Reddit&lt;/strong&gt; &lt;a href="/r/allenai"&gt;r/allenai&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Join Ai2 on Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/6vWDHyTCQV"&gt;https://discord.gg/6vWDHyTCQV&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8"&gt;https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thank you everyone for the kind words and great questions! This AMA has ended as of 2pm PST (5pm EST) on Dec. 16.&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.gg/6vWDHyTCQV"&gt;Join Ai2 on Discord&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:02:55+00:00</published>
  </entry>
</feed>
