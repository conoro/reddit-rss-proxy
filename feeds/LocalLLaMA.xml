<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-10T07:07:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ot6f78</id>
    <title>Quick check - are these the only LLM building blocks?</title>
    <updated>2025-11-10T06:17:20+00:00</updated>
    <author>
      <name>/u/Individual-Library-1</name>
      <uri>https://old.reddit.com/user/Individual-Library-1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working with LLMs for a while now. My understanding is there are basically 4 things - Classification, Summarization, Chat, and Extraction. Chain them together and you get Agents/Workflows.&lt;/p&gt; &lt;p&gt;Am I missing something obvious here? Trying to explain this to both customers and fellow developers and want to make sure I'm not oversimplifying.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual-Library-1"&gt; /u/Individual-Library-1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot6f78/quick_check_are_these_the_only_llm_building_blocks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot6f78/quick_check_are_these_the_only_llm_building_blocks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot6f78/quick_check_are_these_the_only_llm_building_blocks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T06:17:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot301h</id>
    <title>Can I use Qwen 3 coder 30b with a M4 Macbook Pro 48GB</title>
    <updated>2025-11-10T03:11:32+00:00</updated>
    <author>
      <name>/u/thereisnospooongeek</name>
      <uri>https://old.reddit.com/user/thereisnospooongeek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also, Are there any websites where I can check the token rate per each macbook or popular models?&lt;/p&gt; &lt;p&gt;I'm planning to buy the below model, Just wanted to check how will the performance be?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apple M4 Pro chip with 12‚Äëcore CPU, 16‚Äëcore GPU, 16‚Äëcore Neural Engine&lt;/li&gt; &lt;li&gt;48GB unified memory&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thereisnospooongeek"&gt; /u/thereisnospooongeek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot301h/can_i_use_qwen_3_coder_30b_with_a_m4_macbook_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot301h/can_i_use_qwen_3_coder_30b_with_a_m4_macbook_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot301h/can_i_use_qwen_3_coder_30b_with_a_m4_macbook_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T03:11:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot3pbq</id>
    <title>API to MCP Server</title>
    <updated>2025-11-10T03:46:47+00:00</updated>
    <author>
      <name>/u/meetrais</name>
      <uri>https://old.reddit.com/user/meetrais</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you want to develop enterprise grade agentic apps then most likely you need to make use of existing APIs. Best way to give access of your APIs to your agents is through MCP Servers.&lt;/p&gt; &lt;p&gt;My below GitHub repo has comprehensive guide to create MCP Servers/proxy for your existing APIs using products/platforms like AWS, GCP, MS Azure and Postman.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/meetrais/api-to-mcp-server"&gt;https://github.com/meetrais/api-to-mcp-server&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/meetrais"&gt; /u/meetrais &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3pbq/api_to_mcp_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3pbq/api_to_mcp_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3pbq/api_to_mcp_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T03:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot541p</id>
    <title>Local LLaMA model for RTX5090</title>
    <updated>2025-11-10T05:02:10+00:00</updated>
    <author>
      <name>/u/Cuaternion</name>
      <uri>https://old.reddit.com/user/Cuaternion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have the RTX5090 card, I want to run a local LLM with ChatRTX, what model do you recommend I install? Frankly, I'm going to use it to summarize documents and classify images. Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cuaternion"&gt; /u/Cuaternion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot541p/local_llama_model_for_rtx5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot541p/local_llama_model_for_rtx5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot541p/local_llama_model_for_rtx5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T05:02:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1osmnlp</id>
    <title>CodeWiki: Research-Grade Repository Documentation at Scale [Open Source]</title>
    <updated>2025-11-09T15:49:05+00:00</updated>
    <author>
      <name>/u/Prize_Cost_7706</name>
      <uri>https://old.reddit.com/user/Prize_Cost_7706</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osmnlp/codewiki_researchgrade_repository_documentation/"&gt; &lt;img alt="CodeWiki: Research-Grade Repository Documentation at Scale [Open Source]" src="https://external-preview.redd.it/OWYxbGQ4MHg1OTBnMQhj1jeEZTm3kJ-2tjF6W5cKTD-ayqoaa3bxlqaqIxQQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9da75c2f2ce7867337a918970b57e5d6235e9ee4" title="CodeWiki: Research-Grade Repository Documentation at Scale [Open Source]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; communities! I'm excited to share &lt;strong&gt;CodeWiki&lt;/strong&gt;, our newly published research project from FSoft-AI4Code that tackles automated repository-level documentation generation. After seeing DeepWiki and its open-source implementations, we thought the community might appreciate a different approach backed by academic research.&lt;/p&gt; &lt;h1&gt;What is CodeWiki?&lt;/h1&gt; &lt;p&gt;CodeWiki is the first &lt;strong&gt;semi-agentic framework&lt;/strong&gt; specifically designed for comprehensive, repository-level documentation across 7 programming languages (Python, Java, JavaScript, TypeScript, C, C++, C#). Currently submitted to ACL ARR 2025. &lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/FSoft-AI4Code/CodeWiki"&gt;FSoft-AI4Code/CodeWiki&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;How is CodeWiki Different from DeepWiki?&lt;/h1&gt; &lt;p&gt;I've researched both &lt;a href="https://github.com/AsyncFuncAI/deepwiki-open"&gt;AsyncFuncAI/deepwiki-open&lt;/a&gt; and &lt;a href="https://github.com/AIDotNet/OpenDeepWiki"&gt;AIDotNet/OpenDeepWiki&lt;/a&gt;, and here's an honest comparison:&lt;/p&gt; &lt;h1&gt;CodeWiki's Unique Approach:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Hierarchical Decomposition with Dependency Analysis&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Uses static analysis + AST parsing (Tree-Sitter) to build dependency graphs&lt;/li&gt; &lt;li&gt;Identifies architectural entry points and recursively partitions modules&lt;/li&gt; &lt;li&gt;Maintains architectural coherence while scaling to repositories of any size&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Recursive Agentic Processing with Dynamic Delegation&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Agents can dynamically delegate complex sub-modules to specialized sub-agents- Bounded complexity handling through recursive bottom-up processing&lt;/li&gt; &lt;li&gt;Cross-module coherence via intelligent reference management&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Research-Backed Evaluation (CodeWikiBench)&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;First benchmark specifically for repository-level documentation&lt;/li&gt; &lt;li&gt;Hierarchical rubric generation from official docs- Multi-model agentic assessment with reliability metrics&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Outperforms closed-source DeepWiki by 4.73% on average&lt;/strong&gt; (68.79% vs 64.06%)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Differences:&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;CodeWiki&lt;/th&gt; &lt;th align="left"&gt;DeepWiki (Open Source)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Core Focus&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Architectural understanding &amp;amp; scalability&lt;/td&gt; &lt;td align="left"&gt;Quick documentation generation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Dependency-driven hierarchical decomposition&lt;/td&gt; &lt;td align="left"&gt;Direct code analysis&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Agent System&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Recursive delegation with specialized sub-agents&lt;/td&gt; &lt;td align="left"&gt;Single-pass generation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Academic benchmark (CodeWikiBench)&lt;/td&gt; &lt;td align="left"&gt;User-facing features&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Performance Highlights&lt;/h1&gt; &lt;p&gt;On 21 diverse repositories (86K to 1.4M LOC):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;TypeScript&lt;/strong&gt;: +18.54% over DeepWiki&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: +9.41% over DeepWiki&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scripting languages avg&lt;/strong&gt;: 79.14% (vs DeepWiki's 68.67%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Consistent cross-language&lt;/strong&gt; generalization&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's Next?&lt;/h1&gt; &lt;p&gt;We are actively working on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enhanced systems language support&lt;/li&gt; &lt;li&gt;Multi-version documentation tracking&lt;/li&gt; &lt;li&gt;Downstream SE task integration (code migration, bug localization, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear your thoughts, especially from folks who've tried the DeepWiki implementations! What features matter most for automated documentation in your workflows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prize_Cost_7706"&gt; /u/Prize_Cost_7706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/98rdo80x590g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osmnlp/codewiki_researchgrade_repository_documentation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osmnlp/codewiki_researchgrade_repository_documentation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T15:49:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1osj4mb</id>
    <title>Worth the switch from Claude to GLM 4.6 for my coding side hustle?</title>
    <updated>2025-11-09T13:16:58+00:00</updated>
    <author>
      <name>/u/Ok_Investigator_5036</name>
      <uri>https://old.reddit.com/user/Ok_Investigator_5036</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been freelancing web development projects for about 8 months now, mostly custom dashboards, client portals, and admin panels. The economics are tough because clients always want &amp;quot;simple&amp;quot; projects that turn into months of iteration hell. (Never trust anything to be &amp;quot;simple&amp;quot;)&lt;/p&gt; &lt;p&gt;I started using Claude API for rapid prototyping and client demos. Problem is my margins were getting narrow, especially when a client would request their fifth redesign of a data visualization component or want to &amp;quot;just tweak&amp;quot; the entire authentication flow.&lt;/p&gt; &lt;p&gt;Someone in a dev Discord mentioned using GLM-4.6 with Claude Code. They were getting 55% off first year, so GLM Coding Pro works out to $13.5/month vs Claude Pro at $20+, with 3x usage quota.&lt;/p&gt; &lt;p&gt;I've tested GLM-4.6's coding output. It seems on par with Claude for most tasks, but with 3x the usage quota. We're talking 600 prompts every 5 hours vs Claude Max's ~200.&lt;/p&gt; &lt;p&gt;My typical project flow:&lt;/p&gt; &lt;p&gt;- Client consultation and mockups&lt;/p&gt; &lt;p&gt;- Use AI to scaffold React components and API routes&lt;/p&gt; &lt;p&gt;- Rapid iteration on UI/UX (this is where the 3x quota matters)&lt;/p&gt; &lt;p&gt;- Testing, refactoring, deployment&lt;/p&gt; &lt;p&gt;Last month I landed three projects: a SaaS dashboard with Stripe integration and two smaller automation tools. But some months it's just one or two projects with endless revision rounds.&lt;/p&gt; &lt;p&gt;Right now my prompt usage is manageable, but I've had months where client iterations alone hit thousands of prompts, especially when they're A/B testing different UI approaches or want real-time previews of changes.&lt;/p&gt; &lt;p&gt;For me, the limiting factor isn't base capability (GLM-4.6 ‚âà Claude quality), but having the quota to iterate without stressing about costs.&lt;/p&gt; &lt;p&gt;Wondering how you guys optimizing your AI coding setup costs? With all the client demands and iteration cycles, seems smart to go for affordable with high limits.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Investigator_5036"&gt; /u/Ok_Investigator_5036 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osj4mb/worth_the_switch_from_claude_to_glm_46_for_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osj4mb/worth_the_switch_from_claude_to_glm_46_for_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osj4mb/worth_the_switch_from_claude_to_glm_46_for_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T13:16:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1osiog7</id>
    <title>Qwen3-VL works really good with Zoom-in Tool</title>
    <updated>2025-11-09T12:56:03+00:00</updated>
    <author>
      <name>/u/indigos661</name>
      <uri>https://old.reddit.com/user/indigos661</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osiog7/qwen3vl_works_really_good_with_zoomin_tool/"&gt; &lt;img alt="Qwen3-VL works really good with Zoom-in Tool" src="https://a.thumbs.redditmedia.com/GlsFc4qF0jvYKaVjwuK5hhN3xFcCV40Gl5A3-CDlYE4.jpg" title="Qwen3-VL works really good with Zoom-in Tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While Qwen3-VL-30B-A3B(Q6_ud) performs better than previous open-source models in general image recognition, it still has issues with hallucinations and inaccurate recognition. &lt;/p&gt; &lt;p&gt;However, with the zoom_in tool the situation is completely different. On my own frontend implementation with zoom_in, Qwen3-VL can zoom in on the image, significantly improving the accuracy of content recognition. For those who haven't tried it, qwen team has released a reference implementation: &lt;a href="https://github.com/QwenLM/Qwen-Agent/blob/main/examples/cookbook_think_with_images.ipynb"&gt;https://github.com/QwenLM/Qwen-Agent/blob/main/examples/cookbook_think_with_images.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n6zw8zpfa80g1.png?width=1513&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bffdedb3be6155771d24adcaf1df358854043e2"&gt;https://preview.redd.it/n6zw8zpfa80g1.png?width=1513&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bffdedb3be6155771d24adcaf1df358854043e2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are using Qwen3-VL, I strongly recommend using it with this tool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/indigos661"&gt; /u/indigos661 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osiog7/qwen3vl_works_really_good_with_zoomin_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osiog7/qwen3vl_works_really_good_with_zoomin_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osiog7/qwen3vl_works_really_good_with_zoomin_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T12:56:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot2eqd</id>
    <title>[Research] 31 % perplexity drop on 8.4 M transformer model using a lightweight periodic regulator ‚Äî looking for replication on stronger GPUs</title>
    <updated>2025-11-10T02:42:30+00:00</updated>
    <author>
      <name>/u/freeky78</name>
      <uri>https://old.reddit.com/user/freeky78</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I ran a controlled training experiment on an 8.4 M-parameter transformer model and observed a consistent **31 % perplexity reduction** compared to baseline after 2 000 steps.&lt;/p&gt; &lt;p&gt;üìä Full metrics &amp;amp; logs: &lt;a href="https://limewire.com/d/j7jDI#OceCXHWNhG"&gt;https://limewire.com/d/j7jDI#OceCXHWNhG&lt;/a&gt;&lt;/p&gt; &lt;p&gt;**Setup**&lt;/p&gt; &lt;p&gt;- Model: small LM (~8.4 M params)&lt;/p&gt; &lt;p&gt;- GPU: RTX 5070&lt;/p&gt; &lt;p&gt;- Optimizer: AdamW, lr = 2e-6, warmup = 200, grad-clip = 1.0&lt;/p&gt; &lt;p&gt;- Sequence = 256, batch = 8 √ó GA 4&lt;/p&gt; &lt;p&gt;- Seed = 41&lt;/p&gt; &lt;p&gt;- Modification: added a compact periodic regulator in the optimizer update (‚âà 0.07 % extra params)&lt;/p&gt; &lt;p&gt;**Result**&lt;/p&gt; &lt;p&gt;| Metric | Baseline | Regulated | Œî |&lt;/p&gt; &lt;p&gt;|---------|-----------|-----------|---|&lt;/p&gt; &lt;p&gt;| eval CE | 6.731 | 6.360 | ‚àí0.371 |&lt;/p&gt; &lt;p&gt;| eval PPL | 838.17 | **578.49 (‚àí31 %)** |&lt;/p&gt; &lt;p&gt;| stability Œ≤ | ‚Äî | 0.91 |&lt;/p&gt; &lt;p&gt;Same data, same seed, no architecture changes. &lt;/p&gt; &lt;p&gt;The effect is reproducible and stable.&lt;/p&gt; &lt;p&gt;**Why post here**&lt;/p&gt; &lt;p&gt;Looking for:&lt;/p&gt; &lt;p&gt;- community replication on larger GPUs (A100 / L40S / H100)&lt;/p&gt; &lt;p&gt;- discussion about scaling behaviour and scheduler-level interventions&lt;/p&gt; &lt;p&gt;- any pointers to similar experiments you may have seen&lt;/p&gt; &lt;p&gt;I‚Äôll share the Python scripts and configs (ready-to-run) with anyone who wants to test. &lt;/p&gt; &lt;p&gt;The full repo isn‚Äôt public yet but will follow once results are replicated.&lt;/p&gt; &lt;p&gt;Thanks for reading and for any feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freeky78"&gt; /u/freeky78 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot2eqd/research_31_perplexity_drop_on_84_m_transformer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot2eqd/research_31_perplexity_drop_on_84_m_transformer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot2eqd/research_31_perplexity_drop_on_84_m_transformer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T02:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1osqscj</id>
    <title>LM Studio unlocked for "unsupported" hardware ‚Äî Testers wanted!</title>
    <updated>2025-11-09T18:30:25+00:00</updated>
    <author>
      <name>/u/TheSpicyBoi123</name>
      <uri>https://old.reddit.com/user/TheSpicyBoi123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osqscj/lm_studio_unlocked_for_unsupported_hardware/"&gt; &lt;img alt="LM Studio unlocked for &amp;quot;unsupported&amp;quot; hardware ‚Äî Testers wanted!" src="https://b.thumbs.redditmedia.com/Ar2gzGOrS93zpzzfv2Hdae30Ff1to_7JvKq-h-ba6cU.jpg" title="LM Studio unlocked for &amp;quot;unsupported&amp;quot; hardware ‚Äî Testers wanted!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;Quick update ‚Äî a &lt;strong&gt;simple in situ patch&lt;/strong&gt; was found (see GitHub), and the &lt;strong&gt;newest versions of the backends&lt;/strong&gt; are now released for &amp;quot;unsupported&amp;quot; hardware.&lt;/p&gt; &lt;p&gt;Since the last post, &lt;strong&gt;major refinements&lt;/strong&gt; have been made: performance, compatibility, and build stability have all improved. &lt;/p&gt; &lt;p&gt;Here‚Äôs the current testing status:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚úÖ &lt;strong&gt;AVX1 CPU builds:&lt;/strong&gt; working (confirmed working, Ivy Bridge Xeons)&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;AVX1 Vulkan builds:&lt;/strong&gt; working (confirmed working, Ivy Bridge Xeons + Tesla k40 GPUs)&lt;/li&gt; &lt;li&gt;‚ùì &lt;strong&gt;AVX1 CUDA builds:&lt;/strong&gt; untested (no compatible hardware yet)&lt;/li&gt; &lt;li&gt;‚ùì &lt;strong&gt;Non-AVX experimental builds:&lt;/strong&gt; untested (no compatible hardware yet)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôd love for more people to &lt;strong&gt;try the patch instructions on their own architectures&lt;/strong&gt; and share results ‚Äî especially if you have &lt;strong&gt;newer NVIDIA GPUs&lt;/strong&gt; or &lt;strong&gt;non-AVX CPUs&lt;/strong&gt; (like first-gen Intel Core).&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://github.com/theIvanR/lmstudio-unlocked-backend"&gt;https://github.com/theIvanR/lmstudio-unlocked-backend&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My test setup is dual Ivy Bridge Xeons with Tesla K40 GPUs&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7v3vd9ldx90g1.png?width=1106&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58ae1582a47823f049f86ae91ebe2ae368a9b22a"&gt;https://preview.redd.it/7v3vd9ldx90g1.png?width=1106&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58ae1582a47823f049f86ae91ebe2ae368a9b22a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ou8639ofx90g1.png?width=1041&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15f853146d4adde2e4dec84aa76a24b17a5eab3c"&gt;https://preview.redd.it/ou8639ofx90g1.png?width=1041&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15f853146d4adde2e4dec84aa76a24b17a5eab3c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Brief install instructions:&lt;br /&gt; - navigate to backends folder. ex C:\Users\Admin\.lmstudio\extensions\backends&lt;br /&gt; - (recommended for clean install) delete everything except &amp;quot;vendor&amp;quot; folder&lt;br /&gt; - drop contents from compressed backend of your choice&lt;/p&gt; &lt;p&gt;- select it in LM Studio runtimes and enjoy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheSpicyBoi123"&gt; /u/TheSpicyBoi123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osqscj/lm_studio_unlocked_for_unsupported_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osqscj/lm_studio_unlocked_for_unsupported_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osqscj/lm_studio_unlocked_for_unsupported_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T18:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot3xiy</id>
    <title>when did tesla p40s get boost? or did anyone test them on latest moe models?</title>
    <updated>2025-11-10T03:58:50+00:00</updated>
    <author>
      <name>/u/pharrowking</name>
      <uri>https://old.reddit.com/user/pharrowking</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ive been sitting here fuming over ram/gpu prices over the last few months, while everything gets more expensive especially for used hardware on ebay, i've been stuck with my 4 Tesla p40s for awhile. and i never once thought to check if the latest MOE models run well on tesla p40. because i remember my tesla p40s were useless and slow and only got me 2-3 tokens/sec on llama 70B models. &lt;/p&gt; &lt;p&gt;then the other day i said to myself i'm just gonna load the qwen3 30B-A3B coder model and see what happens. the Q4 quant fits fully in vram of the 4 gpus. &lt;/p&gt; &lt;p&gt;well i was quite surprised. i got 53 tokens per second generation speed with qwen3 coder . &lt;/p&gt; &lt;p&gt;i was like oh wow! because i remember the other day i watched a random youtube video of a guy with 5090 getting 48 tokens/sec on the same model, but some his model was running in cpu ram. i also cant remember which quant he used. &lt;/p&gt; &lt;p&gt;so i went and tried downloading a Q2 quant of minimax M2, and that very large model is netting me 19-23 tokens per second of generation speed and 67-71 tokens of processing. &lt;/p&gt; &lt;p&gt;heres an example output with minimax m2 running across all 4 tesla p40s:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 2521.31 ms / 174 tokens ( 14.49 ms per token, 69.01 tokens per second) eval time = 144947.40 ms / 3156 tokens ( 45.93 ms per token, 21.77 tokens per second) total time = 147468.70 ms / 3330 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;these speeds surprised me so much i just ordered 4 more p40s because they are so cheap compared to everything else i plan to use the Q4 quant of minimax m2 with 8 of them. &lt;/p&gt; &lt;p&gt;did something happen recently to make them faster or is this just an unexpected outcome of latest advancements? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pharrowking"&gt; /u/pharrowking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3xiy/when_did_tesla_p40s_get_boost_or_did_anyone_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3xiy/when_did_tesla_p40s_get_boost_or_did_anyone_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3xiy/when_did_tesla_p40s_get_boost_or_did_anyone_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T03:58:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1osxv3y</id>
    <title>built an open-source, AI-native alternative to n8n that outputs clean TypeScript code workflows</title>
    <updated>2025-11-09T23:11:26+00:00</updated>
    <author>
      <name>/u/Informal-Salad-375</name>
      <uri>https://old.reddit.com/user/Informal-Salad-375</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osxv3y/built_an_opensource_ainative_alternative_to_n8n/"&gt; &lt;img alt="built an open-source, AI-native alternative to n8n that outputs clean TypeScript code workflows" src="https://external-preview.redd.it/XQlwNsoGJe2oyHLPY--H7xoqHWnheK5nDgKNk7rhxzQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=941b214538393eaa8fca52a4ada2508045b1ba68" title="built an open-source, AI-native alternative to n8n that outputs clean TypeScript code workflows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey everyone,&lt;/p&gt; &lt;p&gt;Like many of you, I've used workflow automation tools like n8n, zapier etc. they're ok for simpler flows, but I always felt frustrated by the limitations of their proprietary JSON-based nodes. Debugging is a pain, and there's no way to extend into code. &lt;/p&gt; &lt;p&gt;So, I built &lt;strong&gt;Bubble Lab&lt;/strong&gt;: an open-source, typescript-first workflow automation platform, here's how its different: &lt;/p&gt; &lt;p&gt;1/ &lt;strong&gt;prompt to workflow:&lt;/strong&gt; the typescript infra allows for deep compatibility with AI, so you can build/amend workflows with natural language. Our agent orchestrates our composable &lt;strong&gt;bubbles&lt;/strong&gt; (integrations, tools) into a production-ready workflow&lt;/p&gt; &lt;p&gt;2/ &lt;strong&gt;full observability &amp;amp; debugging&lt;/strong&gt;: Because every workflow is compiled with end-to-end type safety and has built-in traceability with rich logs, you can actually see what's happening under the hood&lt;/p&gt; &lt;p&gt;3/ &lt;strong&gt;real code, not JSON blobs&lt;/strong&gt;: Bubble Lab workflows are built in Typescript code. This means you can own it, extend it in your IDE, add it to your existing CI/CD pipelines, and run it anywhere. No more being locked into a proprietary format. &lt;/p&gt; &lt;p&gt;check out our repo (stars are hugely appreciated!), and lmk if you have any feedback or questions!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Informal-Salad-375"&gt; /u/Informal-Salad-375 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/bubblelabai/BubbleLab"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osxv3y/built_an_opensource_ainative_alternative_to_n8n/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osxv3y/built_an_opensource_ainative_alternative_to_n8n/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T23:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1osw6ki</id>
    <title>[Release] Pre-built llama-cpp-python wheels for Blackwell/Ada/Ampere/Turing, up to CUDA 13.0 &amp; Python 3.13 (Windows x64)</title>
    <updated>2025-11-09T22:02:05+00:00</updated>
    <author>
      <name>/u/dougeeai</name>
      <uri>https://old.reddit.com/user/dougeeai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building llama-cpp-python with CUDA on Windows can be a pain. So I embraced the suck and pre-compiled 40 wheels for 4 Nvidia architectures across 4 versions of Python and 3 versions of CUDA.&lt;/p&gt; &lt;p&gt;Figured these might be useful if you want to spin up GGUFs rapidly on Windows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's included:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RTX 50/40/30/20 series support (Blackwell, Ada, Ampere, Turing) &lt;/li&gt; &lt;li&gt;Python 3.10, 3.11, 3.12, 3.13 &lt;/li&gt; &lt;li&gt;CUDA 11.8, 12.1, 13.0 (Blackwell only compiled for CUDA 13)&lt;/li&gt; &lt;li&gt;llama-cpp-python 0.3.16 &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Download:&lt;/strong&gt; &lt;a href="https://github.com/dougeeai/llama-cpp-python-wheels"&gt;https://github.com/dougeeai/llama-cpp-python-wheels&lt;/a&gt; &lt;/p&gt; &lt;p&gt;No Visual Studio. No CUDA Toolkit. Just pip install and run. Windows only for now. Linux wheels coming soon if there's interest. Open to feedback on what other configs would be helpful.&lt;/p&gt; &lt;p&gt;Thanks for letting me post, long time listener, first time caller.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dougeeai"&gt; /u/dougeeai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osw6ki/release_prebuilt_llamacpppython_wheels_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osw6ki/release_prebuilt_llamacpppython_wheels_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osw6ki/release_prebuilt_llamacpppython_wheels_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T22:02:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oss784</id>
    <title>Strix Halo inference Cluster</title>
    <updated>2025-11-09T19:25:06+00:00</updated>
    <author>
      <name>/u/sub_RedditTor</name>
      <uri>https://old.reddit.com/user/sub_RedditTor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oss784/strix_halo_inference_cluster/"&gt; &lt;img alt="Strix Halo inference Cluster" src="https://external-preview.redd.it/QLldEh6cHckh0zu3VOF5RuY9ywGiZCt_x-CXw1nKwvM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6be666f0103fd1f705f54f78d0ee69bc9405d6dc" title="Strix Halo inference Cluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sub_RedditTor"&gt; /u/sub_RedditTor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/0cIcth224hk?si=IfW5yysNbNWUDvFx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oss784/strix_halo_inference_cluster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oss784/strix_halo_inference_cluster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T19:25:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot67nn</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-11-10T06:04:53+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot67nn/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last week in Multimodal AI - Local Edition" src="https://a.thumbs.redditmedia.com/_ehDePF5xr3odzacIrNpvwUbVHDsuf9cAhwbOLzsNz8.jpg" title="Last week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI. Here are the local/edge highlights from this week:&lt;/p&gt; &lt;p&gt;Rolling Forcing - Real-Time Streaming Video on 1 GPU&lt;br /&gt; ‚Ä¢ Generates multi-minute video interactively with joint multi-frame denoising.&lt;br /&gt; ‚Ä¢ Anchors temporal context for stability without heavy clusters.&lt;br /&gt; ‚Ä¢ &lt;a href="https://kunhao-liu.github.io/Rolling_Forcing_Webpage/"&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2509.25161"&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/TencentARC/RollingForcing"&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://huggingface.co/TencentARC/RollingForcing"&gt;&lt;strong&gt;Hugging Face&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ot67nn/video/q45gljk2ed0g1/player"&gt;https://reddit.com/link/1ot67nn/video/q45gljk2ed0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Step-Audio-EditX (3B) - Text-Driven Audio Editing&lt;br /&gt; ‚Ä¢ Controls emotion, style, breaths, laughs via prompts.&lt;br /&gt; ‚Ä¢ Runs on a single GPU; open weights for local pipelines.&lt;br /&gt; ‚Ä¢ &lt;a href="https://stepaudiollm.github.io/step-audio-editx/"&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2511.03601"&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/stepfun-ai/Step-Audio-EditX"&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://huggingface.co/stepfun-ai/Step-Audio-EditX"&gt;&lt;strong&gt;Hugging Face&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fsl15il8ed0g1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=caa10ad203ad44158a1ba8dbe7f303b0eb03cfbd"&gt;An overview of the architecture of Step-Audio-EditX.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;BindWeave - Consistent Subjects, Local Pipelines&lt;br /&gt; ‚Ä¢ Subject-consistent video gen; ComfyUI support.&lt;br /&gt; ‚Ä¢ Drop-in for desktop creative stacks.&lt;br /&gt; ‚Ä¢ &lt;a href="https://lzy-dot.github.io/BindWeave/"&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://huggingface.co/papers/2510.00438"&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/bytedance/BindWeave"&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://huggingface.co/ByteDance/BindWeave"&gt;&lt;strong&gt;Hugging Face&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ot67nn/video/ay7nndyaed0g1/player"&gt;https://reddit.com/link/1ot67nn/video/ay7nndyaed0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;InfinityStar (8B) - Unified Spacetime AR Gen&lt;br /&gt; ‚Ä¢ 8B model targets high-res image/video generation.&lt;br /&gt; ‚Ä¢ Fits prosumer GPUs for local experimentation.&lt;br /&gt; ‚Ä¢ &lt;a href="https://arxiv.org/abs/2511.04675"&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/FoundationVision/InfinityStar"&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://huggingface.co/FoundationVision/InfinityStar"&gt;&lt;strong&gt;Hugging Face&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ot67nn/video/ouipokpbed0g1/player"&gt;https://reddit.com/link/1ot67nn/video/ouipokpbed0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;OlmoEarth-v1-Large - Remote Sensing for Builders&lt;br /&gt; ‚Ä¢ Satellite model ready for on-prem analysis.&lt;br /&gt; ‚Ä¢ Strong for geospatial R&amp;amp;D without cloud lock-in.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/allenai/OlmoEarth-v1-Large"&gt;&lt;strong&gt;Hugging Face&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://www.datocms-assets.com/64837/1762355216-olmoearth_v2.pdf"&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://x.com/allen_ai/status/1985719070407176577"&gt;&lt;strong&gt;Announcement&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ot67nn/video/mkbihhrced0g1/player"&gt;https://reddit.com/link/1ot67nn/video/mkbihhrced0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://open.substack.com/pub/thelivingedge/p/multimodal-monday-32-multi-query?r=12l7fk&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false"&gt;full newsletter&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot67nn/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot67nn/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot67nn/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T06:04:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1osglws</id>
    <title>Kimi K2 Thinking scores lower than Gemini 2.5 Flash on Livebench</title>
    <updated>2025-11-09T11:01:13+00:00</updated>
    <author>
      <name>/u/ihexx</name>
      <uri>https://old.reddit.com/user/ihexx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osglws/kimi_k2_thinking_scores_lower_than_gemini_25/"&gt; &lt;img alt="Kimi K2 Thinking scores lower than Gemini 2.5 Flash on Livebench" src="https://preview.redd.it/mqize91iq70g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=717d19b768a2a88e4b9a2a6690bbde033a817303" title="Kimi K2 Thinking scores lower than Gemini 2.5 Flash on Livebench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihexx"&gt; /u/ihexx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mqize91iq70g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osglws/kimi_k2_thinking_scores_lower_than_gemini_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osglws/kimi_k2_thinking_scores_lower_than_gemini_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T11:01:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ostdcn</id>
    <title>Faster Prompt Processing in llama.cpp: Smart Proxy + Slots + Restore</title>
    <updated>2025-11-09T20:10:26+00:00</updated>
    <author>
      <name>/u/Previous_Nature_5319</name>
      <uri>https://old.reddit.com/user/Previous_Nature_5319</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ostdcn/faster_prompt_processing_in_llamacpp_smart_proxy/"&gt; &lt;img alt="Faster Prompt Processing in llama.cpp: Smart Proxy + Slots + Restore" src="https://preview.redd.it/90im3um0fa0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1880277abb2e16deb196c79509aa47dbb7d349ae" title="Faster Prompt Processing in llama.cpp: Smart Proxy + Slots + Restore" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache"&gt;https://github.com/airnsk/proxycache&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What this service is&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#what-this-service-is"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This service is a smart proxy in front of llama.cpp that makes long‚Äëcontext chat and IDE workflows much faster by managing llama.cpp slots, reusing cached context, and restoring saved caches from disk when needed. It speaks an OpenAI‚Äëcompatible Chat Completions API, so existing clients can connect without changes, including both streaming (SSE) and non‚Äëstream responses depending on request settings.&lt;/p&gt; &lt;h1&gt;Why it‚Äôs needed&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#why-its-needed"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp provides ‚Äúslots,‚Äù each holding a conversation‚Äôs KV cache so repeated requests with the same or very similar prefix can skip recomputing the whole prompt and continue from the first mismatching token, which dramatically cuts latency for large prompts. In real teams the number of users can easily exceed the number of available slots (e.g., 20 developers but only 4 slots), so naive routing causes random slot reuse and cache overwrites that waste time and GPU/CPU cycles. This proxy solves that by steering requests to the right slot, saving evicted caches to disk, and restoring them on demand, so long prompts don‚Äôt need to be recomputed from scratch each time.&lt;/p&gt; &lt;h1&gt;How requests are balanced and slots are chosen&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#how-requests-are-balanced-and-slots-are-chosen"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Slots and heat: When a request lands in a slot and its cache is valid for reuse, the slot is considered ‚Äúhot,‚Äù and new requests won‚Äôt overwrite it if other options exist, preserving useful KV for future reuse.&lt;/li&gt; &lt;li&gt;Similarity matching: The proxy computes a fast, word‚Äëblock prefix similarity between the incoming conversation and existing hot slots, and only reuses a hot slot if the similarity meets a single ratio threshold (e.g., 85% of the shorter sequence), otherwise it rejects reuse to avoid polluting the hot cache with a weakly related prompt.&lt;/li&gt; &lt;li&gt;Free and cold first: If reuse is rejected, the proxy sends the request to a free slot or a cold slot (one not currently carrying a valuable hot cache), protecting high‚Äëvalue contexts from accidental overwrites under load.&lt;/li&gt; &lt;li&gt;Oldest when full: If there are no free or cold slots, the proxy picks the least‚Äërecently used slot and saves its current KV cache to disk before assigning the new request, ensuring nothing valuable is lost when the pool is exhausted.&lt;/li&gt; &lt;li&gt;Restore on demand: When a new request matches a cache that was previously saved, the proxy restores that cache into a free/cold/oldest slot and routes the request there, which takes seconds versus minutes for full prompt recomputation on long contexts, especially in IDE scenarios with 30‚Äì60k tokens.&lt;/li&gt; &lt;li&gt;Concurrency safety: Each slot is guarded with an async lock; if all are busy, the request waits for the first LRU slot to free, preventing race conditions and unintended cache overwrites during concurrent generation.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Save and restore from disk&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#save-and-restore-from-disk"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp‚Äôs HTTP server exposes slot save/restore; saving writes a cache file to the directory provided by --slot‚Äësave‚Äëpath, and restore loads by file basename (e.g., slotcache_.bin), which is exactly how this proxy persists and revives caches across requests and restarts. The proxy keeps small local .meta files describing cached prefixes for fast lookup, while llama.cpp owns the actual KV .bin files under --slot‚Äësave‚Äëpath for correctness and performance.&lt;/p&gt; &lt;h1&gt;Quick start&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#quick-start"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Start llama.cpp ( &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;https://github.com/ggml-org/llama.cpp&lt;/a&gt; ) with slots and a cache directory:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server -m ./model.gguf -np 4 --slot-save-path /var/kvcache --host 0.0.0.0 --port 8080 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This enables the OpenAI‚Äëcompatible HTTP server, a pool of 4 slots, and a directory where slot KV caches are saved and restored by basename.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Run the proxy next to it:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/airnsk/proxycache.git cd proxycache python3 -m venv venv &amp;amp;&amp;amp; source venv/bin/activate &amp;amp;&amp;amp; pip install -r requirements.txt python3 proxycache.py # or: uvicorn app:app --host 0.0.0.0 --port 8081 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your clients should call the proxy‚Äôs /v1/chat/completions endpoint; the proxy will handle similarity, slot selection, save/restore, and streaming vs non‚Äëstreaming automatically.&lt;/p&gt; &lt;p&gt;If you run into issues using gpt-oss-20b with an IDE like Cline, follow these instructions: &lt;a href="https://www.reddit.com/r/CLine/comments/1mtcj2v/making_gptoss_20b_and_cline_work_together/"&gt;https://www.reddit.com/r/CLine/comments/1mtcj2v/making_gptoss_20b_and_cline_work_together/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Parameters&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#parameters"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLAMA_SERVER_URL: The llama.cpp server base URL, e.g., &lt;a href="http://127.0.0.1:8080/"&gt;http://127.0.0.1:8080&lt;/a&gt;, which must expose the OpenAI‚Äëcompatible chat completions endpoint.&lt;/li&gt; &lt;li&gt;SLOTS_COUNT: The number of server slots (should match llama.cpp -np) so the proxy can track and plan reuse/restore correctly under load.&lt;/li&gt; &lt;li&gt;SIMILARITY_MIN_RATIO: One similarity threshold (e.g., 0.85) controlling both active reuse and disk restore; if a match is below this ratio, the proxy will prefer a free/cold slot or restore instead of overwriting a hot slot.&lt;/li&gt; &lt;li&gt;MIN_PREFIX_* (chars/words/blocks): Requests below this size are treated as ‚Äúsmall‚Äù and steered to free/cold/oldest slots to avoid disturbing valuable hot caches used by large, long‚Äërunning prompts.&lt;/li&gt; &lt;li&gt;LOCAL_META_DIR and --slot-save-path: The proxy stores small .meta descriptors locally for fast candidate lookup, while llama.cpp reads/writes the real KV cache files under --slot‚Äësave‚Äëpath using basename in the HTTP API.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why this boosts IDE and long‚Äëcontext productivity&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#why-this-boosts-ide-and-longcontext-productivity"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For 30‚Äì60k‚Äëtoken contexts typical in project‚Äëwide IDE assistants, recomputing a full prompt can take minutes, whereas restoring a previously cached context and continuing from the first mismatching token typically takes seconds on llama.cpp, dramatically improving iteration speed for large teams with limited slots.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Previous_Nature_5319"&gt; /u/Previous_Nature_5319 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/90im3um0fa0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ostdcn/faster_prompt_processing_in_llamacpp_smart_proxy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ostdcn/faster_prompt_processing_in_llamacpp_smart_proxy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T20:10:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1osuat7</id>
    <title>Benchmark Results: GLM-4.5-Air (Q4) at Full Context on Strix Halo vs. Dual RTX 3090</title>
    <updated>2025-11-09T20:47:30+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osuat7/benchmark_results_glm45air_q4_at_full_context_on/"&gt; &lt;img alt="Benchmark Results: GLM-4.5-Air (Q4) at Full Context on Strix Halo vs. Dual RTX 3090" src="https://preview.redd.it/vvimjdf4na0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b8bba52c5a4592461099dbcbbff0318d56011e9" title="Benchmark Results: GLM-4.5-Air (Q4) at Full Context on Strix Halo vs. Dual RTX 3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I benchmarked the GLM-4.5-Air (Q4) model running at a near-maximum context on two very different systems: a Strix Halo APU and a dual RTX 3090 server. Both tests were conducted under Debian GNU/Linux with the latest llama.cpp builds from the day of testing. But I did overlook and there's a one-revision difference between the two llama.cpp builds. Here are the startup commands, environment details, and a diagram that breaks down the performance and energy efficiency of both setups.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX3090:&lt;/strong&gt; ```bash&lt;/p&gt; &lt;p&gt;$ LLAMA_SET_ROWS=1 llama-server -m GLM-4.5-Air-UD-Q4_K_XL-00001-of-00002.gguf --n-cpu-moe 38 \ --tensor-split 28,20 -c 0 --n-gpu-layers 99 --temp 0.9 --flash-attn auto --jinja --host 0.0.0.0 \ --port 8080 -a glm_air --no-context-shift --no-mmap --swa-full --reasoning-format none ```&lt;/p&gt; &lt;p&gt;```bash prompt eval time = 1781631.25 ms / 119702 tokens ( 14.88 ms per token, 67.19 tokens per second) eval time = 1045615.05 ms / 5232 tokens ( 199.85 ms per token, 5.00 tokens per second) total time = 2827246.30 ms / 124934 tokens slot release: id 3 | task 1 | stop processing: n_tokens = 124933, truncated = 0&lt;/p&gt; &lt;p&gt;$ llama-server --version ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 2 CUDA devices: Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes ggml_vulkan: Found 2 Vulkan devices: ggml_vulkan: 0 = NVIDIA GeForce RTX 3090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: KHR_coopmat ggml_vulkan: 1 = NVIDIA GeForce RTX 3090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: KHR_coopmat version: 6990 (53d7d21e6) built with cc (Debian 14.2.0-19) 14.2.0 for x86_64-linux-gnu&lt;/p&gt; &lt;p&gt;Build flags: -DGGML_CUDA=ON -DGGML_CUDA_F16=ON -DGGML_CUDA_FA_ALL_QUANTS=ON -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DCMAKE_CUDA_ARCHITECTURES=86 -DGGML_VULKAN=ON&amp;quot;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;strix halo:&lt;/strong&gt; &lt;code&gt;bash $ llama-server -m GLM-4.5-Air-UD-Q4_K_XL-00001-of-00002.gguf --n-gpu-layers 99 --host 0.0.0.0 \ --port 8080 -a glm_air -c 131072 -fa 1 --no-mmap &lt;/code&gt;&lt;/p&gt; &lt;p&gt;```bash prompt eval time = 5175231.01 ms / 119703 tokens ( 43.23 ms per token, 23.13 tokens per second) eval time = 1430449.98 ms / 5778 tokens ( 247.57 ms per token, 4.04 tokens per second) total time = 6605680.99 ms / 125481 tokens slot update_slots: id 2 | task 1577 | prompt done, n_tokens = 119703, batch.n_tokens = 919&lt;/p&gt; &lt;p&gt;$ llama-server --version ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat version: 6989 (eeee367de) built with cc (Debian 15.2.0-7) 15.2.0 for x86_64-linux-gnu&lt;/p&gt; &lt;p&gt;Build flags: -DGGML_VULKAN=ON -DGGML_HIP_ROCWMMA_FATTN=ON -DAMDGPU_TARGETS=gfx1151 ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vvimjdf4na0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osuat7/benchmark_results_glm45air_q4_at_full_context_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osuat7/benchmark_results_glm45air_q4_at_full_context_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T20:47:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1osufxq</id>
    <title>Running DeepSeek-OCR on vLLM 0.11.1rc6.dev7 in Open WebUI as a test</title>
    <updated>2025-11-09T20:53:15+00:00</updated>
    <author>
      <name>/u/AFruitShopOwner</name>
      <uri>https://old.reddit.com/user/AFruitShopOwner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osufxq/running_deepseekocr_on_vllm_0111rc6dev7_in_open/"&gt; &lt;img alt="Running DeepSeek-OCR on vLLM 0.11.1rc6.dev7 in Open WebUI as a test" src="https://preview.redd.it/j14a86mxka0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21626c1b49c8f27927e9f2fd70e483b0bd2bd4d4" title="Running DeepSeek-OCR on vLLM 0.11.1rc6.dev7 in Open WebUI as a test" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Obviously you're not supposed to use DeepSeek-OCR through a chat UI. I'm just testing to see if it works or not. Also, this is not really an OCR task but I was wondering if I could use this model for general image description. Seems like that works just fine. &lt;/p&gt; &lt;p&gt;I have not yet implemented the helper scripts in the &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/tree/main/DeepSeek-OCR-master/DeepSeek-OCR-vllm"&gt;DeepSeek-OCR github repo&lt;/a&gt;. They seem pretty handy for image/pdf/batch OCR workloads.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AFruitShopOwner"&gt; /u/AFruitShopOwner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j14a86mxka0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osufxq/running_deepseekocr_on_vllm_0111rc6dev7_in_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osufxq/running_deepseekocr_on_vllm_0111rc6dev7_in_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T20:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oswo5v</id>
    <title>Qwen3-VL Now EXL3 Supported</title>
    <updated>2025-11-09T22:21:49+00:00</updated>
    <author>
      <name>/u/Unstable_Llama</name>
      <uri>https://old.reddit.com/user/Unstable_Llama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oswo5v/qwen3vl_now_exl3_supported/"&gt; &lt;img alt="Qwen3-VL Now EXL3 Supported" src="https://b.thumbs.redditmedia.com/SuI8cNmsMD7QQjM85nzVN4go04so8nc4q9t6e8mY17c.jpg" title="Qwen3-VL Now EXL3 Supported" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;‚ö†Ô∏è Requires &lt;a href="https://github.com/turboderp-org/exllamav3"&gt;ExLlamaV3 v0.0.13&lt;/a&gt; (or higher)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/turboderp/Qwen3-VL-8B-Instruct-exl3"&gt;https://huggingface.co/turboderp/Qwen3-VL-8B-Instruct-exl3&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/turboderp/Qwen3-VL-30B-A3B-Instruct-exl3"&gt;https://huggingface.co/turboderp/Qwen3-VL-30B-A3B-Instruct-exl3&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/turboderp/Qwen3-VL-32B-Instruct-exl3"&gt;https://huggingface.co/turboderp/Qwen3-VL-32B-Instruct-exl3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0kso6yh3gb0g1.png?width=1096&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ed40413e383c31a0f466f26dbce1a01df4f6cb6"&gt;CatBench results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Questions? Ask here or in the&lt;a href="https://discord.gg/VbR8wQxf"&gt; exllama discord&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unstable_Llama"&gt; /u/Unstable_Llama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oswo5v/qwen3vl_now_exl3_supported/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oswo5v/qwen3vl_now_exl3_supported/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oswo5v/qwen3vl_now_exl3_supported/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T22:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1osmr6i</id>
    <title>Is the RTX 5090 that good of a deal?</title>
    <updated>2025-11-09T15:53:07+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osmr6i/is_the_rtx_5090_that_good_of_a_deal/"&gt; &lt;img alt="Is the RTX 5090 that good of a deal?" src="https://preview.redd.it/v9cx89gr690g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c96c392f6cfc52c3e86aaa74147a35d574967307" title="Is the RTX 5090 that good of a deal?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to find a model agnostic approach to estimate which cards to pick &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v9cx89gr690g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osmr6i/is_the_rtx_5090_that_good_of_a_deal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osmr6i/is_the_rtx_5090_that_good_of_a_deal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T15:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot6k56</id>
    <title>Kimi infra team: Quantization is not a compromise, it's the next paradigm</title>
    <updated>2025-11-10T06:25:55+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After K2-Thinking's release, many developers have been curious about its native INT4 quantization format.&lt;/p&gt; &lt;p&gt;Shaowei Liu, &lt;strong&gt;infra engineer&lt;/strong&gt; at &lt;a href="/u/Kimi-Moonshot"&gt;u/Kimi-Moonshot&lt;/a&gt; shares an insider's view on why this choice matters, and why quantization today isn't just about sacrificing precision for speed.&lt;/p&gt; &lt;h1&gt;Key idea&lt;/h1&gt; &lt;p&gt;In the context of LLMs, quantization is no longer a trade-off.&lt;/p&gt; &lt;p&gt;With the evolution of param-scaling and test-time-scaling, native low-bit quantization will become a standard paradigm for large model training.&lt;/p&gt; &lt;h1&gt;Why Low-bit Quantization Matters&lt;/h1&gt; &lt;p&gt;In modern LLM inference, there are two distinct optimization goals:&lt;/p&gt; &lt;p&gt;‚Ä¢ High throughput (cost-oriented): maximize GPU utilization via large batch sizes.&lt;/p&gt; &lt;p&gt;‚Ä¢ Low latency (user-oriented): minimize per-query response time.&lt;/p&gt; &lt;p&gt;For Kimi-K2's MoE structure (with &lt;strong&gt;1/48 sparsity&lt;/strong&gt;), &lt;strong&gt;decoding is memory-bound&lt;/strong&gt; ‚Äî the smaller the model weights, the faster the compute.&lt;/p&gt; &lt;p&gt;FP8 weights (‚âà1 TB) already hit the limit of what a single high-speed interconnect GPU node can handle.&lt;/p&gt; &lt;p&gt;By switching to W4A16, latency drops sharply while maintaining quality ‚Äî a perfect fit for low-latency inference.&lt;/p&gt; &lt;h1&gt;Why QAT over PTQ&lt;/h1&gt; &lt;p&gt;Post-training quantization (PTQ) worked well for shorter generations, but &lt;strong&gt;failed in longer reasoning chains&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;‚Ä¢ Error accumulation during long decoding degraded precision.&lt;/p&gt; &lt;p&gt;‚Ä¢ Dependence on calibration data caused &amp;quot;expert distortion&amp;quot; in sparse MoE layers.&lt;/p&gt; &lt;p&gt;Thus, K2-Thinking adopted QAT for &lt;strong&gt;minimal loss&lt;/strong&gt; and &lt;strong&gt;more stable long-context reasoning&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;p&gt;K2-Thinking uses a &lt;strong&gt;weight-only QAT&lt;/strong&gt; with &lt;strong&gt;fake quantization + STE (straight-through estimator)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The pipeline was fully integrated in just days ‚Äî from QAT training ‚Üí INT4 inference ‚Üí RL rollout ‚Äî enabling near lossless results without extra tokens or retraining.&lt;/p&gt; &lt;h1&gt;INT4's hidden advantage in RL&lt;/h1&gt; &lt;p&gt;Few people mention this: &lt;strong&gt;native INT4&lt;/strong&gt; doesn't just speed up inference ‚Äî it &lt;strong&gt;accelerates RL training&lt;/strong&gt; itself.&lt;/p&gt; &lt;p&gt;Because RL rollouts often suffer from &amp;quot;long-tail&amp;quot; inefficiency, INT4's low-latency profile makes those stages much faster.&lt;/p&gt; &lt;p&gt;In practice, each RL iteration runs &lt;strong&gt;10-20% faster end-to-end.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Moreover, quantized RL brings stability: smaller representational space reduces accumulation error, improving learning robustness.&lt;/p&gt; &lt;h1&gt;Why INT4, not MXFP4&lt;/h1&gt; &lt;p&gt;Kimi chose INT4 over &amp;quot;fancier&amp;quot; MXFP4/NVFP4 to better support &lt;strong&gt;non-Blackwell GPUs&lt;/strong&gt;, with strong existing kernel support (e.g., Marlin).&lt;/p&gt; &lt;p&gt;At a quant scale of 1√ó32, INT4 matches FP4 formats in expressiveness while being more hardware-adaptable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot6k56/kimi_infra_team_quantization_is_not_a_compromise/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot6k56/kimi_infra_team_quantization_is_not_a_compromise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot6k56/kimi_infra_team_quantization_is_not_a_compromise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T06:25:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot682o</id>
    <title>Montana Becomes First State to Enshrine ‚ÄòRight to Compute‚Äô Into Law - Montana Newsroom</title>
    <updated>2025-11-10T06:05:33+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot682o/montana_becomes_first_state_to_enshrine_right_to/"&gt; &lt;img alt="Montana Becomes First State to Enshrine ‚ÄòRight to Compute‚Äô Into Law - Montana Newsroom" src="https://external-preview.redd.it/mQmftbFg8dXc1pZL5UOJLL9AO6IH64kyLn5ax_JS4QM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2bc023e5f328155ef589e67ab05e5434c741142" title="Montana Becomes First State to Enshrine ‚ÄòRight to Compute‚Äô Into Law - Montana Newsroom" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Montana has made history as the first state in the U.S. to legally protect its citizens‚Äô right to access and use computational tools and artificial intelligence technologies. Governor Greg Gianforte signed Senate Bill 212, officially known as the Montana Right to Compute Act (MRTCA), into law.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;The groundbreaking legislation affirms Montanans‚Äô fundamental right to own and operate computational resources ‚Äî including hardware, software, and AI tools ‚Äî under the state‚Äôs constitutional protections for property and free expression. Supporters of the bill say it represents a major step in securing digital freedoms in an increasingly AI-driven world.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;‚ÄúMontana is once again leading the way in defending individual liberty,‚Äù said Senator Daniel Zolnikov, the bill‚Äôs sponsor and a longtime advocate for digital privacy. ‚ÄúWith the Right to Compute Act, we are ensuring that every Montanan can access and control the tools of the future.‚Äù&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;While the law allows state regulation of computation in the interest of public health and safety, it sets a high bar: any restrictions must be demonstrably necessary and narrowly tailored to serve a compelling interest. Legal experts note that this is one of the most protective standards available under Montana law.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Hopefully this leads to more states following / similar federal legislation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot682o/montana_becomes_first_state_to_enshrine_right_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot682o/montana_becomes_first_state_to_enshrine_right_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T06:05:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1osnnfn</id>
    <title>How to build an AI computer (version 2.0)</title>
    <updated>2025-11-09T16:28:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osnnfn/how_to_build_an_ai_computer_version_20/"&gt; &lt;img alt="How to build an AI computer (version 2.0)" src="https://preview.redd.it/03t3yj51d90g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d467717022f60865bdfcbb8d96bb265e2bdb541" title="How to build an AI computer (version 2.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/03t3yj51d90g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osnnfn/how_to_build_an_ai_computer_version_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osnnfn/how_to_build_an_ai_computer_version_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T16:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1osydym</id>
    <title>BERTs that chat: turn any BERT into a chatbot with dLLM</title>
    <updated>2025-11-09T23:34:15+00:00</updated>
    <author>
      <name>/u/Individual-Ninja-141</name>
      <uri>https://old.reddit.com/user/Individual-Ninja-141</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osydym/berts_that_chat_turn_any_bert_into_a_chatbot_with/"&gt; &lt;img alt="BERTs that chat: turn any BERT into a chatbot with dLLM" src="https://external-preview.redd.it/aXQyaXFqbnhjYjBnMcUkZmGot1jxvd3JGKHlRvmTnIHsjUGTXFsDE1YCFtzY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ca452c52e70528594ede3b08fdf40c34e19a371" title="BERTs that chat: turn any BERT into a chatbot with dLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code: &lt;a href="https://github.com/ZHZisZZ/dllm"&gt;https://github.com/ZHZisZZ/dllm&lt;/a&gt;&lt;br /&gt; Report: &lt;a href="https://api.wandb.ai/links/asap-zzhou/101h5xvg"&gt;https://api.wandb.ai/links/asap-zzhou/101h5xvg&lt;/a&gt;&lt;br /&gt; Checkpoints: &lt;a href="https://huggingface.co/collections/dllm-collection/bert-chat"&gt;https://huggingface.co/collections/dllm-collection/bert-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;: I couldn‚Äôt find a good ‚ÄúHello World‚Äù tutorial for training &lt;strong&gt;diffusion language models&lt;/strong&gt;, a class of bidirectional language models capable of parallel token generation in arbitrary order, instead of left-to-right autoregression. So I tried finetuning a tiny BERT to make it &lt;strong&gt;talk with discrete diffusion&lt;/strong&gt;‚Äîand it turned out more fun than I expected.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;: With a small amount of open-source instruction data, a standard BERT can gain conversational ability. Specifically, a finetuned &lt;a href="https://huggingface.co/answerdotai/ModernBERT-large"&gt;ModernBERT-large&lt;/a&gt;, with a similar number of parameters, performs close to &lt;a href="https://huggingface.co/Qwen/Qwen1.5-0.5B"&gt;Qwen1.5-0.5B&lt;/a&gt;. All training and evaluation code, along with detailed results and comparisons, is available in our &lt;a href="https://api.wandb.ai/links/asap-zzhou/101h5xvg"&gt;W&amp;amp;B report&lt;/a&gt; and our &lt;a href="https://github.com/ZHZisZZ/dllm/tree/main/examples/bert"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ZHZisZZ/dllm"&gt;&lt;strong&gt;dLLM&lt;/strong&gt;&lt;/a&gt;: The BERT chat series is &lt;em&gt;trained, evaluated and visualized&lt;/em&gt; with &lt;a href="https://github.com/ZHZisZZ/dllm"&gt;dLLM&lt;/a&gt; ‚Äî a unified library for training and evaluating diffusion language models. It brings transparency, reproducibility, and simplicity to the entire pipeline, &lt;strong&gt;serving as an all-in-one, tutorial-style resource.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual-Ninja-141"&gt; /u/Individual-Ninja-141 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/47030knxcb0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osydym/berts_that_chat_turn_any_bert_into_a_chatbot_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osydym/berts_that_chat_turn_any_bert_into_a_chatbot_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T23:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot3lxv</id>
    <title>I tested Strix Halo clustering w/ ~50Gig IB to see if networking is really the bottleneck</title>
    <updated>2025-11-10T03:42:05+00:00</updated>
    <author>
      <name>/u/Hungry_Elk_3276</name>
      <uri>https://old.reddit.com/user/Hungry_Elk_3276</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3lxv/i_tested_strix_halo_clustering_w_50gig_ib_to_see/"&gt; &lt;img alt="I tested Strix Halo clustering w/ ~50Gig IB to see if networking is really the bottleneck" src="https://preview.redd.it/ezjtolwnoc0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d9f058ee27bdab9923ee3d40ab306fea5558c71" title="I tested Strix Halo clustering w/ ~50Gig IB to see if networking is really the bottleneck" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; While InfiniBand is cool, 10 Gbps Thunderbolt is sufficient for llama.cpp.&lt;/p&gt; &lt;p&gt;Recently I got really fascinated by clustering with Strix Halo to get a potential 200 GB of VRAM without significant costs. I'm currently using a 4x4090 solution for research, but it's very loud and power-hungry (plus it doesn't make much sense for normal 1-2 user inference‚Äîthis machine is primarily used for batch generation for research purposes). I wanted to look for a low-power but efficient way to inference ~230B models at Q4. And here we go.&lt;/p&gt; &lt;p&gt;I always had this question of how exactly networking would affect the performance. So I got two modded Mellanox ConnectX-5 Ex 100 Gig NICs which I had some experience with on NCCL. These cards are very cool with reasonable prices and are quite capable. However, due to the Strix Halo platform limitation, I only got a PCIe 4.0 x4 link. But I was still able to get around 6700 MB/s or roughly 55 Gbps networking between the nodes, which is far better than using IP over Thunderbolt (10 Gbps).&lt;/p&gt; &lt;p&gt;I tried using vLLM first and quickly found out that RCCL is not supported on Strix Halo. :( Then I tried using llama.cpp RPC mode with the &lt;code&gt;-c&lt;/code&gt; flag to enable caching, and here are the results I got:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test Type&lt;/th&gt; &lt;th align="left"&gt;Single Machine w/o rpc&lt;/th&gt; &lt;th align="left"&gt;2.5 Gbps&lt;/th&gt; &lt;th align="left"&gt;10 Gbps (TB)&lt;/th&gt; &lt;th align="left"&gt;50 Gbps&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;pp512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;653.74&lt;/td&gt; &lt;td align="left"&gt;603.00&lt;/td&gt; &lt;td align="left"&gt;654.03&lt;/td&gt; &lt;td align="left"&gt;663.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg128&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;49.73&lt;/td&gt; &lt;td align="left"&gt;30.98&lt;/td&gt; &lt;td align="left"&gt;36.44&lt;/td&gt; &lt;td align="left"&gt;35.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;47.54&lt;/td&gt; &lt;td align="left"&gt;29.13&lt;/td&gt; &lt;td align="left"&gt;35.07&lt;/td&gt; &lt;td align="left"&gt;34.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;pp512 @ d512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;601.75&lt;/td&gt; &lt;td align="left"&gt;554.17&lt;/td&gt; &lt;td align="left"&gt;599.76&lt;/td&gt; &lt;td align="left"&gt;611.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg128 @ d512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;45.81&lt;/td&gt; &lt;td align="left"&gt;27.78&lt;/td&gt; &lt;td align="left"&gt;33.88&lt;/td&gt; &lt;td align="left"&gt;32.67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg512 @ d512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;44.90&lt;/td&gt; &lt;td align="left"&gt;27.14&lt;/td&gt; &lt;td align="left"&gt;31.33&lt;/td&gt; &lt;td align="left"&gt;32.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;pp512 @ d2048&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;519.40&lt;/td&gt; &lt;td align="left"&gt;485.93&lt;/td&gt; &lt;td align="left"&gt;528.52&lt;/td&gt; &lt;td align="left"&gt;537.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg128 @ d2048&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;41.84&lt;/td&gt; &lt;td align="left"&gt;25.34&lt;/td&gt; &lt;td align="left"&gt;31.22&lt;/td&gt; &lt;td align="left"&gt;30.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg512 @ d2048&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;41.33&lt;/td&gt; &lt;td align="left"&gt;25.01&lt;/td&gt; &lt;td align="left"&gt;30.66&lt;/td&gt; &lt;td align="left"&gt;30.11&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As you can see, the Thunderbolt connection almost matches the 50 Gbps MLX5 on token generation. Compared to the non-RPC single node inference, the performance difference is still quite substantial‚Äîwith about a 15 token/s difference‚Äîbut as the context lengthens, the text generation difference somehow gets smaller and smaller. Another strange thing is that somehow the prompt processing is better on RPC over 50 Gbps, even better than the single machine. That's very interesting to see.&lt;/p&gt; &lt;p&gt;During inference, I observed that the network was never used at more than maybe ~100 Mbps or 10 MB/s most of the time, suggesting the gain might not come from bandwidth‚Äîmaybe latency? But I don't have a way to prove what exactly is affecting the performance gain from 2.5 Gbps to 10 Gbps IP over Thunderbolt.&lt;/p&gt; &lt;p&gt;Here is the llama-bench command I'm using:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -m ./gpt-oss-120b-mxfp4-00001-of-00003.gguf -d 0,512,2048 -n 128,512 -o md --rpc &amp;lt;IP:PORT&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So the result is pretty clear: you don't need a fancy IB card to gain usable results on llama.cpp with Strix Halo. At least until RCCL supports Strix Halo, I think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hungry_Elk_3276"&gt; /u/Hungry_Elk_3276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ezjtolwnoc0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3lxv/i_tested_strix_halo_clustering_w_50gig_ib_to_see/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3lxv/i_tested_strix_halo_clustering_w_50gig_ib_to_see/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T03:42:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqy1k7</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)</title>
    <updated>2025-11-07T15:53:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" src="https://preview.redd.it/8v2luf5owuzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9bc34ec8dddd94422397eaa91e0310250da5ba3" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v2luf5owuzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:53:33+00:00</published>
  </entry>
</feed>
