<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-16T16:18:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r6bxe1</id>
    <title>Good semantic search (RAG) embedding models for long stories</title>
    <updated>2026-02-16T15:02:59+00:00</updated>
    <author>
      <name>/u/Iwishlife</name>
      <uri>https://old.reddit.com/user/Iwishlife</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for good RAG embedding models, that I want to use on my personal library of books to search (and recommend me) for specific types of stories that would appeal to me. What are the best models for this purpose? I attempted Gwen 0.6b, but the results were subpar.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iwishlife"&gt; /u/Iwishlife &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6bxe1/good_semantic_search_rag_embedding_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6bxe1/good_semantic_search_rag_embedding_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6bxe1/good_semantic_search_rag_embedding_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T15:02:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6c4q6</id>
    <title>BAZINGA — Multi-AI consensus that runs local. No single AI can mess up your code.</title>
    <updated>2026-02-16T15:10:56+00:00</updated>
    <author>
      <name>/u/bitsabhi</name>
      <uri>https://old.reddit.com/user/bitsabhi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on this for a while. The idea: what if multiple AIs had to agree before making changes to your code? &lt;/p&gt; &lt;p&gt;How it works: &lt;/p&gt; &lt;p&gt;- Query multiple AIs (Ollama, Groq, Gemini) simultaneously &lt;/p&gt; &lt;p&gt;- They reach consensus through φ-coherence scoring &lt;/p&gt; &lt;p&gt;- Code changes require 3+ AIs to agree &lt;/p&gt; &lt;p&gt;- Local models get trust bonus (no cloud = more trust) &lt;/p&gt; &lt;p&gt;The &amp;quot;oh shit&amp;quot; protection: &lt;/p&gt; &lt;p&gt;- Destructive commands (rm, sudo, git push) require manual confirmation &lt;/p&gt; &lt;p&gt;- No single AI can mess up your machine &lt;/p&gt; &lt;p&gt;Works fully offline with Ollama, or uses free APIs. &lt;/p&gt; &lt;p&gt;pip install bazinga-indeed &lt;/p&gt; &lt;p&gt;bazinga --multi-ai &amp;quot;your question&amp;quot; &lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/0x-auth/bazinga-indeed"&gt;https://github.com/0x-auth/bazinga-indeed&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bitsabhi"&gt; /u/bitsabhi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6c4q6/bazinga_multiai_consensus_that_runs_local_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6c4q6/bazinga_multiai_consensus_that_runs_local_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6c4q6/bazinga_multiai_consensus_that_runs_local_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T15:10:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5qfb8</id>
    <title>inclusionAI/Ling-2.5-1T · Hugging Face</title>
    <updated>2026-02-15T21:20:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5qfb8/inclusionailing251t_hugging_face/"&gt; &lt;img alt="inclusionAI/Ling-2.5-1T · Hugging Face" src="https://external-preview.redd.it/nCxW8JHyfmzzv3lMTtcAqL8Ez3yOAkDeuLrrPCFMKz4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c049d1c20ccf2dbac44fc910e04d8dc862b0d7b1" title="inclusionAI/Ling-2.5-1T · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;another 1T model :)&lt;/p&gt; &lt;p&gt;from &lt;strong&gt;inclusionAI&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Ling-2.5-1T, Inclusive Intelligence, Instant Impact.&lt;/p&gt; &lt;p&gt;Today, we launch Ling-2.5-1T and make it open source.&lt;/p&gt; &lt;p&gt;Thinking models raise the ceiling of intelligence, while instant models expand its reach by balancing efficiency and performance—making AGI not only more powerful, but also more accessible. As the latest flagship instant model in the Ling family, Ling-2.5-1T delivers comprehensive upgrades across model architecture, token efficiency, and preference alignment, designed to bring universally accessible AI to a new level of quality.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ling-2.5-1T features 1T total parameters (with 63B active parameters). Its pre-training corpus has expanded from 20T to 29T tokens compared to the previous generation. Leveraging an efficient hybrid linear attention architecture and refined data strategy, the model delivers exceptionally high throughput while processing context lengths of up to 1M tokens.&lt;/li&gt; &lt;li&gt;By introducing a composite reward mechanism combining &amp;quot;Correctness&amp;quot; and &amp;quot;Process Redundancy&amp;quot;, Ling-2.5-1T further pushes the frontier of efficiency-performance balance in instant models. At comparable token efficiency levels, Ling-2.5-1T’s reasoning capabilities significantly outperform its predecessor, approaching the level of frontier &amp;quot;thinking models&amp;quot; that typically consume ~4x the output tokens.&lt;/li&gt; &lt;li&gt;Through refined alignment strategies—such as bidirectional RL feedback and Agent-based instruction constraint verification—Ling-2.5-1T achieves substantial improvements over the previous generation in preference alignment tasks, including creative writing and instruction following.&lt;/li&gt; &lt;li&gt;Trained with Agentic RL in large-scale high-fidelity interactive environments, Ling-2.5-1T is compatible with mainstream agent platforms such as Claude Code, OpenCode, and OpenClaw. It achieves leading open-source performance on the general tool-calling benchmark, BFCL-V4.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-2.5-1T"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5qfb8/inclusionailing251t_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5qfb8/inclusionailing251t_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T21:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6bnt2</id>
    <title>Izwi Update: Local Speaker Diarization, Forced Alignment, and better model support</title>
    <updated>2026-02-16T14:52:48+00:00</updated>
    <author>
      <name>/u/zinyando</name>
      <uri>https://old.reddit.com/user/zinyando</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick update on Izwi (local audio inference engine) - we've shipped some major features:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Speaker Diarization&lt;/strong&gt; - Automatically identify and separate multiple speakers using Sortformer models. Perfect for meeting transcripts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Forced Alignment&lt;/strong&gt; - Word-level timestamps between audio and text using Qwen3-ForcedAligner. Great for subtitles.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-Time Streaming&lt;/strong&gt; - Stream responses for transcribe, chat, and TTS with incremental delivery.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi-Format Audio&lt;/strong&gt; - Native support for WAV, MP3, FLAC, OGG via Symphonia.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance&lt;/strong&gt; - Parallel execution, batch ASR, paged KV cache, Metal optimizations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Support:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;TTS:&lt;/strong&gt; Qwen3-TTS (0.6B, 1.7B), LFM2.5-Audio&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ASR:&lt;/strong&gt; Qwen3-ASR (0.6B, 1.7B), Parakeet TDT, LFM2.5-Audio&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat:&lt;/strong&gt; Qwen3 (0.6B, 1.7), Gemma 3 (1B)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Diarization:&lt;/strong&gt; Sortformer 4-speaker&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Docs: &lt;a href="https://izwiai.com/"&gt;https://izwiai.com/&lt;/a&gt;&lt;br /&gt; Github Repo: &lt;a href="https://github.com/agentem-ai/izwi"&gt;https://github.com/agentem-ai/izwi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Give us a star on GitHub and try it out. Feedback is welcome!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zinyando"&gt; /u/zinyando &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://izwiai.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6bnt2/izwi_update_local_speaker_diarization_forced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6bnt2/izwi_update_local_speaker_diarization_forced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T14:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r69w5y</id>
    <title>RAG failure in production: our vector store served a 3-year-old resume and the LLM hallucinated a candidate recommendation</title>
    <updated>2026-02-16T13:40:39+00:00</updated>
    <author>
      <name>/u/tdeliev</name>
      <uri>https://old.reddit.com/user/tdeliev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So we had a pretty embarrassing RAG failure in production last week and I figured this sub would appreciate the post-mortem. I’ve been calling it the “Split Truth” problem internally because that’s basically what happened — our vector store and SQL database gave the agent two different versions of reality, and the agent picked the wrong one.&lt;/p&gt; &lt;p&gt;Quick context on the stack:&lt;/p&gt; &lt;p&gt;We built a recruiting agent that processes around 800 candidates a week using RAG. Pinecone for the vector store (resumes, interview notes, that kind of semantic stuff) and Postgres for structured state — current job status, contact info, availability, etc. Pretty standard setup. Nothing exotic.&lt;/p&gt; &lt;p&gt;What went wrong:&lt;/p&gt; &lt;p&gt;Agent flags a candidate for a Senior Python role. The reasoning it gave looked solid on paper — “Candidate has 5 years of Python experience, strong backend background, relevant projects.” All technically true. Three years ago.&lt;/p&gt; &lt;p&gt;What actually happened is the candidate had updated their profile yesterday to reflect that they’d pivoted to Project Management two years back. They weren’t even looking for dev roles anymore.&lt;/p&gt; &lt;p&gt;Postgres knew this. The vector store — which still had the old resume chunks embedded — had no idea.&lt;/p&gt; &lt;p&gt;Why the LLM hallucinated:&lt;/p&gt; &lt;p&gt;Here’s the part that frustrated me the most. The LLM saw both signals in the context window. But the vector chunks were way more “descriptive” — paragraphs about Python projects, technical skills, specific frameworks. The SQL data was just a couple of flat fields. So the model weighted the richer, more detailed (and completely outdated) context over the sparse but accurate structured data.&lt;/p&gt; &lt;p&gt;It basically hallucinated a hybrid version of this person. Someone who was both an experienced Python dev AND currently available. Neither was true anymore.&lt;/p&gt; &lt;p&gt;How we fixed it:&lt;/p&gt; &lt;p&gt;We stopped treating the vector store as a source of truth for anything time-sensitive.&lt;/p&gt; &lt;p&gt;The actual fix is a deterministic middleware layer that sits between retrieval and the LLM. Before any context reaches the model, the middleware pulls the latest state from Postgres and injects it as a hard constraint in the system prompt. Something like: “Current Status: NOT LOOKING FOR DEV ROLES. Last profile update: [yesterday’s date].”&lt;/p&gt; &lt;p&gt;That constraint overrides whatever the vector search dragged in. The LLM can still use the semantic data for background context, but it can’t contradict the structured state.&lt;/p&gt; &lt;p&gt;I wrote up the full Python implementation with the actual code if anyone wants to dig into the middleware pattern — how we handle TTL on vector chunks, the sanitization logic, all of it: &lt;a href="https://aimakelab.substack.com/p/anatomy-of-an-agent-failure-the-split"&gt;https://aimakelab.substack.com/p/anatomy-of-an-agent-failure-the-split&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious if anyone else has run into this kind of vector drift in a RAG pipeline. We’re now seeing it as a fundamental architectural issue with any system where the underlying data changes faster than your embedding pipeline can keep up. How are you handling the sync?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tdeliev"&gt; /u/tdeliev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r69w5y/rag_failure_in_production_our_vector_store_served/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r69w5y/rag_failure_in_production_our_vector_store_served/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r69w5y/rag_failure_in_production_our_vector_store_served/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T13:40:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r64deu</id>
    <title>Is there a model that is completely uncensored when it comes to controversial topics?</title>
    <updated>2026-02-16T08:38:50+00:00</updated>
    <author>
      <name>/u/ghulamalchik</name>
      <uri>https://old.reddit.com/user/ghulamalchik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know &amp;quot;uncensored&amp;quot; often means NSFW, for role-play, etc, but that's not really what I care about.&lt;/p&gt; &lt;p&gt;I want a model that has no problem not conforming to typical safety rules. It's willing to engage and objectively assess and consider points that might go directly against &amp;quot;safety guidelines&amp;quot;. Think historical topics, societal issues, religious matters.&lt;/p&gt; &lt;p&gt;I do not want the model to agree with everything I say (that's not hard to achieve, but it's pointless for me) I want one that engages with me with no boundaries on any topic while providing accurate data, and is willing to consider my opinion if it thinks it adds up even if it's extremely controversial and &amp;quot;unsafe&amp;quot;.&lt;/p&gt; &lt;p&gt;Many of us have questions that cannot ask publicly and out-loud. I think this is a great use-case for AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghulamalchik"&gt; /u/ghulamalchik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r64deu/is_there_a_model_that_is_completely_uncensored/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r64deu/is_there_a_model_that_is_completely_uncensored/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r64deu/is_there_a_model_that_is_completely_uncensored/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T08:38:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5vdxc</id>
    <title>That's why I go local.The enshittification is at full steam</title>
    <updated>2026-02-16T00:51:55+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5vdxc/thats_why_i_go_localthe_enshittification_is_at/"&gt; &lt;img alt="That's why I go local.The enshittification is at full steam" src="https://preview.redd.it/94yjg9288rjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a9ce11a5649641939d3beafb60ef54f2472733c" title="That's why I go local.The enshittification is at full steam" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just received an email from chatGPT. Ads are beginning to show up. Well, we are cooked. Not we, we, we. But we are cooked.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94yjg9288rjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5vdxc/thats_why_i_go_localthe_enshittification_is_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5vdxc/thats_why_i_go_localthe_enshittification_is_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T00:51:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1r68xpl</id>
    <title>Achieved 375ms voice-to-voice latency using local Nemotron-4 + Kokoro-82M (Bare Metal)</title>
    <updated>2026-02-16T12:57:47+00:00</updated>
    <author>
      <name>/u/AuraHost-1</name>
      <uri>https://old.reddit.com/user/AuraHost-1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’ve spent the last few months trying to build a Voice AI agent that doesn't feel like a walkie-talkie.&lt;/p&gt; &lt;p&gt;I started with the standard &amp;quot;Wrapper Stack&amp;quot; (Twilio $\to$ Vapi $\to$ GPT-4o $\to$ ElevenLabs), but I couldn't get the round-trip latency under &lt;strong&gt;800ms-1200ms&lt;/strong&gt;. The network hops alone were killing the conversational vibe.&lt;/p&gt; &lt;p&gt;So, I decided to move everything to bare metal (NVIDIA Blackwells) and run it locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Stack that got us to ~375ms:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLM:&lt;/strong&gt; &lt;strong&gt;Nemotron-4&lt;/strong&gt; (4-bit quantized). We found it adheres to instructions better than Llama-3 for conversational turns.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TTS:&lt;/strong&gt; &lt;strong&gt;Kokoro-82M&lt;/strong&gt;. This model is a beast. We are running it directly on the same GPU as the LLM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Orchestration:&lt;/strong&gt; Custom Rust middleware handling the audio buffer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; 96GB NVIDIA Blackwells (Unified memory allows us to keep both models hot without swapping).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Architecture:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of 3 API calls, the audio stream hits our server and stays in VRAM.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;ASR (Nemotron) $\to$ Text&lt;/li&gt; &lt;li&gt;LLM (Nemotron) $\to$ Token Stream&lt;/li&gt; &lt;li&gt;TTS (Kokoro) $\to$ Audio&lt;/li&gt; &lt;li&gt;RAG Nemotron&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Because there are zero network hops between the &amp;quot;Brain&amp;quot; and the &amp;quot;Mouth,&amp;quot; the Time-to-First-Byte (TTFB) is virtually instant.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The &amp;quot;Happy Accident&amp;quot; (HIPAA):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Since we control the metal, I set &lt;code&gt;vm.swappiness=0&lt;/code&gt; and disabled all disk logging. We process the entire call in RAM and flush it at the end. This allowed us to be &amp;quot;HIPAA Compliant&amp;quot; by physics (Zero Retention) rather than just policy, which is a huge unlock for the healthcare clients I work with.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current Pain Points:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failover:&lt;/strong&gt; If a card dies, I have to manually reroute traffic right now. Building a proper Kubernetes operator for this is my next nightmare.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM Management:&lt;/strong&gt; Kokoro is small, but keeping a high-context Nemotron loaded for 50 concurrent streams is tricky. (soak tested to 75 concurrent users with .01% error and 900ms TTFA)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about the Kokoro implementation or the bare-metal config.&lt;/p&gt; &lt;p&gt;&lt;em&gt;(P.S. We just launched a beta on Product Hunt if you want to stress-test the latency yourself. Link in comments.)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AuraHost-1"&gt; /u/AuraHost-1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r68xpl/achieved_375ms_voicetovoice_latency_using_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r68xpl/achieved_375ms_voicetovoice_latency_using_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r68xpl/achieved_375ms_voicetovoice_latency_using_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T12:57:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5h1gj</id>
    <title>You can run MiniMax-2.5 locally</title>
    <updated>2026-02-15T15:14:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"&gt; &lt;img alt="You can run MiniMax-2.5 locally" src="https://preview.redd.it/hd369oaucojg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=baf9267391b3836cb000418670d350915c3a8405" title="You can run MiniMax-2.5 locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiniMax-2.5 is a new open LLM achieving SOTA in coding, agentic tool use and search and office work.&lt;/p&gt; &lt;p&gt;The 230B parameters (10B active) model has a &lt;strong&gt;200K context&lt;/strong&gt; window and unquantized bf16 requires &lt;strong&gt;457GB&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Unsloth Dynamic &lt;strong&gt;3-bit&lt;/strong&gt; GGUF reduces size to &lt;strong&gt;101GB&lt;/strong&gt; &lt;strong&gt;(-62%).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official Guide -&lt;/strong&gt; &lt;a href="https://unsloth.ai/docs/models/minimax-2.5"&gt;&lt;strong&gt;https://unsloth.ai/docs/models/minimax-2.5&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GGUF Models -&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/MiniMax-M2.5-GGUF"&gt;&lt;strong&gt;https://huggingface.co/unsloth/MiniMax-M2.5-GGUF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Top LLM, RAG and AI Agents updates of this week&lt;/strong&gt; - &lt;a href="https://aixfunda.substack.com/p/top-llm-rag-and-agent-updates-of-03a"&gt;https://aixfunda.substack.com/p/top-llm-rag-and-agent-updates-of-03a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hd369oaucojg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T15:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r645g6</id>
    <title>Minimax M2.5 vs. GLM-5 vs. Kimi k2.5: How do they compare to Codex and Claude for coding?</title>
    <updated>2026-02-16T08:25:15+00:00</updated>
    <author>
      <name>/u/East-Stranger8599</name>
      <uri>https://old.reddit.com/user/East-Stranger8599</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m looking for community feedback from those of you who have hands-on experience with the recent wave of coding models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Minimax M2.5&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-5&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kimi k2.5&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;There are plenty of benchmarks out there, but I’m interested in your subjective opinions and day-to-day experience.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you use multiple models:&lt;/strong&gt; Have you noticed significant differences in their &amp;quot;personality&amp;quot; or logic when switching between them? For example, is one noticeably better at scaffolding while another is better at debugging or refactoring?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you’ve mainly settled on one:&lt;/strong&gt; How does it stack up against the major incumbents like &lt;strong&gt;Codex&lt;/strong&gt; or &lt;strong&gt;Anthropic’s Claude&lt;/strong&gt; models?&lt;/p&gt; &lt;p&gt;I’m specifically looking to hear if any of these newer models offer a distinct advantage over other or feel different to drive, or if they just feel like &amp;quot;more of the same.&amp;quot;&lt;/p&gt; &lt;p&gt;--- &lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;For exactly the same feature development I gave the exact same prompt to both ZLM 5 and MiniMax 2.5. Then ask Gemini 3.0 to judge between both implementation and Here is what it wrote below:&lt;/p&gt; &lt;h1&gt;Case Study: ZLM 5 vs. MiniMax M2.5 – A Code Architecture Showdown&lt;/h1&gt; &lt;p&gt;In the world of modern frontend development, two distinct philosophies often clash: structural purity vs. pragmatic cohesion. Today, we analyze two implementations of the same task feature—codenamed &lt;strong&gt;ZLM 5&lt;/strong&gt; and &lt;strong&gt;MiniMax M2.5&lt;/strong&gt;—to see which approach yields a more robust, production-ready result.&lt;/p&gt; &lt;h1&gt;The Contenders&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;ZLM 5 (Zai)&lt;/strong&gt; champions the classic &lt;strong&gt;Container/Presenter pattern&lt;/strong&gt;. It strictly separates logic from UI, hoisting all state management and API interactions up to a parent container. The view component is kept &amp;quot;dumb,&amp;quot; receiving data and callbacks purely via props.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MiniMax M2.5 (Mm)&lt;/strong&gt; takes a &lt;strong&gt;Self-Contained Component&lt;/strong&gt; approach. It co-locates logic with the UI, managing its own loading states and utilizing shared, typed API adapters.&lt;/p&gt; &lt;h1&gt;The Analysis&lt;/h1&gt; &lt;h1&gt;1. Architectural Purity vs. Pragmatism&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;ZLM 5&lt;/strong&gt; excels in separation of concerns. By keeping the view dumb, it effectively decouples the UI from the business logic. However, this purity comes at a cost: the parent container becomes bloated with implementation details (like raw &lt;code&gt;fetch&lt;/code&gt; calls), and the prop drilling increases complexity.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MiniMax M2.5&lt;/strong&gt; embraces modern React patterns. By encapsulating its own logic, the component becomes a &amp;quot;smart&amp;quot; unit that can be dropped into any layout without requiring the parent to orchestrate every interaction. This reduces friction and makes the codebase more modular.&lt;/p&gt; &lt;h1&gt;2. Implementation Quality&lt;/h1&gt; &lt;p&gt;Where &lt;strong&gt;ZLM 5&lt;/strong&gt; falters is in its reliance on &lt;strong&gt;raw&lt;/strong&gt; &lt;code&gt;fetch&lt;/code&gt; &lt;strong&gt;calls&lt;/strong&gt;. Hardcoding API endpoints directly inside a component makes refactoring painful and violates DRY (Don't Repeat Yourself) principles.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MiniMax M2.5&lt;/strong&gt;, in contrast, utilizes a &lt;strong&gt;shared API library&lt;/strong&gt;. This not only ensures type safety but also means that if an endpoint changes, you only update it in one place. Additionally, MiniMax M2.5 includes built-in user feedback—loading spinners and toast notifications—which were entirely absent in ZLM 5.&lt;/p&gt; &lt;h1&gt;The Verdict&lt;/h1&gt; &lt;p&gt;While ZLM 5 offers a textbook example of separation, &lt;strong&gt;MiniMax M2.5 is the clear winner&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Its use of reusable API adapters, integrated error handling, and superior user experience makes it the more mature and maintainable solution. In real-world production environments, the maintainability and robustness of MiniMax M2.5 far outweigh the theoretical purity of ZLM 5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/East-Stranger8599"&gt; /u/East-Stranger8599 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r645g6/minimax_m25_vs_glm5_vs_kimi_k25_how_do_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r645g6/minimax_m25_vs_glm5_vs_kimi_k25_how_do_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r645g6/minimax_m25_vs_glm5_vs_kimi_k25_how_do_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T08:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6d9w5</id>
    <title>Local running Qwen3:14b helped fix my internet on Linux while offline</title>
    <updated>2026-02-16T15:53:46+00:00</updated>
    <author>
      <name>/u/iqraatheman</name>
      <uri>https://old.reddit.com/user/iqraatheman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6d9w5/local_running_qwen314b_helped_fix_my_internet_on/"&gt; &lt;img alt="Local running Qwen3:14b helped fix my internet on Linux while offline" src="https://preview.redd.it/3ck7uzopovjg1.png?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=b5119a92733a97601f85e77852bc0d95aaae98f1" title="Local running Qwen3:14b helped fix my internet on Linux while offline" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/3ck7uzopovjg1.png?width=2566&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fe75c88681a864d2962b00d5dff5222ded2cbf0e"&gt;Conversation with Qwen3:14b over Opencode in which it runs a command and correctly diagnoses network problem.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the first things I did after recently installation Arch Linux on my PC was set up Opencode with Ollama just in case my internet went out and I couldn't figure out what commands to run to fix it. I installed the 14B parameter version because I figured it was the best model I could fit in my 16 GB of VRAM on my AMD Radeon RX 7800 XT and it's really fast. I am super grateful that I did this because my internet did get disconnected and luckily in this case it was just because I accidentally unplugged the Ethernet cable as it was laying across the middle of my room but it would've taken me so long to figure out what caused this had I not set this up. I would've had to either google it or ask an AI model running in the cloud from another device, neither of which would be possible had my internet truly been out and it not just being a problem with this device's Ethernet only. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iqraatheman"&gt; /u/iqraatheman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6d9w5/local_running_qwen314b_helped_fix_my_internet_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6d9w5/local_running_qwen314b_helped_fix_my_internet_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6d9w5/local_running_qwen314b_helped_fix_my_internet_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T15:53:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1r68vle</id>
    <title>AI field is changing so quickly and there is so much to read..</title>
    <updated>2026-02-16T12:55:00+00:00</updated>
    <author>
      <name>/u/amisra31</name>
      <uri>https://old.reddit.com/user/amisra31</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone else feel like they're drowning in AI content?&lt;/p&gt; &lt;p&gt;every single day theres a new model, new paper, new breakthrough. i open twitter and scroll for an hour. check reddit for another hour. and somehow i still feel like i learned nothing useful.&lt;/p&gt; &lt;p&gt;its all just surface level stuff. &amp;quot;new model drops!&amp;quot; okay cool but what does it actually DO? no idea because i just read the headline.&lt;/p&gt; &lt;p&gt;the actual important info is scattered everywhere. research papers, github, random blogs, discord servers, some guys newsletter. i cant keep up.&lt;/p&gt; &lt;p&gt;i spend so much time on social media trying to stay updated but its mostly noise and hype. the real valuable stuff? i probably miss it completely.&lt;/p&gt; &lt;p&gt;how do you guys handle this? do you have like a system or something? specific sources you trust?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amisra31"&gt; /u/amisra31 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r68vle/ai_field_is_changing_so_quickly_and_there_is_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r68vle/ai_field_is_changing_so_quickly_and_there_is_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r68vle/ai_field_is_changing_so_quickly_and_there_is_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T12:55:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r67b43</id>
    <title>Forked OpenClaw to run fully air-gapped (no cloud deps)</title>
    <updated>2026-02-16T11:35:00+00:00</updated>
    <author>
      <name>/u/zsb5</name>
      <uri>https://old.reddit.com/user/zsb5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been playing with OpenClaw, but I couldn't actually use it for anything work-related because of the data egress. The agentic stuff is cool, but sending everything to OpenAI/cloud APIs is a non-starter for my setup.&lt;/p&gt; &lt;p&gt;So I spent the weekend ripping out the cloud dependencies to make a fork that runs strictly on-prem.&lt;/p&gt; &lt;p&gt;It’s called Physiclaw (&lt;a href="http://www.physiclaw.dev"&gt;www.physiclaw.dev&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Basically, I swapped the default runtime to target local endpoints (vLLM / llama.cpp) and stripped the telemetry. I also started breaking the agent into specific roles (SRE, SecOps) with limited tool access instead of one generic assistant that has root access to everything.&lt;/p&gt; &lt;p&gt;The code is still pretty raw/alpha, but the architecture for the air-gapped runtime is there.&lt;/p&gt; &lt;p&gt;If anyone is running agents in secure environments or just hates cloud dependencies, take a look and let me know if I missed any obvious leaks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/CommanderZed/Physiclaw"&gt;&lt;strong&gt;https://github.com/CommanderZed/Physiclaw&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zsb5"&gt; /u/zsb5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r67b43/forked_openclaw_to_run_fully_airgapped_no_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r67b43/forked_openclaw_to_run_fully_airgapped_no_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r67b43/forked_openclaw_to_run_fully_airgapped_no_cloud/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T11:35:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6bfky</id>
    <title>Q8: Is the Q8 still the king quant if we have the vram?</title>
    <updated>2026-02-16T14:43:43+00:00</updated>
    <author>
      <name>/u/crowtain</name>
      <uri>https://old.reddit.com/user/crowtain</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; Since I started using LLMs, the consensus was already that Q8 was near FP16 . so even if i was trying using a small model that can run in FP16, i used by default Q8.&lt;br /&gt; of course, if i want some bigger models that doesn't fit in my hardware, i go for more aggressive Quant like Q6 or even Q3 KL for the minimax.&lt;br /&gt; but with the new dynamic quant 2 of unsloth and ubergarm, Q6 seems also to have very few degradations.&lt;br /&gt; So, can the Q6 dynamic quant be used as standard ? to benefit from the small speed increase, model storage and of course a little VRAM/RAM space also?&lt;br /&gt; in the benchmark, the perplexity loss is so low for the Q6, that even in agentic coding using it instead of Q8 seems legit. &lt;/p&gt; &lt;p&gt;P.S: i'm not talking about oh Q2 of 120B is better than Q4 of 60B, there is always this debate that depends on the use case and the model itself &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crowtain"&gt; /u/crowtain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6bfky/q8_is_the_q8_still_the_king_quant_if_we_have_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6bfky/q8_is_the_q8_still_the_king_quant_if_we_have_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6bfky/q8_is_the_q8_still_the_king_quant_if_we_have_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T14:43:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5uhfu</id>
    <title>Deflation: Cost to train A.I. models drops 40% per year - Karpathy</title>
    <updated>2026-02-16T00:11:13+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/karpathy/nanochat/discussions/481"&gt;https://github.com/karpathy/nanochat/discussions/481&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quote: ..., each year the cost to train GPT-2 is falling to approximately 40% of the previous year. (I think this is an underestimate and that further improvements are still quite possible). The gains come from everywhere: better hardware (H100 vs TPU v3), better software (Flash Attention 3, torch.compile), better algorithms (Muon optimizer, architectural improvements), and better data (FineWeb-edu).&lt;/p&gt; &lt;h1&gt;What Worked&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Flash Attention 3&lt;/strong&gt; — ~9% tok/sec improvement. Native tensor layout, single API for training and inference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sliding window attention&lt;/strong&gt; — &lt;code&gt;SSSL&lt;/code&gt; pattern. Compute savings without quality loss.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Muon optimizer overhaul&lt;/strong&gt; — Polar Express, NorMuon variance reduction, cautious weight decay with linear schedule to zero. The cautious WD was a clear win. I tried to delete Muon and couldn't.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Per-layer residual scalars&lt;/strong&gt; — &lt;code&gt;x = λ_resid * x + λ_x0 * x0&lt;/code&gt;. Consistent improvement across all model sizes (0.003-0.01 bpb).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Value Embeddings at alternating layers&lt;/strong&gt; — Models love the value embeddings capacity. Any attempt to reduce it (low-rank, sharing, projections) hurt. We tried U-shaped placement, every layer, alternating—alternating won.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;BOS-aligned dataloader&lt;/strong&gt; — Every row starts with BOS. Made midtraining unnecessary (deleted it). BestFit-Crop packing reduces waste vs naive cropping.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hyperparameter sweep at scale&lt;/strong&gt; — 320 experiments to find that &lt;code&gt;x0_beta1=0.96&lt;/code&gt; is optimal at d20. Key lesson: small-scale tuning doesn't transfer. Validate at target scale.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scaling law discovery&lt;/strong&gt; — We empirically measured the optimal tokens:params ratio to be ~10. It's important to do the actual experiment on your own network.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;What Didn't Work&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Multi-token prediction (MTP)&lt;/strong&gt; — +13GB memory, no improvement&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Varlen attention&lt;/strong&gt; — BOS-aligned dataloader already handles this to some extent. Attending across BOS document boundaries does not seem to make things much worse.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FP8 for lm_head&lt;/strong&gt; — Works, but +2GB memory (!), only 1% speedup, todo to look into more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Half-truncated RoPE&lt;/strong&gt; — No improvement&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Asymmetric softcap&lt;/strong&gt; — Slightly worse&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Skip connections / backout&lt;/strong&gt; — No improvement, +2GB memory&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smear gate, attention gates&lt;/strong&gt; — Negligible improvement, not worth complexity&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch size schedule&lt;/strong&gt; — Deemed a little too complex&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bigram embeddings (Engram-lite)&lt;/strong&gt; — Works, but not by too much, and it bloats complexity and parameter count by a lot, so it was skipped in the end.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hyperball/MuonH&lt;/strong&gt; — Intriguing idea, didn't work out of the box&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5uhfu/deflation_cost_to_train_ai_models_drops_40_per/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5uhfu/deflation_cost_to_train_ai_models_drops_40_per/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5uhfu/deflation_cost_to_train_ai_models_drops_40_per/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T00:11:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r66jyp</id>
    <title>vLLM MAXIMUM performance on multi-3090</title>
    <updated>2026-02-16T10:52:53+00:00</updated>
    <author>
      <name>/u/Nepherpitu</name>
      <uri>https://old.reddit.com/user/Nepherpitu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r66jyp/vllm_maximum_performance_on_multi3090/"&gt; &lt;img alt="vLLM MAXIMUM performance on multi-3090" src="https://preview.redd.it/w72h1xab7ujg1.png?width=140&amp;amp;height=110&amp;amp;auto=webp&amp;amp;s=49132adde3816a5139ccab9cae67486cc9fec858" title="vLLM MAXIMUM performance on multi-3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: install patched p2p driver, patch vllm platform and skip p2p check. You'll get +50% performance on 4x3090 with Qwen3 Coder Next FP8. Free performance, free tokens, very nice :)&lt;/p&gt; &lt;p&gt;So, YOU (yes, YOU) managed to setup vLLM on your multi gpu platform with consumer cards. It's nice, running fast and doesn't lose a lot of performance on long contexts. But there are HIDDEN and FREE performance laying here just for you.&lt;/p&gt; &lt;p&gt;Let's go into the deep.&lt;/p&gt; &lt;h2&gt;Prerequisite&lt;/h2&gt; &lt;p&gt;I assume you have something like cheap RTX 3090 and running vLLM with tensor parallelism on linux without docker. Otherwise I cannot guarantee results. As if I could guarantee anything otherwise, lol.&lt;/p&gt; &lt;h3&gt;Resizable bar&lt;/h3&gt; &lt;p&gt;You need to enable resizable bar. Check it with &lt;code&gt;sudo lspci -vvv | grep -i -A40 'VGA compatible controller'&lt;/code&gt;, look for &lt;code&gt;Region 1: Memory at 17800000000 (64-bit, prefetchable) [size=32G]&lt;/code&gt;. If it's &lt;code&gt;32M&lt;/code&gt;, then you need to flash new BIOS. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.techpowerup.com/download/nvidia-nvflash/"&gt;https://www.techpowerup.com/download/nvidia-nvflash/&lt;/a&gt; - nvflash&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.techpowerup.com/vgabios/231650/msi-rtx3090-24576-210310-1"&gt;https://www.techpowerup.com/vgabios/231650/msi-rtx3090-24576-210310-1&lt;/a&gt; - example where to find updated bios&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Just reboot in safe mode and follow intuitive &lt;code&gt;./nvflash help&lt;/code&gt; output. It's that simple. &lt;/p&gt; &lt;h3&gt;PCIe lanes&lt;/h3&gt; &lt;p&gt;GPUs must be connected with enough PCIe lanes to achieve desired bandwidth. How many lanes? Well... I've didn't seen more than 4GB/s IN + 4GB/s OUT, so PCIe 3.0 X8 OR PCIe 4.0 X4 must be ok enough. Maybe not, who knows. Try it yourself. But PCIe 3.0 X1 is not ok anyway.&lt;/p&gt; &lt;h3&gt;Similar cards in parallel.&lt;/h3&gt; &lt;p&gt;This is tricky, you can't mix 3090 + 4090. I mean, technically you can, and it will be BLAZING FAST. But completely incorrect and incoherent. Maybe. Maybe 30B FP16 models will be good. &lt;/p&gt; &lt;p&gt;Check bug here - &lt;a href="https://github.com/vllm-project/vllm/issues/34437#issuecomment-3903773323"&gt;https://github.com/vllm-project/vllm/issues/34437#issuecomment-3903773323&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Setup instructions&lt;/h2&gt; &lt;h3&gt;Install patched P2P driver&lt;/h3&gt; &lt;p&gt;&lt;a href="https://github.com/aikitoria/open-gpu-kernel-modules"&gt;https://github.com/aikitoria/open-gpu-kernel-modules&lt;/a&gt; - follow instruction here. Don't forget to reboot. Maybe you will need to compile CUDA samples (I don't remember where I get them) with p2pBandwidthTest to verify it works.&lt;/p&gt; &lt;p&gt;You must get similar output:&lt;/p&gt; &lt;p&gt;&lt;code&gt; ~# nvidia-smi topo -p2p r GPU0 GPU1 GPU2 GPU3 GPU0 X OK OK OK GPU1 OK X OK OK GPU2 OK OK X OK GPU3 OK OK OK X &lt;/code&gt;&lt;/p&gt; &lt;p&gt;And if your p2p bandwidth test shows you 0.02GB/s transfer rates, go check and resizable bar support.&lt;/p&gt; &lt;h3&gt;Patch vLLM&lt;/h3&gt; &lt;p&gt;For unknown incomprehensible reason, vLLM tests p2p availability only for NVLink. Yep, you have patched driver and ik_llama.cpp now is blazing fast (probably), but vLLM still show you &amp;quot;Custom all-reduce is disabled, you moron! ~nya&amp;quot;. Time to fix it.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Go to &lt;code&gt;env/lib/blablabla/site-packages/vllm&lt;/code&gt;. Now you can EDIT anything in vllm sources. Well, cuda kernels are compiled, but we are stupid and don't know how to edit them. Otherwise 3090+4090 issue would be already fixed.&lt;/li&gt; &lt;li&gt;You need to do &lt;code&gt;vi env_vllm/lib/python3.13/site-packages/vllm/platforms/cuda.py&lt;/code&gt;. There is line 597 &lt;a href="https://github.com/vllm-project/vllm/blob/main/vllm/platforms/cuda.py#L597"&gt;https://github.com/vllm-project/vllm/blob/main/vllm/platforms/cuda.py#L597&lt;/a&gt; . Make it just &lt;code&gt;return True&lt;/code&gt;. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That's all. We're telling vLLM &amp;quot;Trust me bro, I have my GPUs fully connected AND I DON'T KNOW HOW IT WILL AFFECT MY SYSTEM&amp;quot;.&lt;/p&gt; &lt;h2&gt;Profit!&lt;/h2&gt; &lt;p&gt;And load you're favorite Qwen3 Coder Next FP8 with -tp 4 and look at numbers. Single request will go up from ~100 tps to ~150 tps. Or maybe not, because I'm lucky and you are not lucky.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;(APIServer pid=1689046) INFO 02-16 13:51:25 [loggers.py:259] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 144.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.3%&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nepherpitu"&gt; /u/Nepherpitu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r66jyp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r66jyp/vllm_maximum_performance_on_multi3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r66jyp/vllm_maximum_performance_on_multi3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T10:52:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r63fhu</id>
    <title>Why is everything about code now?</title>
    <updated>2026-02-16T07:41:24+00:00</updated>
    <author>
      <name>/u/falconandeagle</name>
      <uri>https://old.reddit.com/user/falconandeagle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hate hate hate how every time a new model comes out its about how its better at coding. What happened to the heyday of llama 2 finetunes that were all about creative writing and other use cases.&lt;/p&gt; &lt;p&gt;Is it all the vibe coders that are going crazy over the models coding abilities??&lt;/p&gt; &lt;p&gt;Like what about other conversational use cases? I am not even talking about gooning (again opus is best for that too), but long form writing, understanding context at more than a surface level. I think there is a pretty big market for this but it seems like all the models created these days are for fucking coding. Ugh.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/falconandeagle"&gt; /u/falconandeagle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r63fhu/why_is_everything_about_code_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r63fhu/why_is_everything_about_code_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r63fhu/why_is_everything_about_code_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T07:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r657w5</id>
    <title>Qwen3.5 Release Blog Post</title>
    <updated>2026-02-16T09:31:44+00:00</updated>
    <author>
      <name>/u/Stunning_Energy_7028</name>
      <uri>https://old.reddit.com/user/Stunning_Energy_7028</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weights: &lt;a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B"&gt;https://huggingface.co/Qwen/Qwen3.5-397B-A17B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stunning_Energy_7028"&gt; /u/Stunning_Energy_7028 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qwen.ai/blog?id=qwen3.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r657w5/qwen35_release_blog_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r657w5/qwen35_release_blog_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T09:31:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1r68z93</id>
    <title>llama-cpp ROCm Prompt Processing speed on Strix Halo / Ryzen AI Max +50-100%</title>
    <updated>2026-02-16T12:59:54+00:00</updated>
    <author>
      <name>/u/Excellent_Jelly2788</name>
      <uri>https://old.reddit.com/user/Excellent_Jelly2788</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r68z93/llamacpp_rocm_prompt_processing_speed_on_strix/"&gt; &lt;img alt="llama-cpp ROCm Prompt Processing speed on Strix Halo / Ryzen AI Max +50-100%" src="https://preview.redd.it/0o14pkcytujg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78c0fa42f3ed7ac5b4feaf2d6cca6b439cf3f367" title="llama-cpp ROCm Prompt Processing speed on Strix Halo / Ryzen AI Max +50-100%" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit: As the comments pointed out, this was just a bug that was going on for the last ~2 weeks and we are back to the previous performance.&lt;/p&gt; &lt;p&gt;Prompt Processing on Strix Halo (Ryzen AI Max) with ROCm got way faster for a lot of models in the last couple days when using llamacpp-rocm ( &lt;a href="https://github.com/lemonade-sdk/llamacpp-rocm"&gt;https://github.com/lemonade-sdk/llamacpp-rocm&lt;/a&gt; ).&lt;/p&gt; &lt;p&gt;GLM was comparable to Vulkan already on the old version and didnt see major speedup.&lt;/p&gt; &lt;p&gt;Token Generation is ~ the same&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;PP t/s (depth 0)&lt;/th&gt; &lt;th align="left"&gt;Vulkan&lt;/th&gt; &lt;th align="left"&gt;ROCm 1184 (Feb 11)&lt;/th&gt; &lt;th align="left"&gt;ROCm 1188 (Feb 15)&lt;/th&gt; &lt;th align="left"&gt;ROCm vs ROCm&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Nemotron-3-Nano-30B-A3B-Q8_0&lt;/td&gt; &lt;td align="left"&gt;1043&lt;/td&gt; &lt;td align="left"&gt;501&lt;/td&gt; &lt;td align="left"&gt;990&lt;/td&gt; &lt;td align="left"&gt;+98 %&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-OSS-120B-MXFP4&lt;/td&gt; &lt;td align="left"&gt;555&lt;/td&gt; &lt;td align="left"&gt;261&lt;/td&gt; &lt;td align="left"&gt;605&lt;/td&gt; &lt;td align="left"&gt;+132 %&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-Next-MXFP4-MOE&lt;/td&gt; &lt;td align="left"&gt;539&lt;/td&gt; &lt;td align="left"&gt;347&lt;/td&gt; &lt;td align="left"&gt;615&lt;/td&gt; &lt;td align="left"&gt;+77 %&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM4.7-Flash-UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;953&lt;/td&gt; &lt;td align="left"&gt;923&lt;/td&gt; &lt;td align="left"&gt;985&lt;/td&gt; &lt;td align="left"&gt;+7 %&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Interactive Charts:&lt;/p&gt; &lt;p&gt;&lt;a href="https://evaluateai.ai/benchmarks/?models=Nemotron-3-Nano-30B-A3B-Q8_0&amp;amp;versions=b1184%2Cb1185%2Cb1186%2Cb1187%2CB1187%2Cb1188%2Cb7984%2Cb7993%2Cb7999%2Cb8001%2Cb8054%2Cb8058%2Cb8064%2Cb8067&amp;amp;cpus=AMD+RYZEN+AI+MAX%2B+395+w%2F+Radeon+8060S&amp;amp;slots=mn%2Cci%2Cgm%2Cie"&gt;Nemotron&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://evaluateai.ai/benchmarks/?models=gpt-oss-120b-mxfp4&amp;amp;versions=b1184%2Cb1185%2Cb1186%2Cb1187%2CB1187%2Cb1188%2Cb7984%2Cb7993%2Cb7999%2Cb8001%2Cb8054%2Cb8058%2Cb8064%2Cb8067&amp;amp;cpus=AMD+RYZEN+AI+MAX%2B+395+w%2F+Radeon+8060S&amp;amp;slots=mn%2Cci%2Cgm%2Cie"&gt;GPT-OSS-120B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://evaluateai.ai/benchmarks/?models=Qwen3-Coder-Next-MXFP4_MOE&amp;amp;versions=b1184%2Cb1185%2Cb1186%2Cb1187%2CB1187%2Cb1188%2Cb7984%2Cb7993%2Cb7999%2Cb8001%2Cb8054%2Cb8058%2Cb8064%2Cb8067&amp;amp;cpus=AMD+RYZEN+AI+MAX%2B+395+w%2F+Radeon+8060S&amp;amp;slots=mn%2Cci%2Cgm%2Cie"&gt;Qwen3-Coder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://evaluateai.ai/benchmarks/?models=GLM-4.7-Flash-UD-Q4_K_XL&amp;amp;versions=b1184%2Cb1185%2Cb1186%2Cb1187%2CB1187%2Cb1188%2Cb7984%2Cb7993%2Cb7999%2Cb8001%2Cb8054%2Cb8058%2Cb8064%2Cb8067&amp;amp;cpus=AMD+RYZEN+AI+MAX%2B+395+w%2F+Radeon+8060S&amp;amp;slots=mn%2Cci%2Cgm%2Cie"&gt;GLM-4.7-Flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Disclaimer: &lt;a href="http://Evaluateai.ai"&gt;Evaluateai.ai&lt;/a&gt; is my project. I ran performance benchmarks for the last week on a variety of models on my AI Max 395+ and a few on a AMD Epyc CPU only system. Next step is comparing the output quality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Jelly2788"&gt; /u/Excellent_Jelly2788 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0o14pkcytujg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r68z93/llamacpp_rocm_prompt_processing_speed_on_strix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r68z93/llamacpp_rocm_prompt_processing_speed_on_strix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T12:59:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5v1jb</id>
    <title>Anyone actually using Openclaw?</title>
    <updated>2026-02-16T00:36:08+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am highly suspicious that openclaw's virality is organic. I don't know of anyone (online or IRL) that is actually using it and I am deep in the AI ecosystem (both online and IRL). If this sort of thing is up anyone's alley, its the members of localllama - so are you using it? &lt;/p&gt; &lt;p&gt;With the announcement that OpenAI bought OpenClaw, conspiracy theory is that it was manufactured social media marketing (on twitter) to hype it up before acquisition. Theres no way this graph is real: &lt;a href="https://www.star-history.com/#openclaw/openclaw&amp;amp;Comfy-Org/ComfyUI&amp;amp;type=date&amp;amp;legend=top-left"&gt;https://www.star-history.com/#openclaw/openclaw&amp;amp;Comfy-Org/ComfyUI&amp;amp;type=date&amp;amp;legend=top-left&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5v1jb/anyone_actually_using_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5v1jb/anyone_actually_using_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5v1jb/anyone_actually_using_openclaw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T00:36:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60ety</id>
    <title>Qwen 3.5 will be released today</title>
    <updated>2026-02-16T04:54:20+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"&gt; &lt;img alt="Qwen 3.5 will be released today" src="https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=ff46b508a3b564db9ac8039bb61d1b0f08588ef3" title="Qwen 3.5 will be released today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sources reveal that Alibaba will open-source its next-generation large model, Qwen3.5, tonight on Lunar New Year's Eve. The model reportedly features a comprehensive innovation in its architecture.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b95152330c1b5ebdb5b7022dd6762ebe1890fd06"&gt;https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b95152330c1b5ebdb5b7022dd6762ebe1890fd06&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/Sino_Market/status/2023218866370068561?s=20"&gt;https://x.com/Sino_Market/status/2023218866370068561?s=20&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T04:54:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6599e</id>
    <title>Qwen3.5-397B-A17B Unsloth GGUFs</title>
    <updated>2026-02-16T09:34:10+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6599e/qwen35397ba17b_unsloth_ggufs/"&gt; &lt;img alt="Qwen3.5-397B-A17B Unsloth GGUFs" src="https://preview.redd.it/zgfpbga5ttjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b525bb85c217819dae77ecab42757b843211d14" title="Qwen3.5-397B-A17B Unsloth GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen releases Qwen3.5💜! Run 3-bit on a 192GB RAM Mac, or 4-bit (MXFP4) on an M3 Ultra with 256GB RAM (or less). Qwen releases the first open model of their Qwen3.5 family. &lt;a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B"&gt;https://huggingface.co/Qwen/Qwen3.5-397B-A17B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It performs on par with Gemini 3 Pro, Claude Opus 4.5, and GPT-5.2.&lt;/p&gt; &lt;p&gt;Guide to run them: &lt;a href="https://unsloth.ai/docs/models/qwen3.5"&gt;https://unsloth.ai/docs/models/qwen3.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unsloth dynamic GGUFs at: &lt;a href="https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF"&gt;https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Excited for this week! 🙂&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zgfpbga5ttjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6599e/qwen35397ba17b_unsloth_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6599e/qwen35397ba17b_unsloth_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T09:34:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r656d7</id>
    <title>Qwen3.5-397B-A17B is out!!</title>
    <updated>2026-02-16T09:29:03+00:00</updated>
    <author>
      <name>/u/lolxdmainkaisemaanlu</name>
      <uri>https://old.reddit.com/user/lolxdmainkaisemaanlu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B"&gt;https://huggingface.co/Qwen/Qwen3.5-397B-A17B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lolxdmainkaisemaanlu"&gt; /u/lolxdmainkaisemaanlu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T09:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t775</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2026-02-13T16:07:54+00:00</updated>
    <author>
      <name>/u/HardToVary</name>
      <uri>https://old.reddit.com/user/HardToVary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt; &lt;img alt="AMA with MiniMax — Ask Us Anything!" src="https://preview.redd.it/5z2li1ntcajg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=ce3340cd37b7e9408878509be00dd7b871efebde" title="AMA with MiniMax — Ask Us Anything!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;We're &lt;strong&gt;MiniMax&lt;/strong&gt;, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;.5&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining the channel today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — Founder of MiniMax&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Wise_Evidence9973"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ryan85127704"&gt;u/ryan85127704&lt;/a&gt; — Head of Engineering&lt;/li&gt; &lt;li&gt;&lt;a href="/u/HardToVary"&gt;u/HardToVary&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c"&gt;https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. We'll continue monitoring and responding to questions for 48 hours after the end of the AMA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HardToVary"&gt; /u/HardToVary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; 👋&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AM–11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;⚠️ &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don’t post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
</feed>
