<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-30T22:24:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nui2wx</id>
    <title>Any dev using LocalLLMs on daily work want to share their setups and experiences?</title>
    <updated>2025-09-30T16:36:49+00:00</updated>
    <author>
      <name>/u/Safe-Ad6672</name>
      <uri>https://old.reddit.com/user/Safe-Ad6672</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe my google foo is weak today, but I couldn't find many developers sharing their experiences with running localLLMs for daily develoment work&lt;/p&gt; &lt;p&gt;I'm genuinelly thinking about buying some M4 Mac Mini to run a coding agent with KiloCode and sst/OpenCode, because it seems to be the best value for the workload&lt;/p&gt; &lt;p&gt;I think my english fails me by Setup I mean specifically Hardware &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Safe-Ad6672"&gt; /u/Safe-Ad6672 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nui2wx/any_dev_using_localllms_on_daily_work_want_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nui2wx/any_dev_using_localllms_on_daily_work_want_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nui2wx/any_dev_using_localllms_on_daily_work_want_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T16:36:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nugpbu</id>
    <title>ByteBot - Why no hype train for these guys? This is the first Computer Use Agent I’ve seen actually work with local models!</title>
    <updated>2025-09-30T15:44:35+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL:DR I’ve tried a bunch of Computer Use Agent projects and have found them all completely disappointing, useless, and usually janky. While definitely not perfect by any means, ByteBot seems like the most promising CUA project I’ve seen in a long time. It is a bit of a pain to get running with local models, but WOW, this thing has a lot of potential with the right vision model driving it. Is it magic? No, but It’s definitely worth taking a look at if you’re into computer use agent stuff. &lt;/p&gt; &lt;p&gt;ByteBot AI GitHub:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bytebot-ai/bytebot"&gt;https://github.com/bytebot-ai/bytebot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’ve tried like 4 or 5 different projects that promised they were legit Computer Use Agents (CUA’s), but they either just completely didn’t work past the basic canned example or they required paid frontier models and a crap ton of tokens to be useful. Even the ones that did actually work still failed miserably to complete basic tasks that would make them useful for any real work. &lt;/p&gt; &lt;p&gt;I had kind of given up on Computer Use Agents entirely. It just seemed like one of those things that needed like 6 months more of simmering before someone finally cracks the concept and builds something legitimately useful &lt;/p&gt; &lt;p&gt;I tried the TryCUA project, but man, its instructions kinda blow. I never could get it running. I also messed with Microsoft’s Omniparser V2 / OmniBox / OmniTool stack, but it was kind of just a proof-of-concept project they made and it has become abandonware as they aren’t really maintaining it at all. A lot of projects borrow pieces and parts of their tech tho. &lt;/p&gt; &lt;p&gt;I also tried Open Interpreter, that project seemed like it was going somewhere and had potential but they seem to have stalled, their GitHub seems pretty stagnant for the last few months. The same seems true for the Self Operating Computer project which looks to be completely forgotten about and abandoned as well. &lt;/p&gt; &lt;p&gt;So I had pretty low expectations when I stumbled upon ByteBot’s GitHub, but HOLY CARP this thing is the first damn computer use agent that I’ve got to work straight out of the gate. &lt;/p&gt; &lt;p&gt;Granted, I initially used a Gemini 2.5 Flssh API key just to give it a spin, and I’ll be damned if it didn’t open up VS code on its sandbox VM and write me a “hello world” python file and save it. Beyond just kicking the tires, don’t use Gemiii free tier or any other free tier API for anything beyond a quick test because you’ll hit rate limits quick as this thing eats tokens fast. &lt;/p&gt; &lt;p&gt;The ByteBot interface is simple and straightforward, and they use a pretty lightweight sandbox VM for all the computer use stuff and you can load whatever apps you want on the sandbox VM. It can also be called as an MCP which opens up some cool possibilities. &lt;/p&gt; &lt;p&gt;You can do some other cool stuff as well like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAG in docs into prompt for use with tasks&lt;/li&gt; &lt;li&gt;Take over a session in progress to show the AI how to do something and then give it back control&lt;/li&gt; &lt;li&gt;Watch all the steps the AI took to attempt a task. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now for the bad stuff. It’s pretty early days in their dev lifecycle, there are some rough edges and bugs , and their Discord doesn’t seem to have a lot of action on it right now, maybe the devs are too busy cooking, but I would like to see more interaction with their user base. &lt;/p&gt; &lt;p&gt;Thankfully, there is a pretty active forking community on GitHub that is forking this project and maintaining upstream commits.&lt;/p&gt; &lt;p&gt;This post is running a bit long so I’ll stop, but let me leave a few lessons learned before I go&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Don’t even bother trying this with Ollama, I tried to get it to work with it for like 3 days with no luck. Others have reported similar issues. Use LM Studio instead, or Open Router if you need heavy duty models &lt;/li&gt; &lt;li&gt;In LM Studio make sure you’re in dev mode running the local server and MAKE SURE to have default context set to 8192 or higher.&lt;/li&gt; &lt;li&gt;if you’re trying to use ByteBot with free Gemini or any other “big 3” free tier API, you’re probably going to have a bad experience and get bad results because you’ll hit rate limits quick and then your tasks will fail. You’ll see the rate limit errors in the Docker logs for the ByteBot agent container. &lt;/li&gt; &lt;li&gt;Surprisingly, the best smallish local model I’ve gotten to do a multiple step task has been Magistral-Small-2509. &lt;/li&gt; &lt;li&gt;Some other models I’ve heard have good CUA potential are UI-TARS 1.5, Holo1.5 (7b and 72b), the Qwen2.5-VL series, and obviously Qwen3-VL 235b if you have the resources&lt;/li&gt; &lt;li&gt;I recommend trying the ByteBot Hawkeye fork straight out of the gate because it’s tailored for OpenRouter and LM Studio and it seems to be more focused on ensuring the best click accuracy. It adds a grid search and screenshot zoom process to help with it clicking in the right spot within the sandbox VM. Here’s the ByteBot-Hawkeye Fork’s repo. You’ll still want to use most of the installation instructions from the main repo tho. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ByteBot-Hawkeye Fork’s repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/zhound420/bytebot-hawkeye"&gt;https://github.com/zhound420/bytebot-hawkeye&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All that being said, don’t expect a lot from ByteBot with low parameter local models, I think this project has got good bones though and if the community supports these devs and makes meaningful contributions and cool forks like the ByteBot Hawkeye fork, then I think this has the potential to eventually become one of the better CUA tools out there. &lt;/p&gt; &lt;p&gt;Go check it out and show these devs some love! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nugpbu/bytebot_why_no_hype_train_for_these_guys_this_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nugpbu/bytebot_why_no_hype_train_for_these_guys_this_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nugpbu/bytebot_why_no_hype_train_for_these_guys_this_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T15:44:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuin1w</id>
    <title>Which SSD are you using?</title>
    <updated>2025-09-30T16:57:48+00:00</updated>
    <author>
      <name>/u/SubstantialSock8002</name>
      <uri>https://old.reddit.com/user/SubstantialSock8002</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After using mainly Apple silicon, I began using larger MoE models on my 5090 + 64GB RAM PC. Loading models like Qwen3 235B are painfully slow, over 4 minutes. It seems like my SSD is the bottleneck, as I tested read speeds are ~500MB/s. I have a Crucial P3 Plus, which supposed to get 4800MB/s, which I know is not realistic in everyday use, but 10% of that seems unreasonable.&lt;/p&gt; &lt;p&gt;Should I upgrade to a higher quality PCIe 4 SSD like the Samsung 990 PRO? Or go for a PCIe 5? &lt;/p&gt; &lt;p&gt;I'd love to get close to the speeds of my M1 Max MacBook Pro, which can load Qwen3 Next 80B Q4 (42GB) in under 30 seconds.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SubstantialSock8002"&gt; /u/SubstantialSock8002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuin1w/which_ssd_are_you_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuin1w/which_ssd_are_you_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuin1w/which_ssd_are_you_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T16:57:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu71rx</id>
    <title>More detail about GLM4.6</title>
    <updated>2025-09-30T07:45:17+00:00</updated>
    <author>
      <name>/u/Angel-Karlsson</name>
      <uri>https://old.reddit.com/user/Angel-Karlsson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems glm4.6 is finally out! &lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://z.ai/blog/glm-4.6"&gt;https://z.ai/blog/glm-4.6&lt;/a&gt; Hugging face (not working now but later): &lt;a href="https://huggingface.co/zai-org/GLM-4.6"&gt;https://huggingface.co/zai-org/GLM-4.6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Context window from 128k to 200k, better coding, reasoning and agentic performance... &lt;/p&gt; &lt;p&gt;That's quite a nice upgrade!&lt;/p&gt; &lt;p&gt;&amp;quot;The Z.ai API platform offers both GLM-4.6 and GLM-4.6-Air models&amp;quot;&lt;/p&gt; &lt;p&gt;There is an air version but not that's much information...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Angel-Karlsson"&gt; /u/Angel-Karlsson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu71rx/more_detail_about_glm46/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu71rx/more_detail_about_glm46/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu71rx/more_detail_about_glm46/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T07:45:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nujohr</id>
    <title>The issue with SWE bench</title>
    <updated>2025-09-30T17:36:48+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SWE bench and other coding benchmarks relying on real world problems have an issue. The goal is to fix the issue, when it's fixed, it's counted as a pass. But whether the solution is in line with the overall code structure, if it's implemented in a maintainable way or if it's reusing the approach the rest of the repo is using is not considered.&lt;/p&gt; &lt;p&gt;There are so many repos that get screwed by a 'working solution' that is either not efficient or introducing weird paradigms.&lt;/p&gt; &lt;p&gt;Do you see this as an issue as well? Is there a benchmark that rates the maintainability and soundness of the code beyond pure functionality?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nujohr/the_issue_with_swe_bench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nujohr/the_issue_with_swe_bench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nujohr/the_issue_with_swe_bench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T17:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu44n4</id>
    <title>1T open source reasoning model with 50B activation</title>
    <updated>2025-09-30T04:46:03+00:00</updated>
    <author>
      <name>/u/Full_Piano_3448</name>
      <uri>https://old.reddit.com/user/Full_Piano_3448</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"&gt; &lt;img alt="1T open source reasoning model with 50B activation" src="https://preview.redd.it/evmdgk53f8sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3816f8287d904b019c157e3029ce60ec1892bb6" title="1T open source reasoning model with 50B activation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ring-1T-preview: &lt;a href="https://huggingface.co/inclusionAI/Ring-1T-preview"&gt;https://huggingface.co/inclusionAI/Ring-1T-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The first 1 trillion open-source thinking model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Full_Piano_3448"&gt; /u/Full_Piano_3448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/evmdgk53f8sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T04:46:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nunlls</id>
    <title>AI max+ 395 128gb vs 5090 for beginner with ~$2k budget?</title>
    <updated>2025-09-30T20:03:09+00:00</updated>
    <author>
      <name>/u/sputnik13net</name>
      <uri>https://old.reddit.com/user/sputnik13net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m just delving into local llm and want to just play around and learn stuff. For any “real work” my company pays for all the major AI LLM platforms so I don’t need this for productivity.&lt;/p&gt; &lt;p&gt;Based on research it seemed like AI MAX+ 395 128gb would be the best “easy” option as far as being able to run anything I need without much drama.&lt;/p&gt; &lt;p&gt;But looking at the 5060ti vs 9060 comparison video on Alex Ziskind’s YouTube channel, it seems like there can be cases (comfyui) where AMD is just still too buggy.&lt;/p&gt; &lt;p&gt;So do I go for the AI MAX for big memory or 5090 for stability?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sputnik13net"&gt; /u/sputnik13net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nunlls/ai_max_395_128gb_vs_5090_for_beginner_with_2k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nunlls/ai_max_395_128gb_vs_5090_for_beginner_with_2k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nunlls/ai_max_395_128gb_vs_5090_for_beginner_with_2k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T20:03:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu6i00</id>
    <title>z.ai glm-4.6 is alive now</title>
    <updated>2025-09-30T07:09:51+00:00</updated>
    <author>
      <name>/u/cobra91310</name>
      <uri>https://old.reddit.com/user/cobra91310</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6i00/zai_glm46_is_alive_now/"&gt; &lt;img alt="z.ai glm-4.6 is alive now" src="https://a.thumbs.redditmedia.com/Z_UukH0kiXMLnIsw7KdZt46P6CBNTiF2Rn7DRNBC7E8.jpg" title="z.ai glm-4.6 is alive now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;incredible perforamnce for this outsider !&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x1503sc159sf1.png?width=3390&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b175e8d38bd2593f0c52a54ed645a9d89b240f19"&gt;https://preview.redd.it/x1503sc159sf1.png?width=3390&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b175e8d38bd2593f0c52a54ed645a9d89b240f19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;full detail on &lt;a href="https://z.ai/blog/glm-4.6"&gt;https://z.ai/blog/glm-4.6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can use it on claude code with&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;env&amp;quot;: {&lt;/p&gt; &lt;p&gt;&amp;quot;ANTHROPIC_AUTH_TOKEN&amp;quot;: &amp;quot;APIKEY&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;ANTHROPIC_BASE_URL&amp;quot;: &amp;quot;&lt;a href="https://api.z.ai/api/anthropic"&gt;https://api.z.ai/api/anthropic&lt;/a&gt;&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;API_TIMEOUT_MS&amp;quot;: &amp;quot;3000000&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;ANTHROPIC_MODEL&amp;quot;: &amp;quot;glm-4.6&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;ANTHROPIC_SMALL_FAST_MODEL&amp;quot;: &amp;quot;glm-4.5-air&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;ENABLE_THINKING&amp;quot;: &amp;quot;true&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;REASONING_EFFORT&amp;quot;: &amp;quot;ultrathink&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;MAX_THINKING_TOKENS&amp;quot;: &amp;quot;32000&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;ENABLE_STREAMING&amp;quot;: &amp;quot;true&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;MAX_OUTPUT_TOKENS&amp;quot;: &amp;quot;96000&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;MAX_MCP_OUTPUT_TOKENS&amp;quot;: &amp;quot;64000&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;AUTH_HEADER_MODE&amp;quot;: &amp;quot;x-api-key&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;promotional code &lt;a href="https://z.ai/subscribe?ic=DJA7GX6IUW"&gt;https://z.ai/subscribe?ic=DJA7GX6IUW&lt;/a&gt; for a discount !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cobra91310"&gt; /u/cobra91310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6i00/zai_glm46_is_alive_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6i00/zai_glm46_is_alive_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6i00/zai_glm46_is_alive_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T07:09:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nunq7s</id>
    <title>GPT-OSS-120B Performance on 4 x 3090</title>
    <updated>2025-09-30T20:07:59+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have been running a task for synthetic datage neration on a 4 x 3090 rig.&lt;/p&gt; &lt;p&gt;Input sequence length: 250-750 tk&lt;br /&gt; Output sequence lenght: 250 tk&lt;/p&gt; &lt;p&gt;Concurrent requests: 120&lt;/p&gt; &lt;p&gt;Avg. Prompt Throughput: 1.7k tk/s&lt;br /&gt; Avg. Generation Throughput: 1.3k tk/s&lt;/p&gt; &lt;p&gt;Power usage per GPU: Avg 280W&lt;/p&gt; &lt;p&gt;Maybe someone finds this useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nunq7s/gptoss120b_performance_on_4_x_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nunq7s/gptoss120b_performance_on_4_x_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nunq7s/gptoss120b_performance_on_4_x_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T20:07:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nturn1</id>
    <title>Full fine-tuning is not needed anymore.</title>
    <updated>2025-09-29T21:33:22+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nturn1/full_finetuning_is_not_needed_anymore/"&gt; &lt;img alt="Full fine-tuning is not needed anymore." src="https://preview.redd.it/69mpyf7476sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc01e45e5ea6eb71334a7054098e9326040ccd84" title="Full fine-tuning is not needed anymore." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new Thinking Machines blog led by John Schulman (OpenAI co-founder) shows how LoRA in reinforcement learning (RL) can match full-finetuning performance when done right! And all while using 2/3 of the resources of FFT. Blog: &lt;a href="https://thinkingmachines.ai/blog/lora/"&gt;https://thinkingmachines.ai/blog/lora/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is super important as previously, there was a misconception that you must have tonnes (8+) of GPUs to achieve a great thinking model with FFT, but now, with just LoRA, you can achieve the same results on just a single GPU!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dnj5h595d6sf1.png?width=1718&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d529f363c48f1fd837a40277375fcf67f041d03"&gt;https://preview.redd.it/dnj5h595d6sf1.png?width=1718&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d529f363c48f1fd837a40277375fcf67f041d03&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The belief that “LoRA is worse” was a misconception, it simply hadn’t been applied properly. This result reinforces that parameter-efficient fine-tuning is highly effective for most post-training use cases.&lt;/li&gt; &lt;li&gt;Apply LoRA across &lt;strong&gt;every layer&lt;/strong&gt;, not only attention - this includes MLP/MoE blocks.&lt;/li&gt; &lt;li&gt;Train with a learning rate about &lt;strong&gt;10× higher&lt;/strong&gt; than what’s used for full fine-tuning.&lt;/li&gt; &lt;li&gt;LoRA requires only about &lt;strong&gt;two-thirds of the compute&lt;/strong&gt; compared to full fine-tuning.&lt;/li&gt; &lt;li&gt;Even at &lt;strong&gt;rank = 1&lt;/strong&gt;, it performs very well for RL.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This goes to show that you that anyone can train a fantastic RL model with algorithms like GRPO, GSPO etc. for free, even on - all you need to do is have the right &lt;a href="https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide"&gt;hyper-parameters&lt;/a&gt; and strategy!&lt;/p&gt; &lt;p&gt;Ofc FFT still has many use-cases however, but this goes to show that it doesn't need to be forced literally everywhere and in every training run. P.S. some people might've been misinterpreting my title, I'm not saying FFT is dead or useless now, 'not needed anymore' means it's not a 'must' or a 'requirement' anymore!&lt;/p&gt; &lt;p&gt;So hopefully this will make RL so much more accessible to everyone, especially in the long run!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/69mpyf7476sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nturn1/full_finetuning_is_not_needed_anymore/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nturn1/full_finetuning_is_not_needed_anymore/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T21:33:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nul4ti</id>
    <title>Running Qwen3-VL-235B (Thinking &amp; Instruct) AWQ on vLLM</title>
    <updated>2025-09-30T18:30:04+00:00</updated>
    <author>
      <name>/u/Jian-L</name>
      <uri>https://old.reddit.com/user/Jian-L</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since it looks like we won’t be getting &lt;strong&gt;llama.cpp&lt;/strong&gt; support for these two massive Qwen3-VL models anytime soon, I decided to try out &lt;strong&gt;AWQ quantization with vLLM&lt;/strong&gt;. To my surprise, both models run quite well:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ"&gt;QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/QuantTrio/Qwen3-VL-235B-A22B-Thinking-AWQ"&gt;QuantTrio/Qwen3-VL-235B-A22B-Thinking-AWQ&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My Rig:&lt;br /&gt; 8× RTX 3090 (24GB), AMD EPYC 7282, 512GB RAM, Ubuntu 24.04 Headless. But I applied undervolt based on &lt;a href="/u/VoidAlchemy"&gt;u/VoidAlchemy&lt;/a&gt;'s post &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1njlnad/lact_indirect_undervolt_oc_method_beats_nvidiasmi/"&gt;LACT &amp;quot;indirect undervolt &amp;amp; OC&amp;quot; method beats nvidia-smi -pl 400 on 3090TI FE.&lt;/a&gt; and limit the power to 200w.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve &amp;quot;QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ&amp;quot; \ --served-model-name &amp;quot;Qwen3-VL-235B-A22B-Instruct-AWQ&amp;quot; \ --enable-expert-parallel \ --swap-space 16 \ --max-num-seqs 1 \ --max-model-len 32768 \ --gpu-memory-utilization 0.95 \ --tensor-parallel-size 8 \ --trust-remote-code \ --disable-log-requests \ --host &amp;quot;$HOST&amp;quot; \ --port &amp;quot;$PORT&amp;quot; vllm serve &amp;quot;QuantTrio/Qwen3-VL-235B-A22B-Thinking-AWQ&amp;quot; \ --served-model-name &amp;quot;Qwen3-VL-235B-A22B-Thinking-AWQ&amp;quot; \ --enable-expert-parallel \ --swap-space 16 \ --max-num-seqs 1 \ --max-model-len 32768 \ --gpu-memory-utilization 0.95 \ --tensor-parallel-size 8 \ --trust-remote-code \ --disable-log-requests \ --reasoning-parser deepseek_r1 \ --host &amp;quot;$HOST&amp;quot; \ --port &amp;quot;$PORT&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Result:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt throughput: 78.5 t/s&lt;/li&gt; &lt;li&gt;Generation throughput: 46 t/s ~ 47 t/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prefix cache hit rate:&lt;/strong&gt; 0% (as expected for single runs)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope it helps.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jian-L"&gt; /u/Jian-L &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nul4ti/running_qwen3vl235b_thinking_instruct_awq_on_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nul4ti/running_qwen3vl235b_thinking_instruct_awq_on_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nul4ti/running_qwen3vl235b_thinking_instruct_awq_on_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T18:30:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuhgxw</id>
    <title>Qwen3-VL Instruct vs Thinking</title>
    <updated>2025-09-30T16:13:33+00:00</updated>
    <author>
      <name>/u/rem_dreamer</name>
      <uri>https://old.reddit.com/user/rem_dreamer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuhgxw/qwen3vl_instruct_vs_thinking/"&gt; &lt;img alt="Qwen3-VL Instruct vs Thinking" src="https://preview.redd.it/ravrt8evtbsf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edd5cec5d9dd0c33d05e36054c0faf05994231d7" title="Qwen3-VL Instruct vs Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working in Vision-Language Models and notice that VLMs do not necessarily benefit from thinking as it applies for text-only LLMs. I created the following Table asking to ChatGPT (combining benchmark results found &lt;a href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list"&gt;here&lt;/a&gt;), comparing the Instruct and Thinking versions of Qwen3-VL. You will be surprised by the results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rem_dreamer"&gt; /u/rem_dreamer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ravrt8evtbsf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuhgxw/qwen3vl_instruct_vs_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuhgxw/qwen3vl_instruct_vs_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T16:13:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nupsgp</id>
    <title>No GLM 4.6-Air</title>
    <updated>2025-09-30T21:27:00+00:00</updated>
    <author>
      <name>/u/festr2</name>
      <uri>https://old.reddit.com/user/festr2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nupsgp/no_glm_46air/"&gt; &lt;img alt="No GLM 4.6-Air" src="https://b.thumbs.redditmedia.com/yn-g4_XuI0bLOv3QEgjeBe59MVVq9pvBYS_OKDOAN7w.jpg" title="No GLM 4.6-Air" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lb3lyo9tddsf1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43c558f92d47949834e74e2f776f8a4a2b744811"&gt;https://preview.redd.it/lb3lyo9tddsf1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43c558f92d47949834e74e2f776f8a4a2b744811&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/Zai_org/status/1973134943158141421"&gt;https://x.com/Zai_org/status/1973134943158141421&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/festr2"&gt; /u/festr2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nupsgp/no_glm_46air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nupsgp/no_glm_46air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nupsgp/no_glm_46air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T21:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuirbe</id>
    <title>Drummer's Snowpiercer 15B v3 · Allegedly peak creativity and roleplay for 15B and below!</title>
    <updated>2025-09-30T17:02:11+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuirbe/drummers_snowpiercer_15b_v3_allegedly_peak/"&gt; &lt;img alt="Drummer's Snowpiercer 15B v3 · Allegedly peak creativity and roleplay for 15B and below!" src="https://external-preview.redd.it/h8XGnB2ec657hZN1DoBmJMbYl4paWPSF-DePVEM6Bcg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d69d39197025755abde0051dacdfbb726dfaf859" title="Drummer's Snowpiercer 15B v3 · Allegedly peak creativity and roleplay for 15B and below!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Snowpiercer-15B-v3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuirbe/drummers_snowpiercer_15b_v3_allegedly_peak/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuirbe/drummers_snowpiercer_15b_v3_allegedly_peak/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T17:02:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu6dmo</id>
    <title>GLM-4.6 beats Claude Sonnet 4.5???</title>
    <updated>2025-09-30T07:02:12+00:00</updated>
    <author>
      <name>/u/ramphyx</name>
      <uri>https://old.reddit.com/user/ramphyx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6dmo/glm46_beats_claude_sonnet_45/"&gt; &lt;img alt="GLM-4.6 beats Claude Sonnet 4.5???" src="https://preview.redd.it/qm4pw6oh39sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7504dacba279de1b6f2a5e8909c2c5ba1a28ecd" title="GLM-4.6 beats Claude Sonnet 4.5???" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://docs.z.ai/guides/llm/glm-4.6"&gt;https://docs.z.ai/guides/llm/glm-4.6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ramphyx"&gt; /u/ramphyx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qm4pw6oh39sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6dmo/glm46_beats_claude_sonnet_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6dmo/glm46_beats_claude_sonnet_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T07:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu75l3</id>
    <title>Glm 4.6 is out and it's going against claude 4.5</title>
    <updated>2025-09-30T07:52:13+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"&gt; &lt;img alt="Glm 4.6 is out and it's going against claude 4.5" src="https://preview.redd.it/xdaov3whc9sf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51b49323776db858f9523557bb7f0b2fbdcab9f2" title="Glm 4.6 is out and it's going against claude 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xdaov3whc9sf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T07:52:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1numsuq</id>
    <title>DeepSeek-R1 performance with 15B parameters</title>
    <updated>2025-09-30T19:33:12+00:00</updated>
    <author>
      <name>/u/lewtun</name>
      <uri>https://old.reddit.com/user/lewtun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ServiceNow just released a new 15B reasoning model on the Hub which is pretty interesting for a few reasons:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Similar perf as DeepSeek-R1 and Gemini Flash, but fits on a single GPU&lt;/li&gt; &lt;li&gt;No RL was used to train the model, just high-quality mid-training&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;They also made a demo so you can vibe check it: &lt;a href="https://huggingface.co/spaces/ServiceNow-AI/Apriel-Chat"&gt;https://huggingface.co/spaces/ServiceNow-AI/Apriel-Chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm pretty curious to see what the community thinks about it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lewtun"&gt; /u/lewtun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1numsuq/deepseekr1_performance_with_15b_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1numsuq/deepseekr1_performance_with_15b_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1numsuq/deepseekr1_performance_with_15b_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T19:33:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuebie</id>
    <title>Some mad lads at Aperture Science got a quantized AGI running on a potato BTW.</title>
    <updated>2025-09-30T14:13:29+00:00</updated>
    <author>
      <name>/u/Technical-Drag-255</name>
      <uri>https://old.reddit.com/user/Technical-Drag-255</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuebie/some_mad_lads_at_aperture_science_got_a_quantized/"&gt; &lt;img alt="Some mad lads at Aperture Science got a quantized AGI running on a potato BTW." src="https://preview.redd.it/g2g5h9qi8bsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9966f2251d399b479c9d0ad3e45206689b4b5980" title="Some mad lads at Aperture Science got a quantized AGI running on a potato BTW." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Drag-255"&gt; /u/Technical-Drag-255 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g2g5h9qi8bsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuebie/some_mad_lads_at_aperture_science_got_a_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuebie/some_mad_lads_at_aperture_science_got_a_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T14:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuq54g</id>
    <title>No GLM-4.6 Air version is coming out</title>
    <updated>2025-09-30T21:40:52+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq54g/no_glm46_air_version_is_coming_out/"&gt; &lt;img alt="No GLM-4.6 Air version is coming out" src="https://preview.redd.it/mfj4sracgdsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99a9ab99d0f5ab635fe346eff30880f517f02f02" title="No GLM-4.6 Air version is coming out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zhipu-AI just shared on X that there are currently no plans to release an Air version of their newly announced GLM-4.6.&lt;/p&gt; &lt;p&gt;That said, I’m still incredibly excited about what this lab is doing. In my opinion, Zhipu-AI is one of the most promising open-weight AI labs out there right now. I’ve run my own private benchmarks across all major open-weight model releases, and GLM-4.5 stood out significantly, especially for coding and agentic workloads. It’s the closest I’ve seen an open-weight model come to the performance of the closed-weight frontier models.&lt;/p&gt; &lt;p&gt;I’ve also been keeping up with their technical reports, and they’ve been impressively transparent about their training methods. Notably, they even open-sourced their RL post-training framework, Slime, which is a huge win for the community.&lt;/p&gt; &lt;p&gt;I don’t have any insider knowledge, but based on what I’ve seen so far, I’m hopeful they’ll continue approaching/pushing the open-weight frontier and supporting the local LLM ecosystem.&lt;/p&gt; &lt;p&gt;This is an appreciation post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mfj4sracgdsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq54g/no_glm46_air_version_is_coming_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuq54g/no_glm46_air_version_is_coming_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T21:40:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nu6kjc</id>
    <title>Hot take: ALL Coding tools are bullsh*t</title>
    <updated>2025-09-30T07:14:23+00:00</updated>
    <author>
      <name>/u/Adventurous-Slide776</name>
      <uri>https://old.reddit.com/user/Adventurous-Slide776</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me tell you about the dumbest fucking trend in software development: taking the most powerful reasoning engines humanity has ever created and lobotomizing them with middleware.&lt;/p&gt; &lt;p&gt;We have these incredible language models—DeepSeek 3.2, GLM-4.5, Qwen 3 Coder—that can understand complex problems, reason through edge cases, and generate genuinely good code. And what did we do? We wrapped them in so many layers of bullshit that they can barely function.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Scam:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Every coding tool follows the same playbook:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Inject a 20,000 token system prompt explaining how to use tools&lt;/li&gt; &lt;li&gt;Add tool-calling ceremonies for every filesystem operation&lt;/li&gt; &lt;li&gt;Send timezone, task lists, environment info with EVERY request&lt;/li&gt; &lt;li&gt;Read the same files over and over and over&lt;/li&gt; &lt;li&gt;Make tiny edits one at a time&lt;/li&gt; &lt;li&gt;Re-read everything to &amp;quot;verify&amp;quot;&lt;/li&gt; &lt;li&gt;Repeat until you've burned 50,000 tokens&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And then they market this as &amp;quot;agentic&amp;quot; and &amp;quot;autonomous&amp;quot; and charge you $20/month.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Reality:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The model spends 70% of its context window reading procedural garbage it's already seen five times. It's not thinking about your problem—it's playing filesystem navigator. It's not reasoning deeply—it's pattern matching through the noise because it's cognitively exhausted.&lt;/p&gt; &lt;p&gt;You ask it to fix a bug. It reads the file (3k tokens). Checks the timezone (why?). Reviews the task list (who asked?). Makes a one-line change. Reads the file AGAIN to verify. Runs a command. Reads the output. And somehow the bug still isn't fixed because the model never had enough clean context to actually understand the problem.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Insanity:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What you can accomplish in 15,000 tokens with a direct conversation—problem explained, context provided, complete solution generated—these tools spread across 50,000 tokens of redundant slop.&lt;/p&gt; &lt;p&gt;The model generates the same code snippets again and again. It sees the same file contents five times in one conversation. It's drowning in its own output, suffocating under layers of middleware-generated vomit.&lt;/p&gt; &lt;p&gt;And the worst part? &lt;strong&gt;It gives worse results.&lt;/strong&gt; The solutions are half-assed because the model is working with a fraction of its actual reasoning capacity. Everything else is burned on ceremonial bullshit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Market Dynamics:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;VCs threw millions at &amp;quot;AI coding agents.&amp;quot; Companies rushed to ship agentic frameworks. Everyone wanted to be the &amp;quot;autonomous&amp;quot; solution. So they added more tools, more features, more automation.&lt;/p&gt; &lt;p&gt;More context r*pe.&lt;/p&gt; &lt;p&gt;They optimized for demos, not for actual utility. Because in a demo, watching the tool &amp;quot;autonomously&amp;quot; read files and run commands looks impressive. In reality, you're paying 3x the API costs for 0.5x the quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Simple Truth:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Just upload your fucking files to a local chat interface like LobeHub (Open Source). Explain the problem. Let the model think. Get your code in one artifact. Copy it. Done.&lt;/p&gt; &lt;p&gt;No tool ceremonies. No context pollution. No reading the same file seven times. No timezone updates nobody asked for.&lt;/p&gt; &lt;p&gt;The model's full intelligence goes toward your problem, not toward navigating a filesystem through an API. You get better code, faster, for less money.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Irony:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We spent decades making programming languages more expressive so humans could think at a higher level. Then we built AI that can understand natural language and reason about complex systems.&lt;/p&gt; &lt;p&gt;And then we forced it back down into the machine-level bullsh*t of &amp;quot;read file, edit line 47, write file, run command, read output.&amp;quot;&lt;/p&gt; &lt;p&gt;We took reasoning engines and turned them into glorified bash scripts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Future:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I hope we look back at this era and laugh. The &amp;quot;agentic coding tool&amp;quot; phase where everyone was convinced that more automation meant better results. Where we drowned AI in context pollution and called it progress.&lt;/p&gt; &lt;p&gt;The tools that will win aren't the ones with the most features or the most autonomy. They're the ones that get out of the model's way and let it do what it's actually good at: thinking.&lt;/p&gt; &lt;p&gt;Until then, I'll be over here using the chat interface like a sane person, getting better results for less money, while the rest of you pay for the privilege of context r*pe.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Slide776"&gt; /u/Adventurous-Slide776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6kjc/hot_take_all_coding_tools_are_bullsht/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6kjc/hot_take_all_coding_tools_are_bullsht/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nu6kjc/hot_take_all_coding_tools_are_bullsht/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T07:14:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nujx4x</id>
    <title>GLM 4.6 already runs on MLX</title>
    <updated>2025-09-30T17:45:32+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nujx4x/glm_46_already_runs_on_mlx/"&gt; &lt;img alt="GLM 4.6 already runs on MLX" src="https://preview.redd.it/jcb16mqcacsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0df9237936852365f57d5b9dcba46dd846a877fe" title="GLM 4.6 already runs on MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jcb16mqcacsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nujx4x/glm_46_already_runs_on_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nujx4x/glm_46_already_runs_on_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T17:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nulea4</id>
    <title>How can I use this beast to benefit the community? Quantize larger models? It’s a 9985wx, 768 ddr5, 384 gb vram.</title>
    <updated>2025-09-30T18:39:46+00:00</updated>
    <author>
      <name>/u/joninco</name>
      <uri>https://old.reddit.com/user/joninco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nulea4/how_can_i_use_this_beast_to_benefit_the_community/"&gt; &lt;img alt="How can I use this beast to benefit the community? Quantize larger models? It’s a 9985wx, 768 ddr5, 384 gb vram." src="https://preview.redd.it/78yadl81kcsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=977db53773f1bf118531422a3b8b0a76e5905352" title="How can I use this beast to benefit the community? Quantize larger models? It’s a 9985wx, 768 ddr5, 384 gb vram." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any ideas are greatly appreciated to use this beast for good!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joninco"&gt; /u/joninco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/78yadl81kcsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nulea4/how_can_i_use_this_beast_to_benefit_the_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nulea4/how_can_i_use_this_beast_to_benefit_the_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T18:39:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuhe8m</id>
    <title>GLM 4.6 one-shot aquarium simulator with the best looking fishes I've ever seen created by open weight models.</title>
    <updated>2025-09-30T16:10:39+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuhe8m/glm_46_oneshot_aquarium_simulator_with_the_best/"&gt; &lt;img alt="GLM 4.6 one-shot aquarium simulator with the best looking fishes I've ever seen created by open weight models." src="https://preview.redd.it/i68ebwe6rbsf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59d3e23dec72571f34d1e0bef84e936438849af3" title="GLM 4.6 one-shot aquarium simulator with the best looking fishes I've ever seen created by open weight models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fish tails actually wave around while they swim. I admit the rest of the scene is not extremely detailed, but overall this is better that what you get from for example DeepSeek models which are nearly twice as big. Qwen models are usually fairly good at this too, except the buttons all work here which is kinda something note worthy given my previous experience with other models which generate beautiful (and very often ridiculously useless) buttons which don't even work. Here everything works out of the box. No bugs or errors. I said it with GLM 4.5 and I can only say it again with GLM 4.6. GLM is the real deal alternative to closed source proprietary models, guys.&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://jsfiddle.net/n52smvkr/"&gt;Jsfiddle&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i68ebwe6rbsf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuhe8m/glm_46_oneshot_aquarium_simulator_with_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuhe8m/glm_46_oneshot_aquarium_simulator_with_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T16:10:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nufu17</id>
    <title>AMD tested 20+ local models for coding &amp; only 2 actually work (testing linked)</title>
    <updated>2025-09-30T15:11:54+00:00</updated>
    <author>
      <name>/u/nick-baumann</name>
      <uri>https://old.reddit.com/user/nick-baumann</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nufu17/amd_tested_20_local_models_for_coding_only_2/"&gt; &lt;img alt="AMD tested 20+ local models for coding &amp;amp; only 2 actually work (testing linked)" src="https://external-preview.redd.it/eXRyOW5lM2JpYnNmMRoy4uRFePoUQZKQzw3MqAlRHs-miZIp3JL6ldgQ6nGR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=121a931aab3efc98ee57ab2e3519bf172950e05f" title="AMD tested 20+ local models for coding &amp;amp; only 2 actually work (testing linked)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;tldr&lt;/strong&gt;; qwen3-coder (4-bit, 8-bit) is really the only viable local model for coding, if you have 128gb+ of RAM, check out GLM-4.5-air (8-bit)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;hello hello!&lt;/p&gt; &lt;p&gt;So AMD just dropped their comprehensive testing of local models for AI coding and it pretty much validates what I've been preaching about local models&lt;/p&gt; &lt;p&gt;They tested 20+ models and found exactly what many of us suspected: most of them completely fail at actual coding tasks. Out of everything they tested, only three models consistently worked: Qwen3-Coder 30B, GLM-4.5-Air for those with beefy rigs. Magistral Small is worth an honorable mention in my books.&lt;/p&gt; &lt;p&gt;deepseek/deepseek-r1-0528-qwen3-8b, smaller Llama models, GPT-OSS-20B, Seed-OSS-36B (bytedance) all produce broken outputs or can't handle tool use properly. This isn't a knock on the models themselves, they're just not built for the complex tool-calling that coding agents need.&lt;/p&gt; &lt;p&gt;What's interesting is their RAM findings match exactly what I've been seeing. For 32gb machines, Qwen3-Coder 30B at 4-bit is basically your only option, but an extremely viable one at that.&lt;/p&gt; &lt;p&gt;For those with 64gb RAM, you can run the same model at 8-bit quantization. And if you've got 128gb+, GLM-4.5-Air is apparently incredible (this is AMD's #1)&lt;/p&gt; &lt;p&gt;AMD used Cline &amp;amp; LM Studio for all their testing, which is how they validated these specific configurations. Cline is pretty demanding in terms of tool-calling and context management, so if a model works with Cline, it'll work with pretty much anything.&lt;/p&gt; &lt;p&gt;AMD's blog: &lt;a href="https://www.amd.com/en/blogs/2025/how-to-vibe-coding-locally-with-amd-ryzen-ai-and-radeon.html"&gt;https://www.amd.com/en/blogs/2025/how-to-vibe-coding-locally-with-amd-ryzen-ai-and-radeon.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;setup instructions for coding w/ local models: &lt;a href="https://cline.bot/blog/local-models-amd"&gt;https://cline.bot/blog/local-models-amd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nick-baumann"&gt; /u/nick-baumann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fmg3qe3bibsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nufu17/amd_tested_20_local_models_for_coding_only_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nufu17/amd_tested_20_local_models_for_coding_only_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T15:11:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nuerql</id>
    <title>zai-org/GLM-4.6 · Hugging Face</title>
    <updated>2025-09-30T14:31:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuerql/zaiorgglm46_hugging_face/"&gt; &lt;img alt="zai-org/GLM-4.6 · Hugging Face" src="https://external-preview.redd.it/PGKpaG-61JC7z-y_F2XkhwKzdcpyb99tvV79_JhB320.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b34f4363d1d490762c5a458490a60b87ed1e125" title="zai-org/GLM-4.6 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Model Introduction&lt;/h1&gt; &lt;p&gt;Compared with GLM-4.5, &lt;strong&gt;GLM-4.6&lt;/strong&gt; brings several key improvements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Longer context window:&lt;/strong&gt; The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Superior coding performance:&lt;/strong&gt; The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced reasoning:&lt;/strong&gt; GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More capable agents:&lt;/strong&gt; GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Refined writing:&lt;/strong&gt; Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as &lt;strong&gt;DeepSeek-V3.1-Terminus&lt;/strong&gt; and &lt;strong&gt;Claude Sonnet 4&lt;/strong&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nuerql/zaiorgglm46_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nuerql/zaiorgglm46_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-30T14:31:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
