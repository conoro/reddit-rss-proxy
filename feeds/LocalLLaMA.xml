<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-17T11:06:05+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nipox6</id>
    <title>Fine-tuning Small Language models/ qwen2.5 0.5 B</title>
    <updated>2025-09-16T18:33:01+00:00</updated>
    <author>
      <name>/u/Mysterious_Ad_3788</name>
      <uri>https://old.reddit.com/user/Mysterious_Ad_3788</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nipox6/finetuning_small_language_models_qwen25_05_b/"&gt; &lt;img alt="Fine-tuning Small Language models/ qwen2.5 0.5 B" src="https://preview.redd.it/hoplx2colkpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35a01423a78f194d98f7f162fddd9b55eec0fee6" title="Fine-tuning Small Language models/ qwen2.5 0.5 B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been up all week trying to fine-tune a small language model using Unsloth, and I've experimented with RAG. I generated around 1,500 domain-specific questions, but my LLM is still hallucinating. Below is a summary of my training setup and data distribution:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Epochs&lt;/strong&gt;: 20 (training stops around epoch 11)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch size&lt;/strong&gt;: 8&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Learning rate&lt;/strong&gt;: 1e-4&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Warmup ratio&lt;/strong&gt;: 0.5&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Max sequence length&lt;/strong&gt;: 4096&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA rank&lt;/strong&gt;: 32&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA alpha&lt;/strong&gt;: 16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: Includes both positive and negative QA-style examples&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Despite this setup, hallucinations persist the model dont even know what it was finetuned on. Can anyone help me understand what I might be doing wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Ad_3788"&gt; /u/Mysterious_Ad_3788 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hoplx2colkpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nipox6/finetuning_small_language_models_qwen25_05_b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nipox6/finetuning_small_language_models_qwen25_05_b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T18:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj709e</id>
    <title>Any new SOTA music generation models since ACE-step?</title>
    <updated>2025-09-17T07:55:04+00:00</updated>
    <author>
      <name>/u/utofy</name>
      <uri>https://old.reddit.com/user/utofy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;anyone got the links/repos? And not just papers pls because lots of times they never end up publishing the models.&lt;/p&gt; &lt;p&gt;p.s. in response to this post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kg9jkq/new_sota_music_generation_model/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kg9jkq/new_sota_music_generation_model/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/utofy"&gt; /u/utofy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj709e/any_new_sota_music_generation_models_since_acestep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj709e/any_new_sota_music_generation_models_since_acestep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj709e/any_new_sota_music_generation_models_since_acestep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T07:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj9khh</id>
    <title>How to post-train LLM with tokenizer replacement?</title>
    <updated>2025-09-17T10:37:41+00:00</updated>
    <author>
      <name>/u/Objective-Good310</name>
      <uri>https://old.reddit.com/user/Objective-Good310</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried searching Google for guides but couldn't find any. I have an idea to teach LLM a new language, but there is a problem. After I retrained the basic tokenizer of the model, first, the IDs of some system tokens changed, and second, after retraining the model itself with the new tokenizer, it generates garbage. Please advise on how to retrain correctly with the tokenizer replacement. Maybe I'm not retraining the tokenizer correctly? Maybe it needs to be expanded? And is it possible to retrain the model using the tokenizer of another model? I like the organization of the chat template and tokenizer in gpt oss, and I would like to train on it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Objective-Good310"&gt; /u/Objective-Good310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9khh/how_to_posttrain_llm_with_tokenizer_replacement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9khh/how_to_posttrain_llm_with_tokenizer_replacement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9khh/how_to_posttrain_llm_with_tokenizer_replacement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T10:37:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj5n67</id>
    <title>Is anyone able to successfully run Qwen 30B Coder BF16?</title>
    <updated>2025-09-17T06:27:47+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With Llama.cpp and the Unsloth GGUFs for Qwen 3 30B Coder BF16, I am getting frequent crashes on two entirely different systems, a Ryzen AI Max, and a another sustem with an RTX 6000 Blackwell.&lt;/p&gt; &lt;p&gt;Llama.cpp just exits with no error message after a few messages. &lt;/p&gt; &lt;p&gt;VLLM works perfectly on the Blackwell with the official model from Qwen, except tool calling is currently broken, even with the new qwen 3 tool call parser which VLLM added. So the tool call instructions just end up in the chat stream, which makes the model unusable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5n67/is_anyone_able_to_successfully_run_qwen_30b_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5n67/is_anyone_able_to_successfully_run_qwen_30b_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5n67/is_anyone_able_to_successfully_run_qwen_30b_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T06:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj758c</id>
    <title>Can I use Cursor Agent (or similar) with a local LLM setup (8B / 13B)?</title>
    <updated>2025-09-17T08:04:26+00:00</updated>
    <author>
      <name>/u/BudgetPurple3002</name>
      <uri>https://old.reddit.com/user/BudgetPurple3002</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I want to set up a local LLM (running 8B and possibly 13B parameter models). I was wondering if tools like Cursor Agent (or other AI coding agents) can work directly with my local setup, or if they require cloud-based APIs only.&lt;/p&gt; &lt;p&gt;Basically:&lt;/p&gt; &lt;p&gt;Is it possible to connect Cursor (or any similar coding agent) to a local model?&lt;/p&gt; &lt;p&gt;If not Cursor specifically, are there any good agent frameworks that can plug into local models for tasks like code generation and project automation?&lt;/p&gt; &lt;p&gt;Would appreciate any guidance from folks who‚Äôve tried this. üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BudgetPurple3002"&gt; /u/BudgetPurple3002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj758c/can_i_use_cursor_agent_or_similar_with_a_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj758c/can_i_use_cursor_agent_or_similar_with_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj758c/can_i_use_cursor_agent_or_similar_with_a_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T08:04:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj78fl</id>
    <title>Best sub 14b llm for long text summaries?</title>
    <updated>2025-09-17T08:10:41+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Speed is not important (can run overnight if really need be) but accuracy really matters to me. I was wondering if there were good 1M or 512K or even 256k context models That I might not be aware of. &lt;/p&gt; &lt;p&gt;I know qwen3 4b instruct has 256k native but im afraid it might not be accurate enough and hallucinate quite a bit due to its size &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj78fl/best_sub_14b_llm_for_long_text_summaries/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj78fl/best_sub_14b_llm_for_long_text_summaries/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj78fl/best_sub_14b_llm_for_long_text_summaries/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T08:10:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nijikb</id>
    <title>Inference will win ultimately</title>
    <updated>2025-09-16T14:46:07+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nijikb/inference_will_win_ultimately/"&gt; &lt;img alt="Inference will win ultimately" src="https://preview.redd.it/jp7ada3lhjpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2651ab9359b4d75a0e7c1c55003fec8ea92f4fdb" title="Inference will win ultimately" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;inference is where the real value shows up. it‚Äôs where models are actually used at scale.&lt;/p&gt; &lt;p&gt;A few reasons why I think this is where the winners will be: ‚Ä¢Hardware is shifting. Morgan Stanley recently noted that more chips will be dedicated to inference than training in the years ahead. The market is already preparing for this transition. ‚Ä¢Open-source is exploding. Meta‚Äôs Llama models alone have crossed over a billion downloads. That‚Äôs a massive long tail of developers and companies who need efficient ways to serve all kinds of models. ‚Ä¢Agents mean real usage. Training is abstract , inference is what everyday people experience when they use agents, apps, and platforms. That‚Äôs where latency, cost, and availability matter. ‚Ä¢Inefficiency is the opportunity. Right now GPUs are underutilized, cold starts are painful, and costs are high. Whoever cracks this at scale , making inference efficient, reliable, and accessible , will capture enormous value.&lt;/p&gt; &lt;p&gt;In short, inference isn‚Äôt just a technical detail. It‚Äôs where AI meets reality. And that‚Äôs why inference will win.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jp7ada3lhjpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nijikb/inference_will_win_ultimately/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nijikb/inference_will_win_ultimately/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T14:46:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nipldx</id>
    <title>Ktransformers now supports qwen3-next</title>
    <updated>2025-09-16T18:29:30+00:00</updated>
    <author>
      <name>/u/Betadoggo_</name>
      <uri>https://old.reddit.com/user/Betadoggo_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nipldx/ktransformers_now_supports_qwen3next/"&gt; &lt;img alt="Ktransformers now supports qwen3-next" src="https://external-preview.redd.it/GCXHZq6UgvHr-07Ef7MzKApM7hyb5aZQRF1Wd5lmCZ0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45539a92e2924b07b2035937feb0f51a09d5cc5e" title="Ktransformers now supports qwen3-next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was a few days ago but I haven't seen it mentioned here so I figured I'd post it. They claim 6GB of vram usage with 320GB of system memory. Hopefully in the future the system memory requirements can be brought down if they support quantized variants.&lt;/p&gt; &lt;p&gt;I think this could be the ideal way to run it on low vram systems in the short term before llamacpp gets support.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Betadoggo_"&gt; /u/Betadoggo_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Qwen3-Next.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nipldx/ktransformers_now_supports_qwen3next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nipldx/ktransformers_now_supports_qwen3next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T18:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj9e7v</id>
    <title>Opencode plugin for extending local LLM knowledge using Google AI Search - free, unlimited, incognito via Playwright automation</title>
    <updated>2025-09-17T10:27:34+00:00</updated>
    <author>
      <name>/u/igorwarzocha</name>
      <uri>https://old.reddit.com/user/igorwarzocha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So... I was trying to figure out how to integrate Google AI Search as a native tool/plugin and I vibecoded this thing. &lt;a href="https://github.com/IgorWarzocha/Opencode-Google-AI-Search-Plugin"&gt;https://github.com/IgorWarzocha/Opencode-Google-AI-Search-Plugin&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Why? Because local LLMs have a training cutoff date and their knowledge can be limited. This way you can spoonfeed your LLM some extra, up to date info. Yes, you are at risk of feeding the LLM some hallucinations or incorrect replies, but if you ask a reasonably detailed question, you will get a reasonably detailed result, and with links to sources so you can then fetch them for more info.&lt;/p&gt; &lt;p&gt;It's basically a tool that runs a very specific sequence of Playwright events and feeds the output back to the LLM (stumbled upon that idea while using browser control mcps). Unfortunately couldn't get the tool call to display properly (like fetch). LLM calls the tool, ingests the output into the context, and spits out a summary. If you want the full result, you need to ask it for it (it will give you the links, proper formatting etc, so you can then fetch content).&lt;/p&gt; &lt;p&gt;It fires playwright in headless, goes through the cookies, and does the thing. And it works locally in incognito, so your searches are kinda private. &lt;/p&gt; &lt;p&gt;Enjoy it while it lasts, I'm sure Google will do something about it eventually. Let me know if it works for you... &amp;quot;it works on my machine&amp;quot; LOL&lt;/p&gt; &lt;p&gt;PS. I'm pretty damn sure it can be adapted to work with any client and any website since it's a scripted Playwright automation. Scary.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/igorwarzocha"&gt; /u/igorwarzocha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9e7v/opencode_plugin_for_extending_local_llm_knowledge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9e7v/opencode_plugin_for_extending_local_llm_knowledge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9e7v/opencode_plugin_for_extending_local_llm_knowledge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T10:27:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj48gh</id>
    <title>embeddinggemma with Qdrant compatible uint8 tensors output</title>
    <updated>2025-09-17T05:04:49+00:00</updated>
    <author>
      <name>/u/terminoid_</name>
      <uri>https://old.reddit.com/user/terminoid_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hacked on the int8-sized community ONNX model of emnbeddinggemma to get it to output uint8 tensors which are compatible with Qdrant. For some reason it benchmarks higher than the base model on most of the NanoBEIR benchmarks.&lt;/p&gt; &lt;p&gt;benchmarks and info here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/electroglyph/embeddinggemma-300m-ONNX-uint8"&gt;https://huggingface.co/electroglyph/embeddinggemma-300m-ONNX-uint8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terminoid_"&gt; /u/terminoid_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj48gh/embeddinggemma_with_qdrant_compatible_uint8/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj48gh/embeddinggemma_with_qdrant_compatible_uint8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj48gh/embeddinggemma_with_qdrant_compatible_uint8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T05:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj80eu</id>
    <title>Has anyone been able to use GLM 4.5 with the Github copilot extension in VSCode?</title>
    <updated>2025-09-17T09:02:13+00:00</updated>
    <author>
      <name>/u/Intelligent-Top3333</name>
      <uri>https://old.reddit.com/user/Intelligent-Top3333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I couldn't make it work, tried insiders too, I get this error:&lt;br /&gt; ```&lt;/p&gt; &lt;p&gt;Sorry, your request failed. Please try again. Request id: add5bf64-832a-4bd5-afd2-6ba10be9a734&lt;/p&gt; &lt;p&gt;Reason: Rate limit exceeded&lt;/p&gt; &lt;p&gt;{&amp;quot;code&amp;quot;:&amp;quot;1113&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Insufficient balance or no resource package. Please recharge.&amp;quot;}&lt;br /&gt; ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intelligent-Top3333"&gt; /u/Intelligent-Top3333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj80eu/has_anyone_been_able_to_use_glm_45_with_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj80eu/has_anyone_been_able_to_use_glm_45_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj80eu/has_anyone_been_able_to_use_glm_45_with_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T09:02:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nis417</id>
    <title>Alibaba Tongyi released open-source (Deep Research) Web Agent</title>
    <updated>2025-09-16T20:02:18+00:00</updated>
    <author>
      <name>/u/kahlil29</name>
      <uri>https://old.reddit.com/user/kahlil29</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging Face link to weights : &lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kahlil29"&gt; /u/kahlil29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Ali_TongyiLab/status/1967988004179546451?s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nis417/alibaba_tongyi_released_opensource_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nis417/alibaba_tongyi_released_opensource_deep_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T20:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj673e</id>
    <title>STT ‚Äì&gt; LLM ‚Äì&gt; TTS pipeline in C</title>
    <updated>2025-09-17T07:02:35+00:00</updated>
    <author>
      <name>/u/rhinodevil</name>
      <uri>https://old.reddit.com/user/rhinodevil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For &lt;strong&gt;S&lt;/strong&gt;peech-&lt;strong&gt;T&lt;/strong&gt;o-&lt;strong&gt;T&lt;/strong&gt;ext, &lt;strong&gt;L&lt;/strong&gt;arge-&lt;strong&gt;L&lt;/strong&gt;anguage-&lt;strong&gt;M&lt;/strong&gt;odel inference and &lt;strong&gt;T&lt;/strong&gt;ext-&lt;strong&gt;T&lt;/strong&gt;o-&lt;strong&gt;S&lt;/strong&gt;peech I created three wrapper libraries in C/C++ (using &lt;a href="https://github.com/ggml-org/whisper.cpp"&gt;Whisper.cpp&lt;/a&gt;, &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;Llama.cpp&lt;/a&gt; and &lt;a href="https://github.com/rhasspy/piper"&gt;Piper&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;They offer pure C interfaces, Windows and Linux are supported, meant to be used on standard consumer hardware.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/RhinoDevel/mt_stt"&gt;mt_stt&lt;/a&gt; for &lt;strong&gt;S&lt;/strong&gt;peech-&lt;strong&gt;T&lt;/strong&gt;o-&lt;strong&gt;T&lt;/strong&gt;ext.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/RhinoDevel/mt_llm"&gt;mt_llm&lt;/a&gt; for &lt;strong&gt;L&lt;/strong&gt;arge-&lt;strong&gt;L&lt;/strong&gt;anguage-&lt;strong&gt;M&lt;/strong&gt;odel inference.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/RhinoDevel/mt_tts"&gt;mt_tts&lt;/a&gt; for &lt;strong&gt;T&lt;/strong&gt;ext-&lt;strong&gt;T&lt;/strong&gt;o-&lt;strong&gt;S&lt;/strong&gt;peech.&lt;/p&gt; &lt;p&gt;An example implementation of an &lt;strong&gt;STT -&amp;gt; LLM -&amp;gt; TTS pipeline&lt;/strong&gt; in C can be found &lt;a href="https://github.com/RhinoDevel/mt_llm/tree/main/stt_llm_tts-pipeline-example"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rhinodevil"&gt; /u/rhinodevil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj673e/stt_llm_tts_pipeline_in_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj673e/stt_llm_tts_pipeline_in_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj673e/stt_llm_tts_pipeline_in_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T07:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nis0za</id>
    <title>Alibaba-NLP/Tongyi-DeepResearch-30B-A3B ¬∑ Hugging Face</title>
    <updated>2025-09-16T19:59:13+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nis0za/alibabanlptongyideepresearch30ba3b_hugging_face/"&gt; &lt;img alt="Alibaba-NLP/Tongyi-DeepResearch-30B-A3B ¬∑ Hugging Face" src="https://external-preview.redd.it/Br8d0DO81Y2NXG6ObCzOPqMnemqzEFVfpKOIf-1Xb3Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f864231608ef7f1e9dcabddf98002a4ed64cb7df" title="Alibaba-NLP/Tongyi-DeepResearch-30B-A3B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nis0za/alibabanlptongyideepresearch30ba3b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nis0za/alibabanlptongyideepresearch30ba3b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T19:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj5wk4</id>
    <title>OpenAI usage breakdown released</title>
    <updated>2025-09-17T06:44:15+00:00</updated>
    <author>
      <name>/u/LeatherRub7248</name>
      <uri>https://old.reddit.com/user/LeatherRub7248</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5wk4/openai_usage_breakdown_released/"&gt; &lt;img alt="OpenAI usage breakdown released" src="https://preview.redd.it/njcotg7i7opf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e00350509ba0edd32cc7dfb7341451356402cd8" title="OpenAI usage breakdown released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would have thought image generation would be higher... but this might be skewed by the fact that the 4o image (the whole ghibli craze) only came out in march 2025&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.nber.org/system/files/working_papers/w34255/w34255.pdf"&gt;https://www.nber.org/system/files/working_papers/w34255/w34255.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.nber.org/papers/w34255"&gt;https://www.nber.org/papers/w34255&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeatherRub7248"&gt; /u/LeatherRub7248 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/njcotg7i7opf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5wk4/openai_usage_breakdown_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5wk4/openai_usage_breakdown_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T06:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj8hee</id>
    <title>[Release] DASLab GGUF Non-Uniform Quantization Toolkit</title>
    <updated>2025-09-17T09:32:39+00:00</updated>
    <author>
      <name>/u/Loginhe</name>
      <uri>https://old.reddit.com/user/Loginhe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj8hee/release_daslab_gguf_nonuniform_quantization/"&gt; &lt;img alt="[Release] DASLab GGUF Non-Uniform Quantization Toolkit" src="https://a.thumbs.redditmedia.com/z81IhWllCbQrFrgfgKe5CFHfXTjYN85Bz1DiPDLtGE0.jpg" title="[Release] DASLab GGUF Non-Uniform Quantization Toolkit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're excited to release the &lt;strong&gt;first open-source toolkit&lt;/strong&gt; that brings &lt;strong&gt;GPTQ + EvoPress&lt;/strong&gt; to the &lt;strong&gt;GGUF format&lt;/strong&gt;, enabling &lt;em&gt;heterogeneous quantization&lt;/em&gt; based on importance.&lt;br /&gt; &lt;strong&gt;Delivering Higher-quality models, same file size.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;What's inside&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2210.17323"&gt;&lt;strong&gt;GPTQ (ICLR '23)&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;quantization with GGUF export:&lt;/strong&gt; delivers error-correcting calibration for improved performance&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2410.14649"&gt;&lt;strong&gt;EvoPress (ICML '25)&lt;/strong&gt;&lt;/a&gt;: runs evolutionary search to automatically discover optimal per-layer quantization configs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model assembly tools:&lt;/strong&gt; package models to be fully functional with llama.cpp&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;p&gt;Unlike standard uniform quantization, our toolkit &lt;strong&gt;optimizes precision where it matters most&lt;/strong&gt;.&lt;br /&gt; Critical layers (e.g. attention) can use higher precision, while others (e.g. FFN) compress more aggressively.&lt;br /&gt; With &lt;strong&gt;EvoPress search + GPTQ quantization&lt;/strong&gt;, these trade-offs are discovered automatically.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;Below are zero-shot evaluations. Full benchmark results are available in the repo.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3eg7rp0vyopf1.png?width=3569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6590f70e8abf59f3442df57321eaa55ea85ba9c"&gt;https://preview.redd.it/3eg7rp0vyopf1.png?width=3569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6590f70e8abf59f3442df57321eaa55ea85ba9c&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/IST-DASLab/gptq-gguf-toolkit"&gt;DASLab GGUF Quantization Toolkit (GitHub Repo Link)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are happy to get feedback, contributions, and experiments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loginhe"&gt; /u/Loginhe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj8hee/release_daslab_gguf_nonuniform_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj8hee/release_daslab_gguf_nonuniform_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj8hee/release_daslab_gguf_nonuniform_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T09:32:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj4axf</id>
    <title>Thread for CPU-only LLM performance comparison</title>
    <updated>2025-09-17T05:08:52+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I could not find any recent posts about CPU only performance comparison of different CPUs. With recent advancements in CPUs, we are seeing incredible memory bandwidth speeds with DDR5 6400 12 channel EPYC 9005 (614.4 GB/s theoretical bw). AMD also announced that Zen 6 CPUs will have 1.6TB/s memory bw. The future of CPUs looks exciting. But for now, I wanted to test what we already have. I need your help to see where we stand with CPUs currently.&lt;/p&gt; &lt;p&gt;For this CPU only comparison, I want to use ik_llama - &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt; . I compiled and tested both ik_llama and llama.cpp with MoE models like Qwen3 30B3A Q4_1, gpt-oss 120B Q8 and qwen3 235B Q4_1. ik_llama is at least 2x faster prompt processing (PP) and 50% faster in text generation (TG).&lt;/p&gt; &lt;p&gt;For this benchmark, I used Qwen3 30B3A Q4_1 (19.2GB) and ran ik_llama in Ubuntu 24.04.3.&lt;/p&gt; &lt;p&gt;ik_llama installation:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/ikawrakow/ik_llama.cpp.git cd ik_llama.cpp cmake -B build cmake --build build --config Release -j $(nproc) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;llama-bench benchmark (make sure GPUs are disabled with CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; just in case if you compiled for GPUs):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; ./build/bin/llama-bench -m /media/ai-llm/wd_2t/models/Qwen3-30B-A3B-Q4_1.gguf -mmp 0 --threads 32 | model | size | params | backend | threads | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | ------: | ---: | ------------: | ---------------: | | qwen3moe ?B Q4_1 | 17.87 GiB | 30.53 B | CPU | 32 | 0 | pp512 | 263.02 ¬± 2.53 | | qwen3moe ?B Q4_1 | 17.87 GiB | 30.53 B | CPU | 32 | 0 | tg128 | 38.98 ¬± 0.16 | build: 6d2e7ca4 (3884) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GPT-OSS 120B:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; ./build/bin/llama-bench -m /media/ai-llm/wd_2t/models/GPT_OSS_120B_UD-Q8_K_XL/gpt-oss-120b-UD-Q8_K_XL-00001-of-00002.gguf -mmp 0 --threads 32 | model | size | params | backend | threads | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | ------: | ---: | ------------: | ---------------: | | gpt-oss ?B Q8_0 | 60.03 GiB | 116.83 B | CPU | 32 | 0 | pp512 | 163.24 ¬± 4.46 | | gpt-oss ?B Q8_0 | 60.03 GiB | 116.83 B | CPU | 32 | 0 | tg128 | 24.77 ¬± 0.42 | build: 6d2e7ca4 (3884) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So, the requirement for this benchmark is simple:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Required: list your MB, CPU, RAM size, type and channels.&lt;/li&gt; &lt;li&gt;Required: use CPU only inference (No APUs, NPUs, or build-in GPUs allowed)&lt;/li&gt; &lt;li&gt;use ik-llama (any recent version) if possible since llama.cpp will be slower for your CPU performance&lt;/li&gt; &lt;li&gt;Required model: ( &lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/blob/main/Qwen3-30B-A3B-Q4_1.gguf"&gt;https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/blob/main/Qwen3-30B-A3B-Q4_1.gguf&lt;/a&gt; ) Run the standard llama-bench benchmark with Qwen3-30B-A3B-Q4_1.gguf (2703 version should also be fine as long as it is Q4_1) and share the command with output in the comments as I shared above.&lt;/li&gt; &lt;li&gt;Optional (not required but good to have): run CPU only benchmark with GPT-OSS 120B (file here: &lt;a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF/tree/main/UD-Q8%5C_K%5C_XL"&gt;https://huggingface.co/unsloth/gpt-oss-120b-GGUF/tree/main/UD-Q8\_K\_XL&lt;/a&gt;) and share the command with output in the comments.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I will start by adding my CPU performance in this table below.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Motherboard&lt;/th&gt; &lt;th align="left"&gt;CPU (physical cores)&lt;/th&gt; &lt;th align="left"&gt;RAM size and type&lt;/th&gt; &lt;th align="left"&gt;channels&lt;/th&gt; &lt;th align="left"&gt;Qwen3 30B3A Q4_1 TG&lt;/th&gt; &lt;th align="left"&gt;Qwen3 30B3A Q4_1 PP&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;AsRock ROMED8-2T&lt;/td&gt; &lt;td align="left"&gt;AMD EPYC 7532 (32 cores)&lt;/td&gt; &lt;td align="left"&gt;8x32GB DDR4 3200Mhz&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;39.98&lt;/td&gt; &lt;td align="left"&gt;263.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I will check comments daily and keep updating the table.&lt;/p&gt; &lt;p&gt;This awesome community is the best place to collect such performance metrics.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj4axf/thread_for_cpuonly_llm_performance_comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj4axf/thread_for_cpuonly_llm_performance_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj4axf/thread_for_cpuonly_llm_performance_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T05:08:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nit4v6</id>
    <title>Granite 4 release today? Collection updated with 8 private repos.</title>
    <updated>2025-09-16T20:40:36+00:00</updated>
    <author>
      <name>/u/ironwroth</name>
      <uri>https://old.reddit.com/user/ironwroth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nit4v6/granite_4_release_today_collection_updated_with_8/"&gt; &lt;img alt="Granite 4 release today? Collection updated with 8 private repos." src="https://preview.redd.it/ihwp4dy78lpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=310d508b27499694f225a40decad5893a979dfda" title="Granite 4 release today? Collection updated with 8 private repos." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ironwroth"&gt; /u/ironwroth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ihwp4dy78lpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nit4v6/granite_4_release_today_collection_updated_with_8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nit4v6/granite_4_release_today_collection_updated_with_8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T20:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nivz2n</id>
    <title>We got a 2B param model running on iPhone at ~500MB RAM ‚Äî fully offline demo</title>
    <updated>2025-09-16T22:32:32+00:00</updated>
    <author>
      <name>/u/Josiahhenryus</name>
      <uri>https://old.reddit.com/user/Josiahhenryus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nivz2n/we_got_a_2b_param_model_running_on_iphone_at/"&gt; &lt;img alt="We got a 2B param model running on iPhone at ~500MB RAM ‚Äî fully offline demo" src="https://external-preview.redd.it/ZDZxemk3OWFzbHBmMVMFq2pfv69EmnrpZl789HXOOBvSofKD3EML3NWxX5eD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=517811a91d95e57b2a6e0b44c23976628615830f" title="We got a 2B param model running on iPhone at ~500MB RAM ‚Äî fully offline demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ongoing research out of Derive DX Labs in Lafayette, Louisiana. We‚Äôve been experimenting with efficiency optimizations and managed to get a 2B parameter chain-of-thought model running on iPhone with ~400‚Äì500MB RAM, fully offline.&lt;/p&gt; &lt;p&gt;I‚Äôm not super active on Reddit, so please don‚Äôt kill me if I‚Äôm slow to respond to comments ‚Äî but I‚Äôll do my best to answer questions.&lt;/p&gt; &lt;p&gt;[Correction: Meant Gemma-3N not Gemini-3B]&lt;/p&gt; &lt;p&gt;[Update on memory measurement: After running with Instruments, the total unified memory footprint is closer to ~2 GB (CPU + GPU) during inference, not just the 400‚Äì500 MB reported earlier. The earlier number reflected only CPU-side allocations. Still a big step down compared to the usual multi-GB requirements for 2B+ models.]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Josiahhenryus"&gt; /u/Josiahhenryus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6rczu79aslpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nivz2n/we_got_a_2b_param_model_running_on_iphone_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nivz2n/we_got_a_2b_param_model_running_on_iphone_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T22:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1niwb8l</id>
    <title>500,000 public datasets on Hugging Face</title>
    <updated>2025-09-16T22:46:45+00:00</updated>
    <author>
      <name>/u/clem59480</name>
      <uri>https://old.reddit.com/user/clem59480</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niwb8l/500000_public_datasets_on_hugging_face/"&gt; &lt;img alt="500,000 public datasets on Hugging Face" src="https://preview.redd.it/rokftav6vlpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26f96c62b0cfcf4ab8d9a212645ed0b0f54e16e2" title="500,000 public datasets on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem59480"&gt; /u/clem59480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rokftav6vlpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niwb8l/500000_public_datasets_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niwb8l/500000_public_datasets_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T22:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj7pik</id>
    <title>support for the upcoming Olmo3 model has been merged into llama.cpp</title>
    <updated>2025-09-17T08:42:16+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7pik/support_for_the_upcoming_olmo3_model_has_been/"&gt; &lt;img alt="support for the upcoming Olmo3 model has been merged into llama.cpp" src="https://external-preview.redd.it/11Q8uZ2-M8bnIL12n-O39P2wooNtmpfM5ORG4VqYvik.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fab39b6ac711c07bf4835557388faf67e2bdb807" title="support for the upcoming Olmo3 model has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16015"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7pik/support_for_the_upcoming_olmo3_model_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7pik/support_for_the_upcoming_olmo3_model_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T08:42:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nifajh</id>
    <title>I bought a modded 4090 48GB in Shenzhen. This is my story.</title>
    <updated>2025-09-16T11:52:20+00:00</updated>
    <author>
      <name>/u/king_priam_of_Troy</name>
      <uri>https://old.reddit.com/user/king_priam_of_Troy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt; &lt;img alt="I bought a modded 4090 48GB in Shenzhen. This is my story." src="https://external-preview.redd.it/1vD_R63iqu4vnM_qQf7pZNwXb9dy_UDc_Gl2j3LnTpU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5102c5612db16c04c26877a1e72e86700648e25" title="I bought a modded 4090 48GB in Shenzhen. This is my story." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ume4fe3jmipf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9aa908d45211be937b291377b1c495c9917834fe"&gt;https://preview.redd.it/ume4fe3jmipf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9aa908d45211be937b291377b1c495c9917834fe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few years ago, before ChatGPT became popular, I managed to score a Tesla P40 on eBay for around $150 shipped. With a few tweaks, I installed it in a Supermicro chassis. At the time, I was mostly working on video compression and simulation. It worked, but the card consistently climbed to 85¬∞C.&lt;/p&gt; &lt;p&gt;When DeepSeek was released, I was impressed and installed Ollama in a container. With 24GB of VRAM, it worked‚Äîbut slowly. After trying Stable Diffusion, it became clear that an upgrade was necessary.&lt;/p&gt; &lt;p&gt;The main issue was finding a modern GPU that could actually &lt;strong&gt;fit&lt;/strong&gt; in the server chassis. Standard 4090/5090 cards are designed for desktops: they're too large, and the power plug is inconveniently placed on top. After watching the LTT video featuring a modded 4090 with 48GB (and a follow-up from Gamers Nexus), I started searching the only place I knew might have one: Alibaba.com.&lt;/p&gt; &lt;p&gt;I contacted a seller and got a quote: &lt;strong&gt;CNY 22,900&lt;/strong&gt;. Pricey, but cheaper than expected. However, Alibaba enforces VAT collection, and I‚Äôve had bad experiences with DHL‚Äîthere was a non-zero chance I‚Äôd be charged twice for taxes. I was already over ‚Ç¨700 in taxes and fees.&lt;/p&gt; &lt;p&gt;Just for fun, I checked &lt;a href="http://Trip.com"&gt;Trip.com&lt;/a&gt; and realized that for the same amount of money, I could fly to Hong Kong and back, with a few days to explore. After confirming with the seller that they‚Äôd meet me at their business location, I booked a flight and an Airbnb in Hong Kong.&lt;/p&gt; &lt;p&gt;For context, I don‚Äôt speak Chinese at all. Finding the place using a Chinese address was tricky. Google Maps is useless in China, Apple Maps gave some clues, and Baidu Maps was beyond my skill level. With a little help from DeepSeek, I decoded the address and located the place in an industrial estate outside the city center. Thanks to Shenzhen‚Äôs extensive metro network, I didn‚Äôt need a taxi.&lt;/p&gt; &lt;p&gt;After arriving, the manager congratulated me for being the first foreigner to find them unassisted. I was given the card from a large batch‚Äîthey‚Äôre clearly producing these in volume at a factory elsewhere in town (I was proudly shown videos of the assembly line). I asked them to retest the card so I could verify its authenticity.&lt;/p&gt; &lt;p&gt;During the office tour, it was clear that their next frontier is repurposing old mining cards. I saw a large collection of NVIDIA Ampere mining GPUs. I was also told that modded 5090s with over 96GB of VRAM are in development.&lt;/p&gt; &lt;p&gt;After the test was completed, I paid in cash (a &lt;em&gt;lot&lt;/em&gt; of banknotes!) and returned to Hong Kong with my new purchase.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/king_priam_of_Troy"&gt; /u/king_priam_of_Troy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T11:52:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj9601</id>
    <title>Ling Flash 2.0 released</title>
    <updated>2025-09-17T10:14:07+00:00</updated>
    <author>
      <name>/u/abskvrm</name>
      <uri>https://old.reddit.com/user/abskvrm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9601/ling_flash_20_released/"&gt; &lt;img alt="Ling Flash 2.0 released" src="https://b.thumbs.redditmedia.com/Mje87GDqOP-_eCjHaD9MMo5kTGeRXnZhVYDnfTltcjY.jpg" title="Ling Flash 2.0 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ling Flash-2.0, from InclusionAI, a language model with 100B total parameters and 6.1B activated parameters (4.8B non-embedding).&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0"&gt;https://huggingface.co/inclusionAI/Ling-flash-2.0&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abskvrm"&gt; /u/abskvrm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nj9601"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9601/ling_flash_20_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9601/ling_flash_20_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T10:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj7mbu</id>
    <title>Big AI pushes the "we need to beat China" narrative cuz they want fat government contracts and zero democratic oversight. It's an old trick. Fear sells.</title>
    <updated>2025-09-17T08:36:32+00:00</updated>
    <author>
      <name>/u/katxwoods</name>
      <uri>https://old.reddit.com/user/katxwoods</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Throughout the Cold War, the military-industrial complex spent a fortune pushing the false narrative that the Soviet military was far more advanced than they actually were.&lt;/p&gt; &lt;p&gt;Why? To ensure the money from Congress kept flowing.&lt;/p&gt; &lt;p&gt;They lied‚Ä¶ and lied‚Ä¶ and lied again to get bigger and bigger defense contracts.&lt;/p&gt; &lt;p&gt;Now, obviously, there is &lt;em&gt;some&lt;/em&gt; amount of competition between the US and China, but &lt;strong&gt;Big Tech is stoking the flames beyond what is reasonable to terrify Congress into giving them whatever they want.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What they want is fat government contracts and zero democratic oversight. Day after day we hear about another big AI company announcing a giant contract with the Department of Defense.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/katxwoods"&gt; /u/katxwoods &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7mbu/big_ai_pushes_the_we_need_to_beat_china_narrative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7mbu/big_ai_pushes_the_we_need_to_beat_china_narrative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7mbu/big_ai_pushes_the_we_need_to_beat_china_narrative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T08:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nixynv</id>
    <title>The Qwen of Pain.</title>
    <updated>2025-09-16T23:58:16+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"&gt; &lt;img alt="The Qwen of Pain." src="https://preview.redd.it/0px1banw6mpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8edc833e57220e0c00a8b11ba32c881974742ef1" title="The Qwen of Pain." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0px1banw6mpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T23:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
