<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-06T23:06:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pfssoo</id>
    <title>"Router mode is experimental" | llama.cpp now has a router mode and I didn't know.</title>
    <updated>2025-12-06T16:06:19+00:00</updated>
    <author>
      <name>/u/charmander_cha</name>
      <uri>https://old.reddit.com/user/charmander_cha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did anyone else know that llama.cpp has a &amp;quot;router mode&amp;quot;? try out ! it's cool.&lt;/p&gt; &lt;p&gt;Little big history (you can ignore): &lt;/p&gt; &lt;p&gt;I've been trying to keep up with the updates on this sub and ComfyUI, but it's been a bit difficult to stay updated. From what I've observed, there don't seem to be any posts talking about this llama.cpp feature.&lt;/p&gt; &lt;p&gt;Because of this, I decided to share my experience:&lt;/p&gt; &lt;p&gt;I'm using llama.cpp, but I haven't been able to compile it with ROCm support ‚Äî it always gives me trouble when I try to use it.&lt;/p&gt; &lt;p&gt;I also don't use Docker. Every time I try, it doesn't recognize my GPU. I've tried several times to configure it to detect the hardware, but I just can't get it to work.&lt;/p&gt; &lt;p&gt;That's why I've always preferred Ollama for its ease of use. Recently, however, I realized that the GGUF models I want to use are available on Hugging Face and not on Ollama, and when I try to install them manually, I always get some incompatibility error.&lt;/p&gt; &lt;p&gt;I then decided to compile llama.cpp with Vulkan support, which is more universal and would have a better chance of working on my AMD Radeon RX 7600 XT GPU. Fortunately, the compilation was successful and I can now run some models.&lt;/p&gt; &lt;p&gt;However, I couldn't run Qwen-Next, which was frustrating. I thought my PC would run it without problems, since I can run the OpenAI quantized 120B model, so I imagined they would be similar in demand.&lt;/p&gt; &lt;p&gt;Despite this, I managed to run Qwen3-VL-8B-Instruct via Vulkan. When running the llama-serve command, a warning appeared about &amp;quot;router mode,&amp;quot; which basically allows switching between models directly through the interface generated on port 8080.&lt;/p&gt; &lt;p&gt;All this &amp;quot;lore&amp;quot; serves to contextualize my configuration and the challenges I faced using Pop!_OS, and perhaps it can help others who are in similar situations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/charmander_cha"&gt; /u/charmander_cha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfssoo/router_mode_is_experimental_llamacpp_now_has_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfssoo/router_mode_is_experimental_llamacpp_now_has_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfssoo/router_mode_is_experimental_llamacpp_now_has_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T16:06:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfbo6o</id>
    <title>Is there any model truly open, that you can train yourself from zero?</title>
    <updated>2025-12-06T00:38:47+00:00</updated>
    <author>
      <name>/u/puthre</name>
      <uri>https://old.reddit.com/user/puthre</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per title, is there any open source LLM that comes with all the data it was trained on and all the instructions that you can replicate yourself assuming you have access to the necesary hardware? And if not why not?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/puthre"&gt; /u/puthre &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T00:38:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg0jrn</id>
    <title>Built an offline voice-to-text tool for macOS using Parakeet</title>
    <updated>2025-12-06T21:29:26+00:00</updated>
    <author>
      <name>/u/_gordonclark</name>
      <uri>https://old.reddit.com/user/_gordonclark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0jrn/built_an_offline_voicetotext_tool_for_macos_using/"&gt; &lt;img alt="Built an offline voice-to-text tool for macOS using Parakeet" src="https://external-preview.redd.it/aO9ax1823LtOZGcI0aK2n4il89o5Hg6Q3c3cEopkN5Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc887f665d70ccae44e7f3342c50b34faf6196e4" title="Built an offline voice-to-text tool for macOS using Parakeet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been tinkering on a little side project called &lt;strong&gt;SilentKeys&lt;/strong&gt; and figured I‚Äôd share it here in case anyone finds it useful. &lt;/p&gt; &lt;p&gt;It‚Äôs basically &lt;strong&gt;realtime offline dictation for macOS&lt;/strong&gt;. No cloud, no accounts, nothing sent anywhere, it just listens locally and types straight into whatever app you have open. I built it because I wanted dictation that didn‚Äôt ship my voice to a server.&lt;/p&gt; &lt;p&gt;It‚Äôs still early and a bit rough around the edges, but it works surprisingly well. If you‚Äôre into privacy tools, voice workflows, accessibility stuff, or just like trying weird niche projects, I‚Äôd love to hear what you think.&lt;/p&gt; &lt;p&gt;Repo‚Äôs here: &lt;a href="https://github.com/gptguy/silentkeys"&gt;https://github.com/gptguy/silentkeys&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or get roasted gently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_gordonclark"&gt; /u/_gordonclark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/gptguy/silentkeys"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0jrn/built_an_offline_voicetotext_tool_for_macos_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0jrn/built_an_offline_voicetotext_tool_for_macos_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T21:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfcatm</id>
    <title>VoxCPM 1.5B just got released!</title>
    <updated>2025-12-06T01:07:54+00:00</updated>
    <author>
      <name>/u/Hefty_Wolverine_553</name>
      <uri>https://old.reddit.com/user/Hefty_Wolverine_553</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"&gt; &lt;img alt="VoxCPM 1.5B just got released!" src="https://external-preview.redd.it/MIb2iimHkfYqVDgmZztu-h5tz8yFqiAztGcy6umK7o8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08d7fc02333ca119537bfd3af70a4c74b40c2e98" title="VoxCPM 1.5B just got released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was just visiting the &lt;a href="https://github.com/OpenBMB/VoxCPM"&gt;GitHub page&lt;/a&gt; today (setting up a FastAPI TTS server) when I realized that they released a new version of the VoxCPM model. The original VoxCPM-0.5B was already very good in my testing, but this model looks like a straight improvement (it's still a 0.5B model, despite the rather confusing naming scheme).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;VoxCPM&lt;/th&gt; &lt;th align="left"&gt;VoxCPM1.5&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Audio VAE Sampling Rate&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;16kHz&lt;/td&gt; &lt;td align="left"&gt;44.1kHz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LM Token Rate&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;12.5Hz&lt;/td&gt; &lt;td align="left"&gt;6.25Hz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Patch Size&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;SFT Support&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LoRA Support&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;They also added fine-tuning support as well as a guide &lt;a href="https://github.com/OpenBMB/VoxCPM/blob/main/docs/finetune.md"&gt;https://github.com/OpenBMB/VoxCPM/blob/main/docs/finetune.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example output: &lt;a href="https://voca.ro/147qPjN98F6g"&gt;https://voca.ro/147qPjN98F6g&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hefty_Wolverine_553"&gt; /u/Hefty_Wolverine_553 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/openbmb/VoxCPM1.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T01:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfwr6q</id>
    <title>What alternative models are you using for Impossible models(on your system)?</title>
    <updated>2025-12-06T18:48:52+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To rephrase the title : What Small / MOE Alternatives are you using for Big models which don't fit your GPU(s)?&lt;/p&gt; &lt;p&gt;For example, some models are too big for our VRAM. Dense mostly.&lt;/p&gt; &lt;p&gt;In my case, my 8GB VRAM could run up to 14B models(&lt;sup&gt;Qwen3-14B Q4 giving me 20 t/s. If I increase the context, only single digit t/s&lt;/sup&gt;). Gemma3-12B also gave me similar numbers.&lt;/p&gt; &lt;p&gt;So I can't even imagine running 15-32B Dense models. For example, I really would like to use models like Gemma3-27B &amp;amp; Qwen3-32B but couldn't.&lt;/p&gt; &lt;p&gt;Even with offloading &amp;amp; other optimizations, I won't get more than 5 t/s. So during this situation, I go with small models or MOE models which could give better t/s.&lt;/p&gt; &lt;p&gt;Here some examples on my side:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gemma3-4B, Gemma3-12B(Q4), Gemma-3n-E2B &amp;amp; Gemma-3n-E4B &lt;strong&gt;instead of&lt;/strong&gt; Gemma3-27B&lt;/li&gt; &lt;li&gt;Qwen3-8B, Qwen3-14B(Q4), Qwen3-30B-A3B(Q4) &lt;strong&gt;instead of&lt;/strong&gt; Qwen3-32B&lt;/li&gt; &lt;li&gt;Mistral-Nemo-Instruct(12B @ Q4), Ministral-3(3B, 8B, 14B) &lt;strong&gt;instead of&lt;/strong&gt; Mistral-Small, Magistral-Small, Devstral-Small (All are 22-24B)&lt;/li&gt; &lt;li&gt;GPT-OSS-20B &lt;strong&gt;instead of&lt;/strong&gt; GPT-OSS-120B, Seed-OSS-36B, reka-flash, Devstral&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What are yours? Size doesn't matter(&lt;sup&gt;Ex: Some uses GLM Air instead of GLM due to big size&lt;/sup&gt;). &lt;/p&gt; &lt;p&gt;&lt;sup&gt;Personally I want to see what alternatives are there for Mistral 22-24B models(Need for writing&lt;/sup&gt;. Hope both Mistral &amp;amp; Gemma release MOE models in near future.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwr6q/what_alternative_models_are_you_using_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwr6q/what_alternative_models_are_you_using_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwr6q/what_alternative_models_are_you_using_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T18:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg043n</id>
    <title>How to Tune A RAG for Your Use Case [LanceDB √ó Kiln]</title>
    <updated>2025-12-06T21:10:40+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The teams at LanceDB and Kiln just teamed up to published a practical guide on building better RAG systems. We focus on how creating an eval lets you quickly iterate, finding the optimal RAG config for your use case in hours instead of weeks.&lt;/p&gt; &lt;p&gt;üîó Full Post: &lt;a href="https://lancedb.com/blog/rag-isnt-one-size-fits-all"&gt;RAG Isn't One-Size-Fits-All: Here's How to Tune It for Your Use Case&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Overview: Evals + Iteration = Quality&lt;/h1&gt; &lt;p&gt;RAG is a messy, multi-layer system where extraction, chunking, embeddings, retrieval, and generation all interact. Kiln makes it easy to create RAG evals in just a few minutes via a fast, safe evaluation loop so you can iterate with evidence, not vibes.&lt;/p&gt; &lt;p&gt;With Kiln, you can rapidly spin up evals using hundreds of Q&amp;amp;A pairs using our synthetic data generator. Once you have evals, it‚Äôs trivial to try different extraction, chunking and prompting strategies, then compare runs side by side across accuracy, recall, latency, and example-level outputs.&lt;/p&gt; &lt;p&gt;And because you can only improve what you can measure, you only measure what matters:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Answer correctness via Q&amp;amp;A evals&lt;/li&gt; &lt;li&gt;Hallucination rate and context recall&lt;/li&gt; &lt;li&gt;Correct-Call Rate to ensure your system only retrieves when retrieval is needed&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;With a robust eval loop, your RAG stops being fragile. You can safely swap models, retrievers, and test out multiple configs in hours, not weeks.&lt;/p&gt; &lt;h1&gt;Optimization Strategy&lt;/h1&gt; &lt;p&gt;In the post we proposed an optimization order that works well for optimization for most teams: Fix layers in order ‚Äî data ‚Üí chunking ‚Üí embeddings/retrieval ‚Üí generation -&amp;gt; integration.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Improve Document Extraction: better models, better prompts, and custom formats&lt;/li&gt; &lt;li&gt;Optimize Chunking: find the right chunk size based on your content (longer=articles, shorter=FAQs, invoices), and chunking strategy (per doc, fixed, semantic)&lt;/li&gt; &lt;li&gt;Embedding, Indexing &amp;amp; Retrieval: comparing embedding models, and retrieval options (text search, vector search, hybrid)&lt;/li&gt; &lt;li&gt;Integration into agents: ensure your RAG tool name and description gives your agents the information they need to know when and how to call RAG.&lt;/li&gt; &lt;li&gt;What not to grid-search (early on): pitfalls of premature optimization like optimizing perf before correctness or threshold obsession&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Evaluation Strategy&lt;/h1&gt; &lt;p&gt;We also walk though how to create great RAG evals. Once you have automated evals, you unlock rapid experimentation and optimization.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Start with answer-level evaluation (end-to-end evals). Deeper evals like RAG-recall are good to have, but if you aren‚Äôt testing that the RAG tool is called at the right time or that the generation produces a relevant answer, then you‚Äôre optimizing prematurely. If you only write one evaluation, make it end to end.&lt;/li&gt; &lt;li&gt;Use synthetic query+answer pairs for your evals. Usually the most tedious part, but Kiln can generate these automatically for you from your docs!&lt;/li&gt; &lt;li&gt;Evaluate that RAG is called at the right times: measure that RAG is called when needed, and not called when not needed, with tool-use evals.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The full blog post has more detail: &lt;a href="https://lancedb.com/blog/rag-isnt-one-size-fits-all"&gt;RAG Isn't One-Size-Fits-All: Here's How to Tune It for Your Use Case&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let us know if you have any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg043n/how_to_tune_a_rag_for_your_use_case_lancedb_kiln/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg043n/how_to_tune_a_rag_for_your_use_case_lancedb_kiln/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg043n/how_to_tune_a_rag_for_your_use_case_lancedb_kiln/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T21:10:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfrqvh</id>
    <title>Multi-directional ablation with self-organizing maps - anyone tried it yet?</title>
    <updated>2025-12-06T15:20:28+00:00</updated>
    <author>
      <name>/u/IllllIIlIllIllllIIIl</name>
      <uri>https://old.reddit.com/user/IllllIIlIllIllllIIIl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran across this preprint the other day: &lt;/p&gt; &lt;p&gt;Piras, Giorgio, et al. &amp;quot;&lt;a href="https://arxiv.org/abs/2511.08379"&gt;SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models.&lt;/a&gt;&amp;quot; arXiv preprint arXiv:2511.08379 (2025).&lt;/p&gt; &lt;p&gt;They have published their code here: &lt;a href="https://github.com/pralab/som-refusal-directions"&gt;https://github.com/pralab/som-refusal-directions&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Basically rather than the usual difference of means method for ablating a single refusal direction, they train a SOM to learn a refusal manifold and use Bayesian Optimization to determine the best subset of k directions to ablate. They got some pretty impressive results. &lt;/p&gt; &lt;p&gt;They only implemented the method for a handful of smaller models (nothing bigger than 14B), probably because the BO step is rather expensive. But it shouldn't be that hard to extend their code to support new models. &lt;/p&gt; &lt;p&gt;I was able to run the full pipeline on Qwen2.5-3B and replicate the results on that. I started extending the code to support gpt-oss-20b, but the further I got, the more I realized I'm too GPU poor to succeed in running it on that. &lt;/p&gt; &lt;p&gt;Any of you GPU rich bastards try this out on a larger model yet, or want to give it a shot?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IllllIIlIllIllllIIIl"&gt; /u/IllllIIlIllIllllIIIl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfrqvh/multidirectional_ablation_with_selforganizing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfrqvh/multidirectional_ablation_with_selforganizing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfrqvh/multidirectional_ablation_with_selforganizing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T15:20:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfxl6x</id>
    <title>[D] What I learned building code RAG without embeddings</title>
    <updated>2025-12-06T19:23:00+00:00</updated>
    <author>
      <name>/u/rozetyp</name>
      <uri>https://old.reddit.com/user/rozetyp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building a system to give LLMs relevant code context from any repo. The idea seemed simple: let an LLM look at the file tree + function signatures and pick which files to include. No embeddings, no vector DB.&lt;/p&gt; &lt;p&gt;Sharing what I learned because I wish someone had written this before I broke my eval three different ways.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Don‚Äôt eval on famous repos&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I started testing on Flask and FastAPI. GPT got 7/10 without any context - it was just reciting training data, not using my retrieval.&lt;/p&gt; &lt;p&gt;I switched to private repos and obscure OSS (&amp;lt;1K stars). ‚ÄúNo context‚Äù dropped to ~4.9/10. That was the real baseline!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. File paths aren‚Äôt enough&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Showing the LLM `src/auth/handler.py` doesn‚Äôt really tell it what‚Äôs inside. I added AST-extracted symbols:&lt;/p&gt; &lt;p&gt;&lt;em&gt;src/auth/handler.py [login, logout, refresh_token]&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;src/auth/middleware.py [require_auth, rate_limit]&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Retrieval quality jumped noticeably (NDCG went from ~0.85 to ~0.92). The model doesn‚Äôt need to read the full file to know ‚Äúthis smells like auth.‚Äù&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Same-vendor judging is inflated&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GPT-4 judging GPT-4‚Äôs answers gave suspiciously high scores! Switching to cross-vendor (GPT generates, Gemini judges) knocked about 0.5 off the scores and the reviews &lt;em&gt;felt&lt;/em&gt; more honest. The judge was much harsher on vague, confident answers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Generic eval criteria reward BS&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My first judge prompt used vague criteria like ‚Äúshould explain error handling‚Äù. That rewarded confident wrong answers.&lt;/p&gt; &lt;p&gt;What worked better was forcing exact hooks:&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;del&gt;‚ÄúShould explain the request lifecycle‚Äù&lt;/del&gt;&lt;/em&gt;&lt;em&gt;, &amp;quot;Must mention `RequestContext` and `full_dispatch_request()`‚Äù&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Anchoring eval on specific symbols/files made it much easier to spot hand-wavy nonsense.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results after fixing eval (very rough):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM file picker: ~0.92 NDCG, ~8.5/10 answer quality&lt;/li&gt; &lt;li&gt;Embeddings baseline: ~0.79 NDCG, ~8.6/10 answer quality&lt;/li&gt; &lt;li&gt;No context: ~4.9/10&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So the ‚ÄúLLM looks at the tree + symbols and picks files‚Äù setup landed roughly on par with embeddings on answer quality, without the indexing infrastructure. Good enough for me to keep using it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caveats!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small sample (177 questions, 14 repos)&lt;/li&gt; &lt;li&gt;I wrote the questions - probably biased toward what my approach handles&lt;/li&gt; &lt;li&gt;Private-repo results may not generalize beyond the ones I tested&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions for you:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How are you building eval sets that the model &lt;em&gt;hasn‚Äôt&lt;/em&gt; basically memorized?&lt;/li&gt; &lt;li&gt;Any tricks for making LLM-as-judge less biased when you‚Äôre judging your own system?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rozetyp"&gt; /u/rozetyp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxl6x/d_what_i_learned_building_code_rag_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxl6x/d_what_i_learned_building_code_rag_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxl6x/d_what_i_learned_building_code_rag_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T19:23:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf0q99</id>
    <title>You will own nothing and you will be happy!</title>
    <updated>2025-12-05T17:13:24+00:00</updated>
    <author>
      <name>/u/dreamyrhodes</name>
      <uri>https://old.reddit.com/user/dreamyrhodes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Come and put everything in to cloud. We now getting into hardware as a service. The RAM craze will impact everything to the point where consumers can't afford normal hardware anymore because it's all scraped off, locked away and put into datacenters to sell to you services to store your data. (Of course that data also will be used to train AI models to sell to you as a service as well lol.)&lt;/p&gt; &lt;p&gt;You don't need RAM anymore nor do you need SSDs. You will store and process every byte of your digital life in some datacenter and pay a monthly fee to access and process it.&lt;/p&gt; &lt;p&gt;You will own nothing and you will be happy!&lt;/p&gt; &lt;p&gt;GN: WTF Just Happened? | The Corrupt Memory Industry &amp;amp; Micron&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=9A-eeJP0J7c"&gt;https://www.youtube.com/watch?v=9A-eeJP0J7c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dreamyrhodes"&gt; /u/dreamyrhodes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T17:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfqm0y</id>
    <title>Speed of DeepSeek with RAM offload</title>
    <updated>2025-12-06T14:29:05+00:00</updated>
    <author>
      <name>/u/vhthc</name>
      <uri>https://old.reddit.com/user/vhthc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 96GB VRAM. By far not enough to run DeepSeek 3.x - bit I could upgrade my RAM so I can have the active layers on the GPU and the rest in system RAM. Yeah the RAM prices are a catastrophe but I need to run such a large model, and I don‚Äôt want to use cloud - this is locallama!&lt;/p&gt; &lt;p&gt;Has anyone tried this? What speed can I expect with a 64kb context length in prompt processing and tokens per second?&lt;/p&gt; &lt;p&gt;It would be quite the investment so if anyone has real world data that would be great!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vhthc"&gt; /u/vhthc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqm0y/speed_of_deepseek_with_ram_offload/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqm0y/speed_of_deepseek_with_ram_offload/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqm0y/speed_of_deepseek_with_ram_offload/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T14:29:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pftdc6</id>
    <title>Best benchmark website</title>
    <updated>2025-12-06T16:30:46+00:00</updated>
    <author>
      <name>/u/AccomplishedStory327</name>
      <uri>https://old.reddit.com/user/AccomplishedStory327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which website do you use to see benchmark stats of different models, apart from using your own suite?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AccomplishedStory327"&gt; /u/AccomplishedStory327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pftdc6/best_benchmark_website/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pftdc6/best_benchmark_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pftdc6/best_benchmark_website/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T16:30:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfxrv5</id>
    <title>Convert Dense into MOE model?</title>
    <updated>2025-12-06T19:30:30+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did a quick search on this here &amp;amp; found only 2 years old &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1cgo45x/converting_dense_models_into_moes/"&gt;thread&lt;/a&gt; with less replies. That's it.&lt;/p&gt; &lt;p&gt;So still no one figured it out this yet? Totally surprised that no one brought this topic here after that old thread.&lt;/p&gt; &lt;p&gt;I know it's a very big thing. But it would be a miracle if some one comes with this precious solution.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxrv5/convert_dense_into_moe_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxrv5/convert_dense_into_moe_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfxrv5/convert_dense_into_moe_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T19:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg1rhf</id>
    <title>Minimax M2</title>
    <updated>2025-12-06T22:22:16+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What does the community think of Minimax M2?&lt;/p&gt; &lt;p&gt;Benches surprisingly well and the Minimax team tend to be strong at RL.&lt;/p&gt; &lt;p&gt;Any experiences with this model? Any tips or preferred use-cases?&lt;/p&gt; &lt;p&gt;Particularly interested in STEM, coding and agentic but all use-cases welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg1rhf/minimax_m2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg1rhf/minimax_m2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg1rhf/minimax_m2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T22:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfiar0</id>
    <title>Qwen3-TTS</title>
    <updated>2025-12-06T06:21:51+00:00</updated>
    <author>
      <name>/u/Terrible_Scar_9890</name>
      <uri>https://old.reddit.com/user/Terrible_Scar_9890</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-TTS-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-TTS-Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terrible_Scar_9890"&gt; /u/Terrible_Scar_9890 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T06:21:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfsntn</id>
    <title>convert: support Mistral 3 Large MoE by ngxson ¬∑ Pull Request #17730 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-12-06T16:00:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsntn/convert_support_mistral_3_large_moe_by_ngxson/"&gt; &lt;img alt="convert: support Mistral 3 Large MoE by ngxson ¬∑ Pull Request #17730 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/YXlCrbFuGSaJRzk-d-1JftjUbGO215ldNJVTXMLJQi4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4507263a618891c23289c740acf9be9cc8bee393" title="convert: support Mistral 3 Large MoE by ngxson ¬∑ Pull Request #17730 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can now download GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/mistralai_Mistral-Large-3-675B-Instruct-2512-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Mistral-Large-3-675B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but can you run it...? &lt;/p&gt; &lt;p&gt;(that another PR is &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17744"&gt;https://github.com/ggml-org/llama.cpp/pull/17744&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17730"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsntn/convert_support_mistral_3_large_moe_by_ngxson/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsntn/convert_support_mistral_3_large_moe_by_ngxson/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T16:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfq0kd</id>
    <title>PaperDebugger: the Best Overleaf Companion!</title>
    <updated>2025-12-06T14:01:42+00:00</updated>
    <author>
      <name>/u/NuoJohnChen</name>
      <uri>https://old.reddit.com/user/NuoJohnChen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"&gt; &lt;img alt="PaperDebugger: the Best Overleaf Companion!" src="https://b.thumbs.redditmedia.com/LSzFW-bVRkmLrP-afZdLmy0DNmjvCz1MK2UnMO8aqLo.jpg" title="PaperDebugger: the Best Overleaf Companion!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chrome/APP Store: &lt;a href="https://www.paperdebugger.com/"&gt;https://www.paperdebugger.com/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2512.02589"&gt;https://arxiv.org/abs/2512.02589&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/PaperDebugger/PaperDebugger"&gt;https://github.com/PaperDebugger/PaperDebugger&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enhancer: &lt;a href="https://huggingface.co/Xtra-Computing/XtraGPT-7B"&gt;https://huggingface.co/Xtra-Computing/XtraGPT-7B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;An NUS team just released &amp;quot;PaperDebugger&amp;quot;: an in-editor system that uses multiple agents (Reviewer, Researcher, Scorer) to rewrite and critique papers in real-time within Overleaf. Just simply select a rough section, and it launches the full pipeline. &lt;/p&gt; &lt;p&gt;Direct Integration: No copy-pasting. It patches the document with Git-style before/after diffs.&lt;/p&gt; &lt;p&gt;Deep Research: Can pull arXiv papers, summarize them, and generate comparison tables inline.&lt;/p&gt; &lt;p&gt;Tech Stack: Uses an MCP toolchain and Kubernetes to scale the agent reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NuoJohnChen"&gt; /u/NuoJohnChen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pfq0kd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T14:01:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfl0d8</id>
    <title>How big an open source model can I run on 128 GB unified memory?</title>
    <updated>2025-12-06T09:13:03+00:00</updated>
    <author>
      <name>/u/nameless_me</name>
      <uri>https://old.reddit.com/user/nameless_me</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just took delivery of a Minisforum MS-S1 with AMD Ryzen Ai Max+ 395 cpu, 128 GB unified memory architecture and AMD Radeon 8060S Graphics. In the BIOS the UDMA memory for the iGPU is set to 96 GB. Running a Debian Linux terminal in WSL 2, I downloaded and ran ollama which works fine.&lt;/p&gt; &lt;p&gt;Trying a Deepseek-r1:70b model, it refused to load in ollama. I checked a few sources which ended saying this &amp;quot;&lt;strong&gt;DeepSeek-R1-70B INT4 GGUF still requires ~55‚Äì60 GB VRAM equivalent&lt;/strong&gt;. &lt;strong&gt;You cannot run this model on a single consumer APU&lt;/strong&gt;, even with ‚Äú128 GB unified memory‚Äù.&lt;/p&gt; &lt;p&gt;Is the above true? What is the largest LLM model I can run reasonably on this computer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nameless_me"&gt; /u/nameless_me &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T09:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfyrwm</id>
    <title>Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</title>
    <updated>2025-12-06T20:12:22+00:00</updated>
    <author>
      <name>/u/Educational-Pound269</name>
      <uri>https://old.reddit.com/user/Educational-Pound269</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfyrwm/live_avatar_streaming_realtime_audiodriven_avatar/"&gt; &lt;img alt="Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length" src="https://external-preview.redd.it/YjQybWV5bmY1bjVnMatgKKMAYanNbnGU9s9FiIXTW5q8AYgZBBw2qwcYT6Ul.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aad98486fbb5e0f55eb92c0c79b334973e80e088" title="Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They just dropped a REALTIME, infinite length video generator.&lt;/p&gt; &lt;p&gt;Based on Wan, 20 fps, with dialogue&lt;/p&gt; &lt;p&gt;The code will be open source in early December.&lt;br /&gt; &lt;a href="https://liveavatar.github.io/"&gt;https://liveavatar.github.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational-Pound269"&gt; /u/Educational-Pound269 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zewd3onf5n5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfyrwm/live_avatar_streaming_realtime_audiodriven_avatar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfyrwm/live_avatar_streaming_realtime_audiodriven_avatar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T20:12:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg0tbe</id>
    <title>Zebra-Llama: Towards Extremely Efficient Hybrid Models</title>
    <updated>2025-12-06T21:40:53+00:00</updated>
    <author>
      <name>/u/divide0verfl0w</name>
      <uri>https://old.reddit.com/user/divide0verfl0w</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2505.17272"&gt;https://arxiv.org/abs/2505.17272&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HN Link: &lt;a href="https://news.ycombinator.com/item?id=46176289"&gt;https://news.ycombinator.com/item?id=46176289&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/divide0verfl0w"&gt; /u/divide0verfl0w &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0tbe/zebrallama_towards_extremely_efficient_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0tbe/zebrallama_towards_extremely_efficient_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0tbe/zebrallama_towards_extremely_efficient_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T21:40:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfwu8t</id>
    <title>Are MoE models harder to Fine-tune?</title>
    <updated>2025-12-06T18:52:21+00:00</updated>
    <author>
      <name>/u/ComplexType568</name>
      <uri>https://old.reddit.com/user/ComplexType568</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;really sorry if this is a stupid question, but ive been looking around huggingface A LOT and ive noticed a really big trend where theres a ton of dense models being fine-tuned/lora-ed, while most MoE models go untouched. are there any reasons for this? &lt;/p&gt; &lt;p&gt;i dont think its the model size, as ive seen big models like Llama 70B or even 405B turn into Hermes 4 models, Tulu, etc. while pretty good models like practically the entire Qwen3 series, GLM (besides GLM Steam), DeepSeek and Kimi are untouched, id get why DS and Kimi are untouched... but, seriously, Qwen3?? so far ive seen an ArliAI finetune only. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexType568"&gt; /u/ComplexType568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwu8t/are_moe_models_harder_to_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwu8t/are_moe_models_harder_to_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfwu8t/are_moe_models_harder_to_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T18:52:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfg0rh</id>
    <title>The Best Open-Source 8B-Parameter LLM Built in the USA</title>
    <updated>2025-12-06T04:14:17+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"&gt; &lt;img alt="The Best Open-Source 8B-Parameter LLM Built in the USA" src="https://preview.redd.it/r6muiibadi5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f50b4cb0f889ed02690c8f3ff7e90713b46562c" title="The Best Open-Source 8B-Parameter LLM Built in the USA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models.&lt;/p&gt; &lt;p&gt;These models &lt;/p&gt; &lt;ul&gt; &lt;li&gt;perform well across a range of programming languages. &lt;/li&gt; &lt;li&gt;boast strong agentic capabilities (e.g., inside agentic frameworks like mini-SWE-agent). &lt;/li&gt; &lt;li&gt;excel at tool-calling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both raw and instruct variants are available on &lt;a href="https://huggingface.co/collections/EssentialAI/rnj-1"&gt;Hugging Face platform&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Architecture Overview&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Rnj-1's architecture is similar to Gemma 3, except that it uses only global attention, and YaRN for long-context extension.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training Dynamics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;rnj-1&lt;/code&gt; was pre-trained on 8.4T tokens with an 8K context length, after which the model‚Äôs context window was extended to &lt;strong&gt;32K&lt;/strong&gt; through an additional 380B-token mid-training stage. &lt;/p&gt; &lt;p&gt;A final 150B-token SFT stage completed the training to produce &lt;code&gt;rnj-1-instruct&lt;/code&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r6muiibadi5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T04:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfvt9e</id>
    <title>VibeVoice Realtime 0.5B - OpenAI Compatible /v1/audio/speech TTS Server</title>
    <updated>2025-12-06T18:10:19+00:00</updated>
    <author>
      <name>/u/marhensa</name>
      <uri>https://old.reddit.com/user/marhensa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"&gt; &lt;img alt="VibeVoice Realtime 0.5B - OpenAI Compatible /v1/audio/speech TTS Server" src="https://b.thumbs.redditmedia.com/s91ewzR1qewE9gWdg8jOP6ycdK9l1T_UjLsYMD8uNoo.jpg" title="VibeVoice Realtime 0.5B - OpenAI Compatible /v1/audio/speech TTS Server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft recently released &lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;VibeVoice-Realtime-0.5B&lt;/a&gt;, a lightweight &lt;strong&gt;&lt;em&gt;expressive&lt;/em&gt;&lt;/strong&gt; TTS model.&lt;/p&gt; &lt;p&gt;I wrapped it in an OpenAI-compatible API server so it works directly with Open WebUI's TTS settings.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/marhensa/vibevoice-realtime-openai-api.git"&gt;https://github.com/marhensa/vibevoice-realtime-openai-api.git&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop-in using OpenAI-compatible &lt;code&gt;/v1/audio/speech&lt;/code&gt; endpoint&lt;/li&gt; &lt;li&gt;Runs locally with Docker or Python venv (via uv)&lt;/li&gt; &lt;li&gt;Using only ~2GB of VRAM&lt;/li&gt; &lt;li&gt;CUDA-optimized (around ~1x RTF on RTX 3060 12GB)&lt;/li&gt; &lt;li&gt;Multiple voices with OpenAI name aliases (alloy, nova, etc.)&lt;/li&gt; &lt;li&gt;All models auto-download on first run&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1pfvt9e/video/7emfqdbdjm5g1/player"&gt;Video demonstration of \&amp;quot;Mike\&amp;quot; male voice. Audio üì¢ ON.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The expression and flow is better than Kokoro, imho. But Kokoro is faster.&lt;/p&gt; &lt;p&gt;But (for now) it lacks female voice model, there's just two female, and one is weirdly sounds like a male üòÖ.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6r87w5d9pm5g1.png?width=1073&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adfd10fae1523fed7f2898c38ae92816130cbf2d"&gt;vibevoice-realtime-openai-api Settings on Open WebUI: Set chunk splitting to Paragraphs.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Contribution are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marhensa"&gt; /u/marhensa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfvt9e/vibevoice_realtime_05b_openai_compatible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T18:10:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfx3d0</id>
    <title>We need open source hardware lithography</title>
    <updated>2025-12-06T19:02:38+00:00</updated>
    <author>
      <name>/u/bennmann</name>
      <uri>https://old.reddit.com/user/bennmann</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perhaps it's time hardware was more democratized. RISC-V is only 1 step away.&lt;/p&gt; &lt;p&gt;There are real challenges with yield at small scales, requiring a clean environment. But perhaps a small scale system could be made &amp;quot;good enough&amp;quot;, or overcome with some clever tech or small vacuum chambers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bennmann"&gt; /u/bennmann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfx3d0/we_need_open_source_hardware_lithography/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfx3d0/we_need_open_source_hardware_lithography/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfx3d0/we_need_open_source_hardware_lithography/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T19:02:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg0hpw</id>
    <title>A weird lesson I learned after running small LLM experiments for months</title>
    <updated>2025-12-06T21:26:55+00:00</updated>
    <author>
      <name>/u/Mediocre_Common_4126</name>
      <uri>https://old.reddit.com/user/Mediocre_Common_4126</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I kept upgrading models, GPUs and settings thinking the improvements would come from the tech itself. But none of the real breakthroughs came from bigger models. They came from understanding my own data way better than I expected to&lt;/p&gt; &lt;p&gt;The moment things changed was when I stopped treating the dataset like a static object. I started treating it like a living thing. Every small phrasing pattern, every tiny inconsistency, every emotional spike in text was doing more work than any hyperparameter I touched&lt;/p&gt; &lt;p&gt;Once I slowed down and actually studied how people talk in specific situations, the fine tuning started behaving almost predictably. I didn‚Äôt need fancy tricks, I just needed better raw language that matched the task. The outputs felt less robotic and more grounded because the model finally had something real to learn from&lt;/p&gt; &lt;p&gt;It made me realize how much of LLM performance is just the texture of the data. Not size, not magic settings, just the texture. If the texture is right the model wakes up in a different way. It feels more intentional and less brittle&lt;/p&gt; &lt;p&gt;This little shift saved me a lot of compute and frustration and honestly made the work fun again!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mediocre_Common_4126"&gt; /u/Mediocre_Common_4126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0hpw/a_weird_lesson_i_learned_after_running_small_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0hpw/a_weird_lesson_i_learned_after_running_small_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg0hpw/a_weird_lesson_i_learned_after_running_small_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T21:26:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
