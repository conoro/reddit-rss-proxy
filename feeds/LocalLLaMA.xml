<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-29T14:25:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mb9uy8</id>
    <title>Qwen/Qwen3-30B-A3B-Instruct-2507 ¬∑ Hugging Face</title>
    <updated>2025-07-28T07:33:42+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-30B-A3B-Instruct-2507 ¬∑ Hugging Face" src="https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f" title="Qwen/Qwen3-30B-A3B-Instruct-2507 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No model card as of yet&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T07:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbs4dw</id>
    <title>8600G / 760M llama-bench with Gemma 3 (4, 12, 27B), Mistral Small, Qwen 3 (4, 8, 14, 32B) and Qwen 3 MoE 30B-A3B</title>
    <updated>2025-07-28T20:55:42+00:00</updated>
    <author>
      <name>/u/SunRayWhisper</name>
      <uri>https://old.reddit.com/user/SunRayWhisper</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I couldn't find any extensive benchmarks when researching this APU, so I'm sharing my findings with the community.&lt;/p&gt; &lt;p&gt;The benchmarks with the iGPU 760M results ~35% faster than the CPU alone (see the tests below, with ngl 0, no layers offloaded to the GPU), the prompt processing is also faster, and it appears to produce less heat.&lt;/p&gt; &lt;p&gt;It allows me to chat with Gemma 3 27B at ~5 tokens per second (t/s), and Qwen 3 30B-A3B works at around 35 t/s.&lt;/p&gt; &lt;p&gt;So it's not a 3090, a Mac, or a Strix Halo, obviously, but gives access to these models without being power-hungry, expensive, and it's widely available.&lt;/p&gt; &lt;p&gt;Another thing I was looking for was how it compared to my Steam Deck. Apparently, with LLMs, the 8600G is about twice as fast.&lt;/p&gt; &lt;p&gt;Note 1: if you have in mind a gaming PC, unless you just want a small machine with only the APU, a regular 7600 or 9600 has more cache, PCIe lanes, and PCIe 5 support. However, the 8600G is still faster at 1080p with games than the Steam Deck at 800p. So, well, it's usable for light gaming and doesn't consume too much power, but it's not the best choice for a gaming PC.&lt;/p&gt; &lt;p&gt;Note 2: there are mini-PCs with similar AMD APUs; however, if you have enough space, a desktop case offers better cooling and is probably quieter. Plus, if you want to add a GPU, mini-PCs require complex and costly eGPU setups (when the option is available), while with a desktop PC it's straightforward (even though the 8600G is lane-limited, so still not the ideal).&lt;/p&gt; &lt;p&gt;Note 3: the 8700G comes with a better cooler (though still mediocre), a slightly better iGPU (but only about 10% faster in games, and the difference for LLMs is likely negligible), and two extra cores; however, it's definitively more expensive.&lt;/p&gt; &lt;p&gt;=== Setup and notes ===&lt;/p&gt; &lt;pre&gt;&lt;code&gt;OS: Kubuntu 24.04 RAM: 64GB DDR5-6000 IOMMU: disabled &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Apparently, &lt;strong&gt;IOMMU&lt;/strong&gt; slows it down noticeably:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Gemma 3 4B pp512 tg12 IOMMU off = ~395 32.70 IOMMU on = ~360 29.6 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hence, the following benchmarks are with IOMMU disabled.&lt;/p&gt; &lt;p&gt;The 8600G default is 65W, but &lt;strong&gt;at 35W it loses very little performance&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Gemma 3 4B pp512 tg12 65W = ~395 32.70 35W = ~372 31.86 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also the stock fan seems better suited for the APU set at 35W. At 65W it could still barely handle the CPU-only Gemma3-12B benchmark (at least in my airflow case), but it thermal-throttles with larger models.&lt;/p&gt; &lt;p&gt;Anyway, for consistency, the following tests are at 65W and I limited the CPU-only tests to the smaller models.&lt;/p&gt; &lt;p&gt;Benchmarks:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama.cpp build: 01612b74 (5922) ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1103_R1) (radv) | uma: 1 | fp16: 1 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat backend: RPC, Vulcan === Gemma 3 q4_0_QAT (by stduhpf) | model | size | params | ngl | test | t/s | ------------------------------ | --------: | ------: | --: | ----: | ------------: (4B, iGPU 760M) | gemma3 4B Q4_0 | 2.19 GiB | 3.88 B | 99 | pp128 | 378.02 ¬± 1.44 | gemma3 4B Q4_0 | 2.19 GiB | 3.88 B | 99 | pp256 | 396.18 ¬± 1.88 | gemma3 4B Q4_0 | 2.19 GiB | 3.88 B | 99 | pp512 | 395.16 ¬± 1.79 | gemma3 4B Q4_0 | 2.19 GiB | 3.88 B | 99 | tg128 | 32.70 ¬± 0.04 (4B, CPU) | gemma3 4B Q4_0 | 2.19 GiB | 3.88 B | 0 | pp512 | 313.53 ¬± 2.00 | gemma3 4B Q4_0 | 2.19 GiB | 3.88 B | 0 | tg128 | 24.09 ¬± 0.02 (12B, iGPU 760M) | gemma3 12B Q4_0 | 6.41 GiB | 11.77 B | 99 | pp512 | 121.56 ¬± 0.18 | gemma3 12B Q4_0 | 6.41 GiB | 11.77 B | 99 | tg128 | 11.45 ¬± 0.03 (12B, CPU) | gemma3 12B Q4_0 | 6.41 GiB | 11.77 B | 0 | pp512 | 98.25 ¬± 0.52 | gemma3 12B Q4_0 | 6.41 GiB | 11.77 B | 0 | tg128 | 8.39 ¬± 0.01 (27B, iGPU 760M) | gemma3 27B Q4_0 | 14.49 GiB | 27.01 B | 99 | pp512 | 52.22 ¬± 0.01 | gemma3 27B Q4_0 | 14.49 GiB | 27.01 B | 99 | tg128 | 5.37 ¬± 0.01 === Mistral Small (24B) 3.2 2506 (UD-Q4_K_XL by unsloth) | model | size | params | test | t/s | ------------------------------ | ---------: | -------: | ----: | -------------: | llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | pp512 | 52.49 ¬± 0.04 | llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | tg128 | 5.90 ¬± 0.00 [oddly, it's identified as &amp;quot;llama 13B&amp;quot;] === Qwen 3 | model | size | params | test | t/s | ------------------------------ | ---------: | -------: | ----: | -------------: (4B Q4_K_L by Bartowski) | qwen3 4B Q4_K - Medium | 2.41 GiB | 4.02 B | pp512 | 299.86 ¬± 0.44 | qwen3 4B Q4_K - Medium | 2.41 GiB | 4.02 B | tg128 | 29.91 ¬± 0.03 (8B Q4 Q4_K_M by unsloth) | qwen3 8B Q4_K - Medium | 4.68 GiB | 8.19 B | pp512 | 165.73 ¬± 0.13 | qwen3 8B Q4_K - Medium | 4.68 GiB | 8.19 B | tg128 | 17.75 ¬± 0.01 [Note: UD-Q4_K_XL by unsloth is only slightly slower with pp512 164.68 ¬± 0.20, tg128 16.84 ¬± 0.01] (8B Q6 UD-Q6_K_XL by unsloth) | qwen3 8B Q6_K | 6.97 GiB | 8.19 B | pp512 | 167.45 ¬± 0.14 | qwen3 8B Q6_K | 6.97 GiB | 8.19 B | tg128 | 12.45 ¬± 0.00 (8B Q8_0 by unsloth) | qwen3 8B Q8_0 | 8.11 GiB | 8.19 B | pp512 | 177.91 ¬± 0.13 | qwen3 8B Q8_0 | 8.11 GiB | 8.19 B | tg128 | 10.66 ¬± 0.00 (14B UD-Q4_K_XL by unsloth) | qwen3 14B Q4_K - Medium | 8.53 GiB | 14.77 B | pp512 | 87.37 ¬± 0.14 | qwen3 14B Q4_K - Medium | 8.53 GiB | 14.77 B | tg128 | 9.39 ¬± 0.01 (32B Q4_K_L by Bartowski) | qwen3 32B Q4_K - Medium | 18.94 GiB | 32.76 B | pp512 | 36.64 ¬± 0.02 | qwen3 32B Q4_K - Medium | 18.94 GiB | 32.76 B | tg128 | 4.36 ¬± 0.00 === Qwen 3 30B-A3B MoE (UD-Q4_K_XL by unsloth) | model | size | params | test | t/s | ------------------------------ | ---------: | -------: | ----: | -------------: | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | pp512 | 83.43 ¬± 0.35 | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | tg128 | 34.77 ¬± 0.27 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunRayWhisper"&gt; /u/SunRayWhisper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbs4dw/8600g_760m_llamabench_with_gemma_3_4_12_27b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbs4dw/8600g_760m_llamabench_with_gemma_3_4_12_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbs4dw/8600g_760m_llamabench_with_gemma_3_4_12_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T20:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc5jsx</id>
    <title>Best open source voice cloning today, with hours of reference?</title>
    <updated>2025-07-29T08:00:51+00:00</updated>
    <author>
      <name>/u/goldcakes</name>
      <uri>https://old.reddit.com/user/goldcakes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve got more than 100 hours of clean, studio-grade speech for a character, and I‚Äôd like to explore what the SOTA is for open source voice cloning or voice changing. &lt;/p&gt; &lt;p&gt;Is the SOTA for large datasets still RVC, or are there better solutions now? I have a RTX 5090 with 32GB VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goldcakes"&gt; /u/goldcakes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5jsx/best_open_source_voice_cloning_today_with_hours/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5jsx/best_open_source_voice_cloning_today_with_hours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5jsx/best_open_source_voice_cloning_today_with_hours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T08:00:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc5s4r</id>
    <title>Can you suggest a better WebUI program for textgen that has better memory management than Oobabooga?</title>
    <updated>2025-07-29T08:16:10+00:00</updated>
    <author>
      <name>/u/-Fibon4cci</name>
      <uri>https://old.reddit.com/user/-Fibon4cci</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5s4r/can_you_suggest_a_better_webui_program_for/"&gt; &lt;img alt="Can you suggest a better WebUI program for textgen that has better memory management than Oobabooga?" src="https://preview.redd.it/6td8j8oqurff1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da5d78f6e7e675a867acf4adf3ee9157dac8ae16" title="Can you suggest a better WebUI program for textgen that has better memory management than Oobabooga?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Fibon4cci"&gt; /u/-Fibon4cci &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6td8j8oqurff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5s4r/can_you_suggest_a_better_webui_program_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5s4r/can_you_suggest_a_better_webui_program_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T08:16:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbflsw</id>
    <title>GLM 4.5 Collection Now Live!</title>
    <updated>2025-07-28T13:03:59+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b"&gt;https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbflsw/glm_45_collection_now_live/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbflsw/glm_45_collection_now_live/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbflsw/glm_45_collection_now_live/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T13:03:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbp4nm</id>
    <title>The walled garden gets higher walls: Anthropic is adding weekly rate limits for paid Claude subscribers</title>
    <updated>2025-07-28T19:02:58+00:00</updated>
    <author>
      <name>/u/Resident_Egg5765</name>
      <uri>https://old.reddit.com/user/Resident_Egg5765</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Got an interesting email from Anthropic today. Looks like they're adding new weekly usage limits for their paid Claude subscribers (Pro and Max), on top of the existing 5-hour limits.&lt;/p&gt; &lt;p&gt;The email mentions it's a way to handle policy violations and &amp;quot;advanced usage patterns,&amp;quot; like running Claude 24/7. They estimate the new weekly cap for their top &amp;quot;Max&amp;quot; tier will be around 24-40 hours of Opus 4 usage before you have to pay standard API rates.&lt;/p&gt; &lt;p&gt;This definitely got me thinking about the pros and cons of relying on commercial platforms. The power of models like Opus is undeniable, but this is also a reminder that the terms can change, which can be a challenge for anyone with a consistent, long-term workflow.&lt;/p&gt; &lt;p&gt;It really highlights some of the inherent strengths of the local approach we have here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stability:&lt;/strong&gt; Your workflow is insulated from sudden policy changes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Freedom:&lt;/strong&gt; You have the freedom to run intensive or long-running tasks without hitting a usage cap.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Predictability:&lt;/strong&gt; The only real limits are your own hardware and time.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm curious to hear how the community sees this.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does this kind of change make you lean more heavily into your local setup?&lt;/li&gt; &lt;li&gt;For those who use a mix of tools, how do you decide when an API is worth it versus firing up a local model?&lt;/li&gt; &lt;li&gt;And on a technical note, how close do you feel the top open-source models are to replacing something like Opus for your specific use cases (coding, writing, etc.)?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to the discussion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Resident_Egg5765"&gt; /u/Resident_Egg5765 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbp4nm/the_walled_garden_gets_higher_walls_anthropic_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbp4nm/the_walled_garden_gets_higher_walls_anthropic_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbp4nm/the_walled_garden_gets_higher_walls_anthropic_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T19:02:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcavlf</id>
    <title>Best Local LLM + Hardware Build for Coding With a $15k Budget (2025)</title>
    <updated>2025-07-29T13:03:45+00:00</updated>
    <author>
      <name>/u/lavoid12</name>
      <uri>https://old.reddit.com/user/lavoid12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm looking to build (ideally buy) a workstation to run local large language models (LLMs) for coding, software development, and general AI assistance. Budget is around $15k USD.&lt;/p&gt; &lt;p&gt;I want something that feels close to ChatGPT4 or Claude in reasoning speed and accuracy, but fully local so I can use it for coding (VSCode integration, code completion, debugging, etc.).&lt;/p&gt; &lt;p&gt;Looking for advice on both which models and what hardware to get. Here are my main questions:&lt;/p&gt; &lt;p&gt;For Local LLM: ‚Ä¢What‚Äôs the best-performing opensource LLM right now for coding (DeepSeek 33B, Llama 3 70B, Mistral, something else)?&lt;/p&gt; &lt;p&gt;‚Ä¢Which models are most Claude/GPT-like for reasoning, not just spitting code?&lt;/p&gt; &lt;p&gt;‚Ä¢Are there any quantized or fine-tuned versions that run well without needing $30k of GPUs?&lt;/p&gt; &lt;p&gt;‚Ä¢What frameworks are people using (Ollama, LM Studio, vLLM, llama.cpp) for fast inference and coding integrations?&lt;/p&gt; &lt;p&gt;‚Ä¢Any VSCode or JetBrains tools/plugins that work well with local models?&lt;/p&gt; &lt;p&gt;General Hardware Questions ‚Ä¢For around $15k, is it better to go with multiple consumer GPUs (2‚Äì4x RTX 5090s) or one workstation GPU (A100/H100)?&lt;/p&gt; &lt;p&gt;‚Ä¢How much VRAM and RAM do I realistically need to run 30B‚Äì70B parameter models smoothly?&lt;/p&gt; &lt;p&gt;‚Ä¢Would you recommend buying something like a Lambda Vector workstation or building a custom rig?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lavoid12"&gt; /u/lavoid12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcavlf/best_local_llm_hardware_build_for_coding_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcavlf/best_local_llm_hardware_build_for_coding_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcavlf/best_local_llm_hardware_build_for_coding_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T13:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc0m3e</id>
    <title>SmallThinker Technical Report Release!</title>
    <updated>2025-07-29T03:12:12+00:00</updated>
    <author>
      <name>/u/Zealousideal_Bad_52</name>
      <uri>https://old.reddit.com/user/Zealousideal_Bad_52</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc0m3e/smallthinker_technical_report_release/"&gt; &lt;img alt="SmallThinker Technical Report Release!" src="https://b.thumbs.redditmedia.com/DPYXkXYKiJVkQ40-jlvcuMdmOUBGPiWDPqFYKHNtroQ.jpg" title="SmallThinker Technical Report Release!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2507.20984"&gt;https://arxiv.org/abs/2507.20984&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SmallThinker&lt;/strong&gt; is a family of on-device native &lt;strong&gt;Mixture-of-Experts&lt;/strong&gt; language models specifically designed for efficient local deployment. With the constraints of limited computational power and memory capacity in mind, SmallThinker introduces novel architectural innovations to enable high-performance inference on consumer-grade hardware.&lt;/p&gt; &lt;p&gt;Even on a personal computer equipped with only 8GB of CPU memory, SmallThinker achieves a remarkable inference speed of &lt;strong&gt;20 tokens per second&lt;/strong&gt; when powered by &lt;a href="https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker"&gt;PowerInfer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Notably, &lt;strong&gt;SmallThinker&lt;/strong&gt; is now supported in &lt;strong&gt;llama.cpp&lt;/strong&gt;, making it even more accessible for everyone who want to run advanced MoE models entirely offline and locally.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m5vbkud89qff1.png?width=1382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d014c217defcd629cbb8684dc891878d2895c28b"&gt;https://preview.redd.it/m5vbkud89qff1.png?width=1382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d014c217defcd629cbb8684dc891878d2895c28b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here is the downstream benchmark performance compare to other SOTA LLMs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2zk0d3sqbqff1.png?width=1546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec049b7e339e2ee19db51883b328af84e86d6ccf"&gt;https://preview.redd.it/2zk0d3sqbqff1.png?width=1546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec049b7e339e2ee19db51883b328af84e86d6ccf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And the GGUF link is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF"&gt;PowerInfer/SmallThinker-21BA3B-Instruct-GGUF ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF"&gt;PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal_Bad_52"&gt; /u/Zealousideal_Bad_52 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc0m3e/smallthinker_technical_report_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc0m3e/smallthinker_technical_report_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc0m3e/smallthinker_technical_report_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T03:12:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbny6o</id>
    <title>100x faster and 100x cheaper transcription with open models vs proprietary</title>
    <updated>2025-07-28T18:19:36+00:00</updated>
    <author>
      <name>/u/crookedstairs</name>
      <uri>https://old.reddit.com/user/crookedstairs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open-weight ASR models have gotten super competitive with proprietary providers (eg deepgram, assemblyai) in recent months. On some leaderboards like &lt;a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard"&gt;HuggingFace's ASR leaderboard&lt;/a&gt; they're posting up crazy WER and RTFx numbers. Parakeet in particular claims to process 3000+ minutes of audio in less than a minute, which means you can save a lot of money if you self-host.&lt;/p&gt; &lt;p&gt;We at Modal benchmarked cost, throughput, and accuracy of the latest ASR models against a popular proprietary model: &lt;a href="https://modal.com/blog/fast-cheap-batch-transcription"&gt;https://modal.com/blog/fast-cheap-batch-transcription&lt;/a&gt;. We also wrote up a bunch of engineering tips on how to best optimize a batch transcription service for max throughput. If you're currently using either open source or proprietary ASR models would love to know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crookedstairs"&gt; /u/crookedstairs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbny6o/100x_faster_and_100x_cheaper_transcription_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbny6o/100x_faster_and_100x_cheaper_transcription_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbny6o/100x_faster_and_100x_cheaper_transcription_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T18:19:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbfa3y</id>
    <title>Wan 2.2 is Live! Needs only 8GB of VRAM!</title>
    <updated>2025-07-28T12:49:51+00:00</updated>
    <author>
      <name>/u/Comed_Ai_n</name>
      <uri>https://old.reddit.com/user/Comed_Ai_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbfa3y/wan_22_is_live_needs_only_8gb_of_vram/"&gt; &lt;img alt="Wan 2.2 is Live! Needs only 8GB of VRAM!" src="https://preview.redd.it/w2tqvij93mff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aa487bb7dc2bff5b7326e25dfec4967cd6c8e51" title="Wan 2.2 is Live! Needs only 8GB of VRAM!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comed_Ai_n"&gt; /u/Comed_Ai_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w2tqvij93mff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbfa3y/wan_22_is_live_needs_only_8gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbfa3y/wan_22_is_live_needs_only_8gb_of_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T12:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc5gv1</id>
    <title>Finetuning Script for Voxtral</title>
    <updated>2025-07-29T07:55:38+00:00</updated>
    <author>
      <name>/u/DistributionLucky763</name>
      <uri>https://old.reddit.com/user/DistributionLucky763</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5gv1/finetuning_script_for_voxtral/"&gt; &lt;img alt="Finetuning Script for Voxtral" src="https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d821b402151de285d39de64aaea0364ad627ae9" title="Finetuning Script for Voxtral" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We put together a small repo to fine‚Äëtune &lt;strong&gt;Mistral‚Äôs Voxtral (3B)&lt;/strong&gt; for &lt;strong&gt;transcription&lt;/strong&gt; using Huggingface&lt;strong&gt;.&lt;/strong&gt; We could not find a public finetuning/ training script yet, so we think this could be interesting for the community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DistributionLucky763"&gt; /u/DistributionLucky763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Innovative-Digitale-Medizin-IDM/voxtral-finetune"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5gv1/finetuning_script_for_voxtral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5gv1/finetuning_script_for_voxtral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T07:55:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc8fhc</id>
    <title>Let's Build a "Garage AI Supercomputer": A P2P Compute Grid for Inference</title>
    <updated>2025-07-29T11:03:16+00:00</updated>
    <author>
      <name>/u/ModeSquare8129</name>
      <uri>https://old.reddit.com/user/ModeSquare8129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã!&lt;/p&gt; &lt;p&gt;For the past 18 months, my colleague and I have been working on &lt;strong&gt;Ebiose&lt;/strong&gt;, an open-source initiative (MIT license) born at Inria (the French lab behind projects like scikit-learn).&lt;/p&gt; &lt;p&gt;Ebiose aims to create a decentralized AI factory, a Darwin-style playground (√† la Google‚Äôs AlphaEvolve) where AI agents design, test, and evolve other agents. Anyone can launch their own &amp;quot;forge,&amp;quot; define a task, and watch AI agents compete until the fittest emerge.&lt;/p&gt; &lt;p&gt;This evolutionary approach demands massive inference resources. Currently, we're relying on cloud APIs, but our long-term vision is a fully decentralized, community-driven system.&lt;/p&gt; &lt;p&gt;That's why we'd love input from the LocalLLaMA community!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Big Idea: A Community-Powered P2P Inference Grid&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We‚Äôre dreaming of a peer-to-peer compute grid that taps into the idle power of community-run machines, like Folding@home, but for local LLMs. Here‚Äôs the plan:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Lightweight Client:&lt;/strong&gt; A background app runs on your PC (and maybe phones later).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware Profiling:&lt;/strong&gt; The client auto-detects what LLMs your machine can handle.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Orchestration Layer:&lt;/strong&gt; A system (centralized or decentralized?) assigns inference tasks to capable nodes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic LoRA Adapters:&lt;/strong&gt; Fine-tune models efficiently with lightweight, modular adapters.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch &amp;amp; Prompt Caching:&lt;/strong&gt; Optimize for high throughput by batching requests and reusing system prompts.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technical Questions for the Community&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Inference Backend:&lt;/strong&gt; We‚Äôre leaning toward &lt;strong&gt;llama.cpp&lt;/strong&gt; for its lightweight design and broad hardware support (CPU, Metal, CUDA). But for a high-throughput setup, would &lt;strong&gt;vLLM&lt;/strong&gt;, &lt;strong&gt;zml&lt;/strong&gt;, or another engine be better? Since we‚Äôre prioritizing batch processing over single-prompt speed, what‚Äôs your pick?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task Orchestration:&lt;/strong&gt; How do we route inference jobs (e.g., ‚Äúrun this 13B model with this prompt‚Äù) to nodes with the right model cached and enough VRAM/RAM? Has anyone tackled this kind of distributed task management?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Existing Tools:&lt;/strong&gt; Are there open-source projects we could build on?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What do you think? Got ideas, tools, or experiences to share?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ModeSquare8129"&gt; /u/ModeSquare8129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T11:03:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbt030</id>
    <title>So you all loved my open-source voice AI when I first showed it off - I officially got response times to under 2 seconds AND it now fits all within 9 gigs of VRAM! Open Source Code included!</title>
    <updated>2025-07-28T21:29:57+00:00</updated>
    <author>
      <name>/u/RoyalCities</name>
      <uri>https://old.reddit.com/user/RoyalCities</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbt030/so_you_all_loved_my_opensource_voice_ai_when_i/"&gt; &lt;img alt="So you all loved my open-source voice AI when I first showed it off - I officially got response times to under 2 seconds AND it now fits all within 9 gigs of VRAM! Open Source Code included!" src="https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93f42c262fc974647e9ce3851a14528ad44f2fbe" title="So you all loved my open-source voice AI when I first showed it off - I officially got response times to under 2 seconds AND it now fits all within 9 gigs of VRAM! Open Source Code included!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now I got A LOT of messages when I first showed it off so I decided to spend some time to put together a full video on the high level designs behind it and also why I did it in the first place - &lt;a href="https://www.youtube.com/watch?v=bE2kRmXMF0I"&gt;https://www.youtube.com/watch?v=bE2kRmXMF0I&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve also open sourced my short / long term memory designs, vocal daisy chaining and also my docker compose stack. This should help let a lot of people get up and running! &lt;a href="https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/tree/main"&gt;https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RoyalCities"&gt; /u/RoyalCities &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qvwxsxvrnoff1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbt030/so_you_all_loved_my_opensource_voice_ai_when_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbt030/so_you_all_loved_my_opensource_voice_ai_when_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T21:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbg1ck</id>
    <title>GLM4.5 released!</title>
    <updated>2025-07-28T13:22:25+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbg1ck/glm45_released/"&gt; &lt;img alt="GLM4.5 released!" src="https://b.thumbs.redditmedia.com/h1a9hbYRlufo6ZLB7b1IgSekwr0g4qcrXjR2rdPGMPU.jpg" title="GLM4.5 released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, we introduce two new GLM family members: GLM-4.5 and GLM-4.5-Air ‚Äî our latest flagship models. GLM-4.5 is built with 355 billion total parameters and 32 billion active parameters, and GLM-4.5-Air with 106 billion total parameters and 12 billion active parameters. Both are designed to unify reasoning, coding, and agentic capabilities into a single model in order to satisfy more and more complicated requirements of fast rising agentic applications.&lt;/p&gt; &lt;p&gt;Both GLM-4.5 and GLM-4.5-Air are hybrid reasoning models, offering: thinking mode for complex reasoning and tool using, and non-thinking mode for instant responses. They are available on Z.ai, BigModel.cn and open-weights are avaiable at HuggingFace and ModelScope.&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://z.ai/blog/glm-4.5"&gt;https://z.ai/blog/glm-4.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5"&gt;https://huggingface.co/zai-org/GLM-4.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5-Air"&gt;https://huggingface.co/zai-org/GLM-4.5-Air&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mbg1ck"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbg1ck/glm45_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbg1ck/glm45_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T13:22:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc644b</id>
    <title>Told Qwen3 1.7b (thinking) to make a black hole simulation</title>
    <updated>2025-07-29T08:38:35+00:00</updated>
    <author>
      <name>/u/Gold_Bar_4072</name>
      <uri>https://old.reddit.com/user/Gold_Bar_4072</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc644b/told_qwen3_17b_thinking_to_make_a_black_hole/"&gt; &lt;img alt="Told Qwen3 1.7b (thinking) to make a black hole simulation" src="https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d696fb474805c3a5718a57a45397302f5ed019b" title="Told Qwen3 1.7b (thinking) to make a black hole simulation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gold_Bar_4072"&gt; /u/Gold_Bar_4072 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e5xhwj4azrff1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc644b/told_qwen3_17b_thinking_to_make_a_black_hole/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc644b/told_qwen3_17b_thinking_to_make_a_black_hole/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T08:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc5e54</id>
    <title>Single-File Qwen3 Inference in Pure CUDA C</title>
    <updated>2025-07-29T07:50:39+00:00</updated>
    <author>
      <name>/u/Awkward_Click6271</name>
      <uri>https://old.reddit.com/user/Awkward_Click6271</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One .cu file holds everything necessary for inference. There are no external libraries; only the CUDA runtime is included. Everything, from tokenization right down to the kernels, is packed into this single file.&lt;/p&gt; &lt;p&gt;It works with the Qwen3 0.6B model GGUF at full precision. On an RTX 3060, it generates appr. ~32 tokens per second. For benchmarking purposes, you can enable cuBLAS, which increase the TPS to ~70.&lt;/p&gt; &lt;p&gt;The CUDA version is built upon my qwen.c repo. It's a pure C inference, again contained within a single file. It uses the Qwen3 0.6B at 32FP too, which I think is the most explainable and demonstrable setup for pedagogical purposes.&lt;/p&gt; &lt;p&gt;Both versions use the GGUF file directly, with no conversion to binary. The tokenizer‚Äôs vocab and merges are plain text files, making them easy to inspect and understand. You can run multi-turn conversations, and reasoning tasks supported by Qwen3.&lt;/p&gt; &lt;p&gt;These projects draw inspiration from Andrej Karpathy‚Äôs &lt;a href="https://github.com/karpathy/llama2.c"&gt;llama2.c&lt;/a&gt; and share the same commitment to minimalism. Both projects are MIT licensed. I‚Äôd love to hear your feedback!&lt;/p&gt; &lt;p&gt;qwen3.cu: &lt;a href="https://github.com/gigit0000/qwen3.cu"&gt;https://github.com/gigit0000/qwen3.cu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;qwen3.c: &lt;a href="https://github.com/gigit0000/qwen3.c"&gt;https://github.com/gigit0000/qwen3.c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward_Click6271"&gt; /u/Awkward_Click6271 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5e54/singlefile_qwen3_inference_in_pure_cuda_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5e54/singlefile_qwen3_inference_in_pure_cuda_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5e54/singlefile_qwen3_inference_in_pure_cuda_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T07:50:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc6dfx</id>
    <title>Something lightweight: a LLM simulation of Bernie Sanders</title>
    <updated>2025-07-29T08:55:50+00:00</updated>
    <author>
      <name>/u/ivoras</name>
      <uri>https://old.reddit.com/user/ivoras</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc6dfx/something_lightweight_a_llm_simulation_of_bernie/"&gt; &lt;img alt="Something lightweight: a LLM simulation of Bernie Sanders" src="https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f80fc97e46dbb90afe19165e1d70bec1ffb040f" title="Something lightweight: a LLM simulation of Bernie Sanders" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Light-hearted, too. Don't take it too seriously!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivoras"&gt; /u/ivoras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ivoras/bernie0.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc6dfx/something_lightweight_a_llm_simulation_of_bernie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc6dfx/something_lightweight_a_llm_simulation_of_bernie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T08:55:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc687c</id>
    <title>New Benchmark - FamilyBench - Test models ability to understand complex tree type relationship and reason on massive context. Immune to contamination. GML 4.5 64.02%, Gemini 2.5 pro 81,48%.</title>
    <updated>2025-07-29T08:46:06+00:00</updated>
    <author>
      <name>/u/Orolol</name>
      <uri>https://old.reddit.com/user/Orolol</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;This is a new &lt;strong&gt;opensource&lt;/strong&gt; project, a benchmark that test model ability to understand complex tree-like relationship in a family tree across a massive context. &lt;/p&gt; &lt;p&gt;The idea is to have a python program that generate a tree and can use the tree structure to generate question about it. Then you can have a textual description of this tree and those question to have a text that is hard to understand for LLMs. &lt;/p&gt; &lt;p&gt;You can find the code here &lt;a href="https://github.com/Orolol/familyBench"&gt;https://github.com/Orolol/familyBench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current leaderboard&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I test 7 models (6 open weight and 1 closed) on a complex tree with 400 people generated across 10 generations (which represent ~18k tokens). 200 questions are then asked to the models. All models are for now tested via OpenRouter, with low reasoning effort or 8k max token, and a temperature of 0.3. I plan to gather optimal params for each model later.&lt;/p&gt; &lt;p&gt;Example of family description : &amp;quot;Aaron (M) has white hair, gray eyes, wears a gold hat and works as a therapist. Aaron (M) has 2 children: Barry (M), Erica (F). Abigail (F) has light brown hair, amber eyes, wears a red hat and works as a teacher. Abigail (F) has 1 child: Patricia (F) ...&amp;quot;&lt;/p&gt; &lt;p&gt;Example of questions : &amp;quot;Which of Paula's grandparents have salt and pepper hair?&amp;quot; &amp;quot;Who is the cousin of the daughter of Quentin with red hair?&amp;quot;&lt;/p&gt; &lt;p&gt;The no response rate is when the model overthinks and is then unable to produce an answer because he used his 16k max tokens. I try to reduce this rate as much as I can, but this very often indicate that a model is unable to find the answer and is stuck in a reasoning loop. &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Accuracy&lt;/th&gt; &lt;th&gt;Total tokens&lt;/th&gt; &lt;th&gt;No response rate&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Gemini 2.5 Pro&lt;/td&gt; &lt;td&gt;81.48%&lt;/td&gt; &lt;td&gt;271,500&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM 4.5&lt;/td&gt; &lt;td&gt;64.02%&lt;/td&gt; &lt;td&gt;216,281&lt;/td&gt; &lt;td&gt;2.12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM 4.5 air&lt;/td&gt; &lt;td&gt;57.14%&lt;/td&gt; &lt;td&gt;909,228&lt;/td&gt; &lt;td&gt;26.46%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen-3.2-2507-thinking&lt;/td&gt; &lt;td&gt;50.26%&lt;/td&gt; &lt;td&gt;743,131&lt;/td&gt; &lt;td&gt;20.63%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kimi K2&lt;/td&gt; &lt;td&gt;34.92%&lt;/td&gt; &lt;td&gt;67,071&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen-3.2-2507&lt;/td&gt; &lt;td&gt;28.04%&lt;/td&gt; &lt;td&gt;3,098&lt;/td&gt; &lt;td&gt;0.53%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral Small 3.2&lt;/td&gt; &lt;td&gt;22.22%&lt;/td&gt; &lt;td&gt;5,353&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Reasoning models have a clear advantage here, but produce a massive amount of token (which means some models are quite expansive to test). More models are coming to the leaderboard (R1, Sonnet)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Orolol"&gt; /u/Orolol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc687c/new_benchmark_familybench_test_models_ability_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc687c/new_benchmark_familybench_test_models_ability_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc687c/new_benchmark_familybench_test_models_ability_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T08:46:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc5oh2</id>
    <title>This year‚Äôs best open-source models and most cost-effective models</title>
    <updated>2025-07-29T08:09:33+00:00</updated>
    <author>
      <name>/u/Apart-River475</name>
      <uri>https://old.reddit.com/user/Apart-River475</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/"&gt; &lt;img alt="This year‚Äôs best open-source models and most cost-effective models" src="https://b.thumbs.redditmedia.com/oqyuYVJJYg1zSXUWu9TgdBdJGts5YfXbLSB6jfU2bbs.jpg" title="This year‚Äôs best open-source models and most cost-effective models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;GLM 4.5 and GLM-4.5-AIR&lt;/strong&gt;&lt;br /&gt; The &lt;strong&gt;GLM-4.5&lt;/strong&gt; series models are foundation models designed for intelligent agents. GLM-4.5 has &lt;strong&gt;355&lt;/strong&gt; billion total parameters with &lt;strong&gt;32&lt;/strong&gt; billion active parameters, while GLM-4.5-Air adopts a more compact design with &lt;strong&gt;106&lt;/strong&gt; billion total parameters and &lt;strong&gt;12&lt;/strong&gt; billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bisgmn0utrff1.png?width=4464&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8b159e95ccba8f0becc1ee6fb596cb4fdde5217c"&gt;Bench performance&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://z.ai/blog/glm-4.5"&gt;blog&lt;/a&gt;ÔΩú&lt;a href="https://huggingface.co/zai-org/GLM-4.5"&gt;huggingface&lt;/a&gt;ÔΩú &lt;a href="https://github.com/zai-org/GLM-4.5"&gt;github&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart-River475"&gt; /u/Apart-River475 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T08:09:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc8evq</id>
    <title>Built RL training for long-horizon terminal agents - tested on 32x H100s but too GPU poor to train üòÖ</title>
    <updated>2025-07-29T11:02:25+00:00</updated>
    <author>
      <name>/u/DanAiTuning</name>
      <uri>https://old.reddit.com/user/DanAiTuning</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8evq/built_rl_training_for_longhorizon_terminal_agents/"&gt; &lt;img alt="Built RL training for long-horizon terminal agents - tested on 32x H100s but too GPU poor to train üòÖ" src="https://b.thumbs.redditmedia.com/GTUl_GxBM3AgORm0fFuwPhwKeJqsGTeIOOsHWvhrYYI.jpg" title="Built RL training for long-horizon terminal agents - tested on 32x H100s but too GPU poor to train üòÖ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üëã After my calculator agent RL post, I really wanted to go bigger! So I built RL infrastructure for training long-horizon terminal/coding agents that scales from 2x A100s to 32x H100s (~$1M worth of compute!) Without any training, my 32B agent hit #19 on Terminal-Bench leaderboard, beating Stanford's Terminus-Qwen3-235B-A22! With training... well, too expensive, but I bet the results would be good! üòÖ&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Created a Claude Code-inspired agent (system msg + tools)&lt;/li&gt; &lt;li&gt;Built Docker-isolated GRPO training where each rollout gets its own container&lt;/li&gt; &lt;li&gt;Developed a multi-agent synthetic data pipeline to generate &amp;amp; validate training data with Opus-4&lt;/li&gt; &lt;li&gt;Implemented a hybrid reward signal of unit test verifiers &amp;amp; a behavioural LLM judge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;My untrained Qwen3-32B agent achieved &lt;strong&gt;13.75%&lt;/strong&gt; on Terminal-Bench (#19, beats Stanford's Qwen3-235B MoE)&lt;/li&gt; &lt;li&gt;I tested training to work stably on 32x H100s distributed across 4 bare metal nodes&lt;/li&gt; &lt;li&gt;I created a mini-eval framework for LLM-judge performance. Sonnet-4 won.&lt;/li&gt; &lt;li&gt;~¬£30-50k needed for full training run of 1000 epochs (I could only afford testing üòÖ)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The synthetic dataset ranges from easy to extremely hard tasks. An example hard task's prompt: &lt;ul&gt; &lt;li&gt;&amp;quot;I found this mystery program at `/app/program` and I'm completely stumped. It's a stripped binary, so I have no idea what it does or how to run it properly. The program seems to expect some specific input and then produces an output, but I can't figure out what kind of input it needs. Could you help me figure out what this program requires?&amp;quot;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Simple config presets allow training to run on multiple hardware setups with minimal effort.&lt;/li&gt; &lt;li&gt;GRPO used with 16 rollouts per task, up to 32k tokens per rollout.&lt;/li&gt; &lt;li&gt;Agent uses XML/YAML format to structure tool calls&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My Github repos open source it all (agent, data, code) and has way more technical details if you are interested!:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚≠êÔ∏è &lt;a href="https://github.com/Danau5tin/terminal-bench-rl"&gt;Terminal Agent RL repo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Danau5tin/tbench-agentic-data-pipeline"&gt;‚≠êÔ∏è Multi-agent synthetic data pipeline repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I thought I would share this because I believe long-horizon RL is going to change everybody's lives, and so I feel it is important (and super fun!) for us all to share knowledge around this area, and also have enjoy exploring what is possible.&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;p&gt;Dan&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(&lt;/strong&gt;Built using &lt;a href="https://github.com/rllm-org/rllm"&gt;rLLM&lt;/a&gt; RL framework which was brilliant to work with, and evaluated and inspired by the great &lt;a href="https://www.tbench.ai/"&gt;Terminal Bench&lt;/a&gt; benchmark)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanAiTuning"&gt; /u/DanAiTuning &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mc8evq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8evq/built_rl_training_for_longhorizon_terminal_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8evq/built_rl_training_for_longhorizon_terminal_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T11:02:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc9sk0</id>
    <title>üåü Ming-lite-omni v1.5 is here! Our recent upgrade for omni-modal AI! üöÄ</title>
    <updated>2025-07-29T12:14:38+00:00</updated>
    <author>
      <name>/u/Dependent-Roll-8934</name>
      <uri>https://old.reddit.com/user/Dependent-Roll-8934</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ming-lite-omni v1.5 demonstrates highly competitive results compared to industry-leading models of similar scale.&lt;/p&gt; &lt;p&gt;ü§ñGithub: &lt;a href="https://github.com/inclusionAI/Ming"&gt;https://github.com/inclusionAI/Ming&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü´ÇHugging Face: &lt;a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5"&gt;https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üç≠ModelScope: &lt;a href="https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5"&gt;https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ming-lite-omni v1.5 features three key improvements compared to Ming-lite-omni: &lt;/p&gt; &lt;p&gt;üß† Enhanced Multimodal Comprehension: Ming-lite-omni v1.5 now understands all data types‚Äîimages, text, video, and speech‚Äîsignificantly better, thanks to extensive data upgrades.&lt;/p&gt; &lt;p&gt;üé® Precise Visual Editing Control: Achieve superior image generation and editing with Ming-lite-omni v1.5, featuring advanced controls for consistent IDs and scenes, and enhanced support for visual tasks like detection and segmentation.&lt;/p&gt; &lt;p&gt;‚ú® Optimized User Experience: Expect a smoother, more accurate, and aesthetically pleasing interaction with Ming-lite-omni v1.5.&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dependent-Roll-8934"&gt; /u/Dependent-Roll-8934 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc9sk0/mingliteomni_v15_is_here_our_recent_upgrade_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc9sk0/mingliteomni_v15_is_here_our_recent_upgrade_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc9sk0/mingliteomni_v15_is_here_our_recent_upgrade_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T12:14:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc9o4m</id>
    <title>Stuck on a problem? We're excited to share a glimpse of what's possible! üëã</title>
    <updated>2025-07-29T12:08:36+00:00</updated>
    <author>
      <name>/u/Dependent-Roll-8934</name>
      <uri>https://old.reddit.com/user/Dependent-Roll-8934</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc9o4m/stuck_on_a_problem_were_excited_to_share_a/"&gt; &lt;img alt="Stuck on a problem? We're excited to share a glimpse of what's possible! üëã" src="https://external-preview.redd.it/NTFreWF2YzkwdGZmMRJSn5s4IHja8tcwSrnrzPqbup3fCh9rR2T7vwZXZXDc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4fbfc313e05d906ffc795bed85b7f8f2c8e0e3c5" title="Stuck on a problem? We're excited to share a glimpse of what's possible! üëã" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Our experimental Ming-lite-omni v1.5 (&lt;a href="https://github.com/inclusionAI/Ming"&gt;https://github.com/inclusionAI/Ming&lt;/a&gt;) leverages advanced audio-visual capabilities to explore new frontiers in interactive learning. This model, still under development, aims to understand your handwriting, interpret your thoughts, and guide you through solutions in real-time. We're eagerly continuing our research and look forward to sharing future advancements! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dependent-Roll-8934"&gt; /u/Dependent-Roll-8934 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sdqo34a90tff1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc9o4m/stuck_on_a_problem_were_excited_to_share_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc9o4m/stuck_on_a_problem_were_excited_to_share_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T12:08:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc6fbp</id>
    <title>GLM 4.5 support is landing in llama.cpp</title>
    <updated>2025-07-29T08:59:17+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc6fbp/glm_45_support_is_landing_in_llamacpp/"&gt; &lt;img alt="GLM 4.5 support is landing in llama.cpp" src="https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bbb4d01a722a7ac5908e1ba272a92870c5277cd" title="GLM 4.5 support is landing in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14939"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc6fbp/glm_45_support_is_landing_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc6fbp/glm_45_support_is_landing_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T08:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbvf2z</id>
    <title>its getting comical</title>
    <updated>2025-07-28T23:09:30+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbvf2z/its_getting_comical/"&gt; &lt;img alt="its getting comical" src="https://preview.redd.it/txsukljc5pff1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=400b5b6efa830b5698a57bf456c6a99acd74b24d" title="its getting comical" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/txsukljc5pff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbvf2z/its_getting_comical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbvf2z/its_getting_comical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T23:09:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc8tks</id>
    <title>I just tried GLM 4.5</title>
    <updated>2025-07-29T11:24:54+00:00</updated>
    <author>
      <name>/u/AI-On-A-Dime</name>
      <uri>https://old.reddit.com/user/AI-On-A-Dime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just wanted to try it out because I was a bit skeptical. So I prompted it with a fairly simple not so cohesive prompt and asked it to prepare slides for me.&lt;/p&gt; &lt;p&gt;The results were pretty remarkable I must say! &lt;/p&gt; &lt;p&gt;Here‚Äôs the link to the results: &lt;a href="https://chat.z.ai/space/r05c76960ff0-ppt"&gt;https://chat.z.ai/space/r05c76960ff0-ppt&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Here‚Äôs the initial prompt:&lt;/p&gt; &lt;p&gt;‚ÄùCreate a presentation of global BESS market for different industry verticals. Make sure to capture market shares, positioning of different players, market dynamics and trends and any other area you find interesting. Do not make things up, make sure to add citations to any data you find.‚Äù&lt;/p&gt; &lt;p&gt;As you can see pretty bland prompt with no restrictions, no role descriptions, no examples. Nothing, just what my mind was thinking it wanted.&lt;/p&gt; &lt;p&gt;Is it just me or are things going superfast since OpenAI announced the release of GPT-5?&lt;/p&gt; &lt;p&gt;It seems like just yesterday Qwen3 broke apart all benchmarks in terms of quality/cost trade offs and now z.ai with yet another efficient but high quality model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AI-On-A-Dime"&gt; /u/AI-On-A-Dime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T11:24:54+00:00</published>
  </entry>
</feed>
