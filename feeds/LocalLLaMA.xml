<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-15T19:26:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qdj2nn</id>
    <title>solution for local deep research</title>
    <updated>2026-01-15T13:09:44+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am still trying to set up a good local deep research workflow.&lt;/p&gt; &lt;p&gt;What I‚Äôve found so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/assafelovic/gpt-researcher"&gt;https://github.com/assafelovic/gpt-researcher&lt;/a&gt; ‚Äì the best one so far, but I need to refresh the browser after each research run&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/bytedance/deer-flow"&gt;https://github.com/bytedance/deer-flow&lt;/a&gt; ‚Äì another good option, but I was only able to run it in text mode (without webui)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In general, you always need to set the OpenAI endpoint to a local LLM and then switch web search from a paid provider to duckduckgo, for example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$env:OPENAI_BASE_URL = &amp;quot;http://127.0.0.1:8080/v1&amp;quot; $env:RETRIEVER = &amp;quot;duckduckgo&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Another popular project is &lt;a href="https://github.com/Alibaba-NLP/DeepResearch"&gt;https://github.com/Alibaba-NLP/DeepResearch&lt;/a&gt;, but it looks like it requires a specific model.&lt;/p&gt; &lt;p&gt;Do you use something else? Please share your experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdj2nn/solution_for_local_deep_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdj2nn/solution_for_local_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdj2nn/solution_for_local_deep_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T13:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdd1l7</id>
    <title>Step-Audio-R1.1 (Open Weight) by StepFun just set a new SOTA on the Artificial Analysis Speech Reasoning leaderboard</title>
    <updated>2026-01-15T07:20:11+00:00</updated>
    <author>
      <name>/u/Inevitable_Sea8804</name>
      <uri>https://old.reddit.com/user/Inevitable_Sea8804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdd1l7/stepaudior11_open_weight_by_stepfun_just_set_a/"&gt; &lt;img alt="Step-Audio-R1.1 (Open Weight) by StepFun just set a new SOTA on the Artificial Analysis Speech Reasoning leaderboard" src="https://b.thumbs.redditmedia.com/VlFNErXfdwuwoj22ZqGSh4EmErOD8Zi59CLceVgOUFM.jpg" title="Step-Audio-R1.1 (Open Weight) by StepFun just set a new SOTA on the Artificial Analysis Speech Reasoning leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post: &lt;a href="https://x.com/ModelScope2022/status/2011687986338136089"&gt;https://x.com/ModelScope2022/status/2011687986338136089&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/stepfun-ai/Step-Audio-R1.1"&gt;https://huggingface.co/stepfun-ai/Step-Audio-R1.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://modelscope.cn/studios/stepfun-ai/Step-Audio-R1"&gt;https://modelscope.cn/studios/stepfun-ai/Step-Audio-R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;It outperforms Grok, Gemini, and GPT-Realtime with a 96.4% accuracy rate.&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Native Audio Reasoning (End-to-End)&lt;/li&gt; &lt;li&gt;Audio-native CoT (Chain of Thought)&lt;/li&gt; &lt;li&gt;Real-time streaming inference&lt;/li&gt; &lt;li&gt;FULLY OPEN SOURCE&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wln8b464sgdg1.png?width=6507&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc83bc19c38b1c973fe264d3f32ca1b0ee860fbc"&gt;https://preview.redd.it/wln8b464sgdg1.png?width=6507&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc83bc19c38b1c973fe264d3f32ca1b0ee860fbc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nxnh1w35sgdg1.png?width=3960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=303afc1cca072ad309af2b75944f675d033da76c"&gt;https://preview.redd.it/nxnh1w35sgdg1.png?width=3960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=303afc1cca072ad309af2b75944f675d033da76c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/14mu93p5sgdg1.png?width=6008&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=403110a80e8d15fc1e4a48362ab28c34f6a42042"&gt;https://preview.redd.it/14mu93p5sgdg1.png?width=6008&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=403110a80e8d15fc1e4a48362ab28c34f6a42042&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Sea8804"&gt; /u/Inevitable_Sea8804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdd1l7/stepaudior11_open_weight_by_stepfun_just_set_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdd1l7/stepaudior11_open_weight_by_stepfun_just_set_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdd1l7/stepaudior11_open_weight_by_stepfun_just_set_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T07:20:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdrxiu</id>
    <title>OpenAI has signed a $10 billion contract with Cerebras</title>
    <updated>2026-01-15T18:42:39+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://en.ain.ua/2026/01/15/openai-has-signed-a-10-billion-contract-with-cerebras/"&gt;https://en.ain.ua/2026/01/15/openai-has-signed-a-10-billion-contract-with-cerebras/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few days ago, I read some comments about this hypothetical wedding and why it wasn't happening. And yet, it happened!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrxiu/openai_has_signed_a_10_billion_contract_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrxiu/openai_has_signed_a_10_billion_contract_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrxiu/openai_has_signed_a_10_billion_contract_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T18:42:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcusnt</id>
    <title>Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M</title>
    <updated>2026-01-14T18:16:00+00:00</updated>
    <author>
      <name>/u/eugenekwek</name>
      <uri>https://old.reddit.com/user/eugenekwek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/"&gt; &lt;img alt="Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M" src="https://external-preview.redd.it/NXZ5NDNuYTlzY2RnMX4ZwK1s5ENYxRsvoiSEu3mA0RmAAs2-sAvwRMu-2CtN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2a20f3c8df9af0a0fbced04bbc8dc6ec0450abe" title="Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;Today, I am announcing Soprano 1.1! I‚Äôve designed it for massively improved stability and audio quality over the original model. &lt;/p&gt; &lt;p&gt;While many of you were happy with the quality of Soprano, it had a tendency to start, well, &lt;em&gt;Mongolian throat singing&lt;/em&gt;. Contrary to its name, Soprano is &lt;strong&gt;NOT&lt;/strong&gt; supposed to be for singing, so I have reduced the frequency of these hallucinations by &lt;strong&gt;95%&lt;/strong&gt;. Soprano 1.1-80M also has a &lt;strong&gt;50%&lt;/strong&gt; lower WER than Soprano-80M, with comparable clarity to much larger models like Chatterbox-Turbo and VibeVoice. In addition, it now supports sentences up to &lt;strong&gt;30 seconds&lt;/strong&gt; long, up from 15.&lt;/p&gt; &lt;p&gt;The outputs of Soprano could sometimes have a lot of artifacting and high-frequency noise. This was because the model was severely undertrained. I have trained Soprano further to reduce these audio artifacts.&lt;/p&gt; &lt;p&gt;According to a blind study I conducted on my family (against their will), they preferred Soprano 1.1's outputs &lt;strong&gt;63%&lt;/strong&gt; of the time, so these changes have produced a noticeably improved model.&lt;/p&gt; &lt;p&gt;You can check out the new Soprano here:&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/ekwek/Soprano-1.1-80M"&gt;https://huggingface.co/ekwek/Soprano-1.1-80M&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Try Soprano 1.1 Now: &lt;a href="https://huggingface.co/spaces/ekwek/Soprano-TTS"&gt;https://huggingface.co/spaces/ekwek/Soprano-TTS&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/ekwek1/soprano"&gt;https://github.com/ekwek1/soprano&lt;/a&gt; &lt;/p&gt; &lt;p&gt;- Eugene&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eugenekwek"&gt; /u/eugenekwek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/v0c2rda9scdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:16:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdhkxi</id>
    <title>Finally finished my all-in-one Local AI app (Flux, Music, Agent)</title>
    <updated>2026-01-15T11:56:28+00:00</updated>
    <author>
      <name>/u/Motor-Resort-5314</name>
      <uri>https://old.reddit.com/user/Motor-Resort-5314</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdhkxi/finally_finished_my_allinone_local_ai_app_flux/"&gt; &lt;img alt="Finally finished my all-in-one Local AI app (Flux, Music, Agent)" src="https://b.thumbs.redditmedia.com/chF0aujED-GJdFaDbaGz9R-hJ4LMnx1FQUvAHMrsGCA.jpg" title="Finally finished my all-in-one Local AI app (Flux, Music, Agent)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Finally finished my all-in-one Local AI app (Flux, Music, Agent)&lt;/h1&gt; &lt;p&gt;Just wanted to show off what I‚Äôve been building for the last few months.&lt;/p&gt; &lt;p&gt;It‚Äôs called &lt;strong&gt;V6rge&lt;/strong&gt;. Basically, I got tired of dealing with 10 different command-line windows just to run Flux, a Chatbot, and some standard tools. So I built a single, unified desktop app for all of them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does :&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local Mode:&lt;/strong&gt; An agent that can actually control your PC by instructing it .&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image Gen:&lt;/strong&gt; Flux.1 &amp;amp; Qwen-Image (no subscriptions, just your GPU).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Music:&lt;/strong&gt; Generates tracks with MusicGen.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Video:&lt;/strong&gt; HunyuanVideo support.&lt;/li&gt; &lt;li&gt;Vocal Remover&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Update (v0.1.5):&lt;/strong&gt; I posted this a while ago and the installer was... kinda buggy üòÖ. I spent the last week rewriting the backend extraction logic. &lt;strong&gt;v0.1.5 is live now&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt; &lt;a href="https://github.com/Dedsec-b/v6rge-releases-/releases/tag/v0.1.5"&gt;https://github.com/Dedsec-b/v6rge-releases-/releases/tag/v0.1.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if it breaks (but it shouldn't this time lol).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9bg618685idg1.png?width=1343&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=88428534e918dafdea84bc1de329f90e36494700"&gt;https://preview.redd.it/9bg618685idg1.png?width=1343&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=88428534e918dafdea84bc1de329f90e36494700&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3askgei85idg1.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=911bde2512c5a5f5da08c7fa48fe494092d67921"&gt;https://preview.redd.it/3askgei85idg1.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=911bde2512c5a5f5da08c7fa48fe494092d67921&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s1jk3l695idg1.png?width=1353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35eb437bb288447954826a3a474462547f109066"&gt;https://preview.redd.it/s1jk3l695idg1.png?width=1353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35eb437bb288447954826a3a474462547f109066&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/koeschr95idg1.png?width=1365&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=703aebfc380ef332d73589ee99e37d42461583f1"&gt;https://preview.redd.it/koeschr95idg1.png?width=1365&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=703aebfc380ef332d73589ee99e37d42461583f1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Motor-Resort-5314"&gt; /u/Motor-Resort-5314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdhkxi/finally_finished_my_allinone_local_ai_app_flux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdhkxi/finally_finished_my_allinone_local_ai_app_flux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdhkxi/finally_finished_my_allinone_local_ai_app_flux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T11:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdroja</id>
    <title>Framework Desktop vs. 5090 for code analysis</title>
    <updated>2026-01-15T18:33:28+00:00</updated>
    <author>
      <name>/u/Albedo101</name>
      <uri>https://old.reddit.com/user/Albedo101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need opinions on what hardware to get, between Framework Desktop (AMD Stryx Halo 128GB unified RAM) and self-built PC with Nvidia 5090 32GB VRAM.&lt;/p&gt; &lt;p&gt;The use case is somewhat peculiar. I will be working with still copyrighted vintage code, mostly for early x86 PC but some of it for other 80s/90s platforms. Mostly in C89 and some of it in 8086 and 68k assembly. I'm far from an expert in this and I will be working alone. I need an AI assistant for code analysis and expediting the learning process.&lt;/p&gt; &lt;p&gt;I am really not sure how to approach this. I have no experience with local models and don't know what to expect from either option. My worries are that AMD will be slow and 32gb in 5090 might not be enough. In theory, slow is better that nothing, I guess. As long as it's not unbearably slow. The price, form factor and cost of operating are also leaning in AMD's favor. But in any case, I don't want to spent thousands for a doorstop if it can't do the job. Anybody who has experience with this, is most welcome to express their opinion.&lt;/p&gt; &lt;p&gt;I'm not even sure if LLMs are even capable of handling this somewhat obscure code base. But what I have tested with ChatGPT and Claude Code free models handle vintage C and assembly pretty well. But those are commercial cloud solutions, so yeah.... &lt;/p&gt; &lt;p&gt;I am also open to suggestions on which local LLM is the most suitable for this kind of work. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Albedo101"&gt; /u/Albedo101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdroja/framework_desktop_vs_5090_for_code_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdroja/framework_desktop_vs_5090_for_code_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdroja/framework_desktop_vs_5090_for_code_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T18:33:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdpiw8</id>
    <title>I built agent-of-empires: cli session manager to manage all your local LLM coding agents (opencode)</title>
    <updated>2026-01-15T17:17:45+00:00</updated>
    <author>
      <name>/u/river_otter412</name>
      <uri>https://old.reddit.com/user/river_otter412</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdpiw8/i_built_agentofempires_cli_session_manager_to/"&gt; &lt;img alt="I built agent-of-empires: cli session manager to manage all your local LLM coding agents (opencode)" src="https://external-preview.redd.it/M3l2NGppaHJxamRnMfWVxy7rvYMCqiqCOPfcqx6hDwGA71STZapyDGhDWUDn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d8a6973d2be511eeebc577daa3407982d61c1c7e" title="I built agent-of-empires: cli session manager to manage all your local LLM coding agents (opencode)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! My name's Nathan, I'm an MLE at mozilla.ai. &lt;/p&gt; &lt;p&gt;I'm loving my LM Studio LLMs (nemotron, qwen3-coder, gpt-oss) running on a mac mini, and I wanted to give them a try at coding. Unfortunately I'm impatient and since they can run a little slower than the LLMs hosted on the expensive NVIDIA gpus, I found myself opening up a ton of terminal windows to try to do stuff while I waited. I started spending a lot of time toggling between windows to try to figure out which ones were waiting on me vs sitting idle. &lt;/p&gt; &lt;p&gt;So, I built a solution! Agent of Empires (aoe) is terminal session manager that manages your agents with tmux and gives you a TUI dashboard that shows session status at a glance.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Status monitoring - See Running/Waiting/Idle state for all sessions without attaching&lt;/li&gt; &lt;li&gt;Persistent sessions - Sessions survive terminal closure; your agent keeps working&lt;/li&gt; &lt;li&gt;Multiple parallel sessions - Run several agents across projects while you work elsewhere&lt;/li&gt; &lt;li&gt;Git worktree integration - Spin up agents on different branches simultaneously&lt;/li&gt; &lt;li&gt;Docker sandboxing - Isolate agent execution for safety&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Links&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/njbrake/agent-of-empires"&gt;https://github.com/njbrake/agent-of-empires&lt;/a&gt;&lt;/li&gt; &lt;li&gt;MIT licensed, Rust, Linux/macOS&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;install via `brew install njbrake/aoe/aoe` or check out the github repo for the bash script for linux/WSL.&lt;/p&gt; &lt;p&gt;Happy to hear any thoughts about missing features or how it's working for you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/river_otter412"&gt; /u/river_otter412 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7xv99jhrqjdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdpiw8/i_built_agentofempires_cli_session_manager_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdpiw8/i_built_agentofempires_cli_session_manager_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T17:17:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdpjzc</id>
    <title>How to counter Qwen3 VL Thinking emerging catchphrases?</title>
    <updated>2026-01-15T17:18:49+00:00</updated>
    <author>
      <name>/u/IrisColt</name>
      <uri>https://old.reddit.com/user/IrisColt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most people agree that Qwen3 VL Thinking is currently the best dense model under 32B parameters. That said, Qwen3 VL has some quirks that are driving me crazy.&lt;/p&gt; &lt;p&gt;I've noticed a weird pattern that shows up consistently in longer conversations (over 5 turns). It's a type of repetition, but not the straightforward kind that repetition or frequency penalties can fix.&lt;/p&gt; &lt;p&gt;Here's what happens: As the chat goes on, Qwen3 starts ending its responses (not the thinking block) with what becomes essentially a signature catchphrase. This isn't typical AI slop, it's more like an &amp;quot;emerging&amp;quot; tagline... always different. Once the model locks onto a phrase like &amp;quot;Now what?&amp;quot;, it becomes almost impossible to break the pattern without addressing it in the chat. Even worse, it starts standardizing the structure leading up to that catchphrase. Each response becomes a template where it just swaps out variables... like using &amp;quot;Now let's talk about X&amp;quot; over and over, just changing what X is. &lt;/p&gt; &lt;p&gt;The thinking block stays sharp, but it increasingly gets boxed into formatting each answer the same way, and there's a growing, though subtle, disconnect between what it's thinking and what it actually outputs.&lt;/p&gt; &lt;p&gt;Has anyone else run into this? What's the best way to deal with it? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IrisColt"&gt; /u/IrisColt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdpjzc/how_to_counter_qwen3_vl_thinking_emerging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdpjzc/how_to_counter_qwen3_vl_thinking_emerging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdpjzc/how_to_counter_qwen3_vl_thinking_emerging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T17:18:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qd92pm</id>
    <title>stepfun-ai/Step3-VL-10B ¬∑ Hugging Face</title>
    <updated>2026-01-15T03:51:48+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd92pm/stepfunaistep3vl10b_hugging_face/"&gt; &lt;img alt="stepfun-ai/Step3-VL-10B ¬∑ Hugging Face" src="https://preview.redd.it/88t4oaa3rfdg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88fb05f58d5a4493756c36b2bd8d44c42fe3366b" title="stepfun-ai/Step3-VL-10B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step3-VL-10B"&gt;stepfun-ai/Step3-VL-10B ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/88t4oaa3rfdg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd92pm/stepfunaistep3vl10b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qd92pm/stepfunaistep3vl10b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T03:51:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcuerc</id>
    <title>NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency</title>
    <updated>2026-01-14T18:02:19+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve seen some arguments we‚Äôve reached AGI, it‚Äôs just about putting the separate pieces together in the right context. I think having a relatively small model that knows how to connect with other tools and models is exactly the correct route towards very functional systems. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:02:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdax6z</id>
    <title>LFM 2.5 is insanely good</title>
    <updated>2026-01-15T05:22:51+00:00</updated>
    <author>
      <name>/u/guiopen</name>
      <uri>https://old.reddit.com/user/guiopen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's the first model at ~1b that I find not just useful, but altright good and comparable to models 3x larger&lt;/p&gt; &lt;p&gt;Everytime a ultra small model launches with impressive benchmark numbers , it's always the same thing: infinite loops, breaking in multi turn conversations, doesn't know basic facts like the size of an elephant, etc etc... And it is very good at my native language (Portuguese) despite it not being officially supported&lt;/p&gt; &lt;p&gt;But this is different, the benchmarks seem to reflect it's performance really well, and it feels somewhere in between llama 2 7b and llama 3 8b&lt;/p&gt; &lt;p&gt;You should try it. I am running at Q6 and having excelent results for simple tasks like basic QA and summarization.&lt;/p&gt; &lt;p&gt;The jump from lfm2 makes me excited about the 8b-a1b moe model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guiopen"&gt; /u/guiopen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdax6z/lfm_25_is_insanely_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdax6z/lfm_25_is_insanely_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdax6z/lfm_25_is_insanely_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T05:22:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdkqgd</id>
    <title>I've been working on yet another GGUF converter (YaGUFF). It is a GUI on top of llama.cpp (isn't everything?).</title>
    <updated>2026-01-15T14:18:51+00:00</updated>
    <author>
      <name>/u/AllergicToTeeth</name>
      <uri>https://old.reddit.com/user/AllergicToTeeth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdkqgd/ive_been_working_on_yet_another_gguf_converter/"&gt; &lt;img alt="I've been working on yet another GGUF converter (YaGUFF). It is a GUI on top of llama.cpp (isn't everything?)." src="https://preview.redd.it/qbt8bfuh9idg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84e565840ad80ad7d1a4fa5d61ba9da5057dbda1" title="I've been working on yet another GGUF converter (YaGUFF). It is a GUI on top of llama.cpp (isn't everything?)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My goals here were self-educational so I'm curious to see how it survives contact with the outside world. It's supposed to be simple and easy. After weeks of adding features and changing everything I can't be sure. With some luck it should still be intuitive enough. &lt;/p&gt; &lt;p&gt;Installation should be as easy as a git clone and then running the appropriate run_gui script for your system. Let me know how it goes!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/usrname0/YaGGUF"&gt;https://github.com/usrname0/YaGGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AllergicToTeeth"&gt; /u/AllergicToTeeth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qbt8bfuh9idg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdkqgd/ive_been_working_on_yet_another_gguf_converter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdkqgd/ive_been_working_on_yet_another_gguf_converter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T14:18:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdsnul</id>
    <title>translategemma 27b/12b/4b</title>
    <updated>2026-01-15T19:08:59+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdsnul/translategemma_27b12b4b/"&gt; &lt;img alt="translategemma 27b/12b/4b" src="https://b.thumbs.redditmedia.com/ToCVsy0EUgPOXoYv6voopJf0tziQjvqiUpvpgAINVNE.jpg" title="translategemma 27b/12b/4b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TranslateGemma&lt;/strong&gt; is a family of lightweight, state-of-the-art open translation models from Google, based on the &lt;strong&gt;Gemma 3&lt;/strong&gt; family of models.&lt;/p&gt; &lt;p&gt;TranslateGemma models are designed to handle translation tasks across &lt;strong&gt;55 languages&lt;/strong&gt;. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art translation models and helping foster innovation for everyone.&lt;/p&gt; &lt;h1&gt;Inputs and outputs&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Input:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Text string, representing the text to be translated&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Images,&lt;/strong&gt; normalized to 896 x 896 resolution and encoded to 256 tokens each&lt;/li&gt; &lt;li&gt;Total input context of 2K tokens&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Text translated into the target language&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/translategemma-27b-it"&gt;https://huggingface.co/google/translategemma-27b-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/translategemma-12b-it"&gt;https://huggingface.co/google/translategemma-12b-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/translategemma-4b-it"&gt;https://huggingface.co/google/translategemma-4b-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aza4kprrakdg1.png?width=1372&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bed28fac0a9878478a7cec3f0eac6c1c585b8a85"&gt;https://preview.redd.it/aza4kprrakdg1.png?width=1372&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bed28fac0a9878478a7cec3f0eac6c1c585b8a85&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdsnul/translategemma_27b12b4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdsnul/translategemma_27b12b4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdsnul/translategemma_27b12b4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T19:08:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qd88v2</id>
    <title>I trained a model to 'unslop' AI prose</title>
    <updated>2026-01-15T03:12:29+00:00</updated>
    <author>
      <name>/u/N8Karma</name>
      <uri>https://old.reddit.com/user/N8Karma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd88v2/i_trained_a_model_to_unslop_ai_prose/"&gt; &lt;img alt="I trained a model to 'unslop' AI prose" src="https://b.thumbs.redditmedia.com/9LRUV10qj7Dvr1f-NPfxLbXNLOlOCDxfXyYG6Tn3zms.jpg" title="I trained a model to 'unslop' AI prose" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran passages from Project Gutenberg through GPT-4o-mini 10 times over, each time telling it to &amp;quot;make it read far better, adding superior prose, etc.&amp;quot;. This lead to classic literary passages being enslopped. I then reversed this pipeline, and trained a model to go from [slop] -&amp;gt; [original]. The resulting model is capable enough to fool Pangram (a fairly robust AI detector - I take this as a metric of how 'human-sounding' the output is), at very little overall quality cost:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/go88234vifdg1.png?width=2817&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fed2c84e748f4441648e9f53c891258d78ccbb0a"&gt;While quality decreases slightly, humanness jumps from 0 to 0.481. The unslopped version stays firmly above Mistral Large 3 and close to the original GPT-5.2 baseline.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Of course, the model is OSS: &lt;a href="https://huggingface.co/N8Programs/Unslopper-30B-A3B-bf16"&gt;https://huggingface.co/N8Programs/Unslopper-30B-A3B-bf16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And there are now GGUFs: &lt;a href="https://huggingface.co/N8Programs/Unslopper-GGUF"&gt;https://huggingface.co/N8Programs/Unslopper-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And there is a twitter thread (complete with 'thread below üëá', ironic in a thread about slop): &lt;a href="https://x.com/N8Programs/status/2011591738591494625"&gt;https://x.com/N8Programs/status/2011591738591494625&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The goal here is &lt;strong&gt;not&lt;/strong&gt; to fool Pangram, deceive/cheat, etc. I only use Pangram as a proxy for the prose being more readable - ideally, you'd use this model to make your own AI-generated passages more palatable to read, or as part of a larger pipeline in automated writing generation for training, for instance.&lt;/p&gt; &lt;p&gt;Example (GPT-5.2):&lt;br /&gt; The wind rose all at once, as if the mountain itself had inhaled.&lt;/p&gt; &lt;p&gt;She stood at the edge of the pass, cloak snapping like a torn banner behind her, staring down at the valley where the last lights were going out one by one. Each flame extinguished felt deliberate‚Äîan answer to a question she had been too afraid to ask. Somewhere below, bells began to ring, not in warning, but in mourning. They already knew.&lt;/p&gt; &lt;p&gt;‚ÄúDon‚Äôt,‚Äù he said, breathless, reaching for her arm. His fingers brushed fabric, then air. ‚ÄúIf you cross that line, there‚Äôs no coming back.‚Äù&lt;/p&gt; &lt;p&gt;She didn‚Äôt turn. The cold burned her lungs, sharpened her thoughts. ‚ÄúThere was never a way back,‚Äù she replied softly. ‚ÄúOnly a way forward you were willing to pretend didn‚Äôt exist.‚Äù&lt;/p&gt; &lt;p&gt;Thunder rolled, low and ancient. For a moment, the sky fractured into white fire, and in that instant he saw her face‚Äîresolved, terrified, incandescent with purpose. Not the girl he had sworn to protect, but the woman the world had demanded she become.&lt;/p&gt; &lt;p&gt;‚ÄúYou‚Äôll die,‚Äù he said.&lt;/p&gt; &lt;p&gt;She smiled then, finally facing him, eyes bright with unshed tears. ‚ÄúSo will everyone,‚Äù she answered. ‚ÄúThe difference is what we leave burning when we‚Äôre gone.‚Äù&lt;/p&gt; &lt;p&gt;She stepped forward.&lt;/p&gt; &lt;p&gt;The mountain exhaled.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pangram Verdict:&lt;/strong&gt; High Confidence AI Generated&lt;/p&gt; &lt;p&gt;Unslopped (Temp=0.8, Rep Penalty=1.1, 6-bit MLX quant):&lt;br /&gt; And then it just happened. Like the whole of reality had been held breathless for a moment, and suddenly sprang into focus.&lt;/p&gt; &lt;p&gt;She stood at the pass's edge. Her cloak flapped behind her like a banner carried to its last stop. She was looking down into the valley, at lights being snuffed out one by one. Each flame extinguished was a point of answer to questions she had dared not ask. And now bells were ringing, not a signal but a dirge, because they already knew.&lt;/p&gt; &lt;p&gt;&amp;quot;Please,&amp;quot; he said. He reached for her; his hand closed on the snaking line of fabric that trailed behind her, then nothing. &amp;quot;You can't cross over.&amp;quot;&lt;/p&gt; &lt;p&gt;She didn't turn to him. It was cold enough to bite at her throat; she breathed it in, sharp and full of poison. &amp;quot;There was never a way out,&amp;quot; she said softly. &amp;quot;Only the road in that you did not want to remember.&amp;quot;&lt;/p&gt; &lt;p&gt;A low rumble of thunder, older than mountains, filled the air. A white flash lit up the sky for a moment, a vivid static song of fire that illuminated her face in an instant. It was no longer the girl he'd sworn to protect, but the woman she had become.&lt;/p&gt; &lt;p&gt;&amp;quot;You'll die.&amp;quot;&lt;/p&gt; &lt;p&gt;She smiled at him and turned toward him. There were tears on her face, but they did not fall. &amp;quot;We all do,&amp;quot; she said. &amp;quot;The difference is what we leave behind us.&amp;quot;&lt;/p&gt; &lt;p&gt;She started walking toward the edge.&lt;/p&gt; &lt;p&gt;And it all happened at once. The mountain exhaled itself, and took her with it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pangram Verdict:&lt;/strong&gt; High Confidence Human Written&lt;/p&gt; &lt;p&gt;Note that there are some local coherence issues w/ the Unslopper - that's why I'd recommend integrating it into a larger pipeline or editing its output yourself. It's definitely not production ready.&lt;/p&gt; &lt;p&gt;---------&lt;/p&gt; &lt;p&gt;As a bonus, the training of this model was entirely local! Done on one M3 Max w/ mlx-lm. Took 12 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/N8Karma"&gt; /u/N8Karma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd88v2/i_trained_a_model_to_unslop_ai_prose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd88v2/i_trained_a_model_to_unslop_ai_prose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qd88v2/i_trained_a_model_to_unslop_ai_prose/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T03:12:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdgeak</id>
    <title>MiniMax-M2.1 REAP models (0xSero) are fixed!</title>
    <updated>2026-01-15T10:48:34+00:00</updated>
    <author>
      <name>/u/AdamDhahabi</name>
      <uri>https://old.reddit.com/user/AdamDhahabi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Previously, some experts where mistakenly left out and that caused loops, new GGUF uploads happening right now.&lt;br /&gt; - REAP-20 Deprecated&lt;br /&gt; - REAP-30 &lt;strong&gt;Fixed&lt;/strong&gt;&lt;br /&gt; - REAP-40 &lt;strong&gt;Fixed&lt;/strong&gt;&lt;br /&gt; - REAP-50 Deprecated&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/MiniMax-M2.1-REAP-30-GGUF"&gt;https://huggingface.co/mradermacher/MiniMax-M2.1-REAP-30-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/MiniMax-M2.1-REAP-40-GGUF"&gt;https://huggingface.co/mradermacher/MiniMax-M2.1-REAP-40-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdamDhahabi"&gt; /u/AdamDhahabi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdgeak/minimaxm21_reap_models_0xsero_are_fixed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdgeak/minimaxm21_reap_models_0xsero_are_fixed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdgeak/minimaxm21_reap_models_0xsero_are_fixed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T10:48:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdbxei</id>
    <title>Mistral releases Ministral 3 paper</title>
    <updated>2026-01-15T06:16:31+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;details: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2601.08584"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdbxei/mistral_releases_ministral_3_paper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdbxei/mistral_releases_ministral_3_paper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T06:16:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qd6nho</id>
    <title>Zhipu AI breaks US chip reliance with first major model trained on Huawei stack (GLM-Image)</title>
    <updated>2026-01-15T02:01:03+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd6nho/zhipu_ai_breaks_us_chip_reliance_with_first_major/"&gt; &lt;img alt="Zhipu AI breaks US chip reliance with first major model trained on Huawei stack (GLM-Image)" src="https://external-preview.redd.it/67JUXSnUreB8wTlODdM32UrKgKSfJgeIROoAbEyBScs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d29623e3dc2c928508ca7ce7d10f296f2a1d15a" title="Zhipu AI breaks US chip reliance with first major model trained on Huawei stack (GLM-Image)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.scmp.com/tech/tech-war/article/3339869/zhipu-ai-breaks-us-chip-reliance-first-major-model-trained-huawei-stack"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qd6nho/zhipu_ai_breaks_us_chip_reliance_with_first_major/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qd6nho/zhipu_ai_breaks_us_chip_reliance_with_first_major/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T02:01:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdpjnh</id>
    <title>modelgrep: open source project to help you discover the best local model providers</title>
    <updated>2026-01-15T17:18:29+00:00</updated>
    <author>
      <name>/u/Turbulent-Sky5396</name>
      <uri>https://old.reddit.com/user/Turbulent-Sky5396</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;technically not local models in the sense that you run it on your computer, but its a nice way for you to find cheap hosted local models and providers &lt;/p&gt; &lt;p&gt;for context i use LLMs in my project a lot and OSS models are much cheaper/faster than big labs for a lot of my use-cases but its so hard to discover whats available, at what cost, quant, throughput etc so i built this using openrouter data&lt;/p&gt; &lt;p&gt;thought this might be useful for myself and other folk&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent-Sky5396"&gt; /u/Turbulent-Sky5396 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://modelgrep.com"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdpjnh/modelgrep_open_source_project_to_help_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdpjnh/modelgrep_open_source_project_to_help_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T17:18:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdl9za</id>
    <title>Falcon 90M</title>
    <updated>2026-01-15T14:40:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...it's not 90B it's 90M, so you can run it on anything :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-Tiny-90M-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-Tiny-90M-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-Tiny-Coder-90M-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-Tiny-Coder-90M-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-90M-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-90M-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-Tiny-Tool-Calling-90M-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-Tiny-Tool-Calling-90M-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdl9za/falcon_90m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdl9za/falcon_90m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdl9za/falcon_90m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T14:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdpb2v</id>
    <title>Thanks to you guys, Soprano TTS now supports OpenAI-compatible endpoint, ONNX, ComfyUI, WebUI, and CLI on CUDA, MPS, ROCm, and CPU!</title>
    <updated>2026-01-15T17:10:00+00:00</updated>
    <author>
      <name>/u/eugenekwek</name>
      <uri>https://old.reddit.com/user/eugenekwek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdpb2v/thanks_to_you_guys_soprano_tts_now_supports/"&gt; &lt;img alt="Thanks to you guys, Soprano TTS now supports OpenAI-compatible endpoint, ONNX, ComfyUI, WebUI, and CLI on CUDA, MPS, ROCm, and CPU!" src="https://preview.redd.it/581sfa20ojdg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3839305a28dd4d4bbbbd3af2e716c6b974829cd3" title="Thanks to you guys, Soprano TTS now supports OpenAI-compatible endpoint, ONNX, ComfyUI, WebUI, and CLI on CUDA, MPS, ROCm, and CPU!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ekwek1/soprano"&gt;https://github.com/ekwek1/soprano&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ekwek/Soprano-1.1-80M"&gt;https://huggingface.co/ekwek/Soprano-1.1-80M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/ekwek/Soprano-TTS"&gt;https://huggingface.co/spaces/ekwek/Soprano-TTS&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;This final day of updates is dedicated to all of you. When I first released Soprano, I had no idea how much support I would get from the community. Within the first day, I received an enormous number PRs adding onto the codebase. I have finally merged most of them, and am happy to announce that you can now run Soprano on nearly any device, and with a wide number of supported inference methods.&lt;/p&gt; &lt;p&gt;Here is a list of all the contributions you guys have made:&lt;/p&gt; &lt;p&gt;WebUI: (from Mateusz-Dera &amp;amp; humair-m)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;soprano-webui &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;CLI: (from bigattichouse)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;soprano &amp;quot;Hello world!&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;OpenAI-compatible endpoint (from bezo97)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uvicorn soprano.server:app &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In addition, several of you have made your own modifications to Soprano, allowing for ONNX and ComfyUI support! Here are some repos that implement this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SanDiegoDude/ComfyUI-Soprano-TTS"&gt;https://github.com/SanDiegoDude/ComfyUI-Soprano-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jo-nike/ComfyUI-SopranoTTS"&gt;https://github.com/jo-nike/ComfyUI-SopranoTTS&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/KevinAHM/soprano-web-onnx"&gt;https://github.com/KevinAHM/soprano-web-onnx&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Soprano also supports more than just CUDA devices now, too! It also supports CPU (from bigattichouse), MPS (from visionik), and there is an ROCm PR (from Mateusz-Dera) that can be found here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ekwek1/soprano/pull/29"&gt;https://github.com/ekwek1/soprano/pull/29&lt;/a&gt; &lt;/p&gt; &lt;p&gt;If you have an ROCm device I would love some help for testing this PR!&lt;/p&gt; &lt;p&gt;Finally, I want to thank the countless other contributions to Soprano, including an automatic hallucination detector from ChangeTheConstants and transformers streaming support from sheerun. You all have improved Soprano tremendously!&lt;/p&gt; &lt;p&gt;This will likely be my last update for a bit, since I still have some unfinished business left on the roadmap that will take some time. I‚Äôm not abandoning you guys though! New capabilities for Soprano will be coming soon. :)&lt;/p&gt; &lt;p&gt;- Eugene&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eugenekwek"&gt; /u/eugenekwek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/581sfa20ojdg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdpb2v/thanks_to_you_guys_soprano_tts_now_supports/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdpb2v/thanks_to_you_guys_soprano_tts_now_supports/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T17:10:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdrf3o</id>
    <title>Nemotron-3-nano:30b is a spectacular general purpose local LLM</title>
    <updated>2026-01-15T18:24:08+00:00</updated>
    <author>
      <name>/u/DrewGrgich</name>
      <uri>https://old.reddit.com/user/DrewGrgich</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just want to sing the praises of this model. I am stunned at how intelligent it is for a 30b model. Comparing it to Llama 3.3:70b, I have yet to find a general purpose question that Nemotron hasn't answered better. It is quite robotic so I won't be using it for creative or chat purposes. Everything else though has been stellar. &lt;/p&gt; &lt;p&gt;If you have the capacity to give it a try, I highly recommend it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrewGrgich"&gt; /u/DrewGrgich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrf3o/nemotron3nano30b_is_a_spectacular_general_purpose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrf3o/nemotron3nano30b_is_a_spectacular_general_purpose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrf3o/nemotron3nano30b_is_a_spectacular_general_purpose/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T18:24:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdok2i</id>
    <title>google/translategemma</title>
    <updated>2026-01-15T16:42:58+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/google/translategemma"&gt;https://huggingface.co/collections/google/translategemma&lt;/a&gt;&lt;/p&gt; &lt;p&gt;tech report: &lt;a href="https://arxiv.org/abs/2601.09012"&gt;https://arxiv.org/abs/2601.09012&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdok2i/googletranslategemma/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdok2i/googletranslategemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdok2i/googletranslategemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T16:42:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdh28f</id>
    <title>RTX 5070 Ti and RTX 5060 Ti 16 GB no longer manufactured</title>
    <updated>2026-01-15T11:27:15+00:00</updated>
    <author>
      <name>/u/Paramecium_caudatum_</name>
      <uri>https://old.reddit.com/user/Paramecium_caudatum_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nvidia has essentially killed off supply for the RTX 5070 Ti. Also supply of RTX 5060 Ti 16 GB has been significantly reduced. This happened partially due to memory supply shortages. This means that most AIBs will no longer manufacture these GPUs. Prices are already jumping significantly. The 5070 Ti has risen ~$100 over MSRP, and retailers expect further hikes. 8 GB configuration of RTX 5060 Ti remains unaffected. &lt;/p&gt; &lt;p&gt;Credit: Hardware Unboxed &lt;/p&gt; &lt;p&gt;&lt;a href="https://m.youtube.com/watch?v=yteN21aJEvE"&gt;https://m.youtube.com/watch?v=yteN21aJEvE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paramecium_caudatum_"&gt; /u/Paramecium_caudatum_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T11:27:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdna3t</id>
    <title>7x Longer Context Reinforcement Learning in Unsloth</title>
    <updated>2026-01-15T15:56:40+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdna3t/7x_longer_context_reinforcement_learning_in/"&gt; &lt;img alt="7x Longer Context Reinforcement Learning in Unsloth" src="https://preview.redd.it/nmkee12vbjdg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cd97dbf853be6596556f70c467d1dccc0cc22a1" title="7x Longer Context Reinforcement Learning in Unsloth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We're excited to show how Unsloth now enables &lt;strong&gt;7x longer context lengths&lt;/strong&gt; (up to 12x) for Reinforcement Learning! By using 3 new techniques we developed, we enable you to train gpt-oss 20b QLoRA up to &lt;strong&gt;20K context on a 24Gb card&lt;/strong&gt; - all with &lt;strong&gt;no accuracy degradation&lt;/strong&gt;. Unsloth GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For larger GPUs, Unsloth now trains gpt-oss QLoRA with &lt;strong&gt;380K context&lt;/strong&gt; on a single 192GB NVIDIA B200 GPU&lt;/li&gt; &lt;li&gt;Qwen3-8B GRPO reaches &lt;strong&gt;110K context&lt;/strong&gt; on an 80GB VRAM H100 via vLLM and QLoRA, and &lt;strong&gt;65K&lt;/strong&gt; for gpt-oss with BF16 LoRA.&lt;/li&gt; &lt;li&gt;Unsloth GRPO RL runs with Llama, Gemma &amp;amp; all models auto support longer contexts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also, all features in Unsloth can be combined together and work well together:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Unsloth's &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/memory-efficient-rl"&gt;weight-sharing&lt;/a&gt; feature with vLLM and our Standby Feature in &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/memory-efficient-rl"&gt;Memory Efficient RL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Unsloth's &lt;a href="https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune/long-context-gpt-oss-training"&gt;Flex Attention&lt;/a&gt; for long context gpt-oss and our &lt;a href="https://unsloth.ai/docs/new/500k-context-length-fine-tuning"&gt;500K Context Training&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Float8 training in &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/fp8-reinforcement-learning"&gt;FP8 RL&lt;/a&gt; and Unsloth's &lt;a href="https://unsloth.ai/blog/long-context"&gt;async gradient checkpointing&lt;/a&gt; and much more&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You can read our educational blogpost for detailed analysis, benchmarks and more: &lt;a href="https://unsloth.ai/docs/new/grpo-long-context"&gt;https://unsloth.ai/docs/new/grpo-long-context&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And you can of course train any model using our new features and kernels via our free fine-tuning notebooks: &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks"&gt;https://docs.unsloth.ai/get-started/unsloth-notebooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some free Colab notebooks below which has the 7x longer context support backed in:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B"&gt;gpt-oss-20b&lt;/a&gt;-GRPO.ipynb) GSPO Colab&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_VL_(8B"&gt;Qwen3-VL-8B&lt;/a&gt;-Vision-GRPO.ipynb) Vision RL&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_8B_FP8_GRPO.ipynb"&gt;Qwen3-8B - FP8&lt;/a&gt; L4 GPU&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;To update Unsloth to automatically make training faster, do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth_zoo &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And to enable GRPO runs in Unsloth, do&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import os os.environ[&amp;quot;UNSLOTH_VLLM_STANDBY&amp;quot;] = &amp;quot;1&amp;quot; # Standby = extra 30% context lengths! from unsloth import FastLanguageModel import torch max_seq_length = 20000 # Can increase for longer reasoning traces lora_rank = 32 # Larger rank = smarter, but slower model, tokenizer = FastLanguageModel.from_pretrained( model_name = &amp;quot;unsloth/Qwen3-4B-Base&amp;quot;, max_seq_length = max_seq_length, load_in_4bit = False, # False for LoRA 16bit fast_inference = True, # Enable vLLM fast inference max_lora_rank = lora_rank, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope you all have a great rest of the week and thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nmkee12vbjdg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdna3t/7x_longer_context_reinforcement_learning_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdna3t/7x_longer_context_reinforcement_learning_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T15:56:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
