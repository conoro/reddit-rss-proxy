<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-26T06:45:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p69bea</id>
    <title>GLiNER2: Unified Schema-Based Information Extraction</title>
    <updated>2025-11-25T10:43:25+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p69bea/gliner2_unified_schemabased_information_extraction/"&gt; &lt;img alt="GLiNER2: Unified Schema-Based Information Extraction" src="https://b.thumbs.redditmedia.com/jbxiujDqa50qKCnpeAe_kYWcLq_ws_q5F6bgTLhSOkM.jpg" title="GLiNER2: Unified Schema-Based Information Extraction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLiNER2 is an efficient, unified information extraction system that combines named entity recognition, text classification, and hierarchical structured data extraction into a single 205M-parameter model. Built on a pretrained transformer encoder architecture and trained on 254,334 examples of real and synthetic data, it achieves competitive performance with large language models while running efficiently on CPU hardware without requiring GPUs or external APIs.&lt;/p&gt; &lt;p&gt;The system uses a schema-based interface where users can define extraction tasks declaratively through simple Python API calls, supporting features like entity descriptions, multi-label classification, nested structures, and multi-task composition in a single forward pass.&lt;/p&gt; &lt;p&gt;Released as an open-source pip-installable library under Apache 2.0 license with pre-trained models on Hugging Face, GLiNER2 demonstrates strong zero-shot performance across benchmarks—achieving 0.72 average accuracy on classification tasks and 0.590 F1 on the CrossNER benchmark—while maintaining approximately 2.6× speedup over GPT-4o on CPU.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2507.18546"&gt;https://arxiv.org/abs/2507.18546&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code repo: &lt;a href="https://github.com/fastino-ai/GLiNER2"&gt;https://github.com/fastino-ai/GLiNER2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Install: &lt;a href="https://pypi.org/project/gliner2"&gt;https://pypi.org/project/gliner2&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p69bea"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p69bea/gliner2_unified_schemabased_information_extraction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p69bea/gliner2_unified_schemabased_information_extraction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T10:43:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6ru97</id>
    <title>Best local coding LLM for Rust?</title>
    <updated>2025-11-25T23:19:31+00:00</updated>
    <author>
      <name>/u/Spiritual_Tie_5574</name>
      <uri>https://old.reddit.com/user/Spiritual_Tie_5574</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt; &lt;p&gt;I’m looking for recommendations for the best local coding LLM specifically for Rust. &lt;/p&gt; &lt;p&gt;Which model (size/quantisation) are you running, on what hardware, and what sort of latency are you getting? &lt;/p&gt; &lt;p&gt;Any tips for prompting Rust-specific issues or patterns? &lt;/p&gt; &lt;p&gt;Also, any recommended editor integrations or workflows for Rust with a local LLM? &lt;/p&gt; &lt;p&gt;I’m happy to trade a bit of speed for noticeably better Rust quality, so if there’s a clear “this model is just better for Rust” option, I’d really like to hear about it. &lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual_Tie_5574"&gt; /u/Spiritual_Tie_5574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ru97/best_local_coding_llm_for_rust/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ru97/best_local_coding_llm_for_rust/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ru97/best_local_coding_llm_for_rust/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T23:19:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p61ch2</id>
    <title>NVIDIA RTX PRO 6000 Blackwell desktop GPU drops to $7,999</title>
    <updated>2025-11-25T02:56:35+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p61ch2/nvidia_rtx_pro_6000_blackwell_desktop_gpu_drops/"&gt; &lt;img alt="NVIDIA RTX PRO 6000 Blackwell desktop GPU drops to $7,999" src="https://external-preview.redd.it/YCPQesYDmOPQ_XkQN8p_ciK514B0FKoU6bNyhy9mcvg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc94dfb5840da6920f1ea749a1fc15f8c8d11b76" title="NVIDIA RTX PRO 6000 Blackwell desktop GPU drops to $7,999" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you guys think that a RTX Quadro 8000 situation could happen again?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-flagship-rtx-pro-6000-is-now-rtx-5080-cheaper-as-card-price-drops-to-7999"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p61ch2/nvidia_rtx_pro_6000_blackwell_desktop_gpu_drops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p61ch2/nvidia_rtx_pro_6000_blackwell_desktop_gpu_drops/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T02:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6zayh</id>
    <title>DGX spark for training</title>
    <updated>2025-11-26T05:16:11+00:00</updated>
    <author>
      <name>/u/WeatherZealousideal5</name>
      <uri>https://old.reddit.com/user/WeatherZealousideal5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I wanted to ask those of you who have the dgx spark, how does it perform compared to an rtx 3090? I'm currently using vast.ai to train LLMs with unsloth and TTS models with pytorch&lt;/p&gt; &lt;p&gt;I feel like having local hardware would make me more productive, but I'm not sure whether the dgx spark can match the performance of an rtx 3090 24GB in the cloud (which has actually been enough for me)&lt;/p&gt; &lt;p&gt;The benefits are that the dgx spark doesn’t use much electricity, it’s power efficient and it’s small so I could keep trainings running on it many days. The downside though is that in my country it costs around $5,000&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WeatherZealousideal5"&gt; /u/WeatherZealousideal5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6zayh/dgx_spark_for_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6zayh/dgx_spark_for_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6zayh/dgx_spark_for_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T05:16:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6zegt</id>
    <title>Help: Applio 3.5</title>
    <updated>2025-11-26T05:21:21+00:00</updated>
    <author>
      <name>/u/bangteen717</name>
      <uri>https://old.reddit.com/user/bangteen717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I need help with Applio voice training and inference.&lt;/p&gt; &lt;p&gt;We are trying to train a voice but when we do inference, the output is different for audio 1 and audio.&lt;/p&gt; &lt;p&gt;Voice Model - let's name it A&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The voice we trained is more on the normal speaking, narrating side. No high pitches on the audio.&lt;/li&gt; &lt;li&gt;Her voice sounds like around in her mid-20s.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Inference&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Converted audio 1 using voice model A &lt;ul&gt; &lt;li&gt;Sound not exactly as the voice model. Sounds a bit different, slightly robotic and grandma-ish. &lt;/li&gt; &lt;li&gt;The audio 1 is a voice recording of a male in conversational tone with parts that has high pitches.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Converted audio 2 using voice model A &lt;ul&gt; &lt;li&gt;Sounds exactly like the voice model. &lt;/li&gt; &lt;li&gt;The audio 2 is a voice recording of the same guy but this time, it is more on the reading side, no changes on the pitch.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Training&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We tried training with no custom pretrain and with custom pretrains (OV2, Titan, and Singer)&lt;/li&gt; &lt;li&gt;Total epochs were at 300. Maximum is 700.&lt;/li&gt; &lt;li&gt;Voice model A's audio file is 20 mins long&lt;/li&gt; &lt;li&gt;We also tried training voice model A with different sample rate - 32k and 40k&lt;/li&gt; &lt;li&gt;Cleaned the audio, remove background noises using DaVinci.&lt;/li&gt; &lt;li&gt;Used Tensor board to check the best epoch.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Question&lt;/p&gt; &lt;p&gt;Does this have to do with the tone or pitch or the style of the voice model and the audio we are trying to convert?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bangteen717"&gt; /u/bangteen717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6zegt/help_applio_35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6zegt/help_applio_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6zegt/help_applio_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T05:21:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6u40t</id>
    <title>4070 Super (12gb) vs 5070ti (16gb)</title>
    <updated>2025-11-26T01:00:12+00:00</updated>
    <author>
      <name>/u/rabbany05</name>
      <uri>https://old.reddit.com/user/rabbany05</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My friend is selling his ~1 year old 4070S for $600 cad. I was initially planning on buying the 5070ti which will cost me around ~$1200 cad.&lt;/p&gt; &lt;p&gt;Is the 4070S a good deal compared to the 5070ti, considering future proofing and being able to run decent model on the lesser 12gb VRAM?&lt;/p&gt; &lt;p&gt;I already have 9950x and 64gb RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rabbany05"&gt; /u/rabbany05 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6u40t/4070_super_12gb_vs_5070ti_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6u40t/4070_super_12gb_vs_5070ti_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6u40t/4070_super_12gb_vs_5070ti_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T01:00:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6nf1r</id>
    <title>How I replaced Gemini CLI &amp; Copilot with a local stack using Ollama, Continue.dev and MCP servers</title>
    <updated>2025-11-25T20:25:03+00:00</updated>
    <author>
      <name>/u/aaronsky</name>
      <uri>https://old.reddit.com/user/aaronsky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the last few weeks I’ve been trying to get off the treadmill of cloud AI assistants (Gemini CLI, Copilot, Claude-CLI, etc.) and move everything to a local stack.&lt;/p&gt; &lt;p&gt;Goals:&lt;/p&gt; &lt;p&gt;- Keep code on my machine&lt;/p&gt; &lt;p&gt;- Stop paying monthly for autocomplete&lt;/p&gt; &lt;p&gt;- Still get “assistant-level” help in the editor&lt;/p&gt; &lt;p&gt;The stack I ended up with:&lt;/p&gt; &lt;p&gt;- Ollama for local LLMs (Nemotron-9B, Qwen3-8B, etc.)&lt;/p&gt; &lt;p&gt;- &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; inside VS Code for chat + agents&lt;/p&gt; &lt;p&gt;- MCP servers (Filesystem, Git, Fetch, XRAY, SQLite, Snyk…) as tools&lt;/p&gt; &lt;p&gt;What it can do in practice:&lt;/p&gt; &lt;p&gt;- Web research from inside VS Code (Fetch)&lt;/p&gt; &lt;p&gt;- Multi-file refactors &amp;amp; impact analysis (Filesystem + XRAY)&lt;/p&gt; &lt;p&gt;- Commit/PR summaries and diff review (Git)&lt;/p&gt; &lt;p&gt;- Local DB queries (SQLite)&lt;/p&gt; &lt;p&gt;- Security / error triage (Snyk / Sentry)&lt;/p&gt; &lt;p&gt;I wrote everything up here, including:&lt;/p&gt; &lt;p&gt;- Real laptop specs (Win 11 + RTX 6650M, 8 GB VRAM)&lt;/p&gt; &lt;p&gt;- Model selection tips (GGUF → Ollama)&lt;/p&gt; &lt;p&gt;- Step-by-step setup&lt;/p&gt; &lt;p&gt;- Example “agent” workflows (PR triage bot, dep upgrader, docs bot, etc.)&lt;/p&gt; &lt;p&gt;Main article:&lt;/p&gt; &lt;p&gt;&lt;a href="https://aiandsons.com/blog/local-ai-stack-ollama-continue-mcp"&gt;https://aiandsons.com/blog/local-ai-stack-ollama-continue-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo with docs &amp;amp; config:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/aar0nsky/blog-post-local-agent-mcp"&gt;https://github.com/aar0nsky/blog-post-local-agent-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also cross-posted to Medium if that’s easier to read:&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@a.ankiel/ditch-the-monthly-fees-a-more-powerful-alternative-to-gemini-and-copilot-f4563f6530b7"&gt;https://medium.com/@a.ankiel/ditch-the-monthly-fees-a-more-powerful-alternative-to-gemini-and-copilot-f4563f6530b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious how other people are doing local-first dev assistants (what models + tools you’re using).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aaronsky"&gt; /u/aaronsky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6nf1r/how_i_replaced_gemini_cli_copilot_with_a_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6nf1r/how_i_replaced_gemini_cli_copilot_with_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6nf1r/how_i_replaced_gemini_cli_copilot_with_a_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T20:25:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5u44r</id>
    <title>That's why local models are better</title>
    <updated>2025-11-24T21:42:13+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5u44r/thats_why_local_models_are_better/"&gt; &lt;img alt="That's why local models are better" src="https://preview.redd.it/7s5e59vpy93g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91d4c29a99283e56fcfd8614cc10c6d72a0af91a" title="That's why local models are better" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That is why the local ones are better than the private ones in addition to this model is still expensive, I will be surprised when the US models reach an optimized price like those in China, the price reflects the optimization of the model, did you know ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7s5e59vpy93g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5u44r/thats_why_local_models_are_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5u44r/thats_why_local_models_are_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T21:42:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6mmb1</id>
    <title>Trying to build a "Jarvis" that never phones home - on-device AI with full access to your digital life (free beta, roast us)</title>
    <updated>2025-11-25T19:54:58+00:00</updated>
    <author>
      <name>/u/ipav9</name>
      <uri>https://old.reddit.com/user/ipav9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6mmb1/trying_to_build_a_jarvis_that_never_phones_home/"&gt; &lt;img alt="Trying to build a &amp;quot;Jarvis&amp;quot; that never phones home - on-device AI with full access to your digital life (free beta, roast us)" src="https://preview.redd.it/loj0n38hkg3g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34d3a49ae932dab6a03739299fbb51a8a2573cf6" title="Trying to build a &amp;quot;Jarvis&amp;quot; that never phones home - on-device AI with full access to your digital life (free beta, roast us)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/LocalLLaMA,&lt;/p&gt; &lt;p&gt;I know, I know - another &amp;quot;we built something&amp;quot; post. I'll be upfront: this is about something we made, so feel free to scroll past if that's not your thing. But if you're into local inference and privacy-first AI with a WhatsApp/Signal-grade E2E encryption flavor, maybe stick around for a sec.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who we are&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We're Ivan and Dan - two devs from London who've been boiling in the AI field for a while and got tired of the &amp;quot;trust us with your data&amp;quot; model that every AI company seems to push.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What we built and why&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We believe today's AI assistants are powerful but fundamentally disconnected from your actual life. Sure, you can feed ChatGPT a document or paste an email to get a smart-sounding reply. But that's not where AI gets truly useful. Real usefulness comes when AI has real-time access to your entire digital footprint - documents, notes, emails, calendar, photos, health data, maybe even your journal. That level of context is what makes AI actually proactive instead of just reactive.&lt;/p&gt; &lt;p&gt;But here's the hard sell: who's ready to hand all of that to OpenAI, Google, or Meta in one go? We weren't. So we built Atlantis - a two-app ecosystem (desktop + mobile) where all AI processing happens locally. No cloud calls, no &amp;quot;we promise we won't look at your data&amp;quot; - just on-device inference.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it actually does&lt;/strong&gt; (in beta right now):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Morning briefings&lt;/strong&gt; - your starting point for a true &amp;quot;Jarvis&amp;quot;-like AI experience (see demo video on product's main web page)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HealthKit integration&lt;/strong&gt; - ask about your health data (stays on-device where it belongs)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Document vault &amp;amp; email access&lt;/strong&gt; - full context without the cloud compromise&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-term memory&lt;/strong&gt; - AI that actually remembers your conversation history across the chats&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic search&lt;/strong&gt; - across files, emails, and chat history&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reminders &amp;amp; weather&lt;/strong&gt; - the basics, done privately&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why I'm posting here specifically&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This community actually understands local LLMs, their limitations, and what makes them useful (or not). You're also allergic to BS, which is exactly what we need right now.&lt;/p&gt; &lt;p&gt;We're in beta and it's completely free. No catch, no &amp;quot;free tier with limitations&amp;quot; - we're genuinely trying to figure out what matters to users before we even think about monetization.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What we're hoping for:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Brutal honesty about what works and what doesn't&lt;/li&gt; &lt;li&gt;Ideas on what would make this actually useful for your workflow&lt;/li&gt; &lt;li&gt;Technical questions about our architecture (happy to get into the weeds)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Link if you're curious:&lt;/strong&gt; &lt;a href="https://roia.io/atlantis?utm_source=reddit&amp;amp;utm_medium=social&amp;amp;utm_campaign=atlantis_intro_article&amp;amp;utm_content=r_LocalLLaMA"&gt;https://roia.io&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Not asking for upvotes or smth. Just feedback from people who know what they're talking about. Roast us if we deserve it - we'd rather hear it now than after we've gone down the wrong path.&lt;/p&gt; &lt;p&gt;Happy to answer any questions in the comments.&lt;/p&gt; &lt;p&gt;P.S. Before the tomatoes start flying - yes, we're Mac/iOS only at the moment. Windows, Linux, and Android are on the roadmap after our prod rollout in Q2. We had to start somewhere, and we promise we haven't forgotten about you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ipav9"&gt; /u/ipav9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/loj0n38hkg3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6mmb1/trying_to_build_a_jarvis_that_never_phones_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6mmb1/trying_to_build_a_jarvis_that_never_phones_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T19:54:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6gruv</id>
    <title>I tested a few local hosted coding models with VSCode / cline so that you don't have to</title>
    <updated>2025-11-25T16:21:10+00:00</updated>
    <author>
      <name>/u/DrMicrobit</name>
      <uri>https://old.reddit.com/user/DrMicrobit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been running a bunch of &amp;quot;can I actually code with a local model in VS Code?&amp;quot; experiments over the last weeks, focused on task with moderate complexity. I chose simple, well known games as they help to visualise strengths and shortcomings of the results quite easily, also to a layperson. The tasks at hand: Space Invaders &amp;amp; Galaga in a single HTML file. I also did a more serious run with a ~2.3k- word design doc.&lt;/p&gt; &lt;p&gt;Sharing the main takeaways here for anyone trying to use local models with Cline/Ollama for real coding work, not just completions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt; Ubuntu 24.04, 2x 4060 Ti 16 GB (32 GB total VRAM), VS Code + Cline, models served via Ollama / GGUF. Context for local models was usually ~96k tokens (anything much bigger spilled into RAM and became 7-20x slower). Tasks ranged from YOLO prompts (&amp;quot;Write a Space Invaders game in a single HTML file&amp;quot;) to a moderately detailed spec for a modernized Space Invaders.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Headline result:&lt;/strong&gt; Qwen 3 Coder 30B is the only family I tested that consistently worked well with Cline and produced usable games. At 4-bit it's already solid; quality drops noticeably at 3-bit and 2-bit (more logic bugs, more broken runs). With 4-bit and 32 GB VRAM you can keep ~ 100k context and still be reasorably fast. If you can spare more VRAM or live with reduced context, higher-bit Qwen 3 Coder (e.g. 6-bit) does help. But 4-bit is the practical sweet spot for 32 GiB VRAM.&lt;/p&gt; &lt;p&gt;Merges/prunes of Qwen 3 Coder generally underperformed the original. The cerebras REAP 25B prune and YOYO merges were noticeably buggier and less reliable than vanilla Qwen 3 Coder 30B, even at higher bit widths. They sometimes produced runnable code, but with a much higher &amp;quot;Cline has to rerun / you have to hand-debug or giveup&amp;quot; rate. TL;DR: for coding, the unmodified coder models beat their fancy descendants.&lt;/p&gt; &lt;p&gt;Non-coder 30B models and &amp;quot;hot&amp;quot; general models mostly disappointed in this setup. Qwen 3 30B (base/instruct from various sources), devstral 24B, Skyfall 31B v4, Nemotron Nano 9B v2, and Olmo 3 32B either: (a) fought with Cline (rambling, overwriting their own code, breaking the project), or (b) produced very broken game logic that wasn't fixable in one or two debug rounds. Some also forced me to shrink context so much they stopped being interesting for larger tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Guiding the models:&lt;/strong&gt; I wanted to demonstrate, with examples that can be shown to people without much insights, what development means: YOLO prompts (&amp;quot;Make me a Space Invaders / Galaga game&amp;quot;) will produce widely varying results even for big online models, and doubly so for locals. See &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/01_SpaceInvaders_yolo/online/GPT5/t1/space_invaders.html"&gt;this example&lt;/a&gt; for an interesting YOLO from GPT-5, and &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/01_SpaceInvaders_yolo/online/Opus41/t2/space_invaders.html"&gt;this example&lt;/a&gt; for a barebone one from Opus 4.1. Models differ a lot in what they think &amp;quot;Space Invaders&amp;quot; or &amp;quot;Galaga&amp;quot; is, and leave out key features (bunkers, UFO, proper alien movement, etc.).&lt;/p&gt; &lt;p&gt;With a moderately detailed design doc, Qwen 3 Coder 30B can stick reasonably well to spec: &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/03_SpaceInvaders_ddoc01/local/qwen3-coder-30B-unsloth/6bitUD_t1/space_invaders.html"&gt;Example 1&lt;/a&gt;, &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/03_SpaceInvaders_ddoc01/local/qwen3-coder-30B-unsloth/4bitUD_t1/space_invaders.html"&gt;Example 2&lt;/a&gt;, &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/03_SpaceInvaders_ddoc01/local/qwen3-coder-30B-unsloth/4bit_t2/space_invaders.html"&gt;Example 3&lt;/a&gt;. They still tend to repeat certain logic errors (e.g., invader formation movement, missing config entries) and often can't fix them from a high-level bug description without human help.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My current working hypothesis:&lt;/strong&gt; to do enthusiast-level Al-assisted coding in VS Code with Cline, one really needs to have at least 32 GB VRAM for usable models. Preferably use an untampered Qwen 3 Coder 30B (Ollama's default 4-bit, or an unsloth GGUF at 4-6 bits). Avoid going below 4-bit for coding, be wary of fancy merges/prunes, and don't expect miracles without a decent spec. &lt;/p&gt; &lt;p&gt;I documented all runs (code + notes) in a repo on GitHub (&lt;a href="https://github.com/DrMicrobit/lllm_suit"&gt;https://github.com/DrMicrobit/lllm_suit&lt;/a&gt;) if anyone's interested in. The docs there are linked and, going down the experiments, give an idea of what the results looked like with an image and have direct links runnable HTML files, configs, and model variants.&lt;/p&gt; &lt;p&gt;I'd be happy to hear what others think of this kind of simple experimental evaluation, or what other models I could test.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrMicrobit"&gt; /u/DrMicrobit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gruv/i_tested_a_few_local_hosted_coding_models_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gruv/i_tested_a_few_local_hosted_coding_models_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gruv/i_tested_a_few_local_hosted_coding_models_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6znf4</id>
    <title>Built Clamp - Git-like version control for RAG vector databases</title>
    <updated>2025-11-26T05:34:58+00:00</updated>
    <author>
      <name>/u/Lumpy_Repair1252</name>
      <uri>https://old.reddit.com/user/Lumpy_Repair1252</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, I built Clamp - a tool that adds Git-like version control to vector databases (Qdrant for now).&lt;/p&gt; &lt;p&gt;The idea: when you update your RAG knowledge base, you can roll back to previous versions without losing data. Versions are tracked via metadata, rollbacks flip active flags (instant, no data movement).&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;p&gt;- CLI + Python API&lt;/p&gt; &lt;p&gt;- Local SQLite for commit history&lt;/p&gt; &lt;p&gt;- Instant rollbacks&lt;/p&gt; &lt;p&gt;Early alpha, expect rough edges. Built it to learn about versioning systems and vector DB metadata patterns.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/athaapa/clamp"&gt;https://github.com/athaapa/clamp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Install: pip install clamp-rag&lt;/p&gt; &lt;p&gt;Would love feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lumpy_Repair1252"&gt; /u/Lumpy_Repair1252 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6znf4/built_clamp_gitlike_version_control_for_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6znf4/built_clamp_gitlike_version_control_for_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6znf4/built_clamp_gitlike_version_control_for_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T05:34:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6wios</id>
    <title>HunyuanOCR-1B - Dockerized Streamlit OCR App - Quite Amazing.</title>
    <updated>2025-11-26T02:53:59+00:00</updated>
    <author>
      <name>/u/exaknight21</name>
      <uri>https://old.reddit.com/user/exaknight21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p68sjf/tencenthunyuanocr1b/"&gt;this post&lt;/a&gt; this morning as I woke up, and I got very excited. I love vLLM a lot because it allows me to experiment with FastAPI a lot more smoother - and I tend to this vLLM is production grade, so if I can get nice results on my crappy 3060 12 GB, then I can definitely replicate it on beefier GPUs. Anyways, it's a whole learning thing I am doing and I love sharing so here we are.&lt;/p&gt; &lt;p&gt;I spent majority of the day fighting a batter with Grok and DeepSeek, we couldn't get vLLM Nightly Builds to work. We are not coders, so there you have it. At the end, I asked Grok to get it together and get it to work, I just wanna see it work before I throw in the towel. I guess it needed the political motivation and it put together Transformers (mind you I am learning all this so I actually didn't know about Transformers so that is something to study tonight). &lt;/p&gt; &lt;p&gt;The result was: &lt;a href="https://github.com/ikantkode/hunyuan-1b-ocr-app"&gt;https://github.com/ikantkode/hunyuan-1b-ocr-app&lt;/a&gt; - and I wanted to test and record it. I recorder it and that is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=qThh6sqkrF0"&gt;https://www.youtube.com/watch?v=qThh6sqkrF0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model is really good. I guess my only complaints would be it's current BF16 state, I believe FP8 would be very beneficial, and better vLLM support. But then again, I am not educated enough to even voice my opinion yet.&lt;/p&gt; &lt;p&gt;If someone gets vLLM to work, can you please share. I would absolutely love it. I don't know how to quantize a model, and I am pretty sure I lack resources anyways, but one day I will be able to contribute in a better way than hacking a streamlit together for this community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/exaknight21"&gt; /u/exaknight21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6wios/hunyuanocr1b_dockerized_streamlit_ocr_app_quite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6wios/hunyuanocr1b_dockerized_streamlit_ocr_app_quite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6wios/hunyuanocr1b_dockerized_streamlit_ocr_app_quite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T02:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p68sjf</id>
    <title>tencent/HunyuanOCR-1B</title>
    <updated>2025-11-25T10:10:33+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p68sjf/tencenthunyuanocr1b/"&gt; &lt;img alt="tencent/HunyuanOCR-1B" src="https://external-preview.redd.it/euNO2VS0UsDEnKIxd8MnYm5CABYmnLN8JLKug1m_WZw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6451703bfbd1fab35e662fdb90c099a069b6d25b" title="tencent/HunyuanOCR-1B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/HunyuanOCR"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p68sjf/tencenthunyuanocr1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p68sjf/tencenthunyuanocr1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T10:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6ksc6</id>
    <title>I built an AI research platform and just open sourced it.</title>
    <updated>2025-11-25T18:48:11+00:00</updated>
    <author>
      <name>/u/CodingWithSatyam</name>
      <uri>https://old.reddit.com/user/CodingWithSatyam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I've been working on Introlix for some months now. So, today I've open sourced it. It was really hard time building it as an student and a solo developer. This project is not finished yet but its on that stage I can show it to others and ask other for help in developing it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I built:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Introlix is an AI-powered research platform. Think of it as &amp;quot;GitHub Copilot meets Google Docs&amp;quot; for research work.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Research Desk: It is just like google docs but in right side there is an AI pannel where users can ask questions to LLM. And also it can edit or write document for user. So, it is just like github copilot but it is for text editor. There are two modes: Chat and edit. Chat mode is for asking questions and edit mode is for editing the document using AI agent.&lt;/li&gt; &lt;li&gt;Chat: For quick questions you can create a new chat and ask questions.&lt;/li&gt; &lt;li&gt;Workspace: Every chat, and research desk are managed in workspace. A workspace shares data with every items it have. So, when creating an new desk or chat user need to choose a workspace and every items on that workspace will be sharing same data. The data includes the search results and scraped content.&lt;/li&gt; &lt;li&gt;Multiple AI Agents: There are multiple AI agents like: context agent (to understand user prompt better), planner agent, explorer_agent (to search internet), etc.&lt;/li&gt; &lt;li&gt;Auto Format &amp;amp; Reference manage (coming soon): This is a feature to format the document into blog post style or research paper style or any other style and also automatic citation management with inline references.&lt;/li&gt; &lt;li&gt;Local LLMs (coming soon): Will support local llms&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So, I was working alone on this project and because of that codes are little bit messy. And many feature are not that fast. I've never tried to make it perfect as I was focusing on building the MVP. Now after working demo I'll be developing this project into complete working stable project. And I know I can't do it alone. I also want to learn about how to work on very big projects and this could be one of the big opportunity I have. There will be many other students or every other developers that could help me build this project end to end. To be honest I have never open sourced any project before. I have many small project and made it public but never tired to get any help from open source community. So, this is my first time.&lt;/p&gt; &lt;p&gt;I like to get help from senior developers who can guide me on this project and make it a stable project with a lot of features.&lt;/p&gt; &lt;p&gt;Here is github link for technical details: &lt;a href="https://github.com/introlix/introlix"&gt;https://github.com/introlix/introlix&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord link: &lt;a href="https://discord.gg/mhyKwfVm"&gt;https://discord.gg/mhyKwfVm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: I've been still working on adding github issues for development plan.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CodingWithSatyam"&gt; /u/CodingWithSatyam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ksc6/i_built_an_ai_research_platform_and_just_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ksc6/i_built_an_ai_research_platform_and_just_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ksc6/i_built_an_ai_research_platform_and_just_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T18:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1p707ev</id>
    <title>I made a free site with file tools + a local AI chat that connects to Ollama</title>
    <updated>2025-11-26T06:06:13+00:00</updated>
    <author>
      <name>/u/opal-emporium</name>
      <uri>https://old.reddit.com/user/opal-emporium</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a side project called &lt;a href="https://practicalwebtools.com/"&gt;Practical Web Tools&lt;/a&gt; and figured I'd share it here.&lt;/p&gt; &lt;p&gt;It's basically a collection of free browser-based utilities: PDF converters, file compressors, format changers, that kind of stuff. Nothing groundbreaking, but I got tired of sites that either paywall basic features or make you upload files to god-knows-where. Most of the processing happens in your browser so your files stay on your device.&lt;/p&gt; &lt;p&gt;The thing I'm most excited about is a local AI chat interface I just added. It connects directly to Ollama so you can chat with models running on your own machine. No API keys, no usage limits, no sending your conversations to some company's servers. If you've been curious about local LLMs but don't love the command line, it might be worth checking out.&lt;/p&gt; &lt;p&gt;Anyway, it's completely free — no accounts, no premium tiers, none of that. Just wanted to make something useful.&lt;/p&gt; &lt;p&gt;Happy to answer questions or take feedback if anyone has suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/opal-emporium"&gt; /u/opal-emporium &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p707ev/i_made_a_free_site_with_file_tools_a_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p707ev/i_made_a_free_site_with_file_tools_a_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p707ev/i_made_a_free_site_with_file_tools_a_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T06:06:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6cf2p</id>
    <title>How are Chinese AI models claiming such low training costs? Did some research</title>
    <updated>2025-11-25T13:27:51+00:00</updated>
    <author>
      <name>/u/Acrobatic_Solid6023</name>
      <uri>https://old.reddit.com/user/Acrobatic_Solid6023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Doing my little assignment on model cost. deepseek claims $6M training cost. Everyones losing their minds cause ChatGPT-4 cost $40-80M and Gemini Ultra hit $190M.&lt;/p&gt; &lt;p&gt;Got curious if other Chinese models show similar patterns or if deepseeks just marketing bs.&lt;/p&gt; &lt;p&gt;What I found on training costs:&lt;/p&gt; &lt;p&gt;glm-4.6: $8-12M estimated&lt;/p&gt; &lt;ul&gt; &lt;li&gt;357B parameters (thats model size)&lt;/li&gt; &lt;li&gt;More believable than deepseeks $6M but still way under Western models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Kimi K2-0905: $25-35M estimated&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1T parameters total (MoE architecture, only ~32B active at once)&lt;/li&gt; &lt;li&gt;Closer to Western costs but still cheaper&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MiniMax: $15-20M estimated&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mid-range model, mid-range cost&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;deepseek V3.2: $6M (their claim)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seems impossibly low for GPU rental + training time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why the difference?&lt;/p&gt; &lt;p&gt;Training cost = GPU hours × GPU price + electricity + data costs.&lt;/p&gt; &lt;p&gt;Chinese models might be cheaper because:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cheaper GPU access (domestic chips or bulk deals)&lt;/li&gt; &lt;li&gt;Lower electricity costs in China&lt;/li&gt; &lt;li&gt;More efficient training methods (though this is speculation)&lt;/li&gt; &lt;li&gt;Or theyre just lying about the real numbers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;deepseeks $6M feels like marketing. You cant rent enough H100s for months and only spend $6M unless youre getting massive subsidies or cutting major corners.&lt;/p&gt; &lt;p&gt;glms $8-12M is more realistic. Still cheap compared to Western models but not suspiciously fake-cheap.&lt;/p&gt; &lt;p&gt;Kimi at $25-35M shows you CAN build competitive models for less than $100M+ but probably not for $6M.&lt;/p&gt; &lt;p&gt;Are these real training costs or are they hiding infrastructure subsidies and compute deals that Western companies dont get?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acrobatic_Solid6023"&gt; /u/Acrobatic_Solid6023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6cf2p/how_are_chinese_ai_models_claiming_such_low/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6cf2p/how_are_chinese_ai_models_claiming_such_low/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6cf2p/how_are_chinese_ai_models_claiming_such_low/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T13:27:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6o5hf</id>
    <title>Cheapest $/vRAM GPU right now? Is it a good time?</title>
    <updated>2025-11-25T20:53:01+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an rtx 2080 which only has 8Gb vRAM, and I was thinking of upgrading that GPU to an affordable and good $/vRAM ratio GPU. I don't have 8k to drop on an rtx pro 6000 like suggested a few days ago here, I was thinking more in the &amp;lt;1k range.&lt;/p&gt; &lt;p&gt;Here are some options I've seen from most expensive to cheapest:&lt;/p&gt; &lt;p&gt;$1,546 RTX PRO 4000 Blackwell 24 GB GDDR7 $64/Gb&lt;/p&gt; &lt;p&gt;~$900 wait for 5070 ti super? $37/Gb&lt;/p&gt; &lt;p&gt;$800 RTX titan, $33/Gb&lt;/p&gt; &lt;p&gt;$600-800 used 3090, $25-33/Gb &lt;/p&gt; &lt;p&gt;2x$300 mac mini m1 16g cluster using exolabs? (i've used a mac mini cluster before, but it is limited on what you can run) $18/Gb&lt;/p&gt; &lt;p&gt;Is it a good time to guy a GPU? What are your setups like and what can you run in this price range?&lt;/p&gt; &lt;p&gt;I'm worried that the uptrend of RAM prices means GPUs are going to become more expensive in the coming months. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6o5hf/cheapest_vram_gpu_right_now_is_it_a_good_time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6o5hf/cheapest_vram_gpu_right_now_is_it_a_good_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6o5hf/cheapest_vram_gpu_right_now_is_it_a_good_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T20:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6qwok</id>
    <title>Are Imatrix Quants Hurting your Model? (My opinion)</title>
    <updated>2025-11-25T22:40:14+00:00</updated>
    <author>
      <name>/u/Quiet_Joker</name>
      <uri>https://old.reddit.com/user/Quiet_Joker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay, so it all started when i was using TheDrummer/Cydonia-24B-v4.1 for roleplay and i was using the normal Non-imatrix quantized Q5_K_M GGUF. The quality is good, the model is good. I was honestly impressed with it, but i decided to see if i could get better quality by using the Imatrix Q6_K_L from Bartowski, MANY people recommend to use Imatrix quants, so it must be good right?&lt;/p&gt; &lt;p&gt;Well... this is where it got odd, during my usage i started to notice a slight difference in the way the model interpreted the characters. They seemed less... emotional and less prone to act in their own personality as the character card was made, also stuff like little details were easily missed. Almost like someone just took the sense of direction out of them, sure the model/character still tried to act in character and for the most part it was following the context but it wasn't the same. On Q5_K_M (non imatrix) the character acted with more expression in the way they talked, ideas they came up with and small details like if the character touched a wall it would describe what they felt, etc.&lt;/p&gt; &lt;p&gt;I decided to test again this time with a Q5_K_L Imatrix quant from Bartowski, maybe it was the Q6 or something. Well, this time it felt worse than before, the same thing happened, the character didn't think or acted in a way that fitted their personality. The character was more &amp;quot;resistant&amp;quot; to RP and ERP. So i decided to go back and test the normal non-imatrix Q5_K_M and the problems just went away. The character acted like it should, it was more in character and it was more receptive to the ERP than the Imatrix quants.&lt;/p&gt; &lt;p&gt;I could be wrong but this is just my experience, maybe others can share their experiences so we can compare? I know imatrix are served as this &amp;quot;universal&amp;quot; quant magic, but i decided to dig deeper into it. I found out that it DOES matter what dataset you use. Imatrix don't just &amp;quot;decided which weights should have more precision when quantizing&amp;quot; they have to be given a dataset to fit. &lt;/p&gt; &lt;p&gt;I found out that most people use the wikitext dataset for the calibration of the imatrix, so we will go with that as an example. If the calibration dataset doesn't match the use case of the model, it can hurt it. That's the conclusion i came up with after reading the original PR and if the calibration is done as a &amp;quot;one dataset fits all approach&amp;quot;. &lt;/p&gt; &lt;p&gt;I decided to ask Claude and chatgpt mainly for them to search the web and they came up with the same conclusion as well. It depends on the calibration dataset.&lt;/p&gt; &lt;p&gt;Claude gave me this crude visual representation of how it works more or less:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. Calibration Dataset (wiki.train.raw) ↓ 2. Run model, capture activations &amp;quot;The cat sat...&amp;quot; → Layer 1 → [0.3, 1.8, 0.1, 2.4, ...] activations ↓ 3. Square and sum activations across many chunks Weight row 1: 0.3² + 1.2² + 0.8² + ... = 45.2 (importance score) Weight row 2: 1.8² + 0.4² + 2.1² + ... = 123.7 (importance score) ↓ 4. Save importance scores to imatrix.gguf [45.2, 123.7, 67.3, 201.4, ...] ↓ 5. Quantization reads these scores - Weight row 2 (score: 123.7) → preserve with high precision - Weight row 1 (score: 45.2) → can use lower precision ↓ 6. Final quantized model (Q4_K_M with IMatrix guidance) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But when you are quantizing a ERP or RP model... this is where it gets interesting: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;IMatrix thinks is important (from Wikipedia): ├─ Factual information processing: HIGH importance (PRESERVED) ├─ Date/number handling: HIGH importance (PRESERVED) ├─ Formal language patterns: HIGH importance (PRESERVED) └─ Technical terminology: HIGH importance (PRESERVED) Result during quantization: ├─ Emotional language weights: LOW priority → HEAVILY QUANTIZED ├─ Creative description weights: LOW priority → HEAVILY QUANTIZED ├─ Character interaction weights: LOW priority → HEAVILY QUANTIZED └─ Factual/formal weights: HIGH priority → CAREFULLY PRESERVED &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So... what do you guys think? Should Imatrix quantization and calibration datasets be looked into a little bit more? I'd love to hear your thoughts and if i'm wrong on how the imatrix calculations are done and i'm just overthinking it, then please let me know, i'm sure others might be interested in this topic as well. Afterall i could just be making shit up and saying some shit like &amp;quot;Its different!&amp;quot; mainly cause i used a lower quant or something.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet_Joker"&gt; /u/Quiet_Joker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6qwok/are_imatrix_quants_hurting_your_model_my_opinion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6qwok/are_imatrix_quants_hurting_your_model_my_opinion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6qwok/are_imatrix_quants_hurting_your_model_my_opinion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T22:40:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6h63t</id>
    <title>Ryzen AI and Radeon are ready to run LLMs Locally with Lemonade Software</title>
    <updated>2025-11-25T16:36:04+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6h63t/ryzen_ai_and_radeon_are_ready_to_run_llms_locally/"&gt; &lt;img alt="Ryzen AI and Radeon are ready to run LLMs Locally with Lemonade Software" src="https://external-preview.redd.it/JNLQ1TFljv7ecmJGVkBQs2GgRQ8E7p3qY9QpUEtPmD8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88a44b03e9ee2eeae1607d08f00e67f73244cead" title="Ryzen AI and Radeon are ready to run LLMs Locally with Lemonade Software" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.amd.com/en/developer/resources/technical-articles/2025/ryzen-ai-radeon-llms-with-lemonade.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6h63t/ryzen_ai_and_radeon_are_ready_to_run_llms_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6h63t/ryzen_ai_and_radeon_are_ready_to_run_llms_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:36:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6gsjh</id>
    <title>LLaDA2.0 (103B/16B) has been released</title>
    <updated>2025-11-25T16:21:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;LLaDA2.0-flash&lt;/strong&gt; is a diffusion language model featuring a 100BA6B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA2.0 series, it is optimized for practical applications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-flash"&gt;https://huggingface.co/inclusionAI/LLaDA2.0-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLaDA2.0-mini&lt;/strong&gt; is a diffusion language model featuring a 16BA1B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA series, it is optimized for practical applications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-mini"&gt;https://huggingface.co/inclusionAI/LLaDA2.0-mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp support in progress &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17454"&gt;https://github.com/ggml-org/llama.cpp/pull/17454&lt;/a&gt;&lt;/p&gt; &lt;p&gt;previous version of LLaDA is supported &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16003"&gt;https://github.com/ggml-org/llama.cpp/pull/16003&lt;/a&gt; already (please check the comments)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:21:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6ht87</id>
    <title>Flux 2 can be run on 24gb vram!!!</title>
    <updated>2025-11-25T16:59:49+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ht87/flux_2_can_be_run_on_24gb_vram/"&gt; &lt;img alt="Flux 2 can be run on 24gb vram!!!" src="https://preview.redd.it/m9ud0rs8pf3g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=815d30594ab759659c5d269629ebb9cd5bd93a40" title="Flux 2 can be run on 24gb vram!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I dont know why people are complaining......&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9ud0rs8pf3g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ht87/flux_2_can_be_run_on_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ht87/flux_2_can_be_run_on_24gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:59:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6x5dh</id>
    <title>​The White House just launched "The Genesis Mission": A Manhattan Project-style initiative for AI</title>
    <updated>2025-11-26T03:24:43+00:00</updated>
    <author>
      <name>/u/iamnottheabyss</name>
      <uri>https://old.reddit.com/user/iamnottheabyss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6x5dh/the_white_house_just_launched_the_genesis_mission/"&gt; &lt;img alt="​The White House just launched &amp;quot;The Genesis Mission&amp;quot;: A Manhattan Project-style initiative for AI" src="https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb89e898879eb7adef969749433776a6f6a543ad" title="​The White House just launched &amp;quot;The Genesis Mission&amp;quot;: A Manhattan Project-style initiative for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the White House launching The Genesis Mission, what are the implications for Open Source Models now, are we going to get stronger waves of regulation, especiallyon the open-source sector? Should we start backing up the LLMs that are on HuggingFace?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnottheabyss"&gt; /u/iamnottheabyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.whitehouse.gov/presidential-actions/2025/11/launching-the-genesis-mission/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6x5dh/the_white_house_just_launched_the_genesis_mission/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6x5dh/the_white_house_just_launched_the_genesis_mission/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T03:24:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6k0h2</id>
    <title>You can now do FP8 reinforcement learning locally! (&lt;5GB VRAM)</title>
    <updated>2025-11-25T18:19:47+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6k0h2/you_can_now_do_fp8_reinforcement_learning_locally/"&gt; &lt;img alt="You can now do FP8 reinforcement learning locally! (&amp;lt;5GB VRAM)" src="https://preview.redd.it/t5wv1iax1g3g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2fb5f6ea2413c66c20bbe83efc473ce566ff763" title="You can now do FP8 reinforcement learning locally! (&amp;lt;5GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We're getting close to our last release of 2025! Thanks so much for all the support this year. The DeepSeek team back in Jan showcased how powerful FP8 RL can be with GRPO. Well, you can now try it on your local hardware using only 5GB VRAM! RTX 50x, 40x series all work! Unsloth GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why should you do FP8 training?&lt;/strong&gt;&lt;br /&gt; NVIDIA's research finds FP8 training can match BF16 accuracy whilst getting 1.6x faster inference time. We collabed with TorchAO from PyTorch to introduce FP8 RL training, making FP8 GRPO possible on home GPUs with no accuracy loss!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-4B FP8 GRPO works on just 6GB VRAM. Qwen3-1.7B on 5GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1.4x faster RL training and 2× longer context vs BF16/FP16&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;60% less VRAM and 10× longer context than other FP8 RL implementations&lt;/li&gt; &lt;li&gt;Unsloth is the only framework that makes FP8 RL LoRA work on consumer GPUs (e.g. NVIDIA RTX 40 &amp;amp; 50 Series). Also runs on H100, H200, B200.&lt;/li&gt; &lt;li&gt;You may notice &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; now uses much less VRAM than before, enabling even longer context. We’re also implementing faster training soon. Blog coming soon&lt;/li&gt; &lt;li&gt;Our notebooks use 24GB L4s which fit Qwen3-14B as Tesla T4s don’t support FP8.&lt;/li&gt; &lt;li&gt;Our FP8 RL incorporates Unsloth’s weight sharing, Standby, Flex Attention + more.&lt;/li&gt; &lt;li&gt;Works on any NVIDIA RTX 40, 50 series and H100, B200 etc. GPUs&lt;/li&gt; &lt;li&gt;Use &lt;code&gt;load_in_fp8 = True&lt;/code&gt; within &lt;code&gt;FastLanguageModel&lt;/code&gt; to enable FP8 RL.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can read our blogpost for our findings and more: &lt;a href="https://docs.unsloth.ai/new/fp8-reinforcement-learning"&gt;https://docs.unsloth.ai/new/fp8-reinforcement-learning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Llama 3.2 1B FP8 Colab Notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama_FP8_GRPO.ipynb"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama_FP8_GRPO.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the notebook, you can plug in any of our previous reward functions or RL environment examples, including our auto kernel creation and our 2048 game notebooks. To enable fp8:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import os; os.environ['UNSLOTH_VLLM_STANDBY'] = &amp;quot;1&amp;quot; # Saves 30% VRAM from unsloth import FastLanguageModel model, tokenizer = FastLanguageModel.from_pretrained( model_name = &amp;quot;unsloth/Qwen3-8B&amp;quot;, max_seq_length = 2048, load_in_4bit = False, # False for LoRA 16bit fast_inference = True, # Enable vLLM fast inference max_lora_rank = 32, load_in_fp8 = True, # Float8 RL / GRPO! ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope you all have a lovely Thanksgiving, a lovely rest of the week and I'll be here to answer any and all questions! =)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t5wv1iax1g3g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6k0h2/you_can_now_do_fp8_reinforcement_learning_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6k0h2/you_can_now_do_fp8_reinforcement_learning_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T18:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I’m Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; — Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
