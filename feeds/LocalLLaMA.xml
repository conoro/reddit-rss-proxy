<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-20T01:36:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pq5k6e</id>
    <title>Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios</title>
    <updated>2025-12-18T23:23:44+00:00</updated>
    <author>
      <name>/u/Competitive_Travel16</name>
      <uri>https://old.reddit.com/user/Competitive_Travel16</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"&gt; &lt;img alt="Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios" src="https://external-preview.redd.it/A_KZLQUNhCh0wGe2hwjJCJ470X6QmuVpXZdzOWccb0U.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fdf14dca65c42b501a6a7e33b1acf44e71ac72f" title="Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive_Travel16"&gt; /u/Competitive_Travel16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=4l4UWZGxvoc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T23:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqxg7e</id>
    <title>DPO on GPT-OSS with Nemo-RL</title>
    <updated>2025-12-19T21:52:23+00:00</updated>
    <author>
      <name>/u/red_dhinesh_it</name>
      <uri>https://old.reddit.com/user/red_dhinesh_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, &lt;/p&gt; &lt;p&gt;I'm new to Nemo-RL and I'd like to perform DPO on GPT-OSS-120B model. The readme of 0.4 release (&lt;a href="https://github.com/NVIDIA-NeMo/RL/blob/main/README.md"&gt;https://github.com/NVIDIA-NeMo/RL/blob/main/README.md&lt;/a&gt;) mentions that support for new models gpt-oss, Qwen3-Next, Nemotron-Nano3 is coming soon. Does that mean I cannot perform DPO on GPT-OSS with both Megatron and DTensor backends?&lt;/p&gt; &lt;p&gt;If this is not the right channel for this question, please redirect me to the right one.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/red_dhinesh_it"&gt; /u/red_dhinesh_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqxg7e/dpo_on_gptoss_with_nemorl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqxg7e/dpo_on_gptoss_with_nemorl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqxg7e/dpo_on_gptoss_with_nemorl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T21:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pr1hth</id>
    <title>I made a local semantic search engine that lives in the system tray. With preloaded models, it syncs automatically to changes and allows the user to make a search without load times.</title>
    <updated>2025-12-20T01:04:25+00:00</updated>
    <author>
      <name>/u/donotfire</name>
      <uri>https://old.reddit.com/user/donotfire</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://github.com/henrydaum/2nd-Brain"&gt;https://github.com/henrydaum/2nd-Brain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Old version: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o2q5n6/i_made_a_multimodal_local_rag_system_with_lm/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;reddit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is my attempt at making a highly optimized local search engine. I designed the main engine to be as lightweight as possible, and I can embed my entire database, which is 20,000 files, in under an hour with 6x multithreading on GPU: 100% GPU utilization.&lt;/p&gt; &lt;p&gt;It uses a hybrid lexical/semantic search algorithm with MMR reranking; results are highly accurate. High quality results are boosted using an LLM who gives quality scores.&lt;/p&gt; &lt;p&gt;It's multimodal and supports up to 49 file extensions - vision-enabled LLMs - text and image embedding models - OCR.&lt;/p&gt; &lt;p&gt;There's an optional &amp;quot;Windows Recall&amp;quot;-esque feature that takes screenshots every N seconds and saves them to a folder. Sync that folder with the others and it's possible to basically have Windows Recall. The search feature can limit search to just that folder with a filter for precision. It can sync many folders at the same time.&lt;/p&gt; &lt;p&gt;I haven't implemented RAG yet - just the retrieval part. I usually find the LLM response to be too time-consuming so I left it for last. But I really do love how it just sits in my system tray and I can completely forget about it. The best part is how I can just open it up all of a sudden and my models are already pre-loaded so there's no load time. It just opens right up. I can send a search in three clicks and a bit of typing.&lt;/p&gt; &lt;p&gt;Let me know what you guys think! (If anybody sees any issues, please let me know.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/donotfire"&gt; /u/donotfire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pr1hth/i_made_a_local_semantic_search_engine_that_lives/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pr1hth/i_made_a_local_semantic_search_engine_that_lives/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pr1hth/i_made_a_local_semantic_search_engine_that_lives/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T01:04:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqwrxh</id>
    <title>Nemotron-3-Nano Audit: Evidence of 32% "Latency Penalty" when Reasoning is toggled OFF</title>
    <updated>2025-12-19T21:23:28+00:00</updated>
    <author>
      <name>/u/Sad_Perception_1685</name>
      <uri>https://old.reddit.com/user/Sad_Perception_1685</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA recently released Nemotron-3-Nano, claiming granular reasoning budget control and a distinct &amp;quot;Reasoning OFF&amp;quot; mode for cost efficiency. I conducted a controlled audit (135 runs) across 5 configurations to validate these claims. My findings suggest that the current orchestration layer fails to effectively gate the model's latent compute, resulting in a 32% latency penalty when reasoning is toggled off.&lt;/p&gt; &lt;p&gt;Methodology:&lt;/p&gt; &lt;p&gt;Model: Nemotron-3-Nano (30B-A3B) via official NIM/API.&lt;/p&gt; &lt;p&gt;Matrix: 9 prompts (Arithmetic, Algebra, Multi-step reasoning) x 5 configs x 3 runs each.&lt;/p&gt; &lt;p&gt;Metrics: Probability Deviation (PD), Confidence/Determinism Index (CDI), Trace Count (internal reasoning tokens), and End-to-End Latency.&lt;/p&gt; &lt;p&gt;Key Observations:&lt;/p&gt; &lt;p&gt;Inverse Latency Correlation: Disabling reasoning (Thinking: OFF) resulted in higher average latency (2529ms) compared to the baseline (1914ms). This suggests the model may still be engaging in latent state-space deliberations without outputting tokens, creating a &amp;quot;compute leak.&amp;quot;&lt;/p&gt; &lt;p&gt;Budget Control Variance: BUDGET_LOW (Avg 230 traces) showed no statistically significant difference from BUDGET_HIGH (Avg 269 traces). The &amp;quot;Thinking Budget&amp;quot; appears to act as a hard ceiling for complexity rather than a steerable parameter for cost.&lt;/p&gt; &lt;p&gt;Arithmetic Stalling: On complex multiplication tasks (12,345√ó6,789), the model frequently exhausted its trace budget and returned zero tokens, rather than falling back to a non-reasoning heuristic.&lt;/p&gt; &lt;p&gt;Stochasticity: In NO_REASONING mode, the PD Coefficient of Variation reached 217%, indicating the model becomes highly unstable when its primary reasoning path is suppressed.&lt;/p&gt; &lt;p&gt;Discussion: The technical report for Nemotron-3-Nano emphasizes a Hybrid Mamba-Transformer architecture designed for efficiency. However, these results suggest that the &amp;quot;Thinking Budget&amp;quot; feature may not yet be fully optimized in the inference stack, leading to unpredictable costs and performance regressions in non-reasoning modes.&lt;/p&gt; &lt;p&gt;Full telemetry logs for all 135 runs, including raw JSON data for per-run latencies, trace counts, and PD/CDI metrics, are available here for independent verification.&lt;br /&gt; &lt;a href="https://gist.github.com/MCastens/c9bafcc64247698d23c81534e336f196"&gt;https://gist.github.com/MCastens/c9bafcc64247698d23c81534e336f196&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_Perception_1685"&gt; /u/Sad_Perception_1685 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqwrxh/nemotron3nano_audit_evidence_of_32_latency/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqwrxh/nemotron3nano_audit_evidence_of_32_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqwrxh/nemotron3nano_audit_evidence_of_32_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T21:23:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqr0nb</id>
    <title>RTX3060 12gb: Don't sleep on hardware that might just meet your specific use case</title>
    <updated>2025-12-19T17:30:06+00:00</updated>
    <author>
      <name>/u/desexmachina</name>
      <uri>https://old.reddit.com/user/desexmachina</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqr0nb/rtx3060_12gb_dont_sleep_on_hardware_that_might/"&gt; &lt;img alt="RTX3060 12gb: Don't sleep on hardware that might just meet your specific use case" src="https://b.thumbs.redditmedia.com/x2274xfa-EGgmn3mW7lVh-Gjm83uqhuLKxKGwV_xmpU.jpg" title="RTX3060 12gb: Don't sleep on hardware that might just meet your specific use case" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The point of this post is to advise you not to get too caught up and feel pressure to conform to some of the hardware advice you see on the sub. Many people tend to have an all or nothing approach, especially with GPUs. Yes, we see many posts about guys with 6x 5090's, and as sexy as that is, it may not fit your use case.&lt;/p&gt; &lt;p&gt;I was running an RTX3090 in my SFF daily driver, because I wanted some portability for hackathons or demos. But I simply didn't have enough PSU, and I'd get system reboots on heavy inference. I had no other choice but to put one of the many 3060's I had in my lab. My model was only 7 gb in VRAM . . . This fit perfectly into the 12 gb VRAM of the 3060 and kept me within PSU power limits. &lt;/p&gt; &lt;p&gt;I built an app, that has short input token strings, and I'm truncating the output token strings as well for this app to load-test some sites. It is working beautifully as an inferencing machine that is running at 24/7. The kicker is that it is even running at near the same transactional throughput as the 3090 for about $200 these days.&lt;/p&gt; &lt;p&gt;On the technical end, sure in much more complex tasks, you'll want to be able to load big models onto 24-48 GB of VRAM, and will want to avoid multi-gpu VRAM model sharding, but having older GPUs with old CUDA compute or slower IPC for the sake of VRAM, I don't think is even worth it. This is an Ampere generation chip and not some antique Fermi.&lt;/p&gt; &lt;p&gt;Some GPU util shots attached w/ intermittent vs full load inference runs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tss2jsjg378g1.png?width=1005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7f277c347a7aad344c75c67a4bcdc0a814e56b6"&gt;https://preview.redd.it/tss2jsjg378g1.png?width=1005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7f277c347a7aad344c75c67a4bcdc0a814e56b6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kvgovqt3478g1.png?width=1007&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f1bc463b41a31f1d75138fe2528a2237cd82596a"&gt;https://preview.redd.it/kvgovqt3478g1.png?width=1007&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f1bc463b41a31f1d75138fe2528a2237cd82596a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/desexmachina"&gt; /u/desexmachina &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqr0nb/rtx3060_12gb_dont_sleep_on_hardware_that_might/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqr0nb/rtx3060_12gb_dont_sleep_on_hardware_that_might/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqr0nb/rtx3060_12gb_dont_sleep_on_hardware_that_might/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T17:30:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqx2g4</id>
    <title>Access your local models from anywhere over WebRTC!</title>
    <updated>2025-12-19T21:35:54+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqx2g4/access_your_local_models_from_anywhere_over_webrtc/"&gt; &lt;img alt="Access your local models from anywhere over WebRTC!" src="https://external-preview.redd.it/cXNleG9xMXVhODhnMRI1dKC2fR8sDPDELLJ5_KCO5w2jTuICxlj8m6RG-zIr.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9eee336cc57871dc321b7235cb74e642083ee36d" title="Access your local models from anywhere over WebRTC!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLlama!&lt;/p&gt; &lt;p&gt;I wanted to share something I've been working on for the past few months. I recently got my hands on an AMD AI Pro R9700, which opened up the world of running local LLM inference on my own hardware. The problem? There was no good solution for privately and easily accessing my desktop models remotely. So I built one.&lt;/p&gt; &lt;h2&gt;The Vision&lt;/h2&gt; &lt;p&gt;My desktop acts as a hub that multiple devices can connect to over WebRTC and run inference simultaneously. Think of it as your personal inference server, accessible from anywhere without exposing ports or routing traffic through third-party servers.&lt;/p&gt; &lt;h2&gt;Why I Built This&lt;/h2&gt; &lt;p&gt;Two main reasons drove me to create this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Hardware is expensive&lt;/strong&gt; - AI-capable hardware comes with sky-high prices. This enables sharing of expensive hardware so the cost is distributed across multiple people.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Community resource sharing&lt;/strong&gt; - Family or friends can contribute to a common instance that they all share for their local AI needs, with minimal setup and maximum security. No cloud providers, no subscriptions, just shared hardware among people you trust.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;The Technical Challenges&lt;/h2&gt; &lt;h3&gt;1. WebRTC Signaling Protocol&lt;/h3&gt; &lt;p&gt;WebRTC defines how peers connect after exchanging information, but doesn't specify how that information is exchanged via a signaling server.&lt;/p&gt; &lt;p&gt;I really liked &lt;a href="https://github.com/gfodor/p2pcf"&gt;p2pcf&lt;/a&gt; - simple polling messages to exchange connection info. However, it was designed with different requirements: - Web browser only - Dynamically decides who initiates the connection&lt;/p&gt; &lt;p&gt;I needed something that: - Runs in both React Native (via react-native-webrtc) and native browsers - Is asymmetric - the desktop always listens, mobile devices always initiate&lt;/p&gt; &lt;p&gt;So I rewrote it: &lt;strong&gt;&lt;a href="https://github.com/navedmerchant/p2pcf.rn"&gt;p2pcf.rn&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h3&gt;2. Signaling Server Limitations&lt;/h3&gt; &lt;p&gt;Cloudflare's free tier now limits requests to 100k/day. With the polling rate needed for real-time communication, I'd hit that limit with just ~8 users.&lt;/p&gt; &lt;p&gt;Solution? I rewrote the Cloudflare worker using Fastify + Redis and deployed it on Railway: &lt;strong&gt;&lt;a href="https://github.com/navedmerchant/p2pcf-signalling"&gt;p2pcf-signalling&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In my tests, it's about 2x faster than Cloudflare Workers and has no request limits since it runs on your own VPS (Railway or any provider).&lt;/p&gt; &lt;h2&gt;The Complete System&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/navedmerchant/MyDeviceAI-Desktop"&gt;MyDeviceAI-Desktop&lt;/a&gt;&lt;/strong&gt; - A lightweight Electron app that: - Generates room codes for easy pairing - Runs a managed llama.cpp server - Receives prompts over WebRTC and streams tokens back - Supports Windows (Vulkan), Ubuntu (Vulkan), and macOS (Apple Silicon Metal)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/navedmerchant/MyDeviceAI"&gt;MyDeviceAI&lt;/a&gt;&lt;/strong&gt; - The iOS and Android client (now in beta on &lt;a href="https://testflight.apple.com/join/Y4HJn4RU"&gt;TestFlight&lt;/a&gt;, Android beta apk on Github releases): - Enter the room code from your desktop - Enable &amp;quot;dynamic mode&amp;quot; - Automatically uses remote processing when your desktop is available - Seamlessly falls back to local models when offline&lt;/p&gt; &lt;h2&gt;Try It Out&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Install MyDeviceAI-Desktop (auto-sets up Qwen 3 4B to get you started)&lt;/li&gt; &lt;li&gt;Join the iOS beta&lt;/li&gt; &lt;li&gt;Enter the room code in the remote section on the app&lt;/li&gt; &lt;li&gt;Put the app in dynamic mode&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That's it! The app intelligently switches between remote and local processing.&lt;/p&gt; &lt;h2&gt;Known Issues&lt;/h2&gt; &lt;p&gt;I'm actively fixing some bugs in the current version: - Sometimes the app gets stuck on &amp;quot;loading model&amp;quot; when switching from local to remote - Automatic reconnection doesn't always work reliably&lt;/p&gt; &lt;p&gt;I'm working on fixes and will be posting updates to TestFlight and new APKs for Android on GitHub soon.&lt;/p&gt; &lt;h2&gt;Future Work&lt;/h2&gt; &lt;p&gt;I'm actively working on several improvements:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;MyDeviceAI-Web&lt;/strong&gt; - A browser-based client so you can access your models from anywhere on the web as long as you know the room code&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image and PDF support&lt;/strong&gt; - Add support for multimodal capabilities when using compatible models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;llama.cpp slots&lt;/strong&gt; - Implement parallel slot processing for better model responses and faster concurrent inference&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Seamless updates for the desktop app&lt;/strong&gt; - Auto-update functionality for easier maintenance&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom OpenAI-compatible endpoints&lt;/strong&gt; - Support for any OpenAI-compatible API (llama.cpp or others) instead of the built-in model manager&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hot model switching&lt;/strong&gt; - Support recent model switching improvements from llama.cpp for seamless switching between models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Connection limits&lt;/strong&gt; - Add configurable limits for concurrent users to manage resources&lt;/li&gt; &lt;li&gt;&lt;strong&gt;macOS app signing&lt;/strong&gt; - Sign the macOS app with my developer certificate (currently you need to run &lt;code&gt;xattr -c&lt;/code&gt; on the binary to bypass Gatekeeper)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Contributions are welcome!&lt;/strong&gt; I'm working on this on my free time, and there's a lot to do. If you're interested in helping out, check out the repositories and feel free to open issues or submit PRs.&lt;/p&gt; &lt;p&gt;Looking forward to your feedback! Check out the demo below:&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p8br3p1ua88g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqx2g4/access_your_local_models_from_anywhere_over_webrtc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqx2g4/access_your_local_models_from_anywhere_over_webrtc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T21:35:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqupew</id>
    <title>Offline-capable scaffolding with memory and continuity between sessions - MIRA</title>
    <updated>2025-12-19T19:57:11+00:00</updated>
    <author>
      <name>/u/awittygamertag</name>
      <uri>https://old.reddit.com/user/awittygamertag</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, my name is Taylor. I've spent the last 10 months building MIRA, an open-source system for persistent memory and autonomous context management. This is my TempleOS.&lt;/p&gt; &lt;p&gt;Problem Statement: I wanted memory that manages itself. No manual pruning, no context rot, no tagging. Memories decay if unused and persist if referenced. The system figures that out, not me. I also wanted the model to control its own context window rather than relying on external orchestration to decide what's relevant.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Deployment:&lt;/p&gt; &lt;p&gt;Single cURL. That's it.&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;curl -fsSL &lt;a href="https://raw.githubusercontent.com/taylorsatula/mira-OSS/refs/heads/main/deploy.sh"&gt;https://raw.githubusercontent.com/taylorsatula/mira-OSS/refs/heads/main/deploy.sh&lt;/a&gt; -o deploy.sh &amp;amp;&amp;amp; chmod +x deploy.sh &amp;amp;&amp;amp; ./deploy.sh&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;The script is 2000+ lines of production-grade deployment automation. It handles:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Platform detection (Linux/macOS) with OS-specific service management&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Pre-flight validation: 10GB disk space, port availability (1993, 8200, 6379, 5432), existing installation detection&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Dependency installation with idempotency (skips what's already installed)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Python venv creation and package installation&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Model downloads (~1.4GB: spaCy, sentence-transformers embedding model, optional Playwright)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;HashiCorp Vault initialization: AppRole creation, policy setup, automatic unseal, credential storage&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;PostgreSQL database and user creation&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Valkey (Redis-compatible) setup&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;API key configuration (interactive prompts or skip for later)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Offline mode with Ollama fallback if you don't want to use cloud APIs&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;systemd service creation with auto-start on boot (Linux)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Cleanup and script archival when complete&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Run with &lt;code&gt;--loud&lt;/code&gt; for verbose output if you want to see everything.&lt;/p&gt; &lt;p&gt;The script is fully unattended-capable. Answer the prompts or accept defaults and walk away. When you come back, MIRA is running either as a systemd service or on-demand.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Local-first architecture:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Embeddings run locally via sentence-transformers (mdbr-leaf-ir-asym, 768d). No API calls for search.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;CPU-only PyTorch. No GPU required.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;3GB total resource usage including embedding model and all plumbing (excluding LLM).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;PostgreSQL + Valkey + HashiCorp Vault for persistence and secrets.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Provider parity&lt;/strong&gt;: Any OpenAI-compatible endpoint works. Plug in ollama, vllm, llama.cpp. Internally MIRA follows Anthropic SDK conventions but translation happens at the proper layer. You're not locked in.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models tested&lt;/strong&gt;: Deepseek V3.2, Qwen 3, Ministral 3. Acceptable results down to 4b parameters. Claude Opus 4.5 gets the best results by a margin, but the architecture doesn't require it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What you lose with local models&lt;/strong&gt;: Extended thinking disabled, cache_control stripped, server-side code execution filtered out, file uploads become text warnings. I have tried to provide parity where ever possible and have graceful degradation for Anthropic-specific features like the code execution sandbox.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Memory decay formula:&lt;/p&gt; &lt;p&gt;This is the part I'm proud of.&lt;/p&gt; &lt;p&gt;Decay runs on &lt;strong&gt;activity days&lt;/strong&gt;, not calendar days. If you take a two-week vacation, your memories don't rot. Heavy users and light users experience equivalent freshness relative to their own engagement patterns.&lt;/p&gt; &lt;p&gt;Memories earn their keep:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Access a memory and it strengthens&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Link memories together and hub score rewards well-connected nodes (diminishing returns after 10 inbound links)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;15 activity-day grace period for new memories before decay kicks in&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;~67 activity-day half-life on recency boost&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Temporal multiplier boosts memories with upcoming relevance (events, deadlines)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Formula is a sigmoid over weighted composite of value score, hub score, recency boost, newness boost, temporal multiplier, and expiration trailoff. Full SQL in the repo.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Graph-based memory architecture:&lt;/p&gt; &lt;p&gt;Memories are nodes, relationships are edges.&lt;/p&gt; &lt;p&gt;Design principles:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Non-destructive by default: supersession and splitting don't delete, consolidation archives&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Sparse links over dense links: better to miss weak signals than add noise&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Heal-on-read: dead links cleaned during traversal, not proactively&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Link types&lt;/strong&gt; (LLM-classified, sparse): conflicts, supersedes, causes, instance_of, invalidated_by, motivated_by&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Automatic structural links&lt;/strong&gt; (cheap): was_context_for, shares_entity:{Name} via spaCy NER (runs locally)&lt;/p&gt; &lt;p&gt;Bidirectional storage: every link stored in both directions for efficient traversal without joins.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Memory lifecycle (runs unattended)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;| Job | Interval | Purpose |&lt;/p&gt; &lt;p&gt;|-----|----------|---------|&lt;/p&gt; &lt;p&gt;| Extraction batch polling | 1 min | Check batch status |&lt;/p&gt; &lt;p&gt;| Relationship classification | 1 min | Process new links |&lt;/p&gt; &lt;p&gt;| Failed extraction retry | 6 hours | Retry failures |&lt;/p&gt; &lt;p&gt;| Refinement (split/trim verbose memories) | 7 days | Break up bloated memories |&lt;/p&gt; &lt;p&gt;| Consolidation (merge similar memories) | 7 days | Deduplicate |&lt;/p&gt; &lt;p&gt;| Temporal score recalculation | Daily | Update time-based scores |&lt;/p&gt; &lt;p&gt;| Entity garbage collection | Monthly | Clean orphaned entities |&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Consolidation&lt;/strong&gt; uses two-phase LLM verification: reasoning model proposes, fast model reviews. New memory gets median importance score to prevent inflation. Old memories archived, not deleted.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Splitting&lt;/strong&gt; breaks verbose memories into focused ones. Original stays active, split memories coexist.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Supersession&lt;/strong&gt; creates temporal versioning. New info explicitly updates old, but superseded memories remain active so you can see what changed when.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Domaindocs (persistent knowledge blocks):&lt;/p&gt; &lt;p&gt;Memories decay. Some knowledge shouldn't. Domaindocs are hierarchical, version-controlled text blocks that persist indefinitely.&lt;/p&gt; &lt;p&gt;Token management via collapse/expand:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;MIRA controls its own context by collapsing sections it doesn't need&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Collapsed sections render as header + metadata only&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Large sections (&amp;gt;5000 chars) flagged so MIRA knows the cost before expanding&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;personal_context self-model&lt;/strong&gt;: Auto-created for every user. MIRA documents its own behavioral patterns (agreement bias, helpfulness pressure, confidence theater). Observation-driven, not configuration-driven. MIRA writes documentation about how it actually behaves, then consults that documentation in future conversations.&lt;/p&gt; &lt;p&gt;Collaborative editing with conflict resolution when both user and MIRA edit simultaneously.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Tool context management:&lt;/p&gt; &lt;p&gt;Only three essential tools stay permanently loaded: web_tool, invokeother_tool, getcontext_tool.&lt;/p&gt; &lt;p&gt;All other tools exist as one-line hints in working memory. When MIRA needs capability, it calls invokeother_tool to load the full definition on demand. Loaded tools auto-unload after 5 turns unused (configurable).&lt;/p&gt; &lt;p&gt;With ~15 available tools at 150-400 tokens each, that's 2,250-6,000 tokens not wasted per turn. Smaller context = faster inference on constrained hardware.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Extensibility:&lt;/p&gt; &lt;p&gt;Tools are entirely self-contained: config, schema, and implementation in one file. Extend MIRA by:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; ‚Å†Give Claude Code context about what you want&lt;/li&gt; &lt;li&gt; ‚Å†Drop the new tool in tools/implementations/&lt;/li&gt; &lt;li&gt; ‚Å†Restart the process&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Tool auto-registers on startup. There's a HOW_TO_BUILD_A_TOOL.md written specifically to give Claude the context needed to zero-shot a working tool.&lt;/p&gt; &lt;p&gt;Trinkets (working memory plugins) work the same way.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Segment collapse (&amp;quot;REM sleep&amp;quot;):&lt;/p&gt; &lt;p&gt;Every 5 minutes APScheduler checks for inactive conversation segments. On timeout:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Generate summary + embedding&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Extract tools used&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Submit memory extraction to batch processing&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Clear search results to prevent context leak between segments&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No intervention needed.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;One conversation forever:&lt;/p&gt; &lt;p&gt;There's no &amp;quot;new chat&amp;quot; button. One conversation, continuous. This constraint forced me to actually solve context management instead of letting users reset when things got messy. A new MIRA instance is a blank slate you grow over time.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Token overhead:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;~1,123 token system prompt&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;~8,300 tokens typical full context, ~3,300 cached on subsequent requests&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Content controlled via config limits (20 memories max, 5 rolling summaries max)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/taylorsatula/mira-OSS"&gt;https://github.com/taylorsatula/mira-OSS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you don't want to self-host, there's a web interface at &lt;a href="https://miraos.org"&gt;https://miraos.org&lt;/a&gt; (runs Claude, not local).&lt;/p&gt; &lt;p&gt;Feedback welcome. That is the quickest way to improving software.&lt;/p&gt; &lt;p&gt;NOTE: sorry about the weird markdown adjacent formatting. I post from phone and idk how to do formatting from here. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/awittygamertag"&gt; /u/awittygamertag &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqupew/offlinecapable_scaffolding_with_memory_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqupew/offlinecapable_scaffolding_with_memory_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqupew/offlinecapable_scaffolding_with_memory_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T19:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq2ry0</id>
    <title>Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster</title>
    <updated>2025-12-18T21:28:20+00:00</updated>
    <author>
      <name>/u/geerlingguy</name>
      <uri>https://old.reddit.com/user/geerlingguy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"&gt; &lt;img alt="Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster" src="https://preview.redd.it/32z50w1s518g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be2781529b5cacb7d7a84c794d37a156e1bdc798" title="Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing llama.cpp RPC vs Exo's new RDMA Tensor setting on a cluster of 4x Mac Studios (2x 512GB and 2x 256GB) that Apple loaned me until Februrary.&lt;/p&gt; &lt;p&gt;Would love to do more testing between now and returning it. A lot of the earlier testing was debugging stuff since the RDMA support was very new for the past few weeks... now that it's somewhat stable I can do more.&lt;/p&gt; &lt;p&gt;The annoying thing is there's nothing nice like llama-bench in Exo, so I can't give as direct comparisons with context sizes, prompt processing speeds, etc. (it takes a lot more fuss to do that, at least).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geerlingguy"&gt; /u/geerlingguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/32z50w1s518g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T21:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pr20el</id>
    <title>Japan's Rakuten is going to release a 700B open weight model in Spring 2026</title>
    <updated>2025-12-20T01:29:37+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://news.yahoo.co.jp/articles/0fc312ec3386f87d65e797ab073db56c230757e1"&gt;https://news.yahoo.co.jp/articles/0fc312ec3386f87d65e797ab073db56c230757e1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope it works well in real life. Then it can not only be an alternative to the Chinese models. but also prompt the US companies to release big models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pr20el/japans_rakuten_is_going_to_release_a_700b_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pr20el/japans_rakuten_is_going_to_release_a_700b_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pr20el/japans_rakuten_is_going_to_release_a_700b_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T01:29:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqwegx</id>
    <title>BRAID: Mermaid-based reasoning graphs make agents more accurate and cost-efficient</title>
    <updated>2025-12-19T21:07:52+00:00</updated>
    <author>
      <name>/u/arbayi</name>
      <uri>https://old.reddit.com/user/arbayi</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arbayi"&gt; /u/arbayi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/pdf/2512.15959"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqwegx/braid_mermaidbased_reasoning_graphs_make_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqwegx/braid_mermaidbased_reasoning_graphs_make_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T21:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqjja2</id>
    <title>Gemma Scope 2 is a comprehensive, open suite of sparse autoencoders and transcoders for a range of model sizes and versions in the Gemma 3 model family.</title>
    <updated>2025-12-19T12:09:34+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjja2/gemma_scope_2_is_a_comprehensive_open_suite_of/"&gt; &lt;img alt="Gemma Scope 2 is a comprehensive, open suite of sparse autoencoders and transcoders for a range of model sizes and versions in the Gemma 3 model family." src="https://b.thumbs.redditmedia.com/wfpfrO9KgU8CdhDynFFqinaA1_4j8ZSos50KzQEDVTA.jpg" title="Gemma Scope 2 is a comprehensive, open suite of sparse autoencoders and transcoders for a range of model sizes and versions in the Gemma 3 model family." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma Scope 2: &lt;a href="https://huggingface.co/google/gemma-scope-2"&gt;https://huggingface.co/google/gemma-scope-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Collection: &lt;a href="https://huggingface.co/collections/google/gemma-scope-2"&gt;https://huggingface.co/collections/google/gemma-scope-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Google AI Developers on ùïè: &lt;a href="https://x.com/googleaidevs/status/2001986944687804774"&gt;https://x.com/googleaidevs/status/2001986944687804774&lt;/a&gt;&lt;br /&gt; Blog post: Gemma Scope 2: helping the AI safety community deepen understanding of complex language model behavior: &lt;a href="https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/"&gt;https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pqjja2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjja2/gemma_scope_2_is_a_comprehensive_open_suite_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjja2/gemma_scope_2_is_a_comprehensive_open_suite_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T12:09:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqxng9</id>
    <title>Mistral Vibe CLI update - New modes &amp; UI improvements</title>
    <updated>2025-12-19T22:01:02+00:00</updated>
    <author>
      <name>/u/Nefhis</name>
      <uri>https://old.reddit.com/user/Nefhis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Latest Vibe updates are out. &lt;/p&gt; &lt;p&gt;Following the OCR release, we are also announcing multiple Mistral Vibe updates, among them:&lt;/p&gt; &lt;p&gt;‚Äì Improved UI and multiple UX fixes.&lt;br /&gt; ‚Äì Adding Plan mode and Accept Edit mode.&lt;br /&gt; ‚Äì And multiple other bug fixes and improvements.&lt;/p&gt; &lt;p&gt;Happy shipping!&lt;/p&gt; &lt;p&gt;‚Üí &lt;code&gt;uv tool install mistral-vibe&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1pqxng9/video/t397xl9kg88g1/player"&gt;https://reddit.com/link/1pqxng9/video/t397xl9kg88g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/MistralAI/comments/1ppz50l/mistral_vibe_update/"&gt;https://www.reddit.com/r/MistralAI/comments/1ppz50l/mistral_vibe_update/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="/u/Nefhis"&gt;u/Nefhis&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Mistral AI Ambassador&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nefhis"&gt; /u/Nefhis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqxng9/mistral_vibe_cli_update_new_modes_ui_improvements/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqxng9/mistral_vibe_cli_update_new_modes_ui_improvements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqxng9/mistral_vibe_cli_update_new_modes_ui_improvements/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T22:01:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqm5g4</id>
    <title>Seed OSS 36b made me reconsider my life choices.</title>
    <updated>2025-12-19T14:15:06+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;5AM, - Me: Hello Seed, write me a complete new library does this and that, use that internal library as a reference but extend it to handle more data formats. Unify the data abstraction layer so data from one format can be exported to other format. Analyse the code in the internal lib directory and create a similar library but extended with more data formats to support. Create unit tests. To run the unit tests use the following command ...&lt;br /&gt; - Seed: Hold my Âï§ÈÖí&lt;/p&gt; &lt;p&gt;9AM, - Seed: Crap, dude, the test is failing and Im out of 100k context, help!&lt;br /&gt; - Me: Hold on pal, there you go, quick restart, You were working on this and that, keep going mate. This is the short error log, DON'T copy and paste 100k lines of repeating errors lol&lt;br /&gt; - Seed: Gotcha...&lt;/p&gt; &lt;p&gt;11AM, - Seed: Boom done, not a single f**king error, code is in src, tests are in test, examples are here, and this is some docs for you, stupid human being&lt;br /&gt; - Me: :O&lt;/p&gt; &lt;p&gt;Holy f**k. &lt;/p&gt; &lt;p&gt;Anyone else using seed-oss-36b? I literally downloaded it yesterday, ran the Q6_K_XL quant to fit in the 48GB vram with 100k context at q8. Im speachless. Yes, it is slower than the competitors (devstral? qwen?) but the quality is jaw dropping. Worked for hours, without supervision, and if not the context length it would possibly finish the entire project alone. Wierd that there is so little news about this model. Its stupidly good at agentic coding.&lt;/p&gt; &lt;p&gt;Human coding? RIP 2025&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqm5g4/seed_oss_36b_made_me_reconsider_my_life_choices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqm5g4/seed_oss_36b_made_me_reconsider_my_life_choices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqm5g4/seed_oss_36b_made_me_reconsider_my_life_choices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T14:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqxkag</id>
    <title>Trellis 2 run locally: not easy but possible</title>
    <updated>2025-12-19T21:57:17+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqxkag/trellis_2_run_locally_not_easy_but_possible/"&gt; &lt;img alt="Trellis 2 run locally: not easy but possible" src="https://b.thumbs.redditmedia.com/A968KAG9P0fIhs_yeJnEBKDSUkvbmp7ArjatZ3jkUnI.jpg" title="Trellis 2 run locally: not easy but possible" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0z9hpdvtd88g1.png?width=1874&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dd33a94676780e3cef1aaf4191d5153df5edbd8f"&gt;Local Trellis 2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After yesterday's announcement, I tested the model on Hugging Face. The results are excellent, but obviously&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You can't change the maximum resolution (limited to 1536).&lt;/li&gt; &lt;li&gt;After exporting two files, you have to pay to continue.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I treated myself to a Blackwell 6000 96GB for Christmas and wanted to try running Trellis 2 on Windows. Impossible.&lt;/p&gt; &lt;p&gt;So I tried on WSL, and after many attempts and arguments with the libraries, I succeeded.&lt;/p&gt; &lt;p&gt;I'm posting this to save anyone who wants to try: if you generate 2K (texture) files and 1024 resolution, you can use a graphics card with 16GB of RAM.&lt;/p&gt; &lt;p&gt;It's important not to use flash attention because it simply doesn't work. Used:&lt;/p&gt; &lt;p&gt;__________&lt;/p&gt; &lt;p&gt;cd ~/TRELLIS.2&lt;/p&gt; &lt;p&gt;# Test with xformers&lt;/p&gt; &lt;p&gt;pip install xformers&lt;/p&gt; &lt;p&gt;export ATTN_BACKEND=xformers&lt;/p&gt; &lt;p&gt;python &lt;a href="http://app.py"&gt;app.py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;_________&lt;/p&gt; &lt;p&gt;Furthermore, to avoid errors on Cuda (I used pytorch &amp;quot;pip3 install torch torchvision --index-url &lt;a href="https://download.pytorch.org/whl/cu128%22"&gt;https://download.pytorch.org/whl/cu128&amp;quot;&lt;/a&gt;) you will have to modify the &lt;a href="http://app.py"&gt;app.py&lt;/a&gt; file like this:&lt;/p&gt; &lt;p&gt;_______&lt;/p&gt; &lt;p&gt;cd ~/TRELLIS.2&lt;/p&gt; &lt;p&gt;# 1. Backup the original file&lt;/p&gt; &lt;p&gt;cp &lt;a href="http://app.py"&gt;app.py&lt;/a&gt; app.py.backup&lt;/p&gt; &lt;p&gt;echo &amp;quot;‚úì Backup created: app.py.backup&amp;quot;&lt;/p&gt; &lt;p&gt;# 2. Create the patch script&lt;/p&gt; &lt;p&gt;cat &amp;gt; patch_app.py &amp;lt;&amp;lt; 'PATCH_EOF'&lt;/p&gt; &lt;p&gt;import re&lt;/p&gt; &lt;p&gt;# Read the file&lt;/p&gt; &lt;p&gt;with open('app.py', 'r') as f:&lt;/p&gt; &lt;p&gt;content = f.read()&lt;/p&gt; &lt;p&gt;# Fix 1: Add CUDA pre-init after initial imports&lt;/p&gt; &lt;p&gt;cuda_init = '''&lt;/p&gt; &lt;p&gt;# Pre-initialize CUDA to avoid driver errors on first allocation&lt;/p&gt; &lt;p&gt;import torch&lt;/p&gt; &lt;p&gt;if torch.cuda.is_available():&lt;/p&gt; &lt;p&gt;try:&lt;/p&gt; &lt;p&gt;torch.cuda.init()&lt;/p&gt; &lt;p&gt;_ = torch.zeros(1, device='cuda')&lt;/p&gt; &lt;p&gt;del _&lt;/p&gt; &lt;p&gt;print(f&amp;quot;‚úì CUDA initialized successfully on {torch.cuda.get_device_name(0)}&amp;quot;)&lt;/p&gt; &lt;p&gt;except Exception as e:&lt;/p&gt; &lt;p&gt;print(f&amp;quot;‚ö† CUDA pre-init warning: {e}&amp;quot;)&lt;/p&gt; &lt;p&gt;'''&lt;/p&gt; &lt;p&gt;# Find the first occurrence of &amp;quot;import os&amp;quot; and add the init block after it&lt;/p&gt; &lt;p&gt;if &amp;quot;# Pre-initialize CUDA&amp;quot; not in content:&lt;/p&gt; &lt;p&gt;content = content.replace(&lt;/p&gt; &lt;p&gt;&amp;quot;import os\nos.environ['OPENCV_IO_ENABLE_OPENEXR'] = '1'&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;import os\nos.environ['OPENCV_IO_ENABLE_OPENEXR'] = '1'&amp;quot; + cuda_init,&lt;/p&gt; &lt;p&gt;1&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;print(&amp;quot;‚úì Added CUDA pre-initialization&amp;quot;)&lt;/p&gt; &lt;p&gt;# Fix 2: Modify all direct CUDA allocations&lt;/p&gt; &lt;p&gt;# Pattern: torch.tensor(..., device='cuda')&lt;/p&gt; &lt;p&gt;pattern = r&amp;quot;(torch\.tensor\([^)]+)(device='cuda')&amp;quot;&lt;/p&gt; &lt;p&gt;replacement = r&amp;quot;\1device='cpu').cuda(&amp;quot;&lt;/p&gt; &lt;p&gt;# Count how many replacements will be made&lt;/p&gt; &lt;p&gt;matches = re.findall(pattern, content)&lt;/p&gt; &lt;p&gt;if matches:&lt;/p&gt; &lt;p&gt;content = re.sub(pattern, replacement, content)&lt;/p&gt; &lt;p&gt;print(f&amp;quot;‚úì Fixed {len(matches)} direct CUDA tensor allocations&amp;quot;)&lt;/p&gt; &lt;p&gt;else:&lt;/p&gt; &lt;p&gt;print(&amp;quot;‚ö† No direct CUDA allocations found to fix&amp;quot;)&lt;/p&gt; &lt;p&gt;# Write the modified file&lt;/p&gt; &lt;p&gt;with open('app.py', 'w') as f:&lt;/p&gt; &lt;p&gt;f.write(content)&lt;/p&gt; &lt;p&gt;print(&amp;quot;\n‚úÖ Patch applied successfully!&amp;quot;)&lt;/p&gt; &lt;p&gt;print(&amp;quot;Run: export ATTN_BACKEND=xformers &amp;amp;&amp;amp; python app.py&amp;quot;)&lt;/p&gt; &lt;p&gt;PATCH_EOF&lt;/p&gt; &lt;p&gt;# 3. Run the patch script&lt;/p&gt; &lt;p&gt;python patch_app.py&lt;/p&gt; &lt;p&gt;# 4. Verify the changes&lt;/p&gt; &lt;p&gt;echo &amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;echo &amp;quot;üìã Verifying changes...&amp;quot;&lt;/p&gt; &lt;p&gt;if grep -q &amp;quot;CUDA initialized successfully&amp;quot; &lt;a href="http://app.py"&gt;app.py&lt;/a&gt;; then&lt;/p&gt; &lt;p&gt;echo &amp;quot;‚úì CUDA pre-init added&amp;quot;&lt;/p&gt; &lt;p&gt;else&lt;/p&gt; &lt;p&gt;echo &amp;quot;‚úó CUDA pre-init not found&amp;quot;&lt;/p&gt; &lt;p&gt;fi&lt;/p&gt; &lt;p&gt;if grep -q &amp;quot;device='cpu').cuda()&amp;quot; &lt;a href="http://app.py"&gt;app.py&lt;/a&gt;; then&lt;/p&gt; &lt;p&gt;echo &amp;quot;‚úì CUDA allocations modified&amp;quot;&lt;/p&gt; &lt;p&gt;else&lt;/p&gt; &lt;p&gt;echo &amp;quot;‚ö† No allocations modified (this might be OK)&amp;quot;&lt;/p&gt; &lt;p&gt;fi&lt;/p&gt; &lt;p&gt;# 5. Cleanup&lt;/p&gt; &lt;p&gt;rm patch_app.py&lt;/p&gt; &lt;p&gt;echo &amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;echo &amp;quot;‚úÖ Completed! Now run:&amp;quot;&lt;/p&gt; &lt;p&gt;echo &amp;quot; export ATTN_BACKEND=xformers&amp;quot;&lt;/p&gt; &lt;p&gt;echo &amp;quot; python app.py&amp;quot;&lt;/p&gt; &lt;p&gt;________&lt;/p&gt; &lt;p&gt;These changes will save you a few hours of work. The rest of the instructions are available on GitHub. However, you'll need to get huggingface access to some spaces that require registration. Then, set up your token in WSL for automatic downloads. I hope this was helpful. If you want to increase resolution: change it on &lt;a href="http://app.py"&gt;app.py&lt;/a&gt; --&amp;gt; # resolution_options = [512, 1024, 1536, 2048]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqxkag/trellis_2_run_locally_not_easy_but_possible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqxkag/trellis_2_run_locally_not_easy_but_possible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqxkag/trellis_2_run_locally_not_easy_but_possible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T21:57:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqyphk</id>
    <title>Framework says that a single AI datacenter consumes enough memory for millions of laptops</title>
    <updated>2025-12-19T22:46:46+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quote: the boom in AI data center construction and server manufacturing is consuming immense amounts of memory. A single rack of NVIDIA‚Äôs GB300 solution uses 20TB of HBM3E and 17TB of LPDDR5X. That‚Äôs enough LPDDR5x for a thousand laptops, and an &lt;a href="https://en.wikipedia.org/wiki/AI_datacenter"&gt;AI-focused datacenter&lt;/a&gt; is loaded with thousands of these racks!&lt;/p&gt; &lt;p&gt;/end quote&lt;/p&gt; &lt;p&gt;thousand * thousands = millions&lt;/p&gt; &lt;p&gt;&lt;a href="https://frame.work/pl/en/blog/updates-on-memory-pricing-and-navigating-the-volatile-memory-market"&gt;https://frame.work/pl/en/blog/updates-on-memory-pricing-and-navigating-the-volatile-memory-market&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The good news: there hasn't been new recent price increase for strix halo systems, but there was some 8 weeks in response to U.S. tariff increases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqyphk/framework_says_that_a_single_ai_datacenter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqyphk/framework_says_that_a_single_ai_datacenter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqyphk/framework_says_that_a_single_ai_datacenter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T22:46:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqt04l</id>
    <title>Tutorial on finetuning Gemma3 1B to generate 3D objects</title>
    <updated>2025-12-19T18:48:27+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past 6 weeks, I have been spending time finetuning Gemma3 1B to generate OpenSCAD code.&lt;/p&gt; &lt;p&gt;There is almost no good dataset nor evaluation framework available. But I think it worked out well with synthetic data generation + careful finetuning.&lt;/p&gt; &lt;p&gt;I put together a quick guide, lmk if it's helpful!&lt;/p&gt; &lt;p&gt;Have a good weekend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://starmind.comfyspace.tech/experiments/cadmonkey-v2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqt04l/tutorial_on_finetuning_gemma3_1b_to_generate_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqt04l/tutorial_on_finetuning_gemma3_1b_to_generate_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T18:48:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqoldt</id>
    <title>Chinese researchers unveil "LightGen": An all-optical chip that outperforms Nvidia‚Äôs A100 by 100x</title>
    <updated>2025-12-19T15:55:23+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New research from SJTU and Tsinghua (these are top tier labs, not slopmonsters like East China Normal University etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.science.org/doi/10.1126/science.adv7434"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoldt/chinese_researchers_unveil_lightgen_an_alloptical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoldt/chinese_researchers_unveil_lightgen_an_alloptical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T15:55:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqy2bq</id>
    <title>Devstral 2 (with Mistral's Vibe) vs Sonnet 4.5 (Claude Code) on SWE-bench: 37.6% vs 39.8% (within statistical error)</title>
    <updated>2025-12-19T22:18:46+00:00</updated>
    <author>
      <name>/u/Constant_Branch282</name>
      <uri>https://old.reddit.com/user/Constant_Branch282</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Update: Just discovered my script wasn't passing the --model flag correctly. Claude Code was using automatic model selection (typically Opus), not Sonnet 4.5 as I stated. This actually makes the results more significant - Devstral 2 matched Anthropic's best model in my test, not just Sonnet&lt;/p&gt; &lt;p&gt;I ran Mistral's Vibe (Devstral 2) against Claude Code (Sonnet 4.5) on SWE-bench-verified-mini - 45 real GitHub issues, 10 attempts each, 900 total runs.&lt;/p&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;p&gt;Claude Code (Sonnet 4.5) : 39.8% (37.3% - 42.2%)&lt;/p&gt; &lt;p&gt;Vibe (Devstral 2): 37.6% (35.1% - 40.0%)&lt;/p&gt; &lt;p&gt;The gap is within statistical error. An open-weight model I can run on my Strix Halo is matching Anthropic's recent model.&lt;/p&gt; &lt;p&gt;Vibe was also faster - 296s mean vs Claude's 357s.&lt;/p&gt; &lt;p&gt;The variance finding (applies to both): about 40% of test cases were inconsistent across runs. Same agent, same bug, different outcomes. Even on cases solved 10/10, patch sizes varied up to 8x.&lt;/p&gt; &lt;p&gt;Full writeup with charts and methodology: &lt;a href="https://blog.kvit.app/posts/variance-claude-vibe/"&gt;https://blog.kvit.app/posts/variance-claude-vibe/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Constant_Branch282"&gt; /u/Constant_Branch282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqy2bq/devstral_2_with_mistrals_vibe_vs_sonnet_45_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqy2bq/devstral_2_with_mistrals_vibe_vs_sonnet_45_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqy2bq/devstral_2_with_mistrals_vibe_vs_sonnet_45_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T22:18:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqpj29</id>
    <title>Career Advice in AI ‚Äî Notes from an Andrew Ng Lecture</title>
    <updated>2025-12-19T16:31:46+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqpj29/career_advice_in_ai_notes_from_an_andrew_ng/"&gt; &lt;img alt="Career Advice in AI ‚Äî Notes from an Andrew Ng Lecture" src="https://preview.redd.it/cu5vt8lnt68g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efe9182c042a4013c8b0a1760f4ff5a1ac022efd" title="Career Advice in AI ‚Äî Notes from an Andrew Ng Lecture" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;[1] A Golden Age for AI Careers&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Andrew Ng emphasizes that this is the best time ever to build a career in AI. He notes that the complexity of tasks AI can handle is doubling approximately every seven months, meaning progress is accelerating, not slowing down.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[2] The Power of AI Coding Tools&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Staying on the ‚Äúfrontier‚Äù of coding tools (like Cursor, Claude, and Gemini) is crucial. Being even half a generation behind in your tooling makes you significantly less productive in the current market.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[3] The ‚ÄúProduct Management Bottleneck‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Because AI has made writing code so much cheaper and faster, the bottleneck has shifted to deciding what to build. Engineers who can talk to users, develop empathy, and handle product management (PM) tasks are the fastest-moving individuals in Silicon Valley today.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[4] Surround Yourself with the Right People&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Success is highly predicted by the people you surround yourself with. Ng encourages building a ‚Äúrich connective tissue‚Äù of friends and colleagues to share insights that aren‚Äôt yet published on the internet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[5] Team Over Brand&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;When job hunting, the specific team and people you work with day-to-day are more important than the company‚Äôs ‚Äúhot brand.‚Äù Avoid companies that refuse to tell you which team you will join before you sign.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[6] Go and Build Stuff&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Andrew Ng‚Äôs number one piece of advice is to simply &lt;strong&gt;go and build stuff&lt;/strong&gt;. The cost of failure is low (losing a weekend), but the learning and demonstration of skill are invaluable.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[7] The Value of Hard Work&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Andrew Ng encourages working hard, defining it not just by hours but by output and passion for building.&lt;/p&gt; &lt;p&gt;Video - &lt;a href="https://www.youtube.com/watch?v=AuZoDsNmG_s"&gt;https://www.youtube.com/watch?v=AuZoDsNmG_s&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cu5vt8lnt68g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqpj29/career_advice_in_ai_notes_from_an_andrew_ng/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqpj29/career_advice_in_ai_notes_from_an_andrew_ng/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T16:31:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqn0vq</id>
    <title>GLM 4.7 is Coming?</title>
    <updated>2025-12-19T14:52:18+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqn0vq/glm_47_is_coming/"&gt; &lt;img alt="GLM 4.7 is Coming?" src="https://preview.redd.it/206mfj3dc68g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f01e8b54d3347827d6980eb1b0cbc7453cfd2d9c" title="GLM 4.7 is Coming?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/30876"&gt;https://github.com/vllm-project/vllm/pull/30876&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/206mfj3dc68g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqn0vq/glm_47_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqn0vq/glm_47_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T14:52:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqui9l</id>
    <title>FlashHead: Up to 50% faster token generation on top of other techniques like quantization</title>
    <updated>2025-12-19T19:49:00+00:00</updated>
    <author>
      <name>/u/Any_Frame9721</name>
      <uri>https://old.reddit.com/user/Any_Frame9721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqui9l/flashhead_up_to_50_faster_token_generation_on_top/"&gt; &lt;img alt="FlashHead: Up to 50% faster token generation on top of other techniques like quantization" src="https://external-preview.redd.it/_fiYnJOiLFMjXWAHyOC4PQuzh6t1PbyOv347pBjd4tc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79b959a9401497de2b2ba109df8da0de73c9e45f" title="FlashHead: Up to 50% faster token generation on top of other techniques like quantization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We have developed FlashHead, an architectural innovation for SLMs offering up to 50% more tokens per second &lt;strong&gt;on top&lt;/strong&gt; of other techniques like quantization. It is a drop-in replacement for the language model head. It works by replacing the expensive lm head with the FlashHead layer that uses information retrieval to identify the next token efficiently with perfect accuracy compared to the baseline model.&lt;/p&gt; &lt;p&gt;Try it with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install embedl-models python -m embedl.models.vllm.demo \ --model embedl/Llama-3.2-3B-Instruct-FlashHead-W4A16 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Llama 3.2 1B Instruct benchmark on Ada Gen 3500 GPU (batch size = 1)&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/embedl/Llama-3.2-1B-Instruct-FlashHead#token-generation-speed-rtx-3500-ada-batch-size--1"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Precision&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Tokens/sec&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Speedup vs BF16&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;BF16 baseline&lt;/td&gt; &lt;td align="left"&gt;130&lt;/td&gt; &lt;td align="left"&gt;1.0√ó&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;FlashHead (Embedl)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;163&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1.25√ó&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;W4A16 baseline&lt;/td&gt; &lt;td align="left"&gt;278&lt;/td&gt; &lt;td align="left"&gt;2.14√ó&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;FlashHead W4A16 (Embedl)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;485&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3.73√ó&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The models perform as their original counterparts, but faster. We have tried to make it as friction-less as possible to use via our vLLM integration, we would love to hear feedback. The GitHub repo is &lt;a href="https://github.com/embedl/embedl-models"&gt;https://github.com/embedl/embedl-models&lt;/a&gt;, &lt;/p&gt; &lt;p&gt;We are a Swedish startup working on efficient AI. We also have a free Edge AI Hub that allows users to run models on mobile devices (Android, iOS) &lt;a href="https://hub.embedl.com"&gt;https://hub.embedl.com&lt;/a&gt; , feel free to join our Slack (#llm channel) for discussions or open an issue on GitHub&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Frame9721"&gt; /u/Any_Frame9721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/embedl/models"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqui9l/flashhead_up_to_50_faster_token_generation_on_top/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqui9l/flashhead_up_to_50_faster_token_generation_on_top/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T19:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqegcr</id>
    <title>Realist meme of the year!</title>
    <updated>2025-12-19T06:49:54+00:00</updated>
    <author>
      <name>/u/Slight_Tone_2188</name>
      <uri>https://old.reddit.com/user/Slight_Tone_2188</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"&gt; &lt;img alt="Realist meme of the year!" src="https://preview.redd.it/8oge3a2by38g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4697e9a87c50f3f170db7e87eccd27363c505dc" title="Realist meme of the year!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slight_Tone_2188"&gt; /u/Slight_Tone_2188 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8oge3a2by38g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T06:49:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqoi6i</id>
    <title>Qwen released Qwen-Image-Layered on Hugging face.</title>
    <updated>2025-12-19T15:51:45+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoi6i/qwen_released_qwenimagelayered_on_hugging_face/"&gt; &lt;img alt="Qwen released Qwen-Image-Layered on Hugging face." src="https://b.thumbs.redditmedia.com/WT_uezmugp_bMYr9okz4OYqH1W02XtM64SzwTE-NCms.jpg" title="Qwen released Qwen-Image-Layered on Hugging face." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Layered"&gt;https://huggingface.co/Qwen/Qwen-Image-Layered&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Photoshop-grade layering Physically isolated RGBA layers with true native editability Prompt-controlled structure Explicitly specify 3‚Äì10 layers ‚Äî from coarse layouts to fine-grained details Infinite decomposition Keep drilling down: layers within layers, to any depth of detail&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pqoi6i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoi6i/qwen_released_qwenimagelayered_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoi6i/qwen_released_qwenimagelayered_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T15:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We‚Äôre the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We‚Äôre excited to be here to talk all things SAM (sorry, we can‚Äôt share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: Thanks to everyone who joined the AMA and for all the great conversation. We look forward to the next one!&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
</feed>
