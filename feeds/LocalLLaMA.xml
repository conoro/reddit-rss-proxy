<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-03T13:13:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q2t0tn</id>
    <title>Debate Hall mcp server - multi-agent decision making tool (open sourced. please try it out)</title>
    <updated>2026-01-03T12:05:00+00:00</updated>
    <author>
      <name>/u/sbuswell</name>
      <uri>https://old.reddit.com/user/sbuswell</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I built an MCP server that orchestrates structured debates between three cognitive perspectives (Wind/Wall/Door) to help make better decisions. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/elevanaltd/debate-hall-mcp"&gt;https://github.com/elevanaltd/debate-hall-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;THE PROBLEMS&lt;/strong&gt;&lt;br /&gt; 1. When approaching problems, a single review and answers from AI, even when asking them to explore edges/alternatives, doesn't always give the same level of depth as a multi-agent debate would, especially if you're using different models.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In my workflow, the reviewing agent would block the coding agent. This is good, but binary (Yes/No). &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;THE FIX&lt;/strong&gt;&lt;br /&gt; Research from &lt;strong&gt;SWE-Agent&lt;/strong&gt; and &lt;strong&gt;Reflection Patterns&lt;/strong&gt; shows that accuracy improves significantly when agents &lt;strong&gt;debate&lt;/strong&gt;. So I created a debate-hall, and based it on three different types of agent, modelled from Plato's 3 modes of reasoning:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PATHOS (the wind) - The &amp;quot;what if...&amp;quot; voice. Explores the possibilities.&lt;/li&gt; &lt;li&gt;ETHOS (the wall) - The &amp;quot;yes, but...&amp;quot; voice. Grounds and tests against reality.&lt;/li&gt; &lt;li&gt;LOGOS (the door) - The &amp;quot;third way...&amp;quot; voice. Finds the middle ground and synthesises into actions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Essentially, you get AI to fly high as the wind, then block as a wall and ground everything to boring status quo, then get them to find the door that lets the wind through. That's how I visualise it and it seems to work and be understood well by LLMs.&lt;/p&gt; &lt;p&gt;I find the tension between these perspectives offers way better solutions than just asking agents to come up with things. Innovation seems to lie in that sweet spot between wind and wall.&lt;/p&gt; &lt;p&gt;I've created standard agents as well as versions that find hidden vectors or converge to minimal solutions and the debate-hall skill you can use has different patterns the agents use depending on complexity of the problem.&lt;/p&gt; &lt;p&gt;I've set it up as standard to use Gemini for PATHOS agents, Codex for ETHOS agents and Claude for LOGOS agents, but you can configure however you want.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HOW IT WORKS&lt;/strong&gt;&lt;br /&gt; Pretty simple really. just install it and copy the debate-hall skill to your skills folder and the agent prompts to your agents folder. You can have the same agent simulate each, use subagents, or use different models as I do, using &lt;a href="https://github.com/BeehiveInnovations/pal-mcp-server"&gt;https://github.com/BeehiveInnovations/pal-mcp-server&lt;/a&gt; or any other multi-model platform.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install debate-hall-mcp &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run &lt;a href="http://setup-mcp.sh"&gt;setup-mcp.sh&lt;/a&gt; and configure to Claude, Codex or Gemini.&lt;/p&gt; &lt;p&gt;It works with any mcp client.&lt;/p&gt; &lt;p&gt;Then just either instruct the agent to have a debate or&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FEATURES&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hash chain verification - tamper-evident audit trail&lt;/li&gt; &lt;li&gt;GitHub integration - sync debates to Discussions, auto-generate ADRs&lt;/li&gt; &lt;li&gt;Flexible modes - fixed sequence or mediated (orchestrator picks)&lt;/li&gt; &lt;li&gt;Hard limits - debates guaranteed to terminate (no infinite loops)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Optional: OCTAVE Format&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For those into semantic compression, debate-hall-mcp can export transcripts in OCTAVE format - which is a structured notation I've created that's optimised for LLM consumption. you can get it here - &lt;a href="https://github.com/elevanaltd/octave-mcp"&gt;https://github.com/elevanaltd/octave-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FEEDBACK&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This started as an internal tool but I want to open-source and see if it's useful for others. Any feedback or areas to improve would be really useful. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does the Wind/Wall/Door pattern resonate with you/your agents?&lt;/li&gt; &lt;li&gt;Is it easy to use/understand?&lt;/li&gt; &lt;li&gt;Any rough edges in the docs?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any feedback or opinions on this welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sbuswell"&gt; /u/sbuswell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2t0tn/debate_hall_mcp_server_multiagent_decision_making/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2t0tn/debate_hall_mcp_server_multiagent_decision_making/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2t0tn/debate_hall_mcp_server_multiagent_decision_making/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T12:05:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2trfm</id>
    <title>LocalAI Scanning PDFs??</title>
    <updated>2026-01-03T12:45:41+00:00</updated>
    <author>
      <name>/u/gnerfed</name>
      <uri>https://old.reddit.com/user/gnerfed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a bit lost an new to all of this. I have LocalAI installed and working via docker but I cannot seem to get either a normal image or an AIO to read and analyze data in a PDF. Any Googling for help with LocalAI doesn't result in much other than the Docs and RTFM isn't getting me there either. &lt;/p&gt; &lt;p&gt;Can someone point me in the right direction? What terms do I need to research‚Äã? Do I need a specific back end? Is there a way ‚Äã‚Äãto point it at a directory and have it read and analyze everything in the directory?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gnerfed"&gt; /u/gnerfed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2trfm/localai_scanning_pdfs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2trfm/localai_scanning_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2trfm/localai_scanning_pdfs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T12:45:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2rqom</id>
    <title>Local programming vs cloud</title>
    <updated>2026-01-03T10:50:17+00:00</updated>
    <author>
      <name>/u/Photo_Sad</name>
      <uri>https://old.reddit.com/user/Photo_Sad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm personally torn.&lt;br /&gt; Not sure if going 1 or 2 NV 96GB cards is even worth it. Seems that having 96 or 192 doesn't change much effectively compared to 32GB if one wants to run a local model for coding to avoid cloud - cloud being so much better in quality and speed.&lt;br /&gt; Going for 1TB local RAM and do CPU inference might pay-off, but also not sure about model quality.&lt;/p&gt; &lt;p&gt;Any experience by anyone here doing actual pro use at job with os models?&lt;br /&gt; Do 96 or 192 GB VRAM change anything meaningfully?&lt;br /&gt; Is 1TB CPU inference viable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Photo_Sad"&gt; /u/Photo_Sad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2rqom/local_programming_vs_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2rqom/local_programming_vs_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2rqom/local_programming_vs_cloud/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T10:50:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2eau3</id>
    <title>My New Year's resolution was to add Docker support. Only 2 days late. Audiobook Maker v1.1.0</title>
    <updated>2026-01-02T23:32:12+00:00</updated>
    <author>
      <name>/u/DigiJoe79</name>
      <uri>https://old.reddit.com/user/DigiJoe79</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2eau3/my_new_years_resolution_was_to_add_docker_support/"&gt; &lt;img alt="My New Year's resolution was to add Docker support. Only 2 days late. Audiobook Maker v1.1.0" src="https://a.thumbs.redditmedia.com/TNdBuFemSJ4jRJ66xw8XJAlZ-H0HvTXr_8DHB2mjLq8.jpg" title="My New Year's resolution was to add Docker support. Only 2 days late. Audiobook Maker v1.1.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;About three weeks ago I shared my passion project here - an app to create audiobooks from text using local TTS engines like XTTS and Chatterbox. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1piduwm/i_wanted_audiobooks_of_stories_that_dont_exist_so/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1piduwm/i_wanted_audiobooks_of_stories_that_dont_exist_so/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The response was amazing and motivated me to keep going. Special shoutout to &lt;a href="https://github.com/codesterribly"&gt;https://github.com/codesterribly&lt;/a&gt; who pushed me to tackle Docker support - you were right, it was worth it!&lt;/p&gt; &lt;p&gt;So here's my slightly-late New Year's gift to the community: v1.1.0 üéÅ&lt;/p&gt; &lt;p&gt;What's New?&lt;/p&gt; &lt;p&gt;Docker-First Architecture&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No more Python environment hell! Engines come as prebuilt Docker images&lt;/li&gt; &lt;li&gt;One-click installation from the online catalog&lt;/li&gt; &lt;li&gt;Works on Windows, Linux, and partially with macOS (Apple Silicon)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Remote GPU Offloading&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Got a beefy GPU server in your closet? Run VibeVoice 7B there via SSH&lt;/li&gt; &lt;li&gt;Your laptop stays cool while the server does the heavy lifting&lt;/li&gt; &lt;li&gt;Built-in SSH key wizard - no manual config needed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;New TTS Engine: VibeVoice&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Microsoft's long-form multi-speaker TTS&lt;/li&gt; &lt;li&gt;Great for podcasts and dialogues&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Quick Start&lt;/p&gt; &lt;h1&gt;Pull the backend&lt;/h1&gt; &lt;p&gt;docker pull ghcr.io/digijoe79/audiobook-maker/backend:latest&lt;/p&gt; &lt;h1&gt;Run it&lt;/h1&gt; &lt;p&gt;&lt;code&gt;docker run -d --name audiobook-maker-backend \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-p 8765:8765 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--add-host=host.docker.internal:host-gateway \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-e DOCKER_ENGINE_HOST=host.docker.internal \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-v /var/run/docker.sock:/var/run/docker.sock \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-v audiobook-data-path:/app/data \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-v audiobook-media-path:/app/media \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ghcr.io/digijoe79/audiobook-maker/backend:latest&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then grab the desktop app, connect, and install engines from the catalog. That's it!&lt;/p&gt; &lt;p&gt;Links&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/DigiJoe79/audiobook-maker"&gt;https://github.com/DigiJoe79/audiobook-maker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/DigiJoe79/audiobook-maker/releases"&gt;https://github.com/DigiJoe79/audiobook-maker/releases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/DigiJoe79/audiobook-maker/tree/main/docs/samples"&gt;https://github.com/DigiJoe79/audiobook-maker/tree/main/docs/samples&lt;/a&gt; (Moby Dick previews)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What's Next?&lt;/p&gt; &lt;p&gt;Already thinking about v1.2.0 - better batch processing, more for Apple Silicon. Open to suggestions!&lt;/p&gt; &lt;p&gt;Thanks again for all the feedback on the original post. This community is awesome. üôè&lt;/p&gt; &lt;p&gt;Happy (belated) New Year, and happy listening!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DigiJoe79"&gt; /u/DigiJoe79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q2eau3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2eau3/my_new_years_resolution_was_to_add_docker_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2eau3/my_new_years_resolution_was_to_add_docker_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T23:32:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q21zql</id>
    <title>Industry Update: Supermicro Policy on Standalone Motherboards Sales Discontinued ‚Äî Spectrum Sourcing</title>
    <updated>2026-01-02T15:47:29+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21zql/industry_update_supermicro_policy_on_standalone/"&gt; &lt;img alt="Industry Update: Supermicro Policy on Standalone Motherboards Sales Discontinued ‚Äî Spectrum Sourcing" src="https://external-preview.redd.it/hL9IlSGjDxXkUJI7Ss74xpCqhzcKCtv4f-p8hzWkQ_U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=704790efb261d2a51abf0f339a4ace74d00ec32c" title="Industry Update: Supermicro Policy on Standalone Motherboards Sales Discontinued ‚Äî Spectrum Sourcing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This isn't new, but somehow I missed it, and figure many in this community might also not be aware of this.&lt;/p&gt; &lt;p&gt;The TLDR, as the title says: Supermicro is stopping standalone motherboard sales and now selling only entire servers. As if things weren't already bad enough...&lt;/p&gt; &lt;p&gt;I had noticed an uptick in used board prices on ebay, local ads, and tech forums but didn't have an explanation for it. This explains why.&lt;/p&gt; &lt;p&gt;While most discussions in this community center around consumer boards, workstation and server boards offer so many more features and functionality, and used to be much cheaper than their desktop counterparts.&lt;/p&gt; &lt;p&gt;Supermicro was arguably the largest supplier of such boards, and with them stopping motherboard sales, all workstation and server boards in standard industry form-factor (EATX, ATX, MATX, IT, and SSE variants) will have a sharp drop in availability in the foreseeable future.&lt;/p&gt; &lt;p&gt;Add to that the sharp increase in RAM prices, and you can see why many businesses will be hesitant to move to newer DDR5 server platforms and instead choose to stock to DDR4 platforms to reuse their existing memory. I suspect many will consolidate their existing DDR4 based Xeon and early Epyc (Naples) to Epyc Milan servers using existing market supply of servers and boards.&lt;/p&gt; &lt;p&gt;We're barely in 2026, but it's looking like this year will squeeze us, consumer, even more than 2025 has.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.spectrumsourcing.com/spectrum-news-feed/industry-update-supermicro-policy-on-standalone-motherboards-sales-discontinued"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21zql/industry_update_supermicro_policy_on_standalone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q21zql/industry_update_supermicro_policy_on_standalone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T15:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2pqdd</id>
    <title>LLMeQueue: let me queue LLM requests from my GPU - local or over the internet</title>
    <updated>2026-01-03T08:46:47+00:00</updated>
    <author>
      <name>/u/PromptAndHope</name>
      <uri>https://old.reddit.com/user/PromptAndHope</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Ever wondered how you can make the most of your own GPU for your online projects and tasks? Since I have an NVIDIA GPU (5060Ti) available locally, I was thinking about setting up a lightweight public server that only receives requests, while a locally running worker connects to it, processes the requests using the GPU, and sends the results back to the server.&lt;/p&gt; &lt;p&gt;You can find the code here: &lt;a href="https://github.com/gszecsenyi/LLMeQueue"&gt;https://github.com/gszecsenyi/LLMeQueue&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The worker is capable of handling both embedding generation and chat completions concurrently in OpenAI API format. By default, the model used is &lt;code&gt;llama3.2:3b&lt;/code&gt;, but a different model can be specified per request, as long as it is available in the worker‚Äôs Ollama container or local Ollama installation. All inference and processing are handled by Ollama running on the worker.&lt;/p&gt; &lt;p&gt;The original idea was that I could also process the requests myself - essentially a &amp;quot;let me queue&amp;quot; approach - which is where the name &lt;strong&gt;LLMeQueue&lt;/strong&gt; comes from.&lt;/p&gt; &lt;p&gt;Any feedback or ideas are welcome, and I would especially appreciate it if you could star the GitHub repository.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PromptAndHope"&gt; /u/PromptAndHope &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pqdd/llmequeue_let_me_queue_llm_requests_from_my_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pqdd/llmequeue_let_me_queue_llm_requests_from_my_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pqdd/llmequeue_let_me_queue_llm_requests_from_my_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1w1qj</id>
    <title>Most optimal vram/performance per price and advice for Shenzhen GPU market</title>
    <updated>2026-01-02T11:14:30+00:00</updated>
    <author>
      <name>/u/notafakename10</name>
      <uri>https://old.reddit.com/user/notafakename10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/"&gt; &lt;img alt="Most optimal vram/performance per price and advice for Shenzhen GPU market" src="https://preview.redd.it/4nfcarq96xag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34f4fbc2fba6317c3d5435a92332540815eb3714" title="Most optimal vram/performance per price and advice for Shenzhen GPU market" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm in Shanghai at the moment and heading to Shenzhen soon - I‚Äôve got around $1500-3000 USD to get the most optimal setup possible. The people I am with are great at negotiating (natives, speak the language) I just need to figure out what I want‚Ä¶ &lt;/p&gt; &lt;p&gt;I main use local models I would want at least 48gb vram, ideally closer to 96gb an at least some grunt for the odd PyTorch model training run. I‚Äôm open to modded cards (one of my current front runners is 4x 3080 20gb cards) open to both AMD and domestic / enterprise cards. &lt;/p&gt; &lt;p&gt;Prices are best estimates from deep seek - could be wildly wrong, anyone had experience navigating the GPU markets? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notafakename10"&gt; /u/notafakename10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4nfcarq96xag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T11:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2lm5a</id>
    <title>Chinny ‚Äî the unlimited, on-device voice cloner ‚Äî just dropped on iOS! (macOS version pending review üëÄ)</title>
    <updated>2026-01-03T04:59:02+00:00</updated>
    <author>
      <name>/u/Tingxiaojue</name>
      <uri>https://old.reddit.com/user/Tingxiaojue</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2lm5a/chinny_the_unlimited_ondevice_voice_cloner_just/"&gt; &lt;img alt="Chinny ‚Äî the unlimited, on-device voice cloner ‚Äî just dropped on iOS! (macOS version pending review üëÄ)" src="https://external-preview.redd.it/iNDM-52klhSKPCGjZoP0ODll_caIe6IsMg4zOI4TGkk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cee819517aa5be7acd89d5a70276ad6aa1bdf99" title="Chinny ‚Äî the unlimited, on-device voice cloner ‚Äî just dropped on iOS! (macOS version pending review üëÄ)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chinny is an on-device voice cloning app for iOS and macOS, powered by a SoTA AI voice-cloning model (Chatterbox). It runs fully offline with no information leaving your device. &lt;strong&gt;No ads. No registration. No permission required. No network connectivity.&lt;/strong&gt; &lt;strong&gt;No hidden fees. No usage restrictions. Free forever.&lt;/strong&gt; Use it to have a familiar voice read bedtime stories, record personal audiobooks, add voiceovers for videos, generate podcast narration, create game or film temp lines, or provide accessible read-aloud for long articles‚Äîall privately on your device.&lt;/p&gt; &lt;p&gt;You can try the iOS and Mac version at &lt;a href="https://apps.apple.com/us/app/chinny-offline-voice-cloner/id6753816417"&gt;https://apps.apple.com/us/app/chinny-offline-voice-cloner/id6753816417&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Require 3 GB RAM for inference, 3.41 GB space because all models are packed inside the app.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; (1) (You can run a quick test from menu-&amp;gt;multi spkear. If you hit generate and it shows &lt;strong&gt;&amp;quot;Exception during initlization std::bad_alloc&amp;quot;&lt;/strong&gt;, this suggests your iPhone doesn't have enough memory) (2) If it &lt;strong&gt;crashed&lt;/strong&gt;, it is more likely because your phone doesn't have enough memory. You can try with another phone, like iPhone 16 Pro or iPhone 17 Pro.&lt;/p&gt; &lt;p&gt;If you want to clone your voice, prepare a clean voice sample of at least 10 seconds in mp3, wav, or m4a format.&lt;/p&gt; &lt;p&gt;PS: I've anonymized the voice source data to comply with App Store policies&lt;/p&gt; &lt;p&gt;All I need is feedback and reviews on App store!&lt;/p&gt; &lt;p&gt;Happy new year and best wishes to you and your family :).&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1q2lm5a/video/4daiccsff2bg1/player"&gt;Chinny: offline voice Cloner&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tingxiaojue"&gt; /u/Tingxiaojue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2lm5a/chinny_the_unlimited_ondevice_voice_cloner_just/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2lm5a/chinny_the_unlimited_ondevice_voice_cloner_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2lm5a/chinny_the_unlimited_ondevice_voice_cloner_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T04:59:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2jazd</id>
    <title>Part 4 (Finale): Building LLMs from Scratch ‚Äì Evaluation &amp; Deployment [Follow-up to Parts 1, thru 3]</title>
    <updated>2026-01-03T03:10:03+00:00</updated>
    <author>
      <name>/u/amitbahree</name>
      <uri>https://old.reddit.com/user/amitbahree</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Happy new year! I‚Äôm excited to share &lt;strong&gt;Part 4&lt;/strong&gt; (and the final part) of my series on building an LLM from scratch.&lt;/p&gt; &lt;p&gt;This installment covers the ‚Äúokay, but does it &lt;em&gt;work&lt;/em&gt;?‚Äù phase: evaluation, testing, and deployment - taking the trained models from Part 3 and turning them into something you can validate, iterate on, and actually share/use (including publishing to HF).&lt;/p&gt; &lt;p&gt;What you‚Äôll find inside:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A practical evaluation framework (quick vs comprehensive) for historical language models (not just perplexity).&lt;/li&gt; &lt;li&gt;Tests and validation patterns: historical accuracy checks, linguistic checks, temporal consistency, and basic performance sanity checks.&lt;/li&gt; &lt;li&gt;Deployment paths: &lt;ul&gt; &lt;li&gt;local inference from PyTorch checkpoints&lt;/li&gt; &lt;li&gt;Hugging Face Hub publishing + model cards&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;CI-ish smoke checks you can run on CPU to catch obvious regressions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why it matters?&lt;br /&gt; Training is only half the battle. Without evaluation + tests + a repeatable publishing workflow, you can easily end up with a model that ‚Äútrains fine‚Äù but is unreliable, inconsistent, or impossible for others to reproduce/use. This post focuses on making the last mile boring (in the best way).&lt;/p&gt; &lt;p&gt;Resources:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üîó Blog post (Part 4) - &lt;a href="https://blog.desigeek.com/post/2026/01/building-llm-from-scratch-part4-evaluation-deployment/"&gt;Evaluations and Deployment&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó GitHub repo: &lt;a href="https://github.com/bahree/helloLondon"&gt;https://github.com/bahree/helloLondon&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó Hugging Face: &lt;a href="https://huggingface.co/bahree"&gt;https://huggingface.co/bahree&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In case you are interested in the previous parts&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üîó Part 3 - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oluay3/part_3_building_llms_from_scratch_model/"&gt;Model Architecture &amp;amp; GPU Training&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó Part 2 - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o562l3/part_2_building_llms_from_scratch_data_collection/"&gt;Data Collection &amp;amp; Custom Tokenizers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó Part 1 - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1npzstw/a_step_by_step_guide_on_how_to_build_a_llm_from/"&gt;Quick Start &amp;amp; Overview&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üîó LinkedIn &lt;a href="https://www.linkedin.com/posts/amitbahree_building-llms-from-scratch-part-4-evaluation-activity-7413050136974700544-0OwB/"&gt;post&lt;/a&gt; (if that is your thing).&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amitbahree"&gt; /u/amitbahree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jazd/part_4_finale_building_llms_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jazd/part_4_finale_building_llms_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jazd/part_4_finale_building_llms_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T03:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q21wqw</id>
    <title>A deep dive in DeepSeek's mHC: They improved things everyone else thought didn‚Äôt need improving</title>
    <updated>2026-01-02T15:44:21+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;The Context&lt;/h1&gt; &lt;p&gt;Since ResNet (2015), the Residual Connection (x_{l+1} = x_l + F(x_l)) has been the untouchable backbone of deep learning (from CNN to Transformer, from BERT to GPT). It solves the vanishing gradient problem by providing an &amp;quot;identity mapping&amp;quot; fast lane. For 10 years, almost no one questioned it.&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;However, this standard design forces a rigid 1:1 ratio between the input and the new computation, preventing the model from dynamically adjusting how much it relies on past layers versus new information.&lt;/p&gt; &lt;h1&gt;The Innovation&lt;/h1&gt; &lt;p&gt;ByteDace tried to break this rule with &amp;quot;Hyper-Connections&amp;quot; (HC), allowing the model to learn the connection weights instead of using a fixed ratio.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The potential:&lt;/strong&gt; Faster convergence and better performance due to flexible information routing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The issue:&lt;/strong&gt; It was incredibly unstable. Without constraints, signals were amplified by &lt;strong&gt;3000x&lt;/strong&gt; in deep networks, leading to exploding gradients.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Solution: Manifold-Constrained Hyper-Connections (mHC)&lt;/h1&gt; &lt;p&gt;In their new paper, DeepSeek solved the instability by constraining the learnable matrices to be &amp;quot;Double Stochastic&amp;quot; (all elements ‚âß 0, rows/cols sum to 1).&lt;/p&gt; &lt;p&gt;Mathematically, this forces the operation to act as a weighted average (convex combination). It guarantees that signals are never amplified beyond control, regardless of network depth.&lt;/p&gt; &lt;h1&gt;The Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stability:&lt;/strong&gt; Max gain magnitude dropped from &lt;strong&gt;3000 to 1.6&lt;/strong&gt; (3 orders of magnitude improvement).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; mHC beats both the standard baseline and the unstable HC on benchmarks like GSM8K and DROP.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost:&lt;/strong&gt; Only adds ~6% to training time due to heavy optimization (kernel fusion).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ybux3x1wgyag1.png?width=1206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=daafe17d3a61d387adf952ad756eb70af3bc445f"&gt;https://preview.redd.it/ybux3x1wgyag1.png?width=1206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=daafe17d3a61d387adf952ad756eb70af3bc445f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As hinted in the attached tweet, we are seeing a fascinating split in the AI world. While the industry frenzy focuses on commercialization and AI Agents‚Äîexemplified by Meta spending $2 Billion to acquire Manus‚Äîlabs like DeepSeek and Moonshot (Kimi) are playing a different game.&lt;/p&gt; &lt;p&gt;Despite resource constraints, they are digging into the deepest levels of macro-architecture and optimization. They have the audacity to question what we took for granted: &lt;strong&gt;Residual Connections&lt;/strong&gt; (challenged by DeepSeek's mHC) and &lt;strong&gt;AdamW&lt;/strong&gt; (challenged by Kimi's Muon). Just because these have been the standard for 10 years doesn't mean they are the optimal solution.&lt;/p&gt; &lt;p&gt;Crucially, instead of locking these secrets behind closed doors for commercial dominance, they are &lt;strong&gt;open-sourcing&lt;/strong&gt; these findings for the advancement of humanity. This spirit of relentless self-doubt and fundamental reinvention is exactly how we evolve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21wqw/a_deep_dive_in_deepseeks_mhc_they_improved_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21wqw/a_deep_dive_in_deepseeks_mhc_they_improved_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q21wqw/a_deep_dive_in_deepseeks_mhc_they_improved_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T15:44:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2seed</id>
    <title>Production Hybrid Retrieval: 48% better accuracy with BM25 + FAISS on a single t3.medium</title>
    <updated>2026-01-03T11:29:08+00:00</updated>
    <author>
      <name>/u/Ok-Blacksmith-8257</name>
      <uri>https://old.reddit.com/user/Ok-Blacksmith-8257</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;Sharing our hybrid retrieval system that serves 127k+ queries on a single AWS Lightsail instance (no GPU needed for embeddings, optional for reranking). **Stack**: - Embeddings: all-MiniLM-L6-v2 (22M params, CPU-friendly) - Reranker: ms-marco-MiniLM-L-6-v2 (cross-encoder) - Infrastructure: t3.medium (4GB RAM, 2 vCPU) - Cost: ~$50/month **Performance**: - Retrieval: 75ms (BM25 + FAISS + RRF + rerank) - Throughput: 50 queries/min - Accuracy: 91% (vs 62% dense-only) **Why hybrid?** Dense-only failed on &amp;quot;kenteken AB-123-CD&amp;quot; (license plate). Semantic similarity understood the concept but missed the exact entity. Solution: 4-stage cascade combining keyword precision (BM25) + semantic understanding (FAISS). **Latency breakdown**: - BM25: 8ms - FAISS: 15ms (runs parallel with BM25) - RRF fusion: 2ms - Cross-encoder rerank: 50ms (bottleneck but +12% accuracy) **Optimizations**: - Async parallel retrieval - Batch reranking (size 32) - GPU optional (3x speedup for reranker) **Code**: https://github.com/Eva-iq/E.V.A.-Cascading-Retrieval **Write-up**: https://medium.com/@pbronck/better-rag-accuracy-with-hybrid-bm25-dense-vector-search-ea99d48cba93 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Blacksmith-8257"&gt; /u/Ok-Blacksmith-8257 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2seed/production_hybrid_retrieval_48_better_accuracy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2seed/production_hybrid_retrieval_48_better_accuracy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2seed/production_hybrid_retrieval_48_better_accuracy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T11:29:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2sfwx</id>
    <title>ElevenLabs is killing my budget. What are the best "hidden gem" alternatives for documentary style TTS?</title>
    <updated>2026-01-03T11:31:31+00:00</updated>
    <author>
      <name>/u/Ancient_Routine8576</name>
      <uri>https://old.reddit.com/user/Ancient_Routine8576</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I'm running a YouTube channel focused on &amp;quot;War Economics&amp;quot; and &amp;quot;History&amp;quot;. I've been using ElevenLabs (Marcus voice) and the quality is amazing, but the pricing is unsustainable for long-form content (8-10 min videos).&lt;/p&gt; &lt;p&gt;I've tried the usual suspects (Murf, Play.ht) but they sound too robotic or corporate.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I am looking for:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Something with a dark, authoritative, documentary-style tone.&lt;/li&gt; &lt;li&gt;Either a cheaper paid alternative OR a high-quality GitHub/Local solution (I have a decent GPU if needed, like RVC or Tortoise).&lt;/li&gt; &lt;li&gt;Has anyone tried tools like &lt;strong&gt;Fish Audio&lt;/strong&gt; or &lt;strong&gt;OpenAI TTS API&lt;/strong&gt; wrappers?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any &amp;quot;underground&amp;quot; or lesser-known recommendations would be appreciated. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ancient_Routine8576"&gt; /u/Ancient_Routine8576 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T11:31:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2p2wa</id>
    <title>nanbeige4 is an incredible model for running locally</title>
    <updated>2026-01-03T08:06:54+00:00</updated>
    <author>
      <name>/u/Revolutionalredstone</name>
      <uri>https://old.reddit.com/user/Revolutionalredstone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feels like a deepseek moment might have slipped a few people by&lt;/p&gt; &lt;p&gt;nanbeige (weird name- apparently chosen to be bland/uninteresting)&lt;/p&gt; &lt;p&gt;..It's very interesting! basically 3 invalidating most 30B models.&lt;/p&gt; &lt;p&gt;(you can find it up ridiculously high on this chart: for a 3B model)&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/creative_writing.html"&gt;https://eqbench.com/creative_writing.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm stoked to have intelligence like this at home, but I'd love to know how to push this into super fast interference territory! (I've heard about diffusion based conversion etc and am super keen!)&lt;/p&gt; &lt;p&gt;Has anyone else seen something newer (this is a few weeks old now)? Seems like various charts show this one to be an outlier.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Revolutionalredstone"&gt; /u/Revolutionalredstone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p2wa/nanbeige4_is_an_incredible_model_for_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p2wa/nanbeige4_is_an_incredible_model_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p2wa/nanbeige4_is_an_incredible_model_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:06:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2dcje</id>
    <title>ASUS officially announces price hikes from January 5, right before CES 2026</title>
    <updated>2026-01-02T22:53:19+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2dcje/asus_officially_announces_price_hikes_from/"&gt; &lt;img alt="ASUS officially announces price hikes from January 5, right before CES 2026" src="https://external-preview.redd.it/e0KouFF887lm3A9dV6mq_44cYZmQvCh1I3h_7LTZz8c.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c1b6efd1e001ac6907d2df9de1ac4165e4b086a" title="ASUS officially announces price hikes from January 5, right before CES 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/asus-officially-announces-price-hikes-from-january-5-right-before-ces-2026"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2dcje/asus_officially_announces_price_hikes_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2dcje/asus_officially_announces_price_hikes_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T22:53:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2qo2u</id>
    <title>WhisperNote ‚Äî a simple local Whisper-based transcription app (Windows)</title>
    <updated>2026-01-03T09:44:46+00:00</updated>
    <author>
      <name>/u/_fortexe</name>
      <uri>https://old.reddit.com/user/_fortexe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2qo2u/whispernote_a_simple_local_whisperbased/"&gt; &lt;img alt="WhisperNote ‚Äî a simple local Whisper-based transcription app (Windows)" src="https://preview.redd.it/wcbalo8cu3bg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0328ac2605fcb9e788498a1ee7e1ae8c3f63b484" title="WhisperNote ‚Äî a simple local Whisper-based transcription app (Windows)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone.&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a small personal project called WhisperNote. It‚Äôs a simple Windows desktop app for local audio transcription using OpenAI Whisper.&lt;/p&gt; &lt;p&gt;The main goal was not to build ‚Äúthe best‚Äù tool, but a clean and straightforward one: press record or drop an audio file ‚Äî get text.&lt;/p&gt; &lt;p&gt;All processing happens locally on your machine. No cloud, no accounts. It‚Äôs intentionally minimal and focused on doing one thing well. Models are downloaded once, then everything runs offline.&lt;/p&gt; &lt;p&gt;I‚Äôm sharing it here in case someone values simplicity and local-first tools as much as I do. If it‚Äôs useful to you ‚Äî that‚Äôs great. Note: the Windows build is ~4 GB because it bundles Python, PyTorch with CUDA, and FFmpeg for a fully offline, out-of-the-box experience.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/LokiSkardina/WhisperNote"&gt;https://github.com/LokiSkardina/WhisperNote&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_fortexe"&gt; /u/_fortexe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wcbalo8cu3bg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2qo2u/whispernote_a_simple_local_whisperbased/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2qo2u/whispernote_a_simple_local_whisperbased/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T09:44:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2onpg</id>
    <title>Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)</title>
    <updated>2026-01-03T07:42:14+00:00</updated>
    <author>
      <name>/u/atif_dev</name>
      <uri>https://old.reddit.com/user/atif_dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2onpg/built_a_fully_local_ai_assistant_with_longterm/"&gt; &lt;img alt="Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)" src="https://b.thumbs.redditmedia.com/27iIaYMlD6cCAk7xURWf9QOU6i1_KjiShnklurpG_mo.jpg" title="Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a personal project called ATOM ‚Äî a fully local AI assistant designed more like an operating system for intelligence than a chatbot.&lt;/p&gt; &lt;p&gt;Everything runs locally. No cloud inference.&lt;/p&gt; &lt;p&gt;Key components: - Local LLM via LM Studio (currently Qwen3-VL-4B, vision + tool calling) - Tool orchestration (system info, web search via self-hosted SearXNG, file/PDF generation, Home Assistant, robotics) - Long-term memory with ChromaDB - Async memory saving via a smaller ‚Äújudge‚Äù model Semantic retrieval + periodic RAG-style injection - Dedicated local embedding server (OpenAI-style API) - Real hardware control (robotic arm, sensors) - JSON logging + test harness for reproducible scenarios&lt;/p&gt; &lt;p&gt;On the UI side, I built a React + React Three Fiber interface using Firebase Studio that visualizes tool usage as orbiting ‚Äúplanets‚Äù around a central core. It‚Äôs mostly for observability and debugging, but it turned out pretty fun.&lt;/p&gt; &lt;p&gt;Constraints: Hardware is limited (GTX 1650), so performance tradeoffs were necessary System is experimental and some components are still evolving&lt;/p&gt; &lt;p&gt;This is not a product, just a personal engineering project exploring: - long-term memory consolidation - tool-centric reasoning - fully local personal AI systems&lt;/p&gt; &lt;p&gt;Would appreciate feedback, especially from others running local setups or experimenting with memory/tool architectures.&lt;/p&gt; &lt;p&gt;GitHub (backend): &lt;a href="https://github.com/AtifUsmani/A.T.O.M"&gt;https://github.com/AtifUsmani/A.T.O.M&lt;/a&gt; UI repo: &lt;a href="https://github.com/AtifUsmani/ATOM-UI"&gt;https://github.com/AtifUsmani/ATOM-UI&lt;/a&gt; Demo videos linked in the README.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atif_dev"&gt; /u/atif_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q2onpg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2onpg/built_a_fully_local_ai_assistant_with_longterm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2onpg/built_a_fully_local_ai_assistant_with_longterm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T07:42:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2p5dh</id>
    <title>Glm4.7 + CC not bad</title>
    <updated>2026-01-03T08:10:55+00:00</updated>
    <author>
      <name>/u/Federal_Spend2412</name>
      <uri>https://old.reddit.com/user/Federal_Spend2412</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I genuinely think it's pretty good this time - GLM4.7 + CC is actually somewhat close to 4.5 Sonnet, or more accurately I'd say it's on par with 4 Sonnet. I'm subscribed to the middle-tier plan.&lt;/p&gt; &lt;p&gt;I tested it with a project that has a Python backend and TypeScript frontend, asking it to add a feature that involved both backend and frontend work. It handled everything smoothly, and the MCP calls all went through without getting stuck (which used to be a problem before).&lt;/p&gt; &lt;p&gt;Of course, to be completely honest, there's still a massive gap between this and 4.5 Opus - Opus is on a completely insane level&lt;/p&gt; &lt;p&gt;So I'm still keeping my $10/month GitHub Copilot subscription. For the really tough problems, I'll use 4.5 Opus, but for regular stuff, GLM4.7 + CC basically handles everything. GLM4.7 costs me $100/month now, plus the $10 for Copilot - that's less than around $13 per month total(bigmodel.cn coding plan), which feels pretty good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Spend2412"&gt; /u/Federal_Spend2412 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p5dh/glm47_cc_not_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p5dh/glm47_cc_not_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p5dh/glm47_cc_not_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:10:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1q25070</id>
    <title>LeCun Says Llama 4 results "were fudged a little bit"</title>
    <updated>2026-01-02T17:38:01+00:00</updated>
    <author>
      <name>/u/MrPecunius</name>
      <uri>https://old.reddit.com/user/MrPecunius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There was speculation in this sub about suspicious Llama 4 benchmarks some time back, and now LeCun confirms it on his way out. Best I can do is a Slashdot link since the FT article is paywalled:&lt;/p&gt; &lt;p&gt;&lt;a href="https://tech.slashdot.org/story/26/01/02/1449227/results-were-fudged-departing-meta-ai-chief-confirms-llama-4-benchmark-manipulation"&gt;'Results Were Fudged': Departing Meta AI Chief Confirms Llama 4 Benchmark Manipulation &lt;/a&gt;&lt;/p&gt; &lt;p&gt;This bit jumped out at me:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Zuckerberg subsequently &amp;quot;sidelined the entire GenAI organisation,&amp;quot; according to LeCun. &amp;quot;A lot of people have left, a lot of people who haven't yet left will leave.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This explains a lot, if true: we never saw the promised huge Llama 4 model, and there hasn't been any followup since the other releases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrPecunius"&gt; /u/MrPecunius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T17:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2jwsn</id>
    <title>How is Cloud Inference so cheap</title>
    <updated>2026-01-03T03:37:13+00:00</updated>
    <author>
      <name>/u/VolkoTheWorst</name>
      <uri>https://old.reddit.com/user/VolkoTheWorst</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do cloud inference companies like DeepInfra, Together, Chutes, Novita etc manage to be in profit regarding to the price of the GPUs/electricity and the fact that I guess it's difficult to have always someone to serve ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VolkoTheWorst"&gt; /u/VolkoTheWorst &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T03:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2ppkb</id>
    <title>MiniMax-M2.1 Uncensored: PRISM Advanced Abliteration</title>
    <updated>2026-01-03T08:45:29+00:00</updated>
    <author>
      <name>/u/Maxious</name>
      <uri>https://old.reddit.com/user/Maxious</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ppkb/minimaxm21_uncensored_prism_advanced_abliteration/"&gt; &lt;img alt="MiniMax-M2.1 Uncensored: PRISM Advanced Abliteration" src="https://external-preview.redd.it/AAFIDX32Yo3yBOTXBXkwbpmtKeh886wBWSOhkOds4Pc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bc2cf3d74a648b569c2bd55f6d299c6f5f27ea9" title="MiniMax-M2.1 Uncensored: PRISM Advanced Abliteration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxious"&gt; /u/Maxious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Ex0bit/MiniMax-M2.1-PRISM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ppkb/minimaxm21_uncensored_prism_advanced_abliteration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ppkb/minimaxm21_uncensored_prism_advanced_abliteration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2o033</id>
    <title>What is the smartest uncensored nsfw LLM you can run with 20GB VRAM and 24GB RAM</title>
    <updated>2026-01-03T07:04:18+00:00</updated>
    <author>
      <name>/u/Death_12_35_taken</name>
      <uri>https://old.reddit.com/user/Death_12_35_taken</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for something that can stay in character and be fast but also creative. I am looking for models that i can run locally and at decent speed. Just need something that is smart and uncensored. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Death_12_35_taken"&gt; /u/Death_12_35_taken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T07:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2s3hp</id>
    <title>Don't sleep on granite 4 small if you got an 8+32+ system</title>
    <updated>2026-01-03T11:11:06+00:00</updated>
    <author>
      <name>/u/Zestyclose-Shift710</name>
      <uri>https://old.reddit.com/user/Zestyclose-Shift710</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2s3hp/dont_sleep_on_granite_4_small_if_you_got_an_832/"&gt; &lt;img alt="Don't sleep on granite 4 small if you got an 8+32+ system" src="https://b.thumbs.redditmedia.com/iwvWuoXjX_5rS64X-c0qbwyBZ9jpLqp-5OJdBXmYjto.jpg" title="Don't sleep on granite 4 small if you got an 8+32+ system" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My device: a thinkpad p15 with 32gb of ram and a 8gb quadro. Usually only really good enough for the 7-8b class.&lt;/p&gt; &lt;p&gt;The setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use a MoE;&lt;/li&gt; &lt;li&gt;Keep all experts in CPU (llama.cpp parameter);&lt;/li&gt; &lt;li&gt;This leaves you with VRAM to spare. Set the context length so it ~fills it up&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The result:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~200k context (f16 kv cache)&lt;/li&gt; &lt;li&gt;~30b MoE model&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;~10 tkps generation speed&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;But this is where granite 4 comes in: due to being a hybrid transformer+mamba model, it stays fast as context fills&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As such, using Granite 4.0 Small (32B total / 9B activated) with a 50 page (~50.5k tokens) paper in context, it stays at ~7 tkps, which is very usable!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nqpvxiu9a4bg1.png?width=1055&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fd830b29fb3bf890136793590665cf3ceec979b"&gt;Screenshot is from Jan (https://www.jan.ai/), a sort of FOSS LM Studio alternative that I really like&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quite possibly this is all very obvious but I just found this out experimentally and it would probably be useful to others like me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zestyclose-Shift710"&gt; /u/Zestyclose-Shift710 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2s3hp/dont_sleep_on_granite_4_small_if_you_got_an_832/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2s3hp/dont_sleep_on_granite_4_small_if_you_got_an_832/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2s3hp/dont_sleep_on_granite_4_small_if_you_got_an_832/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T11:11:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2pons</id>
    <title>GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)</title>
    <updated>2026-01-03T08:43:56+00:00</updated>
    <author>
      <name>/u/Maxious</name>
      <uri>https://old.reddit.com/user/Maxious</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"&gt; &lt;img alt="GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)" src="https://external-preview.redd.it/RT6xZIQ5U8h3GMBsKzEeqHJyXy63I2_XP8TVKTT_Hvg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c7ca25d885be26a9f257d4e17e2b038061773a" title="GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxious"&gt; /u/Maxious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/0xSero/GLM-4.7-REAP-50-W4A16"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
