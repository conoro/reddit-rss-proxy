<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-23T05:45:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m6bddm</id>
    <title>AMD's Strix Halo "Ryzen AI MAX" APUs Come To DIY PC Builders With New MoDT "Mini-ITX" Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory</title>
    <updated>2025-07-22T11:18:22+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/"&gt; &lt;img alt="AMD's Strix Halo &amp;quot;Ryzen AI MAX&amp;quot; APUs Come To DIY PC Builders With New MoDT &amp;quot;Mini-ITX&amp;quot; Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory" src="https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c63f2527e38ed9f9fb783cd700b8e831108fe01" title="AMD's Strix Halo &amp;quot;Ryzen AI MAX&amp;quot; APUs Come To DIY PC Builders With New MoDT &amp;quot;Mini-ITX&amp;quot; Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-strix-halo-ryzen-ai-max-apus-diy-pc-new-modt-mini-itx-motherboards-128-gb-lpddr5x-memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T11:18:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6u0gt</id>
    <title>Unsloth quants already starting to roll out for Qwen3-Coder</title>
    <updated>2025-07-22T23:45:30+00:00</updated>
    <author>
      <name>/u/arcanemachined</name>
      <uri>https://old.reddit.com/user/arcanemachined</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6u0gt/unsloth_quants_already_starting_to_roll_out_for/"&gt; &lt;img alt="Unsloth quants already starting to roll out for Qwen3-Coder" src="https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=651f424884542b7c34073b3bc62c0fc1b199eaae" title="Unsloth quants already starting to roll out for Qwen3-Coder" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arcanemachined"&gt; /u/arcanemachined &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/unsloth/qwen3-coder-687ff47700270447e02c987d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6u0gt/unsloth_quants_already_starting_to_roll_out_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6u0gt/unsloth_quants_already_starting_to_roll_out_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T23:45:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6ct7u</id>
    <title>Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000</title>
    <updated>2025-07-22T12:31:02+00:00</updated>
    <author>
      <name>/u/aidanjustsayin</name>
      <uri>https://old.reddit.com/user/aidanjustsayin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/"&gt; &lt;img alt="Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000" src="https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47d0469ef510365b725dc72ec2ab1d98d266e09a" title="Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently upgraded my desktop RAM given the large MoE models coming out and I was excited for the maiden voyage to be yesterday's release! I'll put the prompt and code in a comment, this is sort of a test of ability but more so I wanted to confirm Q3_K_L is runnable (though slow) for anybody with similar PC specs and produces something usable!&lt;/p&gt; &lt;p&gt;I used LM Studio for loading the model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Context: 4096 (default)&lt;/li&gt; &lt;li&gt;GPU Offload: 18 / 94&lt;/li&gt; &lt;li&gt;CPU Thread Pool: 16&lt;/li&gt; &lt;li&gt;... all else default besides ...&lt;/li&gt; &lt;li&gt;Flash Attention: On&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When loaded, it used up 23.3GB of VRAM and ~80GB of RAM.&lt;/p&gt; &lt;p&gt;Basic Generation stats: 5.52 tok/sec â€¢ 2202 tokens â€¢ 0.18s to first token&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aidanjustsayin"&gt; /u/aidanjustsayin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1x5u9hrp5fef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T12:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6u3kd</id>
    <title>Qwen3-Coder is available on OpenRouter</title>
    <updated>2025-07-22T23:49:17+00:00</updated>
    <author>
      <name>/u/arcanemachined</name>
      <uri>https://old.reddit.com/user/arcanemachined</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6u3kd/qwen3coder_is_available_on_openrouter/"&gt; &lt;img alt="Qwen3-Coder is available on OpenRouter" src="https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f4da7fa00b2fee69899af4df9a137f3645df9e7" title="Qwen3-Coder is available on OpenRouter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arcanemachined"&gt; /u/arcanemachined &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openrouter.ai/qwen/qwen3-coder"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6u3kd/qwen3coder_is_available_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6u3kd/qwen3coder_is_available_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T23:49:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6cfzi</id>
    <title>The ik_llama.cpp repository is back! \o/</title>
    <updated>2025-07-22T12:13:32+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Friendly reminder to back up all the things!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T12:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6vbds</id>
    <title>Just tried higgsaudio v2: a new multilingual TTS model, pretty impressed</title>
    <updated>2025-07-23T00:45:03+00:00</updated>
    <author>
      <name>/u/Sudden-Tap3484</name>
      <uri>https://old.reddit.com/user/Sudden-Tap3484</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/"&gt; &lt;img alt="Just tried higgsaudio v2: a new multilingual TTS model, pretty impressed" src="https://b.thumbs.redditmedia.com/lDeKkUsKVKJvujnGWUqXtUhpkbsWufoj2laEkKgzAUI.jpg" title="Just tried higgsaudio v2: a new multilingual TTS model, pretty impressed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4"&gt;https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This model showed up on my LinkedIn feed today. After listening to a few examples on their &lt;a href="https://www.boson.ai/technologies/voice"&gt;website&lt;/a&gt;, I feel it is so much better than chatterbox (I used it a lot), might even be better than gemini tts. &lt;/p&gt; &lt;p&gt;Listen to this &lt;a href="https://github.com/user-attachments/assets/0fd73fad-097f-48a9-9f3f-bc2a63b3818d"&gt;demo video&lt;/a&gt;, it will just enable so many use cases.&lt;/p&gt; &lt;p&gt;I tried a few examples in their HF &lt;a href="https://huggingface.co/spaces/smola/higgs_audio_v2"&gt;playground&lt;/a&gt;, it works surprisingly well in terms of cadence and emotion. Also works for Spanish! Havenâ€™t tested all languages or edge cases, Anyone else tried it yet? Curious how it compares to other recent models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sudden-Tap3484"&gt; /u/Sudden-Tap3484 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T00:45:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1m70n7q</id>
    <title>Alibabaâ€™s upgraded Qwen3 235B-A22B 2507 is now the most intelligent non-reasoning model.</title>
    <updated>2025-07-23T05:12:16+00:00</updated>
    <author>
      <name>/u/Fantastic-Emu-3819</name>
      <uri>https://old.reddit.com/user/Fantastic-Emu-3819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m70n7q/alibabas_upgraded_qwen3_235ba22b_2507_is_now_the/"&gt; &lt;img alt="Alibabaâ€™s upgraded Qwen3 235B-A22B 2507 is now the most intelligent non-reasoning model." src="https://b.thumbs.redditmedia.com/44x5FARtQun2iK2pU9UkqoLiKnQmMoq90mJNUYMTKbw.jpg" title="Alibabaâ€™s upgraded Qwen3 235B-A22B 2507 is now the most intelligent non-reasoning model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 235B 2507 scores 60 on the Artificial Analysis Intelligence Index, surpassing Claude 4 Opus and Kimi K2 (both 58), and DeepSeek V3 0324 and GPT-4.1 (both 53). This marks a 13-point leap over the May 2025 non-reasoning release and brings it within two points of the May 2025 reasoning variant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic-Emu-3819"&gt; /u/Fantastic-Emu-3819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m70n7q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m70n7q/alibabas_upgraded_qwen3_235ba22b_2507_is_now_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m70n7q/alibabas_upgraded_qwen3_235ba22b_2507_is_now_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T05:12:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6orbr</id>
    <title>Anyone here who has been able to reproduce their results yet?</title>
    <updated>2025-07-22T20:11:38+00:00</updated>
    <author>
      <name>/u/Original_Log_9899</name>
      <uri>https://old.reddit.com/user/Original_Log_9899</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6orbr/anyone_here_who_has_been_able_to_reproduce_their/"&gt; &lt;img alt="Anyone here who has been able to reproduce their results yet?" src="https://preview.redd.it/cfffg12fghef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f02acda8fde9368279ce55c247aa3eb87536a6a5" title="Anyone here who has been able to reproduce their results yet?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See &lt;a href="https://x.com/makingAGI/status/1947286324735856747"&gt;https://x.com/makingAGI/status/1947286324735856747&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Original_Log_9899"&gt; /u/Original_Log_9899 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cfffg12fghef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6orbr/anyone_here_who_has_been_able_to_reproduce_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6orbr/anyone_here_who_has_been_able_to_reproduce_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T20:11:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6mfic</id>
    <title>Qwen3-Coder Available on chat.qwen.ai</title>
    <updated>2025-07-22T18:44:49+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mfic/qwen3coder_available_on_chatqwenai/"&gt; &lt;img alt="Qwen3-Coder Available on chat.qwen.ai" src="https://preview.redd.it/8xj4raow0hef1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf0cbd6e19276ab7bbf6b36687af35cdf6c00d83" title="Qwen3-Coder Available on chat.qwen.ai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1M token context length&lt;/p&gt; &lt;p&gt;No model weights yet, but Qwen3-Coder is already available for testing on &lt;a href="https://chat.qwen.ai"&gt;Qwen Chat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8xj4raow0hef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mfic/qwen3coder_available_on_chatqwenai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mfic/qwen3coder_available_on_chatqwenai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6rsym</id>
    <title>Qwen Code: A command-line AI workflow tool adapted from Gemini CLI, optimized for Qwen3-Coder models</title>
    <updated>2025-07-22T22:11:21+00:00</updated>
    <author>
      <name>/u/arcanemachined</name>
      <uri>https://old.reddit.com/user/arcanemachined</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6rsym/qwen_code_a_commandline_ai_workflow_tool_adapted/"&gt; &lt;img alt="Qwen Code: A command-line AI workflow tool adapted from Gemini CLI, optimized for Qwen3-Coder models" src="https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21b1ec40f95d195f9c34bb5728616a2b4c3162fd" title="Qwen Code: A command-line AI workflow tool adapted from Gemini CLI, optimized for Qwen3-Coder models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arcanemachined"&gt; /u/arcanemachined &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/QwenLM/qwen-code"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6rsym/qwen_code_a_commandline_ai_workflow_tool_adapted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6rsym/qwen_code_a_commandline_ai_workflow_tool_adapted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T22:11:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6medy</id>
    <title>Qwen3-Coder is imminent</title>
    <updated>2025-07-22T18:43:38+00:00</updated>
    <author>
      <name>/u/Dudensen</name>
      <uri>https://old.reddit.com/user/Dudensen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6medy/qwen3coder_is_imminent/"&gt; &lt;img alt="Qwen3-Coder is imminent" src="https://preview.redd.it/mruaiodv0hef1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=daa5e07dcd586edd4e8488215b2df66df2d2c809" title="Qwen3-Coder is imminent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dudensen"&gt; /u/Dudensen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mruaiodv0hef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6medy/qwen3coder_is_imminent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6medy/qwen3coder_is_imminent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6qkse</id>
    <title>It's here guys and qwen nailed it !!</title>
    <updated>2025-07-22T21:22:09+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qkse/its_here_guys_and_qwen_nailed_it/"&gt; &lt;img alt="It's here guys and qwen nailed it !!" src="https://b.thumbs.redditmedia.com/Tng4SvC83rVHk9iUXovrs4GeXZmRkFJ59wlPU2wB1GM.jpg" title="It's here guys and qwen nailed it !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m6qkse"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qkse/its_here_guys_and_qwen_nailed_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qkse/its_here_guys_and_qwen_nailed_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T21:22:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6zz1v</id>
    <title>Kimi K2 vs Qwen3 Coder 480B</title>
    <updated>2025-07-23T04:34:42+00:00</updated>
    <author>
      <name>/u/Ok-Pattern9779</name>
      <uri>https://old.reddit.com/user/Ok-Pattern9779</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been testing Qwen3-Coder-480B (on Hyperbolics) and Kimi K2 (on Groq) for Rust and Go projects. Neither model is built for deep problem-solving, but in real-world use, the differences are pretty clear. &lt;/p&gt; &lt;p&gt;Qwen3-Coder often ignores system prompts, struggles with context, and its tool calls are rigid, like itâ€™s just filling in templates rather than thinking through the task. Itâ€™s not just about raw capability; the responses are too formulaic, making it hard to use for actual coding tasks. &lt;/p&gt; &lt;p&gt;Some of this might be because Hyperbolics hasnâ€™t fully optimized their setup for Qwen3 yet. But I suspect the bigger issue is the fine-tuning, it seems trained on overly structured responses, so it fails to adapt to natural prompts.&lt;/p&gt; &lt;p&gt;Kimi K2 works much better. Even though itâ€™s not a reasoning-focused model, it stays on task, handles edits and helper functions smoothly, and just feels more responsive when working with multi-file projects. For Rust and Go, itâ€™s consistently the better option.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Pattern9779"&gt; /u/Ok-Pattern9779 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T04:34:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6ztb2</id>
    <title>UI/UX benchmark update 7/22: Newest Qwen models added, Qwen3 takes the lead in terms of win rate (though still early)</title>
    <updated>2025-07-23T04:25:53+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ztb2/uiux_benchmark_update_722_newest_qwen_models/"&gt; &lt;img alt="UI/UX benchmark update 7/22: Newest Qwen models added, Qwen3 takes the lead in terms of win rate (though still early)" src="https://preview.redd.it/lcjgeavzvjef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8550da9c204aaebc89b401002be06079a6beec29" title="UI/UX benchmark update 7/22: Newest Qwen models added, Qwen3 takes the lead in terms of win rate (though still early)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You probably already know about my &lt;a href="https://www.designarena.ai/"&gt;benchmark&lt;/a&gt;, but here's &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lxth6s/comment/n2qoqtk/?context=3"&gt;context&lt;/a&gt; if you missed it. The tldr is that it's a crowdsource benchmark that takes human preferences on frontend and image generations from different models to produce a leaderboard ranking for which models are currently the best at UI and design generation. &lt;/p&gt; &lt;p&gt;I'm going to try to keep these update posts to once-a-week or every other week to not come off as spam (sorry for that earlier, though I'm just seeing interesting results). Also, we realize there are flaws to the leaderboard (as all leaderboards and benchmarks have) that we're progressively trying to improve, but think it has been a good barometer for evaluating the models in particular tiers when it comes to coding. &lt;/p&gt; &lt;p&gt;Anyways, since my last update on the 11th, we've added a few models, and in the last 24 hours, specifically Qwen3-235B-A22B-Instruct-2507 and Qwen3-Coder (less than an hour ago). Though the sample size is still very small, Qwen3-235B-A22B-Instruct-2507 appears to be killing it. I was reading through remarks on Twitter and Reddit that the Instruct model was on par with Opus which I thought was hyperbole at the time, but maybe that claim will hold true in the long run. &lt;/p&gt; &lt;p&gt;What has been your experience with these Qwen models and what do you think? Open source is killing it right now. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lcjgeavzvjef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ztb2/uiux_benchmark_update_722_newest_qwen_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ztb2/uiux_benchmark_update_722_newest_qwen_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T04:25:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6qc8c</id>
    <title>Qwen/Qwen3-Coder-480B-A35B-Instruct</title>
    <updated>2025-07-22T21:12:52+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qc8c/qwenqwen3coder480ba35binstruct/"&gt; &lt;img alt="Qwen/Qwen3-Coder-480B-A35B-Instruct" src="https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1547f625cbccf70a7763a9c35af1919246072a2e" title="Qwen/Qwen3-Coder-480B-A35B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qc8c/qwenqwen3coder480ba35binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qc8c/qwenqwen3coder480ba35binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T21:12:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6mlbk</id>
    <title>Qwen3-Coder-480B-A35B-Instruct</title>
    <updated>2025-07-22T18:50:48+00:00</updated>
    <author>
      <name>/u/gzzhongqi</name>
      <uri>https://old.reddit.com/user/gzzhongqi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct"&gt;https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;hyperolic already has it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gzzhongqi"&gt; /u/gzzhongqi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:50:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6nxh2</id>
    <title>Everyone brace up for qwen !!</title>
    <updated>2025-07-22T19:40:36+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6nxh2/everyone_brace_up_for_qwen/"&gt; &lt;img alt="Everyone brace up for qwen !!" src="https://preview.redd.it/mn8auem2bhef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=855c907a55cf3f70afe582932d52350878ef5e68" title="Everyone brace up for qwen !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mn8auem2bhef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6nxh2/everyone_brace_up_for_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6nxh2/everyone_brace_up_for_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T19:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6ny2q</id>
    <title>Qwen3-Coder Web Development</title>
    <updated>2025-07-22T19:41:12+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ny2q/qwen3coder_web_development/"&gt; &lt;img alt="Qwen3-Coder Web Development" src="https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=113bee11066829bd35da182aa0ce00847ecb4ea0" title="Qwen3-Coder Web Development" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used Qwen3-Coder-408B-A35B-Instruct to generate a procedural 3D planet preview and editor.&lt;/p&gt; &lt;p&gt;Very strong results! Comparable to Kimi-K2-Instruct, maybe a tad bit behind, but still impressive for under 50% the parameter count.&lt;/p&gt; &lt;p&gt;Creds &lt;a href="https://www.youtube.com/@TheFeatureCrew"&gt;The Feature Crew&lt;/a&gt; for the original idea.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ob9yhvcjahef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ny2q/qwen3coder_web_development/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ny2q/qwen3coder_web_development/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T19:41:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6lf9s</id>
    <title>Could this be Deepseek?</title>
    <updated>2025-07-22T18:07:46+00:00</updated>
    <author>
      <name>/u/dulldata</name>
      <uri>https://old.reddit.com/user/dulldata</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6lf9s/could_this_be_deepseek/"&gt; &lt;img alt="Could this be Deepseek?" src="https://preview.redd.it/qzkjkgegugef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e224ff9a214f929b3917304102fe92d67371e639" title="Could this be Deepseek?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dulldata"&gt; /u/dulldata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qzkjkgegugef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6lf9s/could_this_be_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6lf9s/could_this_be_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:07:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6wgs7</id>
    <title>Qwen3-Coder Unsloth dynamic GGUFs</title>
    <updated>2025-07-23T01:38:45+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6wgs7/qwen3coder_unsloth_dynamic_ggufs/"&gt; &lt;img alt="Qwen3-Coder Unsloth dynamic GGUFs" src="https://preview.redd.it/s9cwrvwg1jef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75c9ba63f5cc1768819789d0934d7d2a1e5a5926" title="Qwen3-Coder Unsloth dynamic GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We made dynamic 2bit to 8bit dynamic Unsloth quants for the 480B model! Dynamic 2bit needs 182GB of space (down from 512GB). Also, we're making &lt;strong&gt;1M context length variants&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;You can achieve &amp;gt;6 tokens/s on &lt;strong&gt;182GB unified memory or 158GB RAM + 24GB VRAM&lt;/strong&gt; via MoE offloading. You do not need 182GB of VRAM, since llama.cpp can offload MoE layers to RAM via &lt;/p&gt; &lt;pre&gt;&lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Unfortunately 1bit models cannot be made since there are some quantization issues (similar to Qwen 235B) - we're investigating why this happens.&lt;/p&gt; &lt;p&gt;You can also run the &lt;strong&gt;un-quantized 8bit / 16bit&lt;/strong&gt; versions also using llama,cpp offloading! Use Q8_K_XL which will be completed in an hour or so.&lt;/p&gt; &lt;p&gt;To increase performance and context length, use KV cache quantization, especially the _1 variants (higher accuracy than _0 variants). More details &lt;a href="https://docs.unsloth.ai/basics/qwen3-coder#how-to-fit-long-context-256k-to-1m"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;code&gt;--cache-type-k q4_1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Enable flash attention as well and also try llama.cpp's NEW high throughput mode for multi user inference (similar to vLLM). Details on how to are &lt;a href="https://docs.unsloth.ai/basics/qwen3-coder#improving-generation-speed"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Qwen3-Coder-480B-A35B GGUFs (still ongoing) are at &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;1 million context length variants will be up at &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs on how to run it are here: &lt;a href="https://docs.unsloth.ai/basics/qwen3-coder"&gt;https://docs.unsloth.ai/basics/qwen3-coder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s9cwrvwg1jef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6wgs7/qwen3coder_unsloth_dynamic_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6wgs7/qwen3coder_unsloth_dynamic_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T01:38:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6qixu</id>
    <title>Qwen out here releasing models like itâ€™s a Costco sample table</title>
    <updated>2025-07-22T21:20:04+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qixu/qwen_out_here_releasing_models_like_its_a_costco/"&gt; &lt;img alt="Qwen out here releasing models like itâ€™s a Costco sample table" src="https://preview.redd.it/5eb8n31sshef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f24e0235850da677693988507655dde73bf8e60" title="Qwen out here releasing models like itâ€™s a Costco sample table" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5eb8n31sshef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qixu/qwen_out_here_releasing_models_like_its_a_costco/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qixu/qwen_out_here_releasing_models_like_its_a_costco/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T21:20:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6qnpq</id>
    <title>Qwen3 coder will be in multiple sizes</title>
    <updated>2025-07-22T21:25:25+00:00</updated>
    <author>
      <name>/u/dinesh2609</name>
      <uri>https://old.reddit.com/user/dinesh2609</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qnpq/qwen3_coder_will_be_in_multiple_sizes/"&gt; &lt;img alt="Qwen3 coder will be in multiple sizes" src="https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1547f625cbccf70a7763a9c35af1919246072a2e" title="Qwen3 coder will be in multiple sizes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today, we're announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we're excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dinesh2609"&gt; /u/dinesh2609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qnpq/qwen3_coder_will_be_in_multiple_sizes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qnpq/qwen3_coder_will_be_in_multiple_sizes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T21:25:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6wb5o</id>
    <title>Recent Qwen Benchmark Scores are Questionable</title>
    <updated>2025-07-23T01:31:34+00:00</updated>
    <author>
      <name>/u/Electronic_Ad8889</name>
      <uri>https://old.reddit.com/user/Electronic_Ad8889</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6wb5o/recent_qwen_benchmark_scores_are_questionable/"&gt; &lt;img alt="Recent Qwen Benchmark Scores are Questionable" src="https://preview.redd.it/8gjn0yhf1jef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5550a5410e5e1c751c0140c16c192e6bd86fddd" title="Recent Qwen Benchmark Scores are Questionable" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic_Ad8889"&gt; /u/Electronic_Ad8889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8gjn0yhf1jef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6wb5o/recent_qwen_benchmark_scores_are_questionable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6wb5o/recent_qwen_benchmark_scores_are_questionable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T01:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6mew9</id>
    <title>Qwen3- Coder ðŸ‘€</title>
    <updated>2025-07-22T18:44:10+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mew9/qwen3_coder/"&gt; &lt;img alt="Qwen3- Coder ðŸ‘€" src="https://preview.redd.it/vnhuwe801hef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92b455544fdc9f84aebcf9cf995f7e3e643179a1" title="Qwen3- Coder ðŸ‘€" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Available in &lt;a href="https://chat.qwen.ai"&gt;https://chat.qwen.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vnhuwe801hef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mew9/qwen3_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mew9/qwen3_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:44:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6qdet</id>
    <title>Qwen3-Coder is here!</title>
    <updated>2025-07-22T21:14:07+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/"&gt; &lt;img alt="Qwen3-Coder is here!" src="https://preview.redd.it/0cowg3grrhef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=470c1e7a0a6df4a35a09ad70120a5fef4e93a97b" title="Qwen3-Coder is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;Qwen3-Coder is here! âœ…&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Weâ€™re releasing Qwen3-Coder-480B-A35B-Instruct, our most powerful open agentic code model to date. This 480B-parameter Mixture-of-Experts model (35B active) natively supports 256K context and scales to 1M context with extrapolation. It achieves top-tier performance across multiple agentic coding benchmarks among open models, including SWE-bench-Verified!!! ðŸš€&lt;/p&gt; &lt;p&gt;Alongside the model, we're also open-sourcing a command-line tool for agentic coding: Qwen Code. Forked from Gemini Code, it includes custom prompts and function call protocols to fully unlock Qwen3-Coderâ€™s capabilities. Qwen3-Coder works seamlessly with the communityâ€™s best developer tools. As a foundation model, we hope it can be used anywhere across the digital world â€” Agentic Coding in the World! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0cowg3grrhef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T21:14:07+00:00</published>
  </entry>
</feed>
