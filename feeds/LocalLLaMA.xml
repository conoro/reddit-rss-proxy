<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-17T15:49:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qexs07</id>
    <title>Is there a local/self-hosted alternative to Google NotebookLM?</title>
    <updated>2026-01-17T00:34:31+00:00</updated>
    <author>
      <name>/u/RadiantCandy1600</name>
      <uri>https://old.reddit.com/user/RadiantCandy1600</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is an Alternate to &lt;strong&gt;Google NotebookLM&lt;/strong&gt;?&lt;/p&gt; &lt;p&gt;I would like something local because of concern of uploading sensitive work documents or personal research to Google’s cloud. I’m looking for something I can run &lt;strong&gt;locally on my own hardware&lt;/strong&gt; (or a private VPS) that replicates that &amp;quot;Notebook&amp;quot; experience.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ideally, I’m looking for:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Privacy:&lt;/strong&gt; No data leaving my machine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Source Grounding:&lt;/strong&gt; The ability to chat with specific &amp;quot;Notebooks&amp;quot; or collections of PDFs/Markdown/Text files.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Citations:&lt;/strong&gt; It needs to tell me exactly which page/document the answer came from (this is the best part of NotebookLM).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Audio/Podcasts (Optional):&lt;/strong&gt; The AI podcast generator in NotebookLM is cool, but document analysis is my priority.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What are the best options in 2026?&lt;/strong&gt; I’ve heard names like &lt;strong&gt;AnythingLLM&lt;/strong&gt;, &lt;strong&gt;GPT4All&lt;/strong&gt;, and &lt;strong&gt;Open Notebook&lt;/strong&gt; (the GitHub project) thrown around. Which one is currently the most stable and &amp;quot;NotebookLM-like&amp;quot;? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RadiantCandy1600"&gt; /u/RadiantCandy1600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qexs07/is_there_a_localselfhosted_alternative_to_google/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qexs07/is_there_a_localselfhosted_alternative_to_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qexs07/is_there_a_localselfhosted_alternative_to_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T00:34:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf9ppd</id>
    <title>Running AutoRound</title>
    <updated>2026-01-17T10:18:38+00:00</updated>
    <author>
      <name>/u/1-a-n</name>
      <uri>https://old.reddit.com/user/1-a-n</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried to run AutoRound on 0xSero/MiniMax-M2.1-REAP-40 (for the second time) but whilst it at least runs now I am not convinced it is ok as it hung up when processing a large request. It appears pretty hard to do this wrong, can anyone comment on the validity of my script?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from auto_round import AutoRound ar = AutoRound( './MiniMax-M2.1-REAP-40', device='cuda', device_map='auto', nsamples=64, seqlen=512, batch_size=1 ) ar.quantize_and_save('./MiniMax-M2.1-REAP-40-W4A16', format='auto_round') &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1-a-n"&gt; /u/1-a-n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf9ppd/running_autoround/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf9ppd/running_autoround/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf9ppd/running_autoround/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T10:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf8gw6</id>
    <title>NeuTTS Android APK Sample - Local Inference (Nano Model)</title>
    <updated>2026-01-17T09:01:11+00:00</updated>
    <author>
      <name>/u/RowGroundbreaking982</name>
      <uri>https://old.reddit.com/user/RowGroundbreaking982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Latest release available here: &lt;a href="https://github.com/lookbe/neutts-unity-example/releases"&gt;github.com/lookbe/neutts-unity-example/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’ve put together a sample APK for &lt;strong&gt;NeuTTS&lt;/strong&gt; on Android. It is a slimmed-down build focused on on-device performance using the Nano model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Installation Guide (Manual OBB Setup):&lt;/strong&gt; Since this uses the expansion file system, you must place the data files manually using a PC:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Download&lt;/strong&gt; the APK and both OBB files from the link above.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Install&lt;/strong&gt; the APK on your phone (but do not open it yet).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Connect&lt;/strong&gt; your phone to a PC via USB.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Navigate&lt;/strong&gt; to: &lt;code&gt;Internal Storage/Android/obb/&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; a new folder named exactly: &lt;code&gt;com.lookbe.neutts&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Copy&lt;/strong&gt; both OBB files into that folder.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Technical Details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; NeuTTS Nano (lowest model size due to storage/APK limits).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Precision:&lt;/strong&gt; Int8 Quantized Decoder for mobile CPU efficiency.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Phonemizer:&lt;/strong&gt; Open Phonemizer (eSpeak has been removed).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Assets:&lt;/strong&gt; Reference samples and model are packed into the app/OBB.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Known Issues:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Legacy OBB:&lt;/strong&gt; Might not work on some newer Android versions or specific devices due to how legacy expansion files are handled.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Tier:&lt;/strong&gt; Uses the lowest model to ensure compatibility with mobile hardware constraints.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RowGroundbreaking982"&gt; /u/RowGroundbreaking982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf8gw6/neutts_android_apk_sample_local_inference_nano/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf8gw6/neutts_android_apk_sample_local_inference_nano/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf8gw6/neutts_android_apk_sample_local_inference_nano/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T09:01:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qehf0p</id>
    <title>Maxsun joins Sparkle in making Intel Arc B60 Pro GPUs available to regular consumers, with up to 48GB VRAM</title>
    <updated>2026-01-16T14:28:44+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qehf0p/maxsun_joins_sparkle_in_making_intel_arc_b60_pro/"&gt; &lt;img alt="Maxsun joins Sparkle in making Intel Arc B60 Pro GPUs available to regular consumers, with up to 48GB VRAM" src="https://external-preview.redd.it/L3K-FrP5rshi6B9P4GPKPRWImqHp_K0A7GfSUbA2aKk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e8fca654e67fa2c4d7aa0fa049bfeb96b0c4231" title="Maxsun joins Sparkle in making Intel Arc B60 Pro GPUs available to regular consumers, with up to 48GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pcguide.com/news/maxsun-joins-sparkle-in-making-intel-arc-b60-pro-gpus-available-to-regular-consumers-with-up-to-48gb-vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qehf0p/maxsun_joins_sparkle_in_making_intel_arc_b60_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qehf0p/maxsun_joins_sparkle_in_making_intel_arc_b60_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T14:28:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeupjy</id>
    <title>Made one more step towards getting Offloom on steam! (for free).</title>
    <updated>2026-01-16T22:44:38+00:00</updated>
    <author>
      <name>/u/Little-Put6364</name>
      <uri>https://old.reddit.com/user/Little-Put6364</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeupjy/made_one_more_step_towards_getting_offloom_on/"&gt; &lt;img alt="Made one more step towards getting Offloom on steam! (for free)." src="https://preview.redd.it/m5gfmmmy8sdg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a53a44384c69445581fe10c8acfc0a066c9b0488" title="Made one more step towards getting Offloom on steam! (for free)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's taken quite some time to get this to where it is now. But one thing I noticed is most open source tools are designed with technical folks in mind. I wanted to create a tool that comes preset up. Something for the less technical folks who are interested in AI but don't want to spend time learning how to use local tooling and models. Basically chatGPT levels of ease of use and set up.&lt;/p&gt; &lt;p&gt;Offloom will ship with Image generation. RAG (document and web) all powered by locally ran open source models. It's designed with 12GB VRAM in mind. I might be able to drop it to 8GB, but that's untested so far in the quality sense. It juggles multiple models in an agentic way to help with answer quality. It's a step above the basic implementations you'll find all over the place, but by no means is this ground breaking in the field. Just bringing architectures available in the online third party tools to local users. &lt;/p&gt; &lt;p&gt;I'm probably still a bit from launch as I have a lot of UI/UX polishing that needs to be done. But sometime soon I'll be making a call for some beta testers. Keep an eye out if you're interested! The steam page is currently under review. As long as I filled everything out correctly it should pop up in the next 3-5 days for wish listing! I'm setting a tentative launch date for March. However, that largely depends on how many beta testers I can get with different hardware, and how busy my day job gets between now and then. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Little-Put6364"&gt; /u/Little-Put6364 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m5gfmmmy8sdg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeupjy/made_one_more_step_towards_getting_offloom_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeupjy/made_one_more_step_towards_getting_offloom_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T22:44:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeley8</id>
    <title>vLLM-MLX: Native Apple Silicon LLM inference - 464 tok/s on M4 Max</title>
    <updated>2026-01-16T16:56:21+00:00</updated>
    <author>
      <name>/u/waybarrios</name>
      <uri>https://old.reddit.com/user/waybarrios</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I built vLLM-MLX - a framework that uses Apple's MLX for native GPU acceleration.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- OpenAI-compatible API (drop-in replacement for your existing code)&lt;/p&gt; &lt;p&gt;- Multimodal support: Text, Images, Video, Audio - all in one server&lt;/p&gt; &lt;p&gt;- Continuous batching for concurrent users (3.4x speedup)&lt;/p&gt; &lt;p&gt;- TTS in 10+ languages (Kokoro, Chatterbox models)&lt;/p&gt; &lt;p&gt;- MCP tool calling support&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance on M4 Max:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Llama-3.2-1B-4bit → 464 tok/s&lt;/p&gt; &lt;p&gt;- Qwen3-0.6B → 402 tok/s&lt;/p&gt; &lt;p&gt;- Whisper STT → 197x real-time&lt;/p&gt; &lt;p&gt;Works with standard OpenAI Python SDK - just point it to localhost.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/waybarrios/vllm-mlx"&gt;https://github.com/waybarrios/vllm-mlx&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or take feature requests!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waybarrios"&gt; /u/waybarrios &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeley8/vllmmlx_native_apple_silicon_llm_inference_464/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeley8/vllmmlx_native_apple_silicon_llm_inference_464/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeley8/vllmmlx_native_apple_silicon_llm_inference_464/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T16:56:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf1msz</id>
    <title>Cowork but with local models not to send all your data to a remote cloud!</title>
    <updated>2026-01-17T03:05:24+00:00</updated>
    <author>
      <name>/u/clem59480</name>
      <uri>https://old.reddit.com/user/clem59480</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf1msz/cowork_but_with_local_models_not_to_send_all_your/"&gt; &lt;img alt="Cowork but with local models not to send all your data to a remote cloud!" src="https://external-preview.redd.it/bXh5cnloeGpzdGRnMQQmqaqY6IxBgH-vwmsMWuXB8i4MNI5FplyyvMhZJ44G.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c0f045206bb7f1efd15ba454efa839b61d2ec86" title="Cowork but with local models not to send all your data to a remote cloud!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem59480"&gt; /u/clem59480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wmrdxexjstdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf1msz/cowork_but_with_local_models_not_to_send_all_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf1msz/cowork_but_with_local_models_not_to_send_all_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T03:05:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfcap8</id>
    <title>I implemented a GPT-style model from scratch using PyTorch while reading Sebastian Raschka's book</title>
    <updated>2026-01-17T12:44:06+00:00</updated>
    <author>
      <name>/u/Bthreethree</name>
      <uri>https://old.reddit.com/user/Bthreethree</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've spent the last few weeks building a GPT-style LLM entirely from scratch in PyTorch to understand the architecture. This isn't just a wrapper; it's a full implementation covering the entire lifecycle from tokenization to instruction fine-tuning.&lt;/p&gt; &lt;p&gt;I have followed Sebastian Raschka's 'Build a LLM from Scratch' book for the implementation, here is the breakdown of the repo:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Data &amp;amp; Tokenization (&lt;/strong&gt;&lt;code&gt;src/data.py&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt; Instead of using pre-built tokenizers, I implemented:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;SimpleTokenizerV2&lt;/code&gt;: Handles regex-based splitting and special tokens (&lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;|unk|&amp;gt;&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;code&gt;GPTDatasetV1&lt;/code&gt;: A sliding-window dataset implementation for efficient autoregressive training.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. The Attention Mechanism (&lt;/strong&gt;&lt;code&gt;src/attention.py&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I manually implemented &lt;code&gt;MultiHeadAttention&lt;/code&gt; to understand the tensor math:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Handles the query/key/value projections and splitting heads.&lt;/li&gt; &lt;li&gt;Implements the &lt;strong&gt;Causal Mask&lt;/strong&gt; (using &lt;code&gt;register_buffer&lt;/code&gt;) to prevent the model from &amp;quot;cheating&amp;quot; by seeing future tokens.&lt;/li&gt; &lt;li&gt;Includes &lt;code&gt;SpatialDropout&lt;/code&gt; and scaled dot-product attention.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. The GPT Architecture (&lt;/strong&gt;&lt;code&gt;src/model.py&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt; A complete 124M parameter model assembly:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Combines &lt;code&gt;TransformerBlock&lt;/code&gt;, &lt;code&gt;LayerNorm&lt;/code&gt;, and &lt;code&gt;GELU&lt;/code&gt; activations.&lt;/li&gt; &lt;li&gt;Features positional embeddings and residual connections exactly matching the GPT-2 spec.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;4. Training &amp;amp; Generation (&lt;/strong&gt;&lt;code&gt;src/train.py&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Custom training loop with loss visualization.&lt;/li&gt; &lt;li&gt;Implements &lt;code&gt;generate()&lt;/code&gt; with &lt;strong&gt;Top-K sampling&lt;/strong&gt; and &lt;strong&gt;Temperature scaling&lt;/strong&gt; to control output creativity.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;5. Fine-tuning:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Classification (&lt;/strong&gt;&lt;code&gt;src/finetune_classification.py&lt;/code&gt;&lt;strong&gt;):&lt;/strong&gt; Adapted the backbone to detect Spam/Ham messages (90%+ accuracy on the test set).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruction Tuning (&lt;/strong&gt;&lt;code&gt;src/finetune_instructions.py&lt;/code&gt;&lt;strong&gt;):&lt;/strong&gt; Implemented an Alpaca-style training loop. The model can now handle instruction-response pairs rather than just completing text.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/Nikshaan/llm-from-scratch"&gt;https://github.com/Nikshaan/llm-from-scratch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’ve tried to comment every shape transformation in the code. If you are learning this stuff too, I hope this reference helps!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bthreethree"&gt; /u/Bthreethree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcap8/i_implemented_a_gptstyle_model_from_scratch_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcap8/i_implemented_a_gptstyle_model_from_scratch_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcap8/i_implemented_a_gptstyle_model_from_scratch_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T12:44:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qennp2</id>
    <title>performance benchmarks (72GB VRAM) - llama.cpp server - January 2026</title>
    <updated>2026-01-16T18:15:43+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qennp2/performance_benchmarks_72gb_vram_llamacpp_server/"&gt; &lt;img alt="performance benchmarks (72GB VRAM) - llama.cpp server - January 2026" src="https://b.thumbs.redditmedia.com/srFJwhozimC0YMyd6HH4eWUiiMyisE_g6jJwdqP9TVg.jpg" title="performance benchmarks (72GB VRAM) - llama.cpp server - January 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is meant to demonstrate what models can (or can't) be realistically run and used on 72 GB VRAM.&lt;/p&gt; &lt;p&gt;My setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Three RTX 3090 GPUs&lt;/li&gt; &lt;li&gt;X399 motherboard + Ryzen Threadripper 1920X&lt;/li&gt; &lt;li&gt;DDR4 RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I use the default &lt;code&gt;llama-fit&lt;/code&gt; mechanism, so you can probably get better performance with manual &lt;code&gt;--n-cpu-moe&lt;/code&gt; or &lt;code&gt;-ot&lt;/code&gt; tuning.&lt;/p&gt; &lt;p&gt;I always use all three GPUs, smaller models often run faster with one or two GPUs.&lt;/p&gt; &lt;p&gt;I measure &lt;strong&gt;speed only&lt;/strong&gt;, not accuracy, this says nothing about the quality of these models.&lt;/p&gt; &lt;p&gt;This is &lt;strong&gt;not scientific at all&lt;/strong&gt; (see the screenshots). I simply generate two short sentences per model.&lt;/p&gt; &lt;p&gt;tokens/s:&lt;/p&gt; &lt;p&gt;ERNIE-4.5-21B-A3B-Thinking-Q8_0 — &lt;strong&gt;147.85&lt;/strong&gt;&lt;br /&gt; Qwen_Qwen3-VL-30B-A3B-Instruct-Q8_0 — &lt;strong&gt;131.20&lt;/strong&gt;&lt;br /&gt; gpt-oss-120b-mxfp4 — &lt;strong&gt;130.23&lt;/strong&gt;&lt;br /&gt; nvidia_Nemotron-3-Nano-30B-A3B — &lt;strong&gt;128.16&lt;/strong&gt;&lt;br /&gt; inclusionAI_Ling-flash-2.0-Q4_K_M — &lt;strong&gt;116.49&lt;/strong&gt;&lt;br /&gt; GroveMoE-Inst.Q8_0 — &lt;strong&gt;91.00&lt;/strong&gt;&lt;br /&gt; Qwen_Qwen3-Next-80B-A3B-Instruct-Q5_K_M — &lt;strong&gt;68.58&lt;/strong&gt;&lt;br /&gt; Solar-Open-100B.q4_k_m — &lt;strong&gt;67.15&lt;/strong&gt;&lt;br /&gt; ai21labs_AI21-Jamba2-Mini-Q8_0 — &lt;strong&gt;58.53&lt;/strong&gt;&lt;br /&gt; ibm-granite_granite-4.0-h-small-Q8_0 — &lt;strong&gt;57.79&lt;/strong&gt;&lt;br /&gt; GLM-4.5-Air-UD-Q4_K_XL — &lt;strong&gt;54.31&lt;/strong&gt;&lt;br /&gt; Hunyuan-A13B-Instruct-UD-Q6_K_XL — &lt;strong&gt;45.85&lt;/strong&gt;&lt;br /&gt; dots.llm1.inst-Q4_0 — &lt;strong&gt;33.27&lt;/strong&gt;&lt;br /&gt; Llama-4-Scout-17B-16E-Instruct-Q5_K_M — &lt;strong&gt;33.03&lt;/strong&gt;&lt;br /&gt; mistralai_Magistral-Small-2507-Q8_0 — &lt;strong&gt;32.98&lt;/strong&gt;&lt;br /&gt; google_gemma-3-27b-it-Q8_0 — &lt;strong&gt;26.96&lt;/strong&gt;&lt;br /&gt; MiniMax-M2.1-Q3_K_M — &lt;strong&gt;24.68&lt;/strong&gt;&lt;br /&gt; EXAONE-4.0-32B.Q8_0 — &lt;strong&gt;24.11&lt;/strong&gt;&lt;br /&gt; Qwen3-32B-Q8_0 — &lt;strong&gt;23.67&lt;/strong&gt;&lt;br /&gt; allenai_Olmo-3.1-32B-Think-Q8_0 — &lt;strong&gt;23.23&lt;/strong&gt;&lt;br /&gt; NousResearch_Hermes-4.3-36B-Q8_0 — &lt;strong&gt;21.91&lt;/strong&gt;&lt;br /&gt; ByteDance-Seed_Seed-OSS-36B-Instruct-Q8_0 — &lt;strong&gt;21.61&lt;/strong&gt;&lt;br /&gt; Falcon-H1-34B-Instruct-UD-Q8_K_XL — &lt;strong&gt;19.56&lt;/strong&gt;&lt;br /&gt; Llama-3.3-70B-Instruct-Q4_K_M — &lt;strong&gt;19.18&lt;/strong&gt;&lt;br /&gt; swiss-ai_Apertus-70B-Instruct-2509-Q4_K_M — &lt;strong&gt;18.37&lt;/strong&gt;&lt;br /&gt; Qwen2.5-72B-Instruct-Q4_K_M — &lt;strong&gt;17.51&lt;/strong&gt;&lt;br /&gt; Llama-3.3-Nemotron-Super-49B-v1_5-Q8_0 — &lt;strong&gt;16.16&lt;/strong&gt;&lt;br /&gt; Qwen3-VL-235B-A22B-Instruct-Q3_K_M — &lt;strong&gt;13.54&lt;/strong&gt;&lt;br /&gt; Mistral-Large-Instruct-2407-Q4_K_M — &lt;strong&gt;6.40&lt;/strong&gt;&lt;br /&gt; grok-2.Q2_K — &lt;strong&gt;4.63&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qennp2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qennp2/performance_benchmarks_72gb_vram_llamacpp_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qennp2/performance_benchmarks_72gb_vram_llamacpp_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T18:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qek917</id>
    <title>I reproduced DeepSeek's mHC at 1.7B params (8xH100). The instability is 3x worse than reported (10k vs 3k), but the model didn't explode.</title>
    <updated>2026-01-16T16:14:54+00:00</updated>
    <author>
      <name>/u/poisson_labs</name>
      <uri>https://old.reddit.com/user/poisson_labs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qek917/i_reproduced_deepseeks_mhc_at_17b_params_8xh100/"&gt; &lt;img alt="I reproduced DeepSeek's mHC at 1.7B params (8xH100). The instability is 3x worse than reported (10k vs 3k), but the model didn't explode." src="https://b.thumbs.redditmedia.com/zJw14CUcgynBl13gd34PUFykjliAEc5ivAIvrmPoRJA.jpg" title="I reproduced DeepSeek's mHC at 1.7B params (8xH100). The instability is 3x worse than reported (10k vs 3k), but the model didn't explode." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Following up on my previous post about reproducing the DeepSeek-V2/V3 architecture. I decided to bite the bullet and rent an H100 cluster to scale the &amp;quot;Hyper-Connections&amp;quot; (HC) experiment from 10M to 1.7B parameter&lt;/p&gt; &lt;p&gt;The DeepSeek paper warned that standard Hyper-Connections cause signal variance to explode by ~3,000x at 27B parameters. I wanted to see if that held true or if it was a theoretical upper bound.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Results:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;It's worse than they said.&lt;/strong&gt; At just 1.7B parameters, I measured signal amplification of &lt;strong&gt;10,924x&lt;/strong&gt;. The &amp;quot;Instability Bomb&amp;quot; is real.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Twist&amp;quot;:&lt;/strong&gt; Despite signals amplifying by 10,000x, the loss &lt;strong&gt;didn't diverge&lt;/strong&gt;. The model kept learning. My theory is that modern optimizers (AdamW) and gradient clipping work overtime to mask the issue, but it's basically a ticking time bomb for longer runs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Fix:&lt;/strong&gt; Verified that Manifold Hyper-Connections (mHC) with Sinkhorn projection completely solves this. Variance stays locked at 1.0x with zero compute overhead.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a1gsgd87kqdg1.png?width=4160&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d75dc5207b1401eed9fe3a8e3425e24fe560fc0"&gt;https://preview.redd.it/a1gsgd87kqdg1.png?width=4160&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d75dc5207b1401eed9fe3a8e3425e24fe560fc0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wrote up the full breakdown with the loss curves and Amax graphs here: &lt;a href="https://taylorkolasinski.com/notes/mhc-reproduction-part2/"&gt;https://taylorkolasinski.com/notes/mhc-reproduction-part2/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 1 can be found here: &lt;a href="https://taylorkolasinski.com/notes/mhc-reproduction/"&gt;https://taylorkolasinski.com/notes/mhc-reproduction/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, there's a discussion on HN right now if you want to chat there: &lt;a href="https://news.ycombinator.com/newest?next=46647671&amp;amp;n=31"&gt;https://news.ycombinator.com/newest?next=46647671&amp;amp;n=31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the H100 setup or the implementation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/poisson_labs"&gt; /u/poisson_labs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qek917/i_reproduced_deepseeks_mhc_at_17b_params_8xh100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qek917/i_reproduced_deepseeks_mhc_at_17b_params_8xh100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qek917/i_reproduced_deepseeks_mhc_at_17b_params_8xh100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T16:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qew9df</id>
    <title>Best coding models for RTX 6000 Pro Blackwell</title>
    <updated>2026-01-16T23:39:49+00:00</updated>
    <author>
      <name>/u/az_6</name>
      <uri>https://old.reddit.com/user/az_6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have a RTX 6000 Pro Blackwell (96GB VRAM) and trying to decide what model is best for agentic coding with Aider/OpenCode. What have folks tried and anyone found anything that gets close to Sonnet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/az_6"&gt; /u/az_6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qew9df/best_coding_models_for_rtx_6000_pro_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qew9df/best_coding_models_for_rtx_6000_pro_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qew9df/best_coding_models_for_rtx_6000_pro_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T23:39:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfe81r</id>
    <title>vLLM with offloading vs. llama.cpp?</title>
    <updated>2026-01-17T14:12:53+00:00</updated>
    <author>
      <name>/u/Careful_Breath_1108</name>
      <uri>https://old.reddit.com/user/Careful_Breath_1108</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Getting ~17 tps token generation on gptoss120b with 128gb 2400 ddr4 ram and 32gb of rtx nvidia vram using default llama-server settings on llama.cpp. Trying to figure out if it’s possible to squeeze more performance out. &lt;/p&gt; &lt;p&gt;I thought vLLM was only for models that fit fully into vram, but recently found out that vLLM supports model offloading. Was wondering any one has experience with this and how it compares to llamacpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careful_Breath_1108"&gt; /u/Careful_Breath_1108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfe81r/vllm_with_offloading_vs_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfe81r/vllm_with_offloading_vs_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfe81r/vllm_with_offloading_vs_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T14:12:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qewdza</id>
    <title>What are you building with sub-4B LLMs in early 2025? Real-world use wins?</title>
    <updated>2026-01-16T23:44:29+00:00</updated>
    <author>
      <name>/u/Whiplashorus</name>
      <uri>https://old.reddit.com/user/Whiplashorus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, It's early 2025, and I'm diving deep into tiny LLMs (under 4B params) like Qwen3 4B, LFM2.5 1.2B, or LFM2.5 VL 1.6B.&lt;/p&gt; &lt;p&gt;These base models (no fine-tuning) are super lightweight and run anywhere, but I'm curious: what real-world use cases have you found that actually stick ?&lt;/p&gt; &lt;p&gt;Stuff that's genuinely useful day-to-day, not just benchmarks.Have you plugged them into pipelines like n8n, Make.com, or custom scripts? How's that working out?Any cool automations, agents, or edge deployments (phone, Raspberry Pi, etc.)? Please share your successes, setups, or even failure&lt;/p&gt; &lt;p&gt;I'm all ears! What's the most practical thing you've pulled off?&lt;/p&gt; &lt;p&gt;I wished to do something with my vacant homelab &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whiplashorus"&gt; /u/Whiplashorus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qewdza/what_are_you_building_with_sub4b_llms_in_early/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qewdza/what_are_you_building_with_sub4b_llms_in_early/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qewdza/what_are_you_building_with_sub4b_llms_in_early/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T23:44:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qefa7q</id>
    <title>GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)</title>
    <updated>2026-01-16T12:59:07+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/"&gt; &lt;img alt="GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)" src="https://external-preview.redd.it/t4cNt5D638DSOJgsxl8f-7IwJhLpxHIh7HxK5GHcBJE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b72b5025e78c2cc97de15c8fea348f262235ecb" title="GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I’m Anton from Nebius.&lt;/p&gt; &lt;p&gt;We’ve updated the &lt;strong&gt;SWE-bench leaderboard&lt;/strong&gt; with our &lt;strong&gt;December runs&lt;/strong&gt; on &lt;strong&gt;48 fresh GitHub PR tasks&lt;/strong&gt; (PRs created in the previous month only). The setup is standard SWE-bench: models read real PR issues, edit code, run tests, and must make the full suite pass.&lt;/p&gt; &lt;p&gt;A few observations from this release:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Claude Opus 4.5&lt;/strong&gt; leads this snapshot at &lt;strong&gt;63.3% resolved rate&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT-5.2 (extra high effort)&lt;/strong&gt; follows closely at &lt;strong&gt;61.5%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Flash Preview&lt;/strong&gt; slightly outperforms &lt;strong&gt;Gemini 3 Pro Preview&lt;/strong&gt; (60.0% vs 58.9%), despite being smaller and cheaper.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7&lt;/strong&gt; is currently the strongest open-source model on the leaderboard, ranking alongside closed models like GPT-5.1-codex.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT-OSS-120B&lt;/strong&gt; shows a large jump in performance when run in high-effort reasoning mode, highlighting the impact of inference-time scaling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to your thoughts and feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=dec_2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T12:59:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qee2de</id>
    <title>I fucking love this community</title>
    <updated>2026-01-16T11:57:48+00:00</updated>
    <author>
      <name>/u/alhinai_03</name>
      <uri>https://old.reddit.com/user/alhinai_03</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you guys, thanks to everyone who took the time to write a comment or a post explaining, teaching people how things work, the people behind llama.cpp, vllm, and all the contributors who keep the open-source community thriving.&lt;/p&gt; &lt;p&gt;I'm able to run huge models on my weak ass pc from 10 years ago relatively fast, my fastest one being nemotron-3-nano-30B-a3b-iq4_nl running @14-13.5 t/s with 65k context. While my actual GPU having only 4GB of vram, that's fucking ridiculous and it blows my mind everytime that I'm able to run these models.&lt;/p&gt; &lt;p&gt;What's been key for me is having a good amount of system memory, and as long as the model is a MoE architecture they run pretty decently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alhinai_03"&gt; /u/alhinai_03 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T11:57:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfbt9f</id>
    <title>Local Replacement for Phind.com</title>
    <updated>2026-01-17T12:19:01+00:00</updated>
    <author>
      <name>/u/Past-Economist7732</name>
      <uri>https://old.reddit.com/user/Past-Economist7732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As many are aware, &lt;a href="https://www.phind.com/"&gt;https://www.phind.com/&lt;/a&gt; has shut down. I don’t know how many people on here used it, but I used to love the service back when it was an ai search engine, you could prompt the ai and it would search the internet for relevant info, and ONLY THEN respond. (Don’t get me started on the final iteration of phind, the atrocious “I’m going to build you a website to answer your question”, that was not useful to me). &lt;/p&gt; &lt;p&gt;Is there any way to recreate ai search behavior with local models? Maybe with openwebui somehow? There are some agentic workflows that can kick out to do a web search but sometimes I want to begin with the search and see the results. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Past-Economist7732"&gt; /u/Past-Economist7732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfbt9f/local_replacement_for_phindcom/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfbt9f/local_replacement_for_phindcom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfbt9f/local_replacement_for_phindcom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T12:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeuh0z</id>
    <title>Prompt Repetition Improves Non-Reasoning LLMs - a paper</title>
    <updated>2026-01-16T22:35:01+00:00</updated>
    <author>
      <name>/u/Foreign-Beginning-49</name>
      <uri>https://old.reddit.com/user/Foreign-Beginning-49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2512.14982"&gt;https://arxiv.org/pdf/2512.14982&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I love these little tiny prompt techniques that can potentially lead to greater model accuracy and performance. Simply repeating the prompt twice lead to notable performance gains.&lt;/p&gt; &lt;p&gt;From the paper:&lt;/p&gt; &lt;p&gt;&amp;quot;We show that repeating the prompts consistently improves model performance for a range of models and benchmarks, when not using reasoning. In addition, latency is not impacted, as only the parallelizable pre-fill stage is affected. Prompt repetition does not change the lengths or formats of the generated outputs, and it might be a good default for many models and tasks, when reasoning is not used.&lt;/p&gt; &lt;p&gt;So simple but they demonstrate impressive gains on several benchmark scores. Looks like Deepseek is the only open weights model put through the wringer.&lt;/p&gt; &lt;p&gt;Best of wishes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Foreign-Beginning-49"&gt; /u/Foreign-Beginning-49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T22:35:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfcg4h</id>
    <title>Need to know more about less known engines (ik_llama.cpp, exllamav3..)</title>
    <updated>2026-01-17T12:51:26+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I usually stick to llama.cpp and vllm but llama.cpp speed may not be the best and vllm/sglang can be really annoying if you have several gpus without respecting the power of 2 for tp.&lt;/p&gt; &lt;p&gt;So, for people who really know others projects (I mainly know ik_llama and exl3) could you please provide some feedback on where they really shine and what are their main constraints and limits (model/hardware support, tool calling, stability…).&lt;/p&gt; &lt;p&gt;Testing / understanding stuff may take some time so any usefull info is good to have, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcg4h/need_to_know_more_about_less_known_engines_ik/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcg4h/need_to_know_more_about_less_known_engines_ik/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcg4h/need_to_know_more_about_less_known_engines_ik/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T12:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qff481</id>
    <title>I built Adaptive-K routing: 30-52% compute savings on MoE models (Mixtral, Qwen, OLMoE)</title>
    <updated>2026-01-17T14:50:29+00:00</updated>
    <author>
      <name>/u/Fuzzy_Ad_1390</name>
      <uri>https://old.reddit.com/user/Fuzzy_Ad_1390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Links&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Gabrobals/sbm-efficient"&gt;https://github.com/Gabrobals/sbm-efficient&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Whitepaper: &lt;a href="https://adaptive-k.vercel.app/whitepaper.html"&gt;https://adaptive-k.vercel.app/whitepaper.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TensorRT-LLM PR: &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/pull/10672"&gt;https://github.com/NVIDIA/TensorRT-LLM/pull/10672&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Live demo: &lt;a href="https://huggingface.co/spaces/Gabrobals/adaptive-k-demo"&gt;https://huggingface.co/spaces/Gabrobals/adaptive-k-demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or discuss implementation details!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fuzzy_Ad_1390"&gt; /u/Fuzzy_Ad_1390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://adaptive-k.vercel.app/whitepaper.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qff481/i_built_adaptivek_routing_3052_compute_savings_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qff481/i_built_adaptivek_routing_3052_compute_savings_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T14:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf514i</id>
    <title>"Welcome to the Local Llama. How janky's your rig?</title>
    <updated>2026-01-17T05:44:01+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/"&gt; &lt;img alt="&amp;quot;Welcome to the Local Llama. How janky's your rig?" src="https://preview.redd.it/rzbni3vvkudg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d48b6efbc81ebde1857313c676df8a19d5193dbc" title="&amp;quot;Welcome to the Local Llama. How janky's your rig?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rzbni3vvkudg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T05:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfaxpx</id>
    <title>Analysis of running local LLMs on Blackwell GPUs. TLDR: cheaper to run than cloud api services</title>
    <updated>2026-01-17T11:30:51+00:00</updated>
    <author>
      <name>/u/cchung261</name>
      <uri>https://old.reddit.com/user/cchung261</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;May provide support to management for the cost savings of running local LLMs. The paper also includes amortization costs for the GPUs. I was surprised by the findings and the short break even time with cloud api costs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2601.09527"&gt;https://arxiv.org/abs/2601.09527&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cchung261"&gt; /u/cchung261 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfaxpx/analysis_of_running_local_llms_on_blackwell_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfaxpx/analysis_of_running_local_llms_on_blackwell_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfaxpx/analysis_of_running_local_llms_on_blackwell_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T11:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf77hw</id>
    <title>Nvidia GH200 and AMD Mi325X can be shipped to china now.</title>
    <updated>2026-01-17T07:44:59+00:00</updated>
    <author>
      <name>/u/GPTshop--dot--ai</name>
      <uri>https://old.reddit.com/user/GPTshop--dot--ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf77hw/nvidia_gh200_and_amd_mi325x_can_be_shipped_to/"&gt; &lt;img alt="Nvidia GH200 and AMD Mi325X can be shipped to china now." src="https://preview.redd.it/qlc9o31a6vdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7748ec7117693d51bc563327e85220798d4f7a84" title="Nvidia GH200 and AMD Mi325X can be shipped to china now." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The US export controls have been amended. GH200 and Mi325X can be shipped to china now. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GPTshop--dot--ai"&gt; /u/GPTshop--dot--ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qlc9o31a6vdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf77hw/nvidia_gh200_and_amd_mi325x_can_be_shipped_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf77hw/nvidia_gh200_and_amd_mi325x_can_be_shipped_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T07:44:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfb0gk</id>
    <title>KoboldCpp v1.106 finally adds MCP server support, drop-in replacement for Claude Desktop</title>
    <updated>2026-01-17T11:35:11+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, it's been a hot minute, but I thought I'd share this here since it's quite a big new feature. &lt;/p&gt; &lt;p&gt;Yes, KoboldCpp is still alive and kicking. And besides the major UI overhaul, we've finally added native MCP support in KoboldCpp v1.106! It's designed to be a painless Claude Desktop drop-in replacement with maximum compatibility, the &lt;code&gt;mcp.json&lt;/code&gt; uses the same format so you can swap it in easily. &lt;/p&gt; &lt;p&gt;The KoboldCpp MCP bridge will connect to all provided MCP servers (HTTP and STDIO transports both supported) and automatically forward requests for tools the AI selects to the correct MCP server. This MCP bridge can also be used by third party clients.&lt;/p&gt; &lt;p&gt;On the frontend side, you can fetch the list of all tools from all servers, select the tools you want to let AI use, and optionally enable tool call approvals.&lt;/p&gt; &lt;p&gt;Some demo screenshots of various tool servers being used: &lt;a href="https://imgur.com/a/fKeWKUU"&gt;https://imgur.com/a/fKeWKUU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it here:&lt;/strong&gt; &lt;a href="https://github.com/LostRuins/koboldcpp/releases/latest"&gt;&lt;strong&gt;https://github.com/LostRuins/koboldcpp/releases/latest&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;feedback is welcome. cheers! - concedo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T11:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf5oj0</id>
    <title>DeepSeek Engram : A static memory unit for LLMs</title>
    <updated>2026-01-17T06:18:14+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeeepSeek AI released a new paper titled &amp;quot;Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models&amp;quot; introducing Engram. The key idea: instead of recomputing static knowledge (like entities, facts, or patterns) every time through expensive transformer layers, Engram &lt;strong&gt;adds native memory lookup&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Think of it as separating &lt;strong&gt;remembering from reasoning&lt;/strong&gt;. Traditional MoE focuses on conditional computation, Engram introduces &lt;strong&gt;conditional memory&lt;/strong&gt;. Together, they let LLMs reason deeper, handle long contexts better, and offload early-layer compute from GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Knowledge is &lt;strong&gt;looked up in O(1)&lt;/strong&gt; instead of recomputed.&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;explicit parametric memory&lt;/strong&gt; vs implicit weights only.&lt;/li&gt; &lt;li&gt;Improves reasoning, math, and code performance.&lt;/li&gt; &lt;li&gt;Enables massive memory scaling &lt;strong&gt;without GPU limits&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Frees attention for &lt;strong&gt;global reasoning&lt;/strong&gt; rather than static knowledge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Paper : &lt;a href="https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf"&gt;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video explanation : &lt;a href="https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub"&gt;https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T06:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
