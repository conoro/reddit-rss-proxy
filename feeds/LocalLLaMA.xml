<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-08T20:49:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1o1e0hq</id>
    <title>Required Reading for Llama.cpp Flags?</title>
    <updated>2025-10-08T15:40:27+00:00</updated>
    <author>
      <name>/u/Infamous_Jaguar_2151</name>
      <uri>https://old.reddit.com/user/Infamous_Jaguar_2151</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to level up on llama.cpp and keep running into fragmented docs. There are a lot of startup flags (and some env-var twins), and some tuning seems hardware-specific (EPYC CPUs, multi-GPU splits, Flash-Attention, NUMA, etc.).&lt;/p&gt; &lt;p&gt;Two asks:&lt;/p&gt; &lt;p&gt;1). Best resources that actually explain the flags. Links welcome to any of these, with a note on why you like them:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Official docs/pages (CLI/server/tools), manpages, source files that define the args, curated guides, blog posts, or wikis.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Hardware-specific writeups (EPYC/NUMA, CUDA vs HIP vs Metal vs Vulkan, multi-GPU split strategies).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;“Gotchas” posts (batch vs ubatch, RoPE scaling, KV-cache formats, mlock/mmap, etc.).&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;2). After it’s running: which settings are flags vs per-request?&lt;/p&gt; &lt;p&gt;For generation parameters and server behavior (e.g., temperature, top_p/top_k/min_p, repetition penalty, Mirostat, grammar/JSON schema, draft/speculative decoding, context/rope scaling, concurrency limits), which of these:&lt;/p&gt; &lt;p&gt;A. must be set at startup via flags or env vars,&lt;/p&gt; &lt;p&gt;B. can be changed per request (e.g., HTTP/JSON to llama-server), and&lt;/p&gt; &lt;p&gt;C. can be changed interactively without a restart (if at all)?&lt;/p&gt; &lt;p&gt;If you share your own working presets, please include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware (CPU model + cores/NUMA, GPU model/VRAM, RAM).&lt;/li&gt; &lt;li&gt;Backend (CUDA/HIP/Metal/Vulkan/OpenCL/SYCL) and your build options.&lt;/li&gt; &lt;li&gt;llama.cpp version/commit and the model + context length.&lt;/li&gt; &lt;li&gt;Your key flags (threads, batch/ubatch, n-gpu-layers, split-mode, rope-scaling, cache types, mlock/mmap, etc.) and why you chose them.&lt;/li&gt; &lt;li&gt;Before/after tokens/sec or latency numbers if you have them.&lt;/li&gt; &lt;li&gt;A link to any reference you leaned on.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infamous_Jaguar_2151"&gt; /u/Infamous_Jaguar_2151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1e0hq/required_reading_for_llamacpp_flags/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1e0hq/required_reading_for_llamacpp_flags/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1e0hq/required_reading_for_llamacpp_flags/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T15:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1hr7c</id>
    <title>What do I need to learn to run LLMs on my computer?</title>
    <updated>2025-10-08T17:56:37+00:00</updated>
    <author>
      <name>/u/glassorangebird</name>
      <uri>https://old.reddit.com/user/glassorangebird</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don’t have any formal education in programming or computers, but I’d like to think I’m decently tech savvy and can usually accomplish what I want.&lt;/p&gt; &lt;p&gt;I spent some time trying to set up a TTS LLM last night, and I had difficulty with things that are probably not particularly difficult - using Visual Code Studio (I was working out of notepad files at first), handling Python packages and virtual packages, and setting up a GUI so that I’m not working directly with the code.&lt;/p&gt; &lt;p&gt;I managed to do all of the above, but I don’t feel confident in what I’m doing. I had a lot of trouble with correct Python versions and missing the appropriate packages. This likely goes beyond just LLM information, but I’d love if anyone could point me in the right direction to pick up the basics of setting these up and the coding-specific skills I may need.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/glassorangebird"&gt; /u/glassorangebird &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1hr7c/what_do_i_need_to_learn_to_run_llms_on_my_computer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1hr7c/what_do_i_need_to_learn_to_run_llms_on_my_computer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1hr7c/what_do_i_need_to_learn_to_run_llms_on_my_computer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T17:56:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o13oyn</id>
    <title>30B models at full-size, or 120B models at Q4?</title>
    <updated>2025-10-08T07:09:35+00:00</updated>
    <author>
      <name>/u/arimoto02</name>
      <uri>https://old.reddit.com/user/arimoto02</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a set up with an NVIDIA A100 80GB. Should i run 30B-ish models (like qwen32b) at full size or 120B-ish models (GLM4.5 air?) at Q4?&lt;/p&gt; &lt;p&gt;Also, is there any comprehensive comparison for model degradation with respect to their size/quantize level?&lt;/p&gt; &lt;p&gt;Thank you all!&lt;/p&gt; &lt;p&gt;Edit: Really sorry guys, i somehow remember that there's a qwen3 120b moe (lol). Fixed the post to qwen3 8b vs qwen3 32b.&lt;/p&gt; &lt;p&gt;Edit2: i realized the qwen3 8b vs 32b doesnt really fits with the A100 settings, so i changed it to arbitrary models &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arimoto02"&gt; /u/arimoto02 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o13oyn/30b_models_at_fullsize_or_120b_models_at_q4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o13oyn/30b_models_at_fullsize_or_120b_models_at_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o13oyn/30b_models_at_fullsize_or_120b_models_at_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T07:09:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1gym0</id>
    <title>Sharing my free tool for easy handwritten fine-tuning datasets!</title>
    <updated>2025-10-08T17:27:47+00:00</updated>
    <author>
      <name>/u/ella0333</name>
      <uri>https://old.reddit.com/user/ella0333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I wanted to share a tool that I created for making hand written fine-tuning datasets, originally I built this for myself when I was unable to find conversational datasets formatted the way I needed when I was fine-tuning for the first time and hand typing JSON files seemed like some sort of torture so I built a little simple UI for myself to auto format everything for me. &lt;/p&gt; &lt;p&gt;I originally built this back when I was a beginner, so it is very easy to use with no prior dataset creation/formatting experience, but also has a bunch of added features I believe more experienced devs would appreciate!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I have expanded it to support :&lt;/strong&gt;&lt;br /&gt; - many formats; chatml/chatgpt, alpaca, and sharegpt/vicuna&lt;br /&gt; - multi-turn dataset creation, not just pair-based&lt;br /&gt; - token counting from various models&lt;br /&gt; - custom fields (instructions, system messages, custom IDs),&lt;br /&gt; - auto saves and every format type is written at once&lt;br /&gt; - formats like alpaca have no need for additional data besides input and output, as default instructions are auto-applied (customizable)&lt;br /&gt; - goal tracking bar&lt;/p&gt; &lt;p&gt;I know it seems a bit crazy to be manually typing out datasets, but handwritten data is great for customizing your LLMs and keeping them high-quality. I wrote a 1k interaction conversational dataset within a month during my free time, and this made it much more mindless and easy. &lt;/p&gt; &lt;p&gt;I hope you enjoy! I will be adding new formats over time, depending on what becomes popular or is asked for&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/1mcYsDrXHAA"&gt;&lt;strong&gt;Video Demo&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/ella0333/LLM-Scribe"&gt;&lt;strong&gt;Get it here&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ella0333"&gt; /u/ella0333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1gym0/sharing_my_free_tool_for_easy_handwritten/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1gym0/sharing_my_free_tool_for_easy_handwritten/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1gym0/sharing_my_free_tool_for_easy_handwritten/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T17:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o18p7i</id>
    <title>Replacing Google Translate with LLM translation app on smartphone?</title>
    <updated>2025-10-08T12:11:15+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I chat with a thai person semi-often on my phone, in thai. I use Google Translate to convert between Thai and English. &lt;/p&gt; &lt;p&gt;According to them, the thai text I send them is confusing and sometimes completely incomprehensible. And on my own end, often the english translation of what they send me is very unclear.&lt;/p&gt; &lt;p&gt;Today I tried something new: I tried an LLM on OpenRouter for the translation. They immediately understood, and said my messages were leaps and bounds better than Google Translate.&lt;/p&gt; &lt;p&gt;So now I'm looking to replace Google Translate on my Android phone. I'd really like the convenience of a dedicated translation and don't want to be using a browser and typing &amp;quot;Translate the following to/from $lang: my message here&amp;quot; and having to edit the prompt every time.&lt;/p&gt; &lt;p&gt;However, surprisingly I can't find any LLM translation apps on the Play Store or on F-Droid or in this sub's search. Through more googling I finally managed to find one chinese app, but it only supports OpenAI.&lt;/p&gt; &lt;p&gt;Surely I'm not the first person to hit a wall with Google Translate and wants LLM translation on their phone? Are there any obscure apps for this on Github which I can't find?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18p7i/replacing_google_translate_with_llm_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18p7i/replacing_google_translate_with_llm_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o18p7i/replacing_google_translate_with_llm_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T12:11:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o18yl8</id>
    <title>Building a BPE Tokenizer from scratch - optimizations &amp; experiments</title>
    <updated>2025-10-08T12:23:04+00:00</updated>
    <author>
      <name>/u/garg-aayush</name>
      <uri>https://old.reddit.com/user/garg-aayush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18yl8/building_a_bpe_tokenizer_from_scratch/"&gt; &lt;img alt="Building a BPE Tokenizer from scratch - optimizations &amp;amp; experiments" src="https://external-preview.redd.it/NeXI5jnwEOwXIs7_mVe04Ef6iuVmmMHfc9xat36QzrU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1608f9c8291d391bb636cc82c05cdc14a3ead6b" title="Building a BPE Tokenizer from scratch - optimizations &amp;amp; experiments" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like I did in the past with my GPT-2 reimplementation, this time I followed Andrej Karpathy's “Let's build the GPT Tokenizer&amp;quot; video tutorial and implemented a BPE tokenizer from scratch. :-)&lt;/p&gt; &lt;p&gt;I went several steps further by identifying and optimizing major bottlenecks in both training and inference, implementing a Rust version for fast encoding, training custom tokenizers on large datasets, and evaluating their impact on GPT-2 pre-training.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kwaqe610svtf1.png?width=2670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef0332d4cfa57dd26ec66730bc7280b4f12d812a"&gt;BPE implementation from scratch summary&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My optimizations and experiments include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Improving training speed: 50x faster&lt;/strong&gt; (117s → 2.4s for 20 merges)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Making inference faster: 3.7x faster&lt;/strong&gt; with Rust implementation (21.3s → 5.3s)&lt;/li&gt; &lt;li&gt;Training custom 16K tokenizers on &lt;strong&gt;TinyStoriesV2 (~2.6GB)&lt;/strong&gt; and &lt;strong&gt;FineWeb (~3.3GB)&lt;/strong&gt; datasets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pre-training GPT-2 using custom tokenizers&lt;/strong&gt; and comparing their performance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To be honest, I found understanding tokenizer implementation and optimizing it a lot more confusing and harder than GPT-2 implementation (personal experience!) 😅.&lt;/p&gt; &lt;p&gt;In this implementation, I learned a lot about code profiling and optimizing code for both memory and speed. The Rust vibe-coding was fun and surprisingly successful!&lt;/p&gt; &lt;p&gt;Like always, I've documented everything—the code, optimizations, training runs, experiments, and notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo&lt;/strong&gt;: &lt;a href="https://github.com/garg-aayush/building-from-scratch/tree/main/bpe"&gt;https://github.com/garg-aayush/building-from-scratch/tree/main/bpe&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Notes&lt;/strong&gt;: &lt;a href="https://github.com/garg-aayush/building-from-scratch/blob/main/bpe/lecture_notes.md"&gt;https://github.com/garg-aayush/building-from-scratch/blob/main/bpe/lecture_notes.md&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Detailed Readme&lt;/strong&gt;: &lt;a href="https://github.com/garg-aayush/building-from-scratch/blob/main/bpe/Readme.md"&gt;https://github.com/garg-aayush/building-from-scratch/blob/main/bpe/Readme.md&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Commit-by-commit development&lt;/strong&gt;: Each optimization and experiment is a separate commit for easy understanding&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/garg-aayush"&gt; /u/garg-aayush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18yl8/building_a_bpe_tokenizer_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o18yl8/building_a_bpe_tokenizer_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o18yl8/building_a_bpe_tokenizer_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T12:23:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o11udk</id>
    <title>is GTX 3090 24GB GDDR6 good for local coding?</title>
    <updated>2025-10-08T05:16:36+00:00</updated>
    <author>
      <name>/u/TruthTellerTom</name>
      <uri>https://old.reddit.com/user/TruthTellerTom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Codex-CLI API costs are getting expensive quick. Found a local used 24 GB RTX 3090 at around 500 bucks. Would this be a good investment? and what local coding LLM would you guys recommend with it?&lt;/p&gt; &lt;p&gt;Desktop Specs:&lt;br /&gt; i7 12700 (12th Gen), 32GB RAM, windows 11 x64&lt;/p&gt; &lt;p&gt;ENV.&lt;/p&gt; &lt;p&gt;Web Applications with PHP, MySQL, jQuery.&lt;br /&gt; Mainly Boostrap 5 (or latest) for style/theme/ready-to-us components&lt;br /&gt; Solo Dev. I keep things simple, and focus on functions. 99% Functional programming.&lt;br /&gt; I dont use frameworks like laravel, i have my own Js and php lib and helpers for most stuff.&lt;/p&gt; &lt;p&gt;would appreciate some expert advise&lt;br /&gt; Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TruthTellerTom"&gt; /u/TruthTellerTom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o11udk/is_gtx_3090_24gb_gddr6_good_for_local_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o11udk/is_gtx_3090_24gb_gddr6_good_for_local_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o11udk/is_gtx_3090_24gb_gddr6_good_for_local_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T05:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o184q6</id>
    <title>clem from Hugging Face: the community added 1 million new repos (models, datasets, spaces) in the past 90 days! 100% are now powered by Xet, 40% are private repositories. Enterprise hub subscriptions are our fastest growing line of revenue.</title>
    <updated>2025-10-08T11:43:33+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o184q6/clem_from_hugging_face_the_community_added_1/"&gt; &lt;img alt="clem from Hugging Face: the community added 1 million new repos (models, datasets, spaces) in the past 90 days! 100% are now powered by Xet, 40% are private repositories. Enterprise hub subscriptions are our fastest growing line of revenue." src="https://preview.redd.it/capdedupkvtf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1638aaab704ffb77c8b04fc7e98e53289423588a" title="clem from Hugging Face: the community added 1 million new repos (models, datasets, spaces) in the past 90 days! 100% are now powered by Xet, 40% are private repositories. Enterprise hub subscriptions are our fastest growing line of revenue." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Clement Delangue (clem) on 𝕏: &lt;a href="https://x.com/ClementDelangue/status/1975615257923231969"&gt;https://x.com/ClementDelangue/status/1975615257923231969&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/capdedupkvtf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o184q6/clem_from_hugging_face_the_community_added_1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o184q6/clem_from_hugging_face_the_community_added_1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T11:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0ifyr</id>
    <title>Glm 4.6 air is coming</title>
    <updated>2025-10-07T15:46:04+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"&gt; &lt;img alt="Glm 4.6 air is coming" src="https://preview.redd.it/nmwtp72fnptf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78e29ea88c42c50216e45dc228bec7e885394f0c" title="Glm 4.6 air is coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nmwtp72fnptf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ifyr/glm_46_air_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T15:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o16584</id>
    <title>[2510.05688] vAttention: Verified Sparse Attention</title>
    <updated>2025-10-08T09:51:31+00:00</updated>
    <author>
      <name>/u/Elven77AI</name>
      <uri>https://old.reddit.com/user/Elven77AI</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Elven77AI"&gt; /u/Elven77AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2510.05688"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o16584/251005688_vattention_verified_sparse_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o16584/251005688_vattention_verified_sparse_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T09:51:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1eac0</id>
    <title>What models do you find yourself actually using, and what for?</title>
    <updated>2025-10-08T15:50:21+00:00</updated>
    <author>
      <name>/u/sine120</name>
      <uri>https://old.reddit.com/user/sine120</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got into Local LLMs, went down the rabbit hole, thrashed about trying to get my 9070XT to work in Ollama, gave up, and have been having fun in LM Studio since with models like Qwen3 4B/ 30B, gpt-oss-20B.&lt;/p&gt; &lt;p&gt;I wanted to gauge what people actually use instead of just going off benchmarks. What models are you running/ which ones are your favorites? What kind of hardware do you have? What kind of speeds do you see? What do you actually use your local LLMs for?&lt;/p&gt; &lt;p&gt;So far I'm liking gpt-oss and Qwen3 for the speed and usability in my 16GB of VRAM, but wondering if I should consider others.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sine120"&gt; /u/sine120 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1eac0/what_models_do_you_find_yourself_actually_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1eac0/what_models_do_you_find_yourself_actually_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1eac0/what_models_do_you_find_yourself_actually_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T15:50:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0php3</id>
    <title>Granite Docling WebGPU: State-of-the-art document parsing 100% locally in your browser.</title>
    <updated>2025-10-07T20:00:14+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0php3/granite_docling_webgpu_stateoftheart_document/"&gt; &lt;img alt="Granite Docling WebGPU: State-of-the-art document parsing 100% locally in your browser." src="https://external-preview.redd.it/bXZwenNmemR3cXRmMQIkfIP27ngHfIf2o9FEvt2htapLOK3sF-ey3U1M3aWC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=681db9c824b6d01bd93193fb35668e28576d4f98" title="Granite Docling WebGPU: State-of-the-art document parsing 100% locally in your browser." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IBM recently released Granite Docling, a 258M parameter VLM engineered for efficient document conversion. So, I decided to build a demo which showcases the model running entirely in your browser with WebGPU acceleration. Since the model runs locally, no data is sent to a server (perfect for private and sensitive documents).&lt;/p&gt; &lt;p&gt;As always, the demo is available and open source on Hugging Face: &lt;a href="https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU"&gt;https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope you like it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/33mh4fzdwqtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0php3/granite_docling_webgpu_stateoftheart_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0php3/granite_docling_webgpu_stateoftheart_document/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T20:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0zted</id>
    <title>LFM2-8B-A1B | Quality ≈ 3–4B dense, yet faster than Qwen3-1.7B</title>
    <updated>2025-10-08T03:25:57+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"&gt; &lt;img alt="LFM2-8B-A1B | Quality ≈ 3–4B dense, yet faster than Qwen3-1.7B" src="https://external-preview.redd.it/4KqP_OSthEkz01ewHYfZ8d2q4c416HGrMsoPMLQI0Ig.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52c66b4130900a73ea5487c4cac5cb3f5e8fd5eb" title="LFM2-8B-A1B | Quality ≈ 3–4B dense, yet faster than Qwen3-1.7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LFM2 is a new generation of hybrid models developed by &lt;a href="https://www.liquid.ai/blog/lfm2-8b-a1b-an-efficient-on-device-mixture-of-experts"&gt;Liquid AI&lt;/a&gt;, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oaielly54ttf1.jpg?width=1953&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2013bd08578cf610969ab162642e20a42264eac1"&gt;https://preview.redd.it/oaielly54ttf1.jpg?width=1953&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2013bd08578cf610969ab162642e20a42264eac1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The weights of their first MoE based on LFM2, with 8.3B total parameters and 1.5B active parameters.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LFM2-8B-A1B is the best on-device MoE in terms of both &lt;strong&gt;quality&lt;/strong&gt; (comparable to 3-4B dense models) and &lt;strong&gt;speed&lt;/strong&gt; (faster than Qwen3-1.7B).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code and knowledge&lt;/strong&gt; capabilities are significantly improved compared to LFM2-2.6B.&lt;/li&gt; &lt;li&gt;Quantized variants fit comfortably on high-end &lt;strong&gt;phones, tablets, and laptops&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Find more information about LFM2-8B-A1B in their &lt;a href="https://www.liquid.ai/blog/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-8B-A1B"&gt;https://huggingface.co/LiquidAI/LFM2-8B-A1B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T03:25:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1igj2</id>
    <title>MoE models iGPU benchmarks</title>
    <updated>2025-10-08T18:20:55+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Follow up to request for testing a few other MoE models size 10-35B:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1na96gx/moe_models_tested_on_minipc_igpu_with_vulkan/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1na96gx/moe_models_tested_on_minipc_igpu_with_vulkan/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;System: Kubuntu 25.10 OS, Kernel 6.17.0-5-generic with 64GB DDR5 ram. AMD Radeon Graphics (RADV REMBRANDT) Ryzen 6800H and 680M iGPU. Links to model HF page near end of post. &lt;/p&gt; &lt;p&gt;aquif-3.5-a0.6b-preview-q8_0&lt;/p&gt; &lt;p&gt;Ling-Coder-lite.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;Ling-Coder-Lite-Q4_K_M&lt;/p&gt; &lt;p&gt;LLaDA-MoE-7B-A1B-Base.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;LLaDA-MoE-7B-A1B-Instruct.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;OLMoE-1B-7B-0125.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;OLMoE-1B-7B-0125-Instruct-Q4_K_M&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B-Instruct-2507-Q4_1&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B-Thinking-2507-Q4_K_M&lt;/p&gt; &lt;p&gt;Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL&lt;/p&gt; &lt;p&gt;Ring-lite-2507.i1-Q4_1 Ring-lite-2507.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;Llama.cpp Vulkan build: 152729f8 (6565)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;2.59 GiB&lt;/td&gt; &lt;td align="left"&gt;2.61 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1296.87 ± 11.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;2.59 GiB&lt;/td&gt; &lt;td align="left"&gt;2.61 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;103.45 ± 1.25&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;231.96 ± 0.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.94 ± 0.18&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;232.71 ± 0.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.21 ± 0.53&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;399.54 ± 5.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.91 ± 0.21&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;396.74 ± 1.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.60 ± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;487.74 ± 3.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;78.33 ± 0.47&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;484.79 ± 4.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;78.76 ± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;171.65 ± 0.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;27.04 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;142.18 ± 1.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;28.79 ± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;16.45 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;137.46 ± 0.66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;16.45 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;29.86 ± 0.12&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_1&lt;/td&gt; &lt;td align="left"&gt;9.84 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;292.10 ± 0.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_1&lt;/td&gt; &lt;td align="left"&gt;9.84 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.86 ± 0.40&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;234.03 ± 0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.75 ± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Order with models for table below:&lt;/p&gt; &lt;p&gt;aquif-3.5-a0.6b-preview-q8_0 &lt;/p&gt; &lt;p&gt;Ling-Coder-lite.i1-Q4_K_M &lt;/p&gt; &lt;p&gt;Ling-Coder-Lite-Q4_K_M &lt;/p&gt; &lt;p&gt;LLaDA-MoE-7B-A1B-Base.i1-Q4_K_M &lt;/p&gt; &lt;p&gt;LLaDA-MoE-7B-A1B-Instruct.i1-Q4_K_M &lt;/p&gt; &lt;p&gt;OLMoE-1B-7B-0125.i1-Q4_K_M &lt;/p&gt; &lt;p&gt;OLMoE-1B-7B-0125-Instruct-Q4_K_M &lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B-Instruct-2507-Q4_1 &lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B-Thinking-2507-Q4_K_M &lt;/p&gt; &lt;p&gt;Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL &lt;/p&gt; &lt;p&gt;Ring-lite-2507.i1-Q4_1 &lt;/p&gt; &lt;p&gt;Ring-lite-2507.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;Here is the combined data from all the tables into a single Markdown table:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;2.59 GiB&lt;/td&gt; &lt;td align="left"&gt;2.61 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1296.87 ± 11.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;2.59 GiB&lt;/td&gt; &lt;td align="left"&gt;2.61 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;103.45 ± 1.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;231.96 ± 0.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.94 ± 0.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;232.71 ± 0.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.21 ± 0.53&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;399.54 ± 5.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.91 ± 0.21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;396.74 ± 1.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.60 ± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;487.74 ± 3.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;78.33 ± 0.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;484.79 ± 4.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;78.76 ± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;171.65 ± 0.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;27.04 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;142.18 ± 1.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;28.79 ± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;16.45 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;137.46 ± 0.66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;16.45 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;29.86 ± 0.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_1&lt;/td&gt; &lt;td align="left"&gt;9.84 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;292.10 ± 0.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_1&lt;/td&gt; &lt;td align="left"&gt;9.84 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.86 ± 0.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;234.03 ± 0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.75 ± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Hyperlinks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/aquif-3.5-A4B-Think-GGUF"&gt;aquif-3.5-A4B-Think&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF?show_file_info=aquif-3-moe-17b-a2.8b.Q4_K_M.gguf"&gt;aquif-3-moe-17b-a2.8b-i1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/gabriellarson/Moonlight-16B-A3B-Instruct-GGUF"&gt;Moonlight-16B-A3B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/gpt-oss-20b-GGUF"&gt;gpt-oss-20b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF"&gt;ERNIE-4.5-21B-A3B-PT&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF"&gt;SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/Ling-lite-1.5-2507-GGUF"&gt;Ling-lite-1.5-2507&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lovedheart/Ling-mini-2.0-GGUF"&gt;Ling-mini-2.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/Ling-Coder-lite-i1-GGUF"&gt;Ling-Coder-lite&lt;/a&gt; &lt;a href="https://huggingface.co/redponike/Ling-Coder-lite-GGUF?show_file_info=Ling-Coder-Lite-Q4_K_M.gguf"&gt;2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/Ring-lite-2507-i1-GGUF"&gt;Ring-lite-2507&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lovedheart/Ring-mini-2.0-GGUF"&gt;Ring-mini-2.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5"&gt;Ming-Lite-Omni-1.5&lt;/a&gt; (No GGUF yet)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"&gt;Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/GroveMoE-Inst"&gt;GroveMoE-Inst &lt;/a&gt;(No GGUF yet)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/models?search=FlexOlmo-7x7B-1T"&gt;FlexOlmo-7x7B-1T&lt;/a&gt; (No GGUF yet)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/allenai/FlexOlmo-7x7B-1T-RT"&gt;FlexOlmo-7x7B-1T-RT&lt;/a&gt; (No GGUF yet)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1igj2/moe_models_igpu_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1igj2/moe_models_igpu_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1igj2/moe_models_igpu_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T18:20:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1e04z</id>
    <title>Less is More: Recursive Reasoning with Tiny Networks (7M model beats R1, Gemini 2.5 Pro on ARC AGI)</title>
    <updated>2025-10-08T15:40:05+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Less is More: Recursive Reasoning with Tiny Network&lt;/strong&gt;s, from Samsung Montréal by Alexia Jolicoeur-Martineau, shows how a &lt;strong&gt;7M-parameter Tiny Recursive Model (TRM)&lt;/strong&gt; outperforms trillion-parameter LLMs on hard reasoning benchmarks. TRM learns by &lt;strong&gt;recursively refining its own answers&lt;/strong&gt; using two internal memories: a latent reasoning state (&lt;em&gt;z&lt;/em&gt;) and a current answer (&lt;em&gt;y&lt;/em&gt;). &lt;/p&gt; &lt;p&gt;No chain-of-thought, no fixed-point math, no biological hierarchies. It beats the Hierarchical Reasoning Model (HRM), which used two networks and heavy training tricks. Results: &lt;strong&gt;87% on Sudoku-Extreme&lt;/strong&gt;, &lt;strong&gt;85% on Maze-Hard&lt;/strong&gt;, &lt;strong&gt;45% on ARC-AGI-1&lt;/strong&gt;, &lt;strong&gt;8% on ARC-AGI-2,&lt;/strong&gt; surpassing Gemini 2.5 Pro, DeepSeek R1, and o3-mini despite having &amp;lt;0.01% their size.&lt;br /&gt; &lt;strong&gt;In short:&lt;/strong&gt; recursion, not scale, drives reasoning.&lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://arxiv.org/html/2510.04871v1"&gt;https://arxiv.org/html/2510.04871v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Summary : &lt;a href="https://youtu.be/wQbEITW7BMw?si=U3SFKAGYF5K06fFw"&gt;https://youtu.be/wQbEITW7BMw?si=U3SFKAGYF5K06fFw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1e04z/less_is_more_recursive_reasoning_with_tiny/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1e04z/less_is_more_recursive_reasoning_with_tiny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1e04z/less_is_more_recursive_reasoning_with_tiny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T15:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1jh28</id>
    <title>Free 1,000 CPU + 100 GPU hours for testers. I open sourced the world's simplest cluster compute software</title>
    <updated>2025-10-08T18:54:50+00:00</updated>
    <author>
      <name>/u/Ok_Post_149</name>
      <uri>https://old.reddit.com/user/Ok_Post_149</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everybody,&lt;/p&gt; &lt;p&gt;I’ve always struggled to get data scientists and analysts to scale their code in the cloud. Almost every time, they’d have to hand it over to DevOps, the backlog would grow, and overall throughput would tank.&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://docs.burla.dev/"&gt;Burla&lt;/a&gt;, the simplest cluster compute software that lets even Python beginners run code on massive clusters in the cloud. It’s one function with two parameters: the function and the inputs. You can bring your own Docker image, set hardware requirements, and run jobs as background tasks so you can fire and forget. Responses are fast, and you can call a million simple functions in just a few seconds.&lt;/p&gt; &lt;p&gt;Burla is built for embarrassingly parallel workloads like preprocessing data, hyperparameter tuning, and batch inference.&lt;/p&gt; &lt;p&gt;It's open source, and I’m improving the installation process. I also created managed versions for testing. If you want to try it, I’ll cover 1,000 CPU hours and 100 GPU hours. Email me at [&lt;a href="mailto:joe@burla.dev"&gt;joe@burla.dev&lt;/a&gt;](mailto:&lt;a href="mailto:joe@burla.dev"&gt;joe@burla.dev&lt;/a&gt;) if interested.&lt;/p&gt; &lt;p&gt;Here’s a short intro video:&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=9d22y_kWjyE"&gt;https://www.youtube.com/watch?v=9d22y_kWjyE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub → &lt;a href="https://github.com/Burla-Cloud/burla"&gt;https://github.com/Burla-Cloud/burla&lt;/a&gt;&lt;br /&gt; Docs → &lt;a href="https://docs.burla.dev/"&gt;https://docs.burla.dev&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Post_149"&gt; /u/Ok_Post_149 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1jh28/free_1000_cpu_100_gpu_hours_for_testers_i_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1jh28/free_1000_cpu_100_gpu_hours_for_testers_i_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1jh28/free_1000_cpu_100_gpu_hours_for_testers_i_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T18:54:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1mpt5</id>
    <title>Introducing the ColBERT Nano series of models. All 3 of these models come in at less than 1 million parameters (250K, 450K, 950K)</title>
    <updated>2025-10-08T20:40:36+00:00</updated>
    <author>
      <name>/u/davidmezzetti</name>
      <uri>https://old.reddit.com/user/davidmezzetti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1mpt5/introducing_the_colbert_nano_series_of_models_all/"&gt; &lt;img alt="Introducing the ColBERT Nano series of models. All 3 of these models come in at less than 1 million parameters (250K, 450K, 950K)" src="https://preview.redd.it/okf1858k8ytf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22ce6428ce7e3df677de1c6dc5808dfffdb5ad04" title="Introducing the ColBERT Nano series of models. All 3 of these models come in at less than 1 million parameters (250K, 450K, 950K)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Late interaction models perform shockingly well with small models. Use this method to build small domain-specific models for retrieval and more.&lt;/p&gt; &lt;p&gt;Collection: &lt;a href="https://huggingface.co/collections/NeuML/colbert-68cb248ce424a6d6d8277451"&gt;https://huggingface.co/collections/NeuML/colbert-68cb248ce424a6d6d8277451&lt;/a&gt;&lt;br /&gt; Smallest Model: &lt;a href="https://huggingface.co/NeuML/colbert-muvera-femto"&gt;https://huggingface.co/NeuML/colbert-muvera-femto&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davidmezzetti"&gt; /u/davidmezzetti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/okf1858k8ytf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1mpt5/introducing_the_colbert_nano_series_of_models_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1mpt5/introducing_the_colbert_nano_series_of_models_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T20:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o19ouy</id>
    <title>Can't get my local setups running smoothly, any options for uncensored generation?</title>
    <updated>2025-10-08T12:54:49+00:00</updated>
    <author>
      <name>/u/zemocrise</name>
      <uri>https://old.reddit.com/user/zemocrise</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been trying to get a local environment up and running for uncensored outputs, but honestly, it’s been a pain. Constant issues with dependencies, VRAM limits, crashes, and juggling different models. I have run out of cash and am thinking of trying something new for now. &lt;/p&gt; &lt;p&gt;Is anyone here aware of any powerful online or hybrid alternatives that are fully uncensored? Would love recommendations before my finances improve to get a better local setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zemocrise"&gt; /u/zemocrise &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o19ouy/cant_get_my_local_setups_running_smoothly_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o19ouy/cant_get_my_local_setups_running_smoothly_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o19ouy/cant_get_my_local_setups_running_smoothly_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T12:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1hmzn</id>
    <title>Attention is all you need - As a visual book</title>
    <updated>2025-10-08T17:52:29+00:00</updated>
    <author>
      <name>/u/simplext</name>
      <uri>https://old.reddit.com/user/simplext</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1hmzn/attention_is_all_you_need_as_a_visual_book/"&gt; &lt;img alt="Attention is all you need - As a visual book" src="https://external-preview.redd.it/b3RicGY5NnFkeHRmMU5Bzo1VBINWAdeeu7BAqtWhLwslV_cdGuD17nh6Rn_c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dbc72b1dee0b4ccd99ddf178c4605811bd0f0bac" title="Attention is all you need - As a visual book" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;Imagine if you wanted to turn a research paper into a visual presentation where every small concept and idea was illustrated with an image.&lt;/p&gt; &lt;p&gt;In the video walk through, I take the popular machine learning paper that introduces transformers and turn it into a visual book. I ask questions when I don't understand something so that that more slides can be generated to explain the smaller details.&lt;/p&gt; &lt;p&gt;Visual book is free for a while. Would love for you to try it and give me your feedback. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.visualbook.app/"&gt;https://www.visualbook.app/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simplext"&gt; /u/simplext &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0u7th86qdxtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1hmzn/attention_is_all_you_need_as_a_visual_book/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1hmzn/attention_is_all_you_need_as_a_visual_book/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T17:52:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1fdev</id>
    <title>RTX 4090 48GB price drop?</title>
    <updated>2025-10-08T16:29:50+00:00</updated>
    <author>
      <name>/u/skyfallboom</name>
      <uri>https://old.reddit.com/user/skyfallboom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm seeing many modified 4090 48GB cards listed for half the price of an RTX PRO 6000 96GB. $4,500 vs $9,000. &lt;/p&gt; &lt;p&gt;It doesn't make sense to purchase those when a new 96GB card gives you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;as much memory in a single PCIe slot&lt;/li&gt; &lt;li&gt;better power efficiency &lt;/li&gt; &lt;li&gt;a true warranty&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Who purchases those at this price? The RTX PRO 6000 isn't out stock. &lt;/p&gt; &lt;p&gt;Do you think too many 4090 got modified and we're going to see a price drop soon?&lt;/p&gt; &lt;p&gt;Also, not in the same ballpark but the Intel B60 is supposed to come this year. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skyfallboom"&gt; /u/skyfallboom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1fdev/rtx_4090_48gb_price_drop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1fdev/rtx_4090_48gb_price_drop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1fdev/rtx_4090_48gb_price_drop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T16:29:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1k5rc</id>
    <title>New Intel drivers are fire</title>
    <updated>2025-10-08T19:19:55+00:00</updated>
    <author>
      <name>/u/hasanismail_</name>
      <uri>https://old.reddit.com/user/hasanismail_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1k5rc/new_intel_drivers_are_fire/"&gt; &lt;img alt="New Intel drivers are fire" src="https://preview.redd.it/f43lwzkhuxtf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=726219703cd349e3c9b1d969986f07f0f51541fc" title="New Intel drivers are fire" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I went from getting 30 tokens a second on gptosss20b to 95!!!!!!!!!!!!!!! Holy shit Intel is cooking with the b580 I have 4 total I'm gonna put a rig together with all the cards on a dual socket x99 system(for the pcie lanes) well get back with multi card perf later&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasanismail_"&gt; /u/hasanismail_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f43lwzkhuxtf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1k5rc/new_intel_drivers_are_fire/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1k5rc/new_intel_drivers_are_fire/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T19:19:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1dqiy</id>
    <title>Stop flexing Pass@N — show Pass-all-N</title>
    <updated>2025-10-08T15:30:27+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1dqiy/stop_flexing_passn_show_passalln/"&gt; &lt;img alt="Stop flexing Pass@N — show Pass-all-N" src="https://preview.redd.it/20a6i107owtf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24db5289fda31d349651dd815a145c7f672d3cf6" title="Stop flexing Pass@N — show Pass-all-N" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a claim, and I’m curious what you think. I think model report should also report &lt;strong&gt;Pass-all-N&lt;/strong&gt; for tasks where they use Pass@N (like SWE tasks). Pass@N and mean resolved rate look nice, but they hide instability. Pass-all-N is simple: what share of tasks the model solves in &lt;strong&gt;EVERY&lt;/strong&gt; one of N runs. If it passes 4/5 times, it doesn’t count. For real use I want an agent that solves the task every time, not “sometimes with lucky seed.”&lt;/p&gt; &lt;p&gt;I checked this on &lt;a href="https://swe-rebench.com/"&gt;SWE-rebench&lt;/a&gt; (5 runs per model, August set) and Pass-all-5 is clearly lower than the mean resolved rate for all models. The gap size is different across models too — some are more stable, some are very flaky. That’s exactly the signal I want to see.&lt;/p&gt; &lt;p&gt;I’m not saying to drop &lt;a href="mailto:Pass@N"&gt;Pass@N&lt;/a&gt;. Keep it — but also report &lt;strong&gt;Pass-all-N&lt;/strong&gt; so we can compare reliability, not just the best-case average. Most releases already run multiple seeds to get Pass@N anyway, so it’s basically free to add Pass-all-N from the same runs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/20a6i107owtf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1dqiy/stop_flexing_passn_show_passalln/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1dqiy/stop_flexing_passn_show_passalln/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T15:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o19wvg</id>
    <title>LLM Benchmarks: Gemini 2.5 Flash latest version takes the top spot</title>
    <updated>2025-10-08T13:03:46+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o19wvg/llm_benchmarks_gemini_25_flash_latest_version/"&gt; &lt;img alt="LLM Benchmarks: Gemini 2.5 Flash latest version takes the top spot" src="https://preview.redd.it/bql3o49zyvtf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c9af7b8db7f6c91135a9b78ef4113719931ea7f" title="LLM Benchmarks: Gemini 2.5 Flash latest version takes the top spot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve updated our Task Completion Benchmarks, and this time Gemini 2.5 Flash (latest version) came out on top for overall task completion, scoring highest across context reasoning, SQL, agents, and normalization.&lt;/p&gt; &lt;p&gt;Our TaskBench evaluates how well language models can actually finish a variety of real-world tasks, reporting the percentage of tasks completed successfully using a consistent methodology for all models.&lt;/p&gt; &lt;p&gt;See the full rankings and details: &lt;a href="https://opper.ai/models"&gt;https://opper.ai/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious to hear how others are seeing Gemini Flash's latest version perform vs other models, any surprises or different results in your projects?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bql3o49zyvtf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o19wvg/llm_benchmarks_gemini_25_flash_latest_version/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o19wvg/llm_benchmarks_gemini_25_flash_latest_version/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T13:03:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1drs6</id>
    <title>Ling-1T</title>
    <updated>2025-10-08T15:31:37+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1drs6/ling1t/"&gt; &lt;img alt="Ling-1T" src="https://external-preview.redd.it/GF0ej-9rt3AXeKHvcKd5G-UgA8tEbZGSIvNEsQkOwA0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acd86786f2d1ca8025b03b2b577396fa4bc316d8" title="Ling-1T" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ling-1T is the first flagship non-thinking model in the Ling 2.0 series, featuring 1 trillion total parameters with ≈ 50 billion active parameters per token. Built on the Ling 2.0 architecture, Ling-1T is designed to push the limits of efficient reasoning and scalable cognition.&lt;/p&gt; &lt;p&gt;Pre-trained on 20 trillion+ high-quality, reasoning-dense tokens, Ling-1T-base supports up to 128K context length and adopts an evolutionary chain-of-thought (Evo-CoT) process across mid-training and post-training. This curriculum greatly enhances the model’s efficiency and reasoning depth, allowing Ling-1T to achieve state-of-the-art performance on multiple complex reasoning benchmarks—balancing accuracy and efficiency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-1T"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1drs6/ling1t/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1drs6/ling1t/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T15:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1ac09</id>
    <title>AI21 releases Jamba 3B, the tiny model outperforming Qwen 3 4B and IBM Granite 4 Micro!</title>
    <updated>2025-10-08T13:20:50+00:00</updated>
    <author>
      <name>/u/zennaxxarion</name>
      <uri>https://old.reddit.com/user/zennaxxarion</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ac09/ai21_releases_jamba_3b_the_tiny_model/"&gt; &lt;img alt="AI21 releases Jamba 3B, the tiny model outperforming Qwen 3 4B and IBM Granite 4 Micro!" src="https://b.thumbs.redditmedia.com/HzZ8IknU9-KdwN0J-_2TQJ93jzutAFq4cAeVawprNKM.jpg" title="AI21 releases Jamba 3B, the tiny model outperforming Qwen 3 4B and IBM Granite 4 Micro!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Disclaimer: I work for AI21, creator of the Jamba model family.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;We’re super excited to announce the launch of our brand new model, Jamba 3B!&lt;/p&gt; &lt;p&gt;Jamba 3B is the swiss army knife of models, designed to be ready on the go.&lt;/p&gt; &lt;p&gt;You can run it on your iPhone, Android, Mac or PC for smart replies, conversational assistants, model routing, fine-tuning and much more.&lt;/p&gt; &lt;p&gt;We believe we’ve rewritten what tiny models can do. &lt;/p&gt; &lt;p&gt;Jamba 3B keeps up near 40 t/s even with giant context windows, while others crawl once they pass 128K. &lt;/p&gt; &lt;p&gt;Even though it’s smaller at 3B parameters, it matches or beats Qwen 3 4B and Gemma 3 4B in model intelligence.&lt;/p&gt; &lt;p&gt;We performed benchmarking using the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mac M3 36GB&lt;/li&gt; &lt;li&gt;iPhone 16 Pro&lt;/li&gt; &lt;li&gt;Galaxy S25&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here are our key findings:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Faster and steadier at scale:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Keeps producing ~40 tokens per second on Mac even past 32k context&lt;/li&gt; &lt;li&gt;Still cranks out ~33 t/s at 128k while Qwen 3 4B drops to &amp;lt;1 t/s and Llama 3.2 3B goes down to ~5 t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Best long context efficiency:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;From 1k to 128k context, latency barely moves (43 to 33 t/s). Every rival model loses 70% speed beyond 32k&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;High intelligence per token ratio:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scored 0.31 combined intelligence index at ~40 t/s, above Gemma 3 4B (0.20) and Phi-4 Mini (0.22)&lt;/li&gt; &lt;li&gt;Qwen 3 4B ranks slightly higher in raw score (0.35) but runs 3x slower&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Outpaces IBM Granite 4 Micro:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Produces 5x more tokens per second at 256K on Mac M3 (36 GB) with reasoning intact&lt;/li&gt; &lt;li&gt;First 3B parameter model to stay coherent past 60K tokens. Achieves an effective context window ≈ 200k on desktop and mobile without nonsense outputs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Hardware footprint:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The 4-bit quantized version of Jamba 3B requires the following to run on llama.cpp at context length of 32k: &lt;/p&gt; &lt;p&gt;Model Weights: 1.84 GiB&lt;/p&gt; &lt;p&gt;Total Active Memory: ~2.2 GiB&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Blog:&lt;/strong&gt; &lt;a href="https://www.ai21.com/blog/introducing-jamba-reasoning-3b/"&gt;https://www.ai21.com/blog/introducing-jamba-reasoning-3b/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Huggingface:&lt;/strong&gt; &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zennaxxarion"&gt; /u/zennaxxarion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o1ac09"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ac09/ai21_releases_jamba_3b_the_tiny_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ac09/ai21_releases_jamba_3b_the_tiny_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T13:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
