<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-20T15:34:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nl499c</id>
    <title>Xiaomi's MiMo-Audio: 7B Audio Language Model Revolutionizes Few-Shot Audio Learning!</title>
    <updated>2025-09-19T13:55:47+00:00</updated>
    <author>
      <name>/u/Entire_Maize_6064</name>
      <uri>https://old.reddit.com/user/Entire_Maize_6064</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nl499c/xiaomis_mimoaudio_7b_audio_language_model/"&gt; &lt;img alt="Xiaomi's MiMo-Audio: 7B Audio Language Model Revolutionizes Few-Shot Audio Learning!" src="https://external-preview.redd.it/loidfxMIX6bu4iJslfEaZNObjwpNCnXkQ51HOvtS1Jo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f8ec6bfd9163635dda507779c437523ee639a67" title="Xiaomi's MiMo-Audio: 7B Audio Language Model Revolutionizes Few-Shot Audio Learning!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Xiaomi just dropped something groundbreaking - &lt;strong&gt;MiMo-Audio&lt;/strong&gt;, an audio language model that's completely redefining what's possible with few-shot learning in the audio domain.&lt;/p&gt; &lt;h1&gt;🚀 Project Overview&lt;/h1&gt; &lt;p&gt;MiMo-Audio is Xiaomi's open-source audio language model with a game-changing feature: &lt;strong&gt;powerful few-shot learning capabilities&lt;/strong&gt;. Unlike traditional audio models requiring task-specific fine-tuning, MiMo-Audio generalizes to new audio tasks with just a few examples or simple instructions - just like humans do.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Core Philosophy:&lt;/strong&gt; Successfully applying GPT-3's next-token prediction paradigm to the audio domain, achieving strong generalization through large-scale pretraining.&lt;/p&gt; &lt;h1&gt;🔧 Core Technical Architecture&lt;/h1&gt; &lt;h1&gt;Dual-Component Design&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;MiMo-Audio-Tokenizer (1.2B parameters)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: 25Hz Transformer&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Technical Features&lt;/strong&gt;: 8-layer RVQ (Residual Vector Quantization) stack&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: 200 tokens/second generation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Data&lt;/strong&gt;: 10 million hours audio corpus&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt;: Joint semantic and reconstruction objectives&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;MiMo-Audio-7B (7B parameters)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Base Architecture&lt;/strong&gt;: Qwen2-based language model&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Innovative Design&lt;/strong&gt;: Patch encoder + LLM + patch decoder&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Patch Mechanism&lt;/strong&gt;: Aggregates 4 consecutive RVQ token timesteps into single patches&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sequence Compression&lt;/strong&gt;: Downsamples from 25Hz to 6.25Hz for modeling efficiency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generation Strategy&lt;/strong&gt;: Delayed generation scheme with autoregressive full 25Hz sequence&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Technical Innovations&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Patch Aggregation Mechanism&lt;/strong&gt;: Solves high-frequency sequence modeling efficiency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic-Reconstruction Joint Optimization&lt;/strong&gt;: Balances audio quality and semantic understanding&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Delayed Generation Scheme&lt;/strong&gt;: Balances generation quality and computational efficiency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chain-of-Thought Mechanism&lt;/strong&gt;: Introduces thinking mode in instruction-tuned version&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;📊 Performance Metrics &amp;amp; Benchmarks&lt;/h1&gt; &lt;h1&gt;Training Scale&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Pretraining Data&lt;/strong&gt;: 100+ million hours of audio data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruction Tuning&lt;/strong&gt;: Curated diverse instruction corpus&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Language Support&lt;/strong&gt;: Bilingual (Chinese-English)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Benchmark Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Open-Source SOTA&lt;/strong&gt;: Achieves state-of-the-art performance among open-source models on speech intelligence and audio understanding benchmarks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Closed-Source Competitive&lt;/strong&gt;: MiMo-Audio-7B-Instruct approaches or surpasses closed-source models in multiple evaluations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero-Shot Generalization&lt;/strong&gt;: Handles tasks absent from training data&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Capability Demonstrations&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Few-Shot Learning Tasks:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Voice Conversion&lt;/li&gt; &lt;li&gt;Style Transfer&lt;/li&gt; &lt;li&gt;Speech Editing&lt;/li&gt; &lt;li&gt;Emotional Voice Cloning&lt;/li&gt; &lt;li&gt;Dialect/Accent Mimicking&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Generation Capabilities:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Highly realistic talk shows, recitations, livestreaming content&lt;/li&gt; &lt;li&gt;Multiple speech styles: news, gaming commentary, crosstalk, audiobooks&lt;/li&gt; &lt;li&gt;Context-aware speech generation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Audio Understanding:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Long-form audio comprehension&lt;/li&gt; &lt;li&gt;Complex audio reasoning&lt;/li&gt; &lt;li&gt;Multimodal audio analysis&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🎯 Application Value &amp;amp; Technical Advantages&lt;/h1&gt; &lt;h1&gt;Technical Advantages&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;True Few-Shot Learning&lt;/strong&gt;: Adapts to new tasks without extensive labeled data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Strong Generalization&lt;/strong&gt;: Handles unseen audio task types&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Architecture&lt;/strong&gt;: Patch mechanism improves modeling efficiency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open-Source Friendly&lt;/strong&gt;: Complete model, code, and evaluation toolkit&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Application Scenarios&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Content Creation&lt;/strong&gt;: Audio generation, speech synthesis, voice-over production&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Education&lt;/strong&gt;: Multilingual learning, pronunciation correction, speaking practice&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Entertainment&lt;/strong&gt;: Game voice-over, audiobook production, podcast generation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Assistive Technology&lt;/strong&gt;: Voice cloning, speech restoration, accessibility applications&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Developer Ecosystem&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Complete Toolkit&lt;/strong&gt;: Gradio demo interface and inference scripts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation Framework&lt;/strong&gt;: MiMo-Audio-Eval evaluation toolkit&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy Deployment&lt;/strong&gt;: Supports local deployment and online demos&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;💡 Technical Innovation Summary&lt;/h1&gt; &lt;p&gt;MiMo-Audio represents a significant advancement in audio language modeling, with core innovations including:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Paradigm Shift&lt;/strong&gt;: From task-specific fine-tuning to general few-shot learning&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Architectural Innovation&lt;/strong&gt;: Patch mechanism effectively addresses audio sequence modeling challenges&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scale Effects&lt;/strong&gt;: Emergent capabilities from large-scale pretraining&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Practicality&lt;/strong&gt;: Open-source model achieving commercial-grade performance&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This model demonstrates GPT-3-like breakthrough capabilities in the audio domain, opening new possibilities for audio AI. Its performance on unseen tasks proves the tremendous potential of large-scale pretraining in audio.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official Resources:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub Repository: &lt;a href="https://github.com/XiaomiMiMo/MiMo-Audio"&gt;https://github.com/XiaomiMiMo/MiMo-Audio&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Official Demo Page: &lt;a href="https://xiaomimimo.github.io/MiMo-Audio-Demo/"&gt;https://xiaomimimo.github.io/MiMo-Audio-Demo/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Technical Report PDF: &lt;a href="https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf"&gt;https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hugging Face Models: &lt;a href="https://huggingface.co/collections/XiaomiMiMo/mimo-audio-68cc7202692c27dae881cce0"&gt;https://huggingface.co/collections/XiaomiMiMo/mimo-audio-68cc7202692c27dae881cce0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Update:&lt;/h1&gt; &lt;p&gt;I've been trying out MiMo-Audio and noticed that the official HuggingFace demo can be quite unstable, and the local deployment has some bugs that make it tricky to get running smoothly.&lt;/p&gt; &lt;p&gt;For anyone who wants to quickly experience MiMo-Audio's capabilities without the setup hassle, I found this stable online demo: &lt;/p&gt; &lt;p&gt;&lt;a href="https://vibevoice.info/mimoaudio"&gt;&lt;strong&gt;https://vibevoice.info/mimoaudio&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Entire_Maize_6064"&gt; /u/Entire_Maize_6064 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-Audio-7B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nl499c/xiaomis_mimoaudio_7b_audio_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nl499c/xiaomis_mimoaudio_7b_audio_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T13:55:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nldm6a</id>
    <title>Manufactured 4090 48gb AMA</title>
    <updated>2025-09-19T19:51:53+00:00</updated>
    <author>
      <name>/u/koalfied-coder</name>
      <uri>https://old.reddit.com/user/koalfied-coder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nldm6a/manufactured_4090_48gb_ama/"&gt; &lt;img alt="Manufactured 4090 48gb AMA" src="https://b.thumbs.redditmedia.com/R71H7Ao69fuFtpU0lFV24dML24cO4MPoXXZ-LIG4fzk.jpg" title="Manufactured 4090 48gb AMA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all I have run a Galax manufactured 48gb card for about a year now with flawless results and CUDA up to 13.0. These particular cards are SKU cards not resolders thankfully. The resolders I had were pure garbage. But maybe I got bad batch. Anyhows these cards rock. I'll post t/s asap as its just now coming off rental. Anyhow AMA I love talking cards.&lt;/p&gt; &lt;p&gt;EDIT: the card pictured with serial is the latest batch I have seen and held. The one running for I would say 9-11 months is still being rented. Can deff get pics tho when maintenance come around :)&lt;/p&gt; &lt;p&gt;Also I do get a small discount on my 4090 orders for referrals. If thats not allowed I will not respond to requests. Please just lmk don't ban me I love it here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koalfied-coder"&gt; /u/koalfied-coder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nldm6a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nldm6a/manufactured_4090_48gb_ama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nldm6a/manufactured_4090_48gb_ama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T19:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlqu6q</id>
    <title>Open sourced my AI video generation project</title>
    <updated>2025-09-20T06:28:27+00:00</updated>
    <author>
      <name>/u/ExtremeKangaroo5437</name>
      <uri>https://old.reddit.com/user/ExtremeKangaroo5437</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 OPEN-SOURCED: Modular AI Video Generation Pipeline After making it in my free time to learn and fun, I'm excited to open-source my Modular AI Video Generation Pipeline - a complete end-to-end system that transforms a single topic idea into professional short-form videos with narration, visuals, and text overlays. Best suited for learning.&lt;/p&gt; &lt;p&gt;�� Technical Architecture: Modular Design: Pluggable AI models for each generation step (LLM → TTS → T2I/I2V/T2V) Dual Workflows: Image-to-Video (high quality) vs Text-to-Video (fast generation) State-Driven Pipeline: ProjectManager tracks tasks via JSON state, TaskExecutor orchestrates execution Dynamic Model Discovery: Auto-discovers new modules, making them immediately available in UI&lt;/p&gt; &lt;p&gt;🤖 AI Models Integrated: LLM: Zephyr for script generation TTS: Coqui XTTS (15+ languages, voice cloning support) T2I: Juggernaut-XL v9 with IP-Adapter for character consistency I2V: SVD, LTX, WAN for image-to-video animation T2V: Zeroscope for direct text-to-video generation&lt;/p&gt; &lt;p&gt;⚡ Key Features: Character Consistency: IP-Adapter integration maintains subject appearance across scenes Multi-Language Support: Generate narration in 15+ languages Voice Cloning: Upload a .wav file to clone any voice Stateful Projects: Stop/resume work anytime with full project state persistence Real-time Dashboard: Edit scripts, regenerate audio, modify prompts on-the-fly&lt;/p&gt; &lt;p&gt;🏗️ Built With: Python 3.10+, PyTorch, Diffusers, Streamlit, Pydantic, MoviePy, FFmpeg The system uses abstract base classes (BaseLLM, BaseTTS, BaseT2I, BaseI2V, BaseT2V) making it incredibly easy to add new models - just implement the interface and it's automatically discovered!&lt;/p&gt; &lt;p&gt;💡 Perfect for: Content creators wanting AI-powered video production Developers exploring multi-modal AI pipelines Researchers experimenting with video generation models Anyone interested in modular AI architecture&lt;/p&gt; &lt;p&gt;🎯 What's Next: Working on the next-generation editor with FastAPI backend, Vue frontend, and distributed model serving. Also planning Text-to-Music modules and advanced ControlNet integration.&lt;/p&gt; &lt;p&gt;🔗 GitHub: &lt;a href="https://github.com/gowrav-vishwakarma/ai-video-generator-editor"&gt;https://github.com/gowrav-vishwakarma/ai-video-generator-editor&lt;/a&gt; 📺 Demo: &lt;a href="https://www.youtube.com/watch?v=0YBcYGmYV4c"&gt;https://www.youtube.com/watch?v=0YBcYGmYV4c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Contributors welcome! This is designed to be a community-driven project for advancing AI video generation.&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;Best Part: It's extensible, you can add new modules and new models very easily.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtremeKangaroo5437"&gt; /u/ExtremeKangaroo5437 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlqu6q/open_sourced_my_ai_video_generation_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlqu6q/open_sourced_my_ai_video_generation_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlqu6q/open_sourced_my_ai_video_generation_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T06:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlndrw</id>
    <title>Qwen3 Next Sycophancy</title>
    <updated>2025-09-20T03:13:32+00:00</updated>
    <author>
      <name>/u/Arrival3098</name>
      <uri>https://old.reddit.com/user/Arrival3098</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems way too agreeable / overly instruction tuned?&lt;/p&gt; &lt;p&gt;Are others getting the same behaviour?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arrival3098"&gt; /u/Arrival3098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlndrw/qwen3_next_sycophancy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlndrw/qwen3_next_sycophancy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlndrw/qwen3_next_sycophancy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T03:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nl3q0o</id>
    <title>A list of models released or updated last week on this sub, in case you any (19 sep)</title>
    <updated>2025-09-19T13:33:51+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fellows, here is the list of models (releases and updates), I found mentioned on the LocalLlama this week, let me know if I have missed something. Great weekend :)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Reddit Link&lt;/th&gt; &lt;th align="left"&gt;Hugging Face / Repo&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Decart-AI – Lucy Edit&lt;/strong&gt; – video editing model&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nkkghp/decartai_releases_open_source_nano_banana_for/"&gt;Reddit post&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/decart-ai/Lucy-Edit-Dev"&gt;HF link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Magistral Small 2509&lt;/strong&gt; – compact Mistral release&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1njgovj/magistral_small_2509_has_been_released/"&gt;Reddit post&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2509"&gt;HF link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Ling Flash 2.0&lt;/strong&gt; – 100B sparse LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nj9601"&gt;Reddit post&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0"&gt;HF link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3-Next-80B-A3B&lt;/strong&gt; – reasoning-optimized MoE&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1ng1fa5"&gt;Reddit post&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking"&gt;Thinking&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct"&gt;Instruct&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Ling-mini 2.0&lt;/strong&gt; – CPU-only 16B model&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nfjljo/release_inclusionailingmini20/"&gt;Reddit post&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0"&gt;HF link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;SongBloom&lt;/strong&gt; (edit) – music generation model&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nkbrk1/local_suno_just_dropped/"&gt;Reddit post&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/fredconex/SongBloom-Safetensors"&gt;HF link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Arcee AFM-4.5B&lt;/strong&gt; – Apache 2.0 licensed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1njkqdm/arcee_going_apache_20/"&gt;Reddit post&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B"&gt;HF link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Meta MobileLLM-R1 (950M)&lt;/strong&gt; – mobile-friendly LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://i.redd.it/huchm6bahrof1.png"&gt;Reddit post&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/facebook/MobileLLM-R1-950M"&gt;HF link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen235b 2507 quants&lt;/strong&gt; – mxfp4 quantized release&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nguiko/qwen235b_2507_mxfp4_quants/"&gt;Reddit post&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/sm54/Qwen3-235B-A22B-Thinking-2507-MXFP4_MOE"&gt;HF link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Other projects mentioned this week on the sub&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Project&lt;/th&gt; &lt;th align="left"&gt;Link&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;ClaraVerse v0.2.0&lt;/strong&gt; – unified local AI workspace&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nh5fn0/spent_4_months_building_unified_local_ai"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://github.com/badboysm890/ClaraVerse"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LocalAI v3.5.0&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ngw3sb/project_update_localai_v350_is_out_huge_update/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/mudler/LocalAI"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;New Free AI Agent Framework&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://i.redd.it/xr8c1buja0pf1.png"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/bsides230/LYRN"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;OpenWebUI Mobile Companion (Conduit)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://v.redd.it/6eh7mfucuxof1"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/cogwheel0/conduit"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;VRAM Approximation Tool for GGUF&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nk1jbc/i_just_made_vram_approximation_tool_for_llm/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/KolosalAI/model-memory-calculator"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nl3q0o/a_list_of_models_released_or_updated_last_week_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nl3q0o/a_list_of_models_released_or_updated_last_week_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nl3q0o/a_list_of_models_released_or_updated_last_week_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T13:33:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nls9ot</id>
    <title>Tired of bloated WebUIs? Here’s a lightweight llama.cpp + llama-swap stack (from Pi 5 without llama-swap to full home LLM server with it) - And the new stock Svelte 5 webui from llama.cpp is actually pretty great!</title>
    <updated>2025-09-20T07:57:10+00:00</updated>
    <author>
      <name>/u/Serveurperso</name>
      <uri>https://old.reddit.com/user/Serveurperso</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like the new stock Svelte WebUI in llama.cpp : it’s clean, fast, and a great base to build on. &lt;/p&gt; &lt;p&gt;The idea is simple: keep everything light and self-contained.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;stay up to date with llama.cpp using just &lt;code&gt;git pull / build&lt;/code&gt;&lt;/li&gt; &lt;li&gt;swap in any new model instantly with llama-swap YAML&lt;/li&gt; &lt;li&gt;no heavy DB or wrapper stack, just localStorage + reverse proxy&lt;/li&gt; &lt;li&gt;same workflow works from a Raspberry Pi 5 to a high-end server&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I patched the new Svelte webui so it stays usable even if llama-server is offline. That way you can keep browsing conversations, send messages, and swap models without breaking the UI.&lt;/p&gt; &lt;p&gt;Short video shows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;llama.cpp + llama-swap + patched webui + reverse proxy + llama-server offline test on real domain&lt;/li&gt; &lt;li&gt;Raspberry Pi 5 (16 GB) running Qwen3-30B A3B @ ~5 tokens/s&lt;/li&gt; &lt;li&gt;Server with multiple open-weight models, all managed through the same workflow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Video:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1nls9ot/video/943wpcu7z9qf1/player"&gt;https://reddit.com/link/1nls9ot/video/943wpcu7z9qf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please don’t abuse my server : I'm keeping it open for testing and feedback. If it gets abused, I’ll close it with API key and HTTP auth.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serveurperso"&gt; /u/Serveurperso &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nls9ot/tired_of_bloated_webuis_heres_a_lightweight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nls9ot/tired_of_bloated_webuis_heres_a_lightweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nls9ot/tired_of_bloated_webuis_heres_a_lightweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T07:57:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlguk9</id>
    <title>PyTorch now offers native quantized variants of popular models!</title>
    <updated>2025-09-19T22:03:35+00:00</updated>
    <author>
      <name>/u/formlog</name>
      <uri>https://old.reddit.com/user/formlog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLLaMa community,&lt;/p&gt; &lt;p&gt;I'm a developer working on PyTorch quantization / &lt;a href="https://github.com/pytorch/ao"&gt;torchao&lt;/a&gt;, I'd like to share what &lt;a href="https://github.com/pytorch/ao"&gt;TorchAO&lt;/a&gt; team, &lt;a href="https://github.com/pytorch/executorch"&gt;ExecuTorch&lt;/a&gt; team and &lt;a href="https://unsloth.ai/"&gt;Unsloth AI&lt;/a&gt; have been working on recently. Please let us know if you have any thoughts, including what model would like to see quantized, what new quantization techniques you would like to use, and how are you using quantized models in general.&lt;/p&gt; &lt;p&gt;PyTorch now offers native quantized variants of Phi4-mini-instruct, Qwen3, SmolLM3-3B and gemma-3-270m-it through a collaboration between the TorchAO team and Unsloth!&lt;/p&gt; &lt;p&gt;🔎 Learn more: &lt;a href="https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fhubs%2Ela%2FQ03Kb6Cs0&amp;amp;urlhash=j39h&amp;amp;trk=public_post-text"&gt;https://hubs.la/Q03Kb6Cs0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights include:&lt;br /&gt; 🔹 We released pre-quantized models optimized for both server and mobile platforms: for users who want to deploy a faster model in production&lt;br /&gt; 🔹 We released comprehensive, reproducible quantization recipes and guides that cover model quality evaluation and performance benchmarking: for users applying PyTorch native quantization to their own models and datasets&lt;br /&gt; 🔹 You can also finetune with unsloth and quantize the finetuned model with TorchAO&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/formlog"&gt; /u/formlog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlguk9/pytorch_now_offers_native_quantized_variants_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlguk9/pytorch_now_offers_native_quantized_variants_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlguk9/pytorch_now_offers_native_quantized_variants_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T22:03:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlc3w4</id>
    <title>Qwen3-Next EXL3</title>
    <updated>2025-09-19T18:53:53+00:00</updated>
    <author>
      <name>/u/Unstable_Llama</name>
      <uri>https://old.reddit.com/user/Unstable_Llama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlc3w4/qwen3next_exl3/"&gt; &lt;img alt="Qwen3-Next EXL3" src="https://external-preview.redd.it/-ZAHeRkIYvxHnoXJsJuTyf1N4ahQAZ_eCyGmivqD2TI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a82fd38e024196c7aa2103e90a3d5fa61f5b8241" title="Qwen3-Next EXL3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-Next-80B-A3B-Instruct quants from turboderp! I would recommend one of the optimized versions if you can fit them.&lt;/p&gt; &lt;p&gt;Note from Turboderp: &amp;quot;Should note that support is currently in the &lt;code&gt;dev&lt;/code&gt; branch. New release build will be probably tomorrow maybe. Probably. Needs more tuning.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unstable_Llama"&gt; /u/Unstable_Llama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/turboderp/Qwen3-Next-80B-A3B-Instruct-exl3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlc3w4/qwen3next_exl3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlc3w4/qwen3next_exl3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T18:53:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nldom8</id>
    <title>KaniTTS – Fast and high-fidelity TTS with just 450M params</title>
    <updated>2025-09-19T19:54:33+00:00</updated>
    <author>
      <name>/u/ylankgz</name>
      <uri>https://old.reddit.com/user/ylankgz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nldom8/kanitts_fast_and_highfidelity_tts_with_just_450m/"&gt; &lt;img alt="KaniTTS – Fast and high-fidelity TTS with just 450M params" src="https://external-preview.redd.it/DHUhIc9SPOwzaKR_faGHZdzuKbPMt8UKVWBWJ3cSLrY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1f32ca50f3cbdefcb4a5f2038d3ddf15b761caf" title="KaniTTS – Fast and high-fidelity TTS with just 450M params" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;We've been tinkering with TTS models for a while, and I'm excited to share KaniTTS – an open-source text-to-speech model we built at NineNineSix.ai. It's designed for speed and quality, hitting real-time generation on consumer GPUs while sounding natural and expressive.&lt;/p&gt; &lt;h1&gt;Quick overview:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: Two-stage pipeline – a LiquidAI LFM2-350M backbone generates compact semantic/acoustic tokens from text (handling prosody, punctuation, etc.), then NVIDIA's NanoCodec synthesizes them into 22kHz waveforms. Trained on ~50k hours of data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: On an RTX 5080, it generates 15s of audio in ~1s with only 2GB VRAM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Languages&lt;/strong&gt;: English-focused, but tokenizer supports Arabic, Chinese, French, German, Japanese, Korean, Spanish (fine-tune for better non-English prosody).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use cases&lt;/strong&gt;: Conversational AI, edge devices, accessibility, or research. Batch up to 16 texts for high throughput.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's Apache 2.0 licensed, so fork away. Check the audio comparisons on the &lt;a href="https://www.nineninesix.ai/n/kani-tts"&gt;https://www.nineninesix.ai/n/kani-tts&lt;/a&gt; – it holds up well against ElevenLabs or Cartesia.&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/nineninesix/kani-tts-450m-0.1-pt"&gt;https://huggingface.co/nineninesix/kani-tts-450m-0.1-pt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Space: &lt;a href="https://huggingface.co/spaces/nineninesix/KaniTTS"&gt;https://huggingface.co/spaces/nineninesix/KaniTTS&lt;/a&gt;&lt;br /&gt; Page: &lt;a href="https://www.nineninesix.ai/n/kani-tts"&gt;https://www.nineninesix.ai/n/kani-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/nineninesix-ai/kani-tts"&gt;https://github.com/nineninesix-ai/kani-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ylankgz"&gt; /u/ylankgz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-450m-0.1-pt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nldom8/kanitts_fast_and_highfidelity_tts_with_just_450m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nldom8/kanitts_fast_and_highfidelity_tts_with_just_450m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T19:54:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nly2mk</id>
    <title>LM Client - A cross-platform native Rust app for interacting with LLMs</title>
    <updated>2025-09-20T13:22:58+00:00</updated>
    <author>
      <name>/u/Severe-Win-9089</name>
      <uri>https://old.reddit.com/user/Severe-Win-9089</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly2mk/lm_client_a_crossplatform_native_rust_app_for/"&gt; &lt;img alt="LM Client - A cross-platform native Rust app for interacting with LLMs" src="https://external-preview.redd.it/_N_Y2fUCX-guiqEJ1K0jV16fiO6q0QfHGEyW8KT6yb4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc9c3cf9eb95bb967e53c37460cc24873cd6528b" title="LM Client - A cross-platform native Rust app for interacting with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;LM Client - an open-source desktop application I've been working on that lets you interact with Language Models through a clean, native UI. It's built entirely in Rust using the Iced GUI framework.&lt;/h1&gt; &lt;h1&gt;What is LM Client?&lt;/h1&gt; &lt;p&gt;LM Client is a standalone desktop application that provides a seamless interface to various AI models through OpenAI-compatible APIs. Unlike browser-based solutions, it's a completely native app focused on performance and a smooth user experience.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;💬 &lt;strong&gt;Chat Interface&lt;/strong&gt;: Clean conversations with AI models&lt;/li&gt; &lt;li&gt;🔄 &lt;strong&gt;RAG Support&lt;/strong&gt;: Use your documents as context for more relevant responses&lt;/li&gt; &lt;li&gt;🌐 &lt;strong&gt;Multiple Providers&lt;/strong&gt;: Works with OpenAI, Ollama, Gemini, and any OpenAI API-compatible services&lt;/li&gt; &lt;li&gt;📂 &lt;strong&gt;Conversation Management&lt;/strong&gt;: Organize chats in folders&lt;/li&gt; &lt;li&gt;⚙️ &lt;strong&gt;Presets&lt;/strong&gt;: Save and reuse configurations for different use cases&lt;/li&gt; &lt;li&gt;📊 &lt;strong&gt;Vector Database&lt;/strong&gt;: Built-in storage for embeddings&lt;/li&gt; &lt;li&gt;🖥️ &lt;strong&gt;Cross-Platform&lt;/strong&gt;: Works on macOS, Windows, and Linux&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Tech Stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Rust&lt;/strong&gt; (2024 edition)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Iced&lt;/strong&gt; for the GUI (pure Rust UI framework, inspired ELM-architecture)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SQLite&lt;/strong&gt; for local database&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why I Built This&lt;/h1&gt; &lt;p&gt;I wanted a native, fast, private LLM client that didn't rely on a browser or electron.&lt;/p&gt; &lt;h1&gt;Screenshots&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dz458dz8lbqf1.png?width=3022&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb62a665a0be7f1d5b2b272ea9d23ee41739d1c5"&gt;https://preview.redd.it/dz458dz8lbqf1.png?width=3022&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb62a665a0be7f1d5b2b272ea9d23ee41739d1c5&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Roadmap&lt;/h1&gt; &lt;p&gt;I am planning several improvements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Custom markdown parser with text selection&lt;/li&gt; &lt;li&gt;QOL and UI improvements&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/pashaish/lm_client"&gt;github.com/pashaish/lm_client&lt;/a&gt;&lt;br /&gt; Pre-built binaries available in the Releases section &lt;/p&gt; &lt;h1&gt;Looking For:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Feedback on the UI/UX&lt;/li&gt; &lt;li&gt;Ideas for additional features&lt;/li&gt; &lt;li&gt;Contributors who are interested in Rust GUI development&lt;/li&gt; &lt;li&gt;Testing on different platforms&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Win-9089"&gt; /u/Severe-Win-9089 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly2mk/lm_client_a_crossplatform_native_rust_app_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly2mk/lm_client_a_crossplatform_native_rust_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nly2mk/lm_client_a_crossplatform_native_rust_app_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T13:22:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm0mzw</id>
    <title>Whisper Large v3 running in real-time on a M2 Macbook Pro</title>
    <updated>2025-09-20T15:08:21+00:00</updated>
    <author>
      <name>/u/rruk01</name>
      <uri>https://old.reddit.com/user/rruk01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm0mzw/whisper_large_v3_running_in_realtime_on_a_m2/"&gt; &lt;img alt="Whisper Large v3 running in real-time on a M2 Macbook Pro" src="https://external-preview.redd.it/NnkxeHk1bTIxY3FmMbdFe5hFZkGFnrWFqBq5GQzhAAe-tezJH5BHnp8SS6Dh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4c3798d766be6b03e6447b1663fb9590cdfcffe" title="Whisper Large v3 running in real-time on a M2 Macbook Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on using the Whisper models on device for 2-3 years now and wanted to share my progress. &lt;/p&gt; &lt;p&gt;I've figured out several optimisations which combined together means I can run the Whisper Large v3 (not turbo) model on a macbook with about 350-600ms latency for live (hypothesis/cyan) requests and 900-1200ms for completed (white) requests. It can also run on an iPhone 14 Pro with about 650-850ms latency for live requests and 1900ms for completed requests. The optimisations work for all the Whisper models and would probably work for the NVIDIA Parakeet / Canary models too. &lt;/p&gt; &lt;p&gt;The optimisations include speeding up the encoder on Apple Neural Engine so it runs at &lt;strong&gt;150ms&lt;/strong&gt; per run, this is compared to a naive 'ANE-optimised' encoder which runs at about &lt;strong&gt;500ms&lt;/strong&gt;. This does not require significant quantisation. The model running in the demo is quantised at Q8, but mainly so it takes up less hard-disk space, FP16 runs at similar speed. I've also optimised hypothesis requests so the output is much more stable. &lt;/p&gt; &lt;p&gt;If there's interest I'd be happy to write up a blog post on these optimisations, I'm also considering making an open source SDK so people can run this themselves, again if there's interest. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rruk01"&gt; /u/rruk01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2ibrz4m21cqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm0mzw/whisper_large_v3_running_in_realtime_on_a_m2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm0mzw/whisper_large_v3_running_in_realtime_on_a_m2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T15:08:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlzpmu</id>
    <title>Kimi K2 and hallucinations</title>
    <updated>2025-09-20T14:31:25+00:00</updated>
    <author>
      <name>/u/ramendik</name>
      <uri>https://old.reddit.com/user/ramendik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I spent some time using Kimi K2 as the daily driver, first on kimi dot com, then on my own OpenWebUI/LiteLLM setup that it helped me set up, step by step.&lt;/p&gt; &lt;p&gt;The lack of sycophancy! It wastes no time telling me how great my ideas are, instead it spits out code to try and make them work.&lt;/p&gt; &lt;p&gt;The ability to push back on bad ideas! The creative flight when discussing a draft novel/musical - and the original draft was in Russian! (Though it did become more coherent and really creative when the discussion switched to a potentian English-language musical adaptation).&lt;/p&gt; &lt;p&gt;This is all great and quite unique. The model has a personality, it's the kind of personality some writers expected to see in robots, and by &amp;quot;some&amp;quot; I mean the writers of Futurama. Extremely enjoyable, projecting a &amp;quot;confident and blunt nerd&amp;quot;. The reason I let it guide the VPS setup was because that personality was needed to help me break out of perfectionist tweaking of the idea and into the actual setup.&lt;/p&gt; &lt;p&gt;The downside: quite a few of the config files it prepared for me had non-obvious errors. The nerd is &lt;em&gt;over&lt;/em&gt;confident.&lt;/p&gt; &lt;p&gt;The level of hallucination in Kimi K2 is &lt;em&gt;something&lt;/em&gt;. When discussing general ideas this is kinda even fun - it once invented an entire experiment it did &amp;quot;with a colleague&amp;quot;! One can get used to any unsourced numbers likely being faked. But it's harder to get used to hallucinations when they concern practical technical things: configs, UI paths, terminal commands, and so on. Especially since Kimi's hallycinations in these matters &lt;em&gt;make sense&lt;/em&gt;. It's not random blabber - Kimi infers how it &lt;em&gt;should be&lt;/em&gt;, and assumes that's how it is.&lt;/p&gt; &lt;p&gt;I even considered looking into finding hosted DPO training for the model to try and train in flagging uncertainty, but then I realized that apart from any expenses, training a MoE is just tricky.&lt;/p&gt; &lt;p&gt;I &lt;em&gt;could&lt;/em&gt; try a multi-model pathway, possibly pitting K2 &lt;em&gt;against itself&lt;/em&gt; with another instance checking the output of the first one for hallucinations. What intervened next, for now, is money: I found that Qwen 235B A22 Instruct provides rather good inference much cheaper. So now, instead of trying to trick hallucinations out of K2, I'm trying to prompt sycophancy out of A22, and a two-step with a sycophancy filter is on the cards if I can't. I'll keep K2 on tap in my system for cases when I want strong pushback and wild ideation, not facts nor configs.&lt;/p&gt; &lt;p&gt;But maybe someone else faced the K2 hallucination issue and found a solution? Maybe there is a system prompt trick that works and that I just didn't think of, for example?&lt;/p&gt; &lt;p&gt;P.S. I wrote a more detailed review some time ago, based on my imi dot com experience: &lt;a href="https://www.lesswrong.com/posts/cJfLjfeqbtuk73Kja/kimi-k2-personal-review-part-1"&gt;https://www.lesswrong.com/posts/cJfLjfeqbtuk73Kja/kimi-k2-personal-review-part-1&lt;/a&gt; . An update to it is that on the API, even served by Moonshot (via OpenRouter), censorship is no longer an issue. It talked about Tiananmen - &lt;em&gt;on its own initiative&lt;/em&gt;, my prompt was about &amp;quot;China's history after the Cultural Revolution&amp;quot;. Part 2 of the review is not yet ready because I want to run my own proprietary mini-benchmark on long context retrieval, but got stuck on an OpenWebUI bug. I also will review Qwen 235B A22 after I spend more time with it; I can already report censorship is not an issue there either (though I use it from a non-Chinese cloud server). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ramendik"&gt; /u/ramendik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlzpmu/kimi_k2_and_hallucinations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlzpmu/kimi_k2_and_hallucinations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlzpmu/kimi_k2_and_hallucinations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T14:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlyt65</id>
    <title>What is the best LLM for psychology, coach or emotional support.</title>
    <updated>2025-09-20T13:54:55+00:00</updated>
    <author>
      <name>/u/pumukidelfuturo</name>
      <uri>https://old.reddit.com/user/pumukidelfuturo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried Qwen3 and sucks big time. It only says very stupid things.&lt;/p&gt; &lt;p&gt;Yes, you shouldn't use llm's for that. I know. In any case give some solid names plox.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pumukidelfuturo"&gt; /u/pumukidelfuturo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyt65/what_is_the_best_llm_for_psychology_coach_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyt65/what_is_the_best_llm_for_psychology_coach_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyt65/what_is_the_best_llm_for_psychology_coach_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T13:54:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nluem1</id>
    <title>Tips for a new rig (192Gb vram)</title>
    <updated>2025-09-20T10:12:29+00:00</updated>
    <author>
      <name>/u/Breath_Unique</name>
      <uri>https://old.reddit.com/user/Breath_Unique</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nluem1/tips_for_a_new_rig_192gb_vram/"&gt; &lt;img alt="Tips for a new rig (192Gb vram)" src="https://preview.redd.it/kl9xtbueoaqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e80234226dd01975a2b2a479345cd9ffcde2924d" title="Tips for a new rig (192Gb vram)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. We are about to receive some new hardware for running local models. Please see the image for the specs. We were thinking Kimi k2 would be a good place to start, running it through ollama. Does anyone have any tips re utilizing this much vram? Any optimisations we should look into etc? Any help would be greatly appreciated. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Breath_Unique"&gt; /u/Breath_Unique &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kl9xtbueoaqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nluem1/tips_for_a_new_rig_192gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nluem1/tips_for_a_new_rig_192gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T10:12:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlxchg</id>
    <title>How to think about GPUs (by Google)</title>
    <updated>2025-09-20T12:50:05+00:00</updated>
    <author>
      <name>/u/notdl</name>
      <uri>https://old.reddit.com/user/notdl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlxchg/how_to_think_about_gpus_by_google/"&gt; &lt;img alt="How to think about GPUs (by Google)" src="https://preview.redd.it/dtyx6xrfgbqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46161052c3a29fd8459abfcce3ffb4aa7283efca" title="How to think about GPUs (by Google)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notdl"&gt; /u/notdl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dtyx6xrfgbqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlxchg/how_to_think_about_gpus_by_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlxchg/how_to_think_about_gpus_by_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T12:50:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlq337</id>
    <title>Making LLMs more accurate by using all of their layers</title>
    <updated>2025-09-20T05:43:15+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlq337/making_llms_more_accurate_by_using_all_of_their/"&gt; &lt;img alt="Making LLMs more accurate by using all of their layers" src="https://external-preview.redd.it/Xfy8b5oz8xAgNpbj0L9Mmjzxactj5HdaKRFOmBPu0YE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9dad5b13e20f57d64f5fc0bbc7415c9f4186b1d" title="Making LLMs more accurate by using all of their layers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.google/blog/making-llms-more-accurate-by-using-all-of-their-layers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlq337/making_llms_more_accurate_by_using_all_of_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlq337/making_llms_more_accurate_by_using_all_of_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T05:43:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlxq55</id>
    <title>1K+ schemas of agentic projects visualized</title>
    <updated>2025-09-20T13:07:26+00:00</updated>
    <author>
      <name>/u/altsoph</name>
      <uri>https://old.reddit.com/user/altsoph</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I analyzed 1K+ Reddit posts about AI agent projects, processed them automatically into graphical schemas, and studied them. You can play with them interactively: &lt;a href="https://altsoph.com/pp/aps/"&gt;https://altsoph.com/pp/aps/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Besides many really strange constructions, I found three dominant patterns: chat-with-data (50%), business process automation (25%), and tool-assisted planning (15%). Each has specific requirements and pain points, and these patterns seem remarkably consistent with my own experience building agent systems.&lt;/p&gt; &lt;p&gt; I'd love to discuss if others see different patterns in this data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/altsoph"&gt; /u/altsoph &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlxq55/1k_schemas_of_agentic_projects_visualized/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlxq55/1k_schemas_of_agentic_projects_visualized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlxq55/1k_schemas_of_agentic_projects_visualized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T13:07:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlwhmk</id>
    <title>CodeRabbit commits $1 million to open source</title>
    <updated>2025-09-20T12:07:58+00:00</updated>
    <author>
      <name>/u/Motor_Cycle7600</name>
      <uri>https://old.reddit.com/user/Motor_Cycle7600</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlwhmk/coderabbit_commits_1_million_to_open_source/"&gt; &lt;img alt="CodeRabbit commits $1 million to open source" src="https://external-preview.redd.it/4kvDM7gPrl0ixzXFp7sXBnwp2EZyHR1-9DufiPVEqAE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aeda450f719cb582ac0b17b8b6737424b7b57034" title="CodeRabbit commits $1 million to open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Motor_Cycle7600"&gt; /u/Motor_Cycle7600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.coderabbit.ai/blog/coderabbit-commits-1-million-to-open-source"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlwhmk/coderabbit_commits_1_million_to_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlwhmk/coderabbit_commits_1_million_to_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T12:07:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nltfwx</id>
    <title>AI CEOs: only I am good and wise enough to build ASI (artificial superintelligence). Everybody else is evil or won't do it right.</title>
    <updated>2025-09-20T09:12:06+00:00</updated>
    <author>
      <name>/u/FinnFarrow</name>
      <uri>https://old.reddit.com/user/FinnFarrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nltfwx/ai_ceos_only_i_am_good_and_wise_enough_to_build/"&gt; &lt;img alt="AI CEOs: only I am good and wise enough to build ASI (artificial superintelligence). Everybody else is evil or won't do it right." src="https://external-preview.redd.it/MHdnZnppa2VkYXFmMfboDEJV_8E07yibCTC4f2dErk0sK7LfErgP63h2qGj9.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=322dc704f600fd94bc23fadf748b6bdace23594e" title="AI CEOs: only I am good and wise enough to build ASI (artificial superintelligence). Everybody else is evil or won't do it right." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FinnFarrow"&gt; /u/FinnFarrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kdoptgkedaqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nltfwx/ai_ceos_only_i_am_good_and_wise_enough_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nltfwx/ai_ceos_only_i_am_good_and_wise_enough_to_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T09:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlfm4p</id>
    <title>Matthew McConaughey says he wants a private LLM on Joe Rogan Podcast</title>
    <updated>2025-09-19T21:11:21+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlfm4p/matthew_mcconaughey_says_he_wants_a_private_llm/"&gt; &lt;img alt="Matthew McConaughey says he wants a private LLM on Joe Rogan Podcast" src="https://external-preview.redd.it/YzFwanVkZnpzNnFmMbLrEG3LS8K9xI7Zo9NLFNWl_BVRzdP5tkFGVRvYzADE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01aa65a0a1dadcdaa683d6c6c12e54de616964d9" title="Matthew McConaughey says he wants a private LLM on Joe Rogan Podcast" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Matthew McConaughey says he wants a private LLM, fed only with his books, notes, journals, and aspirations, so he can ask it questions and get answers based solely on that information, without any outside influence.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/JonhernandezIA/status/1969054219647803765"&gt;https://x.com/JonhernandezIA/status/1969054219647803765&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey Matthew, what you described already exists. It's called &lt;a href="https://hyperlink.nexa.ai/"&gt;Hyperlink&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n2vmpefzs6qf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlfm4p/matthew_mcconaughey_says_he_wants_a_private_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlfm4p/matthew_mcconaughey_says_he_wants_a_private_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T21:11:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlufzx</id>
    <title>llama.ui: new updates!</title>
    <updated>2025-09-20T10:14:45+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlufzx/llamaui_new_updates/"&gt; &lt;img alt="llama.ui: new updates!" src="https://preview.redd.it/mjwmirusoaqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b3160b178b7387b2186e3b81e5c0b04c1d83fe5" title="llama.ui: new updates!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm excited to announce an update to &lt;strong&gt;llama.ui&lt;/strong&gt;, a privacy focused web interface for interacting with Large Language Models! We bring some awesome new features and performance improvements: - Configuration Presets: Save and load your favorite configurations for different models and use cases. - Text-to-Speech: Listen to the AI's responses! Supports multiple voices and languages. - Database Export/Import: Backup your chat history or transfer to a new device! - Conversation Branching: Experiment with different paths in your conversations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mjwmirusoaqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlufzx/llamaui_new_updates/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlufzx/llamaui_new_updates/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T10:14:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nly3w1</id>
    <title>Qwen 3 VL next week</title>
    <updated>2025-09-20T13:24:32+00:00</updated>
    <author>
      <name>/u/Long_Bluejay_5368</name>
      <uri>https://old.reddit.com/user/Long_Bluejay_5368</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"&gt; &lt;img alt="Qwen 3 VL next week" src="https://a.thumbs.redditmedia.com/fLB-QxQX_aAn0F5HXNaiy2dlb5JbWlBjS-VuT3q3TC0.jpg" title="Qwen 3 VL next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/19dxif2kmbqf1.png?width=788&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56a8c11d753f68cacd685640484117a43de99ce3"&gt;https://preview.redd.it/19dxif2kmbqf1.png?width=788&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56a8c11d753f68cacd685640484117a43de99ce3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;what do you think about it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Long_Bluejay_5368"&gt; /u/Long_Bluejay_5368 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T13:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlkwr3</id>
    <title>OpenWebUI is the most bloated piece of s**t on earth, not only that but it's not even truly open source anymore, now it just pretends it is because you can't remove their branding from a single part of their UI. Suggestions for new front end?</title>
    <updated>2025-09-20T01:08:06+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Honestly, I'm better off straight up using SillyTavern, I can even have some fun with a cute anime girl as my assistant helping me code or goof off instead of whatever dumb stuff they're pulling.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlkwr3/openwebui_is_the_most_bloated_piece_of_st_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlkwr3/openwebui_is_the_most_bloated_piece_of_st_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlkwr3/openwebui_is_the_most_bloated_piece_of_st_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T01:08:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlyy6n</id>
    <title>Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping</title>
    <updated>2025-09-20T14:00:49+00:00</updated>
    <author>
      <name>/u/PhantomWolf83</name>
      <uri>https://old.reddit.com/user/PhantomWolf83</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"&gt; &lt;img alt="Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping" src="https://external-preview.redd.it/942g63AteF3sF5KI6YzwLlHNUjooze5_uZcUA7PiVqQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=259bd6663f4a689dc50651317dca845a29e37f3f" title="Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhantomWolf83"&gt; /u/PhantomWolf83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/intel-arc-pro-b60-24gb-professional-gpu-listed-at-599-in-stock-and-shipping"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T14:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlu3cd</id>
    <title>The iPhone 17 Pro can run LLMs fast!</title>
    <updated>2025-09-20T09:53:52+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlu3cd/the_iphone_17_pro_can_run_llms_fast/"&gt; &lt;img alt="The iPhone 17 Pro can run LLMs fast!" src="https://a.thumbs.redditmedia.com/lazvh4ZugenSKXRU1IYFLEO6hichFPkV7Tw3LqJ6h_8.jpg" title="The iPhone 17 Pro can run LLMs fast!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The new A19 Pro finally integrates neural accelerators into the GPU cores themselves, essentially Apple’s version of Nvidia’s Tensor cores which are used for accelerating matrix multiplication that is prevalent in the transformers models we love so much. So I thought it would be interesting to test out running our smallest finetuned models on it!&lt;/p&gt; &lt;p&gt;Boy does the GPU fly compared to running the model only on CPU. The token generation is only about double but the prompt processing is over 10x faster! It’s so much faster that it’s actually usable even on longer context as the prompt processing doesn’t quickly become too long and the token generation speed is still high.&lt;/p&gt; &lt;p&gt;I tested using the Pocket Pal app on IOS which runs regular llamacpp with MLX Metal optimizations as far as I know. Shown are the comparison of the model running on GPU fully offloaded with Metal API and flash attention enabled vs running on CPU only. &lt;/p&gt; &lt;p&gt;Judging by the token generation speed, the A19 Pro must have about 70-80GB/s of memory bandwidth to the GPU and the CPU can access only about half of that bandwidth. &lt;/p&gt; &lt;p&gt;Anyhow the new GPU with the integrated tensor cores now look very interesting for running LLMs. Perhaps when new Mac Studios with updated M chips comes out with a big version of this new GPU architecture, I might even be able to use them to serve models for our low cost API. 🤔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nlu3cd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlu3cd/the_iphone_17_pro_can_run_llms_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlu3cd/the_iphone_17_pro_can_run_llms_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T09:53:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkft9l</id>
    <title>AMA with the LM Studio team</title>
    <updated>2025-09-18T18:12:24+00:00</updated>
    <author>
      <name>/u/yags-lms</name>
      <uri>https://old.reddit.com/user/yags-lms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We're excited for this AMA. Thank you for having us here today. We got a full house from the LM Studio team: &lt;/p&gt; &lt;p&gt;- Yags &lt;a href="https://t.co/ERfA4NrR96"&gt;https://reddit.com/user/yags-lms/&lt;/a&gt; (founder)&lt;br /&gt; - Neil &lt;a href="https://t.co/KyiHVfv0QG"&gt;https://reddit.com/user/neilmehta24/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Will &lt;a href="https://t.co/IjAZJL2JMK"&gt;https://reddit.com/user/will-lms/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Matt &lt;a href="https://t.co/6MNkItPYnI"&gt;https://reddit.com/user/matt-lms/&lt;/a&gt; (LLM engines, runtime, and APIs)&lt;br /&gt; - Ryan &lt;a href="https://t.co/0snuNUPizo"&gt;https://reddit.com/user/ryan-lms/&lt;/a&gt; (Core system and APIs)&lt;br /&gt; - Rugved &lt;a href="https://t.co/xGtYHsJZI3"&gt;https://reddit.com/user/rugved_lms/&lt;/a&gt; (CLI and SDKs)&lt;br /&gt; - Alex &lt;a href="https://t.co/wtT2IFf0z6"&gt;https://reddit.com/user/alex-lms/&lt;/a&gt; (App)&lt;br /&gt; - Julian &lt;a href="https://www.reddit.com/user/julian-lms/"&gt;https://www.reddit.com/user/julian-lms/&lt;/a&gt; (Ops) &lt;/p&gt; &lt;p&gt;Excited to chat about: the latest local models, UX for local models, steering local models effectively, LM Studio SDK and APIs, how we support multiple LLM engines (llama.cpp, MLX, and more), privacy philosophy, why local AI matters, our open source projects (mlx-engine, lms, lmstudio-js, lmstudio-python, venvstacks), why ggerganov and Awni are the GOATs, where is TheBloke, and more. &lt;/p&gt; &lt;p&gt;Would love to hear about people's setup, which models you use, use cases that really work, how you got into local AI, what needs to improve in LM Studio and the ecosystem as a whole, how you use LM Studio, and anything in between!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Everyone: it was awesome to see your questions here today and share replies! Thanks a lot for the welcoming AMA. We will continue to monitor this post for more questions over the next couple of days, but for now we're signing off to continue building 🔨&lt;/p&gt; &lt;p&gt;We have several marquee features we've been working on for a loong time coming out later this month that we hope you'll love and find lots of value in. And don't worry, UI for n cpu moe is on the way too :)&lt;/p&gt; &lt;p&gt;Special shoutout and thanks to ggerganov, Awni Hannun, TheBloke, Hugging Face, and all the rest of the open source AI community!&lt;/p&gt; &lt;p&gt;Thank you and see you around! - Team LM Studio 👾&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yags-lms"&gt; /u/yags-lms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:12:24+00:00</published>
  </entry>
</feed>
