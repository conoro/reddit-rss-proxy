<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-02T17:50:09+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qtyuon</id>
    <title>Info on performance (accuracy) when context window reaches a certain size?</title>
    <updated>2026-02-02T15:56:51+00:00</updated>
    <author>
      <name>/u/fragment_me</name>
      <uri>https://old.reddit.com/user/fragment_me</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recall seeing some graphs shared here about big models (GLM 4.7, mini 2.1, Gemini variants, GPT, Claude) and their accuracy falling after the context window reaches a certain size. The graph was very interesting, but I never saved it. I'm trying to find the sweet/safe spot to set my max context size to, and right now I default it to 50%. I've been searching for this info but for some reason it eludes me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fragment_me"&gt; /u/fragment_me &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtyuon/info_on_performance_accuracy_when_context_window/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtyuon/info_on_performance_accuracy_when_context_window/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtyuon/info_on_performance_accuracy_when_context_window/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T15:56:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtwl1n</id>
    <title>Guidance Needed: Best Option for Light Fine-Tuning &amp; Inference (Dell Pro Max GB10 vs PGX vs GX10 vs DGX Spark): We absolutely need CUDA</title>
    <updated>2026-02-02T14:31:23+00:00</updated>
    <author>
      <name>/u/Imaginary_Context_32</name>
      <uri>https://old.reddit.com/user/Imaginary_Context_32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôre currently evaluating three workstation options and would appreciate your recommendation based on our actual workload and the constraints we‚Äôve observed so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dell Pro Max with GB10&lt;/li&gt; &lt;li&gt;ThinkStation PGX&lt;/li&gt; &lt;li&gt;Asus Ascent GX10&lt;/li&gt; &lt;li&gt;nvidia dgx spark&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our primary use case is basic inference with fine-tuning jobs. We will be doing sustained or heavy training (hence CUDA) workloads.&lt;/p&gt; &lt;p&gt;That said, we‚Äôve run into some important concerned limitations on similar systems that we want to factor into the decision:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Thermal limits appear to prevent reliable moderate training.&lt;/li&gt; &lt;li&gt;These failures occurred despite sufficient memory, with the unit powering off unexpectedly?&lt;/li&gt; &lt;li&gt;For inference-only workloads, performance has been acceptable, but software constraints (CUDA/OS version lock-ins) have caused friction and reinstallation overhead.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Given these realities, we‚Äôre trying to determine:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Which of the three systems is most reliable and well-designed for inference-first usage&lt;/li&gt; &lt;li&gt;Which offers the best thermal and power stability headroom, even if training is limited&lt;/li&gt; &lt;li&gt;Whether any of these platforms meaningfully outperform the others in practical, not theoretical, workloads&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Based on your experience, which option would you recommend for our needs, and why?&lt;/p&gt; &lt;p&gt;Appreciate it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Imaginary_Context_32"&gt; /u/Imaginary_Context_32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtwl1n/guidance_needed_best_option_for_light_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtwl1n/guidance_needed_best_option_for_light_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtwl1n/guidance_needed_best_option_for_light_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T14:31:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtj039</id>
    <title>What's your dream in 2026?</title>
    <updated>2026-02-02T02:46:40+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hope that guys from Wall Street would make price of RAM/SSD back to normal, by whatever means.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtj039/whats_your_dream_in_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtj039/whats_your_dream_in_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtj039/whats_your_dream_in_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T02:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt9gyf</id>
    <title>I built a pentesting platform that lets AI control 400+ hacking tools</title>
    <updated>2026-02-01T20:17:14+00:00</updated>
    <author>
      <name>/u/Justachillguypeace</name>
      <uri>https://old.reddit.com/user/Justachillguypeace</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt9gyf/i_built_a_pentesting_platform_that_lets_ai/"&gt; &lt;img alt="I built a pentesting platform that lets AI control 400+ hacking tools" src="https://external-preview.redd.it/MmhocXdobTl5eGdnMS7Ny9qzMAmuinIQRg---a-6I7vN05-3-TDw6Gj1XVF3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e0dd22cb6593dff42a1b197f8e3eb8049aa617e" title="I built a pentesting platform that lets AI control 400+ hacking tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on this project for the past month as a side project (I'm a pentester).&lt;/p&gt; &lt;p&gt;The idea: give your AI agent a full pentesting environment. Claude can execute tools directly in a Docker container, chain attacks based on what it finds, and document everything automatically.&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;p&gt;- AI agent connects via MCP to an Exegol container (400+ security tools)&lt;/p&gt; &lt;p&gt;- Executes nmap, sqlmap, nuclei, ffuf, etc. directly&lt;/p&gt; &lt;p&gt;- Tracks findings in a web dashboard&lt;/p&gt; &lt;p&gt;- Maintains full context across the entire assessment&lt;/p&gt; &lt;p&gt;No more copy-pasting commands back and forth between Claude and your terminal :)&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Vasco0x4/AIDA"&gt;https://github.com/Vasco0x4/AIDA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://www.youtube.com/watch?v=yz6ac-y4g08"&gt;https://www.youtube.com/watch?v=yz6ac-y4g08&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is my first big open source project, so I'm waiting for honest reviews and feedback. Not trying to monetize it, just sharing with the community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Justachillguypeace"&gt; /u/Justachillguypeace &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sfk44fm9yxgg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt9gyf/i_built_a_pentesting_platform_that_lets_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt9gyf/i_built_a_pentesting_platform_that_lets_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T20:17:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu1s1i</id>
    <title>System Audit Scanning</title>
    <updated>2026-02-02T17:39:10+00:00</updated>
    <author>
      <name>/u/Fantastic-Issue1020</name>
      <uri>https://old.reddit.com/user/Fantastic-Issue1020</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1s1i/system_audit_scanning/"&gt; &lt;img alt="System Audit Scanning" src="https://external-preview.redd.it/w8cZUy7yQyXkDYLTXEYQu5pjuwJ3tNlUC7Lh63KfUdI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1446699828ecde62e6ca275c44382a9557de2b02" title="System Audit Scanning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;in case you are using AI tools and want to make deep security audits of your system and generate cryptographically signed, tamper-evident reports you can use this repo, also lmk if you want it into the central registry or other platforms!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic-Issue1020"&gt; /u/Fantastic-Issue1020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vigil-xy/vigil-mcp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1s1i/system_audit_scanning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1s1i/system_audit_scanning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T17:39:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtlxnz</id>
    <title>Why is RVC still the king of STS after 2 years of silence? Is there a technical plateau?</title>
    <updated>2026-02-02T05:04:32+00:00</updated>
    <author>
      <name>/u/lnkhey</name>
      <uri>https://old.reddit.com/user/lnkhey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I have been thinking about where Speech to Speech (STS) is heading for music use. RVC has not seen a major update in ages and I find it strange that we are still stuck with it. Even with the best forks like Applio or Mangio, those annoying artifacts and other issues are still present in almost every render.&lt;/p&gt; &lt;p&gt;Is it because the research has shifted towards Text to Speech (TTS) or Zero-shot models because they are more commercially viable? Or is it a bottleneck with current vocoders that just can not handle complex singing perfectly?&lt;/p&gt; &lt;p&gt;I also wonder if the industry is prioritizing real-time performance (low latency) over actual studio quality. Are there any diffusion-based models that are actually usable for singing without having all these artifacts ??&lt;/p&gt; &lt;p&gt;It feels like we are on a plateau while every other AI field is exploding. What am I missing here? Is there a &amp;quot;RVC killer&amp;quot; in the works or are we just repurposing old tech forever?&lt;/p&gt; &lt;p&gt;Thanks for your insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lnkhey"&gt; /u/lnkhey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtlxnz/why_is_rvc_still_the_king_of_sts_after_2_years_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtlxnz/why_is_rvc_still_the_king_of_sts_after_2_years_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtlxnz/why_is_rvc_still_the_king_of_sts_after_2_years_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T05:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtj87p</id>
    <title>What's the most complicated project you've built with AI?</title>
    <updated>2026-02-02T02:56:39+00:00</updated>
    <author>
      <name>/u/jazir555</name>
      <uri>https://old.reddit.com/user/jazir555</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bonus points if its complex and purely vibe coded&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jazir555"&gt; /u/jazir555 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtj87p/whats_the_most_complicated_project_youve_built/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtj87p/whats_the_most_complicated_project_youve_built/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtj87p/whats_the_most_complicated_project_youve_built/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T02:56:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtzmfu</id>
    <title>[Release] AI Video Clipper v3.5: Ultimate Dataset Creator with UV Engine &amp; RTX 5090 Support</title>
    <updated>2026-02-02T16:23:51+00:00</updated>
    <author>
      <name>/u/Ill_Tour2308</name>
      <uri>https://old.reddit.com/user/Ill_Tour2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtzmfu/release_ai_video_clipper_v35_ultimate_dataset/"&gt; &lt;img alt="[Release] AI Video Clipper v3.5: Ultimate Dataset Creator with UV Engine &amp;amp; RTX 5090 Support" src="https://preview.redd.it/2i0ix6gox3hg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c0646deb4e0bba283bc41a2318c93f7fe002ac9" title="[Release] AI Video Clipper v3.5: Ultimate Dataset Creator with UV Engine &amp;amp; RTX 5090 Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! üëÅÔ∏èüêß I've just released v3.5 of my open-source tool for LoRA dataset creation. It features a new blazing-fast UV installer, native Linux/WSL support, and verified fixes for the RTX 5090. Full details and GitHub link in the first comment below!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill_Tour2308"&gt; /u/Ill_Tour2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2i0ix6gox3hg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtzmfu/release_ai_video_clipper_v35_ultimate_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtzmfu/release_ai_video_clipper_v35_ultimate_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T16:23:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtws29</id>
    <title>"Tier kings" list? - Lookign for model recommendations per V/RAM tier</title>
    <updated>2026-02-02T14:39:12+00:00</updated>
    <author>
      <name>/u/IngwiePhoenix</name>
      <uri>https://old.reddit.com/user/IngwiePhoenix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is inspired directly by this post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have been trying to look for model recommendations - be it for in-editor autocomplete, or full agentic workloads (OpenCode, Zed).&lt;/p&gt; &lt;p&gt;Right now, I &lt;em&gt;only&lt;/em&gt; have a 4090 with 24GB of VRAM - but I plan to upgrade my setup, and it'd be quite nice to know what the current &amp;quot;tiers&amp;quot; are - especially in regards to quants or contexts. A coding agent seems to be doing quite fine with ~100k context, whilst an autocomplete'er won't need that much.&lt;/p&gt; &lt;p&gt;Let's say the tiers were 24, 48, 128 and 256 for the Mac Studio people (I am not buying one, but definitively curious regardless).&lt;/p&gt; &lt;p&gt;Thanks :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IngwiePhoenix"&gt; /u/IngwiePhoenix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtws29/tier_kings_list_lookign_for_model_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtws29/tier_kings_list_lookign_for_model_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtws29/tier_kings_list_lookign_for_model_recommendations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T14:39:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu1mf9</id>
    <title>Transformer Lab can Now Train Across Clusters of GPUs</title>
    <updated>2026-02-02T17:33:33+00:00</updated>
    <author>
      <name>/u/aliasaria</name>
      <uri>https://old.reddit.com/user/aliasaria</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You may have seen our open source work called Transformer Lab. Now, we built &lt;strong&gt;Transformer Lab for Teams&lt;/strong&gt; to support AI work that can scale across clusters of GPUs. &lt;/p&gt; &lt;p&gt;After talking to numerous labs and individuals training models beyond a single node we heard:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The frontier labs invest a ton to build and maintain their own proprietary tooling.&lt;/li&gt; &lt;li&gt;Most other AI/ML research teams work with a fragmented landscape of legacy scripts, manual workflows which gets more complicated as you grow your team and run more experiments&lt;/li&gt; &lt;li&gt;Researchers spend almost half their time dealing with logistics. For example, results get lost or rerun because jobs fail before finishing and artifacts aren‚Äôt tracked consistently.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How Transformer Lab for Teams is helpful:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Unified Interface:&lt;/strong&gt; A single dashboard to manage data ingestion, model fine-tuning, and evaluation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Seamless Scaling:&lt;/strong&gt; The platform is architected to run locally on personal hardware (Apple Silicon, NVIDIA/AMD GPUs) and seamlessly scale to high-performance computing clusters using orchestrators like Slurm and SkyPilot.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extensibility:&lt;/strong&gt; A flexible plugin system allows researchers to add custom training loops, evaluation metrics, and model architectures without leaving the platform.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Privacy-First:&lt;/strong&gt; The platform processes data within the user's infrastructure, whether on-premise or in a private cloud, ensuring sensitive research data never leaves the lab's control.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simplifying workflows:&lt;/strong&gt; Capabilities that used to require complex engineering are now built-in. &lt;ul&gt; &lt;li&gt;Capturing checkpoints (with auto-restart)&lt;/li&gt; &lt;li&gt;One-line to add hyperparameter sweeps&lt;/li&gt; &lt;li&gt;Storing artifacts in a global object store accessible even after ephemeral nodes terminate.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our goal is to make LLM/Diffusion/Audio training easier as you scale: from a single machine to multi-GPU, multi-node setups. All without rewriting your training code.&lt;/p&gt; &lt;p&gt;The project is &lt;strong&gt;open source and free to use&lt;/strong&gt;. It also works on CLI. &lt;/p&gt; &lt;p&gt;We just launched the beta here: &lt;a href="https://lab.cloud/"&gt;https://lab.cloud/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm one of the maintainers and can walk you through install or even provide a live demo if you‚Äôd like. Have a look and let us know how we can make it better for you. &lt;/p&gt; &lt;p&gt;Ask any questions here! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aliasaria"&gt; /u/aliasaria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1mf9/transformer_lab_can_now_train_across_clusters_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1mf9/transformer_lab_can_now_train_across_clusters_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1mf9/transformer_lab_can_now_train_across_clusters_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T17:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt76qs</id>
    <title>Mistral Vibe 2.0</title>
    <updated>2026-02-01T18:56:50+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt76qs/mistral_vibe_20/"&gt; &lt;img alt="Mistral Vibe 2.0" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral Vibe 2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like I missed Mistral Vibe 2.0 being announced because I‚Äôve been busy with OpenCode.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-vibe-2-0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt76qs/mistral_vibe_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt76qs/mistral_vibe_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T18:56:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtoukf</id>
    <title>CISA acting director reportedly uploaded sensitive documents to ChatGPT</title>
    <updated>2026-02-02T07:46:50+00:00</updated>
    <author>
      <name>/u/EchoOfOppenheimer</name>
      <uri>https://old.reddit.com/user/EchoOfOppenheimer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Acting Director of CISA, the top cybersecurity agency in the US, was just caught uploading sensitive government documents to the PUBLIC version of ChatGPT. He reportedly bypassed his own agency's security blocks to do it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EchoOfOppenheimer"&gt; /u/EchoOfOppenheimer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.scworld.com/brief/cisa-acting-director-reportedly-uploaded-sensitive-documents-to-chatgpt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtoukf/cisa_acting_director_reportedly_uploaded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtoukf/cisa_acting_director_reportedly_uploaded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T07:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtuwe7</id>
    <title>Local model fully replacing subscription service</title>
    <updated>2026-02-02T13:22:41+00:00</updated>
    <author>
      <name>/u/Icy_Distribution_361</name>
      <uri>https://old.reddit.com/user/Icy_Distribution_361</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm really impressed with local models on a Macbook Pro M4 Pro with 24GB memory. For my usecase, I don't really see the need anymore for a subscription model. While I'm a pretty heavy user of ChatGPT, I don't really ask complicated questions usually. It's mostly &amp;quot;what does the research say about this&amp;quot;, &amp;quot;who is that&amp;quot;, &amp;quot;how does X work&amp;quot;, &amp;quot;what's the etymology of ...&amp;quot; and so on. I don't really do much extensive writing together with it, or much coding (a little bit sometimes). I just hadn't expected Ollama + GPT-OSS:20b to be as high quality and fast as it is. And yes, I know about all the other local models out there, but I actually like GPT-OSS... I know it gets a lot of crap.&lt;/p&gt; &lt;p&gt;Anyone else considering, or has already, cancelling subscriptions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Distribution_361"&gt; /u/Icy_Distribution_361 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtuwe7/local_model_fully_replacing_subscription_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtuwe7/local_model_fully_replacing_subscription_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtuwe7/local_model_fully_replacing_subscription_service/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T13:22:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtisy5</id>
    <title>Step 3.5 Flash 200B</title>
    <updated>2026-02-02T02:37:59+00:00</updated>
    <author>
      <name>/u/limoce</name>
      <uri>https://old.reddit.com/user/limoce</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/stepfun-ai/Step-3.5-Flash"&gt;https://huggingface.co/stepfun-ai/Step-3.5-Flash&lt;/a&gt;&lt;br /&gt; News: &lt;a href="https://static.stepfun.com/blog/step-3.5-flash/"&gt;https://static.stepfun.com/blog/step-3.5-flash/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: 196B A11B&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/limoce"&gt; /u/limoce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtisy5/step_35_flash_200b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtisy5/step_35_flash_200b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtisy5/step_35_flash_200b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T02:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu1j8f</id>
    <title>ggml-cpu: FA split across kv for faster TG</title>
    <updated>2026-02-02T17:30:24+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1j8f/ggmlcpu_fa_split_across_kv_for_faster_tg/"&gt; &lt;img alt="ggml-cpu: FA split across kv for faster TG" src="https://external-preview.redd.it/R4jvaeUcXiua-hwaogdXuUXVYGR6WfvIUnqzyL6NDik.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f1403d66bb0d55b925437fb753efc214331c697" title="ggml-cpu: FA split across kv for faster TG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CPU Flash-Attention decoding speed-up (long contexts).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19209"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1j8f/ggmlcpu_fa_split_across_kv_for_faster_tg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu1j8f/ggmlcpu_fa_split_across_kv_for_faster_tg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T17:30:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtqy6f</id>
    <title>Playing Civilization VI with a Computer-Use agent</title>
    <updated>2026-02-02T09:56:52+00:00</updated>
    <author>
      <name>/u/Working_Original9624</name>
      <uri>https://old.reddit.com/user/Working_Original9624</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqy6f/playing_civilization_vi_with_a_computeruse_agent/"&gt; &lt;img alt="Playing Civilization VI with a Computer-Use agent" src="https://external-preview.redd.it/aG01OHh4ZzUwMmhnMSU9p-WqwsrPdmT3GD6YCmDr1IKgEI58rOR3KY0kqV6w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a0af494b21ef8e01bbfc861df9a131b193dcc03" title="Playing Civilization VI with a Computer-Use agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With recent advances in VLMs, Computer-Use‚ÄîAI directly operating a real computer‚Äîhas gained a lot of attention.&lt;br /&gt; That said, most demos still rely on clean, API-controlled environments.&lt;/p&gt; &lt;p&gt;To push beyond that, I‚Äôm using Civilization VI, a complex turn-based strategy game, as the testbed.&lt;/p&gt; &lt;p&gt;The agent doesn‚Äôt receive structured game state via MCP alone.&lt;br /&gt; Instead, it reads the screen, interprets the UI, combines that with game data to plan, and controls the game via keyboard and mouse‚Äîlike a human player.&lt;/p&gt; &lt;p&gt;Civ VI involves long-horizon, non-structured decision making across science, culture, diplomacy, and warfare.&lt;br /&gt; Making all of this work using only vision + input actions is a fairly challenging setup.&lt;/p&gt; &lt;p&gt;After one week of experiments, the agent has started to understand the game interface and perform its first meaningful actions.&lt;/p&gt; &lt;p&gt;Can a Computer-Use agent autonomously lead a civilization all the way to prosperity‚Äîand victory?&lt;br /&gt; We‚Äôll see. üëÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Working_Original9624"&gt; /u/Working_Original9624 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pxraikg502hg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqy6f/playing_civilization_vi_with_a_computeruse_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqy6f/playing_civilization_vi_with_a_computeruse_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T09:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtjhc8</id>
    <title>Step-3.5-Flash (196b/A11b) outperforms GLM-4.7 and DeepSeek v3.2</title>
    <updated>2026-02-02T03:07:42+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtjhc8/step35flash_196ba11b_outperforms_glm47_and/"&gt; &lt;img alt="Step-3.5-Flash (196b/A11b) outperforms GLM-4.7 and DeepSeek v3.2" src="https://b.thumbs.redditmedia.com/sBia_JVk3vzY7mBVsXJhMax7j8mOpxC8QNyJrUyazbc.jpg" title="Step-3.5-Flash (196b/A11b) outperforms GLM-4.7 and DeepSeek v3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The newly released Stepfun model Step-3.5-Flash outperforms DeepSeek v3.2 on multiple coding and agentic benchmarks, despite using far fewer parameters.&lt;/p&gt; &lt;p&gt;Step-3.5-Flash: 196B total / 11B active parameters&lt;/p&gt; &lt;p&gt;DeepSeek v3.2: 671B total / 37B active parameters&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/stepfun-ai/Step-3.5-Flash"&gt;https://huggingface.co/stepfun-ai/Step-3.5-Flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qtjhc8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtjhc8/step35flash_196ba11b_outperforms_glm47_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtjhc8/step35flash_196ba11b_outperforms_glm47_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T03:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtqspu</id>
    <title>1 Day Left Until ACE-Step 1.5 ‚Äî Open-Source Music Gen That Runs on &lt;4GB VRAM Open suno alternative (and yes, i made this frontend)</title>
    <updated>2026-02-02T09:47:32+00:00</updated>
    <author>
      <name>/u/ExcellentTrust4433</name>
      <uri>https://old.reddit.com/user/ExcellentTrust4433</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqspu/1_day_left_until_acestep_15_opensource_music_gen/"&gt; &lt;img alt="1 Day Left Until ACE-Step 1.5 ‚Äî Open-Source Music Gen That Runs on &amp;lt;4GB VRAM Open suno alternative (and yes, i made this frontend)" src="https://external-preview.redd.it/dXBiYXJlb295MWhnMYGTlVfp4XddQFbQ7RXlmhemkMaIRdSQh0Jy7FObZ7qD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34c74a399f0ef7e36cb52af6dda02f6967165407" title="1 Day Left Until ACE-Step 1.5 ‚Äî Open-Source Music Gen That Runs on &amp;lt;4GB VRAM Open suno alternative (and yes, i made this frontend)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An open-source model with quality approaching Suno v4.5/v5... running locally on a potato GPU. No subscriptions. No API limits. Just you and your creativity. &lt;/p&gt; &lt;p&gt;We're so lucky to be in this era of open-source AI. A year ago this was unthinkable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExcellentTrust4433"&gt; /u/ExcellentTrust4433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2geqqfooy1hg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqspu/1_day_left_until_acestep_15_opensource_music_gen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqspu/1_day_left_until_acestep_15_opensource_music_gen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T09:47:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qttq5w</id>
    <title>devstral small is faster and better than glm 4.7 flash for local agentic coding.</title>
    <updated>2026-02-02T12:28:47+00:00</updated>
    <author>
      <name>/u/theghost3172</name>
      <uri>https://old.reddit.com/user/theghost3172</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i just realised token per second is not the only thing that matters in agentic coding. glm 4.7 flash is almlst 3x faster but it keeps thinking for way more than 3 times the total tokens it generates so yes at the end devstral small finishes the task slighter faster than glm 4.7 flash. while obiously being much much better at agentic coding.&lt;/p&gt; &lt;p&gt;token efficiency of devstral small has to be discussed more often. its incredble.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theghost3172"&gt; /u/theghost3172 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qttq5w/devstral_small_is_faster_and_better_than_glm_47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qttq5w/devstral_small_is_faster_and_better_than_glm_47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qttq5w/devstral_small_is_faster_and_better_than_glm_47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T12:28:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtu8x1</id>
    <title>GLM 5 Coming Soon</title>
    <updated>2026-02-02T12:54:04+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtu8x1/glm_5_coming_soon/"&gt; &lt;img alt="GLM 5 Coming Soon" src="https://a.thumbs.redditmedia.com/TZGydN7rCAg4Jr3KMn3FCkLWjDYSFSPsrvNKU23imL4.jpg" title="GLM 5 Coming Soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/3i8wkkp8w2hg1.png?width=635&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd400f6ceedc90114cc90feedd2126e2bad951dc"&gt;https://preview.redd.it/3i8wkkp8w2hg1.png?width=635&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd400f6ceedc90114cc90feedd2126e2bad951dc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/jietang/status/2018246490775498791"&gt;https://x.com/jietang/status/2018246490775498791&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtu8x1/glm_5_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtu8x1/glm_5_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtu8x1/glm_5_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T12:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtvo4r</id>
    <title>128GB devices have a new local LLM king: Step-3.5-Flash-int4</title>
    <updated>2026-02-02T13:55:00+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's the HF Repo: &lt;a href="http://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4"&gt;http://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4&lt;/a&gt; (this is a GGUF repo)&lt;/p&gt; &lt;p&gt;I've been running this LLM for about an hour and it has handled all coding tests I've thrown at it in chat mode. IMO this is as good if not better than GLM 4.7, Minimax 2.1 while being much more efficient. Later I will try some agentic coding to see how it performs, but I already have high hopes for it.&lt;/p&gt; &lt;p&gt;I use a 128GB M1 ultra mac studio and can run it at full context (256k). Not only it is fast, but also super efficient in RAM usage.&lt;/p&gt; &lt;p&gt;*Update: I ran llama-bench with up to 100k prefill. Here are the results:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;% llama-bench -m step3p5_flash_Q4_K_S.gguf -fa 1 -t 1 -ngl 99 -b 2048 -ub 2048 -d 0,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000 ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices ggml_metal_library_init: using embedded metal library ggml_metal_library_init: loaded in 0.024 sec ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s) ggml_metal_device_init: GPU name: Apple M1 Ultra ggml_metal_device_init: GPU family: MTLGPUFamilyApple7 (1007) ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003) ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3 (5001) ggml_metal_device_init: simdgroup reduction = true ggml_metal_device_init: simdgroup matrix mul. = true ggml_metal_library_init: using embedded metal library ggml_metal_library_init: loaded in 0.024 sec ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s) ggml_metal_device_init: GPU name: Apple M1 Ultra ggml_metal_device_init: GPU family: MTLGPUFamilyApple7 (1007) ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003) ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3 (5001) ggml_metal_device_init: simdgroup reduction = true ggml_metal_device_init: simdgroup matrix mul. = true ggml_metal_device_init: has unified memory = true ggml_metal_device_init: has bfloat = true ggml_metal_device_init: has tensor = false ggml_metal_device_init: use residency sets = true ggml_metal_device_init: use shared buffers = true ggml_metal_device_init: recommendedMaxWorkingSetSize = 134217.73 MB | model | size | params | backend | threads | n_ubatch | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | ------: | -------: | -: | --------------: | -------------------: | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 | 281.09 ¬± 1.57 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 | 34.70 ¬± 0.01 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d10000 | 248.10 ¬± 1.08 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d10000 | 31.69 ¬± 0.04 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d20000 | 222.18 ¬± 0.49 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d20000 | 30.02 ¬± 0.04 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d30000 | 200.68 ¬± 0.78 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d30000 | 28.62 ¬± 0.02 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d40000 | 182.86 ¬± 0.55 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d40000 | 26.89 ¬± 0.02 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d50000 | 167.61 ¬± 0.23 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d50000 | 25.37 ¬± 0.03 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d60000 | 154.50 ¬± 0.19 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d60000 | 24.10 ¬± 0.01 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d70000 | 143.60 ¬± 0.29 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d70000 | 22.95 ¬± 0.01 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d80000 | 134.02 ¬± 0.35 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d80000 | 21.87 ¬± 0.02 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d90000 | 125.34 ¬± 0.19 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d90000 | 20.66 ¬± 0.02 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | pp512 @ d100000 | 117.72 ¬± 0.07 | | step35 ?B Q4_K - Small | 103.84 GiB | 196.96 B | Metal,BLAS | 1 | 2048 | 1 | tg128 @ d100000 | 19.78 ¬± 0.01 | build: a0dce6f (24) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is still very usable with 100k prefill, so a good option for CLI coding agents!&lt;/p&gt; &lt;p&gt;You need to build a llama.cpp fork to run it, instructions at the HF repo. Though this model is so good that I believe it will soon be supported by llama.cpp upstream.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T13:55:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtvp74</id>
    <title>GLM-5 Coming in February! It's confirmed.</title>
    <updated>2026-02-02T13:56:14+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"&gt; &lt;img alt="GLM-5 Coming in February! It's confirmed." src="https://preview.redd.it/rq0meza173hg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71bbd7ed37e31d92af89abf19ffb4ef0e1d8925a" title="GLM-5 Coming in February! It's confirmed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Twitter Link: &lt;a href="https://x.com/jietang/status/2018246490775498791?s=20"&gt;https://x.com/jietang/status/2018246490775498791?s=20&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rq0meza173hg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T13:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtwqq2</id>
    <title>Unreal</title>
    <updated>2026-02-02T14:37:43+00:00</updated>
    <author>
      <name>/u/analgerianabroad</name>
      <uri>https://old.reddit.com/user/analgerianabroad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtwqq2/unreal/"&gt; &lt;img alt="Unreal" src="https://preview.redd.it/6a90dq5re3hg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55916889b3631b51651d2d49cc3db51ccf6b7cf5" title="Unreal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/analgerianabroad"&gt; /u/analgerianabroad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6a90dq5re3hg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtwqq2/unreal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtwqq2/unreal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T14:37:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
