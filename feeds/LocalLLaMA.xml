<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-19T06:10:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pq7wqc</id>
    <title>For Local LLM RAG ‚Äî 64GB vs 128GB RAM?</title>
    <updated>2025-12-19T01:11:05+00:00</updated>
    <author>
      <name>/u/TeacherIll7604</name>
      <uri>https://old.reddit.com/user/TeacherIll7604</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm planning a local machine mainly for:&lt;/p&gt; &lt;p&gt;- Local LLM experimentation (RAG pipelines, embeddings, indexing)&lt;/p&gt; &lt;p&gt;- Some light fine-tuning / training experiments&lt;/p&gt; &lt;p&gt;- Gaming on the same machine&lt;/p&gt; &lt;p&gt;Planned specs:&lt;/p&gt; &lt;p&gt;- CPU: i9-14900K&lt;/p&gt; &lt;p&gt;- GPU: RTX 4090 (24GB)&lt;/p&gt; &lt;p&gt;- Storage: NVMe SSD&lt;/p&gt; &lt;p&gt;My main question is about system RAM.&lt;/p&gt; &lt;p&gt;Memory prices are going up a lot, so I'm trying to decide between 64GB and 128GB.&lt;/p&gt; &lt;p&gt;1) For local LLM + RAG workflows (vector DB, embeddings, inference), is 64GB realistically enough, or does 128GB make life much easier?&lt;/p&gt; &lt;p&gt;2) With a single RTX 4090 (24GB), what Qwen model sizes would you recommend for practical local use? (7B / 14B / 32B?)&lt;/p&gt; &lt;p&gt;3) Any real-world pain points with 64GB RAM that made you upgrade?&lt;/p&gt; &lt;p&gt;Thanks in advance ‚Äî real-world experience would be really helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeacherIll7604"&gt; /u/TeacherIll7604 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq7wqc/for_local_llm_rag_64gb_vs_128gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq7wqc/for_local_llm_rag_64gb_vs_128gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq7wqc/for_local_llm_rag_64gb_vs_128gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T01:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppongx</id>
    <title>Fast on-device Speech-to-text for Home Assistant (open source)</title>
    <updated>2025-12-18T11:34:57+00:00</updated>
    <author>
      <name>/u/banafo</name>
      <uri>https://old.reddit.com/user/banafo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"&gt; &lt;img alt="Fast on-device Speech-to-text for Home Assistant (open source)" src="https://external-preview.redd.it/6PRNLd3TFMw1DCfYP7618_nVHzwQRPRrDRjMqQg7XGU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cff7a166c2a85ced6d24604f32dc307cf599fedf" title="Fast on-device Speech-to-text for Home Assistant (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just released &lt;a href="https://github.com/orgs/kroko-ai/repositories"&gt;kroko-onnx-home-assistant &lt;/a&gt; is a &lt;strong&gt;local&lt;/strong&gt; streaming STT pipeline for home assistant.&lt;/p&gt; &lt;p&gt;It's currently just a fork of the excellent &lt;a href="https://github.com/ptbsare/sherpa-onnx-tts-stt"&gt;https://github.com/ptbsare/sherpa-onnx-tts-stt&lt;/a&gt; with support for our models added, hopefully it will be accepted in the main project. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;High quality&lt;/li&gt; &lt;li&gt;Real streaming (partial results, low latency)&lt;/li&gt; &lt;li&gt;100% local &amp;amp; privacy-first&lt;/li&gt; &lt;li&gt;optimized for fast CPU inference, even in low resources raspberry pi's&lt;/li&gt; &lt;li&gt;Does not require additional VAD&lt;/li&gt; &lt;li&gt;Home Assistant integration&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo:&lt;br /&gt; [&lt;a href="https://github.com/kroko-ai/kroko-onnx-home-assistant%5D()"&gt;https://github.com/kroko-ai/kroko-onnx-home-assistant]()&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to test the model quality before installing: the huggingface models running in the browser is the easiest way: &lt;a href="https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm"&gt;https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A big thanks to:&lt;br /&gt; - NaggingDaivy on discord, for the assistance.&lt;br /&gt; - the sherpa-onnx-tts-stt team for adding support for streaming models in record time.&lt;/p&gt; &lt;p&gt;Want us to integrate with your favorite open source project ? Contact us on discord:&lt;br /&gt; &lt;a href="https://discord.gg/TEbfnC7b"&gt;https://discord.gg/TEbfnC7b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some releases you may have missed:&lt;br /&gt; - Freewitch Module: &lt;a href="https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko"&gt;https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko&lt;/a&gt;&lt;br /&gt; - Asterisk Module: &lt;a href="https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko"&gt;https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko&lt;/a&gt;&lt;br /&gt; - Full Asterisk based voicebot running with Kroko streaming models: &lt;a href="https://github.com/hkjarral/Asterisk-AI-Voice-Agent"&gt;https://github.com/hkjarral/Asterisk-AI-Voice-Agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are still working on the main models, code and documentation as well, but held up a bit with urgent paid work deadlines, more coming there soon too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/banafo"&gt; /u/banafo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kroko-ai/kroko-onnx-home-assistant"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T11:34:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppv68d</id>
    <title>[Blog from Hugging Face] Tokenization in Transformers v5: Simpler, Clearer, and More Modular</title>
    <updated>2025-12-18T16:30:13+00:00</updated>
    <author>
      <name>/u/Disastrous-Work-1632</name>
      <uri>https://old.reddit.com/user/Disastrous-Work-1632</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppv68d/blog_from_hugging_face_tokenization_in/"&gt; &lt;img alt="[Blog from Hugging Face] Tokenization in Transformers v5: Simpler, Clearer, and More Modular" src="https://preview.redd.it/ggovkfrtoz7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d79c723d2ee425a8d0fe89be6ee4871ab9baba7b" title="[Blog from Hugging Face] Tokenization in Transformers v5: Simpler, Clearer, and More Modular" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This blog explains how tokenization works in Transformers and why v5 is a major redesign, with clearer internals, a clean class hierarchy, and a single fast backend. It‚Äôs a practical guide for anyone who wants to understand, customize, or train model-specific tokenizers instead of treating them as black boxes.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/blog/tokenizers"&gt;https://huggingface.co/blog/tokenizers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Work-1632"&gt; /u/Disastrous-Work-1632 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ggovkfrtoz7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppv68d/blog_from_hugging_face_tokenization_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppv68d/blog_from_hugging_face_tokenization_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T16:30:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqcra7</id>
    <title>Can an ASUS Hyper M.2 x16 Gen5 NVMe RAID be used as a RAM replacement or ultra-fast memory tier for GPU workloads?</title>
    <updated>2025-12-19T05:12:49+00:00</updated>
    <author>
      <name>/u/khoi_khoi123</name>
      <uri>https://old.reddit.com/user/khoi_khoi123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqcra7/can_an_asus_hyper_m2_x16_gen5_nvme_raid_be_used/"&gt; &lt;img alt="Can an ASUS Hyper M.2 x16 Gen5 NVMe RAID be used as a RAM replacement or ultra-fast memory tier for GPU workloads?" src="https://a.thumbs.redditmedia.com/HFZ5g_89dZgq3p10AfhaZHWW7e2-6NFqOtmQa6qnD-8.jpg" title="Can an ASUS Hyper M.2 x16 Gen5 NVMe RAID be used as a RAM replacement or ultra-fast memory tier for GPU workloads?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/hp8yytq5h38g1.png?width=1495&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef36abeaa84106c71195fb0682612bf9cf7b8e71"&gt;https://preview.redd.it/hp8yytq5h38g1.png?width=1495&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef36abeaa84106c71195fb0682612bf9cf7b8e71&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm exploring whether extremely fast NVMe storage can act as a substitute for system RAM in high-throughput GPU workloads.&lt;/p&gt; &lt;p&gt;Specifically, I‚Äôm looking at the &lt;strong&gt;ASUS Hyper M.2 x16 Gen5 card&lt;/strong&gt;, which can host &lt;strong&gt;4√ó NVMe Gen5 SSDs&lt;/strong&gt; in &lt;strong&gt;RAID 0&lt;/strong&gt;, theoretically delivering &lt;strong&gt;40‚Äì60 GB/s sequential throughput&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;My question is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Can this setup realistically be used as a &lt;em&gt;RAM replacement&lt;/em&gt; or an &lt;em&gt;ultra-fast memory tier&lt;/em&gt;?&lt;/li&gt; &lt;li&gt;In scenarios where &lt;strong&gt;data does NOT fit in VRAM&lt;/strong&gt; and must be &lt;strong&gt;continuously streamed to the GPU&lt;/strong&gt;, would NVMe RAID over PCIe Gen5 meaningfully reduce bottlenecks?&lt;/li&gt; &lt;li&gt;How does this compare to: &lt;ul&gt; &lt;li&gt;System RAM (DDR5)&lt;/li&gt; &lt;li&gt;PCIe-native GPU access&lt;/li&gt; &lt;li&gt;eGPU over Thunderbolt 4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Is the limitation mainly &lt;strong&gt;latency&lt;/strong&gt;, &lt;strong&gt;PCIe transaction overhead&lt;/strong&gt;, or &lt;strong&gt;CPU/GPU memory architecture&lt;/strong&gt;?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm especially interested in perspectives related to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AI / LLM inference&lt;/li&gt; &lt;li&gt;Streaming large batches to GPU&lt;/li&gt; &lt;li&gt;Memory-mapped files, Unified Memory, or swap-on-NVMe tricks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At what point (if any) does ultra-fast NVMe stop being ‚Äústorage‚Äù and start behaving like ‚Äúmemory‚Äù for real-world GPU workloads?&lt;/p&gt; &lt;p&gt;Thanks in advance ‚Äî looking forward to a deep technical discussion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khoi_khoi123"&gt; /u/khoi_khoi123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqcra7/can_an_asus_hyper_m2_x16_gen5_nvme_raid_be_used/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqcra7/can_an_asus_hyper_m2_x16_gen5_nvme_raid_be_used/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqcra7/can_an_asus_hyper_m2_x16_gen5_nvme_raid_be_used/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T05:12:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppu35l</id>
    <title>Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting.</title>
    <updated>2025-12-18T15:47:20+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"&gt; &lt;img alt="Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting." src="https://b.thumbs.redditmedia.com/CTdnDnhEVXKhvcC_xn7Fo04JbVfjTe3Wx_yk_R9kVRw.jpg" title="Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://mistral.ai/news/mistral-ocr-3"&gt;https://mistral.ai/news/mistral-ocr-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mistral OCR 3 sets new benchmarks in both accuracy and efficiency, outperforming enterprise document processing solutions as well as AI-native OCR.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ppu35l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:47:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppstef</id>
    <title>Thoughts on recent small (under 20B) models</title>
    <updated>2025-12-18T14:55:45+00:00</updated>
    <author>
      <name>/u/surubel</name>
      <uri>https://old.reddit.com/user/surubel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently we're been graced with quite a few small (under 20B) models and I've tried most of them.&lt;/p&gt; &lt;p&gt;The initial benchmarks seemed a bit too good to be true, but I've tried them regardless. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;RNJ-1: this one had probably the most &amp;quot;honest&amp;quot; benchmark results. About as good as QWEN3 8B, which seems fair from my limited usage. &lt;/li&gt; &lt;li&gt;GLM 4.6v Flash: even after the latest llama.cpp update and Unsloth quantization I still have mixed feelings. Can't get it to think in English, but produces decent results. Either there are still issues with llama.cpp / quantization or it's a bit benchmaxxed&lt;/li&gt; &lt;li&gt;Ministral 3 14B: solid vision capabilities, but tends to overthink a lot. Occasionally messes up tool calls. A bit unreliable.&lt;/li&gt; &lt;li&gt;Nemotron cascade 14B. Similar to Ministral 3 14B tends to overthink a lot. Although it has great coding benchmarks, I couldn't get good results out of it. GPT OSS 20B and QWEN3 8B VL seem to give better results. This was the most underwhelming for me.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Did anyone get different results from these models? Am I missing something?&lt;/p&gt; &lt;p&gt;Seems like GPT OSS 20B and QWEN3 8B VL are still the most reliable small models, at least for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/surubel"&gt; /u/surubel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T14:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqd7sy</id>
    <title>I've been experimenting with SLM's a lot recently. My goal was to prove even SLMs can be accurate with the right architecture behind it.</title>
    <updated>2025-12-19T05:37:51+00:00</updated>
    <author>
      <name>/u/Little-Put6364</name>
      <uri>https://old.reddit.com/user/Little-Put6364</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqd7sy/ive_been_experimenting_with_slms_a_lot_recently/"&gt; &lt;img alt="I've been experimenting with SLM's a lot recently. My goal was to prove even SLMs can be accurate with the right architecture behind it." src="https://external-preview.redd.it/ejdpemFoZnloMzhnMWGEd-zNYE7e7CLbHm_rOf9Rp-W6GE7TweEbQZSklBMz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=054b952c38361bd74bb25ed593bcaebaa995ebda" title="I've been experimenting with SLM's a lot recently. My goal was to prove even SLMs can be accurate with the right architecture behind it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even though it looks simple. This thing has quite the process behind it. I am using Godot Mono, with LLamaSharp (llama.cpp under the hood) for inferencing. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;I start with Phi-3.5 mini. It rewrites the users query into 4 alternative queries&lt;/li&gt; &lt;li&gt;I take those queries and use Qwen 3 embedding model to pull back vector db results for each one&lt;/li&gt; &lt;li&gt;I then dedupe and run a reranking algorithm to limit the results down to around 10 'hits'&lt;/li&gt; &lt;li&gt;Next up is taking the hits and expanding it to include neighboring 'chunks' in the document&lt;/li&gt; &lt;li&gt;Then I format the chunks neatly&lt;/li&gt; &lt;li&gt;Then I pass the context and user's prompt to Qwen 8B with thinking active for it to answer the users question.&lt;/li&gt; &lt;li&gt;Finally the output is sent back to Phi-3.5 mini to 'extract' the answer out of the thinking model's response and format it for the UI. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There's a lot of checks and looping going on in the background too. Lots of juggling with chat history. But by using these small models, it runs very quickly on VRAM. Because the models are small I can just load and unload per request without the load times being crazy. &lt;/p&gt; &lt;p&gt;I won't say this is perfect. And I haven't taken this process and ran it against any benchmarks. But it's honestly gone ALOT better than I ever anticipated. The quality could even improve more when I implement a &amp;quot;Deep Think&amp;quot; mode next. Which will basically just be an agent setup to loop and pull in more relevant context. &lt;/p&gt; &lt;p&gt;But if there's anything I've learned throughout this process...It's that even small language models can answer questions reliably. As long as you give proper context. Context engineering is the most important piece of the pie. We don't need these 300B plus models for most AI needs.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Offloom is just the name I gave my proof of concept. This thing isn't on the market, and probably never will be. It's my own personal playground for proving out concepts. I enjoy making things look nice. Even for POCs.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Little-Put6364"&gt; /u/Little-Put6364 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h85i48fyh38g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqd7sy/ive_been_experimenting_with_slms_a_lot_recently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqd7sy/ive_been_experimenting_with_slms_a_lot_recently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T05:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwylg</id>
    <title>What's your favourite local coding model?</title>
    <updated>2025-12-18T17:40:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwylg/whats_your_favourite_local_coding_model/"&gt; &lt;img alt="What's your favourite local coding model?" src="https://preview.redd.it/q8ipunvr008g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4e54f5d47898a4570fb732cd3140edf2551267b" title="What's your favourite local coding model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried (with Mistral Vibe Cli)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;mistralai_Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf - works but it's kind of slow for coding &lt;/li&gt; &lt;li&gt;nvidia_Nemotron-3-Nano-30B-A3B-Q8_0.gguf - text generation is fast, but the actual coding is slow and often incorrect&lt;/li&gt; &lt;li&gt;Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf - works correctly and it's fast&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What else would you recommend? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q8ipunvr008g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwylg/whats_your_favourite_local_coding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwylg/whats_your_favourite_local_coding_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqcdvy</id>
    <title>üìå Day 11: 21 Days of Building a Small Language Model: Multi Query Attentionüìå</title>
    <updated>2025-12-19T04:53:45+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqcdvy/day_11_21_days_of_building_a_small_language_model/"&gt; &lt;img alt="üìå Day 11: 21 Days of Building a Small Language Model: Multi Query Attentionüìå" src="https://b.thumbs.redditmedia.com/rpEeI_2phyvuFOjQ_RyUBGcHa5maF0c_zm-GX_gSvbE.jpg" title="üìå Day 11: 21 Days of Building a Small Language Model: Multi Query Attentionüìå" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to Day 11 of 21 Days of Building a Small Language Model. The topic for today is Multi-Query Attention. Yesterday, we explored the KV cache and saw how it dramatically speeds up inference but creates massive memory requirements. Today, we'll discover how Multi-Query Attention solves the memory problem by asking a simple question: Do we really need separate keys and values for every attention head?&lt;/p&gt; &lt;h1&gt;Problem&lt;/h1&gt; &lt;p&gt;Yesterday we learned that the KV cache requires storing keys and values for every layer, every head, and every token. The memory formula looks straightforward, but when you plug in real numbers from production models, the KV cache alone can consume hundreds of gigabytes.&lt;/p&gt; &lt;p&gt;The memory grows linearly with sequence length and linearly with the number of heads. This creates serious problems: inference slows down, long context windows become expensive, serving costs increase dramatically, GPUs hit memory limits, and you can't batch many users together.&lt;/p&gt; &lt;p&gt;Consider a model with 32 attention heads. With standard multi head attention, you store 32 separate sets of keys and values in the KV cache. That's 32 times the memory requirement just for the cache.&lt;/p&gt; &lt;p&gt;This raises a fundamental question: do we really need a separate key and value tensor for every attention head? This question leads us directly to Multi Query Attention, one of the simplest yet most impactful innovations in large language model inference.&lt;/p&gt; &lt;h1&gt;Core&lt;/h1&gt; &lt;p&gt;In classical multi head attention, every head maintains its own separate projections. Each head has its own query projection, its own key projection, and its own value projection. If you have H heads in your model, you end up with Q1, K1, V1 for the first head, Q2, K2, V2 for the second head, and so on up to QH, KH, VH for the H th head.&lt;/p&gt; &lt;p&gt;When researchers at Google were developing more efficient transformer architectures, they made a fascinating observation: while queries need to be separate per head to maintain the diversity of attention patterns, keys and values don't necessarily need to be.&lt;/p&gt; &lt;p&gt;This insight became the foundation of Multi Query Attention. The key realization is that most of the diversity in attention patterns comes from the different queries, not from the keys and values. The query controls what the model is looking for, while keys and values mostly represent what the sequence contains.&lt;/p&gt; &lt;p&gt;Minimize image&lt;/p&gt; &lt;p&gt;Edit image&lt;/p&gt; &lt;p&gt;Delete image&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kaw9g6xbd38g1.png?width=1834&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=03039eb8f157610523f6e6fc2d0c39ec964f5493"&gt;Ref: Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;How Multi-Query Attention works&lt;/h1&gt; &lt;p&gt;Multi Query Attention keeps multiple queries but shares keys and values across all heads. In MQA, you still have H query heads: Q1, Q2, and so on up to QH. But you now have only one key projection and one value projection: K_shared and V_shared.&lt;/p&gt; &lt;p&gt;Visually, standard multi head attention has Head 1 with Q1, K1, V1, Head 2 with Q2, K2, V2, Head 3 with Q3, K3, V3, Head 4 with Q4, K4, V4, and so on. Multi Query Attention has Head 1 with Q1, Head 2 with Q2, Head 3 with Q3, Head 4 with Q4, and so on, with all heads sharing K_shared and V_shared.&lt;/p&gt; &lt;p&gt;The number of keys reduces from H to 1, and the number of values reduces from H to 1. That is a massive reduction.&lt;/p&gt; &lt;h1&gt;Memory Savings&lt;/h1&gt; &lt;p&gt;Let's compute the KV cache size before and after with the help of an examples. The general memory formula for the KV cache is:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Size of KV cache = l*b*n*h*s*2*2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Where:&lt;/p&gt; &lt;p&gt;‚Ä¢ l = number of transformer blocks (layers)&lt;/p&gt; &lt;p&gt;‚Ä¢ b = batch size ‚Ä¢ n = number of attention heads (or number of K/V sets)&lt;/p&gt; &lt;p&gt;‚Ä¢ h = attention head size&lt;/p&gt; &lt;p&gt;‚Ä¢ s = context length&lt;/p&gt; &lt;p&gt;‚Ä¢ First 2 = number of caches per transformer block (K, V)&lt;/p&gt; &lt;p&gt;‚Ä¢ Second 2 = bytes per parameter (FP16 uses 2 bytes)&lt;/p&gt; &lt;p&gt;For standard multi head attention, the number of K/V sets equals the number of heads (H), so:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Size of KV cache (MHA) = l*b*H*h*s*2*2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For Multi Query Attention, the number of K/V sets is 1 (all heads share one key and one value projection), so:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Size of KV cache (MQA) = l*b*1*h*s*2*2 = l*b*h*s*2*2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The memory savings factor is:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Memory Savings Factor = Size (MHA) / Size (MQA) = (l*b*H*h*s*2*2) / (l*b*h*s*2*2) = H &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This means MQA reduces the KV cache size by a factor of H, where H is the number of attention heads.&lt;/p&gt; &lt;h1&gt;For example 1&lt;/h1&gt; &lt;p&gt;Consider a model with 32 attention heads, a head dimension of 128, 32 layers, and a sequence length of 8,192 tokens, using FP16 precision with batch size 1.&lt;/p&gt; &lt;p&gt;Before, with standard multi head attention:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Size of KV cache (MHA) = l*b*H*h*s*2*2 = 32*1*32*128*8192*2*2 = 4,294,967,296 bytes ‚âà 4 GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After, with Multi Query Attention:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Size of KV cache (MQA) = l*b*h*s*2*2 = 32*1*128*8192*2*2 = 134,217,728 bytes ‚âà 128 MB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This represents a 32 times reduction in KV cache memory. The total KV cache memory drops from approximately 4 gigabytes to approximately 128 megabytes. This massive reduction makes long context windows practical and dramatically reduces serving costs.&lt;/p&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;p&gt;Remember the purpose of multi head attention: each head is designed to capture different perspectives of the input sequence. In a well trained model with full multi head attention, different heads learn to specialize in different aspects of language understanding. One head might focus on tracking named entities, another might capture syntactic relationships, another might identify long range dependencies, and another might recognize stylistic patterns. This diversity of perspectives is what makes multi head attention powerful.&lt;/p&gt; &lt;p&gt;Multi Query Attention breaks this design principle. The limitations include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Reduced diversity of perspectives&lt;/strong&gt;: By forcing all heads to share the same key and value projections during inference, all heads are forced to look at the same representation of the input. While each head still has its own query projection, which allows heads to ask different questions, they're all asking those questions about the same underlying information.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Single bottleneck constraint&lt;/strong&gt;: The entire attention mechanism is constrained by a single key and value space, reducing the diversity of perspectives that multi head attention is designed to provide. This creates a bottleneck that limits the model's ability to simultaneously process multiple different aspects of the input.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Impact on complex reasoning tasks&lt;/strong&gt;: The model loses some of its ability to simultaneously track multiple different linguistic signals, which can be particularly problematic for reasoning heavy tasks that require the model to maintain and integrate multiple different types of information.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is why Multi Query Attention is primarily used as an inference time optimization. Models are trained with full multi head attention to learn rich, diverse attention patterns, and then MQA is applied during inference to reduce KV cache memory. This approach gets the best of both worlds: the rich representational power of multi head attention during training, and the memory efficiency of MQA during inference.&lt;/p&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;Today we discovered Multi Query Attention, one of the simplest yet most impactful optimizations in large language models. The core idea is elegant: share keys and values across all heads while keeping queries separate. This simple change reduces KV cache memory by a factor equal to the number of heads.&lt;/p&gt; &lt;p&gt;For a model with 32 heads, that's a 32 times reduction. However, the optimization comes with tradeoffs. By sharing keys and values, we reduce the diversity of perspectives that multi head attention provides. This is why MQA works best as an inference time optimization, applied to models that were trained with full multi head attention.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqcdvy/day_11_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqcdvy/day_11_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqcdvy/day_11_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T04:53:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq45po</id>
    <title>New AI Dungeon Model: Hearthfire 24B</title>
    <updated>2025-12-18T22:24:22+00:00</updated>
    <author>
      <name>/u/NottKolby</name>
      <uri>https://old.reddit.com/user/NottKolby</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today AI Dungeon open sourced a new narrative roleplay model!&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LatitudeGames/Hearthfire-24B"&gt;Hearthfire 24B&lt;/a&gt;&lt;/h1&gt; &lt;blockquote&gt; &lt;p&gt;Hearthfire is our new Mistral Small 3.2 finetune, and it's the lo-fi hip hop beats of AI storytelling. Built for slice-of-life moments, atmospheric scenes, and narratives where the stakes are personal rather than apocalyptic. It won't rush you toward the next plot point. It's happy to linger.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NottKolby"&gt; /u/NottKolby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq45po/new_ai_dungeon_model_hearthfire_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq45po/new_ai_dungeon_model_hearthfire_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq45po/new_ai_dungeon_model_hearthfire_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T22:24:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppu4lc</id>
    <title>Fine-tuning Qwen3 at home to respond to any prompt with a dad joke</title>
    <updated>2025-12-18T15:48:58+00:00</updated>
    <author>
      <name>/u/InvadersMustLive</name>
      <uri>https://old.reddit.com/user/InvadersMustLive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"&gt; &lt;img alt="Fine-tuning Qwen3 at home to respond to any prompt with a dad joke" src="https://external-preview.redd.it/aeJXUJD-EG13fwr7w155noLxr7JTSfAKwf9XG0w-u3s.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9431c18c37e750b69f2ab16532111dd97d789f41" title="Fine-tuning Qwen3 at home to respond to any prompt with a dad joke" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InvadersMustLive"&gt; /u/InvadersMustLive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nixiesearch.substack.com/p/fine-tuning-qwen3-at-home-to-respond"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwfw3</id>
    <title>Key Highlights of Google's New Open Model, FunctionGemma</title>
    <updated>2025-12-18T17:19:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwfw3/key_highlights_of_googles_new_open_model/"&gt; &lt;img alt="Key Highlights of Google's New Open Model, FunctionGemma" src="https://external-preview.redd.it/f3OilJIGGaBNRWiWULRSz5XOCY6YipQN2XKt886yVr0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0508cadd0c3629606d28322469362c690c52148b" title="Key Highlights of Google's New Open Model, FunctionGemma" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;[1] Function-calling specialized&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Built on the &lt;em&gt;Gemma 3 270M&lt;/em&gt; foundation and fine-tuned for function calling tasks, turning natural language into structured function calls for API/tool execution.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[2] Lightweight &amp;amp; open&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;A compact, open-weight model (~270 M parameters) designed for efficient use on resource-constrained hardware (laptops, desktops, cloud, edge) and democratizing access to advanced function-call agents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[3] 32K token context&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports up to ~32 k token context window, like other 270M Gemma models, making it suitable for moderately long prompts and complex sequences.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[4] Fine-tuning friendly&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intended to be further fine-tuned for specific custom actions, improving accuracy and customization for particular domains or workflows (e.g., mobile actions, custom APIs).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model - &lt;a href="https://huggingface.co/google/functiongemma-270m-it"&gt;https://huggingface.co/google/functiongemma-270m-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model GGUF - &lt;a href="https://huggingface.co/unsloth/functiongemma-270m-it-GGUF"&gt;https://huggingface.co/unsloth/functiongemma-270m-it-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/google/functiongemma-270m-it"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwfw3/key_highlights_of_googles_new_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwfw3/key_highlights_of_googles_new_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq6h6b</id>
    <title>T5 Gemma Text to Speech</title>
    <updated>2025-12-19T00:04:46+00:00</updated>
    <author>
      <name>/u/ObjectiveOctopus2</name>
      <uri>https://old.reddit.com/user/ObjectiveOctopus2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq6h6b/t5_gemma_text_to_speech/"&gt; &lt;img alt="T5 Gemma Text to Speech" src="https://external-preview.redd.it/OoYCpcn_PwfmbZl7Fy8iEFdYUosTf1a8HGTxpebEBLY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be91b4470a2fc8d77abd575b339b2d41dce4c231" title="T5 Gemma Text to Speech" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;T5Gemma-TTS-2b-2b is a multilingual Text-to-Speech (TTS) model. It utilizes an Encoder-Decoder LLM architecture, supporting English, Chinese, and Japanese. And its üî•&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ObjectiveOctopus2"&gt; /u/ObjectiveOctopus2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Aratako/T5Gemma-TTS-2b-2b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq6h6b/t5_gemma_text_to_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq6h6b/t5_gemma_text_to_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T00:04:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppzof4</id>
    <title>LatitudeGames/Hearthfire-24B ¬∑ Hugging Face</title>
    <updated>2025-12-18T19:24:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzof4/latitudegameshearthfire24b_hugging_face/"&gt; &lt;img alt="LatitudeGames/Hearthfire-24B ¬∑ Hugging Face" src="https://external-preview.redd.it/A3gGg_h4D053EFPLZSslW4oGkfGx4Yyo44cLXCFOpgw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af804b5dc2799b163e1ddae03ccbee1392cf7d39" title="LatitudeGames/Hearthfire-24B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hearthfire is a narrative longform writing model designed to embrace the quiet moments between the chaos. While most roleplay models are trained to relentlessly drive the plot forward with high-stakes action and constant external pressure, Hearthfire is tuned to appreciate atmosphere, introspection, and the slow burn of a scene.&lt;/p&gt; &lt;p&gt;It prioritizes vibes over velocity. It is comfortable with silence. It will not force a goblin attack just because the conversation lulled.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LatitudeGames/Hearthfire-24B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzof4/latitudegameshearthfire24b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzof4/latitudegameshearthfire24b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T19:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppt1xb</id>
    <title>Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction</title>
    <updated>2025-12-18T15:05:22+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"&gt; &lt;img alt="Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction" src="https://preview.redd.it/go7lager9z7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96b4f63fa1cdd2136e6c82f35c609cc6cc1ead9c" title="Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/facebook/map-anything-v1"&gt;https://huggingface.co/facebook/map-anything-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It supports 12+ tasks like multi-view stereo and SfM in a single feed-forward pass&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/go7lager9z7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:05:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqala0</id>
    <title>MBZUAI releases K2-V2 - 70B fully open model.</title>
    <updated>2025-12-19T03:20:39+00:00</updated>
    <author>
      <name>/u/LoveMind_AI</name>
      <uri>https://old.reddit.com/user/LoveMind_AI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Holy frijoles. Has anyone given this a look? Fully open like Olmo 3, but a solid 70B of performance. I‚Äôm not sure why I‚Äôm just hearing about it, but, definitely looking forward to seeing how folks receive it!&lt;/p&gt; &lt;p&gt;&lt;a href="https://mbzuai.ac.ae/news/k2v2-full-openness-finally-meets-real-performance/"&gt;https://mbzuai.ac.ae/news/k2v2-full-openness-finally-meets-real-performance/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(I searched for other posts on this but didn‚Äôt see anything - let me know if I missed a thread!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoveMind_AI"&gt; /u/LoveMind_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqala0/mbzuai_releases_k2v2_70b_fully_open_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqala0/mbzuai_releases_k2v2_70b_fully_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqala0/mbzuai_releases_k2v2_70b_fully_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T03:20:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq2uvi</id>
    <title>192GB VRAM 8x 3090s + 512GB DDR4 RAM AMA</title>
    <updated>2025-12-18T21:31:29+00:00</updated>
    <author>
      <name>/u/Sero_x</name>
      <uri>https://old.reddit.com/user/Sero_x</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"&gt; &lt;img alt="192GB VRAM 8x 3090s + 512GB DDR4 RAM AMA" src="https://b.thumbs.redditmedia.com/rqYvfP2xSe7ILLKpKsQzha57H6-7i7Cnwe-N3-UA3RM.jpg" title="192GB VRAM 8x 3090s + 512GB DDR4 RAM AMA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ft7xpejo618g1.jpg?width=1013&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eef45da10a0cc8b74000c8d586d9f442865a39ab"&gt;https://preview.redd.it/ft7xpejo618g1.jpg?width=1013&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eef45da10a0cc8b74000c8d586d9f442865a39ab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I bought and built this 3 months ago, I started with 4x 3090s and really loved the process so got another 4x 3090s&lt;/p&gt; &lt;p&gt;Now I‚Äôm convinced I need double the VRAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sero_x"&gt; /u/Sero_x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T21:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwqki</id>
    <title>FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!</title>
    <updated>2025-12-18T17:31:14+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"&gt; &lt;img alt="FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!" src="https://external-preview.redd.it/MXBiZjQzZTd4ejdnMYei2aDWEA5WccTd6X2Ceg7tONZcTZmqT6GgxYYEX2jv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c19158fb01b0628ef68c006e673dc09cd2cf081" title="FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Google released FunctionGemma, a lightweight (270M), open foundation model built for creating specialized function calling models! To test it out, I built a small game where you use natural language to solve physics simulation puzzles. It runs entirely locally in your browser on WebGPU, powered by Transformers.js.&lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; - Game: &lt;a href="https://huggingface.co/spaces/webml-community/FunctionGemma-Physics-Playground"&gt;https://huggingface.co/spaces/webml-community/FunctionGemma-Physics-Playground&lt;/a&gt;&lt;br /&gt; - FunctionGemma on Hugging Face: &lt;a href="https://huggingface.co/google/functiongemma-270m-it"&gt;https://huggingface.co/google/functiongemma-270m-it&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k33t7zd7xz7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:31:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq2rx7</id>
    <title>Exo 1.0 is finally out</title>
    <updated>2025-12-18T21:28:19+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2rx7/exo_10_is_finally_out/"&gt; &lt;img alt="Exo 1.0 is finally out" src="https://preview.redd.it/zxmsw724618g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=912f00f6d6f4874ab451714c731bec0bbc5a59be" title="Exo 1.0 is finally out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can download from &lt;a href="https://exolabs.net/"&gt;https://exolabs.net/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zxmsw724618g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2rx7/exo_10_is_finally_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2rx7/exo_10_is_finally_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T21:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppzhtq</id>
    <title>T5Gemma 2: The next generation of encoder-decoder models</title>
    <updated>2025-12-18T19:17:53+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"&gt; &lt;img alt="T5Gemma 2: The next generation of encoder-decoder models" src="https://external-preview.redd.it/_rnSBYMvSInq6EN43nG_cTgBC4Jp6XTPNyUPRgnGKn0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9dbe7f224d36b036fe98650042395413b48e5a4" title="T5Gemma 2: The next generation of encoder-decoder models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;T5Gemma 2 models, based on Gemma 3, are multilingual and multimodal, handling text and image input and generating text output, with open weights for three pretrained sizes (270M-270M, 1B-1B, and 4B-4B).&lt;/p&gt; &lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tied embeddings:&lt;/strong&gt; Embeddings are tied between the encoder and decoder. This significantly reduces the overall parameter count and allowing to pack more active capabilities into the same memory footprint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Merged attention:&lt;/strong&gt; The decoder uses a merged attention mechanism, combining self- and cross-attention into a single, unified attention layer. This reduces model parameters and architectural complexity, improving model parallelization and benefiting inference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodality:&lt;/strong&gt; T5Gemma 2 models can understand and process images alongside text. By utilizing a highly efficient vision encoder, the models can seamlessly perform visual question answering and multimodal reasoning tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extended long context:&lt;/strong&gt; Leveraging Gemma 3's alternating local and global attention mechanism, T5Gemma 2 can handle context windows of up to 128K tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Massively multilingual:&lt;/strong&gt; Trained on a larger, more diverse dataset, these models now support over 140 languages out of the box.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Models - &lt;a href="https://huggingface.co/collections/google/t5gemma-2"&gt;https://huggingface.co/collections/google/t5gemma-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Official Blog post - &lt;a href="https://blog.google/technology/developers/t5gemma-2/"&gt;https://blog.google/technology/developers/t5gemma-2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/t5gemma-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T19:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq5k6e</id>
    <title>Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios</title>
    <updated>2025-12-18T23:23:44+00:00</updated>
    <author>
      <name>/u/Competitive_Travel16</name>
      <uri>https://old.reddit.com/user/Competitive_Travel16</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"&gt; &lt;img alt="Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios" src="https://external-preview.redd.it/A_KZLQUNhCh0wGe2hwjJCJ470X6QmuVpXZdzOWccb0U.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fdf14dca65c42b501a6a7e33b1acf44e71ac72f" title="Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive_Travel16"&gt; /u/Competitive_Travel16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=4l4UWZGxvoc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T23:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppun3v</id>
    <title>Google's Gemma models family</title>
    <updated>2025-12-18T16:09:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"&gt; &lt;img alt="Google's Gemma models family" src="https://preview.redd.it/59w0vja4lz7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dd2d66ee23d4078bf31aba81cdeecc769669af4" title="Google's Gemma models family" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/59w0vja4lz7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T16:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq2ry0</id>
    <title>Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster</title>
    <updated>2025-12-18T21:28:20+00:00</updated>
    <author>
      <name>/u/geerlingguy</name>
      <uri>https://old.reddit.com/user/geerlingguy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"&gt; &lt;img alt="Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster" src="https://preview.redd.it/32z50w1s518g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be2781529b5cacb7d7a84c794d37a156e1bdc798" title="Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing llama.cpp RPC vs Exo's new RDMA Tensor setting on a cluster of 4x Mac Studios (2x 512GB and 2x 256GB) that Apple loaned me until Februrary.&lt;/p&gt; &lt;p&gt;Would love to do more testing between now and returning it. A lot of the earlier testing was debugging stuff since the RDMA support was very new for the past few weeks... now that it's somewhat stable I can do more.&lt;/p&gt; &lt;p&gt;The annoying thing is there's nothing nice like llama-bench in Exo, so I can't give as direct comparisons with context sizes, prompt processing speeds, etc. (it takes a lot more fuss to do that, at least).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geerlingguy"&gt; /u/geerlingguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/32z50w1s518g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T21:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We‚Äôre the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We‚Äôre excited to be here to talk all things SAM (sorry, we can‚Äôt share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We‚Äôll be answering questions live on Thursday, Dec. 18, from 2-3pm PT. Hope to see you there.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
</feed>
