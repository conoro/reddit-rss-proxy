<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-07T04:38:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oqe1kq</id>
    <title>No negative impact using Oculink eGPU: A quick test.</title>
    <updated>2025-11-06T23:08:48+00:00</updated>
    <author>
      <name>/u/MexInAbu</name>
      <uri>https://old.reddit.com/user/MexInAbu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I have seen mixed information about the impact of using oculink for our local LLM projects. Well, just today I connected an RTX 3090 through oculink to my RTX A6000 SFF PC and I have some llama.cpp benchmarks using gemma3 27B Q8:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;th align="left"&gt;gpu_config&lt;/th&gt; &lt;th align="left"&gt;devices&lt;/th&gt; &lt;th align="left"&gt;build&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q8_0&lt;/td&gt; &lt;td align="left"&gt;26.73 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;1396.93&lt;/td&gt; &lt;td align="left"&gt;1Ã— RTX A6000&lt;/td&gt; &lt;td align="left"&gt;CUDA_VISIBLE_DEVICES=0&lt;/td&gt; &lt;td align="left"&gt;7f09a680a (6970)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q8_0&lt;/td&gt; &lt;td align="left"&gt;26.73 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;1341.08&lt;/td&gt; &lt;td align="left"&gt;1Ã— RTX A6000&lt;/td&gt; &lt;td align="left"&gt;CUDA_VISIBLE_DEVICES=0&lt;/td&gt; &lt;td align="left"&gt;7f09a680a (6970)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q8_0&lt;/td&gt; &lt;td align="left"&gt;26.73 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;pp16384&lt;/td&gt; &lt;td align="left"&gt;1368.39&lt;/td&gt; &lt;td align="left"&gt;1Ã— RTX A6000&lt;/td&gt; &lt;td align="left"&gt;CUDA_VISIBLE_DEVICES=0&lt;/td&gt; &lt;td align="left"&gt;7f09a680a (6970)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q8_0&lt;/td&gt; &lt;td align="left"&gt;26.73 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;20.68&lt;/td&gt; &lt;td align="left"&gt;1Ã— RTX A6000&lt;/td&gt; &lt;td align="left"&gt;CUDA_VISIBLE_DEVICES=0&lt;/td&gt; &lt;td align="left"&gt;7f09a680a (6970)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q8_0&lt;/td&gt; &lt;td align="left"&gt;26.73 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;2360.41&lt;/td&gt; &lt;td align="left"&gt;A6000 + 3090&lt;/td&gt; &lt;td align="left"&gt;CUDA_VISIBLE_DEVICES=0,1&lt;/td&gt; &lt;td align="left"&gt;7f09a680a (6970)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q8_0&lt;/td&gt; &lt;td align="left"&gt;26.73 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;pp8192&lt;/td&gt; &lt;td align="left"&gt;2466.44&lt;/td&gt; &lt;td align="left"&gt;A6000 + 3090&lt;/td&gt; &lt;td align="left"&gt;CUDA_VISIBLE_DEVICES=0,1&lt;/td&gt; &lt;td align="left"&gt;7f09a680a (6970)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q8_0&lt;/td&gt; &lt;td align="left"&gt;26.73 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;pp16384&lt;/td&gt; &lt;td align="left"&gt;2547.94&lt;/td&gt; &lt;td align="left"&gt;A6000 + 3090&lt;/td&gt; &lt;td align="left"&gt;CUDA_VISIBLE_DEVICES=0,1&lt;/td&gt; &lt;td align="left"&gt;7f09a680a (6970)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q8_0&lt;/td&gt; &lt;td align="left"&gt;26.73 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;22.74&lt;/td&gt; &lt;td align="left"&gt;A6000 + 3090&lt;/td&gt; &lt;td align="left"&gt;CUDA_VISIBLE_DEVICES=0,1&lt;/td&gt; &lt;td align="left"&gt;7f09a680a (6970)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I think this a good setup for a test as the two GPUs are fairly close in power and Gemma3 is a relative large dense model that also fits in 8 bit on the A6000.&lt;/p&gt; &lt;p&gt;As you can see, I got a significant increase with both GPUs enabled. This was surprising to me as I was expecting the results to be about the same. Yes, the 3090 is a bit faster, but it also running pin 4xPCiE 4.0 oculink connection.&lt;/p&gt; &lt;p&gt;These are the commands I used in case anyone is wondering:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1 \ ./bin/llama-bench \ -m /PATH/gemma-3-27b-it-Q8_0.gguf \ -t 1 -fa 1 \ -b 1024 -ub 512 \ -sm layer \ -ngl 99 \ -ts 0.5/0.5 \ -p 2048,8192,16384 --- ~/llamacpp$ CUDA_VISIBLE_DEVICES=0 \ ./bin/llama-bench \ -m /PATH/gemma-3-27b-it-Q8_0.gguf \ -t 1 -fa 1 \ -b 1024 -ub 512 \ -sm layer \ -ngl 99 \ -p 2048,8192,16384 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MexInAbu"&gt; /u/MexInAbu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqe1kq/no_negative_impact_using_oculink_egpu_a_quick_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqe1kq/no_negative_impact_using_oculink_egpu_a_quick_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqe1kq/no_negative_impact_using_oculink_egpu_a_quick_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T23:08:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1opx6p1</id>
    <title>We just Fine-Tuned a Japanese Manga OCR Model with PaddleOCR-VL!</title>
    <updated>2025-11-06T12:08:18+00:00</updated>
    <author>
      <name>/u/erinr1122</name>
      <uri>https://old.reddit.com/user/erinr1122</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opx6p1/we_just_finetuned_a_japanese_manga_ocr_model_with/"&gt; &lt;img alt="We just Fine-Tuned a Japanese Manga OCR Model with PaddleOCR-VL!" src="https://b.thumbs.redditmedia.com/yFGQjvFykSgFy7I5SA6oM7N5D5pziNhR7HfUDN7uUKo.jpg" title="We just Fine-Tuned a Japanese Manga OCR Model with PaddleOCR-VL!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi all! ðŸ‘‹&lt;/strong&gt;&lt;br /&gt; Hope you donâ€™t mind a little self-promo, but we just finished fine-tuning &lt;strong&gt;PaddleOCR-VL&lt;/strong&gt; to build a model specialized in &lt;strong&gt;Japanese manga text recognition&lt;/strong&gt; â€” and it works surprisingly well! ðŸŽ‰&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/jzhang533/PaddleOCR-VL-For-Manga"&gt;PaddleOCR-VL-For-Manga&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; Manga109-s + 1.5 million synthetic samples&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Accuracy:&lt;/strong&gt; 70% full-sentence accuracy (vs. 27% from the original model)&lt;/p&gt; &lt;p&gt;It handles manga speech bubbles and stylized fonts really nicely. There are still challenges with full-width vs. half-width characters, but overall itâ€™s a big step forward for domain-specific OCR.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to use&lt;/strong&gt;&lt;br /&gt; You can use this model with &lt;strong&gt;Transformers&lt;/strong&gt;, &lt;strong&gt;PaddleOCR&lt;/strong&gt;, or any library that supports PaddleOCR-VL to recognize manga text.&lt;br /&gt; For structured documents, try pairing it with &lt;strong&gt;PP-DocLayoutV2&lt;/strong&gt; for layout analysis â€” though manga layouts are a bit different.&lt;/p&gt; &lt;p&gt;Weâ€™d love to hear your thoughts or see your own fine-tuned versions!&lt;br /&gt; Really excited to see how we can push OCR models even further. ðŸš€ &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ampi1bppmmzf1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43153284175cefb14a0f7cd8f415d127a33e26b4"&gt;https://preview.redd.it/ampi1bppmmzf1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43153284175cefb14a0f7cd8f415d127a33e26b4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/erinr1122"&gt; /u/erinr1122 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opx6p1/we_just_finetuned_a_japanese_manga_ocr_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opx6p1/we_just_finetuned_a_japanese_manga_ocr_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opx6p1/we_just_finetuned_a_japanese_manga_ocr_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T12:08:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqgk4c</id>
    <title>RzenEmbed-v2-7B (multimodal embedding)</title>
    <updated>2025-11-07T00:56:21+00:00</updated>
    <author>
      <name>/u/the__storm</name>
      <uri>https://old.reddit.com/user/the__storm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqgk4c/rzenembedv27b_multimodal_embedding/"&gt; &lt;img alt="RzenEmbed-v2-7B (multimodal embedding)" src="https://external-preview.redd.it/zjYj496B25Dh4S-OXammKuVuVC52psHmoaE_lB7iL4E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9be59fb9af9942dfe0de415b392058c2cfbbf90" title="RzenEmbed-v2-7B (multimodal embedding)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/the__storm"&gt; /u/the__storm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/qihoo360/RzenEmbed"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqgk4c/rzenembedv27b_multimodal_embedding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqgk4c/rzenembedv27b_multimodal_embedding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T00:56:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqiint</id>
    <title>Open AI testing new model, properly wanting to give more open source</title>
    <updated>2025-11-07T02:26:54+00:00</updated>
    <author>
      <name>/u/Vozer_bros</name>
      <uri>https://old.reddit.com/user/Vozer_bros</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiint/open_ai_testing_new_model_properly_wanting_to/"&gt; &lt;img alt="Open AI testing new model, properly wanting to give more open source" src="https://b.thumbs.redditmedia.com/Vd5aJU7CMWB_eKmSXQO5kJWOHZw917gCTwQIQrn-XIM.jpg" title="Open AI testing new model, properly wanting to give more open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/j6ivwd3vwqzf1.png?width=1155&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=28e660573c66f273c53691054f94b8b832139d50"&gt;https://preview.redd.it/j6ivwd3vwqzf1.png?width=1155&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=28e660573c66f273c53691054f94b8b832139d50&lt;/a&gt;&lt;/p&gt; &lt;p&gt;People tried this model and say the response is just like ChatGPT.&lt;br /&gt; And it is bad for most difficult tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7fzqxbcazqzf1.png?width=1002&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b626c081ef5ba96a4f83b7b97a2b1d40e3a080b5"&gt;https://preview.redd.it/7fzqxbcazqzf1.png?width=1002&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b626c081ef5ba96a4f83b7b97a2b1d40e3a080b5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;#EDIT: Additionally, the cutting time for data set is the same as GPT-5. Hence, in my opinion, they are cooking new member for OSS family. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vozer_bros"&gt; /u/Vozer_bros &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiint/open_ai_testing_new_model_properly_wanting_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiint/open_ai_testing_new_model_properly_wanting_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiint/open_ai_testing_new_model_properly_wanting_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T02:26:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq7qav</id>
    <title>Coding Success Depends More on Language Than Math</title>
    <updated>2025-11-06T19:04:03+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq7qav/coding_success_depends_more_on_language_than_math/"&gt; &lt;img alt="Coding Success Depends More on Language Than Math" src="https://b.thumbs.redditmedia.com/dJfmRO1sB40MsA3zzvKeWq3qsPsO50YLpT2QyCdjlUw.jpg" title="Coding Success Depends More on Language Than Math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The biggest factor in how good someone is at coding might surprise you. It is not math it is language.&lt;/p&gt; &lt;p&gt;A Nature study found that your ability with numbers explains only two percent of the difference in coding skill while language related brain activity explains seventy percent.&lt;/p&gt; &lt;p&gt;So maybe coding is less about numbers and more about how clearly you can think and express ideas in words.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oq7qav"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq7qav/coding_success_depends_more_on_language_than_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq7qav/coding_success_depends_more_on_language_than_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T19:04:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqjsif</id>
    <title>Polaris Alpha Review</title>
    <updated>2025-11-07T03:28:36+00:00</updated>
    <author>
      <name>/u/learningtolivee101</name>
      <uri>https://old.reddit.com/user/learningtolivee101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjsif/polaris_alpha_review/"&gt; &lt;img alt="Polaris Alpha Review" src="https://b.thumbs.redditmedia.com/zf4-g6IjDDefg8_uSSYKiwPIbOw3JLlu5LHiYjOLTeo.jpg" title="Polaris Alpha Review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Polaris Alpha, a stealth AI model launched on OpenRouter on November 6, 2025, is generating significant excitement in AI communities for its capabilities in coding, tool calling, and instruction adherence. As a cloaked model from an undisclosed provider, it features a 256,000 token context window, free access ($0 per million input/output tokens), and is designed for feedback collection, with all prompts logged for potential improvements. Community speculations heavily favor it being a preview of OpenAI's GPT-5.1, based on naming conventions (e.g., similar to &amp;quot;Horizon-Alpha&amp;quot;), exclusive developer message features, and performance traits.&lt;/p&gt; &lt;p&gt;From my sessions, Polaris Alpha shines in practicality. Its 256k context window means you can throw massive docs or codebases at it without losing trackâ€”huge for my dev side projects. The informal tone? Endearing at first (&amp;quot;Love this question!&amp;quot;), but it grows on you, making interactions feel less robotic. Tool calling is seamless; I tested it with a mock API integration prompt, and it nailed the logic without fluff. Instruction following is spot-on tooâ€”no more prompting tweaks to avoid derails.&lt;/p&gt; &lt;p&gt;But it's alpha, so expect quirks. One prompt for a complex data analysis stumbled on edge cases, needing a retry. Still, compared to older models, it hallucinates way less, which warms my heart after too many &amp;quot;confidently wrong&amp;quot; responses from others.&lt;/p&gt; &lt;p&gt;Pros: Free access, lightning-fast responses, excels in technical tasksâ€”it's like a turbocharged assistant for coders and thinkers. The community feedback echoes this; users report it outperforming in benchmarks like Korean language tests or SVG generations.&lt;/p&gt; &lt;p&gt;Cons: Being cloaked means no transparency on origins or training data, and prompts are logged for improvements (fair warning for privacy folks). It's not open-weights, so no local runs yet, and very niche prompts might need massaging.&lt;/p&gt; &lt;p&gt;Speculations and My Predictions&lt;br /&gt; The elephant in the room: Who's behind it? Community sleuths on X and Reddit point to OpenAIâ€”naming patterns like &amp;quot;Horizon-Alpha&amp;quot; from past previews, exclusive developer messages, and that sky-themed vibe (Polaris is a star, after all). Some guess Grok or Google, but the 256k context doesn't scream Gemini's million-token beasts. I lean OpenAI too; it has that polished feel without the censorship baggage.&lt;/p&gt; &lt;p&gt;Predicting ahead, if this is GPT-5.1 teasing, expect a full rollout soon with even better multi-modal chops. It could disrupt coding tools or agents, making them more intuitive. But I'm skeptical about over-hypeâ€”alphas often evolve, and privacy logging might turn off some. Still, it's reignited my passion for experimenting; imagine pairing it with local tools once (if) weights drop.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dgvxmm7s7rzf1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f1c5c0ca6004eb36a49beb11c463cbae6c85a7b5"&gt;https://openrouter.ai/openrouter/polaris-alpha&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/learningtolivee101"&gt; /u/learningtolivee101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjsif/polaris_alpha_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjsif/polaris_alpha_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjsif/polaris_alpha_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T03:28:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqe8o2</id>
    <title>1 second voice-to-voice latency with all open models &amp; frameworks</title>
    <updated>2025-11-06T23:16:45+00:00</updated>
    <author>
      <name>/u/crookedstairs</name>
      <uri>https://old.reddit.com/user/crookedstairs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Voice-to-voice latency needs to be under a certain threshold for conversational agents to sound natural. A general target is 1s or less. The Modal team wanted to see how fast we could get a STT &amp;gt; LLM &amp;gt; TTS pipeline working with self-deployed, open models only: &lt;a href="https://modal.com/blog/low-latency-voice-bot"&gt;https://modal.com/blog/low-latency-voice-bot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We used:&lt;/p&gt; &lt;p&gt;- Parakeet-tdt-v3* [STT]&lt;br /&gt; - Qwen3-4B-Instruct-2507 [LLM]&lt;br /&gt; - KokoroTTS&lt;/p&gt; &lt;p&gt;plus Pipecat, an open-source voice AI framework, to orchestrate these services. &lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt; An interesting finding is that Parakeet (paired with VAD for segmentation) was so fast, it beat open-weights streaming models we tested*!&lt;/p&gt; &lt;p&gt;Getting down to 1s latency required optimizations along several axes ðŸª„&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Streaming vs not-streaming STT models&lt;/li&gt; &lt;li&gt;Colocating VAD (voice activity detection) with Pipecat vs with the STT service &lt;/li&gt; &lt;li&gt;Different parameterizations for vLLM, the inference engine we used&lt;/li&gt; &lt;li&gt;Optimizing audio chunk size and silence clipping for TTS&lt;/li&gt; &lt;li&gt;Using WebRTC for client to bot communication. We used SmallWebRTC, an open-source transport from Daily.&lt;/li&gt; &lt;li&gt;Using WebSockets for streaming inputs and outputs of the STT and TTS services.&lt;/li&gt; &lt;li&gt;Pinning all our services to the same region.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;While we ran all the services on Modal, we think that many of these latency optimizations are relevant no matter where you deploy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crookedstairs"&gt; /u/crookedstairs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqe8o2/1_second_voicetovoice_latency_with_all_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqe8o2/1_second_voicetovoice_latency_with_all_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqe8o2/1_second_voicetovoice_latency_with_all_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T23:16:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqkmt6</id>
    <title>kat-coder, as in KAT-Coder-Pro V1 is trash and is scamming clueless people at an exorbitant $0.98/$3.8 per million tokens</title>
    <updated>2025-11-07T04:11:06+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqkmt6/katcoder_as_in_katcoderpro_v1_is_trash_and_is/"&gt; &lt;img alt="kat-coder, as in KAT-Coder-Pro V1 is trash and is scamming clueless people at an exorbitant $0.98/$3.8 per million tokens" src="https://b.thumbs.redditmedia.com/Q8IcSQMEuVBr0MAMpeBpDDGSofCNWXL088FnLZpIrNY.jpg" title="kat-coder, as in KAT-Coder-Pro V1 is trash and is scamming clueless people at an exorbitant $0.98/$3.8 per million tokens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to thank Novita for making this model free for some time but this model is not worth using even as a free model. kwai should absolutely be crucified for the prices they were trying to charge for this model, or will be trying to charge if they dont change their prices.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jhmzz5o6erzf1.png?width=1215&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a81506c5db2c32fef3c6a492fe5fdb683250f58"&gt;https://preview.redd.it/jhmzz5o6erzf1.png?width=1215&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a81506c5db2c32fef3c6a492fe5fdb683250f58&lt;/a&gt;&lt;/p&gt; &lt;p&gt;this is my terminal-bench run of on kat-coder using your api with the terminus-2 harness, only 28.75%, this is the lowest score ive tested to date. this would not be a big deal if the model were cheaper or only slightly worse since some models might do worse at some kinds of coding tasks but this is abhorrently bad. for comparison (including a lot of the worst scoring runs I've had):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;qwen3 coder from nvidia nim api scores 37.5%, this is the same score qwen has in the modelcard. keep in mind that this is using terminus-2 harness, which works well with most models, but qwen3 coder models in particular seem to underperform with any agent that isnt qwen3-code cli. this model is free from nvidia nim api for unlimited use or 2000 req per day from qwen oath.&lt;/li&gt; &lt;li&gt;qwen3 coder 30b a3b scores 31.3% with the same harness. please tell me how on earth kat-coder is worse than a very easily run, small local moe. significantly worse too. its a 2.55% score difference, that is a large gap.&lt;/li&gt; &lt;li&gt;Deepseek v3.1 terminus from nvidia nim with the same harness scores 36.25%, this is another model that is handicapped by the terminus-2 harness, it works better with things like aider, etc. this model is also way cheaper api cost that kat-coder, or just completely free via nvidia nim.&lt;/li&gt; &lt;li&gt;kimi k2 with terminus-2 from nvidia nim api scores 41.25% in my tests, moonshot got a score of 44.5% in their first party testing.&lt;/li&gt; &lt;li&gt;minimax m2:free from openrouter 43.75%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;$0.98/$3.8 api cost for this (the price we will be paying after this free usage period if it goes back to original cost) is absolutely disgusting, this is more expensive than all the models I mentioned here. Seriously, there are so many better free options. I would not be surprised if this is just another checkpoint of their 72b model that they saw scored a little higher in their eval harness against some cherrypicked benchmarks, that they decided to try and release as a &amp;quot;high end&amp;quot; coding model to make money off dumb vibe coders that fall victim to confirmation bias.&lt;/p&gt; &lt;p&gt;I still have all my terminal bench sessions saved and can prove my results are real. I also ran against kat-coder and most of these models more than once so I can verify theyre accurate. I do a full system and volumes prune on docker before every run, and run every session under the exact same conditions. You can do your own run too with docker and terminal bench, here's the command to replicate my results: &lt;/p&gt; &lt;p&gt;&lt;code&gt;terminal-bench run -a terminus-2 -m novita/kat-coder -d terminal-bench-core==0.1.1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Just set your novita key in your environment under a NOVITA_API_KEY variable (refer to litellm docs for testing other models/providers). I suggest setting LITELLM_LOG to &amp;quot;ERROR&amp;quot; in your environment variables as well to get only error logging (otherwise you get a ton of debugging warning cause kat-coder isnt implemented for cost calculations in litellm). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqkmt6/katcoder_as_in_katcoderpro_v1_is_trash_and_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqkmt6/katcoder_as_in_katcoderpro_v1_is_trash_and_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqkmt6/katcoder_as_in_katcoderpro_v1_is_trash_and_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T04:11:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqevxz</id>
    <title>SGLang is integrating ktransformers for hybrid CPU/GPU inference</title>
    <updated>2025-11-06T23:43:49+00:00</updated>
    <author>
      <name>/u/waiting_for_zban</name>
      <uri>https://old.reddit.com/user/waiting_for_zban</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is rather a really exciting news (if you have 2TB of RAM ...)! I know 2TB is huge, but it's still &amp;quot;more manageable&amp;quot; than VRAM (also technically you only need 1TB I think). &lt;/p&gt; &lt;p&gt;Based on this &lt;a href="https://github.com/sgl-project/sglang/issues/11425"&gt;PR (WIP)&lt;/a&gt;, it seems it's possible to run the &lt;strong&gt;latest Kimi K2 Thinking&lt;/strong&gt; with &lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2-Thinking.md"&gt;SGLang with ktransformers CPU kernels&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;To give you some context, right now, the main way to run LLMs for GPU poor (us), but RAM rich (whoever snagged some before the hike), would be using GGUF with llama.cpp. But that comes with few compromises: we need to wait for the quants, and if a model has a new architecture, this would take quite some time. Not to forget, quality usually takes a hit (although ik_llama and unsloth UD are neat). &lt;/p&gt; &lt;p&gt;Now beside &lt;a href="https://docs.vllm.ai/en/latest/getting_started/installation/gpu/"&gt;vllm&lt;/a&gt; (arguably the best GPU inference engine), &lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt; from top universities researchers (UC Berkley, Stanford, etc ...) is relatively new, and it seems they're collaborating with the creator of Kimi K2 and &lt;a href="https://github.com/kvcache-ai/ktransformers"&gt;ktransformers&lt;/a&gt; (I didn't know they had the same team behind them), to provide more scalable hybrid inference! &lt;/p&gt; &lt;p&gt;And it's even possible to &lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/SFT_Installation_Guide_KimiK2.md"&gt;Lora finetune it&lt;/a&gt;! Of course if you have 2TB of RAM.&lt;br /&gt; Anyway the performance on their testing: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Their System Configuration:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPUs: 8Ã— NVIDIA L20&lt;/li&gt; &lt;li&gt;CPU: Intel(R) Xeon(R) Gold 6454S&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Bench prefill&lt;/strong&gt;&lt;br /&gt; ============ Serving Benchmark Result ============ Backend: sglang&lt;br /&gt; Traffic request rate: inf&lt;br /&gt; Max request concurrency: not set&lt;br /&gt; Successful requests: 37&lt;br /&gt; Benchmark duration (s): 65.58&lt;br /&gt; Total input tokens: 37888&lt;br /&gt; Total input text tokens: 37888&lt;br /&gt; Total input vision tokens: 0&lt;br /&gt; Total generated tokens: 37&lt;br /&gt; Total generated tokens (retokenized): 37&lt;br /&gt; Request throughput (req/s): 0.56&lt;br /&gt; Input token throughput (tok/s): 577.74&lt;br /&gt; Output token throughput (tok/s): 0.56&lt;br /&gt; Total token throughput (tok/s): 578.30&lt;br /&gt; Concurrency: 23.31&lt;br /&gt; ----------------End-to-End Latency----------------&lt;br /&gt; Mean E2E Latency (ms): 41316.50&lt;br /&gt; Median E2E Latency (ms): 41500.35&lt;br /&gt; ---------------Time to First Token----------------&lt;br /&gt; Mean TTFT (ms): 41316.48&lt;br /&gt; Median TTFT (ms): 41500.35&lt;br /&gt; P99 TTFT (ms): 65336.31&lt;br /&gt; ---------------Inter-Token Latency----------------&lt;br /&gt; Mean ITL (ms): 0.00&lt;br /&gt; Median ITL (ms): 0.00&lt;br /&gt; P95 ITL (ms): 0.00&lt;br /&gt; P99 ITL (ms): 0.00&lt;br /&gt; Max ITL (ms): 0.00&lt;br /&gt; ================================================== &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bench decode&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;============ Serving Benchmark Result ============&lt;br /&gt; Backend: sglang&lt;br /&gt; Traffic request rate: inf&lt;br /&gt; Max request concurrency: not set&lt;br /&gt; Successful requests: 37&lt;br /&gt; Benchmark duration (s): 412.66&lt;br /&gt; Total input tokens: 370&lt;br /&gt; Total input text tokens: 370&lt;br /&gt; Total input vision tokens: 0&lt;br /&gt; Total generated tokens: 18944&lt;br /&gt; Total generated tokens (retokenized): 18618&lt;br /&gt; Request throughput (req/s): 0.09&lt;br /&gt; Input token throughput (tok/s): 0.90&lt;br /&gt; Output token throughput (tok/s): 45.91&lt;br /&gt; Total token throughput (tok/s): 46.80&lt;br /&gt; Concurrency: 37.00&lt;br /&gt; ----------------End-to-End Latency----------------&lt;br /&gt; Mean E2E Latency (ms): 412620.35&lt;br /&gt; Median E2E Latency (ms): 412640.56&lt;br /&gt; ---------------Time to First Token----------------&lt;br /&gt; Mean TTFT (ms): 3551.87&lt;br /&gt; Median TTFT (ms): 3633.59&lt;br /&gt; P99 TTFT (ms): 3637.37&lt;br /&gt; ---------------Inter-Token Latency----------------&lt;br /&gt; Mean ITL (ms): 800.53&lt;br /&gt; Median ITL (ms): 797.89&lt;br /&gt; P95 ITL (ms): 840.06&lt;br /&gt; P99 ITL (ms): 864.96&lt;br /&gt; Max ITL (ms): 3044.56&lt;br /&gt; ==================================================&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waiting_for_zban"&gt; /u/waiting_for_zban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqevxz/sglang_is_integrating_ktransformers_for_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqevxz/sglang_is_integrating_ktransformers_for_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqevxz/sglang_is_integrating_ktransformers_for_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T23:43:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqc01w</id>
    <title>Epoch: LLMs that generate interactive UI instead of text walls</title>
    <updated>2025-11-06T21:46:26+00:00</updated>
    <author>
      <name>/u/ItzCrazyKns</name>
      <uri>https://old.reddit.com/user/ItzCrazyKns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqc01w/epoch_llms_that_generate_interactive_ui_instead/"&gt; &lt;img alt="Epoch: LLMs that generate interactive UI instead of text walls" src="https://preview.redd.it/elog79cngpzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2a6a284595f32b69211b6ce05cbcd3fd5a10860" title="Epoch: LLMs that generate interactive UI instead of text walls" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So generally LLMs generate text or sometimes charts (via tool calling) but I gave it the ability to generate UI&lt;/p&gt; &lt;p&gt;So instead of LLMs outputting markdown, I built &lt;strong&gt;Epoch&lt;/strong&gt; where the LLM generates actual interactive components.&lt;/p&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;p&gt;The LLM outputs a &lt;strong&gt;structured component tree&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Component = { type: &amp;quot;Card&amp;quot; | &amp;quot;Button&amp;quot; | &amp;quot;Form&amp;quot; | &amp;quot;Input&amp;quot; | ... properties: { ... } children?: Component[] } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;My renderer walks this tree and builds React components. So responses aren't text but they're interfaces with buttons, forms, inputs, cards, tabs, whatever.&lt;/p&gt; &lt;h1&gt;The interesting part&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;It's bidirectional.&lt;/strong&gt; You can click a button or submit a form -&amp;gt; that interaction gets serialized back into conversation history -&amp;gt; LLM generates new UI in response.&lt;/p&gt; &lt;p&gt;So you get actual stateful, explorable interfaces. You ask a question -&amp;gt; get cards with action buttons -&amp;gt; click one -&amp;gt; form appears -&amp;gt; submit it -&amp;gt; get customized results.&lt;/p&gt; &lt;h1&gt;Tech notes&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Works with &lt;strong&gt;Ollama&lt;/strong&gt; (local/private) and &lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Structured output schema doesn't take context, but I also included it in the system prompt for better performance with smaller Ollama models (system prompt is a bit bigger now, finding a workaround later)&lt;/li&gt; &lt;li&gt;25+ components, real time SSE streaming, web search, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically I'm turning LLMs from text generators into &lt;strong&gt;interface compilers&lt;/strong&gt;. Every response is a composable UI tree.&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://github.com/itzcrazykns/epoch"&gt;github.com/itzcrazykns/epoch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Built with Next.js, TypeScript, Vercel AI SDK, shadcn/ui. Feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ItzCrazyKns"&gt; /u/ItzCrazyKns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/elog79cngpzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqc01w/epoch_llms_that_generate_interactive_ui_instead/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqc01w/epoch_llms_that_generate_interactive_ui_instead/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T21:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1opzdow</id>
    <title>We have a new Autoregressive Text-to-Speech in town!</title>
    <updated>2025-11-06T13:48:06+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opzdow/we_have_a_new_autoregressive_texttospeech_in_town/"&gt; &lt;img alt="We have a new Autoregressive Text-to-Speech in town!" src="https://preview.redd.it/3gtxm0bl5nzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62bf8df51385db28e73fba54de34caa842cb3b13" title="We have a new Autoregressive Text-to-Speech in town!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/maya-research/maya1"&gt;https://huggingface.co/maya-research/maya1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3gtxm0bl5nzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opzdow/we_have_a_new_autoregressive_texttospeech_in_town/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opzdow/we_have_a_new_autoregressive_texttospeech_in_town/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T13:48:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq5msi</id>
    <title>Speculative Decoding is AWESOME with Llama.cpp!</title>
    <updated>2025-11-06T17:46:38+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried it earlier this year with LM Studio and was incredibly disappointed. The gains were marginal at best, and sometimes slowed down inference, and I quickly abandoned it.&lt;/p&gt; &lt;p&gt;Fast forward to this week, I decided to try out Speculative Decoding (SD) with Llama.cpp, and it's truly worth using. Models I tried, and rough performance gains (all models are Unsloth's dynamic Q4_K_XL) - Running this on a unified memory with RX 890m iGPU:&lt;/p&gt; &lt;p&gt;- Llama3.3-70B: Without SD, 2.2 t/s. With SD (llama-3.2-1B) as draft, I get 3.2-4 t/s with average of 3.5 t/s&lt;/p&gt; &lt;p&gt;-Qwen3-32B: Without SD, 4.4 t/s. With SD (Qwen3-0.6B) as draft, I get 5-9 t/s&lt;/p&gt; &lt;p&gt;I tried larger/smarter draft models, different quant levels for the small models, but landed on the Q4's as the best compromise. Ran tool calling, processed large context, and tried obvious and obscure niche type prompts. The performance always holds at 10% better at the worst case. For average use cases I was getting 30-50% improvements which is huge for a humble machine like mine.&lt;/p&gt; &lt;p&gt;Some might call a 2.2 t/s to 4 t/s a no gain, but the quality of a 70B model responses for certain prompts it's still unmatched by any MOE in that size or larger (except for coding). Getting 6-7t/s for Qwen3-32B dense brings the model back to my most used list again. YMMV with faster dGPUs, faster unified memory like on the Strix Halo.&lt;/p&gt; &lt;p&gt;This was done with all the default llama.cpp parameters, I just add -md /path/to/model/model.gguf. Who knows how much better I can get the performance with non-default SD parameters.&lt;/p&gt; &lt;p&gt;I'm now on the hunt for the perfect draft model to hook with Mistral Small-24B. If you have any suggestions, please let me know.&lt;/p&gt; &lt;p&gt;EDIT: adding my llama.cpp command and parameters for others to replicate. No customization to the draft settings, just adding the draft model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Llama3.3-70B&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;${llamasvr} -m ${mpath}\\Llama-3.3-70B-Instruct-UD-Q4_K_XL.gguf -md ${mpath}\\Llama-3.2-1B-Instruct-UD-Q4_K_XL.gguf --jinja --no-mmap --ctx-size 16000 --temp 0.7&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-32B&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;${llamasvr} -m ${mpath}\\Qwen3-32B-UD-Q4_K_XL.gguf -md ${mpath}\\Qwen3-0.6B-UD-Q4_K_XL.gguf --jinja --no-mmap --ctx-size 24000 --temp 0.6 --top-p 0.95 --top-k 20 --min-p 0.00&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mistral-Small-24B&lt;/strong&gt;&lt;br /&gt; &lt;code&gt;${llamasvr} -m ${mpath}\\Mistral-Small-3.2-24B-Instruct-2506-UD-Q4_K_XL.gguf -md ${mpath}\\Mistral-Small-3.1-DRAFT-0.5B-Q4_K_M.gguf --jinja --no-mmap --ctx-size 32000 --temp 0.15 --top-p 1.00&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq5msi/speculative_decoding_is_awesome_with_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq5msi/speculative_decoding_is_awesome_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq5msi/speculative_decoding_is_awesome_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T17:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq9vg6</id>
    <title>Bombshell report exposes how Meta relied on scam ad profits to fund AI</title>
    <updated>2025-11-06T20:24:52+00:00</updated>
    <author>
      <name>/u/srwaxalot</name>
      <uri>https://old.reddit.com/user/srwaxalot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9vg6/bombshell_report_exposes_how_meta_relied_on_scam/"&gt; &lt;img alt="Bombshell report exposes how Meta relied on scam ad profits to fund AI" src="https://external-preview.redd.it/6_FxJznQ24YNhZIspAvn3ERygtZKSAS_E8jP1rIyqCc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4057f4ab73241df2a29bc789074f67f648ea470" title="Bombshell report exposes how Meta relied on scam ad profits to fund AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srwaxalot"&gt; /u/srwaxalot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arstechnica.com/tech-policy/2025/11/bombshell-report-exposes-how-meta-relied-on-scam-ad-profits-to-fund-ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9vg6/bombshell_report_exposes_how_meta_relied_on_scam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9vg6/bombshell_report_exposes_how_meta_relied_on_scam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T20:24:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq3ls6</id>
    <title>Lemonade's C++ port is available in beta today, let me know what you think</title>
    <updated>2025-11-06T16:31:00+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq3ls6/lemonades_c_port_is_available_in_beta_today_let/"&gt; &lt;img alt="Lemonade's C++ port is available in beta today, let me know what you think" src="https://preview.redd.it/yemgirr6wnzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba5d3a212198dbafbf221f509993023f52307bc5" title="Lemonade's C++ port is available in beta today, let me know what you think" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A couple weeks ago I asked on here if Lemonade should switch from Python and go native and got a strong &amp;quot;yes.&amp;quot; So now I'm back with a C++ beta! If anyone here has time to try this out and give feedback that would be awesome.&lt;/p&gt; &lt;p&gt;As a refresher: Lemonade is a local LLM server-router, like a local OpenRouter. It helps you quickly get started with llama.cpp Vulkan or ROCm, as well as AMD NPU (on Windows) with the RyzenAI SW and FastFlowLM backends. Everything is unified behind a single API and web ui.&lt;/p&gt; &lt;p&gt;To try the C++ beta, head to the latest release page: &lt;a href="https://github.com/lemonade-sdk/lemonade/releases/tag/v8.2.1"&gt;Release v8.2.1 Â· lemonade-sdk/lemonade&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Windows users: download Lemonade_Server_Installer_beta.exe and run it.&lt;/li&gt; &lt;li&gt;Linux users: download lemonade-server-9.0.0-Linux.deb, run &lt;code&gt;sudo dpkg -i lemonade-server-9.0.0-Linux.deb&lt;/code&gt;, and run &lt;code&gt;lemonade-server-beta serve&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My immediate next steps are to fix any problems identified in the beta, then completely replace the Python with the C++ for users! This will happen in a week unless there's a blocker. &lt;/p&gt; &lt;p&gt;The Lemonade GitHub has links for issues and discord if you want to share thoughts there. And I always appreciate a star if you like the project's direction!&lt;/p&gt; &lt;p&gt;PS. The usual caveats apply for LLMs on AMD NPU. Only available on Windows right now, Linux is being worked on, but there is no ETA for Linux support. I share all of the community's Linux feedback with the team at AMD, so feel free to let me have it in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yemgirr6wnzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq3ls6/lemonades_c_port_is_available_in_beta_today_let/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq3ls6/lemonades_c_port_is_available_in_beta_today_let/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T16:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqi4qp</id>
    <title>My Hands-On Review of Kimi K2 Thinking: The Open-Source AI That's Changing the Game</title>
    <updated>2025-11-07T02:08:35+00:00</updated>
    <author>
      <name>/u/Radiant-Act4707</name>
      <uri>https://old.reddit.com/user/Radiant-Act4707</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqi4qp/my_handson_review_of_kimi_k2_thinking_the/"&gt; &lt;img alt="My Hands-On Review of Kimi K2 Thinking: The Open-Source AI That's Changing the Game" src="https://b.thumbs.redditmedia.com/VmexduXKTXTd7OzipOLDhJ-FQQgYkFWKvGwDOtPOSQs.jpg" title="My Hands-On Review of Kimi K2 Thinking: The Open-Source AI That's Changing the Game" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Overview&lt;/h1&gt; &lt;p&gt;As someone who's tested numerous AI models, Kimi K2 Thinking stands out for its balance of power and efficiency. Released by Moonshot AI on November 6, 2025, it's designed as a &amp;quot;thinking agent&amp;quot; with a 1 trillion-parameter MoE architecture, activating 32 billion parameters per inference. This allows it to run on reasonable hardware while delivering impressive results in reasoning and tool use.&lt;/p&gt; &lt;h1&gt;Key Strengths&lt;/h1&gt; &lt;p&gt;In my tests, it handled up to 300 sequential tool calls without losing coherence, a big improvement over prior models. For coding, it achieved high scores like 71.3% on SWE-Bench Verified, and I saw it generate functional games and fix bugs seamlessly. It's available on Hugging Face and supports OpenAI-compatible APIs, making integration straightforward.&lt;/p&gt; &lt;h1&gt;Getting Started&lt;/h1&gt; &lt;p&gt;Download from Hugging Face or try via the Moonshot API. Check the docs at platform.moonshot.ai for setup.&lt;/p&gt; &lt;p&gt;Hey r/ LocalLLaMA, I've been tinkering with AI models for years, and Moonshot AI's Kimi K2 Thinking, launched on November 6, 2025, has genuinely impressed me. Positioned as an open-source &amp;quot;thinking agent,&amp;quot; it specializes in deep reasoning, autonomous tool orchestration, and coding. After running it on my setup with two M3 Ultras at around 15 tokens per second, I can vouch for its efficiency and capabilities. The 256K context window handled large projects without hiccups, and its native INT4 quantization provided a 2x speedup in inference without compromising quality.&lt;/p&gt; &lt;p&gt;What sets it apart is the Mixture-of-Experts (MoE) architecture: 61 layers, 7168 attention hidden dimension, 384 experts selecting 8 per token, SwiGLU activation, and a 160K vocabulary. This setup, with 1 trillion total parameters but only 32 billion active, makes it resource-friendly yet powerful. In my sessions, it chained 200-300 tool calls autonomously, interleaving chain-of-thought with functions for tasks like research or writing.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s53z4idbtqzf1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=10e6b11229f8971e433349f14671e6f96b8045ad"&gt;Kimi K2 â€” Open-Source Agentic Model | by Shravan Kumar | Medium&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Technical Dive&lt;/h1&gt; &lt;p&gt;The model's checkpoints are in compressed-tensors format, and I easily converted them to FP8/BF16 for testing. It supports frameworks like vLLM and SGLang, and the turbo variant hit 171 tokens/second with 2.17-second first-token latencyâ€”faster than competitors like MiniMax-M2. Hardware requirements are manageable, under 600GB for weights, which is great for hobbyists.&lt;/p&gt; &lt;p&gt;In hands-on experiments, I tasked it with building a Space Invaders game in HTML/JavaScriptâ€”it delivered working code in one prompt. For creative tasks, it generated editable SVGs and even replicated a macOS interface with file management. Multilingual coding shone through, handling Japanese seamlessly and producing human-like emotional writing.&lt;/p&gt; &lt;h1&gt;Benchmark Insights&lt;/h1&gt; &lt;p&gt;I verified several benchmarks myself, and the results were consistent with reports. It scored 44.9% on Humanity's Last Exam with tools, outperforming Claude Sonnet 4.5 in agentic search (60.2% on BrowseComp vs. 24.1%). Math tasks were strong, with 99.1% on AIME25 using Python. While it edges GPT-5 in some areas like GPQA Diamond (85.7% vs. 84.5%), users on X have noted occasional long-context weaknesses.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lt29qbudtqzf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ff070c0931247250a0f2ee48e1362482a9bf940"&gt;5 Thoughts on Kimi K2 Thinking - by Nathan Lambert&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's a table of key benchmarks from my evaluation:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;Setting&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Humanity's Last Exam (Text-only)&lt;/td&gt; &lt;td align="left"&gt;No tools&lt;/td&gt; &lt;td align="left"&gt;23.9%&lt;/td&gt; &lt;td align="left"&gt;Solid baseline reasoning.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Humanity's Last Exam&lt;/td&gt; &lt;td align="left"&gt;With tools&lt;/td&gt; &lt;td align="left"&gt;44.9%&lt;/td&gt; &lt;td align="left"&gt;Beats proprietary models in expert questions.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HLE (Heavy)&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;51.0%&lt;/td&gt; &lt;td align="left"&gt;Enhanced with parallel trajectories.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AIME25&lt;/td&gt; &lt;td align="left"&gt;No tools&lt;/td&gt; &lt;td align="left"&gt;94.5%&lt;/td&gt; &lt;td align="left"&gt;Excellent math performance.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AIME25&lt;/td&gt; &lt;td align="left"&gt;With Python&lt;/td&gt; &lt;td align="left"&gt;99.1%&lt;/td&gt; &lt;td align="left"&gt;Near-perfect tool-assisted.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HMMT25&lt;/td&gt; &lt;td align="left"&gt;No tools&lt;/td&gt; &lt;td align="left"&gt;89.4%&lt;/td&gt; &lt;td align="left"&gt;Tournament-level math prowess.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BrowseComp&lt;/td&gt; &lt;td align="left"&gt;With tools&lt;/td&gt; &lt;td align="left"&gt;60.2%&lt;/td&gt; &lt;td align="left"&gt;Superior to GPT-5 (54.9%).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BrowseComp-ZH&lt;/td&gt; &lt;td align="left"&gt;With tools&lt;/td&gt; &lt;td align="left"&gt;62.3%&lt;/td&gt; &lt;td align="left"&gt;Strong in Chinese browsing.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SWE-Bench Verified&lt;/td&gt; &lt;td align="left"&gt;With tools&lt;/td&gt; &lt;td align="left"&gt;71.3%&lt;/td&gt; &lt;td align="left"&gt;Agentic coding leader.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMLU-Pro&lt;/td&gt; &lt;td align="left"&gt;No tools&lt;/td&gt; &lt;td align="left"&gt;84.6%&lt;/td&gt; &lt;td align="left"&gt;Broad knowledge base.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPQA Diamond&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;85.7%&lt;/td&gt; &lt;td align="left"&gt;Matches top closed models.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LiveCodeBench v6&lt;/td&gt; &lt;td align="left"&gt;â€”&lt;/td&gt; &lt;td align="left"&gt;83.1%&lt;/td&gt; &lt;td align="left"&gt;Competitive programming strength.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Community Feedback and Implications&lt;/h1&gt; &lt;p&gt;On X, the buzz is positiveâ€”posts highlight its macOS replication and game generation. Experts discuss its role in AI timelines, with open-source now rivaling closed models, potentially accelerating innovation while questioning proprietary dominance. Enterprises like Airbnb are exploring similar tech for cost savings.&lt;/p&gt; &lt;p&gt;The Modified MIT License allows commercial use with attribution for large deployments, democratizing access. However, potential benchmark biases and hardware needs are worth noting. Overall, I'd rate it 9/10 for open-source AIâ€”transformative, but with room for recall improvements in ultra-long tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5qocbotltqzf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d68b50858d33f6639aff9f7aac5bb69bc1358d64"&gt;https://preview.redd.it/5qocbotltqzf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d68b50858d33f6639aff9f7aac5bb69bc1358d64&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For access, head to Hugging Face, kimi.com, or the API at platform.moonshot.ai.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Radiant-Act4707"&gt; /u/Radiant-Act4707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqi4qp/my_handson_review_of_kimi_k2_thinking_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqi4qp/my_handson_review_of_kimi_k2_thinking_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqi4qp/my_handson_review_of_kimi_k2_thinking_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T02:08:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1oprsln</id>
    <title>What is your take on this?</title>
    <updated>2025-11-06T06:38:31+00:00</updated>
    <author>
      <name>/u/ya_Priya</name>
      <uri>https://old.reddit.com/user/ya_Priya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oprsln/what_is_your_take_on_this/"&gt; &lt;img alt="What is your take on this?" src="https://external-preview.redd.it/enAybXNsNngwbHpmMSG2HwlQpQ6Hj-82EDUoyhNg7YK-n8qL0itnzTKon9hZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9dc87934b9d0d8de087571ccc6f9351740b8dac" title="What is your take on this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: Mobile Hacker on twitter&lt;/p&gt; &lt;p&gt;Some of you were trying to find it. &lt;/p&gt; &lt;p&gt;Hey guys, this is their website - &lt;a href="https://droidrun.ai/"&gt;https://droidrun.ai/&lt;/a&gt;&lt;br /&gt; and the github - &lt;a href="https://github.com/droidrun/droidrun"&gt;https://github.com/droidrun/droidrun&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The guy who posted on X - &lt;a href="https://x.com/androidmalware2/status/1981732061267235050"&gt;https://x.com/androidmalware2/status/1981732061267235050&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can't add so many links, but they have detailed docs on their website.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ya_Priya"&gt; /u/ya_Priya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zp20kj6x0lzf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oprsln/what_is_your_take_on_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oprsln/what_is_your_take_on_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T06:38:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq8mmy</id>
    <title>Kimi K2 Thinking and DeepSeek R1 Architectures Side by Side</title>
    <updated>2025-11-06T19:37:54+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"&gt; &lt;img alt="Kimi K2 Thinking and DeepSeek R1 Architectures Side by Side" src="https://b.thumbs.redditmedia.com/bLGXBa7gA85RfSO792H013zd_aNhH4CnZeSr7OxsZVc.jpg" title="Kimi K2 Thinking and DeepSeek R1 Architectures Side by Side" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 is based on the DeepSeek V3/R1 architecture, and here's a side-by-side comparison.&lt;/p&gt; &lt;p&gt;- 2Ã— fewer attention heads (64 vs. 128)&lt;br /&gt; - ~1.5Ã— more experts per MoE layer (384 vs. 256)&lt;br /&gt; - Bigger vocabulary (160k vs. 129k)&lt;br /&gt; - K2 activates ~32B parameters per token (vs. 37B in DeepSeek R1)&lt;br /&gt; - Fewer dense FFN blocks before MoE&lt;br /&gt; - 2x longer supported context&lt;/p&gt; &lt;p&gt;In short, Kimi K2 is a slightly scaled DeepSeek V3/R1. And the gains are in the data and training recipes. Hopefully, we will see some details on those soon, too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/98ghpsqn9pzf1.jpg?width=2200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ca505a76cd4755ed0b557ac281e621eeac3da9e"&gt;https://preview.redd.it/98ghpsqn9pzf1.jpg?width=2200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ca505a76cd4755ed0b557ac281e621eeac3da9e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T19:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqcd1y</id>
    <title>Just want to take a moment to express gratitude for this tech</title>
    <updated>2025-11-06T22:00:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What a time to be alive! &lt;/p&gt; &lt;p&gt;I was just randomly reflecting today - a single file with just a bunch of numbers can be used to make poems, apps, reports and so much more. And that's just LLMs.. But then this applies to image, video, speech, music, audio, 3D models and whatever else that can be expressed digitally&lt;/p&gt; &lt;p&gt;Anyone can do this with publicly available downloads and software. You dont need sophisticated computers or hardware.&lt;/p&gt; &lt;p&gt;Possibly most insane of all is that you can do all of this for free.&lt;/p&gt; &lt;p&gt;This is just utter insanity. If you had told me this would be the ecosystem before this wave happened, I would have never believed you. Regardless of how things evolve, I think we should be immensely grateful for all of this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqcd1y/just_want_to_take_a_moment_to_express_gratitude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqcd1y/just_want_to_take_a_moment_to_express_gratitude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqcd1y/just_want_to_take_a_moment_to_express_gratitude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T22:00:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq1i9b</id>
    <title>Kimi K2 Thinking Huggingface</title>
    <updated>2025-11-06T15:12:59+00:00</updated>
    <author>
      <name>/u/DistanceSolar1449</name>
      <uri>https://old.reddit.com/user/DistanceSolar1449</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1i9b/kimi_k2_thinking_huggingface/"&gt; &lt;img alt="Kimi K2 Thinking Huggingface" src="https://external-preview.redd.it/H-gfQMTLwEzPYBcfO_Qq4uuh_Gu1NEE3y2PjVFhCwx0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73256a6e56665a31c845dbe43d4cf687ee6b4218" title="Kimi K2 Thinking Huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DistanceSolar1449"&gt; /u/DistanceSolar1449 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1i9b/kimi_k2_thinking_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1i9b/kimi_k2_thinking_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T15:12:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq9ui3</id>
    <title>Microsoftâ€™s AI Scientist</title>
    <updated>2025-11-06T20:23:52+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9ui3/microsofts_ai_scientist/"&gt; &lt;img alt="Microsoftâ€™s AI Scientist" src="https://preview.redd.it/jbv9rmub4pzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7b040b383a3c04d5034fca2fe81396d6e5d57a9" title="Microsoftâ€™s AI Scientist" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft literally just dropped the first AI scientist&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jbv9rmub4pzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9ui3/microsofts_ai_scientist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9ui3/microsofts_ai_scientist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T20:23:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq9b7e</id>
    <title>Nvidia's Jensen Huang: 'China is going to win the AI race,' FT reports</title>
    <updated>2025-11-06T20:03:28+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b7e/nvidias_jensen_huang_china_is_going_to_win_the_ai/"&gt; &lt;img alt="Nvidia's Jensen Huang: 'China is going to win the AI race,' FT reports" src="https://external-preview.redd.it/B5kYZqF-LXs8_vBUF8bfaXMktkNYepX59paDPfYv7go.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04b0e3c2929dde65c0820bb5e348487a3bb39955" title="Nvidia's Jensen Huang: 'China is going to win the AI race,' FT reports" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/world/asia-pacific/nvidias-jensen-huang-says-china-will-win-ai-race-with-us-ft-reports-2025-11-05/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b7e/nvidias_jensen_huang_china_is_going_to_win_the_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b7e/nvidias_jensen_huang_china_is_going_to_win_the_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T20:03:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqjgnh</id>
    <title>30 days to become AI engineer</title>
    <updated>2025-11-07T03:12:00+00:00</updated>
    <author>
      <name>/u/CayleneKole</name>
      <uri>https://old.reddit.com/user/CayleneKole</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m moving from 12 years in cybersecurity (big tech) into a Staff AI Engineer role.&lt;br /&gt; I have 30 days (~16h/day) to get production-ready, prioritizing context engineering, RAG, and reliable agents.&lt;br /&gt; I need a focused path: the few resources, habits, and pitfalls that matter most.&lt;br /&gt; If youâ€™ve done this or ship real LLM systems, how would you spend the 30 days?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CayleneKole"&gt; /u/CayleneKole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjgnh/30_days_to_become_ai_engineer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjgnh/30_days_to_become_ai_engineer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjgnh/30_days_to_become_ai_engineer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T03:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq1arc</id>
    <title>Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model</title>
    <updated>2025-11-06T15:04:59+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt; &lt;img alt="Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model" src="https://b.thumbs.redditmedia.com/NupD3tHHs6sXvqucL46py-jFU7OPNJHTwiCDt_n7fGc.jpg" title="Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d01vorgfjnzf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a8f26127a8125731e93b25522a7bcdc28637d6f"&gt;https://preview.redd.it/d01vorgfjnzf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a8f26127a8125731e93b25522a7bcdc28637d6f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech blog:&lt;/strong&gt; &lt;a href="https://moonshotai.github.io/Kimi-K2/thinking.html"&gt;https://moonshotai.github.io/Kimi-K2/thinking.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weights &amp;amp; code:&lt;/strong&gt; &lt;a href="https://huggingface.co/moonshotai"&gt;https://huggingface.co/moonshotai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T15:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqiduq</id>
    <title>Kimi 2 is the #1 creative writing AI right now. better than sonnet 4.5</title>
    <updated>2025-11-07T02:20:29+00:00</updated>
    <author>
      <name>/u/Excellent-Run7265</name>
      <uri>https://old.reddit.com/user/Excellent-Run7265</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just tried Kimi 2 and I'm genuinely impressed. It's the best creative writer AI I've usedâ€”better than Sonnet 4.5, better than anything else out there. And it's dirt cheap compared to Sonnet.&lt;/p&gt; &lt;p&gt;I never thought a cheap, open model would beat Anthropic at writing. don't do coding as much, but its understanding is so strong that it's probably capable there too. This is amazing for us consumers.&lt;/p&gt; &lt;p&gt;The giants now have to slash prices significantly or lose to China. At this pace, we'll see locally-run LLMs outperforming current top models in months. That's terrible for big companies like OpenAI and Anthropicâ€”they'll need AGI or something massively better to justify their cost difference or cut the price down to half at least for now.&lt;/p&gt; &lt;p&gt;This market is unpredictable and wild. With the US and Chinese companies pushing each other like this and not holding back, AI will become so powerful so fast that we won't have to do anything ourselves anymore.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent-Run7265"&gt; /u/Excellent-Run7265 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T02:20:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqebr3</id>
    <title>World's strongest agentic model is now open source</title>
    <updated>2025-11-06T23:20:15+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"&gt; &lt;img alt="World's strongest agentic model is now open source" src="https://preview.redd.it/jd607rvrzpzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f84c70ace26fdbd5db78313787e58d2403961e38" title="World's strongest agentic model is now open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jd607rvrzpzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T23:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
