<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-16T02:16:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o7hxao</id>
    <title>Challenges in Tracing and Debugging AI Workflows</title>
    <updated>2025-10-15T17:47:49+00:00</updated>
    <author>
      <name>/u/dinkinflika0</name>
      <uri>https://old.reddit.com/user/dinkinflika0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I work on evaluation and observability at Maxim, and I’ve been closely looking at how teams trace, debug, and maintain reliable AI workflows. Across multi-agent systems, RAG pipelines, and LLM-driven applications, getting full visibility into agent decisions and workflow failures is still a major challenge.&lt;/p&gt; &lt;p&gt;From my experience, common pain points include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Failure visibility across multi-step workflows:&lt;/strong&gt; Token-level logs are useful, but understanding the trajectory of an agent across multiple steps or chained models is hard without structured traces.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Debugging complex agent interactions:&lt;/strong&gt; When multiple models or tools interact, pinpointing which step caused a failure often requires reproducing the workflow from scratch.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Integrating human review effectively:&lt;/strong&gt; Automated metrics are great, but aligning evaluations with human judgment, especially for nuanced tasks, is still tricky.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Maintaining reliability in production:&lt;/strong&gt; Ensuring that your AI remains trustworthy under real-world usage and scaling scenarios can be difficult without end-to-end observability.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At &lt;a href="https://getmax.im/maxim"&gt;Maxim&lt;/a&gt;, we’ve built our platform to tackle these exact challenges. Some of the ways teams benefit include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Structured evaluations at multiple levels:&lt;/strong&gt; You can attach automated checks or human-in-the-loop reviews at the session, trace, or span level. This lets you catch issues early and iterate faster.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full visibility into agent trajectories:&lt;/strong&gt; Simulations and logging across multi-agent workflows give teams insights into failure modes and decision points.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom dashboards and alerts:&lt;/strong&gt; Teams can slice and dice traces, define performance criteria, and get Slack or PagerDuty alerts when issues arise.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;End-to-end observability:&lt;/strong&gt; From pre-release simulations to post-release monitoring, evaluation, and dataset curation, the platform is designed to give teams a complete picture of AI quality and reliability.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’ve seen that structured, full-stack evaluation workflows not only make debugging and tracing faster but also improve overall trustworthiness of AI systems. Would love to hear how others are tackling these challenges and what tools or approaches you’ve found effective for tracing, debugging, and reliability in complex AI pipelines.&lt;/p&gt; &lt;p&gt;(I humbly apologize if this comes across as self promo)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dinkinflika0"&gt; /u/dinkinflika0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7hxao/challenges_in_tracing_and_debugging_ai_workflows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7hxao/challenges_in_tracing_and_debugging_ai_workflows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7hxao/challenges_in_tracing_and_debugging_ai_workflows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T17:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7q86j</id>
    <title>SGLang vs TabbyAPI &amp; vLLM Benchmark Increases (Multi-GPU + Single-GPU)</title>
    <updated>2025-10-15T23:08:16+00:00</updated>
    <author>
      <name>/u/darkmaniac7</name>
      <uri>https://old.reddit.com/user/darkmaniac7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7q86j/sglang_vs_tabbyapi_vllm_benchmark_increases/"&gt; &lt;img alt="SGLang vs TabbyAPI &amp;amp; vLLM Benchmark Increases (Multi-GPU + Single-GPU)" src="https://b.thumbs.redditmedia.com/hYCPQDiKpniqgCSka5edKkISG9Nwr6PsYz6ozz6dRAs.jpg" title="SGLang vs TabbyAPI &amp;amp; vLLM Benchmark Increases (Multi-GPU + Single-GPU)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I wanted to share some benchmark results comparing different inference frameworks after migrating my setups from TabbyAPI and vLLM over to SGLang. I saw only a few posts mentioning it, so figured I'd add 2 examples I have if anyone is interested. The results honestly blew me away.&lt;/p&gt; &lt;p&gt;About a year ago TabbyAPI seemed to be what everyone suggested for fastest single request inference for multiple consumer cards. I went with that &amp;amp; 6x3090's. I also have 2 production servers in Colo's doing mostly Log analysis and inference for a data pipeline and outputting recommendations using vLLM and an RTX200 Ada&lt;/p&gt; &lt;p&gt;Both setups are using ESXi 8 with Ubuntu 24.04&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;h1&gt;System 1 – Multi-GPU Rig (Main Lab)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GPUs: 6× RTX 3090 (24GB each, 4 used for testing)&lt;/li&gt; &lt;li&gt;CPU: AMD EPYC 73F3&lt;/li&gt; &lt;li&gt;RAM: 512GB DDR4&lt;/li&gt; &lt;li&gt;OS: Ubuntu 24.04 (ESXi VM Passthrough + NVLink active)&lt;/li&gt; &lt;li&gt;Models Tested: &lt;ul&gt; &lt;li&gt;Mistral-Large-2411-AWQ4 (123B)&lt;/li&gt; &lt;li&gt;KAT-Dev (32B AWQ 8-bit)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;System 2 – Low-End Node&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GPU: RTX 2000 Ada (16GB, 70W TDP)&lt;/li&gt; &lt;li&gt;OS: Ubuntu 24.04 (ESXi VM passthrough)&lt;/li&gt; &lt;li&gt;Model: Gemma-3-12B-IT-AWQ4 (12B)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;----&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Framework&lt;/th&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;GPUs&lt;/th&gt; &lt;th align="left"&gt;Power&lt;/th&gt; &lt;th align="left"&gt;Tokens/s&lt;/th&gt; &lt;th align="left"&gt;Gain&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;TabbyAPI (ExLlamaV2)&lt;/td&gt; &lt;td align="left"&gt;Q6 EXL2&lt;/td&gt; &lt;td align="left"&gt;Mistral 123B&lt;/td&gt; &lt;td align="left"&gt;4×3090&lt;/td&gt; &lt;td align="left"&gt;165W&lt;/td&gt; &lt;td align="left"&gt;12 tok/s&lt;/td&gt; &lt;td align="left"&gt;Baseline&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SGLang&lt;/td&gt; &lt;td align="left"&gt;Q4 AWQ&lt;/td&gt; &lt;td align="left"&gt;Mistral 123B&lt;/td&gt; &lt;td align="left"&gt;4×3090&lt;/td&gt; &lt;td align="left"&gt;165W&lt;/td&gt; &lt;td align="left"&gt;32 tok/s&lt;/td&gt; &lt;td align="left"&gt;+167%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SGLang ( NVLink)&lt;/td&gt; &lt;td align="left"&gt;Q4 AWQ&lt;/td&gt; &lt;td align="left"&gt;Mistral 123B&lt;/td&gt; &lt;td align="left"&gt;4×3090&lt;/td&gt; &lt;td align="left"&gt;250–300W&lt;/td&gt; &lt;td align="left"&gt;36–37 tok/s&lt;/td&gt; &lt;td align="left"&gt;+200%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SGLang (NVLink + Torch.compile)&lt;/td&gt; &lt;td align="left"&gt;Q4 AWQ&lt;/td&gt; &lt;td align="left"&gt;Mistral 123B&lt;/td&gt; &lt;td align="left"&gt;4×3090&lt;/td&gt; &lt;td align="left"&gt;320W&lt;/td&gt; &lt;td align="left"&gt;37.1 tok/s&lt;/td&gt; &lt;td align="left"&gt;+209%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SGLang (NVLink + Torch.compile)&lt;/td&gt; &lt;td align="left"&gt;Q4 AWQ&lt;/td&gt; &lt;td align="left"&gt;KAT-Dev 32B&lt;/td&gt; &lt;td align="left"&gt;4×3090&lt;/td&gt; &lt;td align="left"&gt;300W&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;61.5 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;+66% vs Mistral&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;vLLM (baseline)&lt;/td&gt; &lt;td align="left"&gt;Q4 AWQ&lt;/td&gt; &lt;td align="left"&gt;Gemma 12B&lt;/td&gt; &lt;td align="left"&gt;1×2000 Ada&lt;/td&gt; &lt;td align="left"&gt;70W&lt;/td&gt; &lt;td align="left"&gt;20–21 tok/s&lt;/td&gt; &lt;td align="left"&gt;Baseline&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SGLang (AWQ + Torch.compile)&lt;/td&gt; &lt;td align="left"&gt;Q4 AWQ&lt;/td&gt; &lt;td align="left"&gt;Gemma 12B&lt;/td&gt; &lt;td align="left"&gt;1×2000 Ada&lt;/td&gt; &lt;td align="left"&gt;70W&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;23.4–23.8 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;+15–18%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;my 4x3090 Config:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sglang serve /models/mistral-large-awq \ --tensor-parallel-size 4 \ --enable-cuda-graph \ --flash-attn \ --gpu-memory-utilization 0.9 \ --kv-cache-dtype fp16 \ --block-size 16 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Why not push to 390/430w? Breaker flipping, UPS Screaming, and one of the SlimSAS Riser cards gets pissy going over 320w. Took the A/C unit off the same circuit, Ordered a new 4000w UPS, and new &amp;amp; better Riser cards that will hopefully be here at the end of the week. For now I'm capped at 320w. I wouldn't expect more than ~8% speed difference anyways based on the uplift from 165w to 320w&lt;/p&gt; &lt;p&gt;Model switching is a bit of a PITA, but using a model switcher script Open-WebUI can call different models when selecting it from the dropdown and it reboots the SGLang service with the new model.&lt;/p&gt; &lt;p&gt;Have also tested a few other 70b Models like llama, Qwen, deepseek distilled R1 llama, all seem fairly consistent for the uplift. +/- 10% &lt;/p&gt; &lt;p&gt;Would love feedback or other people’s results, especially curious how it scales on 4090s or L40S cards.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPT Summarization:&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;🧮 Key Takeaways&lt;/h1&gt; &lt;h1&gt;🔥 Backend matters&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;SGLang is &lt;strong&gt;3× faster than TabbyAPI&lt;/strong&gt; for large models (123B+).&lt;/li&gt; &lt;li&gt;Even on low-end cards, it’s &lt;strong&gt;15–18% faster than vLLM&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;⚡ Quantization wins&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;AWQ (weight-only Q4) massively reduces bandwidth pressure.&lt;/li&gt; &lt;li&gt;You can drop from Q6 → Q4 with minimal quality loss and huge speed gain.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔗 NVLink helps&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Just adding NVLink gave a &lt;strong&gt;+12.5% uplift&lt;/strong&gt; over PCIe Gen4.&lt;/li&gt; &lt;li&gt;Keeps TP communication local to GPU pairs, slashing latency.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🧠 Torch.compile isn’t magic&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Only ~0.3% gain for bandwidth-bound TP workloads (but worth enabling for long-running services).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;💡 Power scaling&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;165W → 320W = only +15% more speed but nearly double the power.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sweet spot:&lt;/strong&gt; ~250–300W per GPU (best stability/power/perf).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🧩 Virtualization friendly&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Both systems run under ESXi passthrough — &lt;strong&gt;no measurable overhead&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🏆 Performance Highlights&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Tokens/s&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral-Large 123B&lt;/td&gt; &lt;td align="left"&gt;4×3090, Q4 AWQ&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;37 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;3.1× faster than TabbyAPI&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KAT-Dev 32B&lt;/td&gt; &lt;td align="left"&gt;4×3090, 8-bit&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;61.5 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Best for agentic workflows&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma-3 12B&lt;/td&gt; &lt;td align="left"&gt;RTX 2000 Ada&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;23.7 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;+18% over vLLM baseline&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral-Large 123B (165W)&lt;/td&gt; &lt;td align="left"&gt;4×3090&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;32 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Most efficient (0.048 tok/s/W)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;⚡ TL;DR My results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;TabbyAPI → SGLang:&lt;/strong&gt; +200–300% faster&lt;/li&gt; &lt;li&gt;&lt;strong&gt;vLLM → SGLang:&lt;/strong&gt; +15–18% faster&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NVLink:&lt;/strong&gt; +12.5% more throughput&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best Efficiency:&lt;/strong&gt; 165–250W range&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best Performance:&lt;/strong&gt; 320W (37 tok/s)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fastest small model:&lt;/strong&gt; KAT-Dev @ 61.5 tok/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Virtualization:&lt;/strong&gt; ~ No penalty&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkmaniac7"&gt; /u/darkmaniac7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o7q86j"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7q86j/sglang_vs_tabbyapi_vllm_benchmark_increases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7q86j/sglang_vs_tabbyapi_vllm_benchmark_increases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T23:08:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o78zuc</id>
    <title>New models Qwen3-VL-4b/8b: hands-on notes</title>
    <updated>2025-10-15T11:59:11+00:00</updated>
    <author>
      <name>/u/chenqian615</name>
      <uri>https://old.reddit.com/user/chenqian615</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve got a pile of scanned PDFs, whiteboard photos, and phone receipts. The 4B Instruct fits well. For “read text fast and accurately,” the ramp-up is basically zero; most errors are formatting or extreme noise. Once it can read, I hand off to a text model for summarizing, comparison, and cleanup. This split beats forcing VQA reasoning on a small model.&lt;/p&gt; &lt;p&gt;For OCR + desktop/mobile GUI automation (“recognize → click → run flow”), the 8B Thinking is smooth. As a visual agent, it can spot UI elements and close the loop on tasks. The “visual coding enhancement” can turn screenshots into Draw.io/HTML/CSS/JS skeletons, which saves me scaffolding time.&lt;/p&gt; &lt;p&gt;Long videos: I search meeting recordings by keywords and the returned timestamps are reasonably accurate. The official notes mention structural upgrades for long-horizon/multi-scale (Interleaved‑MRoPE, DeepStack, Text–Timestamp Alignment). Net effect for me: retrieval feels more direct.&lt;/p&gt; &lt;p&gt;If I must nitpick: on complex logic or multi-step visual reasoning, the smaller models sometimes produce “looks right” answers. I don’t fight it, let them handle recognition; route reasoning to a bigger model. That’s more stable in production. I also care about spatial understanding, especially for UI/flowchart localization. From others’ tests, 2D/3D grounding looks solid this gen, finding buttons, arrows, and relative positions is reliable. For long/tall images, the 256K context (extendable to 1M) is friendly for multi-panel reading; cross-page references actually connect.&lt;/p&gt; &lt;p&gt;References: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe"&gt;https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chenqian615"&gt; /u/chenqian615 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o78zuc/new_models_qwen3vl4b8b_handson_notes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o78zuc/new_models_qwen3vl4b8b_handson_notes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o78zuc/new_models_qwen3vl4b8b_handson_notes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T11:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7ngw8</id>
    <title>gpt-oss 20b|120b mxfp4 ground truth?</title>
    <updated>2025-10-15T21:15:35+00:00</updated>
    <author>
      <name>/u/leo-k7v</name>
      <uri>https://old.reddit.com/user/leo-k7v</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ngw8/gptoss_20b120b_mxfp4_ground_truth/"&gt; &lt;img alt="gpt-oss 20b|120b mxfp4 ground truth?" src="https://external-preview.redd.it/Y7Y9gd-LFMslfMPUgEuWR7y8GwbSwaOpmAnv26HXX74.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2067bb9e143d5641ec12e479dda77a906fd8fb2c" title="gpt-oss 20b|120b mxfp4 ground truth?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am still a bit confused about ground truth for OpenAI gpt-oss 20b and 120b models. &lt;/p&gt; &lt;p&gt;There are several incarnations of quantized models for both and I actually do not want to add to the mess with my own quantizing, just want to understand which one would be an authoritative source (if at all possible)...&lt;/p&gt; &lt;p&gt;Any help would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/gpt-oss-20b-GGUF/discussions/17"&gt;https://huggingface.co/unsloth/gpt-oss-20b-GGUF/discussions/17&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/ollama/ollama/issues/11714#issuecomment-3172893576"&gt;https://github.com/ollama/ollama/issues/11714#issuecomment-3172893576&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r4s5hdsedcvf1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=db5bd6390c67d4b3890c1dcb68def97d341af4cf"&gt;https://preview.redd.it/r4s5hdsedcvf1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=db5bd6390c67d4b3890c1dcb68def97d341af4cf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leo-k7v"&gt; /u/leo-k7v &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ngw8/gptoss_20b120b_mxfp4_ground_truth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ngw8/gptoss_20b120b_mxfp4_ground_truth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ngw8/gptoss_20b120b_mxfp4_ground_truth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T21:15:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o751o9</id>
    <title>My first 15 days with GLM-4.6 — honest thoughts after using Opus and Sonnet</title>
    <updated>2025-10-15T08:02:52+00:00</updated>
    <author>
      <name>/u/DecisionLow2640</name>
      <uri>https://old.reddit.com/user/DecisionLow2640</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I first subscribed and started using &lt;strong&gt;GLM-4.6&lt;/strong&gt; with &lt;strong&gt;KiloCode&lt;/strong&gt;, I was honestly a bit disappointed. I had gotten used to the kind of UI/UX-focused results I was getting from &lt;strong&gt;Opus 4.1&lt;/strong&gt; and &lt;strong&gt;Sonnet&lt;/strong&gt;, and GLM felt different at first.&lt;/p&gt; &lt;p&gt;But after a couple of weeks of real use, I’ve started to really appreciate it. For &lt;strong&gt;pure programming tasks&lt;/strong&gt; — not design-related — GLM-4.6 is actually more &lt;strong&gt;precise, structured, and professional&lt;/strong&gt;. It doesn’t create as much random hard-coded mock data like Sonnet 4.5 often does. Every day it surprises me by solving problems more accurately and providing deeper diagnostics — even when I’m using it inside the &lt;strong&gt;VS Code KiloCode extension&lt;/strong&gt;, not ClaudeCode itself.&lt;/p&gt; &lt;p&gt;I had a case where Sonnet “solved” an issue but the bug was still there. I gave the exact same prompt to GLM-4.6, and it fixed it perfectly using proper &lt;strong&gt;software-engineering logic&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I also love that KiloCode can auto-generate &lt;strong&gt;UML diagrams&lt;/strong&gt;, which honestly reminded me of my early programming days in C and C++.&lt;/p&gt; &lt;p&gt;So yeah — I used to rely on Opus for its relaxed, intuitive style, but now I’m seeing the real &lt;strong&gt;power and precision of GLM-4.6&lt;/strong&gt;. If you have at least a basic understanding of programming, this model is a beast — more detailed, reliable, and consistent than Sonnet in many cases.&lt;/p&gt; &lt;p&gt;That’s my experience so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DecisionLow2640"&gt; /u/DecisionLow2640 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o751o9/my_first_15_days_with_glm46_honest_thoughts_after/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o751o9/my_first_15_days_with_glm46_honest_thoughts_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o751o9/my_first_15_days_with_glm46_honest_thoughts_after/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:02:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7o7k4</id>
    <title>GitHub - ibuhs/Kokoro-TTS-Pause: Enhances Kokoro TTS output by merging segments with dynamic, programmable pauses for meditative or narrative flow.</title>
    <updated>2025-10-15T21:44:54+00:00</updated>
    <author>
      <name>/u/Junior_Kale2569</name>
      <uri>https://old.reddit.com/user/Junior_Kale2569</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Junior_Kale2569"&gt; /u/Junior_Kale2569 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ibuhs/Kokoro-TTS-Pause"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7o7k4/github_ibuhskokorottspause_enhances_kokoro_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7o7k4/github_ibuhskokorottspause_enhances_kokoro_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T21:44:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6ocfs</id>
    <title>If it's not local, it's not yours.</title>
    <updated>2025-10-14T18:57:54+00:00</updated>
    <author>
      <name>/u/inkberk</name>
      <uri>https://old.reddit.com/user/inkberk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"&gt; &lt;img alt="If it's not local, it's not yours." src="https://preview.redd.it/zzv4ey22j4vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebc1f207746b0fa04e90a129bafad3aef0ca9971" title="If it's not local, it's not yours." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkberk"&gt; /u/inkberk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zzv4ey22j4vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T18:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7k7zz</id>
    <title>DGX SPARK Compiled llama.cpp Benchmarks Compared to M4 MAX (non-MLX)</title>
    <updated>2025-10-15T19:12:20+00:00</updated>
    <author>
      <name>/u/Noble00_</name>
      <uri>https://old.reddit.com/user/Noble00_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First, not trying to incite some feud discussion between Nvidia/Apple folks. I don't have either machines and just compiled this for amusement and just so others are aware. NOTE: Models aren't in mlx. If anyone is willing to share, it would be greatly appreciated. This would be really interesting.&lt;/p&gt; &lt;p&gt;Also, to any Strix Halo/Ryzen AI Max+ 395 users, if you'd like to compare:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m [model.gguf] -fa 1 -d 0,4096,8192,16384,32768 -p 2048 -n 32 -ub 2048 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16578"&gt;Source of DGX SPARK data&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://nitter.net/richinseattle/status/1978244945845657863"&gt;Source of M4 MAX data&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s (M4 MAX)&lt;/th&gt; &lt;th align="left"&gt;t/s (Spark)&lt;/th&gt; &lt;th align="left"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;1761.99 ± 78.03&lt;/td&gt; &lt;td align="left"&gt;3610.56 ± 15.16&lt;/td&gt; &lt;td align="left"&gt;2.049&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;tg32&lt;/td&gt; &lt;td align="left"&gt;118.95 ± 0.21&lt;/td&gt; &lt;td align="left"&gt;79.74 ± 0.43&lt;/td&gt; &lt;td align="left"&gt;0.670&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d4096&lt;/td&gt; &lt;td align="left"&gt;1324.28 ± 46.34&lt;/td&gt; &lt;td align="left"&gt;3361.11 ± 12.95&lt;/td&gt; &lt;td align="left"&gt;2.538&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d4096&lt;/td&gt; &lt;td align="left"&gt;98.76 ± 5.75&lt;/td&gt; &lt;td align="left"&gt;74.63 ± 0.15&lt;/td&gt; &lt;td align="left"&gt;0.756&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d8192&lt;/td&gt; &lt;td align="left"&gt;1107.91 ± 11.12&lt;/td&gt; &lt;td align="left"&gt;3147.73 ± 15.77&lt;/td&gt; &lt;td align="left"&gt;2.841&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d8192&lt;/td&gt; &lt;td align="left"&gt;94.19 ± 1.85&lt;/td&gt; &lt;td align="left"&gt;69.49 ± 1.12&lt;/td&gt; &lt;td align="left"&gt;0.738&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="left"&gt;733.77 ± 54.67&lt;/td&gt; &lt;td align="left"&gt;2685.54 ± 5.76&lt;/td&gt; &lt;td align="left"&gt;3.660&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d16384&lt;/td&gt; &lt;td align="left"&gt;80.68 ± 2.49&lt;/td&gt; &lt;td align="left"&gt;64.02 ± 0.72&lt;/td&gt; &lt;td align="left"&gt;0.794&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="left"&gt;518.68 ± 17.73&lt;/td&gt; &lt;td align="left"&gt;2055.34 ± 20.43&lt;/td&gt; &lt;td align="left"&gt;3.963&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d32768&lt;/td&gt; &lt;td align="left"&gt;69.94 ± 4.19&lt;/td&gt; &lt;td align="left"&gt;55.96 ± 0.07&lt;/td&gt; &lt;td align="left"&gt;0.800&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;871.16 ± 31.85&lt;/td&gt; &lt;td align="left"&gt;1689.47 ± 107.67&lt;/td&gt; &lt;td align="left"&gt;1.939&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;tg32&lt;/td&gt; &lt;td align="left"&gt;62.85 ± 0.36&lt;/td&gt; &lt;td align="left"&gt;52.87 ± 1.70&lt;/td&gt; &lt;td align="left"&gt;0.841&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d4096&lt;/td&gt; &lt;td align="left"&gt;643.32 ± 12.00&lt;/td&gt; &lt;td align="left"&gt;1733.41 ± 5.19&lt;/td&gt; &lt;td align="left"&gt;2.694&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d4096&lt;/td&gt; &lt;td align="left"&gt;56.48 ± 0.72&lt;/td&gt; &lt;td align="left"&gt;51.02 ± 0.65&lt;/td&gt; &lt;td align="left"&gt;0.903&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d8192&lt;/td&gt; &lt;td align="left"&gt;516.77 ± 7.33&lt;/td&gt; &lt;td align="left"&gt;1705.93 ± 7.89&lt;/td&gt; &lt;td align="left"&gt;3.301&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d8192&lt;/td&gt; &lt;td align="left"&gt;50.79 ± 1.37&lt;/td&gt; &lt;td align="left"&gt;48.46 ± 0.53&lt;/td&gt; &lt;td align="left"&gt;0.954&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="left"&gt;351.42 ± 7.31&lt;/td&gt; &lt;td align="left"&gt;1514.78 ± 5.66&lt;/td&gt; &lt;td align="left"&gt;4.310&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d16384&lt;/td&gt; &lt;td align="left"&gt;46.20 ± 1.17&lt;/td&gt; &lt;td align="left"&gt;44.78 ± 0.07&lt;/td&gt; &lt;td align="left"&gt;0.969&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="left"&gt;235.87 ± 2.88&lt;/td&gt; &lt;td align="left"&gt;1221.23 ± 7.85&lt;/td&gt; &lt;td align="left"&gt;5.178&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d32768&lt;/td&gt; &lt;td align="left"&gt;40.22 ± 0.29&lt;/td&gt; &lt;td align="left"&gt;38.76 ± 0.06&lt;/td&gt; &lt;td align="left"&gt;0.964&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;1656.65 ± 86.70&lt;/td&gt; &lt;td align="left"&gt;2933.39 ± 9.43&lt;/td&gt; &lt;td align="left"&gt;1.771&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;tg32&lt;/td&gt; &lt;td align="left"&gt;84.50 ± 0.87&lt;/td&gt; &lt;td align="left"&gt;59.95 ± 0.26&lt;/td&gt; &lt;td align="left"&gt;0.709&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d4096&lt;/td&gt; &lt;td align="left"&gt;938.23 ± 29.08&lt;/td&gt; &lt;td align="left"&gt;2537.98 ± 7.17&lt;/td&gt; &lt;td align="left"&gt;2.705&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d4096&lt;/td&gt; &lt;td align="left"&gt;67.70 ± 2.34&lt;/td&gt; &lt;td align="left"&gt;52.70 ± 0.75&lt;/td&gt; &lt;td align="left"&gt;0.778&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d8192&lt;/td&gt; &lt;td align="left"&gt;681.07 ± 20.63&lt;/td&gt; &lt;td align="left"&gt;2246.86 ± 6.45&lt;/td&gt; &lt;td align="left"&gt;3.299&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d8192&lt;/td&gt; &lt;td align="left"&gt;61.06 ± 6.02&lt;/td&gt; &lt;td align="left"&gt;44.48 ± 0.34&lt;/td&gt; &lt;td align="left"&gt;0.728&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="left"&gt;356.12 ± 16.62&lt;/td&gt; &lt;td align="left"&gt;1772.41 ± 10.58&lt;/td&gt; &lt;td align="left"&gt;4.977&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d16384&lt;/td&gt; &lt;td align="left"&gt;43.32 ± 3.04&lt;/td&gt; &lt;td align="left"&gt;37.10 ± 0.05&lt;/td&gt; &lt;td align="left"&gt;0.856&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="left"&gt;223.23 ± 12.23&lt;/td&gt; &lt;td align="left"&gt;1252.10 ± 2.16&lt;/td&gt; &lt;td align="left"&gt;5.609&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d32768&lt;/td&gt; &lt;td align="left"&gt;35.09 ± 5.53&lt;/td&gt; &lt;td align="left"&gt;27.82 ± 0.01&lt;/td&gt; &lt;td align="left"&gt;0.793&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;684.35 ± 15.08&lt;/td&gt; &lt;td align="left"&gt;2267.08 ± 6.38&lt;/td&gt; &lt;td align="left"&gt;3.313&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;tg32&lt;/td&gt; &lt;td align="left"&gt;46.82 ± 11.44&lt;/td&gt; &lt;td align="left"&gt;29.40 ± 0.02&lt;/td&gt; &lt;td align="left"&gt;0.628&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d4096&lt;/td&gt; &lt;td align="left"&gt;633.50 ± 3.78&lt;/td&gt; &lt;td align="left"&gt;2094.87 ± 11.61&lt;/td&gt; &lt;td align="left"&gt;3.307&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d4096&lt;/td&gt; &lt;td align="left"&gt;54.66 ± 0.74&lt;/td&gt; &lt;td align="left"&gt;28.31 ± 0.10&lt;/td&gt; &lt;td align="left"&gt;0.518&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d8192&lt;/td&gt; &lt;td align="left"&gt;496.85 ± 21.23&lt;/td&gt; &lt;td align="left"&gt;1906.26 ± 4.45&lt;/td&gt; &lt;td align="left"&gt;3.837&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d8192&lt;/td&gt; &lt;td align="left"&gt;51.15 ± 0.85&lt;/td&gt; &lt;td align="left"&gt;27.53 ± 0.04&lt;/td&gt; &lt;td align="left"&gt;0.538&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="left"&gt;401.98 ± 4.97&lt;/td&gt; &lt;td align="left"&gt;1634.82 ± 6.67&lt;/td&gt; &lt;td align="left"&gt;4.067&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d16384&lt;/td&gt; &lt;td align="left"&gt;47.91 ± 0.18&lt;/td&gt; &lt;td align="left"&gt;26.03 ± 0.03&lt;/td&gt; &lt;td align="left"&gt;0.543&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="left"&gt;293.33 ± 2.23&lt;/td&gt; &lt;td align="left"&gt;1302.32 ± 4.58&lt;/td&gt; &lt;td align="left"&gt;4.440&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 7B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.54 GiB&lt;/td&gt; &lt;td align="left"&gt;7.62 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d32768&lt;/td&gt; &lt;td align="left"&gt;40.78 ± 0.42&lt;/td&gt; &lt;td align="left"&gt;22.08 ± 0.03&lt;/td&gt; &lt;td align="left"&gt;0.541&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;339.64 ± 21.28&lt;/td&gt; &lt;td align="left"&gt;841.44 ± 12.67&lt;/td&gt; &lt;td align="left"&gt;2.477&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;tg32&lt;/td&gt; &lt;td align="left"&gt;37.79 ± 3.84&lt;/td&gt; &lt;td align="left"&gt;22.59 ± 0.11&lt;/td&gt; &lt;td align="left"&gt;0.598&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d4096&lt;/td&gt; &lt;td align="left"&gt;241.85 ± 6.50&lt;/td&gt; &lt;td align="left"&gt;749.08 ± 2.10&lt;/td&gt; &lt;td align="left"&gt;3.097&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d4096&lt;/td&gt; &lt;td align="left"&gt;27.22 ± 2.67&lt;/td&gt; &lt;td align="left"&gt;20.10 ± 0.01&lt;/td&gt; &lt;td align="left"&gt;0.738&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d8192&lt;/td&gt; &lt;td align="left"&gt;168.44 ± 4.12&lt;/td&gt; &lt;td align="left"&gt;680.95 ± 1.38&lt;/td&gt; &lt;td align="left"&gt;4.043&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d8192&lt;/td&gt; &lt;td align="left"&gt;29.13 ± 0.14&lt;/td&gt; &lt;td align="left"&gt;18.78 ± 0.07&lt;/td&gt; &lt;td align="left"&gt;0.645&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="left"&gt;122.06 ± 9.23&lt;/td&gt; &lt;td align="left"&gt;565.44 ± 1.47&lt;/td&gt; &lt;td align="left"&gt;4.632&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d16384&lt;/td&gt; &lt;td align="left"&gt;20.96 ± 1.20&lt;/td&gt; &lt;td align="left"&gt;16.47 ± 0.01&lt;/td&gt; &lt;td align="left"&gt;0.786&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;418.84 ± 0.53&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B Q4_K&lt;/td&gt; &lt;td align="left"&gt;67.85 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47 B&lt;/td&gt; &lt;td align="left"&gt;tg32 @ d32768&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;13.19 ± 0.01&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;From the data here we can see PP on the DGX SPARK is ~3.35x faster than the M4 MAX, while TG ~0.73x. Interesting as MBW on SPARK is ~273GB/s and MAX ~546GB/s.&lt;/p&gt; &lt;p&gt;So, here is my question for &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;. Inference performance is really important, but how much does PP really matter in all these discussions compared to TG? Also, yes, there is another important factor and that is price.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noble00_"&gt; /u/Noble00_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7k7zz/dgx_spark_compiled_llamacpp_benchmarks_compared/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7k7zz/dgx_spark_compiled_llamacpp_benchmarks_compared/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7k7zz/dgx_spark_compiled_llamacpp_benchmarks_compared/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T19:12:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7liam</id>
    <title>Llamacpp Model Loader GUI for noobs</title>
    <updated>2025-10-15T20:01:21+00:00</updated>
    <author>
      <name>/u/CabinetNational3461</name>
      <uri>https://old.reddit.com/user/CabinetNational3461</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7liam/llamacpp_model_loader_gui_for_noobs/"&gt; &lt;img alt="Llamacpp Model Loader GUI for noobs" src="https://preview.redd.it/msr7wyiwxbvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cf3cb84527273f0b3b22fdcb8c887bfed231273" title="Llamacpp Model Loader GUI for noobs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I a noob at this LLM stuff and recently switched from LM Studio/Ollama to llamacpp and loving it so far as far as speed/performance. One thing I dislike is how tedious it is to modify and play around with the parameters and using command line so I vibe coded some python code using Gemini 2.5 Pro for something easier to mess around with. I attached the code, sample model files and commands. I am using window 10 FYI. I had Gemini gen up some doc as am not much of a writer so here it is:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Introduction&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Llama.cpp Model Launcher is a powerful desktop GUI that transforms the complex llama-server.exe command line into an intuitive, point-and-click experience. Effortlessly launch models, dynamically edit every parameter in a visual editor, and manage a complete library of your model configurations. Designed for both beginners and power users, it provides a centralized dashboard to streamline your workflow and unlock the full potential of Llama.cpp without ever touching a terminal.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Intuitive Graphical Control:&lt;/strong&gt; Ditch the terminal. Launch, manage, and shut down the llama-server with simple, reliable button clicks, eliminating the risk of command-line typos.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic Parameter Editor:&lt;/strong&gt; Visually build and modify launch commands in real-time. Adjust values in text fields, toggle flags with checkboxes, and add new parameters on the fly without memorizing syntax.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Configuration Management:&lt;/strong&gt; Build and maintain a complete library of your models. Effortlessly add new profiles, edit names and parameters, and delete old configurations, all from within the application.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-Time Monitoring:&lt;/strong&gt; Instantly know the server's status with a colored indicator (Red, Yellow, Green) and watch the live output log to monitor model loading, API requests, and potential errors as they happen.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Integrated Documentation:&lt;/strong&gt; Access a complete Llama.cpp command reference and a formatted user guide directly within the interface, eliminating the need to search for external help.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Running the Application&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are two primary ways to run this application:&lt;/p&gt; &lt;p&gt;Method 1: Run from Python Source&lt;/p&gt; &lt;p&gt;This method is ideal for developers or users who have Python installed and are comfortable with a code editor.&lt;/p&gt; &lt;p&gt;Method 2: Compile to a Standalone Executable (.exe)&lt;/p&gt; &lt;p&gt;This method packages the application into a single `.exe` file that can be run on any Windows machine without needing Python installed.&lt;/p&gt; &lt;p&gt;code: &lt;a href="https://drive.google.com/file/d/1NWU1Kp_uVLmhErqgaSv5pGHwqy5BUUdp/view?usp=drive_link"&gt;https://drive.google.com/file/d/1NWU1Kp_uVLmhErqgaSv5pGHwqy5BUUdp/view?usp=drive_link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;help_file: &lt;a href="https://drive.google.com/file/d/1556aMxnNxoaZFzJyAw_ZDgfwkrkK7kTP/view?usp=drive_link"&gt;https://drive.google.com/file/d/1556aMxnNxoaZFzJyAw_ZDgfwkrkK7kTP/view?usp=drive_link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;sample_moldel_commands: &lt;a href="https://drive.google.com/file/d/1ksDD1wcEA27LCVqTOnQrzU9yZe1iWjd_/view?usp=drive_link"&gt;https://drive.google.com/file/d/1ksDD1wcEA27LCVqTOnQrzU9yZe1iWjd_/view?usp=drive_link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope someone find it useful&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CabinetNational3461"&gt; /u/CabinetNational3461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/msr7wyiwxbvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7liam/llamacpp_model_loader_gui_for_noobs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7liam/llamacpp_model_loader_gui_for_noobs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T20:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7k6e5</id>
    <title>NVIDIA DGX Spark™ + Apple Mac Studio = 4x Faster LLM Inference with EXO 1.0</title>
    <updated>2025-10-15T19:10:38+00:00</updated>
    <author>
      <name>/u/Careless_Garlic1438</name>
      <uri>https://old.reddit.com/user/Careless_Garlic1438</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well this is quite interesting!&lt;/p&gt; &lt;p&gt;&lt;a href="https://blog.exolabs.net/nvidia-dgx-spark/"&gt;https://blog.exolabs.net/nvidia-dgx-spark/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careless_Garlic1438"&gt; /u/Careless_Garlic1438 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7k6e5/nvidia_dgx_spark_apple_mac_studio_4x_faster_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7k6e5/nvidia_dgx_spark_apple_mac_studio_4x_faster_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7k6e5/nvidia_dgx_spark_apple_mac_studio_4x_faster_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T19:10:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7rchv</id>
    <title>LLama.cpp GPU Support on Android Device</title>
    <updated>2025-10-15T23:56:47+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7rchv/llamacpp_gpu_support_on_android_device/"&gt; &lt;img alt="LLama.cpp GPU Support on Android Device" src="https://a.thumbs.redditmedia.com/VeG6UZmL3mBW6TmARr_WVpKD3xQ0T0XIPkiAj730lQ8.jpg" title="LLama.cpp GPU Support on Android Device" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have figured out a way to Use Android - GPU for LLAMA.CPP&lt;br /&gt; I mean it is not what you would expect like boost in tk/s but it is good for background work mostly&lt;/p&gt; &lt;p&gt;and i didn't saw much of a difference in both GPU and CPU mode&lt;/p&gt; &lt;p&gt;i was using &lt;a href="https://huggingface.co/Menlo/Lucy-128k-gguf/tree/main"&gt;lucy-128k&lt;/a&gt; model, i mean i am also using k-v cache + state file saving so yaa that's all that i got&lt;br /&gt; love to hear more about it from you guys : )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o7rchv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7rchv/llamacpp_gpu_support_on_android_device/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7rchv/llamacpp_gpu_support_on_android_device/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T23:56:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7l1io</id>
    <title>LM Studio and VL models</title>
    <updated>2025-10-15T19:43:32+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LM Studio currently downsizes images for VL inference, which can significantly hurt OCR performance. &lt;/p&gt; &lt;p&gt;v0.3.6 release notes: &lt;strong&gt;&amp;quot;Added image auto-resizing for vision model inputs, hardcoded to 500px width while keeping the aspect ratio.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmstudio.ai/blog/lmstudio-v0.3.6"&gt;https://lmstudio.ai/blog/lmstudio-v0.3.6&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Related GitHub reports:&lt;br /&gt; &lt;a href="https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/941"&gt;https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/941&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/880"&gt;https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/880&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/967"&gt;https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/967&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/990"&gt;https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/990&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If your image is a dense page of text and the VL model seems to underperform, LM Studio preprocessing is likely the culprit. Consider using a different app.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7l1io/lm_studio_and_vl_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7l1io/lm_studio_and_vl_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7l1io/lm_studio_and_vl_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T19:43:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7brfl</id>
    <title>DGX Spark is just a more expensive (probably underclocked) AGX Thor</title>
    <updated>2025-10-15T13:58:50+00:00</updated>
    <author>
      <name>/u/waiting_for_zban</name>
      <uri>https://old.reddit.com/user/waiting_for_zban</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It was weird not to see any detailed specs on Nvidia's DGX Spark spec sheet. No mention of how many cuda/tensor cores (they mention the cuda core counts only in the &lt;a href="https://docs.nvidia.com/dgx/dgx-spark/dgx-spark.pdf"&gt;DGX Guide&lt;/a&gt; for developers but still why so buried). This is in contrast to AGX Thor, where they list in details the specs. So i assumed that the DGX Spark is a nerfed version of the AGX Thor, given that NVidia's marketing states that the Thor throughput is 2000TFLOPs and the Spark is 1000TFLOPs. Thor has similar ecosystem too and tech stack (ie Nvidia branded Ubuntu). &lt;/p&gt; &lt;p&gt;But then &lt;a href="https://www.theregister.com/2025/10/14/dgx_spark_review"&gt;the register in their review yesterday&lt;/a&gt;, actually listed the number of cuda cores, tensor cores, and RT cores. To my surprise the spark packs 2x cuda cores and 2x tensor cores, even 48 rt cores than the THor. &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;DGX Spark&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;** AGX Thor**&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;TDP&lt;/td&gt; &lt;td align="left"&gt;~140 W&lt;/td&gt; &lt;td align="left"&gt;40 – 130 W&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA Cores&lt;/td&gt; &lt;td align="left"&gt;6 144&lt;/td&gt; &lt;td align="left"&gt;2 560&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tensor Cores&lt;/td&gt; &lt;td align="left"&gt;192 (unofficial really)&lt;/td&gt; &lt;td align="left"&gt;96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Peak FP4 (sparse)&lt;/td&gt; &lt;td align="left"&gt;≈ 1 000 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;≈ 2 070 TFLOPS&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And now I have more questions than answers. The &lt;a href="https://i.ibb.co/C3BGBy0T/2025-10-15-15-27.png"&gt;benchmarks of the Thor&lt;/a&gt; actually show numbers similar &lt;a href="https://www.youtube.com/watch?v=FVPE5zCte_E"&gt;to the Ryzen AI Max and M4 Pro&lt;/a&gt;, so again more confusion, because the Thor should be &amp;quot;twice as fast for AI&amp;quot; than the Spark. This goes to show that the metric of &amp;quot;AI TFLOPS&amp;quot; is absolutely useless, because also on paper Spark packs more cores. Maybe it matters for training/finetuning, but then we would have observed this for inference too. &lt;/p&gt; &lt;p&gt;The only explanation is that Nvidia underclocked the DGX Spark (some reviewers like NetworkChuck reported very hot devices) so the small form factor is not helping take full advantage of the hardware, and I wonder how it will fair with continuous usage (ie finetuning / training). We've seen this with the Ryzen AI where the EVO-x2 takes off to space with those fans.&lt;br /&gt; I saw some benchmarks with vLLM and &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16578"&gt;batched llama.cpp&lt;/a&gt; being very good, which is probably where the extra cores that Spark has would shine compared to Mac or Ryzen AI or the Thor. &lt;/p&gt; &lt;p&gt;Nonetheless, the value offering for the Spark (4k $) is nearly similar (at least in observed performance) to that of the Thor (3.5k $), yet it costs more. If you go by &amp;quot;AI TFLOPS&amp;quot; on paper the Thor is a better deal, and a bit cheaper.&lt;br /&gt; If you go by raw numbers, the Spark (probably if properly overclocked) might give you on the long term better bang for bucks (good luck with warranty though). &lt;/p&gt; &lt;p&gt;But if you want inference: get a Ryzen AI Max if you're on a budget, or splurge on a Mac. If you have space and don't mind leeching power, probably DDR4 servers + old AMD GPUs are the way to go, or even the just announced M5 (with that meager 150GB/s memory bandwidth). &lt;/p&gt; &lt;p&gt;For batched inference, we need better data for comparison. But from what I have seen so far, it's a tough market for the DGX Spark, and Nvidia marketing is not helping at all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waiting_for_zban"&gt; /u/waiting_for_zban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7brfl/dgx_spark_is_just_a_more_expensive_probably/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7brfl/dgx_spark_is_just_a_more_expensive_probably/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7brfl/dgx_spark_is_just_a_more_expensive_probably/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T13:58:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7j6ri</id>
    <title>Microcenter has RTX3090Ti’s</title>
    <updated>2025-10-15T18:34:30+00:00</updated>
    <author>
      <name>/u/flanconleche</name>
      <uri>https://old.reddit.com/user/flanconleche</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7j6ri/microcenter_has_rtx3090tis/"&gt; &lt;img alt="Microcenter has RTX3090Ti’s" src="https://b.thumbs.redditmedia.com/Tx7CGhbxlRoqyuJQwDyZMIXkM4c-MnruHZbkvMAEyJk.jpg" title="Microcenter has RTX3090Ti’s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if anyone cares but my local Microcenter has refurb RTX 3090Ti’s for $800. If your on the market for 3090’s it might be worth checking your local Microcenter. The used market prices have gone up to $900 and at Least you have some sort of warranty. &lt;/p&gt; &lt;p&gt;Also got a chance to play with the dgx spark, that thing is really cool. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/flanconleche"&gt; /u/flanconleche &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o7j6ri"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7j6ri/microcenter_has_rtx3090tis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7j6ri/microcenter_has_rtx3090tis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T18:34:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7kkf0</id>
    <title>Poor GPU Club : 8GB VRAM - MOE models' t/s with llama.cpp</title>
    <updated>2025-10-15T19:25:03+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Continuation to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nyxmci/poor_gpu_club_8gb_vram_qwen330ba3b_gptoss20b_ts/"&gt;my previous thread&lt;/a&gt;. This time I got better pp numbers with tg because of additional parameters. Tried with latest llama.cpp. &lt;/p&gt; &lt;p&gt;&lt;sup&gt;My System Info: (&lt;/sup&gt;&lt;strong&gt;&lt;sup&gt;8GB VRAM &amp;amp; 32GB RAM&lt;/sup&gt;&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Intel(R&lt;/sup&gt; Core(TM) i7-14700HX 2.10 GHz | 32 GB RAM | 64-bit OS, x64-based processor | NVIDIA GeForce RTX 4060 Laptop GPU |) &lt;strong&gt;&lt;sup&gt;Cores - 20 | Logical Processors - 28&lt;/sup&gt;&lt;/strong&gt;&lt;sup&gt;.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-UD-Q4_K_XL - 33 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Qwen3-30B-A3B-UD-Q4_K_XL.gguf -ngl 99 -ncmoe 29 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 160.45 ± 18.06 | | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 33.73 ± 0.74 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;gpt-oss-20b-mxfp4 - 42 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\gpt-oss-20b-mxfp4.gguf -ngl 99 -ncmoe 10 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 823.93 ± 109.69 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 42.06 ± 0.56 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ling-lite-1.5-2507.i1-Q6_K - 34 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Ling-lite-1.5-2507.i1-Q6_K.gguf -ngl 99 -ncmoe 15 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | bailingmoe 16B Q6_K | 14.01 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 585.52 ± 18.03 | | bailingmoe 16B Q6_K | 14.01 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 34.38 ± 1.54 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ling-lite-1.5-2507.i1-Q5_K_M - 50 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Ling-lite-1.5-2507.i1-Q5_K_M.gguf -ngl 99 -ncmoe 12 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | bailingmoe 16B Q5_K - Medium | 11.87 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 183.79 ± 16.55 | | bailingmoe 16B Q5_K - Medium | 11.87 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 50.03 ± 0.46 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ling-Coder-lite.i1-Q6_K - 35 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Ling-Coder-lite.i1-Q6_K.gguf -ngl 99 -ncmoe 15 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | bailingmoe 16B Q6_K | 14.01 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 470.17 ± 113.93 | | bailingmoe 16B Q6_K | 14.01 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 35.05 ± 3.33 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ling-Coder-lite.i1-Q5_K_M - 47 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Ling-Coder-lite.i1-Q5_K_M.gguf -ngl 99 -ncmoe 14 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | bailingmoe 16B Q5_K - Medium | 11.87 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 593.95 ± 91.55 | | bailingmoe 16B Q5_K - Medium | 11.87 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 47.39 ± 0.68 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;SmallThinker-21B-A3B-Instruct-QAT.Q4_K_M - 34 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\SmallThinker-21B-A3B-Instruct-QAT.Q4_K_M.gguf -ngl 99 -ncmoe 27 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | smallthinker 20B Q4_K - Medium | 12.18 GiB | 21.51 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 512.92 ± 109.33 | | smallthinker 20B Q4_K - Medium | 12.18 GiB | 21.51 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 34.75 ± 0.22 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;SmallThinker-21BA3B-Instruct-IQ4_XS - 38 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\SmallThinker-21BA3B-Instruct-IQ4_XS.gguf -ngl 99 -ncmoe 25 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | smallthinker 20B IQ4_XS - 4.25 bpw | 10.78 GiB | 21.51 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 635.01 ± 105.46 | | smallthinker 20B IQ4_XS - 4.25 bpw | 10.78 GiB | 21.51 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 37.47 ± 0.37 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;ERNIE-4.5-21B-A3B-PT-UD-Q4_K_XL - 44 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\ERNIE-4.5-21B-A3B-PT-UD-Q4_K_XL.gguf -ngl 99 -ncmoe 14 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | ernie4_5-moe 21B.A3B Q4_K - Medium | 11.91 GiB | 21.83 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 568.99 ± 134.16 | | ernie4_5-moe 21B.A3B Q4_K - Medium | 11.91 GiB | 21.83 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 44.83 ± 1.72 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Phi-mini-MoE-instruct-Q8_0 - 65 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Phi-mini-MoE-instruct-Q8_0.gguf -ngl 99 -ncmoe 4 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | phimoe 16x3.8B Q8_0 | 7.58 GiB | 7.65 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 2570.72 ± 48.54 | | phimoe 16x3.8B Q8_0 | 7.58 GiB | 7.65 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 65.41 ± 0.19 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'll be updating this thread whenever I get optimization tips &amp;amp; tricks from others AND I'll be including additional results here with updated commands. Also whenever new MOE models get released. Currently I'm checking bunch more MOE models, I'll add those here in this week. Thanks&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Updates : To be updated&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;sup&gt;My Upcoming threads (Planned&lt;/sup&gt; :)&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;sup&gt;8GB VRAM - Dense models' t/s with llama.cpp&lt;/sup&gt;&lt;/li&gt; &lt;li&gt;&lt;sup&gt;8GB VRAM - MOE &amp;amp; Dense models' t/s with llama.cpp - CPU only&lt;/sup&gt;&lt;/li&gt; &lt;li&gt;&lt;sup&gt;8GB VRAM - MOE &amp;amp; Dense models' t/s with ik\&lt;/sup&gt;llama.cpp (Still I'm looking for help on ik_llama.cpp))&lt;/li&gt; &lt;li&gt;&lt;sup&gt;8GB VRAM - MOE &amp;amp; Dense models' t/s with ik\&lt;/sup&gt;llama.cpp - CPU only)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7kkf0/poor_gpu_club_8gb_vram_moe_models_ts_with_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7kkf0/poor_gpu_club_8gb_vram_moe_models_ts_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7kkf0/poor_gpu_club_8gb_vram_moe_models_ts_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T19:25:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7b1i3</id>
    <title>Looks like the DGX Spark a bad 4K investment vs Mac</title>
    <updated>2025-10-15T13:29:44+00:00</updated>
    <author>
      <name>/u/meshreplacer</name>
      <uri>https://old.reddit.com/user/meshreplacer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b1i3/looks_like_the_dgx_spark_a_bad_4k_investment_vs/"&gt; &lt;img alt="Looks like the DGX Spark a bad 4K investment vs Mac" src="https://b.thumbs.redditmedia.com/pPegjJ4GiV-jDUO1MueuQ5ieZJJkj24-yiTMqhNj4TQ.jpg" title="Looks like the DGX Spark a bad 4K investment vs Mac" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/om6zy3z42avf1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=31dff7de8ac355eff8c2962f8f03084cec0ada0c"&gt;https://preview.redd.it/om6zy3z42avf1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=31dff7de8ac355eff8c2962f8f03084cec0ada0c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looks like 4K gets you a slower more expensive product limited In what you can do. I could just imagine how bad it would compare to an M4 128gb Mac Studio. Day late dollar short.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/meshreplacer"&gt; /u/meshreplacer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b1i3/looks_like_the_dgx_spark_a_bad_4k_investment_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b1i3/looks_like_the_dgx_spark_a_bad_4k_investment_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b1i3/looks_like_the_dgx_spark_a_bad_4k_investment_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T13:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7miyx</id>
    <title>Just ordered new 3090 TI from MicroCenter 🤔</title>
    <updated>2025-10-15T20:39:56+00:00</updated>
    <author>
      <name>/u/GravyPoo</name>
      <uri>https://old.reddit.com/user/GravyPoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"&gt; &lt;img alt="Just ordered new 3090 TI from MicroCenter 🤔" src="https://preview.redd.it/mzozs3957cvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb5a83e22f624acd437f0414ec334d5a460f063d" title="Just ordered new 3090 TI from MicroCenter 🤔" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GravyPoo"&gt; /u/GravyPoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mzozs3957cvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T20:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7pe3g</id>
    <title>Matthew McConaughey LLaMa</title>
    <updated>2025-10-15T22:34:38+00:00</updated>
    <author>
      <name>/u/ContextualNina</name>
      <uri>https://old.reddit.com/user/ContextualNina</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We thought it would be fun to build something for Matthew McConaughey, based on his recent Rogan podcast interview.&lt;/p&gt; &lt;p&gt;&amp;quot;Matthew McConaughey says he wants a private LLM, fed only with his books, notes, journals, and aspirations, so he can ask it questions and get answers based solely on that information, without any outside influence.&amp;quot;&lt;/p&gt; &lt;p&gt;Pretty classic RAG/context engineering challenge, right? And we use a fine-tuned Llama model in this setup, which also happens to be the most factual and grounded LLM according to the FACTS benchmark (link in comment), Llama-3-Glm-V2. &lt;/p&gt; &lt;p&gt;Here's how we built it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;We found public writings, podcast transcripts, etc, as our base materials to upload as a proxy for the all the information Matthew mentioned in his interview (of course our access to such documents is very limited compared to his).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The agent ingested those to use as a source of truth&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;We configured the agent to the specifications that Matthew asked for in his interview. Note that we already have the most grounded language model (GLM) as the generator, and multiple guardrails against hallucinations, but additional response qualities can be configured via prompt.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Now, when you converse with the agent, it knows to only pull from those sources instead of making things up or use its other training data.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;However, the model retains its overall knowledge of how the world works, and can reason about the responses, in addition to referencing uploaded information verbatim.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The agent is powered by Contextual AI's APIs, and we deployed the full web application on Vercel to create a publicly accessible demo.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ContextualNina"&gt; /u/ContextualNina &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.alrightalrightalright.ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe3g/matthew_mcconaughey_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe3g/matthew_mcconaughey_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T22:34:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7jy1o</id>
    <title>GLM 4.6 is the new top open weight model on Design Arena</title>
    <updated>2025-10-15T19:01:56+00:00</updated>
    <author>
      <name>/u/Helpful_Jacket8953</name>
      <uri>https://old.reddit.com/user/Helpful_Jacket8953</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7jy1o/glm_46_is_the_new_top_open_weight_model_on_design/"&gt; &lt;img alt="GLM 4.6 is the new top open weight model on Design Arena" src="https://external-preview.redd.it/kjHAP1F1s2g_NoQgjQsq0SUMOf5owow_LkPL9Q--SUo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca33eecb340f9f09d0b23530fc35bb3db655ab0f" title="GLM 4.6 is the new top open weight model on Design Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/hepvwbezobvf1.png?width=1877&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87d242fe8af470adee79fa9b604930404192741c"&gt;https://preview.redd.it/hepvwbezobvf1.png?width=1877&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87d242fe8af470adee79fa9b604930404192741c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM models make up 20% of the top 10 and beat every iteration of GPT-5 except minimal. It has surpassed DeepSeek, Qwen, and even Sonnet 4 and 3.7. If their front-end performance continues to improve at this pace for GLM 5, they could break in the top 5. China is approaching SOTA (&lt;a href="https://www.designarena.ai/"&gt;https://www.designarena.ai/&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Helpful_Jacket8953"&gt; /u/Helpful_Jacket8953 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7jy1o/glm_46_is_the_new_top_open_weight_model_on_design/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7jy1o/glm_46_is_the_new_top_open_weight_model_on_design/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7jy1o/glm_46_is_the_new_top_open_weight_model_on_design/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T19:01:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7ep8a</id>
    <title>Apple M5 Officially Announced: is this a big deal?</title>
    <updated>2025-10-15T15:48:34+00:00</updated>
    <author>
      <name>/u/ontorealist</name>
      <uri>https://old.reddit.com/user/ontorealist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(&lt;em&gt;Edit: To be clear, only the *&lt;/em&gt;base** M5 has been announced. My question is primarily about whether M5 &lt;strong&gt;Pro&lt;/strong&gt; and higher-end M5 chips with more high bandwidth memory, etc. are more compelling compared to PC builds for inference given the confirmed specs for the base M5.*)&lt;/p&gt; &lt;p&gt;If I’m understanding correctly:&lt;/p&gt; &lt;p&gt;• &lt;strong&gt;3.5x faster AI performance&lt;/strong&gt; compared to the M4 (though the exact neural engine improvements aren’t yet confirmed)&lt;br /&gt; • &lt;strong&gt;153 GB/s memory bandwidth&lt;/strong&gt; (~30% improvement)&lt;br /&gt; • &lt;strong&gt;4x increase in GPU compute&lt;/strong&gt;&lt;br /&gt; • &lt;strong&gt;Unified memory architecture&lt;/strong&gt;, eliminating the need for CPU↔GPU data transfers, as with previous gens&lt;/p&gt; &lt;p&gt;Even if the neural accelerators on the base M5 aren’t dedicated matmul units (which seems unlikely given the A19 Pro), will this translate into noticeably faster prompt processing speeds?&lt;/p&gt; &lt;p&gt;At $1,600 for an entry-level 16GB M5 ($2K for 32GB), serious inference workloads feels limiting, especially when compared to refurbished M-series models with more RAM. That said, it seems like a solid choice for new users exploring local AI experiences, particularly when working with sub-30B models for RAG or large context windows at faster speeds. That, along with another LM Studio feature in the press release, is a good sign, no? &lt;/p&gt; &lt;p&gt;Do the specs / pricing represent a meaningful upgrade for anyone considering the M5 Pro, Max, or Ultra? I’d love to hear others’ thoughts.&lt;/p&gt; &lt;p&gt;Read the announcement &lt;a href="https://www.apple.com/newsroom/2025/10/apple-unleashes-m5-the-next-big-leap-in-ai-performance-for-apple-silicon/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ontorealist"&gt; /u/ontorealist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T15:48:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7mhf5</id>
    <title>Google &amp; Yale release C2S Scale, a Gemma-based model for cell analysis</title>
    <updated>2025-10-15T20:38:17+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! This is Omar, from the Gemma team.&lt;/p&gt; &lt;p&gt;I'm super excited to share this research based on Gemma. Today, we're releasing a 27B model for single-cell analysis. This model generated hypotheses about how cancer cells behave, and we were able to confirm the predictions with experimental validation in living cells. This reveals a promising new pathway for developing therapies to fight cancer. &lt;/p&gt; &lt;p&gt;This applications of open models for medical use cases are super exciting for me. It's one of many examples of how open models can change the world&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B"&gt;https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2"&gt;https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/"&gt;https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T20:38:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7pe1u</id>
    <title>gigaResearch</title>
    <updated>2025-10-15T22:34:35+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"&gt; &lt;img alt="gigaResearch" src="https://preview.redd.it/nb2hmgqircvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71c101f2683e8df117cbc2a9abd685bcac5cbce0" title="gigaResearch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nb2hmgqircvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T22:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o75kkb</id>
    <title>AI has replaced programmers… totally.</title>
    <updated>2025-10-15T08:37:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt; &lt;img alt="AI has replaced programmers… totally." src="https://preview.redd.it/bnnb2fb9m8vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1a55140b6915df726dfa4932943df64e43e7d94" title="AI has replaced programmers… totally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bnnb2fb9m8vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7b5i4</id>
    <title>Apple unveils M5</title>
    <updated>2025-10-15T13:34:26+00:00</updated>
    <author>
      <name>/u/Agreeable-Rest9162</name>
      <uri>https://old.reddit.com/user/Agreeable-Rest9162</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"&gt; &lt;img alt="Apple unveils M5" src="https://preview.redd.it/5ehnojlm2avf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbc46c6e19f88c18588d2f5384d7fb2dd4717f50" title="Apple unveils M5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following the iPhone 17 AI accelerators, most of us were expecting the same tech to be added to M5. Here it is! Lets see what M5 Pro &amp;amp; Max will add. The speedup from M4 to M5 seems to be around 3.5x for prompt processing. &lt;/p&gt; &lt;p&gt;Faster SSDs &amp;amp; RAM:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Additionally, with up to 2x faster SSD performance than the prior generation, the new 14-inch MacBook Pro lets users load a local LLM faster, and they can now choose up to 4TB of storage. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;150GB/s of unified memory bandwidth&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable-Rest9162"&gt; /u/Agreeable-Rest9162 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5ehnojlm2avf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T13:34:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7gpr8</id>
    <title>Got the DGX Spark - ask me anything</title>
    <updated>2025-10-15T17:02:50+00:00</updated>
    <author>
      <name>/u/sotech117</name>
      <uri>https://old.reddit.com/user/sotech117</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"&gt; &lt;img alt="Got the DGX Spark - ask me anything" src="https://preview.redd.it/9mr835ne4bvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42dc8e85dcff8b55d4174e98495bb8d2d144fd7d" title="Got the DGX Spark - ask me anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If there’s anything you want me to benchmark (or want to see in general), let me know, and I’ll try to reply to your comment. I will be playing with this all night trying a ton of different models I’ve always wanted to run. &lt;/p&gt; &lt;p&gt;(&amp;amp; shoutout to microcenter my goats!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sotech117"&gt; /u/sotech117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9mr835ne4bvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T17:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
