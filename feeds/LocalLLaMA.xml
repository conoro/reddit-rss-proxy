<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-19T12:49:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r8vi2t</id>
    <title>Chinese Modded 20gb 3080 REBAR bios?</title>
    <updated>2026-02-19T10:12:39+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey I bought a 20gb 3080 from china and noticed the card does not have rebar enabled, does anyone know if I can just flash a 10gb bios with rebar enabled or if I need a special 20gb version?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vi2t/chinese_modded_20gb_3080_rebar_bios/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vi2t/chinese_modded_20gb_3080_rebar_bios/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vi2t/chinese_modded_20gb_3080_rebar_bios/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T10:12:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8cc72</id>
    <title>Model: support GLM-OCR merged! LLama.cpp</title>
    <updated>2026-02-18T19:15:13+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19677"&gt;https://github.com/ggml-org/llama.cpp/pull/19677&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can't wait to test!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8cc72/model_support_glmocr_merged_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8cc72/model_support_glmocr_merged_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8cc72/model_support_glmocr_merged_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T19:15:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r87ou8</id>
    <title>UPDATE#3: repurposing 800 RX 580s converted to AI cluster</title>
    <updated>2026-02-18T16:30:14+00:00</updated>
    <author>
      <name>/u/rasbid420</name>
      <uri>https://old.reddit.com/user/rasbid420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey everyone, posting an update on the ETH mining farm conversion project. last time i posted we were still figuring out what to even do with 800 rx 580s (mix of 4gb and 8gb sapphire nitro+ and pulse cards) sitting in an old ethereum mining farm&lt;/p&gt; &lt;p&gt;so the tldr is we think we finally found a good use case. maybe two actually.&lt;/p&gt; &lt;p&gt;the fundamental problem with these gpus is the interdevice communication. they have good usable vram 8GB but low pcie speeds, low memory bandwith, and each card sitting on its a celeron g3950 board with 8gb of system ram. you cant do tensor parallelism across nodes with these things. we tried, its not happening. the latency between devices kills anything... so we had to completely rethink the approach. instead of trying to make them work together on one big model through parallelism on a node or even RPC in network, we treat each gpu as a completely independant inference worker. one model per gpu, one request at a time, working in parallel across a cluster.&lt;/p&gt; &lt;p&gt;getting llama.cpp to run on gfx803 polaris in 2026 is... an experience. rocm support for more than one card is dismal for these cards and the biggest issue still is &amp;quot;PCI-E ATOMICS support&amp;quot;... we can't build llama.cpp with a HIP backend because we have 6 cards on each rig and it doesn't see more than one card...&lt;/p&gt; &lt;p&gt;so we went with vulkan and tested and benchmarked internally all the possible permutations and combinations with vulkan / ubuntu&lt;/p&gt; &lt;p&gt;and came up with the most optimal settings to run and build llama.cpp's vulkan for rx580 support&lt;/p&gt; &lt;p&gt;so our dockerfile_v43 that builds the entire graphics stack from source looks like this:&lt;/p&gt; &lt;p&gt;- libdrm 2.4.121 from source&lt;/p&gt; &lt;p&gt;- wayland 1.22 from source&lt;/p&gt; &lt;p&gt;- mesa 24.2.0 from source with llvm 15 and the radv vulkan driver&lt;/p&gt; &lt;p&gt;- vulkan sdk 1.3.283&lt;/p&gt; &lt;p&gt;- then llama.cpp on top of all that&lt;/p&gt; &lt;p&gt;we had to build with GGML_NATIVE=ON because avx2/fma produces a binary that segfaults on every worker node because celerons dont have avx. we had to explicitly disable everything except sse4.2:&lt;/p&gt; &lt;p&gt;-DGGML_NATIVE=OFF -DGGML_AVX=OFF -DGGML_AVX2=OFF -DGGML_FMA=OFF -DGGML_F16C=OFF -DGGML_SSE42=ON&lt;/p&gt; &lt;p&gt;CXXFLAGS=&amp;quot;-march=x86-64 -mtune=generic&amp;quot;&lt;/p&gt; &lt;p&gt;the model we use is qwen3-vl-8b-instruct which is a visual language model. the q4 quantization fits on a single 8gb card with room for 6k context tokens. we run 4 tiers of quantization across the fleet: q4 on 1 gpu, q8 on 2 gpus, bf16 on 3 or 6 gpus for quality escalation AND / OR bigger context&lt;/p&gt; &lt;p&gt;&lt;strong&gt;use case #1: mass document OCR / visual document understanding&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;we can process large documents like textbooks, medical literature, legal docs for high quality text extractions. the pdf gets split into individual pages, each page gets converted to an image and sent to a seperate gpu for visual understanding. you can get 200 gpus to process 200 pages simultaneously.&lt;/p&gt; &lt;p&gt;our quality benchmark is a clinical opthalmology of 966 pages of dense medical terminology, complex diagrams, photographic plates, multi-column layouts, tables, cursive annotations. the works. doing this through openai api with a visual model costs about $12 per run. we do it for roughly $0.50 in electricity at our local hydro rate of $0.065/kwh. thats 24x cheaper on opex and the capex is essentially nothing because we already had the hardware sitting there from the mining days. cards cost us like $80 per 8gb of vram vs $365/gb if you compare with an h100.&lt;/p&gt; &lt;p&gt;quality wise, its honestly comparable for document understanding work. cursive text, messy handwriting, charts, tables, images, the quantized qwen3-vl handles it.&lt;/p&gt; &lt;p&gt;the escalation path goes: tier 1 (q4, 175 dpi) &amp;gt; tier 2 (q8, 200 dpi) &amp;gt; tier 3 (bf16, 250 dpi) &amp;gt; tier 4 (bf16 on 6 gpus, 300 dpi). after 3 retries we accept degraded quality if it's impossible work but it works suprisingly well... most pages resolve on tier 1, only the really nasty scans escalate up.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;use case #2: video frame analysis (work in progress)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;this is the next thing were working on. same architecture but for video. 60 seconds of video at ~13fps = 800 frames. distribute 800 frames across 800 gpus,&lt;/p&gt; &lt;p&gt;each one describes what it sees in that frame. then you do temporal clustering, entity tracking, event extraction, and build a scene summary on top&lt;/p&gt; &lt;p&gt;the idea is to provide an endpoint where users can send video data and get back structured visual analysis. you could build monitoring alerts, safety assessments, quality assurance checks on top of it. stuff that currently costs way too much through traditional api calls to be practical at scale&lt;/p&gt; &lt;p&gt;were still early on this one but the architecture should translate pretty directly from the document pipeline. the hard part will be the temporal synthesis layers on top.&lt;/p&gt; &lt;p&gt;anyway... thats where were at. the mining farm to ai cluster conversion has been a year of pain but we finally have something that we can call useful&lt;/p&gt; &lt;p&gt;the key advantage of this cluster is the low cost of text extraction from documents which in turn can should be fed into a RAG pipeline like a chatgpt window for embedding/vectorization/good high quality chat on top of that document&lt;/p&gt; &lt;p&gt;happy to hear any feedback or any further ideas about this&lt;/p&gt; &lt;p&gt;&lt;a href="https://hyperstract.com"&gt;https://hyperstract.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the system is capable of processing big pdfs of 400 pages per minute but please don't abuse it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rasbid420"&gt; /u/rasbid420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r87ou8/update3_repurposing_800_rx_580s_converted_to_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r87ou8/update3_repurposing_800_rx_580s_converted_to_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r87ou8/update3_repurposing_800_rx_580s_converted_to_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T16:30:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8d4iq</id>
    <title>model: support GLM-OCR by ngxson · Pull Request #19677 · ggml-org/llama.cpp</title>
    <updated>2026-02-18T19:44:23+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8d4iq/model_support_glmocr_by_ngxson_pull_request_19677/"&gt; &lt;img alt="model: support GLM-OCR by ngxson · Pull Request #19677 · ggml-org/llama.cpp" src="https://external-preview.redd.it/gy3Bao2ncM4JSj1HjFdjb15hySU2009NljOUnQ4h7EI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b6bc867195227228f82cc5f935733bfd4718813" title="model: support GLM-OCR by ngxson · Pull Request #19677 · ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr &lt;strong&gt;0.9B OCR model (you can run it on any potato)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;GLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoder–decoder architecture. It introduces Multi-Token Prediction (MTP) loss and stable full-task reinforcement learning to improve training efficiency, recognition accuracy, and generalization. The model integrates the CogViT visual encoder pre-trained on large-scale image–text data, a lightweight cross-modal connector with efficient token downsampling, and a GLM-0.5B language decoder. Combined with a two-stage pipeline of layout analysis and parallel recognition based on PP-DocLayout-V3, GLM-OCR delivers robust and high-quality OCR performance across diverse document layouts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;State-of-the-Art Performance&lt;/strong&gt;: Achieves a score of 94.62 on OmniDocBench V1.5, ranking #1 overall, and delivers state-of-the-art results across major document understanding benchmarks, including formula recognition, table recognition, and information extraction.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimized for Real-World Scenarios&lt;/strong&gt;: Designed and optimized for practical business use cases, maintaining robust performance on complex tables, code-heavy documents, seals, and other challenging real-world layouts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Inference&lt;/strong&gt;: With only 0.9B parameters, GLM-OCR supports deployment via vLLM, SGLang, and Ollama, significantly reducing inference latency and compute cost, making it ideal for high-concurrency services and edge deployments.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy to Use&lt;/strong&gt;: Fully open-sourced and equipped with a comprehensive &lt;a href="https://github.com/zai-org/GLM-OCR"&gt;SDK&lt;/a&gt; and inference toolchain, offering simple installation, one-line invocation, and smooth integration into existing production pipelines.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19677"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8d4iq/model_support_glmocr_by_ngxson_pull_request_19677/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8d4iq/model_support_glmocr_by_ngxson_pull_request_19677/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T19:44:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8t684</id>
    <title>I built a lightweight self-hosted AI gateway in Python — stdlib only, no frameworks, 25 modules, 32 tools</title>
    <updated>2026-02-19T07:48:10+00:00</updated>
    <author>
      <name>/u/Special-Argument-558</name>
      <uri>https://old.reddit.com/user/Special-Argument-558</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8t684/i_built_a_lightweight_selfhosted_ai_gateway_in/"&gt; &lt;img alt="I built a lightweight self-hosted AI gateway in Python — stdlib only, no frameworks, 25 modules, 32 tools" src="https://preview.redd.it/a2lccnrtuekg1.gif?frame=1&amp;amp;width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=49fb444de552467b5fb25dc8b94fa1aa0143fb83" title="I built a lightweight self-hosted AI gateway in Python — stdlib only, no frameworks, 25 modules, 32 tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/a2lccnrtuekg1.gif"&gt;https://i.redd.it/a2lccnrtuekg1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I've been working on SalmAlm — a personal AI gateway that runs as a single Python process. No Django, no Flask, no aiohttp. Pure stdlib.&lt;/p&gt; &lt;p&gt;Why I built this:&lt;/p&gt; &lt;p&gt;I wanted a self-hosted AI interface that I actually control — something between &amp;quot;curl the API&amp;quot; and &amp;quot;deploy a full SaaS stack.&amp;quot; Most alternatives either need Docker + Redis + Postgres or are just thin wrappers around one provider.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;p&gt;- **Multi-provider routing** — OpenAI, Anthropic, Google, xAI, Ollama (local models), and more from one interface&lt;/p&gt; &lt;p&gt;- **32 built-in tools** — web search, code execution (sandboxed), file I/O, system monitoring, RAG, TTS/STT, image analysis&lt;/p&gt; &lt;p&gt;- **Encrypted vault** — AES-256-GCM for API keys, no plaintext config files&lt;/p&gt; &lt;p&gt;- **Web UI** — multi-session chat, markdown rendering, code highlighting, dashboard with usage stats&lt;/p&gt; &lt;p&gt;- **Ollama integration** — auto-detects local models, zero config&lt;/p&gt; &lt;p&gt;- **PWA support** — install as desktop/mobile app&lt;/p&gt; &lt;p&gt;- **One-liner install:** `pip install salmalm &amp;amp;&amp;amp; python -m salmalm`&lt;/p&gt; &lt;p&gt;What it's NOT:&lt;/p&gt; &lt;p&gt;- Not a team tool or SaaS replacement — it's a personal weapon&lt;/p&gt; &lt;p&gt;- Not trying to compete with Open WebUI or LibreChat — different philosophy (minimal deps, single process)&lt;/p&gt; &lt;p&gt;- No Docker required (but works in Docker if you want)&lt;/p&gt; &lt;p&gt;**Tech stats:**&lt;/p&gt; &lt;p&gt;- ~10,400 lines across 25 Python modules&lt;/p&gt; &lt;p&gt;- 370+ tests, 48% coverage, mypy 0 errors&lt;/p&gt; &lt;p&gt;- CSP nonce (no unsafe-inline), audit logging with hash chain&lt;/p&gt; &lt;p&gt;- Works on Python 3.10–3.14, Linux/macOS/Windows&lt;/p&gt; &lt;p&gt;**Local LLM angle:**&lt;/p&gt; &lt;p&gt;If you're running Ollama or any OpenAI-compatible local server, SalmAlm gives you a proper UI + tool ecosystem without pulling in heavy dependencies. The tool system (code exec, web search, file ops) works with any model that supports function calling — including local ones.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/hyunjun6928-netizen/salmalm"&gt;https://github.com/hyunjun6928-netizen/salmalm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PyPI: &lt;a href="https://pypi.org/project/salmalm/"&gt;https://pypi.org/project/salmalm/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or take feedback. This is a solo project so roast it if something's dumb.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special-Argument-558"&gt; /u/Special-Argument-558 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8t684/i_built_a_lightweight_selfhosted_ai_gateway_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8t684/i_built_a_lightweight_selfhosted_ai_gateway_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8t684/i_built_a_lightweight_selfhosted_ai_gateway_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:48:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r89a4y</id>
    <title>Vellium: open-source desktop app for creative writing with visual controls instead of prompt editing</title>
    <updated>2026-02-18T17:26:07+00:00</updated>
    <author>
      <name>/u/Possible_Statement84</name>
      <uri>https://old.reddit.com/user/Possible_Statement84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r89a4y/vellium_opensource_desktop_app_for_creative/"&gt; &lt;img alt="Vellium: open-source desktop app for creative writing with visual controls instead of prompt editing" src="https://preview.redd.it/jdgxyzrhdakg1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=7956ef19d5a35f0a082596b3cb054ece5781faf7" title="Vellium: open-source desktop app for creative writing with visual controls instead of prompt editing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of digging through SillyTavern's config every time I wanted to change the tone of a scene. So I built my own thing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The idea:&lt;/strong&gt; sliders instead of prompts. Want slow burn? Drag pacing down. High tension? Push intensity up. The app handles prompt injections behind the scenes. There are presets too if you don't want to tweak manually.&lt;/p&gt; &lt;p&gt;Chat with an inspector panel: Mood, Pacing, Intensity, Dialogue Style, Initiative, Descriptiveness, Unpredictability, Emotional Depth. All visual, no prompt editing needed.&lt;/p&gt; &lt;p&gt;Writer mode for longer stuff. Each chapter gets its own controls: Tone, Pacing, POV, Creativity, Tension, Detail, Dialogue Share. You can generate, expand, rewrite or summarize scenes. Generation runs in the background so you can chat while it writes.&lt;/p&gt; &lt;p&gt;Characters are shared between chat and writing. Build one in chat, drop them into a novel. Imports ST V2 cards and JSON. Avatars pull from Chub.&lt;/p&gt; &lt;p&gt;Lorebooks with keyword activation. MCP tool calling with per-function toggles. Multi-agent chat with auto turn switching. File attachments and vision in chat. Export to MD/DOCX.&lt;/p&gt; &lt;p&gt;Works with Ollama, LM Studio, OpenAI, OpenRouter, or any compatible endpoint. Light and dark themes. English, Russian, Chinese, Japanese.&lt;/p&gt; &lt;p&gt;Still rough around the edges but actively developing. Would love feedback.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/tg-prplx/vellium"&gt;https://github.com/tg-prplx/vellium&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Possible_Statement84"&gt; /u/Possible_Statement84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r89a4y"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r89a4y/vellium_opensource_desktop_app_for_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r89a4y/vellium_opensource_desktop_app_for_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T17:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8c6th</id>
    <title>FlashLM v4: 4.3M ternary model trained on CPU in 2 hours — coherent stories from adds and subtracts only</title>
    <updated>2026-02-18T19:09:37+00:00</updated>
    <author>
      <name>/u/Own-Albatross868</name>
      <uri>https://old.reddit.com/user/Own-Albatross868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Back with v4. Some of you saw v3 — 13.6M params, ternary weights, trained on CPU, completely incoherent output. Went back to the drawing board and rebuilt everything from scratch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;4.3M parameter language model where every weight in the model body is -1, 0, or +1. Trained for 2 hours on a free Deepnote notebook (2 threads, 5GB RAM). No GPU at any point — not for training, not for inference. The model generates coherent children’s stories with dialogue and narrative structure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fair comparison using BPC:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Quick note on the metric — you can’t directly compare validation loss across models with different tokenizers because the tokenizer changes how many tokens a sentence gets split into. BPC (bits-per-character) fixes this by measuring compression per character of raw text instead of per token. Tokenizer drops out of the equation entirely.&lt;/p&gt; &lt;p&gt;Evaluated on 500 TinyStories validation stories (405K characters):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;FlashLM v4&lt;/th&gt; &lt;th align="left"&gt;TinyStories-1M&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Params&lt;/td&gt; &lt;td align="left"&gt;4.3M (ternary)&lt;/td&gt; &lt;td align="left"&gt;3.7M (float32)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BPC&lt;/td&gt; &lt;td align="left"&gt;0.88&lt;/td&gt; &lt;td align="left"&gt;0.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hardware&lt;/td&gt; &lt;td align="left"&gt;2-thread CPU (free tier)&lt;/td&gt; &lt;td align="left"&gt;V100 GPU&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Training time&lt;/td&gt; &lt;td align="left"&gt;2 hours&lt;/td&gt; &lt;td align="left"&gt;Hours (GPU)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tokens seen&lt;/td&gt; &lt;td align="left"&gt;10.6M&lt;/td&gt; &lt;td align="left"&gt;~470M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Architecture&lt;/td&gt; &lt;td align="left"&gt;Gated conv + GLU (no attention)&lt;/td&gt; &lt;td align="left"&gt;GPT-Neo (attention)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;We’re behind, but we’ve seen 2.3% of their training data and the loss curve was still going down when time ran out. The model is undertrained, not underdesigned.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What changed from v3:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;v3’s fatal flaw was the output layer. 50,257 vocab with d_model=256 meant 86% of training compute went to the softmax projection. The actual ternary model core got 14% of the compute budget. Also trained on FineWeb-Edu which is way too broad for a tiny model — like asking a 4-year-old to memorize Wikipedia.&lt;/p&gt; &lt;p&gt;v4 changes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vocab 50K → 10K with weight-tied embeddings, killed the softmax bottleneck&lt;/li&gt; &lt;li&gt;FineWeb-Edu → TinyStories, a focused dataset proven to work at small scale&lt;/li&gt; &lt;li&gt;New token mixer: gated causal depthwise convolution (kernel=8) instead of attention — O(T) not O(T²)&lt;/li&gt; &lt;li&gt;Added ternary GLU feed-forward (SiLU gating, 192→512→192)&lt;/li&gt; &lt;li&gt;RMSNorm instead of LayerNorm&lt;/li&gt; &lt;li&gt;6 blocks, d_model=192, 16.7MB total&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Architecture:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Embedding (10K × 192, float, weight-tied) → 6× BoltBlock: RMSNorm → GatedConvMixer (ternary depthwise conv + gate) + residual RMSNorm → TernaryGLU (ternary gate/up/down, SiLU) + residual → RMSNorm → Output Head (tied to embedding) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No attention anywhere. Token mixing is a gated causal conv with receptive field of 8 per layer (48 across all 6 layers). All linear projections use ternary quantization with straight-through estimator. At inference time the core ops are just adds, subtracts, and zeros.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Sample output (step 5000):&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;The [] are UNK tokens from the 10K vocab not covering all TinyStories words — fixable by building vocab from actual corpus frequencies instead of taking the first 10K GPT-2 tokens.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training curve:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Val loss went from 9.2 → 2.10 over 5,199 steps (10.6M tokens). Never plateaued. Speed was ~1,480 tokens/sec on 2 threads.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Step&lt;/th&gt; &lt;th align="left"&gt;Val Loss&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;500&lt;/td&gt; &lt;td align="left"&gt;2.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1000&lt;/td&gt; &lt;td align="left"&gt;2.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2000&lt;/td&gt; &lt;td align="left"&gt;2.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3000&lt;/td&gt; &lt;td align="left"&gt;2.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4000&lt;/td&gt; &lt;td align="left"&gt;2.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5000&lt;/td&gt; &lt;td align="left"&gt;2.10&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;What’s next:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Someone in my DMs from the v3 post offered SSH access to a Ryzen 7950X3D (16 cores, 96MB V-Cache, 128GB RAM). Planning to train a scaled-up version (~15M params, d=384, 8 blocks) on that machine for multiple days with a proper frequency-based tokenizer. Target is closing the BPC gap with TinyStories-1M and pushing toward TinyStories-28M territory.&lt;/p&gt; &lt;p&gt;Also planning to release a standalone &lt;a href="http://train.py/"&gt;train.py&lt;/a&gt; so anyone can reproduce this on their own hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model + weights + model card: &lt;a href="https://huggingface.co/changcheng967/flashlm-v4-bolt"&gt;https://huggingface.co/changcheng967/flashlm-v4-bolt&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo: &lt;a href="https://huggingface.co/spaces/changcheng967/flashlm-v4-demo"&gt;https://huggingface.co/spaces/changcheng967/flashlm-v4-demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;v3 for comparison: &lt;a href="https://huggingface.co/changcheng967/flashlm-v3-13m"&gt;https://huggingface.co/changcheng967/flashlm-v3-13m&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Code and model are MIT licensed. Happy to answer questions about the architecture or training.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Albatross868"&gt; /u/Own-Albatross868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8c6th/flashlm_v4_43m_ternary_model_trained_on_cpu_in_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8c6th/flashlm_v4_43m_ternary_model_trained_on_cpu_in_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8c6th/flashlm_v4_43m_ternary_model_trained_on_cpu_in_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T19:09:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8kybv</id>
    <title>Best coding models (or other models) one can run on an rtx5070ti (16gb vram) with of 64gb RAM</title>
    <updated>2026-02-19T00:51:53+00:00</updated>
    <author>
      <name>/u/cmdr-William-Riker</name>
      <uri>https://old.reddit.com/user/cmdr-William-Riker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just playing around. I am aware that this isn't going to be anything groundbreaking you can run on hardware like this, but I am curious if there are any small models that have any genuine use for coding in particular or other use cases if not that could fit in moderate consumer hardware yet. I've run Deepseek and llama 8b models, which are definitely good, but I was actually able to run those models on an rtx3050 with 8gb of vram and 32gb of ram easily. I'm just wondering if there are any models that can make use of slightly better hardware that I have now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cmdr-William-Riker"&gt; /u/cmdr-William-Riker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8kybv/best_coding_models_or_other_models_one_can_run_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8kybv/best_coding_models_or_other_models_one_can_run_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8kybv/best_coding_models_or_other_models_one_can_run_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T00:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8u3x3</id>
    <title>How we gave up and picked back up evals driven development (EDD)</title>
    <updated>2026-02-19T08:46:53+00:00</updated>
    <author>
      <name>/u/sunglasses-guy</name>
      <uri>https://old.reddit.com/user/sunglasses-guy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; I posted this originally in &lt;a href="/r/AIEval"&gt;r/AIEval&lt;/a&gt;, I thought it would be good to share in other communities too related to LLMs.&lt;/p&gt; &lt;p&gt;Hey r/AIEval, wanted to share how we gave up on and ultimately went back to evals driven development (EDD) over the past 2 months of setup, trial-and-error, testing exhaustion, and ultimately, a workflow that we were able to compromise on actually stick to.&lt;/p&gt; &lt;p&gt;For context, we're a team of 6 building a multi-turn customer support agent for a fintech product. We handle billing disputes, account changes, and compliance-sensitive stuff. Stakes are high enough that &amp;quot;vibes-based testing&amp;quot; wasn't cutting it anymore.&lt;/p&gt; &lt;h1&gt;How it started.... the &amp;quot;by the book&amp;quot; attempt&lt;/h1&gt; &lt;p&gt;A lot of folks base their belief on something they've read online, a video they've watched, and that included us.&lt;/p&gt; &lt;p&gt;We read every blog post about EDD and went all in. Built a golden dataset of 400+ test cases. Wrote custom metrics for tone, accuracy, and policy compliance. Hooked everything into CI/CD so evals ran on every PR.&lt;/p&gt; &lt;p&gt;Within 2 weeks, nobody on the team wanted to touch the eval pipeline:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Our golden dataset was stale almost immediately. We changed our system prompt 3 times in week 1 alone, and suddenly half the expected outputs were wrong. Nobody wanted to update 400 rows in a spreadsheet.&lt;/li&gt; &lt;li&gt;Metric scores were noisy. We were using LLM-as-a-judge for most things, and scores would fluctuate between runs. Engineers started ignoring failures because &amp;quot;it was probably just the judge being weird.&amp;quot;&lt;/li&gt; &lt;li&gt;CI/CD evals took 20+ minutes per run. Developers started batching PRs to avoid triggering the pipeline, which defeated the entire purpose.&lt;/li&gt; &lt;li&gt;Nobody agreed on thresholds. PM wanted 0.9 on answer relevancy. Engineering said 0.7 was fine. We spent more time arguing about numbers than actually improving the agent.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We quietly stopped running evals around week 4. Back to manual testing and spot checks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;But, right around this time,&lt;/strong&gt; our agent told a user they could dispute a charge by &amp;quot;contacting their bank directly and requesting a full reversal.&amp;quot; That's not how our process works at all. It slipped through because nobody was systematically checking outputs anymore.&lt;/p&gt; &lt;p&gt;In hindsight, I think it had nothing to do with us going back to manual testing, since our process was utterly broken already.&lt;/p&gt; &lt;h1&gt;How we reformed our EDD approach&lt;/h1&gt; &lt;p&gt;Instead of trying to eval everything on every PR, we stripped it way back:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;50 test cases, not 400.&lt;/strong&gt; We picked the 50 scenarios that actually matter for our use case. Edge cases that broke things before. Compliance-sensitive interactions. The stuff that would get us in trouble. Small enough that one person can review the entire set in 10-15 mins.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;3 metrics, not 12.&lt;/strong&gt; Answer correctness, hallucination, and a custom policy compliance metric. That's it. We use DeepEval for this since it plugs into pytest and our team already knows the workflow.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evals run nightly, not on every PR.&lt;/strong&gt; This was the big mental shift. We treat evals like a regression safety net, not a gate on every code change. Engineers get results in Slack every morning. If something broke overnight, we catch it before standup.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Monthly dataset review.&lt;/strong&gt; First Monday of every month, our PM and one engineer spend an hour reviewing and updating the golden dataset. It's a calendar invite. Non-negotiable. This alone fixed 80% of the staleness problem.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Threshold agreement upfront.&lt;/strong&gt; We spent one meeting defining pass/fail thresholds and wrote them down. No more debates on individual PRs. If the threshold needs changing, it goes through the monthly review.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The most important thing here is we took our dataset quality much more seriously, and went the extra mile to make sure the metrics we chose deserves to be in our daily benchmarks.&lt;/p&gt; &lt;p&gt;I think this was what changed our PM's perspective on evals and got them more engaged, because they could actually see how a test case's failing/passing metrics correlated to real-world outcomes.&lt;/p&gt; &lt;h1&gt;What we learned&lt;/h1&gt; &lt;p&gt;EDD failed for us the first time because we treated it like traditional test-driven development where you need 100% coverage from day one. LLM apps don't work like that. The outputs are probabilistic, the metrics are imperfect, and your use case evolves faster than your test suite.&lt;/p&gt; &lt;p&gt;The version that stuck is intentionally minimal (50 cases, 3 metrics, nightly runs, monthly maintenance).&lt;/p&gt; &lt;p&gt;It's not glamorous, but we've caught 3 regressions in the last 3 weeks that would've hit production otherwise.&lt;/p&gt; &lt;p&gt;One thing I want to call out: at such an early stage of setting up EDD, the tooling was rarely the problem. We initially blamed our setup (DeepEval + Confident AI), but after we reformed our process we kept the exact same tools and everything worked. The real issue was that we were abusing our data and exhausting the team's attention by overloading them with way too much information.&lt;/p&gt; &lt;p&gt;I get into tooling debates pretty often, and honestly, at the early stages of finding an EDD workflow that sticks, just focus on the data. The tool matters way less than what you're testing and how much of it you're asking people to care about.&lt;/p&gt; &lt;p&gt;If you're struggling to make EDD work, try scaling way down before scaling up. Start with the 10 to 20 scenarios that would actually embarrass your company if they failed. Measure those reliably. Expand once you trust the process.&lt;/p&gt; &lt;p&gt;But who knows if this is an unique perspective from me, maybe someone had a different experience where large volumes of data worked? Keen to hear any thoughts you guys might have, and what worked/didn't work for you.&lt;/p&gt; &lt;p&gt;(Reminder: We were at the very initial stages of setup, still 2 months in)&lt;/p&gt; &lt;p&gt;Our next goal is to make evals a more no-code workflow within the next 2 weeks, keen to hear any suggestions on this as well, especially for product owner buy-in.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunglasses-guy"&gt; /u/sunglasses-guy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8u3x3/how_we_gave_up_and_picked_back_up_evals_driven/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8u3x3/how_we_gave_up_and_picked_back_up_evals_driven/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8u3x3/how_we_gave_up_and_picked_back_up_evals_driven/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T08:46:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8g0iw</id>
    <title>MiniMax-M2.5-REAP from cerebras</title>
    <updated>2026-02-18T21:32:00+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/cerebras/MiniMax-M2.5-REAP-172B-A10B"&gt;https://huggingface.co/cerebras/MiniMax-M2.5-REAP-172B-A10B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cerebras/MiniMax-M2.5-REAP-139B-A10B"&gt;https://huggingface.co/cerebras/MiniMax-M2.5-REAP-139B-A10B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;REAP are smaller versions of models that you can fit on your setup and be happy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8g0iw/minimaxm25reap_from_cerebras/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8g0iw/minimaxm25reap_from_cerebras/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8g0iw/minimaxm25reap_from_cerebras/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T21:32:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8pohi</id>
    <title>Last Week in Multimodal AI - Local Edition</title>
    <updated>2026-02-19T04:31:57+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pohi/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last Week in Multimodal AI - Local Edition" src="https://preview.redd.it/12la8ajmpdkg1.png?width=140&amp;amp;height=90&amp;amp;auto=webp&amp;amp;s=cac74816a78676e7abbdfae0b17c0b819bb2629d" title="Last Week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly multimodal AI roundup, here are the local/open-source highlights from last week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3.5-397B-A17B - Native Vision-Language Foundation Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;397B-parameter MoE model (17B active) with hybrid linear attention and native multimodal integration.&lt;/li&gt; &lt;li&gt;Handles document parsing, chart analysis, and visual reasoning without a separate vision encoder.&lt;/li&gt; &lt;li&gt;&lt;a href="https://qwen.ai/blog?id=qwen3.5"&gt;Blog&lt;/a&gt; | &lt;a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/12la8ajmpdkg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d39b1ea44a322f087f3b33e35564a96454f25c9"&gt;https://preview.redd.it/12la8ajmpdkg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d39b1ea44a322f087f3b33e35564a96454f25c9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PersonaPlex-7B - Full-Duplex Voice Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;NVIDIA's 7B voice model that listens and speaks simultaneously with natural interruption support.&lt;/li&gt; &lt;li&gt;Eliminates turn-taking latency for real-time voice conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nvidia/personaplex-7b-v1"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1r8pohi/video/8f15ixwnpdkg1/player"&gt;https://reddit.com/link/1r8pohi/video/8f15ixwnpdkg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MiniMax M2.5 - Open-Source Productivity Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Frontier model tuned for coding, writing, and structured analysis.&lt;/li&gt; &lt;li&gt;Prioritizes instruction-following accuracy over open-ended chat.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.5"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/on0tek5qpdkg1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0988ea083b38e580baf2961778187892fd50517a"&gt;https://preview.redd.it/on0tek5qpdkg1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0988ea083b38e580baf2961778187892fd50517a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepGen 1.0 - 5B Unified Multimodal Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lightweight model with native visual understanding built into the architecture.&lt;/li&gt; &lt;li&gt;Small enough for consumer hardware.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/deepgenteam/DeepGen-1.0"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m1yn8xxrpdkg1.png?width=2376&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9b56d294a054b3e38244bdcf0e988abc61a8ffbf"&gt;https://preview.redd.it/m1yn8xxrpdkg1.png?width=2376&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9b56d294a054b3e38244bdcf0e988abc61a8ffbf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-TTS - 1.7B Speech Synthesis&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clean, natural speech synthesis with custom voice support.&lt;/li&gt; &lt;li&gt;Open weights from Qwen.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1r8pohi/video/qg4slbrvpdkg1/player"&gt;https://reddit.com/link/1r8pohi/video/qg4slbrvpdkg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;KaniTTS2 - 400M TTS in 3GB VRAM&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open-source text-to-speech that runs on modest local hardware.&lt;/li&gt; &lt;li&gt;400M parameters, optimized for local deployment.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-2-pt"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;MioTTS-2.6B - Fast English/Japanese TTS&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lightweight text-to-speech optimized for inference speed.&lt;/li&gt; &lt;li&gt;Supports English and Japanese out of the box.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Aratako/MioTTS-2.6B"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Ming-flash-omni 2.0 - Multimodal Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;New open multimodal model from InclusionAI.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-2.0"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SoulX-Singer - Zero-Shot Singing Voice Synthesis&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;High-quality singing voice synthesis with no fine-tuning required.&lt;/li&gt; &lt;li&gt;Open-source with code on GitHub.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Soul-AILab/SoulX-Singer/tree/main"&gt;GitHub&lt;/a&gt; | &lt;a href="https://huggingface.co/Soul-AILab/SoulX-Singer"&gt;Hugging&lt;/a&gt; Face&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ewez41tzpdkg1.png?width=1016&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9614a31ecd2dd373b2abddd730eee0d4c52cedaa"&gt;https://preview.redd.it/ewez41tzpdkg1.png?width=1016&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9614a31ecd2dd373b2abddd730eee0d4c52cedaa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://open.substack.com/pub/thelivingedge/p/last-week-in-multimodal-ai-45-no?utm_campaign=post-expanded-share&amp;amp;utm_medium=web"&gt;full roundup&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;p&gt;* I was delayed this week but normally i post these roundups on Mondays&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/submit/?source_id=t3_1r8pftg"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pohi/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pohi/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pohi/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T04:31:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8gb3p</id>
    <title>Do we want the benefits of Ollama API without actually using Ollama?</title>
    <updated>2026-02-18T21:43:03+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8gb3p/do_we_want_the_benefits_of_ollama_api_without/"&gt; &lt;img alt="Do we want the benefits of Ollama API without actually using Ollama?" src="https://preview.redd.it/ye8e5rinobkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fee113ba5ec5804912453771ad9dbfd4c1c4053" title="Do we want the benefits of Ollama API without actually using Ollama?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apps with native Ollama API integration often have smoother setup and model management than what we get with the OpenAI API alone. For example, in Open WebUI (see image), the server is auto-detected on port &lt;code&gt;11434&lt;/code&gt; and you can pull, eject, and check the status of models right from the web ui.&lt;/p&gt; &lt;p&gt;As an experiment this week I added Ollama API support to Lemonade Server. We already had the functions, so I just had to hook them up to &lt;code&gt;/api&lt;/code&gt; endpoints. I think it's pretty neat, so I'm interested to hear what you all think.&lt;/p&gt; &lt;p&gt;Here's how it works:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h1&gt;First: stop the Ollama service if you have it running&lt;/h1&gt; &lt;h1&gt;Start Lemonade on the Ollama port&lt;/h1&gt; &lt;p&gt;lemonade-server serve --port 11434&lt;/p&gt; &lt;h1&gt;Optional: use any llamacpp binaries you like&lt;/h1&gt; &lt;p&gt;export LEMONADE_LLAMACPP_VULKAN_BIN=/path/to/llama-server-folder&lt;/p&gt; &lt;h1&gt;or&lt;/h1&gt; &lt;p&gt;export LEMONADE_LLAMACPP_ROCM_BIN=/path/to/llama-server-folder&lt;/p&gt; &lt;h1&gt;Optional: use your own GGUFs from llamacpp -hf or LM Studio&lt;/h1&gt; &lt;p&gt;lemonade-server serve --port 11434 --extra-models-dir ~/.cache/llama.cpp&lt;/p&gt; &lt;h1&gt;or&lt;/h1&gt; &lt;p&gt;lemonade-server serve --port 11434 --extra-models-dir ~/.lmstudio/models ```&lt;/p&gt; &lt;p&gt;Then, start Open WebUI and it should auto-detect Lemonade, populate the models list with your GGUF and/or NPU models, and give you access to features that were otherwise Ollama-only.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;Get Lemonade v9.3.4 here&lt;/a&gt; if you want to give it a spin, and let me know your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ye8e5rinobkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8gb3p/do_we_want_the_benefits_of_ollama_api_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8gb3p/do_we_want_the_benefits_of_ollama_api_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T21:43:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8smbk</id>
    <title>Local VLMs (Qwen 3 VL) for document OCR with bounding box detection for PII detection/redaction workflows (blog post and open source app)</title>
    <updated>2026-02-19T07:13:57+00:00</updated>
    <author>
      <name>/u/Sonnyjimmy</name>
      <uri>https://old.reddit.com/user/Sonnyjimmy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8smbk/local_vlms_qwen_3_vl_for_document_ocr_with/"&gt; &lt;img alt="Local VLMs (Qwen 3 VL) for document OCR with bounding box detection for PII detection/redaction workflows (blog post and open source app)" src="https://preview.redd.it/1pwglerfhekg1.jpg?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=635f2955ab9e9dd86f185894924ee6bbe9da3633" title="Local VLMs (Qwen 3 VL) for document OCR with bounding box detection for PII detection/redaction workflows (blog post and open source app)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://seanpedrick-case.github.io/doc_redaction/src/redaction_with_vlm_and_llms.html"&gt;Blog post link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A while ago I made a post here in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; asking about using local VLMs for OCR in PII detection/redaction processes for documents (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kspe8c/best_local_model_ocr_solution_for_pdf_document/"&gt;here&lt;/a&gt;). The document redaction process differs from other OCR processes in that we need to identify the bounding boxes of words on the page, as well as the text content, to successfully redact the document.&lt;/p&gt; &lt;p&gt;I have now implemented OCR with bounding box detection into the &lt;a href="https://github.com/seanpedrick-case/doc_redaction"&gt;Document redaction app&lt;/a&gt; I have been working on. The VLM models help with OCR either 1. to extract all text and bounding boxes from the page directly or 2. in combination with a 'traditional' OCR model (PaddleOCR), where Paddle first pulls out accurate line-level bounding boxes, then passes words with low confidence to the VLM in a hybrid approach.&lt;/p&gt; &lt;p&gt;I wanted to use small VLM models such as Qwen 3 VL 8B Instruct for this task to see whether local models that can fit in consumer grade GPUs (i.e. 24GB VRAM or less) could be used for redaction tasks.&lt;/p&gt; &lt;p&gt;My experiments with using VLMs in the redaction OCR process are demonstrated in &lt;a href="https://seanpedrick-case.github.io/doc_redaction/src/redaction_with_vlm_and_llms.html"&gt;this blog post&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1pwglerfhekg1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5f443be8011738ed0e186ff06a42602ea399881b"&gt;Unclear text on handwritten note analysed with hybrid PaddleOCR + Qwen 3 VL 8B Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All the examples can be replicated using this &lt;a href="https://huggingface.co/spaces/seanpedrickcase/document_redaction_vlm"&gt;Hugging Face space for free&lt;/a&gt;. The code for the underlying Document Redaction app is available for anyone to view and use, and can be found &lt;a href="https://github.com/seanpedrick-case/doc_redaction"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;My blog post used Qwen 3 VL 8B Instruct as the small VLM for OCR. My conclusion at the moment is that the hybrid PaddleOCR + Qwen 3 VL approach is better than the pure VLM approach for 'difficult' handwritten documents. However, both approaches are not quite there for perfect accuracy.&lt;/p&gt; &lt;p&gt;This conclusion may soon change with the imminent release of the Qwen 3.5 VL models, after which I will redo my analysis and post about it here.&lt;/p&gt; &lt;p&gt;The blog post also shows how VLMs can be used for detecting signatures, and PII in images such as people's faces. I also demonstrate how mid-level local LLMs of ~30GB parameter size (Gemma 27B) can be used to detect custom entities in document text.&lt;/p&gt; &lt;p&gt;Any comments on the approach or the app in general are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sonnyjimmy"&gt; /u/Sonnyjimmy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8smbk/local_vlms_qwen_3_vl_for_document_ocr_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8smbk/local_vlms_qwen_3_vl_for_document_ocr_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8smbk/local_vlms_qwen_3_vl_for_document_ocr_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:13:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1r86i3o</id>
    <title>LLMs grading other LLMs 2</title>
    <updated>2026-02-18T15:47:24+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r86i3o/llms_grading_other_llms_2/"&gt; &lt;img alt="LLMs grading other LLMs 2" src="https://preview.redd.it/rmq2mwriw9kg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07e6fa12e92be2b51d119d6c78ac4e28ccf7e1cb" title="LLMs grading other LLMs 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A year ago I made a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt;meta-eval here on the sub&lt;/a&gt;, asking LLMs to grade a few criterias about other LLMs. &lt;/p&gt; &lt;p&gt;Time for the part 2.&lt;/p&gt; &lt;p&gt;The premise is very simple, the model is asked a few ego-baiting questions and other models are then asked to rank it. The scores in the pivot table are normalised.&lt;/p&gt; &lt;p&gt;You can find &lt;a href="https://huggingface.co/datasets/av-codes/cringebench"&gt;all the data on HuggingFace&lt;/a&gt; for your analysis.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rmq2mwriw9kg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r86i3o/llms_grading_other_llms_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r86i3o/llms_grading_other_llms_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T15:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8ta57</id>
    <title>I retrained /u/Own-Albatross868's FlashLM v4 "Bolt" model from scratch using GreedyPhrase tokenizer on the full TinyStories dataset. I scaled up to 15M parameters with a 65K vocab, achieving smooth convergence and coherent story generation in just 2.2 hours on an RTX 2080 Ti</title>
    <updated>2026-02-19T07:54:51+00:00</updated>
    <author>
      <name>/u/reditzer</name>
      <uri>https://old.reddit.com/user/reditzer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FlashLM v4 &amp;quot;Bolt&amp;quot; retrained from scratch on the full TinyStories dataset using our &lt;a href="https://github.com/rayonnant-ai/greedyphrase"&gt;GreedyPhrase&lt;/a&gt; tokenizer instead of the original GPT-2 10K tokenizer.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;&lt;a href="https://huggingface.co/changcheng967/flashlm-v4-bolt"&gt;Original&lt;/a&gt;&lt;/th&gt; &lt;th&gt;&lt;a href="https://huggingface.co/rrezel/flashlm-v4-bolt-greedyphrase"&gt;This Run&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Tokenizer&lt;/td&gt; &lt;td&gt;GPT-2 (tiktoken), 10K vocab&lt;/td&gt; &lt;td&gt;GreedyPhrase, 65K vocab&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Parameters&lt;/td&gt; &lt;td&gt;4.3M&lt;/td&gt; &lt;td&gt;15.0M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hardware&lt;/td&gt; &lt;td&gt;2 vCPU (CPU only)&lt;/td&gt; &lt;td&gt;RTX 2080 Ti (GPU)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Training time&lt;/td&gt; &lt;td&gt;2 hours&lt;/td&gt; &lt;td&gt;~2.2 hours&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Tokens seen&lt;/td&gt; &lt;td&gt;10.6M (2.3% of data)&lt;/td&gt; &lt;td&gt;818M (3.3 epochs)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Best val loss&lt;/td&gt; &lt;td&gt;2.0976&lt;/td&gt; &lt;td&gt;3.9352&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Throughput&lt;/td&gt; &lt;td&gt;1,479 tok/s&lt;/td&gt; &lt;td&gt;103,000 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Training Configuration&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Architecture&lt;/td&gt; &lt;td&gt;FlashLM v4 Bolt (ternary gated causal conv)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hidden dim&lt;/td&gt; &lt;td&gt;192&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Blocks&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Conv kernel size&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLU expansion dim&lt;/td&gt; &lt;td&gt;512&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Vocab size&lt;/td&gt; &lt;td&gt;65,280 (padded from 65,218 actual)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Sequence length&lt;/td&gt; &lt;td&gt;256 tokens&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Effective batch size&lt;/td&gt; &lt;td&gt;64 (micro=16, grad_accum=4)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Optimizer&lt;/td&gt; &lt;td&gt;AdamW (weight_decay=0.01)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Peak learning rate&lt;/td&gt; &lt;td&gt;4e-3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LR schedule&lt;/td&gt; &lt;td&gt;Cosine with 500-step warmup&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gradient clipping&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Precision&lt;/td&gt; &lt;td&gt;AMP float16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Total steps&lt;/td&gt; &lt;td&gt;50,000&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Dataset&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Source:&lt;/strong&gt; TinyStories (roneneldan/TinyStories), 2.1 GB text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Preprocessing:&lt;/strong&gt; &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt; replaced with &lt;code&gt;&amp;lt;/s&amp;gt;&lt;/code&gt; (EOS token ID 3)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tokenized size:&lt;/strong&gt; 248M tokens (496 MB binary uint16)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compression ratio:&lt;/strong&gt; ~8.88 bytes/token (vs ~4.5 for GPT-2)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Train/val split:&lt;/strong&gt; 99.5% / 0.5%&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;h3&gt;Loss Curve&lt;/h3&gt; &lt;p&gt;&lt;code&gt; Step Train Loss Val Loss 0 11.13 — 500 6.73 5.96 1000 5.46 5.12 2500 4.72 4.61 5000 4.43 4.39 10000 4.17 4.19 20000 4.03 4.03 30000 3.95 3.97 40000 3.92 3.95 50000 3.94 3.94 Best — 3.9352 (step 47500) &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Metrics&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Metric&lt;/th&gt; &lt;th&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Best validation loss&lt;/td&gt; &lt;td&gt;3.9352&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Token-level perplexity&lt;/td&gt; &lt;td&gt;51.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Bits per token&lt;/td&gt; &lt;td&gt;5.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Bits per character (estimated)&lt;/td&gt; &lt;td&gt;0.64&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Comparing Val Loss Across Tokenizers&lt;/h3&gt; &lt;p&gt;The raw validation loss numbers are &lt;strong&gt;not directly comparable&lt;/strong&gt; between the original (val_loss 2.10 with 10K vocab) and this run (val_loss 3.94 with 65K vocab) because:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Larger vocabulary = harder prediction task.&lt;/strong&gt; Random-chance loss is ln(65280) = 11.09 vs ln(10000) = 9.21. The model must distribute probability over 6.5x more tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fewer tokens per story.&lt;/strong&gt; GreedyPhrase compresses TinyStories at ~9 bytes/token vs ~4.5 bytes/token for GPT-2. Each token carries more information, so predicting the next token is inherently harder.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bits-per-character is the fair comparison.&lt;/strong&gt; At 0.64 BPC this model is competitive with the original's 0.88 BPC, suggesting the GreedyPhrase tokenizer's higher compression ratio pays off in information-theoretic efficiency.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Generation Samples (Step 49,500)&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;Once upon a time there was a little girl named Sarah. She was only three years old and loved exploring. One day Sarah went to the park with her mother. She saw a little boy playing with a ball.&lt;/p&gt; &lt;p&gt;Once upon a time there was a very deep lake. It was great! Every morning he would jump off the water and look for something wonderful.&lt;/p&gt; &lt;p&gt;Once upon a time there was a little girl named Mary. Mary loved animals, especially especially loved the ocean. Every day Mary would go out on a walk around the waves and swimming around on the beach.&lt;/p&gt; &lt;/blockquote&gt; &lt;h3&gt;Prompt: &amp;quot;The little dog&amp;quot;&lt;/h3&gt; &lt;blockquote&gt; &lt;p&gt;The little dog wanted to protect his bone, so he held it up to the cat and tried to protect him. But the big cat was jealous. It wanted to take the bone from him, but it ran away.&lt;/p&gt; &lt;p&gt;The cat was sad and began to cry. Then, he saw a big hole in the ground and started to shake it. The cat growled and tried to run away. The dog was scared and ran back to the cat. The cat saw the fox and was scared. The cat took the kitten and ran away. The dog was sad. The fox did not get the mitten anymore. The cat was happy and played with Spot and the other friends.&lt;/p&gt; &lt;/blockquote&gt; &lt;h2&gt;Files&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;File&lt;/th&gt; &lt;th&gt;Size&lt;/th&gt; &lt;th&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;flashlm_v4_bolt_greedyphrase.pt&lt;/code&gt;&lt;/td&gt; &lt;td&gt;58 MB&lt;/td&gt; &lt;td&gt;Final model (step 50,000)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;best.pt&lt;/code&gt;&lt;/td&gt; &lt;td&gt;172 MB&lt;/td&gt; &lt;td&gt;Best checkpoint with optimizer state (step 47,500)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;checkpoint.pt&lt;/code&gt;&lt;/td&gt; &lt;td&gt;172 MB&lt;/td&gt; &lt;td&gt;Latest periodic checkpoint&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;tinystories.tokens&lt;/code&gt;&lt;/td&gt; &lt;td&gt;496 MB&lt;/td&gt; &lt;td&gt;Tokenized dataset (uint16 binary)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;model.py&lt;/code&gt;&lt;/td&gt; &lt;td&gt;—&lt;/td&gt; &lt;td&gt;Model architecture&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;train.py&lt;/code&gt;&lt;/td&gt; &lt;td&gt;—&lt;/td&gt; &lt;td&gt;Training script&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Observations&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Convergence was smooth.&lt;/strong&gt; Loss dropped from 11.13 to ~3.94 over 50K steps with no instability, despite ternary weight quantization via straight-through estimators.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The loss curve was still slowly declining at 50K steps.&lt;/strong&gt; Extended training or a second cosine cycle could improve results further.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;GreedyPhrase's long phrases help coherence.&lt;/strong&gt; With ~9 bytes/token, the 256-token context window covers ~2,300 characters (~400 words), much more than the original's ~1,150 characters. This gives the model more context per sequence.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The larger embedding table dominates parameter count.&lt;/strong&gt; 65K vocab x 192 dim = 12.5M parameters in the embedding alone (84% of total), vs 1.9M for the original's 10K vocab. The model body (blocks) is identical.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Throughput benefited from GPU + AMP.&lt;/strong&gt; At 103K tokens/sec on an RTX 2080 Ti, this is 70x faster than the original's 1.5K tokens/sec on CPU, allowing 3.3 full epochs in roughly the same wall-clock time.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reditzer"&gt; /u/reditzer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ta57/i_retrained_uownalbatross868s_flashlm_v4_bolt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ta57/i_retrained_uownalbatross868s_flashlm_v4_bolt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ta57/i_retrained_uownalbatross868s_flashlm_v4_bolt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:54:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r83irw</id>
    <title>PSA: DDR5 RDIMM price passed the point were 3090 are less expensive per gb..</title>
    <updated>2026-02-18T13:51:04+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;Just wanted to note that RDIMM prices are so wild.. Stacking rdimms starts to be as expensive as stacking 3090s.. But RDIMM don't come with compute included..&lt;/p&gt; &lt;p&gt;What a crazy time, shall we stack rdimms or 3090, what's your take on that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r83irw/psa_ddr5_rdimm_price_passed_the_point_were_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r83irw/psa_ddr5_rdimm_price_passed_the_point_were_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r83irw/psa_ddr5_rdimm_price_passed_the_point_were_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T13:51:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8rgcp</id>
    <title>Minimax 2.5 on Strix Halo Thread</title>
    <updated>2026-02-19T06:06:20+00:00</updated>
    <author>
      <name>/u/Equivalent-Belt5489</name>
      <uri>https://old.reddit.com/user/Equivalent-Belt5489</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I just tried out Minimax 2.5 on headless Fedora 43 with the kyuz0 rocm nightlies toolbox, Jan 26 firmware, 6.18.9 Kernel, &lt;a href="https://huggingface.co/unsloth/MiniMax-M2.5-GGUF"&gt;https://huggingface.co/unsloth/MiniMax-M2.5-GGUF&lt;/a&gt; there are some changes necessary so it fits in the RAM. Using MiniMax-M2.5-Q3_K_M there is just enough RAM for approx 80k. The quality is really impressive! But its slow! Its almost not usabe, but the quality is so great I would like to continue with it. &lt;/p&gt; &lt;p&gt;Do you have any tips or do you have a faster setup?&lt;/p&gt; &lt;p&gt;I use now this: &lt;code&gt;export HIP_VISIBLE_DEVICES=0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export GGML_CUDA_ENABLE_UNIFIED_MEMORY=1&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;export HIP_VISIBLE_DEVICES=0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export HIP_ENABLE_DEVICE_MALLOC=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export HIP_ENABLE_UNIFIED_MEMORY=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export HSA_OVERRIDE_GFX_VERSION=11.5.1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export HIP_FORCE_DEV_KERNARG=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export GGML_CUDA_ENABLE_UNIFIED_MEMORY=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export GGML_HIP_UMA=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export HIP_HOST_COHERENT=0&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;export HIP_TRACE_API=0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export HIP_LAUNCH_BLOCKING=0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;export ROCBLAS_USE_HIPBLASLT=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server -m /run/host/data/models/MiniMax-M2.5-Q3_K_M-00001-of-00004.gguf -fa on --no-mmap -c 66600 -ub 1024 --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8080 --jinja -ngl 99&lt;/code&gt; &lt;/p&gt; &lt;p&gt;However its quite slow, if I let it run longer and with more context i get results like: pp 43 t/s, tg 3 t/s...&lt;/p&gt; &lt;p&gt;In the very beginning with 17k kontext&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 81128.69 ms / 17363 tokens ( 4.67 ms per token, 214.02 tokens per second) eval time = 21508.09 ms / 267 tokens ( 80.55 ms per token, 12.41 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;after 8 toolusages and with 40k context&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 25168.38 ms / 1690 tokens ( 14.89 ms per token, 67.15 tokens per second) eval time = 21207.71 ms / 118 tokens ( 179.73 ms per token, 5.56 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;after long usage its getting down to where it stays (still 40 k context)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 13968.84 ms / 610 tokens ( 22.90 ms per token, 43.67 tokens per second) eval time = 24516.70 ms / 82 tokens ( 298.98 ms per token, 3.34 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;llama-bench&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m /run/host/data/models/MiniMax-M2.5-Q3_K_M-00001-of-00004.gguf -ngl 99 -fa on -ngl 99 ggml_cuda_init: found 1 ROCm devices: Device 0: Radeon 8060S Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | ROCm | 99 | pp512 | 200.82 ± 1.38 | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | ROCm | 99 | tg128 | 27.27 ± 0.01 | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | ROCm | 99 | pp512 | 200.38 ± 1.53 | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | ROCm | 99 | tg128 | 27.27 ± 0.00 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With the kyuz vulkan radv toolbox:&lt;/p&gt; &lt;p&gt;The pp is 30% slower, tg a bit faster.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m /run/host/data/models/MiniMax-M2.5-Q3_K_M-00001-of-00004.gguf -ngl 99 -fa on -ngl 99 ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | Vulkan | 99 | pp512 | 157.18 ± 1.29 | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | Vulkan | 99 | tg128 | 32.37 ± 1.67 | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | Vulkan | 99 | pp512 | 176.17 ± 0.85 | | minimax-m2 230B.A10B Q3_K - Medium | 101.76 GiB | 228.69 B | Vulkan | 99 | tg128 | 33.09 ± 0.03 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I try now the Q3_K_XL. I doubt it will improve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent-Belt5489"&gt; /u/Equivalent-Belt5489 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8rgcp/minimax_25_on_strix_halo_thread/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8rgcp/minimax_25_on_strix_halo_thread/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8rgcp/minimax_25_on_strix_halo_thread/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T06:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8mh8m</id>
    <title>How do you get more GPUs than your motheboard natively supports?</title>
    <updated>2026-02-19T02:00:35+00:00</updated>
    <author>
      <name>/u/WizardlyBump17</name>
      <uri>https://old.reddit.com/user/WizardlyBump17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am planning on building an AI server for myself and I want to have 8 GPUs. The problem is that all motherboards I reaserched (FCLGA4710), dont have 8 PCIe slots, with the one with most slots having only 6. I have seen some people here with a lot of GPUs and I am pretty sure they dont have a motherboard with slots for all of them, as I remember some of the GPUs being far from the motherboard. I have done some research and I found out about risers and something about connecting the GPU using an USB, but I couldnt understand how everything works together. Anyone to help with that? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WizardlyBump17"&gt; /u/WizardlyBump17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8mh8m/how_do_you_get_more_gpus_than_your_motheboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8mh8m/how_do_you_get_more_gpus_than_your_motheboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8mh8m/how_do_you_get_more_gpus_than_your_motheboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T02:00:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8ectu</id>
    <title>I plugged a $30 radio into my Mac mini and told my AI "connect to this" — now I control my smart home and send voice messages over radio with zero internet</title>
    <updated>2026-02-18T20:30:14+00:00</updated>
    <author>
      <name>/u/anvarazizov</name>
      <uri>https://old.reddit.com/user/anvarazizov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;So I live in Ukraine during the war. Power goes out a lot here – russia regularly attacks our power grid. When it happens, internet dies, cell towers go dark, and suddenly all my smart home stuff and AI tools become useless. Got tired of it, so I did something kind of ridiculous.&lt;/p&gt; &lt;p&gt;I bought two Lilygo T-Echo radios (~$30 each, LoRa 433MHz, running Meshtastic firmware). Plugged one into my always-on Mac mini via USB. Took the other one as my portable radio. Then I opened up my OpenClaw AI agent and basically said: &amp;quot;hey, there's a Meshtastic radio plugged in. Figure it out.&amp;quot;&lt;/p&gt; &lt;p&gt;And it did.&lt;/p&gt; &lt;h1&gt;What happened next&lt;/h1&gt; &lt;p&gt;It identified the Meshtastic device, installed the CLI, configured an encrypted channel, and then – without me writing a single line of code – built a full Python listener daemon that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Monitors the radio 24/7 for incoming messages&lt;/li&gt; &lt;li&gt;Routes them intelligently: if internet is up, forwards to Discord where a cloud AI responds. If internet is down, routes everything to local models via Ollama&lt;/li&gt; &lt;li&gt;Uses phi4-mini as a lightweight intent classifier (&amp;quot;is this a smart home command or a question?&amp;quot;) and gemma3:12b for actual answers ()&lt;/li&gt; &lt;li&gt;Talks to Home Assistant so I can control lights, read sensors, check who's home — all over radio&lt;/li&gt; &lt;li&gt;Auto-chunks responses to fit the 200-char LoRa limit&lt;/li&gt; &lt;li&gt;Watches an outbox folder – if the AI needs to alert me about something (like a power outage), it drops a message file there and the listener transmits it over LoRa&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The whole thing just worked. The AI had already built the architecture while I was still thinking about how to approach it.&lt;/p&gt; &lt;h1&gt;The voice thing (this is the cool part)&lt;/h1&gt; &lt;p&gt;Then I added one more feature. If I prefix a Meshtastic message with &lt;code&gt;SAY:&lt;/code&gt;, the listener takes the text, calls Home Assistant's TTS service, and plays it through my HA Voice PE speaker at home. In Ukrainian.&lt;/p&gt; &lt;p&gt;So I can be walking around with a T-Echo in my pocket, completely off-grid, type &lt;code&gt;SAY: Привіт, я скоро буду вдома&lt;/code&gt; (Hi, I'll come back home soon) – and my house literally speaks. No internet anywhere in the chain. Just radio waves → Mac mini → TTS → speaker.&lt;/p&gt; &lt;p&gt;Honestly didn't expect it to feel this magical.&lt;/p&gt; &lt;h1&gt;The stack&lt;/h1&gt; &lt;p&gt;Everything's open source except Claude (which is only used when internet is available):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OpenClaw&lt;/strong&gt; – you know what is this &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Meshtastic&lt;/strong&gt; – LoRa mesh networking firmware. The magic sauce for off-grid communication – open source, encrypted, and any Meshtastic radio can relay messages to extend range&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lilygo T-Echo&lt;/strong&gt; – the $30 radio hardware running Meshtastic&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt; – you know as well&lt;/li&gt; &lt;li&gt;&lt;strong&gt;phi4-mini&lt;/strong&gt; – lightweight router/classifier&lt;/li&gt; &lt;li&gt;&lt;strong&gt;gemma3:12b&lt;/strong&gt; – the actual brain for offline responses&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Home Assistant&lt;/strong&gt; – smart home + TTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HA Voice PE&lt;/strong&gt; – the speaker that reads messages aloud&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mac mini M4 16GB&lt;/strong&gt; – always-on server, running on battery backup&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;T-Echo (portable) │ LoRa 433MHz, encrypted ▼ T-Echo (USB) → Mac mini │ ├── SAY: prefix → HA TTS → Voice PE speaker ├── AI: prefix → phi4-mini → gemma3:12b (always local) ├── status → Home Assistant sensors ├── Online? → forward to Discord (cloud AI) └── Offline? → route everything to local Ollama models Outbox: AI drops .msg files → listener sends over LoRa (power outage alerts, reminders, etc.) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;What's next&lt;/h1&gt; &lt;p&gt;I'm thinking about where this goes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mesh AI network&lt;/strong&gt; – Meshtastic is a mesh protocol, every radio relays. Multiple nodes running local LLMs could create a neighborhood-scale AI network with zero internet&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bigger local models&lt;/strong&gt; – looking at upgrading hardware for 30B+ parameter models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dead man's switch&lt;/strong&gt; — auto-alert if I don't check in within a time window&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What do you think? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anvarazizov"&gt; /u/anvarazizov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T20:30:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8jjtq</id>
    <title>More quantization visualization types (repost)</title>
    <updated>2026-02-18T23:51:43+00:00</updated>
    <author>
      <name>/u/copingmechanism</name>
      <uri>https://old.reddit.com/user/copingmechanism</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jjtq/more_quantization_visualization_types_repost/"&gt; &lt;img alt="More quantization visualization types (repost)" src="https://preview.redd.it/af1o3s52cckg1.gif?frame=1&amp;amp;width=140&amp;amp;height=140&amp;amp;auto=webp&amp;amp;s=399ab3abe9aebeae4217cd2925119b0a76b11883" title="More quantization visualization types (repost)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by this post from &lt;a href="/u/VoidAlchemy"&gt;u/VoidAlchemy&lt;/a&gt; a few months back: &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Intrusive thoughts had me try to reproduce and extend the work to include more quantization types, with/without imatrix, and some PPL/KLD measurements to see what an &amp;quot;efficient&amp;quot; quantization looks like. MXFP4 really doesn't like to participate in this sort of experiment, I don't have much faith this is a very accurate representation of the quant but oh-well.&lt;/p&gt; &lt;p&gt;The (vibe) code for this is here &lt;a href="https://codeberg.org/mailhost/quant-jaunt"&gt;https://codeberg.org/mailhost/quant-jaunt&lt;/a&gt; along with a sample of summary output (from lenna.bmp) and some specifications that might help keep the vibes on track.&lt;/p&gt; &lt;p&gt;*reposted to respect Lenna's retirement&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/copingmechanism"&gt; /u/copingmechanism &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r8jjtq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jjtq/more_quantization_visualization_types_repost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8jjtq/more_quantization_visualization_types_repost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-18T23:51:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8vhhq</id>
    <title>ZUNA "Thought-to-Text": a 380M-parameter BCI foundation model for EEG data (Apache 2.0)</title>
    <updated>2026-02-19T10:11:39+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vhhq/zuna_thoughttotext_a_380mparameter_bci_foundation/"&gt; &lt;img alt="ZUNA &amp;quot;Thought-to-Text&amp;quot;: a 380M-parameter BCI foundation model for EEG data (Apache 2.0)" src="https://preview.redd.it/4knvh57lefkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e0e8a3a25b920023bf1670c3f5ded76380521f2" title="ZUNA &amp;quot;Thought-to-Text&amp;quot;: a 380M-parameter BCI foundation model for EEG data (Apache 2.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- Technical paper: &lt;a href="https://zyphra.com/zuna-technical-paper"&gt;https://zyphra.com/zuna-technical-paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Technical blog: &lt;a href="https://zyphra.com/post/zuna"&gt;https://zyphra.com/post/zuna&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Hugging Face: &lt;a href="https://huggingface.co/Zyphra/ZUNA"&gt;https://huggingface.co/Zyphra/ZUNA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- GitHub: &lt;a href="https://github.com/Zyphra/zuna"&gt;https://github.com/Zyphra/zuna&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Zyphra on 𝕏: &lt;a href="https://x.com/ZyphraAI/status/2024114248020898015"&gt;https://x.com/ZyphraAI/status/2024114248020898015&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4knvh57lefkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vhhq/zuna_thoughttotext_a_380mparameter_bci_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8vhhq/zuna_thoughttotext_a_380mparameter_bci_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T10:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8qh08</id>
    <title>I'm 100% convinced that it's the NFT-bros pushing all the openclawd engagement on X</title>
    <updated>2026-02-19T05:13:10+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"&gt; &lt;img alt="I'm 100% convinced that it's the NFT-bros pushing all the openclawd engagement on X" src="https://preview.redd.it/97driy8r0ekg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=43dd9d0bf4042a0843c9b3d69c60aedb8cfa6185" title="I'm 100% convinced that it's the NFT-bros pushing all the openclawd engagement on X" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm absolutely sure of it. The same usual suspects, the same language, the same who stole from whom the next million dollar ideas. It's insane. NFT-bros are now peddling openclawd crypto schemes. It's all the same BS quasi-tech lingo wrapped into neverending posts with meme-like pictures full of slogans, and graphs that literally means less than nothing, that lead back to 'blockchain, blah, blah blah, agentic, blah, blah, prediction markets&amp;quot;. I have enough of this.&lt;/p&gt; &lt;p&gt;Is this the sign of a real bubble? In the fall people were talking on X about how AI is in a bubble - which is never the time for bubbles to burst. But now every grifter discovered AI agents. Now, normally it takes 1-2 years to get from one stage to another, (sorry I'm old) but we are in a super accelerated scenario. Felt like 1998 in fall. It feels we jumped to 2000 suddenly. So IDK. Smells like a bubble is expanding rapidly. Where is my thumbtack?&lt;/p&gt; &lt;p&gt;Is&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/97driy8r0ekg1.png?width=692&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=037d07f7ab4c22bb2356a92c036939830cabe611"&gt;AGI is coming on X (Sign of something?)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T05:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8pztp</id>
    <title>Kitten TTS V0.8 is out: New SOTA Super-tiny TTS Model (Less than 25 MB)</title>
    <updated>2026-02-19T04:48:29+00:00</updated>
    <author>
      <name>/u/ElectricalBar7464</name>
      <uri>https://old.reddit.com/user/ElectricalBar7464</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pztp/kitten_tts_v08_is_out_new_sota_supertiny_tts/"&gt; &lt;img alt="Kitten TTS V0.8 is out: New SOTA Super-tiny TTS Model (Less than 25 MB)" src="https://external-preview.redd.it/Z3FpM3Y4czRyZGtnMWkMiFyATszvzYKXXKWtHcR48BLv2WbhyR3IwK5gi6zR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08b665759677acd0f60146592eee9094aea60bda" title="Kitten TTS V0.8 is out: New SOTA Super-tiny TTS Model (Less than 25 MB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Model introduction:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;New Kitten models are out. Kitten ML has released open source code and weights for three new tiny expressive TTS models - 80M, 40M, 14M (all Apache 2.0)&lt;/p&gt; &lt;p&gt;Discord: &lt;a href="https://discord.com/invite/VJ86W4SURW"&gt;https://discord.com/invite/VJ86W4SURW&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/KittenML/KittenTTS"&gt;https://github.com/KittenML/KittenTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face - Kitten TTS V0.8:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mini 80M: &lt;a href="https://huggingface.co/KittenML/kitten-tts-mini-0.8"&gt;https://huggingface.co/KittenML/kitten-tts-mini-0.8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Micro 40M: &lt;a href="https://huggingface.co/KittenML/kitten-tts-micro-0.8"&gt;https://huggingface.co/KittenML/kitten-tts-micro-0.8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Nano 14M: &lt;a href="https://huggingface.co/KittenML/kitten-tts-nano-0.8"&gt;https://huggingface.co/KittenML/kitten-tts-nano-0.8&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The smallest model is less than 25 MB, and around 14M parameters. All models have a major quality upgrade from previous versions, and can run on just CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features and Advantages&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Eight expressive voices:&lt;/strong&gt; 4 female and 4 male voices across all three models. They all have very high expressivity, with 80M being the best in quality. English support in this release, multilingual coming in future releases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Super-small in size:&lt;/strong&gt; The 14M model is just 25 megabytes. 40M and 80M are slightly bigger, with high quality and expressivity even for longer chunks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runs literally anywhere lol:&lt;/strong&gt; Forget &amp;quot;no GPU required.&amp;quot; This is designed for resource-constrained edge devices. Great news for GPU-poor folks like us.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open source (hell yeah!):&lt;/strong&gt; The models can be used for free under Apache 2.0.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unlocking on-device voice agents and applications:&lt;/strong&gt; Matches cloud TTS quality for most use cases, but runs entirely on-device (can also be hosted on a cheap GPU). If you're building voice agents, assistants, or any local speech application, no API calls needed. Free local inference. Just ship it.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What changed from V0.1 to V0.8:&lt;/strong&gt; Higher quality, expressivity, and realism. Better training pipelines and 10x larger datasets.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectricalBar7464"&gt; /u/ElectricalBar7464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rzgwarr4rdkg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pztp/kitten_tts_v08_is_out_new_sota_supertiny_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8pztp/kitten_tts_v08_is_out_new_sota_supertiny_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T04:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; 👋&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AM–11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;⚠️ &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don’t post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
