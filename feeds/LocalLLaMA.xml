<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-02T05:24:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lp37v0</id>
    <title>LoRA training on NVIDIA Jetson AGX Orin 64GB</title>
    <updated>2025-07-01T14:32:48+00:00</updated>
    <author>
      <name>/u/ahstanin</name>
      <uri>https://old.reddit.com/user/ahstanin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/"&gt; &lt;img alt="LoRA training on NVIDIA Jetson AGX Orin 64GB" src="https://b.thumbs.redditmedia.com/oh8UHfFWv9AwkzOmulxotDC0dlTYauybGEDMEiEkogE.jpg" title="LoRA training on NVIDIA Jetson AGX Orin 64GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/sye5ssnxv9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d312de9207cf7cd5edd21029849b10a3b23bbb5"&gt;https://preview.redd.it/sye5ssnxv9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d312de9207cf7cd5edd21029849b10a3b23bbb5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dy46sdb5x9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=afe062e1e653c8467fb97741a2b2591a467e2c3d"&gt;https://preview.redd.it/dy46sdb5x9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=afe062e1e653c8467fb97741a2b2591a467e2c3d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I successfully ran LoRA training on an NVIDIA Jetson AGX Orin 64GB. Both 8-bit and FP16 modes are working. I'm currently training the Qwen 2.5 7B model. Although the process is slow, it's sufficient for my needs since there's no urgency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ahstanin"&gt; /u/ahstanin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T14:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpd3y7</id>
    <title>Current best options to convert to FP4</title>
    <updated>2025-07-01T20:52:16+00:00</updated>
    <author>
      <name>/u/zelkovamoon</name>
      <uri>https://old.reddit.com/user/zelkovamoon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perplexity hasn't had too much for me - I'm assuming you know better&lt;/p&gt; &lt;p&gt;I have never quantized / converted a full weights model to anything, but since I'm getting a GB10 DGX I want to have options if the model I want isn't already available in FP4. I know TensorRT model optimizer can do it, but it looks like it only supports NV-FP4 and I guess I'd prefer something non proprietary in the spirit of open source. &lt;/p&gt; &lt;p&gt;So what options are there. Which one is the best. &lt;/p&gt; &lt;p&gt;Don't tell me FP4 isn't worth it, not the question, thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zelkovamoon"&gt; /u/zelkovamoon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpd3y7/current_best_options_to_convert_to_fp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpd3y7/current_best_options_to_convert_to_fp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpd3y7/current_best_options_to_convert_to_fp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T20:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lorbc5</id>
    <title>Is the rumours true about Apple abandoning MLX?</title>
    <updated>2025-07-01T03:17:23+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some folks on X are saying&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T03:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lph2zh</id>
    <title>Tool calling with LlamaCpp</title>
    <updated>2025-07-01T23:41:33+00:00</updated>
    <author>
      <name>/u/Dry_Yam_322</name>
      <uri>https://old.reddit.com/user/Dry_Yam_322</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am new to locally hosting LLM with llamaCpp. I am eager to know how people are doing tool calls with it since i am having troubles both while using it as a part of LangChain or when using it with python binding library python-llama-cpp&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;LlamaCpp in LangChain: doesnt allow &amp;quot;auto&amp;quot; as a tool_call parameter and needs user to specify the tools manually. Also cant seem to add more than one tool to tool_choice. I dont know how it is useful with this limitation as how is tool calling useful if LLM cant choose tools by itself based on the prompt.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;With python-llama-cpp: does allow &amp;quot;auto&amp;quot; in parameter and allows multiple tool binding but always return function calling parameters even for prompts which doesnt require tool falling.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Is there any way how i can use llamaCpp for intelligent and automatic tool calling? Any guidance would be appreciated. Thank you!&lt;/p&gt; &lt;p&gt;P.S. - I want to have a functionality in which i could swap the models by passing a command from outside so I am not sure if running local llm on local server and connecting it to openAI compatible api end point would help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Yam_322"&gt; /u/Dry_Yam_322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lph2zh/tool_calling_with_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lph2zh/tool_calling_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lph2zh/tool_calling_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T23:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpi0mn</id>
    <title>Local 405B Model on 3 DGX Spark units.</title>
    <updated>2025-07-02T00:25:15+00:00</updated>
    <author>
      <name>/u/elephantgif</name>
      <uri>https://old.reddit.com/user/elephantgif</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've pre ordered 3 Spark units which will be connected via infiniband at 200 GB/s. While not cheap, all other options that are comperable seem to be much more expensive. AMD's max+ is cheaper, but also less capable, particularly with interconnect. Mac's equivalent has much better memory bandwidth, but that's about it. Tenstorrent's Blackhole is tempting, but lack of literature is too much of a risk for me. I just wanted to check to see if I was missing a better option.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elephantgif"&gt; /u/elephantgif &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T00:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp8e8m</id>
    <title>Good/Best MOE Models for 32GB RAM?</title>
    <updated>2025-07-01T17:50:32+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Please share worthy MOE models for 32GB RAM. Useful for my laptop which has tiny GPU. I'm expecting at least 20 t/s response. Thanks.&lt;/p&gt; &lt;p&gt;Today I tried Qwen3-30B-A3B Q4 (Unsloth Qwen3-30B-A3B-UD-Q4_K_XL - 17GB size). Applied same settings mentioned in unsloth page.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;For non-thinking mode (&lt;code&gt;enable_thinking=False&lt;/code&gt;), we suggest using &lt;strong&gt;Temperature=0.7, TopP=0.8, TopK=20, and MinP=0&lt;/strong&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I use JanAI &amp;amp; used default &lt;strong&gt;Context Size 8192&lt;/strong&gt; only. And tried different values for &lt;strong&gt;GPU Layers&lt;/strong&gt; (-1, 0, 48, etc.,)&lt;/p&gt; &lt;p&gt;After all this, I'm getting only &lt;strong&gt;3-9 t/s&lt;/strong&gt;. Tried Kobaldcpp with same &amp;amp; got same single digit t/s.&lt;/p&gt; &lt;p&gt;Closer to what 14B models, Q4 quants giving me(10-15t/s). I'll be trying to tweak on settings &amp;amp; etc., to increase the t/s since this is my first time I'm trying this size &amp;amp; MOE model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp8e8m/goodbest_moe_models_for_32gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp8e8m/goodbest_moe_models_for_32gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp8e8m/goodbest_moe_models_for_32gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T17:50:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp2h0e</id>
    <title>Training and Finetuning Sparse Embedding Models with Sentence Transformers v5</title>
    <updated>2025-07-01T14:02:02+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2h0e/training_and_finetuning_sparse_embedding_models/"&gt; &lt;img alt="Training and Finetuning Sparse Embedding Models with Sentence Transformers v5" src="https://external-preview.redd.it/Q7oEnpq4LYUPvgpkKMeoddSo-4Wn8UDKMbqnVIBZL8s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2eca6467642c913566d063063339907e970775c0" title="Training and Finetuning Sparse Embedding Models with Sentence Transformers v5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sentence Transformers v5.0 was just released, and it introduced sparse embedding models. These are the kind of search models that are often combined with the &amp;quot;standard&amp;quot; dense embedding models for &amp;quot;hybrid search&amp;quot;. On paper, this can help performance a lot. From the release notes:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A big question is: How do sparse embedding models stack up against the ‚Äústandard‚Äù dense embedding models, and what kind of performance can you expect when combining various?&lt;/p&gt; &lt;p&gt;For this, I ran a variation of our &lt;a href="https://github.com/UKPLab/sentence-transformers/blob/master/examples/sparse_encoder/applications/retrieve_rerank/hybrid_search.py"&gt;hybrid_search.py&lt;/a&gt; evaluation script, with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;a href="https://huggingface.co/datasets/zeta-alpha-ai/NanoMSMARCO"&gt;NanoMSMARCO&lt;/a&gt; dataset (a subset of the MS MARCO eval split)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Embedding-0.6B"&gt;Qwen/Qwen3-Embedding-0.6B&lt;/a&gt; dense embedding model&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/naver/splade-v3-doc"&gt;naver/splade-v3-doc&lt;/a&gt; sparse embedding model, inference free for queries&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/gte-reranker-modernbert-base"&gt;Alibaba-NLP/gte-reranker-modernbert-base&lt;/a&gt; reranker&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Which resulted in this evaluation:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Dense&lt;/th&gt; &lt;th&gt;Sparse&lt;/th&gt; &lt;th&gt;Reranker&lt;/th&gt; &lt;th&gt;NDCG@10&lt;/th&gt; &lt;th&gt;MRR@10&lt;/th&gt; &lt;th&gt;MAP&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;65.33&lt;/td&gt; &lt;td&gt;57.56&lt;/td&gt; &lt;td&gt;57.97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;67.34&lt;/td&gt; &lt;td&gt;59.59&lt;/td&gt; &lt;td&gt;59.98&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;72.39&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;66.99&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;67.59&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;68.37&lt;/td&gt; &lt;td&gt;62.76&lt;/td&gt; &lt;td&gt;63.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;69.02&lt;/td&gt; &lt;td&gt;63.66&lt;/td&gt; &lt;td&gt;64.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;68.28&lt;/td&gt; &lt;td&gt;62.66&lt;/td&gt; &lt;td&gt;63.44&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Here, the sparse embedding model actually already outperforms the dense one, but the real magic happens when combining the two: hybrid search. In our case, we used Reciprocal Rank Fusion to merge the two rankings. &lt;/p&gt; &lt;p&gt;Rerankers also help improve the performance of the dense or sparse model here, but hurt the performance of the hybrid search, as its performance is already beyond what the reranker can achieve.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;So, on paper you can now get more freedom over the &amp;quot;lexical&amp;quot; part of your hybrid search pipelines. I'm very excited about it personally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/train-sparse-encoder"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2h0e/training_and_finetuning_sparse_embedding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2h0e/training_and_finetuning_sparse_embedding_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T14:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpl0u5</id>
    <title>Anyone building or using homegrown local LLM coding assistant?</title>
    <updated>2025-07-02T02:56:03+00:00</updated>
    <author>
      <name>/u/Andvig</name>
      <uri>https://old.reddit.com/user/Andvig</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone building or using homegrown local LLM coding assistant? If so why and how are you finding it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Andvig"&gt; /u/Andvig &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl0u5/anyone_building_or_using_homegrown_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl0u5/anyone_building_or_using_homegrown_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl0u5/anyone_building_or_using_homegrown_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T02:56:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp01c7</id>
    <title>Deepseek R1 at 6,5 tk/s on an Nvidia Tesla P40</title>
    <updated>2025-07-01T12:12:19+00:00</updated>
    <author>
      <name>/u/dc740</name>
      <uri>https://old.reddit.com/user/dc740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I figured I'd post my final setup since many people asked about the P40 and assumed you couldn't do much with it (but you can!).&lt;/p&gt; &lt;pre&gt;&lt;code&gt;numactl --cpunodebind=0 -- ./ik_llama.cpp/build/bin/llama-cli \ --numa numactl \ --model models/unsloth/DeepSeek-R1-0528-GGUF/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00001-of-00006.gguf \ --threads 40 \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ --top-p 0.95 \ --temp 0.6 \ --ctx-size 32768 \ --seed 3407 \ --n-gpu-layers 62 \ -ot &amp;quot;exps=CPU&amp;quot; \ --mlock \ --no-mmap \ -mla 2 -fa -fmoe \ -ser 5,1 \ -amb 512 \ --prompt &amp;quot;&amp;lt;ÔΩúUserÔΩú&amp;gt;Create a Flappy Bird game in Python.&amp;lt;ÔΩúAssistantÔΩú&amp;gt;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result at the end of the run is around 6.5tk/s. &amp;lt;EDIT: Did another run and added the results. 7tk/s!&amp;gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama_print_timings: load time = 896376.08 ms llama_print_timings: sample time = 594.81 ms / 2549 runs ( 0.23 ms per token, 4285.42 tokens per second) llama_print_timings: prompt eval time = 1193.93 ms / 12 tokens ( 99.49 ms per token, 10.05 tokens per second) llama_print_timings: eval time = 363871.92 ms / 2548 runs ( 142.81 ms per token, 7.00 tokens per second) llama_print_timings: total time = 366975.53 ms / 2560 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm open to ideas on how to improve it.&lt;/p&gt; &lt;p&gt;Hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fully populated Dell R740 (in performance profile)&lt;/li&gt; &lt;li&gt;Nvidia Tesla P40 (24GB vram)&lt;/li&gt; &lt;li&gt;Xeon Gold 6138&lt;/li&gt; &lt;li&gt;1.5TB of ram (all ram slots populated)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For other models, like Mistral or QwQ I get around 10tk/s&lt;/p&gt; &lt;p&gt;These are my QwQ settings (I use the regular llama.cpp for this one)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;numactl --cpunodebind=0 -- ./llama.cpp/build/bin/llama-cli \ --numa numactl \ --model models/unsloth/unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \ --threads 40 \ --ctx-size 16384 \ --n-gpu-layers 99 \ --seed 3407 \ --temp 0.6 \ --repeat-penalty 1.1 \ --min-p 0.01 \ --top-k 40 \ --top-p 0.95 \ --dry-multiplier 0.5 \ --mlock \ --no-mmap \ --prio 3 \ -no-cnv \ -fa \ --samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; \ --prompt &amp;quot;&amp;lt;|im_start|&amp;gt;user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&amp;lt;think&amp;gt;\n&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The details on the selected quants are in the model path. Surprisingly, using ik_llama.cpp optimized models from &lt;em&gt;ubergarm&lt;/em&gt; did not speed up Deepseek, but it slowed it down greatly.&lt;/p&gt; &lt;p&gt;Feel free to suggest improvements. For models different than deepseek, ik_llama.cpp was giving me a lot of gibberish output if I enabled fast attention. And some models I couldn't even run on it, so that's why I still use the regular llama.cpp for some of them.&lt;/p&gt; &lt;p&gt;-----&lt;/p&gt; &lt;p&gt;EDIT&lt;/p&gt; &lt;p&gt;I left it running in the background while doing other stuff, and with the community suggestions, I'm up to 7.57 tk/s! Thank you all! (notice that I can now use the 80 threads, but the performance is the same as 40 threads, because the bottleneck is in the memory bandwidth)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;numactl --interleave=all -- ./ik_llama.cpp/build/bin/llama-cli \ --numa numactl \ --model models/unsloth/DeepSeek-R1-0528-GGUF/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00001-of-00006.gguf \ --threads 80 \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ --top-p 0.95 \ --temp 0.6 \ --ctx-size 32768 \ --seed 3407 \ --n-gpu-layers 62 \ -ot &amp;quot;exps=CPU&amp;quot; \ --mlock \ --no-mmap \ -mla 2 -fa -fmoe \ -ser 5,1 \ -amb 512 \ --run-time-repack -b 4096 -ub 4096 \ --prompt &amp;quot;&amp;lt;ÔΩúUserÔΩú&amp;gt;Create a Flappy Bird game in Python.&amp;lt;ÔΩúAssistantÔΩú&amp;gt;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama_print_timings: load time = 210631.90 ms llama_print_timings: sample time = 600.64 ms / 2410 runs ( 0.25 ms per token, 4012.41 tokens per second) llama_print_timings: prompt eval time = 686.07 ms / 12 tokens ( 57.17 ms per token, 17.49 tokens per second) llama_print_timings: eval time = 317916.13 ms / 2409 runs ( 131.97 ms per token, 7.58 tokens per second) llama_print_timings: total time = 320903.99 ms / 2421 tokens &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dc740"&gt; /u/dc740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T12:12:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp5pt0</id>
    <title>Day 7/50: Building a Small Language Model from Scratch ‚Äì Coding Positional Embeddings</title>
    <updated>2025-07-01T16:09:54+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday, we discussed &lt;em&gt;what&lt;/em&gt; &lt;a href="https://www.ideaweaver.ai/blog/day6.html"&gt;positional embeddings &lt;/a&gt;are and &lt;em&gt;why&lt;/em&gt; they‚Äôre essential in Transformer models. Today, let‚Äôs jump into the code and see exactly how they're implemented.&lt;/p&gt; &lt;p&gt;The reference implementation comes from an open-source GPT-style model I‚Äôve been experimenting with &lt;a href="https://github.com/ideaweaver-ai/Tiny-Children-Stories-30M-model"&gt;Tiny Children Stories 30M&lt;/a&gt;. It's designed to generate short children's stories and offers a clean, minimal setup perfect for understanding the internals.&lt;/p&gt; &lt;h1&gt;Quick Recap: Why Transformers Need Positional Embeddings&lt;/h1&gt; &lt;p&gt;Transformer models process all tokens in parallel (unlike RNNs), so they don‚Äôt naturally understand word order. For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;The cat sat on the mat&amp;quot; &amp;quot;The mat sat on the cat&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To a transformer without positional embeddings, those look identical, same tokens, shuffled order, same representation. That‚Äôs a problem.&lt;/p&gt; &lt;h1&gt;What Are Positional Embeddings?&lt;/h1&gt; &lt;p&gt;They‚Äôre additional vectors that encode the &lt;em&gt;position&lt;/em&gt; of each token in the sequence. These are added to token embeddings so that the model knows what the token is and where it is located.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://github.com/ideaweaver-ai/Tiny-Children-Stories-30M-model/blob/main/src/model/gpt.py"&gt;Step-by-Step Code Walkthrough&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;1. Model Config&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;u/dataclass class GPTConfig: vocab_size: int = 50257 block_size: int = 1024 n_layer: int = 6 n_head: int = 8 n_embd: int = 512 dropout: float = 0.1 bias: bool = True &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;block_size&lt;/code&gt; defines the maximum sequence length and thus the number of positional embeddings needed.&lt;/p&gt; &lt;h1&gt;2. Defining the Embedding Layers&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;self.transformer = nn.ModuleDict(dict( wte=nn.Embedding(config.vocab_size, config.n_embd), # token embeddings wpe=nn.Embedding(config.block_size, config.n_embd), # positional embeddings ... )) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Both embeddings are of shape &lt;code&gt;(sequence_length, embedding_dim)&lt;/code&gt;, so they can be added together.&lt;/p&gt; &lt;h1&gt;3. Forward Pass&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pos = torch.arange(0, t, dtype=torch.long, device=device) tok_emb = self.transformer.wte(idx) pos_emb = self.transformer.wpe(pos) x = self.transformer.drop(tok_emb + pos_emb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generate position indices &lt;code&gt;[0, 1, 2, ..., t-1]&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Look up token and position embeddings&lt;/li&gt; &lt;li&gt;Add them&lt;/li&gt; &lt;li&gt;Apply dropout&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Example&lt;/h1&gt; &lt;p&gt;Input: &lt;code&gt;&amp;quot;The cat sat&amp;quot;&lt;/code&gt;&lt;br /&gt; Token IDs: &lt;code&gt;[464, 2368, 3290]&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Token&lt;/th&gt; &lt;th align="left"&gt;Token Embedding&lt;/th&gt; &lt;th align="left"&gt;Positional Embedding&lt;/th&gt; &lt;th align="left"&gt;Combined Embedding&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;The&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.1, -0.3, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.0, 0.1, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.1, -0.2, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cat&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.5, 0.2, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.1, 0.0, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.6, 0.2, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;sat&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[-0.2, 0.8, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.2, -0.1, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;[0.0, 0.7, ‚Ä¶]&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Now the model knows both the identity and the order of the tokens.&lt;/p&gt; &lt;h1&gt;Now the question is why This Matters&lt;/h1&gt; &lt;p&gt;By adding token + position, the model learns:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Semantics&lt;/strong&gt; (what the word is)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context&lt;/strong&gt; (where the word is)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is crucial in generation tasks like storytelling, where position changes meaning.&lt;/p&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fixed length&lt;/strong&gt;: Can‚Äôt handle sequences longer than &lt;code&gt;block_size&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No relative awareness&lt;/strong&gt;: Doesn't know how far two tokens are apart.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sparse training&lt;/strong&gt;: If you never train on long sequences, performance drops.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Alternatives&lt;/h1&gt; &lt;h1&gt;Sinusoidal Positional Embeddings&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;def get_sinusoidal_embeddings(seq_len, embed_dim): pos = torch.arange(seq_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim)) pe = torch.zeros(seq_len, embed_dim) pe[:, 0::2] = torch.sin(pos * div_term) pe[:, 1::2] = torch.cos(pos * div_term) return pe &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Infinite length&lt;/li&gt; &lt;li&gt;No learned parameters&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Relative Positional Embeddings&lt;/h1&gt; &lt;p&gt;Rather than saying &amp;quot;this is position 5&amp;quot;, you tell the model &amp;quot;this token is 3 positions to the left of that one.&amp;quot;&lt;/p&gt; &lt;p&gt;Great for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Reasoning&lt;/li&gt; &lt;li&gt;Long document understanding&lt;/li&gt; &lt;li&gt;Question answering&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Tips&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Don‚Äôt overextend &lt;code&gt;block_size,&lt;/code&gt; it increases memory consumption fast.&lt;/li&gt; &lt;li&gt;Ensure your training data has diverse sequence lengths.&lt;/li&gt; &lt;li&gt;For long inputs, check out RoPE or relative embeddings.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Thoughts&lt;/h1&gt; &lt;p&gt;Positional embeddings are the quiet workhorses of transformer models. Just by combining two vectors (token + position), we enable the model to process ordered text meaningfully.&lt;/p&gt; &lt;p&gt;Without this, a model wouldn't know if ‚ÄúThe End‚Äù belongs at the start or the finish of your story.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Coming Up Next:&lt;/strong&gt;&lt;br /&gt; Tomorrow we‚Äôll dive into Rotary Positional Embeddings (RoPE), a more scalable and elegant solution to position encoding.&lt;/p&gt; &lt;p&gt;If you're following this series, feel free to share or &lt;a href="https://www.linkedin.com/in/prashant-lakhera-696119b/"&gt;connect&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5pt0/day_750_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5pt0/day_750_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5pt0/day_750_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T16:09:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp2ji0</id>
    <title>Reasoning models are risky. Anyone else experiencing this?</title>
    <updated>2025-07-01T14:04:52+00:00</updated>
    <author>
      <name>/u/interviuu</name>
      <uri>https://old.reddit.com/user/interviuu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building a job application tool and have been testing pretty much every LLM model out there for different parts of the product. One thing that's been driving me crazy: reasoning models seem particularly dangerous for business applications that need to go from A to B in a somewhat rigid way.&lt;/p&gt; &lt;p&gt;I wouldn't call it &amp;quot;deterministic output&amp;quot; because that's not really what LLMs do, but there are definitely use cases where you need a certain level of consistency and predictability, you know?&lt;/p&gt; &lt;p&gt;Here's what I keep running into with reasoning models:&lt;/p&gt; &lt;p&gt;During the reasoning process (and I know Anthropic has shown that what we read isn't the &amp;quot;real&amp;quot; reasoning happening), the LLM tends to ignore guardrails and specific instructions I've put in the prompt. The output becomes way more unpredictable than I need it to be.&lt;/p&gt; &lt;p&gt;Sure, I can define the format with JSON schemas (or objects) and that works fine. But the actual content? It's all over the place. Sometimes it follows my business rules perfectly, other times it just doesn't. And there's no clear pattern I can identify.&lt;/p&gt; &lt;p&gt;For example, I need the model to extract specific information from resumes and job posts, then match them according to pretty clear criteria. With regular models, I get consistent behavior most of the time. With reasoning models, it's like they get &amp;quot;creative&amp;quot; during their internal reasoning and decide my rules are more like suggestions.&lt;/p&gt; &lt;p&gt;I've tested almost all of them (from Gemini to DeepSeek) and honestly, none have convinced me for this type of structured business logic. They're incredible for complex problem-solving, but for &amp;quot;follow these specific steps and don't deviate&amp;quot; tasks? Not so much.&lt;/p&gt; &lt;p&gt;Anyone else dealing with this? Am I missing something in my prompting approach, or is this just the trade-off we make with reasoning models? I'm curious if others have found ways to make them more reliable for business applications.&lt;/p&gt; &lt;p&gt;What's been your experience with reasoning models in production?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/interviuu"&gt; /u/interviuu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2ji0/reasoning_models_are_risky_anyone_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2ji0/reasoning_models_are_risky_anyone_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2ji0/reasoning_models_are_risky_anyone_else/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T14:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lplaqk</id>
    <title>Models to run in browser</title>
    <updated>2025-07-02T03:10:05+00:00</updated>
    <author>
      <name>/u/the100rabh</name>
      <uri>https://old.reddit.com/user/the100rabh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;looking from the community to help me guide to selecting a models which can be run in browser. I see most models being too large to be run in browser. Ideally looking for something under a GB. Any suggestions would be helpful.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/the100rabh"&gt; /u/the100rabh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lplaqk/models_to_run_in_browser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lplaqk/models_to_run_in_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lplaqk/models_to_run_in_browser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T03:10:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpkhdc</id>
    <title>Watch a Photo Come to Life: AI Singing Video via Audio-Driven Animation</title>
    <updated>2025-07-02T02:28:13+00:00</updated>
    <author>
      <name>/u/Deep-Jellyfish6717</name>
      <uri>https://old.reddit.com/user/Deep-Jellyfish6717</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpkhdc/watch_a_photo_come_to_life_ai_singing_video_via/"&gt; &lt;img alt="Watch a Photo Come to Life: AI Singing Video via Audio-Driven Animation" src="https://external-preview.redd.it/dDlyZGM3ZGdnZGFmMd6_yl4KD4SDfLlRzY-8FYANWGWkzq3vBHvX2byZMrhl.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d8adc2f94ef20d963b1c6f6311511954660da6f1" title="Watch a Photo Come to Life: AI Singing Video via Audio-Driven Animation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep-Jellyfish6717"&gt; /u/Deep-Jellyfish6717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/71tan5dggdaf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpkhdc/watch_a_photo_come_to_life_ai_singing_video_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpkhdc/watch_a_photo_come_to_life_ai_singing_video_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T02:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpm1k8</id>
    <title>Any recommendations on B200 servers?</title>
    <updated>2025-07-02T03:50:04+00:00</updated>
    <author>
      <name>/u/--pengu--</name>
      <uri>https://old.reddit.com/user/--pengu--</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're finally getting a B200 x8 server. Right now it's between the DGX B200 and ASUS's version. Which one should I go for? Do you have some experience with either of them? Which one would be easier to manage?&lt;/p&gt; &lt;p&gt;p.s. Interestingly, DGX seems to be cheaper. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/--pengu--"&gt; /u/--pengu-- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm1k8/any_recommendations_on_b200_servers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm1k8/any_recommendations_on_b200_servers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm1k8/any_recommendations_on_b200_servers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T03:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp8kfw</id>
    <title>Sophgo TPU SC11 FP300, 256GB, 1.1Tb/s, PCIE-5</title>
    <updated>2025-07-01T17:57:16+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp8kfw/sophgo_tpu_sc11_fp300_256gb_11tbs_pcie5/"&gt; &lt;img alt="Sophgo TPU SC11 FP300, 256GB, 1.1Tb/s, PCIE-5" src="https://external-preview.redd.it/dfaryXk7Svqxh1IPzsdtnwh-Tb9PLhB1df8BVMC0jGM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ed05f87f6e0b04968a548f7e0b236a2438424f8" title="Sophgo TPU SC11 FP300, 256GB, 1.1Tb/s, PCIE-5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.scmp.com/tech/tech-trends/article/3316363/chinese-chipmaker-sophgo-adapts-compute-card-deepseek-beijings-self-reliance-push?module=perpetual_scroll_0&amp;amp;pgtype=article"&gt;https://www.scmp.com/tech/tech-trends/article/3316363/chinese-chipmaker-sophgo-adapts-compute-card-deepseek-beijings-self-reliance-push?module=perpetual_scroll_0&amp;amp;pgtype=article&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SC11 FP300&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8ufk2n2zwaaf1.jpg?width=1453&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=33aa8e8aef095e4db012b08d42ddc4d432e49416"&gt;https://preview.redd.it/8ufk2n2zwaaf1.jpg?width=1453&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=33aa8e8aef095e4db012b08d42ddc4d432e49416&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ktec5womwaaf1.png?width=1459&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c131257e9cd12b940a5780570891ee556d0c9a4"&gt;https://preview.redd.it/ktec5womwaaf1.png?width=1459&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c131257e9cd12b940a5780570891ee556d0c9a4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I didn't find the price, but I found these tables&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp8kfw/sophgo_tpu_sc11_fp300_256gb_11tbs_pcie5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp8kfw/sophgo_tpu_sc11_fp300_256gb_11tbs_pcie5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp8kfw/sophgo_tpu_sc11_fp300_256gb_11tbs_pcie5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T17:57:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpl3mv</id>
    <title>Hosting your local Huanyuan A13B MOE</title>
    <updated>2025-07-02T03:00:05+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/"&gt; &lt;img alt="Hosting your local Huanyuan A13B MOE" src="https://external-preview.redd.it/gQRnTfcC6aPcmeuf8YDaS3B32gWvzvTmzY9W4xzLeHo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2aec10232e41be314e9b6831041b90cf9d7b4600" title="Hosting your local Huanyuan A13B MOE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/70byco93mdaf1.png?width=2353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=226d3dc6055ad2ad9c952ed13dca4a1451ae5d2a"&gt;https://preview.redd.it/70byco93mdaf1.png?width=2353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=226d3dc6055ad2ad9c952ed13dca4a1451ae5d2a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;it is a PR of ik_llama.cpp, by ubergarm , not yet merged.&lt;/p&gt; &lt;p&gt;Instruction to compile, by ubergarm (from: &lt;a href="https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF#note-building-experimental-prs"&gt;ubergarm/Hunyuan-A13B-Instruct-GGUF ¬∑ Hugging Face&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# get the code setup cd projects git clone https://github.com/ikawrakow/ik_llama.cpp.git git ik_llama.cpp git fetch origin git remote add ubergarm https://github.com/ubergarm/ik_llama.cpp git fetch ubergarm git checkout ug/hunyuan-moe-2 git checkout -b merge-stuff-here git merge ikawrakow/ik/iq3_ks_v2 # build for CUDA cmake -B build -DCMAKE_BUILD_TYPE=Release -DGGML_CUDA=ON -DGGML_VULKAN=OFF -DGGML_RPC=OFF -DGGML_BLAS=OFF -DGGML_CUDA_F16=ON -DGGML_SCHED_MAX_COPIES=1 cmake --build build --config Release -j $(nproc) # clean up later if things get merged into main git checkout main git branch -D merge-stuff-here ``` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GGUF download: &lt;a href="https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF/tree/main"&gt;ubergarm/Hunyuan-A13B-Instruct-GGUF at main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the running command (better read it here, and modified by yourself):&lt;br /&gt; &lt;a href="https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF#note-building-experimental-prs"&gt;ubergarm/Hunyuan-A13B-Instruct-GGUF ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;a api/webui hosted by ubergarm, for early testing&lt;br /&gt; WebUI: &lt;a href="https://llm.ubergarm.com/"&gt;https://llm.ubergarm.com/&lt;/a&gt;&lt;br /&gt; APIEndpoint: &lt;a href="https://llm.ubergarm.com/"&gt;https://llm.ubergarm.com/&lt;/a&gt; (it is llama-server API endpoint with no API key)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T03:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpmx00</id>
    <title>I built a cli tool to automatically figure out tensor overrides in llama.cpp</title>
    <updated>2025-07-02T04:38:28+00:00</updated>
    <author>
      <name>/u/kevin_1994</name>
      <uri>https://old.reddit.com/user/kevin_1994</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone&lt;/p&gt; &lt;p&gt;Running MoE models on my machine, I'm constantly frustrated working with `--overide-tensor` regexes in llama.cpp. They're hard to maintain, break easily, and are unreadable &lt;/p&gt; &lt;p&gt;I built a little cli tool which builds these `--override-tensor` arguments automatically for your architecture.&lt;/p&gt; &lt;p&gt;On my machine (Xeon e5 2699v3, 128GB DDR4, 2x3090, 1x3060) this runs Qwen3 235B Q4XL at 5.5 tok/s&lt;/p&gt; &lt;pre&gt;&lt;code&gt;#!/bin/bash export CUDA_VISIBLE_DEVICES=2,0,1 # Generate tensor overrides TENSOR_OVERRIDES=$(gguf-tensor-overrider -g https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00001-of-00003.gguf -c 32000 --gpu-percentage 0.85) # Build command with tensor overrides CMD=&amp;quot;/home/kevin/llama.cpp/build/bin/llama-cli \ -hf unsloth/Qwen3-235B-A22B-GGUF:Q4_K_XL \ -c 32000 \ -fa \ -sm row \ $TENSOR_OVERRIDES&amp;quot; # Execute command directly (no pipe) eval &amp;quot;$CMD&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; hey there &amp;lt;think&amp;gt; Okay, the user just said &amp;quot;hey there&amp;quot;. That's pretty casual. I should respond in a friendly and welcoming way. Maybe ask how they're doing and offer help. Let me keep it simple and approachable. I need to make sure the response is open-ended so they feel comfortable to ask anything. Avoid any technical jargon. Just a warm greeting and an offer to assist with whatever they need. Yeah, that should work. &amp;lt;/think&amp;gt; Hello! How can I assist you today? üòä &amp;gt; llama_perf_sampler_print: sampling time = 15.58 ms / 114 runs ( 0.14 ms per token, 7318.01 tokens per second) llama_perf_context_print: load time = 152623.89 ms llama_perf_context_print: prompt eval time = 1918.59 ms / 10 tokens ( 191.86 ms per token, 5.21 tokens per second) llama_perf_context_print: eval time = 18799.44 ms / 103 runs ( 182.52 ms per token, 5.48 tokens per second) llama_perf_context_print: total time = 30823.94 ms / 113 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These commands should also work with ik_llama.cpp. 5.5 tok/s is about what I was getting before with ik_llama.cpp.&lt;/p&gt; &lt;p&gt;Here is the link to the repository: &lt;a href="https://github.com/k-koehler/gguf-tensor-overrider/tree/main"&gt;https://github.com/k-koehler/gguf-tensor-overrider&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hopefully some of your find this useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kevin_1994"&gt; /u/kevin_1994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T04:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp653l</id>
    <title>Reuse non-prefix KV Cache and speed up RAG by 3X with LMCache.</title>
    <updated>2025-07-01T16:26:03+00:00</updated>
    <author>
      <name>/u/Nice-Comfortable-650</name>
      <uri>https://old.reddit.com/user/Nice-Comfortable-650</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/"&gt; &lt;img alt="Reuse non-prefix KV Cache and speed up RAG by 3X with LMCache." src="https://preview.redd.it/9eq6ted4haaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=576ff3381e11049410b474e07e3ae6108a604a27" title="Reuse non-prefix KV Cache and speed up RAG by 3X with LMCache." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey&lt;a href="https://www.reddit.com/r/MachineLearning/"&gt; &lt;/a&gt;&lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;A while back, we shared our open-source project LMCache here and were blown away by the incredible support and feedback. Today, our team is thrilled to share more about one of our core components: &lt;strong&gt;CacheBlend&lt;/strong&gt;. Recognized with a &lt;strong&gt;Best Paper Award at ACM EuroSys 2025,&lt;/strong&gt; this technique is a pain killer for efficient RAG applications &lt;/p&gt; &lt;h1&gt;The Problem: Your KV Cache is Wasting Potential&lt;/h1&gt; &lt;p&gt;In modern LLM applications like RAG and Agents, we constantly feed the model new context. For example, in RAG, we retrieve relevant documents and stuff them into the prompt.&lt;/p&gt; &lt;p&gt;The issue is that this dynamically retrieved context doesn't always appear at the beginning of the input sequence. Traditional KV caching only reuses a &amp;quot;common prefix,&amp;quot; so if the new information isn't at the very start, the cache hit rate plummets, and your GPU ends up recomputing the same things over and over.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution: CacheBlend - 100% Hit Rate, No Compromises&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;CacheBlend changes the game by allowing for the reuse of pre-computed KV caches &lt;strong&gt;regardless of their position in the input sequence&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This means we can finally achieve a &lt;strong&gt;100% KV Cache hit rate&lt;/strong&gt; in applications like RAG. The performance gains are significant:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Faster Time-To-First-Token (TTFT):&lt;/strong&gt; Get your initial response much quicker.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More Throughput:&lt;/strong&gt; Serve significantly more users with the same hardware.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Almost lossless Output Quality:&lt;/strong&gt; All of this is achieved with little degradation in the model's generation quality.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How does it work?&lt;/h1&gt; &lt;p&gt;CacheBlend intelligently handles the two main challenges of reusing non-prefix caches:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Positional Encoding Update:&lt;/strong&gt; It efficiently updates positional encodings to ensure the model always knows the correct position of each token, even when we're stitching together cached and new data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Selective Attention Recalculation:&lt;/strong&gt; Instead of recomputing everything, it strategically recalculates only the minimal cross-attention needed between the new and cached chunks to maintain perfect generation quality.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For detailed analysis, please refer to the official paper: &lt;a href="https://dl.acm.org/doi/10.1145/3689031.3696098"&gt;https://dl.acm.org/doi/10.1145/3689031.3696098&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Where can I try it?&lt;/h1&gt; &lt;p&gt;Try the newest interactive CacheBlend demo at: &lt;a href="https://github.com/LMCache/LMCache-Examples/tree/main/demo-rag-blending"&gt;https://github.com/LMCache/LMCache-Examples/tree/main/demo-rag-blending&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ask us anything!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nice-Comfortable-650"&gt; /u/Nice-Comfortable-650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9eq6ted4haaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T16:26:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpm6cv</id>
    <title>ERNIE-4.5-VL-28B-A3B is a hidden gem that can decently tackle challenging chinese/japanese OCR problems.</title>
    <updated>2025-07-02T03:57:24+00:00</updated>
    <author>
      <name>/u/mixivivo</name>
      <uri>https://old.reddit.com/user/mixivivo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm6cv/ernie45vl28ba3b_is_a_hidden_gem_that_can_decently/"&gt; &lt;img alt="ERNIE-4.5-VL-28B-A3B is a hidden gem that can decently tackle challenging chinese/japanese OCR problems." src="https://b.thumbs.redditmedia.com/A5xBWO7s3WJ33IXN1UsaHXSRINAm2j5ql5UQhM4yZSM.jpg" title="ERNIE-4.5-VL-28B-A3B is a hidden gem that can decently tackle challenging chinese/japanese OCR problems." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Âõæ‰∏≠ÊñáÊú¨ËΩ¨ÂΩïÂ¶Ç‰∏ãÔºö&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;ÂÄ≠ÁéãÊ≠¶„ÅÆ‰∏äË°®Êñá&lt;/p&gt; &lt;p&gt;ÂÄ≠„Éª‰ªªÈÇ£„ÉªÂä†ÁΩó„ÉªÁß¶Èü©„ÉªÊÖïÈü©‰∏ÉÂõΩËØ∏ÂÜõ‰∫ãÂÆâ‰∏úÂ§ßÂ∞ÜÂÜõÁΩó„Éª‰ªªÈÇ£„ÉªÂä†ÁΩó„ÉªÁß¶Èü©„ÉªÊÖïÈü©‰∏ÉÂõΩËØ∏ÂÜõ‰∫ãÂÆâ‰∏úÂ§ßÂ∞ÜÂÜõÂÄ≠ÂõΩÁéã„Å®Áß∞„Åô„ÄÇÈ°∫Â∏ù„ÅÆÊòáÊòé‰∫åÂπ¥‚ë†‰ΩøÈÅ£„Åó„Å¶‰∏äË°®„Åô„Çã„ÄÇÊòî„Åó„Å¶Êõ∞„Åè„ÄÅÂ∞ÅÂõΩ‚ë°„ÅØÂÅèÈÅó„Åó„Å¶Ëó©„ÇíÂ§ñ„Å´‰Ωú„Çã„ÄÇÊòî„Çà„ÇäÁ•ñÁ•¢‚ë¢Ë∫¨Áî≤ËÉÑÊèîÊñ°„ÄÅÂ±±Â∑ù„ÇíË∑ãÊ∂â„Åó„Å¶ÂØõÂ§Ñ‚ë£„Å´Ëøõ„ÇÅ„ÅÇ„Åö„ÄÅË•ø„ÅØË°ÜÂ§∑‚ë•„ÇíÊúç„Åô„Çã„Åì„Å®„Å´ÂÖ≠ÂçÅÂÖ≠ÂõΩ„ÄÅÊ∏°„Å£„Å¶Êµ∑Âåó‚ë¶„ÇíÂπ≥„Åè„Çã„Åì„Å®‰πùÂçÅ‰∫îÂõΩ„ÄÇ&lt;/p&gt; &lt;p&gt;(ÂÆã‰π¶ ÂÄ≠ÂõΩ‰º† ÂéüÊ±âÊñá)&lt;/p&gt; &lt;p&gt;‚ë†Âõõ‰∏ÉÂÖ´Âπ¥„ÄÇ‚ë°È¢ÜÂüé„ÄÅËá™ÂàÜ„ÅÆÂõΩ„ÅÆ„Åì„Å®„ÄÇ‚ë¢Áà∂Á•ñ„Å®„ÅÑ„ÅÜËØ¥„Å®„Åå„ÅÇ„Çã„ÄÇ‚ë£„Åä„Å°„Å§„ÅÑ„Å¶„ÅÆÊúÄ„ÇÇ„Å™„ÅÑ„ÄÇ‚ë§Ëõ≠È°µ„ÅÆ„Åì„Å®„Å®„Åã„ÄÇ‚ë¶ÊúùÈ≤úÂçäÂ≤õ„ÅÆ„Åì„Å®„Åã„ÄÇ&lt;/p&gt; &lt;p&gt;Á´ñÁ©¥ÂºèÁü≥ÂÆ§„ÅÆÊ®°ÂºèÂõ≥&lt;/p&gt; &lt;p&gt;„ÄêÊó•Êú¨Êõ∏Á¥Ä„Äë„ÄêÂÆãÊõ∏„Äë&lt;/p&gt; &lt;p&gt;ÂÄ≠„ÅÆ‰∫îÁéã„Å®Â§©Áöá&lt;/p&gt; &lt;p&gt;„ÄåÂÆãÊõ∏„ÄçÂÄ≠‰ºù„Å´ËØª„ÉªÁèç(ÂΩå)„ÉªÊµé„ÉªÂ••„ÉªÊ≠¶„ÅÆ‰∫îÁéã„ÅÆÂêç„ÅåËÆ∞„Åï„Çå„Å¶„Çã„ÄÇÊµé‰ª•‰∏ã„ÅØËÆ∞Á∫™„Å´‰ºù„Åà„ÇãÂ∞§ÊÅ≠„ÉªÂÆâÂ∫∑„ÉªÈõÑÁï•„ÅÆÂêÑÂ§©Áöá„Å´„ÅÇ„Å¶„Çâ„Çå„Çã„Åå„ÄÅËØª„Å´„ÅØÂø§Á•û„Éª‰ªÅÂæ∑„ÉªÂ±•‰∏≠Â§©Áöá„Çí„ÅÇ„Å¶„Å¶„ÇãËØ∏ËØ¥„Åå„ÅÇ„Çã„ÄÇÁèç„Å´„ÇÇ‰ªÅÂæ∑„ÉªÂèçÊ≠£Â§©Áöá„ÅÇ„Å¶„Å¶„Çã2ËØ¥„Åå„ÅÇ„Çã„ÄÇ&lt;/p&gt; &lt;p&gt;Á∫™„Å´„Åã„Åë„Å¶„ÅÆ„Åì„Å®„Åß„ÅÇ„Çã„ÄÇÈ´òÂè•È∫ó„ÅÆÂ•ΩÂ§™Áéã„ÅÆÁ¢ëÊñá‚ë†„Å´„ÅØ„ÄÅÂÄ≠„ÅåÊúùÈ≤úÂçäÂ≤õ„Å´ËøõÂá∫„ÅóÈ´òÂè•È∫ó„Å®‰∫§Êà¶„Åó„Åü„Åì„Å®„ÅåËÆ∞„Åï„Çå„Å¶„ÅÑ„Çã„ÄÇ„Åì„Çå„ÅØ„ÄÅÂ§ßÂíåÊîøÊ®©„ÅåÊúùÈ≤úÂçäÂ≤õ„ÅÆËøõ„Çì„Å†ÊäÄÊúØ„ÇÑÈâÑËµÑÊ∫ê„ÇíËé∑Âæó„Åô„Çã„Åü„ÇÅ„Å´Âä†ÁΩó(‰ªªÈÇ£)„Å´ËøõÂá∫„Åó„ÄÅ„Åù„Åì„ÇíÊã†ÁÇπ„Å®„Åó„Å¶È´òÂè•È∫ó„ÅÆÂäøÂäõ„Å®ÂØπÊäó„Åó„Åü„Åì„Å®„ÇíÁâ©ËØ≠„Å£„Å¶„ÅÑ„Çã„ÄÇ&lt;/p&gt; &lt;p&gt;„ÄåÂÆã‰π¶„Äç„Å™„Å©„Å´„ÅØ„ÄÅ5‰∏ñÁ∫™Âàù„ÇÅ„Åã„Çâ„Åª„Åº1‰∏ñÁ∫™„ÅÆÈó¥„ÄÅÂÄ≠„ÅÆ‰∫îÁéã„Åå‰∏≠ÂõΩ„ÅÆÂçóÊúù„Å´ÊúùË¥°„Åó„ÄÅÈ´ò„ÅÑÁß∞Âè∑„Çí„Åà„Çà„ÅÜ„Å®„Åó„Åü„Åì„Å®„ÅåËÆ∞„Åï„Çå„Å¶„ÅÑ„Çã„ÄÇ„Åì„Çå„ÅØ‰∏≠ÂõΩ„ÅÆÁöáÂ∏ù„ÅÆÊ®©Â®Å„ÇíÂà©Áî®„Åó„Å¶„ÄÅÊúùÈ≤úËØ∏ÂõΩ„Å´ÂØæ„Åô„ÇãÊîøÊ≤ªÁöÑÁ´ãÂú∫„ÇíÊúâÂà©„Å´„Åó„Çà„ÅÜ„Å®„Åó„Åü„ÇÇ„ÅÆ„Å®ËÄÉ„Åà„Çâ„Çå„Çã„ÄÇ&lt;/p&gt; &lt;p&gt;ÊúùÈ≤úÂçäÂ≤õ„Éª‰∏≠ÂõΩÂçóÊúù„Å®„ÅÆ‰∫§Ê∏â„Çí„Å§„Å•„Åò„Å¶„ÄÅÂ§ßÂíåÊîøÊ®©„ÅØÂ§ßÈôÜ„ÅÆËøõ„Çì„Å†ÊäÄÊúØ„Å®ÊñáÂåñ„Çí„Å®„Çä„ÅÑ„Çå„ÄÅÂäø„ÅÑ„ÇíÂº∫„ÇÅ„Åü„ÄÇ4‰∏ñÁ∫™Êú´„Åã„Çâ5‰∏ñÁ∫™„Å´„Åã„Åë„Å¶„ÅÆ‰∏≠„ÅÆÂè§Â¢≥„ÅØÊÄ•ÊøÄ„Å´Â∑®Â§ßÂåñ„Åó„ÄÅÂ§ßÂíåÊîøÊ®©„ÅÆÊúÄÈ´ò„ÅÆÈ¶ñÈïø„Åß„ÅÇ„ÇãÂ§ßÁéã‚ë°„ÅÆÊ®©Âäõ„ÅåÂº∫Â§ßÂåñ„Åó„Åü„Åì„Å®„ÇíÁâ©ËØ≠„Å£„Å¶„ÅÑ„Çã„ÄÇ&lt;/p&gt; &lt;p&gt;‚ë† Â•ΩÂ§™Áéã(Â∫ÉÂºÄÂúüÁéã)‰∏Ä‰ª£„ÅÆ‰∫ã‰∏ö„ÇíËÆ∞„Åó„ÅüÁü≥Á¢ë„Åß„ÄÅÈ´òÂè•È∫ó„ÅÆÈÉΩ„ÅÆ„ÅÇ„Å£„Åü‰∏≠ÂõΩÂêâÊûóÁúÅÈõÜÂÆâÁúå„Å´„ÅÇ„Çã„ÄÇÂΩìÊó∂„ÅÆÊúùÈ≤úÂçäÂ≤õ„ÅÆÊÉÖÂäø„ÇíÁü•„Çã„Åü„ÇÅ„ÅÆË¥µÈáç„Å™Âè≤Êñô„Åß„ÄÅ„Åù„ÅÆ„Å™„Åã„Å´„ÄåÁôæÊ∏à(ÁôæÊµé)„ÄçÊñ∞ÁΩó„ÅØÊóßÊòØÂ±ûÊ∞ë„Çä„ÄÇÁî±Êù•ÊúùË¥°„Åô„ÄÇËÄå„Çã„Å´ÂÄ≠„ÄÅËæõÂçØ„ÅÆÂπ¥(391Âπ¥)„Çà„Çä„Åì„ÅÆ„Åã„Åü„ÄÅÊµ∑Ê∏°„Å£„Å¶ÁôæÊ∏à‚ñ°‚ñ°‚ñ°ÁΩó„ÇíÁ†¥„Çä„ÄÅ‰ª•„Å£„Å¶Ëá£Ê∞ë„Å®„ÅÇ„Åö„ÄÅÊó•Êú¨„ÅÆÊúùÈ≤úÂçäÂ≤õ„Å∏„ÅÆËøõÂá∫„Çí‰ºù„Åà„Å¶„ÅÑ„Çã„ÄÇ&lt;/p&gt; &lt;p&gt;‚ë° ÁÜäÊú¨ÁúåÁéâÂêçÈÉ°ËèäÊ∞¥Áî∫„ÅÆÊ±üÁî∞ËàπÂ±±Âè§Â¢≥Âá∫Âúü„ÅÆÂ§ßÂàÄÈì≠„Å´„ÅØ„ÄåÊ≤ªÂ§©‰∏ãÁå®‚ñ°‚ñ°‚ñ°ÁΩóÂ§ßÁéã‰∏ñ‚Ä¶‚Ä¶„Äç„Å®„ÅÇ„Çä„ÄÅÂüºÁéâÁúåË°åÁî∞Â∏Ç„ÅÆÊ•¢Ëç∑Â±±Âè§Â¢≥Âá∫Âúü„ÅÆÈìÅÂäîÈì≠(‚Üíp.26Âõ≥Áâà)„Å´„ÇÇ„ÄåÂÄ≠Âä†Â§öÊîØÊñáÂ§ßÁéã„Äç„Å®„ÇÇ„Å™„Çã„ÄÇ„ÄåÂ§ßÁéã„Äç„ÅØ„ÄÅÂÄ≠„ÅÆ‰∫îÁéã„ÅÆ1‰∫∫Ê≠¶„ÄÅËÆ∞Á∫™Ôºà„ÄåÂè§‰∫ãËÆ∞„Äç„ÄåÊó•Êú¨‰π¶Á∫™„ÄçÔºâ„Å´„ÉØ„Ç´„Çø„Ç±„É´„ÅÆÂêç„ÅßËÆ∞Èå≤„Åï„Çå„ÅüÈõÑÁï•Â§©Áöá„Çí„Åï„Åô„Å®ËÄÉ„Åà„Çâ„Çå„Çã„ÄÇ„Åì„Çå„Çâ„ÅÆÂ§ßÂàÄ„ÇÑÈìÅÂäî„Çí„ÇÇ„Å§Âè§Â¢≥„ÅÆË¢´Ëë¨ËÄÖ„ÅØ„ÄÅÂ§ßÂíåÊîøÊ®©„Å®ÂØÜÊé•„Å™Èñ¢Á≥ª„Å´„ÅÇ„Å£„Åü„Å®Êé®Êµã„Åï„Çå„Çã„ÄÇ&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mixivivo"&gt; /u/mixivivo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lpm6cv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm6cv/ernie45vl28ba3b_is_a_hidden_gem_that_can_decently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm6cv/ernie45vl28ba3b_is_a_hidden_gem_that_can_decently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T03:57:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpejnj</id>
    <title>Qwen3 inference engine in C: simple, educational, fun</title>
    <updated>2025-07-01T21:49:58+00:00</updated>
    <author>
      <name>/u/adrian-cable</name>
      <uri>https://old.reddit.com/user/adrian-cable</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who may be interested, a free-time project that I've now put up on Github: &lt;a href="https://github.com/adriancable/qwen3.c"&gt;https://github.com/adriancable/qwen3.c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Run Qwen3-architecture models (like Qwen3-4B, or DeepSeek-R1-0528-Qwen3-8B) locally, no GPU required, using an LLM inference engine you build yourself from just 1 file of C source, with no dependencies. Only requirement is enough RAM to load the models. Think llama.cpp but 100X smaller and simpler, although it's still very functional: multi-language input/output, multi-core CPU support, supports reasoning/thinking models etc.&lt;/p&gt; &lt;p&gt;All you need to build and run is Python3 and a C compiler. The C source is so small, it compiles in around a second. Then, go have fun with the models!&lt;/p&gt; &lt;p&gt;After you've played around for a bit, if you already understand a bit about how transformers work but want to really learn the detail, the inference engine's C source (unlike llama.cpp) is small enough to dig into without getting a heart attack. Once you've understood how it ticks, you're a transformers expert! üòÉ&lt;/p&gt; &lt;p&gt;Not intended to compete with 'heavyweight' engines like llama.cpp, rather, the focus is on being (fun)ctional and educational.&lt;/p&gt; &lt;p&gt;MIT license so you can do whatever you want with the source, no restrictions.&lt;/p&gt; &lt;p&gt;Project will be a success if at least one person here enjoys it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrian-cable"&gt; /u/adrian-cable &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T21:49:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp5nhy</id>
    <title>Gemma 3n Fine-tuning now in Unsloth - 1.5x faster with 50% less VRAM + Fixes</title>
    <updated>2025-07-01T16:07:29+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"&gt; &lt;img alt="Gemma 3n Fine-tuning now in Unsloth - 1.5x faster with 50% less VRAM + Fixes" src="https://external-preview.redd.it/Z9xq13sybpzZE8ZKJtOY4SuppgTg5x6rIeOPF_Fl1qk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7593f97dd0c1af68e044aad5a89b7cf7f0e2b642" title="Gemma 3n Fine-tuning now in Unsloth - 1.5x faster with 50% less VRAM + Fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLlama! We made finetuning Gemma 3N 1.5x faster in a free Colab with &lt;a href="http://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; in under 16GB of VRAM! We also managed to find and fix issues for Gemma 3N:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ollama &amp;amp; GGUF fixes&lt;/strong&gt; - All Gemma 3N GGUFs could not load in Ollama properly since &lt;code&gt;per_layer_token_embd&lt;/code&gt; had loading issues. Use our quants in Ollama for our fixes. All dynamic quants in our &lt;a href="https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339"&gt;Gemma 3N collection&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NaN and infinities in float16 GPUs&lt;/strong&gt; - we found Conv2D weights (the vision part) have very large magnitudes - we upcast them to float32 to remove infinities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v2w9vippbaaf1.jpg?width=1888&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9c617026ca9deecc699787547badded628f081bc"&gt;Green crosses are large Conv2D weights&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Free Colab to fine-tune Gemma 3N 4B&lt;/strong&gt; in a free Colab + audio + text + vision inference: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb&lt;/a&gt;-Conversational.ipynb)&lt;/p&gt; &lt;p&gt;Update Unsloth via &lt;code&gt;pip install --upgrade unsloth unsloth_zoo&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from unsloth import FastModel import torch model, tokenizer = FastModel.from_pretrained( model_name = &amp;quot;unsloth/gemma-3n-E4B-it&amp;quot;, max_seq_length = 1024, load_in_4bit = True, full_finetuning = False, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Detailed technical analysis&lt;/strong&gt; and guide on how to use Gemma 3N effectively: &lt;a href="https://docs.unsloth.ai/basics/gemma-3n"&gt;https://docs.unsloth.ai/basics/gemma-3n&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also uploaded GGUFs for the new FLUX model: &lt;a href="https://huggingface.co/unsloth/FLUX.1-Kontext-dev-GGUF"&gt;https://huggingface.co/unsloth/FLUX.1-Kontext-dev-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T16:07:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpl656</id>
    <title>GLM-4.1V-Thinking</title>
    <updated>2025-07-02T03:03:37+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl656/glm41vthinking/"&gt; &lt;img alt="GLM-4.1V-Thinking" src="https://external-preview.redd.it/bgfOhKzxcgalBLIa5eyKdcFfts71dHE0qj65OmHVMu0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1f66974e5478d143d6f55b57fcf633e79edaf66" title="GLM-4.1V-Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/THUDM/glm-41v-thinking-6862bbfc44593a8601c2578d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl656/glm41vthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl656/glm41vthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T03:03:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp9gh2</id>
    <title>Huawei releases an open weight model Pangu Pro 72B A16B. Weights are on HF. It should be competitive with Qwen3 32B and it was trained entirely on Huawei Ascend NPUs. (2505.21411)</title>
    <updated>2025-07-01T18:30:51+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/"&gt; &lt;img alt="Huawei releases an open weight model Pangu Pro 72B A16B. Weights are on HF. It should be competitive with Qwen3 32B and it was trained entirely on Huawei Ascend NPUs. (2505.21411)" src="https://external-preview.redd.it/KKUaRRu1NZXsmquOk2Id9DRnEhBD6P6w5Y5xZQur5Yc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd39a4d6488e7f71969bdc8665d7c2dbe902c2b5" title="Huawei releases an open weight model Pangu Pro 72B A16B. Weights are on HF. It should be competitive with Qwen3 32B and it was trained entirely on Huawei Ascend NPUs. (2505.21411)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/IntervitensInc/pangu-pro-moe-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T18:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpep3m</id>
    <title>Tenstorrent Blackhole Cards</title>
    <updated>2025-07-01T21:56:17+00:00</updated>
    <author>
      <name>/u/SashaUsesReddit</name>
      <uri>https://old.reddit.com/user/SashaUsesReddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpep3m/tenstorrent_blackhole_cards/"&gt; &lt;img alt="Tenstorrent Blackhole Cards" src="https://preview.redd.it/ffghybw34caf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7e024c8281faff0ddc04029b2d8b6f4dc59b373" title="Tenstorrent Blackhole Cards" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got in some Blackhole p150b cards! Excited to try these out... Anyone else on here running some of these? Curious to collaborate! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SashaUsesReddit"&gt; /u/SashaUsesReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ffghybw34caf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpep3m/tenstorrent_blackhole_cards/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpep3m/tenstorrent_blackhole_cards/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T21:56:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lphhj3</id>
    <title>DeepSeek-r1-0528 in top 5 on new SciArena benchmark, the ONLY open-source model</title>
    <updated>2025-07-01T23:59:59+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lphhj3/deepseekr10528_in_top_5_on_new_sciarena_benchmark/"&gt; &lt;img alt="DeepSeek-r1-0528 in top 5 on new SciArena benchmark, the ONLY open-source model" src="https://preview.redd.it/xxfqfefhpcaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=525931b3ac9b9155ccc34e486fc5f097170ed00c" title="DeepSeek-r1-0528 in top 5 on new SciArena benchmark, the ONLY open-source model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post: &lt;a href="https://allenai.org/blog/sciarena"&gt;https://allenai.org/blog/sciarena&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Allen AI puts out good work and contributes heavily to open-source, I am a big fan of Nathan Lambert. &lt;/p&gt; &lt;p&gt;They just released this scientific literature research benchmark and DeepSeek-r1-0528 is the &lt;strong&gt;only&lt;/strong&gt; open-source model in the top 5, sharing the pie with the like of OpenAI's o3, Claude 4 Open, and Gemini 2.5 Pro.&lt;/p&gt; &lt;p&gt;I like to trash DeepSeek here, but not anymore. This level of performance is just insane.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xxfqfefhpcaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lphhj3/deepseekr10528_in_top_5_on_new_sciarena_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lphhj3/deepseekr10528_in_top_5_on_new_sciarena_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T23:59:59+00:00</published>
  </entry>
</feed>
