<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-11T22:00:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r1lkfw</id>
    <title>My NAS runs an 80B LLM at 18 tok/s on its iGPU. No discrete GPU. Still optimizing.</title>
    <updated>2026-02-11T02:52:10+00:00</updated>
    <author>
      <name>/u/BetaOp9</name>
      <uri>https://old.reddit.com/user/BetaOp9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I didn't want to buy two systems. That was the whole thing.&lt;/p&gt; &lt;p&gt;I needed a NAS. I also wanted to mess around with local LLMs. And I really didn't want to explain to my wife why I needed a second box just to talk to a chatbot that sometimes hallucinates, I have my father-in-law for that. So when I was specing out my NAS build, I went a little heavier than most people would and crossed my fingers that the system could pull double duty down the road.&lt;/p&gt; &lt;p&gt;Honestly? I was prepared to be wrong. Worst case I'd have an overpowered NAS that never breaks a sweat. I could live with that.&lt;/p&gt; &lt;p&gt;But it actually worked. And way better than I expected.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Build&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://store.minisforum.com/products/minisforum-n5-pro"&gt;Minisforum N5 Pro&lt;/a&gt;&lt;/li&gt; &lt;li&gt;AMD Ryzen AI 9 HX PRO 370 (12c/24t, 16 RDNA 3.5 CUs)&lt;/li&gt; &lt;li&gt;96GB DDR5-5600 (2x 48GB SO-DIMMs)&lt;/li&gt; &lt;li&gt;5x 26TB Seagate Exos in RAIDZ2 (~70TB usable)&lt;/li&gt; &lt;li&gt;2x 1.92TB Samsung PM983 NVMe (ZFS metadata mirror)&lt;/li&gt; &lt;li&gt;TrueNAS SCALE&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Day to day it runs Jellyfin with VAAPI hardware transcoding, Sonarr, Radarr, Prowlarr, qBittorrent, FlareSolverr, Tailscale, and Dockge. It was already earning its keep before I ever touched LLM inference.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Experiment&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The model is Qwen3-Coder-Next, 80 billion parameters, Mixture of Experts architecture with 3B active per token. I'm running the Q4_K_M quantization through llama.cpp with the Vulkan backend. Here's how it actually went:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3 tok/s&lt;/strong&gt; - First successful run. Vanilla llama.cpp and Qwen3-Coder-Next Q8 quantization, CPU-only inference. Technically working. Almost physically painful to watch. But it proved the model could run.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5 tok/s&lt;/strong&gt; - Moved to Q4_K_M quantization and started tuning. Okay. Nearly double the speed and still slow as hell...but maybe usable for an overnight code review job. Started to think maybe this hardware just won't cut it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;10 tok/s&lt;/strong&gt; - Ran across a note in a subreddit that someone got Vulkan offloading and doing 11 tok/s on similar hardware but when I tried it...I couldn't load the full model into VRAM despite having plenty of RAM. Interesting. I tried partial offload, 30 out of 49 layers to the iGPU. It worked. Now it actually felt usable but it didn't make sense that I had all this RAM and it wouldn't load all of the expert layers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;15 tok/s&lt;/strong&gt; - Then the dumb breakthrough. I discovered that &lt;code&gt;--no-mmap&lt;/code&gt; was quietly destroying everything. On UMA architecture, where the CPU and GPU share the same physical RAM, that flag forces the model to be allocated twice into the same space. Once for the CPU, once for GPU-mapped memory, both pulling from the same DDR5 pool. I couldn't even load all 49 layers without OOM errors with that flag set. Dropped it. All 49 layers loaded cleanly. 46GB Vulkan buffer. No discrete GPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;18 tok/s&lt;/strong&gt; - Still I wanted more. I enabled flash attention. An extra 3 tok/s, cut KV cache memory in half, and significantly boosted the context window.&lt;/p&gt; &lt;p&gt;3 â†’ 5 â†’ 10 â†’ 15 â†’ 18. Each step was one discovery away from quitting. Glad I didn't.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results (Flash Attention Enabled)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Up to 18 tok/s text generation&lt;/li&gt; &lt;li&gt;53.8 tok/s prompt processing&lt;/li&gt; &lt;li&gt;50% less KV cache memory&lt;/li&gt; &lt;li&gt;Fully coherent output at any context length&lt;/li&gt; &lt;li&gt;All while Jellyfin was streaming to the living room for the kids&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Couldn't I just have bought a box purpose built for this? Yep. For reference, a Mac Mini M4 Pro with 64GB runs $2,299 and gets roughly 20-25 tok/s on the same model. Apple's soldered LPDDR5x gives it a real bandwidth advantage. But then it wouldn't run my media stack, store 70TB of data in RAIDZ2. I'm not trying to dunk on the Mac at all. Just saying I didn't have to buy one AND a NAS.&lt;/p&gt; &lt;p&gt;Which was the whole point.&lt;/p&gt; &lt;p&gt;No exotic kernel flags. No custom drivers. No ritual sacrifices. Vulkan just works on RDNA 3.5 under TrueNAS.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Still On the Table&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've barely scratched the surface on optimization, which is either exciting or dangerous depending on your relationship with optimizing. Speculative decoding could 2-3x effective speed. EXPO memory profiles might not even be enabled, meaning I could be leaving free bandwidth sitting at JEDEC defaults. Thread tuning, KV cache quantization, newer Vulkan backends with RDNA 3.5 optimizations landing regularly, UMA buffer experimentation, different quant formats.&lt;/p&gt; &lt;p&gt;On top of all that, the model wasn't even designed to run on standard transformer attention. It was built for DeltaNet, a linear attention mechanism that scales way better at long context. There's an active PR implementing it and we've been helping test and debug it. The fused kernel already hits 16 tok/s on a single CPU thread with perfect output, but there's a threading bug that breaks it at multiple cores. When that gets fixed and it can use all 12 cores plus Vulkan offloading, the headroom is significant. Especially for longer conversations where standard attention starts to choke.&lt;/p&gt; &lt;p&gt;18 tok/s is where I am but I'm hopeful it's not where this tops out.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Takeaway&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm not saying everyone should overbuild their NAS for an LLM machine or that this was even a good idea. But if you're like me, enjoy tinkering and learning, and are already shopping for a NAS and you're curious about local LLMs, it might be worth considering specing a little higher if you can afford it and giving yourself the option. I didn't know if this would work when I bought the hardware, a lot of people said it wasn't worth the effort. I just didn't want to buy two systems if I didn't have to.&lt;/p&gt; &lt;p&gt;Turns out I didn't have to. If you enjoyed the journey with me, leave a comment. If you think I'm an idiot, leave a comment. If you've already figured out what I'm doing wrong to get more tokens, definitely leave a comment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BetaOp9"&gt; /u/BetaOp9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T02:52:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1yixu</id>
    <title>My dumb little poor person cluster</title>
    <updated>2026-02-11T14:16:14+00:00</updated>
    <author>
      <name>/u/braydon125</name>
      <uri>https://old.reddit.com/user/braydon125</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1yixu/my_dumb_little_poor_person_cluster/"&gt; &lt;img alt="My dumb little poor person cluster" src="https://external-preview.redd.it/NWVhMHBseXhpdmlnMRJoz5GZPp4-AiH5TTKcOLtdgsvUlCaDDrlIjaUl2bR8.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fb2870083871ea9c15ad3586e230743a2d4ae9a" title="My dumb little poor person cluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;connecting two 64gb agx orin dev kits, and one 3090 node (ryzen9 5900/128gb ram) for a larger resource pool! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/braydon125"&gt; /u/braydon125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eo2ct3yxivig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1yixu/my_dumb_little_poor_person_cluster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1yixu/my_dumb_little_poor_person_cluster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T14:16:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1smw0</id>
    <title>Step-3.5-Flash AIME 2026 Results</title>
    <updated>2026-02-11T09:14:14+00:00</updated>
    <author>
      <name>/u/Abject-Ranger4363</name>
      <uri>https://old.reddit.com/user/Abject-Ranger4363</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1smw0/step35flash_aime_2026_results/"&gt; &lt;img alt="Step-3.5-Flash AIME 2026 Results" src="https://preview.redd.it/rmyb80pq0uig1.png?width=140&amp;amp;height=80&amp;amp;auto=webp&amp;amp;s=253b7e7d59d984a403a04ff45087b6ccdc79de68" title="Step-3.5-Flash AIME 2026 Results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/rmyb80pq0uig1.png?width=2594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2740fd8bb22cb112379e2d248a14b11661cdaf5e"&gt;https://preview.redd.it/rmyb80pq0uig1.png?width=2594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2740fd8bb22cb112379e2d248a14b11661cdaf5e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Best open model on MathArena for AIME 2026 I.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fd627h831uig1.png?width=2612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=878a922dd6f0101ca489502ffb939abe76b8f5e5"&gt;https://preview.redd.it/fd627h831uig1.png?width=2612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=878a922dd6f0101ca489502ffb939abe76b8f5e5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://matharena.ai/?view=problem&amp;amp;comp=aime--aime_2026"&gt;https://matharena.ai/?view=problem&amp;amp;comp=aime--aime_2026&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also the best Overall model:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fd627h831uig1.png?width=2612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=878a922dd6f0101ca489502ffb939abe76b8f5e5"&gt;https://preview.redd.it/fd627h831uig1.png?width=2612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=878a922dd6f0101ca489502ffb939abe76b8f5e5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Abject-Ranger4363"&gt; /u/Abject-Ranger4363 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1smw0/step35flash_aime_2026_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1smw0/step35flash_aime_2026_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1smw0/step35flash_aime_2026_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T09:14:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1vegx</id>
    <title>MiniMax M2.5 is currently undergoing internal testing and is available to a small number of users</title>
    <updated>2026-02-11T11:55:31+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vegx/minimax_m25_is_currently_undergoing_internal/"&gt; &lt;img alt="MiniMax M2.5 is currently undergoing internal testing and is available to a small number of users" src="https://preview.redd.it/rzn30tyytuig1.png?width=140&amp;amp;height=42&amp;amp;auto=webp&amp;amp;s=d76aec4ee8daf1af60654c398ab3a75babe3bad9" title="MiniMax M2.5 is currently undergoing internal testing and is available to a small number of users" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/rudrank/status/2021534943932031226?s=20"&gt;https://x.com/rudrank/status/2021534943932031226?s=20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rzn30tyytuig1.png?width=626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=361c1704ab37823746ab84fe45b4dcd3d378685a"&gt;https://preview.redd.it/rzn30tyytuig1.png?width=626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=361c1704ab37823746ab84fe45b4dcd3d378685a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1vqjp3n1uuig1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c9967df4c6af84af29af6ae5272b243a6ad1693"&gt;https://preview.redd.it/1vqjp3n1uuig1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c9967df4c6af84af29af6ae5272b243a6ad1693&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vegx/minimax_m25_is_currently_undergoing_internal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vegx/minimax_m25_is_currently_undergoing_internal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vegx/minimax_m25_is_currently_undergoing_internal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T11:55:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1tfbm</id>
    <title>DeepSeek just updated to a 1M context window!</title>
    <updated>2026-02-11T10:03:06+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tfbm/deepseek_just_updated_to_a_1m_context_window/"&gt; &lt;img alt="DeepSeek just updated to a 1M context window!" src="https://preview.redd.it/9z2ggdgy9uig1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=2ab7565635c402cdabef3c7d20eae901df30fa52" title="DeepSeek just updated to a 1M context window!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The DeepSeek app was just updated with 1M context, and the knowledge cutoff date is now May 2025. It's unclear for now if this is a new model. Also, there hasn't been any movement on their Hugging Face page yet.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9z2ggdgy9uig1.png?width=1179&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3f48da856b53751f2db2b17ac5f49baaf9add55"&gt;https://preview.redd.it/9z2ggdgy9uig1.png?width=1179&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3f48da856b53751f2db2b17ac5f49baaf9add55&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tfbm/deepseek_just_updated_to_a_1m_context_window/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tfbm/deepseek_just_updated_to_a_1m_context_window/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tfbm/deepseek_just_updated_to_a_1m_context_window/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T10:03:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2a3yq</id>
    <title>GLM5.0 says drive the car to the car wash</title>
    <updated>2026-02-11T21:24:24+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2a3yq/glm50_says_drive_the_car_to_the_car_wash/"&gt; &lt;img alt="GLM5.0 says drive the car to the car wash" src="https://preview.redd.it/49o3zdjhnxig1.png?width=140&amp;amp;height=120&amp;amp;auto=webp&amp;amp;s=acfc313237e4fc30b1bce725ef229dff535bac2d" title="GLM5.0 says drive the car to the car wash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/49o3zdjhnxig1.png?width=839&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04b1bb16a14a5dfacb9b30df64991d2f85b4cb85"&gt;https://preview.redd.it/49o3zdjhnxig1.png?width=839&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04b1bb16a14a5dfacb9b30df64991d2f85b4cb85&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2a3yq/glm50_says_drive_the_car_to_the_car_wash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2a3yq/glm50_says_drive_the_car_to_the_car_wash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2a3yq/glm50_says_drive_the_car_to_the_car_wash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T21:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r21tzb</id>
    <title>Community Evals on Hugging Face</title>
    <updated>2026-02-11T16:23:19+00:00</updated>
    <author>
      <name>/u/HauntingMoment</name>
      <uri>https://old.reddit.com/user/HauntingMoment</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r21tzb/community_evals_on_hugging_face/"&gt; &lt;img alt="Community Evals on Hugging Face" src="https://preview.redd.it/iijfx1dk5wig1.png?width=140&amp;amp;height=102&amp;amp;auto=webp&amp;amp;s=52c016113a72f21ef056a0e339801a2fbe7bfa48" title="Community Evals on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey! I'm Nathan (SaylorTwift) from huggingface we have a big update from the hf hub that actually fixes one of the most annoying things about model evaluation.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/iijfx1dk5wig1.png?width=1049&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a544cd848e26b2ff06d926dae85d711495f3bb6"&gt;Humanity's Last exam dataset on Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;community evals are now live on huggingface! it's a decentralized, transparent way for the community to report and share model evaluations.&lt;/p&gt; &lt;p&gt;why ?&lt;/p&gt; &lt;p&gt;everyoneâ€™s stats are scattered across papers, model cards, platforms and sometimes contradict each other. thereâ€™s no unified single source of truth. community evals aim to fix that by making eval reporting open and reproducible.&lt;/p&gt; &lt;p&gt;what's changed ?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;benchmarks host leaderboards right in the dataset repo (e.g. mmlu-pro, gpqa, hle)&lt;/li&gt; &lt;li&gt;models store their own results in .eval_results/*.yaml and they show up on model cards and feed into the dataset leaderboards.&lt;/li&gt; &lt;li&gt;anyone can submit eval results via a pr without needing the model author to merge. those show up as community results.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the key idea is that scores arenâ€™t hidden in black-box leaderboards anymore. everyone can see who ran what, how, and when, and build tools, dashboards, comparisons on top of that!&lt;/p&gt; &lt;p&gt;If you want to &lt;a href="https://huggingface.co/blog/community-evals"&gt;read more&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HauntingMoment"&gt; /u/HauntingMoment &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r21tzb/community_evals_on_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r21tzb/community_evals_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r21tzb/community_evals_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T16:23:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1oan9</id>
    <title>EpsteinFiles-RAG: Building a RAG Pipeline on 2M+ Pages</title>
    <updated>2026-02-11T05:02:36+00:00</updated>
    <author>
      <name>/u/Cod3Conjurer</name>
      <uri>https://old.reddit.com/user/Cod3Conjurer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love playing around with RAG and AI, optimizing every layer to squeeze out better performance. Last night I thought: why not tackle something massive?&lt;/p&gt; &lt;p&gt;Took the Epstein Files dataset from Hugging Face (teyler/epstein-files-20k) â€“ 2 million+ pages of trending news and documents. The cleaning, chunking, and optimization challenges are exactly what excites me.&lt;/p&gt; &lt;p&gt;What I built:&lt;/p&gt; &lt;p&gt;- Full RAG pipeline with optimized data processing&lt;/p&gt; &lt;p&gt;- Processed 2M+ pages (cleaning, chunking, vectorization)&lt;/p&gt; &lt;p&gt;- Semantic search &amp;amp; Q&amp;amp;A over massive dataset&lt;/p&gt; &lt;p&gt;- Constantly tweaking for better retrieval &amp;amp; performance&lt;/p&gt; &lt;p&gt;- Python, MIT Licensed, open source&lt;/p&gt; &lt;p&gt;Why I built this:&lt;/p&gt; &lt;p&gt;Itâ€™s trending, real-world data at scale, the perfect playground.&lt;/p&gt; &lt;p&gt;When you operate at scale, every optimization matters. This project lets me experiment with RAG architectures, data pipelines, and AI performance tuning on real-world workloads.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/AnkitNayak-eth/EpsteinFiles-RAG"&gt;https://github.com/AnkitNayak-eth/EpsteinFiles-RAG&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Open to ideas, optimizations, and technical discussions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cod3Conjurer"&gt; /u/Cod3Conjurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oan9/epsteinfilesrag_building_a_rag_pipeline_on_2m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oan9/epsteinfilesrag_building_a_rag_pipeline_on_2m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oan9/epsteinfilesrag_building_a_rag_pipeline_on_2m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T05:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1r3nk</id>
    <title>Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts</title>
    <updated>2026-02-11T07:38:59+00:00</updated>
    <author>
      <name>/u/Tiny_Minimum_4384</name>
      <uri>https://old.reddit.com/user/Tiny_Minimum_4384</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1r3nk/nanbeige413b_a_small_general_model_that_reasons/"&gt; &lt;img alt="Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts" src="https://preview.redd.it/82hjsn98ktig1.png?width=140&amp;amp;height=95&amp;amp;auto=webp&amp;amp;s=f06a27761905099dec3c58ed9398dbb2a40f6816" title="Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone ðŸ‘‹&lt;/p&gt; &lt;p&gt;Weâ€™re excited to share Nanbeige4.1-3B, the latest iteration of our open-source 3B model from Nanbeige LLM Lab. Our goal with this release is to explore whether a small general model can simultaneously achieve strong reasoning, robust preference alignment, and agentic behavior.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/82hjsn98ktig1.png?width=4920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14ab960015daf8b38ae74fe9d4332208011f4f05"&gt;https://preview.redd.it/82hjsn98ktig1.png?width=4920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14ab960015daf8b38ae74fe9d4332208011f4f05&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strong Reasoning Capability&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Solves complex problems through sustained and coherent reasoning within a single forward pass. It achieves strong results on challenging tasks such as &lt;strong&gt;LiveCodeBench-Pro&lt;/strong&gt;, &lt;strong&gt;IMO-Answer-Bench&lt;/strong&gt;, and &lt;strong&gt;AIME 2026 I&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Robust Preference Alignment&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Besides solving hard problems, it also demonstrates strong alignment with human preferences. Nanbeige4.1-3B achieves &lt;strong&gt;73.2 on Arena-Hard-v2&lt;/strong&gt; and &lt;strong&gt;52.21 on Multi-Challenge&lt;/strong&gt;, demonstrating superior performance compared to larger models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic and Deep-Search Capability in a 3B Model&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Beyond chat tasks such as alignment, coding, and mathematical reasoning, Nanbeige4.1-3B also demonstrates solid native agent capabilities. It natively supports deep-search and achieves strong performance on tasks such as &lt;strong&gt;xBench-DeepSearch&lt;/strong&gt; and &lt;strong&gt;GAIA&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-Context and Sustained Reasoning&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Nanbeige4.1-3B supports context lengths of up to 256k tokens, enabling deep-search with hundreds of tool calls, as well as 100k+ token single-pass reasoning for complex problems&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ðŸ¤— Model Weight: &lt;a href="https://huggingface.co/Nanbeige/Nanbeige4.1-3B"&gt;https://huggingface.co/Nanbeige/Nanbeige4.1-3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ðŸ“„ Technical Report: Coming Soon&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tiny_Minimum_4384"&gt; /u/Tiny_Minimum_4384 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1r3nk/nanbeige413b_a_small_general_model_that_reasons/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1r3nk/nanbeige413b_a_small_general_model_that_reasons/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1r3nk/nanbeige413b_a_small_general_model_that_reasons/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T07:38:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1r21ojm</id>
    <title>We've built memory into 4 different agent systems. Here's what actually works and what's a waste of time.</title>
    <updated>2026-02-11T16:17:39+00:00</updated>
    <author>
      <name>/u/arapkuliev</name>
      <uri>https://old.reddit.com/user/arapkuliev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After building memory layers for multiple agent setups, here's the shit nobody tells you in the tutorials. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's a waste of time:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;- &lt;strong&gt;&amp;quot;Just use a vector store&amp;quot;&lt;/strong&gt; -- Congrats, you built keyword search with extra steps and worse debugging. Embeddings are great for fuzzy matching, terrible for precise retrieval. Your agent will confidently pull up something &lt;em&gt;semantically similar&lt;/em&gt; instead of the &lt;em&gt;actual thing it needs&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Dumping full conversation logs as memory&lt;/strong&gt; -- Your agent doesn't need to remember that the user said &amp;quot;thanks&amp;quot; 47 times. Unfiltered logs are noise with a few signal fragments buried in them. And you're burning tokens retrieving garbage.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;One retrieval strategy&lt;/strong&gt; -- If you're only doing semantic search, you're missing exact matches. If you're only doing keyword search, you're missing relationships. Pick one and you'll spend months wondering why retrieval &amp;quot;feels off.&amp;quot; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What actually works:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Entity resolution pipelines.&lt;/strong&gt; Actively identify and link entities across conversations. &amp;quot;The Postgres migration,&amp;quot; &amp;quot;that DB move we discussed,&amp;quot; and &amp;quot;the thing Jake proposed last Tuesday&amp;quot; are the same thing. If your memory doesn't know that, it's broken.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Temporal tagging.&lt;/strong&gt; When was this learned? Is it still valid? A decision from 3 months ago might be reversed. If your memory treats everything as equally fresh, your agent will confidently act on outdated context. Timestamps aren't metadata. They're core to whether a memory is useful.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Explicit priority systems.&lt;/strong&gt; Not everything is worth remembering. Let users or systems mark what matters and what should decay. Without this you end up with a memory that &amp;quot;remembers&amp;quot; everything equally, which means it effectively remembers nothing.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Contradiction detection.&lt;/strong&gt; Your system will inevitably store conflicting information. &amp;quot;We're using Redis for caching&amp;quot; and &amp;quot;We moved off Redis last sprint.&amp;quot; If you silently store both, your agent flips a coin on which one it retrieves. Flag conflicts. Surface them. Let a human resolve it.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Multi-strategy retrieval.&lt;/strong&gt; Run keyword, semantic, and graph traversal in parallel. Merge results. The answer to &amp;quot;why did we pick this architecture?&amp;quot; might be spread across a design doc, a Slack thread, and a PR description. No single strategy finds all three. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The uncomfortable truth:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;None of this &amp;quot;solves&amp;quot; memory. These are tactical patches for specific retrieval problems. But implemented carefully, they make systems that &lt;em&gt;feel&lt;/em&gt; like memory instead of feeling like a database you have to babysit. &lt;/p&gt; &lt;p&gt;The bar isn't &amp;quot;perfect recall.&amp;quot; The bar is &amp;quot;better than asking the same question twice.&amp;quot; &lt;/p&gt; &lt;p&gt;What's actually working in your setups?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arapkuliev"&gt; /u/arapkuliev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r21ojm/weve_built_memory_into_4_different_agent_systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r21ojm/weve_built_memory_into_4_different_agent_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r21ojm/weve_built_memory_into_4_different_agent_systems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T16:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1u2ne</id>
    <title>Grok-3 joins upcoming models list</title>
    <updated>2026-02-11T10:41:33+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1u2ne/grok3_joins_upcoming_models_list/"&gt; &lt;img alt="Grok-3 joins upcoming models list" src="https://preview.redd.it/ueoiz6yrfuig1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc9154b12fb8ee19cbdde7d47a510f5ad934b95f" title="Grok-3 joins upcoming models list" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/elonmusk/status/2020878250516341110"&gt;Tweet link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;First question is when?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ueoiz6yrfuig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1u2ne/grok3_joins_upcoming_models_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1u2ne/grok3_joins_upcoming_models_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T10:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1snhv</id>
    <title>DeepSeek has launched grayscale testing for its new model on both its official website and app. 1M content length!</title>
    <updated>2026-02-11T09:15:17+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1snhv/deepseek_has_launched_grayscale_testing_for_its/"&gt; &lt;img alt="DeepSeek has launched grayscale testing for its new model on both its official website and app. 1M content length!" src="https://preview.redd.it/vahfibvk4uig1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=ffb9da4726561f044fd768dc2f75838e643edf5f" title="DeepSeek has launched grayscale testing for its new model on both its official website and app. 1M content length!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vahfibvk4uig1.png?width=828&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15d8b657dd69d496af701aeb4c20ed62b4bbce98"&gt;This model know Gemini 2.5 Pro on not web search &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ontumt5s3uig1.jpg?width=657&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=efff85457597b8fd9dbcbcf3d1d99d62a0678ea2"&gt;https://preview.redd.it/ontumt5s3uig1.jpg?width=657&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=efff85457597b8fd9dbcbcf3d1d99d62a0678ea2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek has launched grayscale testing for its new model on both its official website and app. The new model features a 1M context window and an updated knowledge base. Currently, access is limited to a select group of accounts.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j1qiarng1uig1.png?width=1163&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a99f1652ea755a7aeaa600250ff4856133fbfca"&gt;https://preview.redd.it/j1qiarng1uig1.png?width=1163&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a99f1652ea755a7aeaa600250ff4856133fbfca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It look Like V4 Lite not actually V4&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1snhv/deepseek_has_launched_grayscale_testing_for_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1snhv/deepseek_has_launched_grayscale_testing_for_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1snhv/deepseek_has_launched_grayscale_testing_for_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T09:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2005l</id>
    <title>Mini AI Machine</title>
    <updated>2026-02-11T15:14:31+00:00</updated>
    <author>
      <name>/u/KnownAd4832</name>
      <uri>https://old.reddit.com/user/KnownAd4832</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2005l/mini_ai_machine/"&gt; &lt;img alt="Mini AI Machine" src="https://preview.redd.it/4vmjqryjtvig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c30c9580fd072df0983a74667d5a2fb1848c656e" title="Mini AI Machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I do a lot of text processing &amp;amp; generation on small model. RTX 4000 Blackwell SFF (75W max) + 32GB DDR5 + DeskMeet 8L PC running PopOS and vLLM ðŸŽ‰&lt;/p&gt; &lt;p&gt;Anyone else has mini AI rig?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KnownAd4832"&gt; /u/KnownAd4832 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4vmjqryjtvig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2005l/mini_ai_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2005l/mini_ai_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T15:14:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r20uwk</id>
    <title>Releasing MioTTS: A family of lightweight, fast LLM-based TTS models (0.1B - 2.6B) with Zero-shot Voice Cloning</title>
    <updated>2026-02-11T15:46:56+00:00</updated>
    <author>
      <name>/u/Askxc</name>
      <uri>https://old.reddit.com/user/Askxc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Iâ€™ve been developing a personal project to create a lightweight and fast TTS model. Today Iâ€™m releasing &lt;strong&gt;MioTTS&lt;/strong&gt;, a family of LLM-based models ranging from &lt;strong&gt;0.1B to 2.6B&lt;/strong&gt; parameters.&lt;/p&gt; &lt;p&gt;The main focus was to achieve high-fidelity audio at the 0.1B parameter scale. I wanted to see how efficient it could be while maintaining quality, so I also developed a custom neural audio codec (&lt;strong&gt;MioCodec&lt;/strong&gt;) to minimize latency.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Zero-shot Voice Cloning:&lt;/strong&gt; Supports high-fidelity cloning from short reference audio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bilingual:&lt;/strong&gt; Trained on ~100k hours of English and Japanese speech data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Codec:&lt;/strong&gt; Built on top of &lt;strong&gt;MioCodec&lt;/strong&gt;, a custom neural audio codec I developed to allow for faster generation (low token rate) while maintaining audio fidelity. The codec is also released under MIT license.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Model Family:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Iâ€™ve released multiple sizes to balance quality and resource usage. Licenses depend on the base model used.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Base Model&lt;/th&gt; &lt;th align="left"&gt;License&lt;/th&gt; &lt;th align="left"&gt;RTF (approx.)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;0.1B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Falcon-H1-Tiny&lt;/td&gt; &lt;td align="left"&gt;Falcon-LLM&lt;/td&gt; &lt;td align="left"&gt;0.04 - 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;0.4B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;LFM2-350M&lt;/td&gt; &lt;td align="left"&gt;LFM Open v1.0&lt;/td&gt; &lt;td align="left"&gt;0.035 - 0.045&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;0.6B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen3-0.6B&lt;/td&gt; &lt;td align="left"&gt;Apache 2.0&lt;/td&gt; &lt;td align="left"&gt;0.055 - 0.065&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;1.2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;LFM2.5-1.2B&lt;/td&gt; &lt;td align="left"&gt;LFM Open v1.0&lt;/td&gt; &lt;td align="left"&gt;0.065 - 0.075&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;1.7B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen3-1.7B&lt;/td&gt; &lt;td align="left"&gt;Apache 2.0&lt;/td&gt; &lt;td align="left"&gt;0.10 - 0.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2.6B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;LFM2-2.6B&lt;/td&gt; &lt;td align="left"&gt;LFM Open v1.0&lt;/td&gt; &lt;td align="left"&gt;0.135 - 0.145&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I'd love to hear your feedback, especially on the English prosody (since I primarily develop in Japanese).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Collection:&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/Aratako/miotts"&gt;https://huggingface.co/collections/Aratako/miotts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference Code:&lt;/strong&gt; &lt;a href="https://github.com/Aratako/MioTTS-Inference"&gt;https://github.com/Aratako/MioTTS-Inference&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Demo (0.1B):&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/Aratako/MioTTS-0.1B-Demo"&gt;https://huggingface.co/spaces/Aratako/MioTTS-0.1B-Demo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Askxc"&gt; /u/Askxc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r20uwk/releasing_miotts_a_family_of_lightweight_fast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r20uwk/releasing_miotts_a_family_of_lightweight_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r20uwk/releasing_miotts_a_family_of_lightweight_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T15:46:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1wvos</id>
    <title>MOSS-TTS has been released</title>
    <updated>2026-02-11T13:06:41+00:00</updated>
    <author>
      <name>/u/Xiami2019</name>
      <uri>https://old.reddit.com/user/Xiami2019</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wvos/mosstts_has_been_released/"&gt; &lt;img alt="MOSS-TTS has been released" src="https://preview.redd.it/u56s8amp6vig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd362ae4aaee8f23d85c9c94bcdc2e0f1a676bf2" title="MOSS-TTS has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seed TTS Eval&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xiami2019"&gt; /u/Xiami2019 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u56s8amp6vig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wvos/mosstts_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wvos/mosstts_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T13:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1tuh1</id>
    <title>Just finished building this bad boy</title>
    <updated>2026-02-11T10:28:00+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tuh1/just_finished_building_this_bad_boy/"&gt; &lt;img alt="Just finished building this bad boy" src="https://preview.redd.it/ju0ed5uceuig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04eab77fdf6e1df2e0b04b0581b6a1d713e805b5" title="Just finished building this bad boy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;6x Gigabyte 3090 Gaming OC all running at PCIe 4.0 16x speed&lt;/p&gt; &lt;p&gt;Asrock Romed-2T motherboard with Epyc 7502 CPU&lt;/p&gt; &lt;p&gt;8 sticks of DDR4 8GB 2400Mhz running in octochannel mode&lt;/p&gt; &lt;p&gt;Modified Tinygrad Nvidia drivers with P2P enabled, intra GPU bandwidth tested at 24.5 GB/s&lt;/p&gt; &lt;p&gt;Total 144GB VRam, will be used to experiment with training diffusion models up to 10B parameters from scratch&lt;/p&gt; &lt;p&gt;All GPUs set to 270W power limit&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ju0ed5uceuig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tuh1/just_finished_building_this_bad_boy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tuh1/just_finished_building_this_bad_boy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T10:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r20wki</id>
    <title>Add Kimi-K2.5 support</title>
    <updated>2026-02-11T15:48:39+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r20wki/add_kimik25_support/"&gt; &lt;img alt="Add Kimi-K2.5 support" src="https://external-preview.redd.it/5gup_oD4lytsLI1wID-Zo3RkPiRxiRbU2Hm7r-fkB2I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58c51ab74c9a734ec60838d3f67b78c6df26076b" title="Add Kimi-K2.5 support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19170"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r20wki/add_kimik25_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r20wki/add_kimik25_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T15:48:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r25lvf</id>
    <title>The hunt continues ...</title>
    <updated>2026-02-11T18:38:03+00:00</updated>
    <author>
      <name>/u/Lzlxlclvlblnlmao</name>
      <uri>https://old.reddit.com/user/Lzlxlclvlblnlmao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r25lvf/the_hunt_continues/"&gt; &lt;img alt="The hunt continues ..." src="https://preview.redd.it/f8zp85xrtwig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e054a1ad8e41fd415e85050a1e4cfd4a37da628a" title="The hunt continues ..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tbf it did work with Deep Thinking enabled &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lzlxlclvlblnlmao"&gt; /u/Lzlxlclvlblnlmao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f8zp85xrtwig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r25lvf/the_hunt_continues/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r25lvf/the_hunt_continues/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T18:38:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1x0qi</id>
    <title>GLM 5.0 &amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?</title>
    <updated>2026-02-11T13:12:51+00:00</updated>
    <author>
      <name>/u/Appropriate-Lie-8812</name>
      <uri>https://old.reddit.com/user/Appropriate-Lie-8812</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"&gt; &lt;img alt="GLM 5.0 &amp;amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?" src="https://preview.redd.it/k4rtczs47vig1.png?width=140&amp;amp;height=56&amp;amp;auto=webp&amp;amp;s=46cd0e4543f951137b6e945d501812280005a7d3" title="GLM 5.0 &amp;amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 5.0 (&lt;a href="https://chat.z.ai/"&gt;https://chat.z.ai/&lt;/a&gt;) and MiniMax 2.5 (&lt;a href="https://agent.minimax.io"&gt;https://agent.minimax.io&lt;/a&gt;) just dropped, both clearly moving beyond simple chat into agent-style workflows.&lt;/p&gt; &lt;p&gt;GLM 5.0 seems focused on stronger reasoning and coding, while MiniMax 2.5 emphasizes task decomposition and longer-running execution.&lt;/p&gt; &lt;p&gt;Feels like the competition is shifting from &amp;quot;who writes better answers&amp;quot; to &amp;quot;who can actually finish the job.&amp;quot;&lt;/p&gt; &lt;p&gt;Planning to test both in a few setups , maybe straight API benchmarks, Cursor-style IDE workflows, and a multi-agent orchestration tool like Verdent, to see how they handle longer tasks and repo-level changes. Will report back if anything interesting breaks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Appropriate-Lie-8812"&gt; /u/Appropriate-Lie-8812 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r1x0qi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T13:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1wnj9</id>
    <title>MiniMax M2.5 Released</title>
    <updated>2026-02-11T12:56:37+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt; &lt;img alt="MiniMax M2.5 Released" src="https://preview.redd.it/uou9tmkx4vig1.png?width=140&amp;amp;height=80&amp;amp;auto=webp&amp;amp;s=3113e726ff999e0cdee3a5021d7abd5f90521d6e" title="MiniMax M2.5 Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uou9tmkx4vig1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01ab95d308d2f7ab77567a92ec882f3ac2d71755"&gt;https://preview.redd.it/uou9tmkx4vig1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01ab95d308d2f7ab77567a92ec882f3ac2d71755&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://agent.minimax.io/"&gt;https://agent.minimax.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T12:56:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r28xxz</id>
    <title>GLM-5 scores 50 on the Intelligence Index and is the new open weights leader!</title>
    <updated>2026-02-11T20:40:32+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r28xxz/glm5_scores_50_on_the_intelligence_index_and_is/"&gt; &lt;img alt="GLM-5 scores 50 on the Intelligence Index and is the new open weights leader!" src="https://preview.redd.it/gauvtw6qfxig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfd410a74fdc338ff7e16ec354e8d19a667622e8" title="GLM-5 scores 50 on the Intelligence Index and is the new open weights leader!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gauvtw6qfxig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r28xxz/glm5_scores_50_on_the_intelligence_index_and_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r28xxz/glm5_scores_50_on_the_intelligence_index_and_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T20:40:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1wl6x</id>
    <title>GLM 5 Released</title>
    <updated>2026-02-11T12:53:30+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt; &lt;img alt="GLM 5 Released" src="https://preview.redd.it/mvdnn18e4vig1.png?width=140&amp;amp;height=42&amp;amp;auto=webp&amp;amp;s=5b006a25f178b73764138eabdb11ae38eb368d7f" title="GLM 5 Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://chat.z.ai/"&gt;https://chat.z.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mvdnn18e4vig1.png?width=799&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6324969f9d24fa0aeefbd5e8da2de3da0f5f948e"&gt;https://preview.redd.it/mvdnn18e4vig1.png?width=799&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6324969f9d24fa0aeefbd5e8da2de3da0f5f948e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T12:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r22hlq</id>
    <title>GLM-5 Officially Released</title>
    <updated>2026-02-11T16:47:29+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/"&gt; &lt;img alt="GLM-5 Officially Released" src="https://preview.redd.it/h2bmmfa5awig1.jpg?width=140&amp;amp;height=95&amp;amp;auto=webp&amp;amp;s=48723badc371c5206ca5e6292829eb25b9ec00d5" title="GLM-5 Officially Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are launching GLM-5, targeting complex systems engineering and long-horizon agentic tasks. Scaling is still one of the most important ways to improve the intelligence efficiency of Artificial General Intelligence (AGI). Compared to GLM-4.5, GLM-5 scales from 355B parameters (32B active) to 744B parameters (40B active), and increases pre-training data from 23T to 28.5T tokens. GLM-5 also integrates DeepSeek Sparse Attention (DSA), significantly reducing deployment cost while preserving long-context capacity.&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://z.ai/blog/glm-5"&gt;https://z.ai/blog/glm-5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/zai-org/GLM-5"&gt;https://huggingface.co/zai-org/GLM-5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/zai-org/GLM-5"&gt;https://github.com/zai-org/GLM-5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r22hlq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T16:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r26zsg</id>
    <title>Z.ai said they are GPU starved, openly.</title>
    <updated>2026-02-11T19:28:16+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/"&gt; &lt;img alt="Z.ai said they are GPU starved, openly." src="https://preview.redd.it/kjy1wqzt2xig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e573128364215e6c6e3a97ac576d0f84213ac948" title="Z.ai said they are GPU starved, openly." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kjy1wqzt2xig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T19:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
