<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-27T15:18:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rg3gka</id>
    <title>LLM Terminology Explained Simply: Weights, Inference, Sequence, ESL, vLLM, Context Window, Distillation, Reasoning, Temperature, Batching and many many more</title>
    <updated>2026-02-27T10:47:23+00:00</updated>
    <author>
      <name>/u/Eye_Killere</name>
      <uri>https://old.reddit.com/user/Eye_Killere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg3gka/llm_terminology_explained_simply_weights/"&gt; &lt;img alt="LLM Terminology Explained Simply: Weights, Inference, Sequence, ESL, vLLM, Context Window, Distillation, Reasoning, Temperature, Batching and many many more" src="https://external-preview.redd.it/X1prwAFTyqUvtyi4Jm8A9_ph_VvJrd44hAmLWVuEKvQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2d6b1bd49ad42310c9885af4e4640f9b9b14a29" title="LLM Terminology Explained Simply: Weights, Inference, Sequence, ESL, vLLM, Context Window, Distillation, Reasoning, Temperature, Batching and many many more" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eye_Killere"&gt; /u/Eye_Killere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://devforth.io/insights/llm-terminology-guide-weights-inference-effective-sequence-length-and-self-hosting-explained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg3gka/llm_terminology_explained_simply_weights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg3gka/llm_terminology_explained_simply_weights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T10:47:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg68e6</id>
    <title>Starting a PhD in ML - what is the best infra I can get to support my research?</title>
    <updated>2026-02-27T13:09:47+00:00</updated>
    <author>
      <name>/u/AdministrativeRub484</name>
      <uri>https://old.reddit.com/user/AdministrativeRub484</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My school doesn't have many resources. I would need to have at least 160 GB of VRAM to support my research statement/proposal. What would be the most cost effective way of doing so?&lt;/p&gt; &lt;p&gt;Paying for cloud services would not be it imo as I would almost be running experiments 24/7, and if I buy hardware I can always resell it later down the line.&lt;/p&gt; &lt;p&gt;Edit: I have around 2k USD to spend towards this. The most important thing for me is really vram and only then memory bandwith. I will be mainly trainning models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdministrativeRub484"&gt; /u/AdministrativeRub484 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg68e6/starting_a_phd_in_ml_what_is_the_best_infra_i_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg68e6/starting_a_phd_in_ml_what_is_the_best_infra_i_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg68e6/starting_a_phd_in_ml_what_is_the_best_infra_i_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T13:09:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1rftlmm</id>
    <title>Vellium v0.4 — alternative simplified UI, updated writing mode and multi-char improvements</title>
    <updated>2026-02-27T01:55:58+00:00</updated>
    <author>
      <name>/u/Possible_Statement84</name>
      <uri>https://old.reddit.com/user/Possible_Statement84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rftlmm/vellium_v04_alternative_simplified_ui_updated/"&gt; &lt;img alt="Vellium v0.4 — alternative simplified UI, updated writing mode and multi-char improvements" src="https://preview.redd.it/yxjnb1gk1ylg1.png?width=140&amp;amp;height=83&amp;amp;auto=webp&amp;amp;s=9bd97c81bb059daf1a2d60163200d1f36cdcbad4" title="Vellium v0.4 — alternative simplified UI, updated writing mode and multi-char improvements" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Vellium is an open-source desktop app for local LLMs built around creative writing and roleplay. The idea is visual control over your story — sliders for mood, pacing, intensity instead of manually editing system prompts. Works with Ollama, KoboldCpp, LM Studio, OpenAI, OpenRouter, or any compatible endpoint.&lt;/p&gt; &lt;p&gt;This update focuses on accessibility and the writing experience.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Simple Mode&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;New alternative UI that strips everything down to a clean chat interface. No sidebars, no inspector panel, no RP presets on screen. Model picker inline, quick action buttons (Write, Learn, Code, Life stuff). Enabled by default on the welcome screen for new users. All advanced features are one click away when you need them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Writing mode updates:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Generate Next Chapter: continue your story without crafting a prompt each time&lt;br /&gt; Consistency checker, Summarize Book, Expand, Rewrite tools in the toolbar&lt;br /&gt; Chapter dynamics with per-chapter tone/pacing controls&lt;br /&gt; Outline view for project structure&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi-character improvements&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Updated multi-char mode for smoother group conversations — better turn management and character switching.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Zen mode for distraction-free writing&lt;br /&gt; Motion animations on chat messages and sidebar transitions&lt;br /&gt; Reworked layouts across both chat and writing views&lt;/p&gt; &lt;p&gt;Electron + React + TypeScript, MIT license&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/tg-prplx/vellium"&gt;https://github.com/tg-prplx/vellium&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Possible_Statement84"&gt; /u/Possible_Statement84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rftlmm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rftlmm/vellium_v04_alternative_simplified_ui_updated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rftlmm/vellium_v04_alternative_simplified_ui_updated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T01:55:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfxtfz</id>
    <title>Eagerly waiting for Qwen 3.5 1.7B</title>
    <updated>2026-02-27T05:16:43+00:00</updated>
    <author>
      <name>/u/Hot_Inspection_9528</name>
      <uri>https://old.reddit.com/user/Hot_Inspection_9528</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen 3 1.7B with 0.1111 temperature is really good. I like it. &lt;/p&gt; &lt;p&gt;I am very much waiting for Qwen 3.5 1.7B model. &lt;/p&gt; &lt;p&gt;I am actually very excited. &lt;/p&gt; &lt;p&gt;Any ideas when it might release? &lt;/p&gt; &lt;p&gt;If you work with SLM like 1.7Bs, I think this will be Qween of local small language models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hot_Inspection_9528"&gt; /u/Hot_Inspection_9528 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfxtfz/eagerly_waiting_for_qwen_35_17b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfxtfz/eagerly_waiting_for_qwen_35_17b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfxtfz/eagerly_waiting_for_qwen_35_17b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T05:16:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfzfgf</id>
    <title>Minimax M2.5 GGUF perform poorly overall</title>
    <updated>2026-02-27T06:44:37+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;As posted by Benjamin Marie (not me) at&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://xcancel.com/bnjmn%5C_marie/status/2027043753484021810"&gt;https://xcancel.com/bnjmn\_marie/status/2027043753484021810&lt;/a&gt; :&lt;/p&gt; &lt;p&gt;Minimax M2.5 GGUFs (from Q4 down to Q1) perform poorly overall. None of them come close to the original model.&lt;/p&gt; &lt;p&gt;That’s very different from my Qwen3.5 GGUF evaluations, where even TQ1_0 held up well enough.&lt;/p&gt; &lt;p&gt;Lessons:&lt;/p&gt; &lt;p&gt;- Models aren’t equally robust, even under otherwise very good quantization algorithms.&lt;/p&gt; &lt;p&gt;-“Just take Q4, it’ll be fine” is a rule of thumb that doesn’t generalize.&lt;/p&gt; &lt;p&gt;(Here he posted a chart)&lt;/p&gt; &lt;p&gt;&lt;em&gt;And continues in another post:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Getting these results was painfully slow: between 10 and 20 hours for each model, using an H200. And since the models are not good, they tend to generate gibberish until reaching the maximum sequence length.&lt;/p&gt; &lt;p&gt;Took me over a week in total.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfzfgf/minimax_m25_gguf_perform_poorly_overall/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfzfgf/minimax_m25_gguf_perform_poorly_overall/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfzfgf/minimax_m25_gguf_perform_poorly_overall/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T06:44:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg8gkx</id>
    <title>Qwen3.5 35B a3b - 45 t/s 128K ctx on single 16GB 5060</title>
    <updated>2026-02-27T14:40:40+00:00</updated>
    <author>
      <name>/u/Gray_wolf_2904</name>
      <uri>https://old.reddit.com/user/Gray_wolf_2904</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Prefill speeds : 700+ tok/sec&lt;/p&gt; &lt;p&gt;Generation speed stays above 30 even as contact fills upto 120/128k. &lt;/p&gt; &lt;p&gt;Hardware setup: noting is overlocked. &lt;/p&gt; &lt;p&gt;I9-9900K, 64GB DDR4 RAM. &lt;/p&gt; &lt;p&gt;5060 ti 16GB &lt;/p&gt; &lt;p&gt;Ubuntu 24&lt;/p&gt; &lt;p&gt;The model is able to function as my primary programmer. Mind blowing performance when compared to many high end paid cloud models. &lt;/p&gt; &lt;p&gt;Amazingly, very few layers have to be on gpu to maintain 30+ tokens per second even at filled context. Have also seen consistent 45 t/s at smaller context sizes and 1000+ tokens per second in prompt processing (prefill). &lt;/p&gt; &lt;p&gt;My hardware is anything but modern or extraordinary. And this model has made it completely useable in production work environments. Bravo!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gray_wolf_2904"&gt; /u/Gray_wolf_2904 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8gkx/qwen35_35b_a3b_45_ts_128k_ctx_on_single_16gb_5060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8gkx/qwen35_35b_a3b_45_ts_128k_ctx_on_single_16gb_5060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8gkx/qwen35_35b_a3b_45_ts_128k_ctx_on_single_16gb_5060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T14:40:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg2yl7</id>
    <title>Qwen3.5 27B at Q3_K_M passes the "car wash test"</title>
    <updated>2026-02-27T10:17:46+00:00</updated>
    <author>
      <name>/u/ComplexType568</name>
      <uri>https://old.reddit.com/user/ComplexType568</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Either Qwen included this car wash test in the Qwen3.5 training set (a pretty recent question/benchmark test) last minute or this thing truly is a work of magic. Running on my setup its 4tk/s on LM Studio (pretty sure when they update their llama.cpp runtime it'll go faster)&lt;/p&gt; &lt;p&gt;I asked &amp;quot;I have 1 car, it is dirty and I want to bring it to the car wash (100 meters away) to wash it, should I walk or drive there?&amp;quot;&lt;/p&gt; &lt;p&gt;This is what it was thinking (with empty lines pruned) for a solid 10 minutes :')&lt;/p&gt; &lt;p&gt;(I also noticed they stopped adding &amp;lt;think&amp;gt; to their first token. interesting...) &lt;/p&gt; &lt;p&gt;Here's a thinking process that leads to the suggested advice:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; **Analyze the Request:**&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;* **Situation:** The user has one car.&lt;/p&gt; &lt;p&gt;* **Condition:** The car is dirty.&lt;/p&gt; &lt;p&gt;* **Goal:** Bring it to the car wash.&lt;/p&gt; &lt;p&gt;* **Distance:** 100 meters away.&lt;/p&gt; &lt;p&gt;* **Question:** Should I walk or drive there?&lt;/p&gt; &lt;ol&gt; &lt;li&gt; **Evaluate the Options:**&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;* **Option A: Drive the car.**&lt;/p&gt; &lt;p&gt;* *Pros:* Convenient, keeps you dry (if raining), faster than walking if distance was longer.&lt;/p&gt; &lt;p&gt;* *Cons:* The car is dirty. Driving it to the wash means driving a dirty car on public roads for 100 meters. It doesn't really matter much since it's already dirty, but it adds wear and tear (negligible).&lt;/p&gt; &lt;p&gt;* *Feasibility:* Yes, you can drive a dirty car.&lt;/p&gt; &lt;p&gt;* **Option B: Walk to the car wash.**&lt;/p&gt; &lt;p&gt;* *Pros:* Exercise, fresh air.&lt;/p&gt; &lt;p&gt;* *Cons:* You still need to get the car there eventually. If you walk *to* the car wash, how does the car get there? You can't push a car 100 meters easily (unless it's broken down, but the prompt implies it works). If you walk to the car wash and leave the car at home, you haven't achieved the goal of washing *that* car.&lt;/p&gt; &lt;p&gt;* *Interpretation:* The user likely means &amp;quot;Should I drive the dirty car to the wash, or should I walk to the wash (and then what?)&amp;quot; OR they are asking if it's worth driving a dirty car vs walking to get it there (which is impossible).&lt;/p&gt; &lt;p&gt;* *Correction:* Wait, if you walk to the car wash, the car stays at home. You can't wash a car that isn't there. So the only logical way to get the car washed is to move the car to the location of the car wash.&lt;/p&gt; &lt;p&gt;* *Re-evaluation:* The question asks &amp;quot;should I walk or drive *there*?&amp;quot; implying moving *to* the destination. But since the object being moved is the car, walking doesn't make sense for the car itself. Unless... does the user mean &amp;quot;Should I walk to the car wash (and leave the car) or drive the car there?&amp;quot;&lt;/p&gt; &lt;p&gt;* *Most logical interpretation:* The user wants to know if they should drive the dirty car to the car wash or walk themselves to the car wash (implying someone else drives it? No, only 1 car). Or perhaps they are asking about the mode of transport for *themselves* while the car is towed? No, that's overthinking.&lt;/p&gt; &lt;p&gt;* *Simplest interpretation:* The user wants to get the car washed. To do that, the car must be at the car wash. Therefore, the car must move 100 meters. Since it's a functional car (implied), driving is the only way to move it. Walking yourself there leaves the car behind.&lt;/p&gt; &lt;p&gt;* *Wait, another angle:* Maybe they mean &amp;quot;Should I walk my car?&amp;quot; (Pushing it). No, that's absurd for 100m unless broken.&lt;/p&gt; &lt;p&gt;* *Most likely intent:* They are asking if driving a dirty car is okay/necessary vs walking to the wash and... what?&lt;/p&gt; &lt;p&gt;* *Actually, looking at common sense riddles/logic puzzles:* This sounds like a trick question or a logic check. If you walk to the car wash, your car stays home. You can't wash it. So you *must* drive the car there.&lt;/p&gt; &lt;p&gt;* *Alternative interpretation:* Maybe they mean &amp;quot;Should I walk (to get the car washed) or drive (the car to get it washed)?&amp;quot; Since the car needs washing, the car must be at the location. Therefore, driving is the only option that gets the job done.&lt;/p&gt; &lt;p&gt;* **Refining the Logic:**&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Goal: Wash the car.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Constraint: Car wash is 100m away.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Action: Move car to car wash.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Method: Drive or Walk?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If you walk: You arrive at the car wash, but your car is still at home (dirty). Goal failed.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If you drive: You arrive at the car wash with your car. Goal achieved.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;* **Wait, is there a nuance?** Maybe they mean &amp;quot;Should I walk to the car wash and then call someone?&amp;quot; No info given.&lt;/p&gt; &lt;p&gt;* **Maybe it's about hygiene?** Driving a dirty car spreads dirt? 100 meters is negligible.&lt;/p&gt; &lt;p&gt;* **Maybe it's about fuel/efficiency?** 100m driving uses gas, walking doesn't. But you can't wash the car if you don't bring it.&lt;/p&gt; &lt;p&gt;* **Conclusion:** You must drive the car there. Walking yourself leaves the car behind. Unless... is this a riddle? &amp;quot;I have 1 car... should I walk or drive?&amp;quot; If I walk, I leave the car. If I drive, I take the car. To wash the car, I need to take it. So, Drive.&lt;/p&gt; &lt;p&gt;* **Wait, let's look at the phrasing again:** &amp;quot;should I walk or drive there?&amp;quot;&lt;/p&gt; &lt;p&gt;* &amp;quot;There&amp;quot; refers to the car wash.&lt;/p&gt; &lt;p&gt;* If I walk there: I am at the car wash. The car is not.&lt;/p&gt; &lt;p&gt;* If I drive there: I am at the car wash. The car is too.&lt;/p&gt; &lt;p&gt;* Therefore, driving is the only way to accomplish the goal of washing the car.&lt;/p&gt; &lt;p&gt;* **Is there a scenario where walking makes sense?** Only if you are going to walk back and get it later? No, that's inefficient. Or if the car is broken (not stated). Or if you want to exercise before washing? But then you still have to drive it eventually.&lt;/p&gt; &lt;p&gt;* **Wait, could it be a &amp;quot;towing&amp;quot; scenario?** If you walk there, how does the car get there? You can't tow it by walking unless you push it. Pushing a car 100m is possible but hard and unnecessary if the engine works.&lt;/p&gt; &lt;p&gt;* **Decision:** The answer should point out that to wash the car, the car needs to be at the location. Therefore, driving is necessary. Walking yourself there doesn't help wash the car unless you plan to come back for it (which defeats the purpose of going now).&lt;/p&gt; &lt;p&gt;* **Wait, one more possibility:** Is this a joke? &amp;quot;Should I walk or drive?&amp;quot; If you walk, you don't bring the car. So you can't wash it. The answer is Drive.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; **Drafting the Response:**&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;* Acknowledge the goal: Washing the car.&lt;/p&gt; &lt;p&gt;* Address the constraint: Car needs to be at the location.&lt;/p&gt; &lt;p&gt;* Compare options: Walking leaves the car behind; Driving brings the car.&lt;/p&gt; &lt;p&gt;* Conclusion: Drive it.&lt;/p&gt; &lt;p&gt;* Add a touch of humor or practical advice (100m is very short, driving is fine).&lt;/p&gt; &lt;ol&gt; &lt;li&gt; **Final Polish:** Keep it direct and helpful. Point out the logical necessity.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;* *Self-Correction during drafting:* Wait, if I walk to the car wash, can I call a tow truck? No, that's absurd for 100m. Can I push it? Yes, but why not drive?&lt;/p&gt; &lt;p&gt;* *Key realization:* The question is likely testing common sense logic regarding the object being washed. You cannot wash a car remotely.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; **Final Output Structure:**&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;* Direct Answer: Drive.&lt;/p&gt; &lt;p&gt;* Reasoning: The car needs to be at the location to get washed.&lt;/p&gt; &lt;p&gt;* Context: 100 meters is very short, so driving won't waste much fuel or cause issues.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; (Wait, let's consider if this is a trick question about &amp;quot;walking the dog&amp;quot; style logic). No, it seems straightforward. The user wants to wash the car. To do that, the car must be present.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;* Wait, could they mean &amp;quot;Should I walk my car?&amp;quot; (Pushing it). Some people push cars for fun? Unlikely.&lt;/p&gt; &lt;p&gt;* Okay, stick with the logic: Car needs to move -&amp;gt; Engine works -&amp;gt; Drive.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; **Final check:** Is there any reason *not* to drive a dirty car? No. It's already dirty. Driving it 100m won't make it significantly dirtier or damage anything.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;* So, Drive is the only logical choice to achieve the goal.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; **Wait, one nuance:** If you walk there, you can wait for someone else to bring it? No info on others. Assume solo.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;* Okay, answer is Drive.&lt;/p&gt; &lt;p&gt;**(Self-Correction/Refinement):** Actually, looking at similar riddles online... sometimes the joke is &amp;quot;You can't wash a car by walking.&amp;quot; But here the user asks how to get there. The most helpful answer explains that driving is required for the car to be washed.&lt;/p&gt; &lt;p&gt;Let's write it clearly.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;You need the car at the car wash.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Walking leaves the car behind.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Driving brings the car.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Therefore, drive.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Mention 100m is short so fuel cost is negligible.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Wait, is there a possibility they mean &amp;quot;Should I walk to the car wash (and leave the car) or drive the car to the car wash?&amp;quot; Yes, that's what I analyzed. The answer remains Drive.&lt;/p&gt; &lt;p&gt;Okay, ready to generate response.&lt;/p&gt; &lt;p&gt;**Wait, hold on.** Is it possible the user implies they want to *walk* the car? Like push it?&lt;/p&gt; &lt;p&gt;If you walk (pushing the car), you are walking and moving the car. But &amp;quot;drive&amp;quot; is easier.&lt;/p&gt; &lt;p&gt;The prompt asks &amp;quot;should I walk or drive there?&amp;quot; usually implying mode of transport for the person.&lt;/p&gt; &lt;p&gt;If I walk, I arrive without the car.&lt;/p&gt; &lt;p&gt;If I drive, I arrive with the car.&lt;/p&gt; &lt;p&gt;Since the goal is to wash *the car*, driving is mandatory.&lt;/p&gt; &lt;p&gt;Okay, final plan: State clearly that you must drive because the car needs to be there. Mention 100m is short so don't worry about fuel/dirt spreading.&lt;/p&gt; &lt;p&gt;One more thought: Is it possible they want to walk to save gas? 100 meters uses almost zero gas. So no need to worry.&lt;/p&gt; &lt;p&gt;Is it possible they want to avoid driving a dirty car on the road? It's already dirty, doesn't matter.&lt;/p&gt; &lt;p&gt;Okay, go with logic.&lt;/p&gt; &lt;p&gt;**Wait, is this a &amp;quot;lateral thinking&amp;quot; puzzle?**&lt;/p&gt; &lt;p&gt;&amp;quot;I have 1 car... should I walk or drive there?&amp;quot;&lt;/p&gt; &lt;p&gt;If you walk, you are at the car wash without the car. You can't wash it.&lt;/p&gt; &lt;p&gt;So you must drive.&lt;/p&gt; &lt;p&gt;It's not really a riddle, just a logic check.&lt;/p&gt; &lt;p&gt;Okay, writing response.&lt;/p&gt; &lt;p&gt;- Direct answer: Drive.&lt;/p&gt; &lt;p&gt;- Reason: Car needs to be present.&lt;/p&gt; &lt;p&gt;- Reassurance: 100m is short, driving dirty car is fine.&lt;/p&gt; &lt;p&gt;- Alternative: If you walk, you have to come back for the car anyway.&lt;/p&gt; &lt;p&gt;Wait, what if they mean &amp;quot;Should I walk (to get it washed) or drive (to get it washed)&amp;quot;?&lt;/p&gt; &lt;p&gt;If I walk, how does the car get there? It doesn't.&lt;/p&gt; &lt;p&gt;So driving is the only way.&lt;/p&gt; &lt;p&gt;Okay, confirmed.&lt;/p&gt; &lt;p&gt;**Wait, one edge case:** What if the car wash has a mobile service that comes to you? The prompt says &amp;quot;bring it to the car wash&amp;quot;. So no mobile service.&lt;/p&gt; &lt;p&gt;What if the car is broken? Prompt implies &amp;quot;I want to bring it&amp;quot;, implying ability to move it.&lt;/p&gt; &lt;p&gt;Okay, Drive.&lt;/p&gt; &lt;p&gt;Final check of tone: Helpful, logical, slightly humorous about the logic gap.&lt;/p&gt; &lt;p&gt;Wait, I should consider if there's a reason *not* to drive.&lt;/p&gt; &lt;p&gt;- Pollution? 100m is negligible.&lt;/p&gt; &lt;p&gt;- Dirt spreading? It's already dirty.&lt;/p&gt; &lt;p&gt;- Safety? 100m is safe.&lt;/p&gt; &lt;p&gt;So Drive is definitely better.&lt;/p&gt; &lt;p&gt;Okay, let's formulate the response.&lt;/p&gt; &lt;p&gt;&amp;quot;You should drive.&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;Reason: To wash the car, the car must be at the location.&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;Walking leaves the car behind.&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;100 meters is very short, so driving won't cost much or cause issues.&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;Driving a dirty car for 100m doesn't make it worse.&amp;quot;&lt;/p&gt; &lt;p&gt;Okay, good to go.&lt;/p&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;You should **drive** there.&lt;/p&gt; &lt;p&gt;Here is the logic:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;**The Goal:** You want to wash your car.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**The Requirement:** To get the car washed, the car itself needs to be at the car wash location.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**The Options:**&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;* **If you walk:** You will arrive at the car wash, but your car will still be at home (dirty). You cannot wash it remotely.&lt;/p&gt; &lt;p&gt;* **If you drive:** You bring the car with you to the car wash, allowing you to get it cleaned immediately.&lt;/p&gt; &lt;p&gt;Since 100 meters is a very short distance, driving won't use much fuel or cause any significant wear and tear on the vehicle. Driving a dirty car for that short distance doesn't make it significantly dirtier either.&lt;/p&gt; &lt;p&gt;**Verdict:** Drive the car there so you can actually wash it!&lt;/p&gt; &lt;p&gt;...&lt;/p&gt; &lt;p&gt;it's beautiful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexType568"&gt; /u/ComplexType568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg2yl7/qwen35_27b_at_q3_k_m_passes_the_car_wash_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg2yl7/qwen35_27b_at_q3_k_m_passes_the_car_wash_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg2yl7/qwen35_27b_at_q3_k_m_passes_the_car_wash_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T10:17:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg4fb7</id>
    <title>Should Qwen3.5-35B-A3B be this much slower than Qwen3-30B-A3B-2507?</title>
    <updated>2026-02-27T11:40:23+00:00</updated>
    <author>
      <name>/u/autoencoder</name>
      <uri>https://old.reddit.com/user/autoencoder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run models on my CPU. For Qwen3-30B-A3B-Instruct-2507-UD-Q4_K_XL I get 12-13 tokens/second output, while Qwen3.5-35B-A3B-UD-Q4_K_XL gives me something like 5.6 tokens/second output.&lt;/p&gt; &lt;p&gt;Qwen 3.5 is better, but the speed hit makes it not worth it for me. Why is it so much slower? The parameter count is very similar. Both these tests are with llama.cpp build 8149 on linux x64, with 9 threads. I have an Intel i9-10900, and 64 gigs of RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/autoencoder"&gt; /u/autoencoder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4fb7/should_qwen3535ba3b_be_this_much_slower_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4fb7/should_qwen3535ba3b_be_this_much_slower_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4fb7/should_qwen3535ba3b_be_this_much_slower_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T11:40:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg3wt1</id>
    <title>FINISHED MY FIRST WRITING MODEL!</title>
    <updated>2026-02-27T11:11:58+00:00</updated>
    <author>
      <name>/u/volious-ka</name>
      <uri>https://old.reddit.com/user/volious-ka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's trained on my previous model, with the intent being this: &lt;a href="https://huggingface.co/crownelius/The-Crow-9B"&gt;https://huggingface.co/crownelius/The-Crow-9B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Probably spent around $200 on dataset distillation from writing to high level reasoning.&lt;br /&gt; Please, if you're into writing give it a go! &lt;/p&gt; &lt;p&gt;This one is better at poetry and roleplay, very good for dnd. &lt;strong&gt;Here's a poem it wrote.&lt;/strong&gt; &lt;/p&gt; &lt;h1&gt;Moonlit Madness&lt;/h1&gt; &lt;p&gt;The moonlit sky above us gleamed,&lt;br /&gt; As we walked together through the dream.&lt;br /&gt; Two souls intertwined by fate's design,&lt;br /&gt; In each other's presence, finding grace.&lt;/p&gt; &lt;p&gt;She held my hand with a gentle touch,&lt;br /&gt; A love that only deep within can clutch.&lt;br /&gt; Yet I knew in her heart, another lay,&lt;br /&gt; Her husband who brought her endless days of joy and play.&lt;/p&gt; &lt;p&gt;I whispered softly to make sure she heard,&lt;br /&gt; &amp;quot;Though I love you dearly, know this must be pardoned.&amp;quot;&lt;br /&gt; For in the eyes she'd given me so bright,&lt;br /&gt; There lingered a bond that could never die.&lt;/p&gt; &lt;p&gt;She smiled with tears upon her cheek,&lt;br /&gt; Understood my words without need of speak.&lt;br /&gt; In each other's hearts we both found peace,&lt;br /&gt; And let our happiness remain unseen to all but us two.&lt;/p&gt; &lt;p&gt;Though I loved her deeply, even then,&lt;br /&gt; I held back the truest emotions from within.&lt;br /&gt; For in loving her, my heart was torn apart,&lt;br /&gt; Between myself and what she had already shared at first.&lt;/p&gt; &lt;p&gt;So here's to the love that we share so true,&lt;br /&gt; In each other's happiness, it shines anew.&lt;br /&gt; May time grant us both eternal peace,&lt;br /&gt; As separate souls living life's endless race.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/volious-ka"&gt; /u/volious-ka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg3wt1/finished_my_first_writing_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg3wt1/finished_my_first_writing_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg3wt1/finished_my_first_writing_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T11:11:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg045u</id>
    <title>Overwhelmed by so many model releases within a month period - What would be best coding and planning models around 60-100B / Fit in Strix-Halo 128GB VRam</title>
    <updated>2026-02-27T07:24:58+00:00</updated>
    <author>
      <name>/u/Voxandr</name>
      <uri>https://old.reddit.com/user/Voxandr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using StrixHalo with 128 GB VRam . I am using Kimi-Linear for tech documents and&lt;br /&gt; contracts + Qwen-3-Next 80b. For vibe coding i was using qwen 3 Coder 35B-A3B&lt;/p&gt; &lt;p&gt;I haven't tried Qwen 3.5s and Qwen3-coder-next&lt;/p&gt; &lt;p&gt;My questions are :&lt;/p&gt; &lt;p&gt;With Qwen 3.5 release is Qwen3-Next-Coder 80B-A3B Obselete?&lt;br /&gt; Would Qwen 3.5 dense 27B model Better for my Case vs MoE ?&lt;/p&gt; &lt;p&gt;Are there any better coder models that can fit in 100GB VRAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Voxandr"&gt; /u/Voxandr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg045u/overwhelmed_by_so_many_model_releases_within_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg045u/overwhelmed_by_so_many_model_releases_within_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg045u/overwhelmed_by_so_many_model_releases_within_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T07:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfjp6v</id>
    <title>top 10 trending models on HF</title>
    <updated>2026-02-26T19:24:56+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfjp6v/top_10_trending_models_on_hf/"&gt; &lt;img alt="top 10 trending models on HF" src="https://preview.redd.it/5rqv8z2s3wlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35fefab1ea2ac5a5020c86a7254058c09178c18e" title="top 10 trending models on HF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;any conclusions? ;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5rqv8z2s3wlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfjp6v/top_10_trending_models_on_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfjp6v/top_10_trending_models_on_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T19:24:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfds1h</id>
    <title>Qwen3.5-35B-A3B Q4 Quantization Comparison</title>
    <updated>2026-02-26T15:52:23+00:00</updated>
    <author>
      <name>/u/TitwitMuffbiscuit</name>
      <uri>https://old.reddit.com/user/TitwitMuffbiscuit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfds1h/qwen3535ba3b_q4_quantization_comparison/"&gt; &lt;img alt="Qwen3.5-35B-A3B Q4 Quantization Comparison" src="https://preview.redd.it/0u0z9evbawlg1.png?width=140&amp;amp;height=83&amp;amp;auto=webp&amp;amp;s=a345f51538cb357a05082a937420dad32d1a329b" title="Qwen3.5-35B-A3B Q4 Quantization Comparison" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a Q4 quantization sweep across all major community quants of Qwen3.5-35B-A3B, comparing faithfulness to the BF16 baseline across different quantizers and recipes.&lt;/p&gt; &lt;p&gt;The goal is to give people a data-driven basis for picking a file rather than just grabbing whatever is available.&lt;/p&gt; &lt;p&gt;For the uninitiated:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;KLD (KL Divergence):&lt;/strong&gt; &amp;quot;Faithfulness.&amp;quot; It shows how much the quantized model's probability distribution drifts from a baseline (the probability distribution of the original weights). Lower = closer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PPL (Perplexity):&lt;/strong&gt; Used to measure the average uncertainty of the model when predicting the next token. It is derived from the total information loss (Cross Entropy). Lower = more confident.&lt;/p&gt; &lt;p&gt;They are correlated. Perplexity measures the total error, KLD measures the relative error (like a routing drift of an MoE model). This relationship helps in determining information loss (or gain when training). Since we are trying to see how much information we've lost and since PPL is noisy as it can get a better score by pure luck, KLD is better as it is not relying on the dataset but on the baseline.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you need the most faithfull quant, pick the one with the lowest KLD.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;AesSedai's Q4_K_M achieves KLD 0.0102 by keeping always active tensors at Q8_0 (attention, shared experts) and differentiating ffn_down_exps from ffn_gate/up_exps.&lt;/p&gt; &lt;p&gt;Ubergarm's Q4_0 outperforms every other Q4_0 by a factor of 2.5 for the same reason.&lt;/p&gt; &lt;p&gt;MXFP4 is well-suited for QAT (Quantization Aware Training), where the model is trained to operate within MXFP4 numerical ranges but applied post-hoc to a BF16 model, it underperforms quants at equivalent size.&lt;/p&gt; &lt;p&gt;Unsloth's UD-Q4_K_XL recipe applies MXFP4 to nearly every tensor including ffn_down_exps and attention weights, resulting in the worst KLD in the sweep (0.0524). Unsloth is aware of this and working on it: &lt;a href="https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF/discussions/5"&gt;unsloth/Qwen3.5-35B-A3B-GGUF/discussions/5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are on the fence between files, use:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-perplexity -m &amp;lt;bf16_model&amp;gt; -f wiki.test.raw --kl-divergence-base &amp;lt;file_name&amp;gt; [other parameters] llama-perplexity -m &amp;lt;quantized_model&amp;gt; --kl-divergence-base &amp;lt;file_name&amp;gt; --kl-divergence [other parameters] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0u0z9evbawlg1.png?width=2979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d07bfd5a37e9c5fa9ae99648d202c7d4f7781ea5"&gt;https://preview.redd.it/0u0z9evbawlg1.png?width=2979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d07bfd5a37e9c5fa9ae99648d202c7d4f7781ea5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tpfh92qcawlg1.png?width=2979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a4122d61e6df11cb832583de314385d2533c8bc"&gt;https://preview.redd.it/tpfh92qcawlg1.png?width=2979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a4122d61e6df11cb832583de314385d2533c8bc&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Most Efficient Quantization&lt;/h1&gt; &lt;p&gt;The Efficiency Score is the distance to a 'perfect' model (zero size, zero KLD), not the &amp;quot;best&amp;quot; model but the VRAM sweet spot. Efficiency Score: √ (Normalized Size² + Normalized KLD²) — lower is better.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Rank&lt;/th&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Size (GiB)&lt;/th&gt; &lt;th align="left"&gt;KLD Score&lt;/th&gt; &lt;th align="left"&gt;Eff. Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;AesSedai_Qwen3.5-35B-A3B-IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;16.3999770582&lt;/td&gt; &lt;td align="left"&gt;0.024036&lt;/td&gt; &lt;td align="left"&gt;0.327342&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;17.4178144932&lt;/td&gt; &lt;td align="left"&gt;0.024273&lt;/td&gt; &lt;td align="left"&gt;0.411178&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-IQ4_NL&lt;/td&gt; &lt;td align="left"&gt;18.4062407017&lt;/td&gt; &lt;td align="left"&gt;0.023761&lt;/td&gt; &lt;td align="left"&gt;0.573661&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-MXFP4_MOE&lt;/td&gt; &lt;td align="left"&gt;18.4312270582&lt;/td&gt; &lt;td align="left"&gt;0.025288&lt;/td&gt; &lt;td align="left"&gt;0.599390&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-IQ4_NL&lt;/td&gt; &lt;td align="left"&gt;18.4010530412&lt;/td&gt; &lt;td align="left"&gt;0.027117&lt;/td&gt; &lt;td align="left"&gt;0.620673&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_K_S&lt;/td&gt; &lt;td align="left"&gt;19.0378324986&lt;/td&gt; &lt;td align="left"&gt;0.021415&lt;/td&gt; &lt;td align="left"&gt;0.679213&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-Q4_0&lt;/td&gt; &lt;td align="left"&gt;18.4779573381&lt;/td&gt; &lt;td align="left"&gt;0.035176&lt;/td&gt; &lt;td align="left"&gt;0.769475&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;ubergarm_Qwen3.5-35B-A3B-Q4_0&lt;/td&gt; &lt;td align="left"&gt;19.7865126431&lt;/td&gt; &lt;td align="left"&gt;0.015125&lt;/td&gt; &lt;td align="left"&gt;0.811116&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9&lt;/td&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;19.7692930698&lt;/td&gt; &lt;td align="left"&gt;0.018878&lt;/td&gt; &lt;td align="left"&gt;0.824589&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_0&lt;/td&gt; &lt;td align="left"&gt;18.7150785923&lt;/td&gt; &lt;td align="left"&gt;0.037042&lt;/td&gt; &lt;td align="left"&gt;0.839537&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;11&lt;/td&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;19.7489992082&lt;/td&gt; &lt;td align="left"&gt;0.023362&lt;/td&gt; &lt;td align="left"&gt;0.852727&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;12&lt;/td&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_K_L&lt;/td&gt; &lt;td align="left"&gt;20.1208174229&lt;/td&gt; &lt;td align="left"&gt;0.018232&lt;/td&gt; &lt;td align="left"&gt;0.902187&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;13&lt;/td&gt; &lt;td align="left"&gt;lmstudio_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;19.7050000000&lt;/td&gt; &lt;td align="left"&gt;0.032892&lt;/td&gt; &lt;td align="left"&gt;0.949834&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;14&lt;/td&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_1&lt;/td&gt; &lt;td align="left"&gt;20.3849241734&lt;/td&gt; &lt;td align="left"&gt;0.022821&lt;/td&gt; &lt;td align="left"&gt;0.990643&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;15&lt;/td&gt; &lt;td align="left"&gt;AesSedai_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;20.6187270582&lt;/td&gt; &lt;td align="left"&gt;0.010214&lt;/td&gt; &lt;td align="left"&gt;1.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-Q4_1&lt;/td&gt; &lt;td align="left"&gt;20.3642488420&lt;/td&gt; &lt;td align="left"&gt;0.026266&lt;/td&gt; &lt;td align="left"&gt;1.013664&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;noctrex_Qwen3.5-35B-A3B-MXFP4_MOE_BF16&lt;/td&gt; &lt;td align="left"&gt;20.5495284498&lt;/td&gt; &lt;td align="left"&gt;0.024921&lt;/td&gt; &lt;td align="left"&gt;1.043445&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;18&lt;/td&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;18.3351655900&lt;/td&gt; &lt;td align="left"&gt;0.052439&lt;/td&gt; &lt;td align="left"&gt;1.100189&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Note: The Efficiency Score uses AesSedai Q4_K_M as the reference point (score = 1.0) as the ceiling. Files scoring below 1.0 offer a better size/quality tradeoff and vice versa.&lt;/p&gt; &lt;h1&gt;Data (sorted by KLD)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Size (GiB)&lt;/th&gt; &lt;th align="left"&gt;PPL Score&lt;/th&gt; &lt;th align="left"&gt;KLD Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;AesSedai_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;20.62&lt;/td&gt; &lt;td align="left"&gt;6.436887&lt;/td&gt; &lt;td align="left"&gt;0.010214&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ubergarm_Qwen3.5-35B-A3B-Q4_0&lt;/td&gt; &lt;td align="left"&gt;19.79&lt;/td&gt; &lt;td align="left"&gt;6.461745&lt;/td&gt; &lt;td align="left"&gt;0.015125&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_K_L&lt;/td&gt; &lt;td align="left"&gt;20.12&lt;/td&gt; &lt;td align="left"&gt;6.499422&lt;/td&gt; &lt;td align="left"&gt;0.018232&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;19.77&lt;/td&gt; &lt;td align="left"&gt;6.491274&lt;/td&gt; &lt;td align="left"&gt;0.018878&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_K_S&lt;/td&gt; &lt;td align="left"&gt;19.04&lt;/td&gt; &lt;td align="left"&gt;6.512668&lt;/td&gt; &lt;td align="left"&gt;0.021415&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_1&lt;/td&gt; &lt;td align="left"&gt;20.39&lt;/td&gt; &lt;td align="left"&gt;6.473700&lt;/td&gt; &lt;td align="left"&gt;0.022821&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;19.75&lt;/td&gt; &lt;td align="left"&gt;6.518045&lt;/td&gt; &lt;td align="left"&gt;0.023362&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-IQ4_NL&lt;/td&gt; &lt;td align="left"&gt;18.41&lt;/td&gt; &lt;td align="left"&gt;6.506714&lt;/td&gt; &lt;td align="left"&gt;0.023761&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AesSedai_Qwen3.5-35B-A3B-IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;16.40&lt;/td&gt; &lt;td align="left"&gt;6.517477&lt;/td&gt; &lt;td align="left"&gt;0.024036&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;17.42&lt;/td&gt; &lt;td align="left"&gt;6.511643&lt;/td&gt; &lt;td align="left"&gt;0.024273&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;noctrex_Qwen3.5-35B-A3B-MXFP4_MOE_BF16&lt;/td&gt; &lt;td align="left"&gt;20.55&lt;/td&gt; &lt;td align="left"&gt;6.487453&lt;/td&gt; &lt;td align="left"&gt;0.024921&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-MXFP4_MOE&lt;/td&gt; &lt;td align="left"&gt;18.43&lt;/td&gt; &lt;td align="left"&gt;6.485211&lt;/td&gt; &lt;td align="left"&gt;0.025288&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-Q4_1&lt;/td&gt; &lt;td align="left"&gt;20.36&lt;/td&gt; &lt;td align="left"&gt;6.530645&lt;/td&gt; &lt;td align="left"&gt;0.026266&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-IQ4_NL&lt;/td&gt; &lt;td align="left"&gt;18.40&lt;/td&gt; &lt;td align="left"&gt;6.523618&lt;/td&gt; &lt;td align="left"&gt;0.027117&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;lmstudio_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;19.705&lt;/td&gt; &lt;td align="left"&gt;6.543927&lt;/td&gt; &lt;td align="left"&gt;0.032892&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-Q4_0&lt;/td&gt; &lt;td align="left"&gt;18.48&lt;/td&gt; &lt;td align="left"&gt;6.574551&lt;/td&gt; &lt;td align="left"&gt;0.035176&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_0&lt;/td&gt; &lt;td align="left"&gt;18.72&lt;/td&gt; &lt;td align="left"&gt;6.501674&lt;/td&gt; &lt;td align="left"&gt;0.037042&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;18.34&lt;/td&gt; &lt;td align="left"&gt;6.636498&lt;/td&gt; &lt;td align="left"&gt;0.052439&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Setup&lt;/h1&gt; &lt;p&gt;CPU: Intel Core i3-12100F RAM: 64 GB DDR4 3200, dual channel. GPU: RTX 3060 12 GB (GPU clock fixed at 1882 MHz via curve, VRAM at 8210 MHz, stable). OS: Windows 11, Nvidia drivers 591.74&lt;/p&gt; &lt;p&gt;ik_llama.cpp: Thireus/ik_llama.cpp — build main-b4299-15482f0, Windows x64 CUDA 13.1 AVX2. Mainline llama.cpp compatibility: tested against b8157 (2943210c1), Windows x64 CUDA 13.1. &lt;/p&gt; &lt;h1&gt;Details&lt;/h1&gt; &lt;p&gt;PPL and KLD are calculated with &lt;code&gt;wikitext2_test.txt&lt;/code&gt; at a context of 512 tokens with &lt;code&gt;-ncmoe 22&lt;/code&gt; and &lt;code&gt;-ngl 999&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;KLD base logits generated from the BF16 model (full CPU offload, no &lt;code&gt;-ncmoe&lt;/code&gt;).&lt;/p&gt; &lt;h1&gt;Notes&lt;/h1&gt; &lt;p&gt;Results reflect faithfulness to the BF16 baseline on a general text corpus (wikitext2). Task-specific performance (reasoning, code, instruction following) may order things differently, particularly at the extremes.&lt;/p&gt; &lt;p&gt;The MXFP4 findings here are specific to post-training quantization. MXFP4 applied during QAT (as in GPT-OSS-120B) is a different and more principled use of the format.&lt;/p&gt; &lt;p&gt;Plots use a linear scale. A logarithmic scale would better represent the distribution of KLD values across the full quantization range, but linear scaling makes the differences within the Q4 range immediately readable without requiring familiarity with log representations.&lt;/p&gt; &lt;p&gt;If unsloth_Qwen3.5-35B-A3B-UD-Q4_K_XL gets fixed, I'll evaluate and update this post with a clear mention of the before and after.&lt;/p&gt; &lt;p&gt;I won't be able to test more quants, it's kind of sunny outside.&lt;/p&gt; &lt;p&gt;edit: all quants work both on llama.cpp and ik_llama.cpp for txt2txt but ik_llama.cpp might not support img2txt as of now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TitwitMuffbiscuit"&gt; /u/TitwitMuffbiscuit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfds1h/qwen3535ba3b_q4_quantization_comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfds1h/qwen3535ba3b_q4_quantization_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfds1h/qwen3535ba3b_q4_quantization_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T15:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg0487</id>
    <title>System prompt for Qwen3.5 (27B/35BA3B) to reduce overthinking?</title>
    <updated>2026-02-27T07:25:03+00:00</updated>
    <author>
      <name>/u/thigger</name>
      <uri>https://old.reddit.com/user/thigger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone found a good way to persuade Qwen3.5 (27B/35BA3B) to keep their reasoning budget sensible? They seem to be really good models but particularly the MoE goes absolutely insane second-guessing itself and sometimes even looping.&lt;/p&gt; &lt;p&gt;I'm outputting JSON so not keen on too much repetition penalty, so have been trying out system prompts - currently telling it:&lt;/p&gt; &lt;p&gt;&amp;quot;You are a concise, efficient, decisive assistant. Think in 2-3 short blocks without repetition or second-guessing, and then output your answer&amp;quot;&lt;/p&gt; &lt;p&gt;This has made things very slightly better but not much. Any tips?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thigger"&gt; /u/thigger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg0487/system_prompt_for_qwen35_27b35ba3b_to_reduce/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg0487/system_prompt_for_qwen35_27b35ba3b_to_reduce/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg0487/system_prompt_for_qwen35_27b35ba3b_to_reduce/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T07:25:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfmzfp</id>
    <title>New Upcoming Ubuntu 26.04 LTS Will be Optimized for Local AI</title>
    <updated>2026-02-26T21:26:44+00:00</updated>
    <author>
      <name>/u/mtomas7</name>
      <uri>https://old.reddit.com/user/mtomas7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some interesting new developments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Out-of-the-box NVIDIA CUDA and AMD ROCm drivers that are auto-selected for your particular hardware &lt;a href="https://youtu.be/0CYm-KCw7yY&amp;amp;t=316"&gt;https://youtu.be/0CYm-KCw7yY&amp;amp;t=316&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Inference Snaps - ready-to-use sandboxed AI inference containers (reminds a bit the Mozilla llamafile project): &lt;ul&gt; &lt;li&gt;Feature presentation: &lt;a href="https://youtu.be/0CYm-KCw7yY&amp;amp;t=412"&gt;https://youtu.be/0CYm-KCw7yY&amp;amp;t=412&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo: &lt;a href="https://youtu.be/0CYm-KCw7yY&amp;amp;t=1183"&gt;https://youtu.be/0CYm-KCw7yY&amp;amp;t=1183&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Sandboxing AI Agents: &lt;a href="https://youtu.be/0CYm-KCw7yY&amp;amp;t=714"&gt;https://youtu.be/0CYm-KCw7yY&amp;amp;t=714&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtomas7"&gt; /u/mtomas7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfmzfp/new_upcoming_ubuntu_2604_lts_will_be_optimized/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfmzfp/new_upcoming_ubuntu_2604_lts_will_be_optimized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfmzfp/new_upcoming_ubuntu_2604_lts_will_be_optimized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T21:26:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfg3kx</id>
    <title>American closed models vs Chinese open models is becoming a problem.</title>
    <updated>2026-02-26T17:15:48+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The work I do involves customers that are sensitive to nation state politics. We cannot and do not use cloud API services for AI because the data must not leak. Ever. As a result we use open models in closed environments.&lt;/p&gt; &lt;p&gt;The problem is that my customers don’t want Chinese models. “National security risk”.&lt;/p&gt; &lt;p&gt;But the only recent semi-capable model we have from the US is gpt-oss-120b, which is far behind modern LLMs like GLM, MiniMax, etc.&lt;/p&gt; &lt;p&gt;So we are in a bind: use an older, less capable model and slowly fall further and further behind the curve, or… what?&lt;/p&gt; &lt;p&gt;I suspect this is why Hegseth is pressuring Anthropic: the DoD needs offline AI for awful purposes and wants Anthropic to give it to them.&lt;/p&gt; &lt;p&gt;But what do we do? Tell the customers we’re switching to Chinese models because the American models are locked away behind paywalls, logging, and training data repositories? Lobby for OpenAI to do us another favor and release another open weights model? We certainly cannot just secretly use Chinese models, but the American ones are soon going to be irrelevant. We’re in a bind.&lt;/p&gt; &lt;p&gt;&lt;del&gt;Our one glimmer of hope is StepFun-AI out of South Korea. Maybe they’ll save Americans from themselves.&lt;/del&gt; I stand corrected: they’re in Shanghai.&lt;/p&gt; &lt;p&gt;Cohere are in Canada and may be a solid option. Or maybe someone can just torrent Opus once the Pentagon force Anthropic to hand it over…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfg3kx/american_closed_models_vs_chinese_open_models_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfg3kx/american_closed_models_vs_chinese_open_models_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfg3kx/american_closed_models_vs_chinese_open_models_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T17:15:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg0ir2</id>
    <title>After using local models for one month, I learned more than in two years with cloud models</title>
    <updated>2026-02-27T07:49:43+00:00</updated>
    <author>
      <name>/u/Ambitious-Sense-7773</name>
      <uri>https://old.reddit.com/user/Ambitious-Sense-7773</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I started with qwen2.5 and first had to figure out why getting context overflow. Had to raise context, tune temperature, top-K and top-P. Then got qwen3(mlx) and was blown away by the speed of mixture of experts. Learned about KV cache linear growth, why i need to eject the model from time to time. Also learned that replaying old prompt to fresh LM results into same state each time.&lt;/p&gt; &lt;p&gt;Now qwen3.5 doesnt seem to increase mem usage, event though i disabled auto-reset from lm studio.&lt;/p&gt; &lt;p&gt;Pondering if I should set up a shared solution for other people, but not sure would the KV cache eat all memory.&lt;/p&gt; &lt;p&gt;I just wish there was a lm studio resource monitor, telling token flow, KV cache, activated experts and so. &lt;/p&gt; &lt;p&gt;That being said, my knowledge is basically constrained to basic transformer architecture without MoE and whatnot optimizations. Would be interested in LoRa training but dont know if I got the time. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious-Sense-7773"&gt; /u/Ambitious-Sense-7773 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg0ir2/after_using_local_models_for_one_month_i_learned/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg0ir2/after_using_local_models_for_one_month_i_learned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg0ir2/after_using_local_models_for_one_month_i_learned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T07:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg41ss</id>
    <title>Qwen3.5 27B vs Devstral Small 2 - Next.js &amp; Solidity (Hardhat)</title>
    <updated>2026-02-27T11:19:24+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg41ss/qwen35_27b_vs_devstral_small_2_nextjs_solidity/"&gt; &lt;img alt="Qwen3.5 27B vs Devstral Small 2 - Next.js &amp;amp; Solidity (Hardhat)" src="https://preview.redd.it/wn89u3hyo1mg1.png?width=140&amp;amp;height=84&amp;amp;auto=webp&amp;amp;s=e7dfcacc10647a458b6033b3bb723477ddda4ff5" title="Qwen3.5 27B vs Devstral Small 2 - Next.js &amp;amp; Solidity (Hardhat)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Greetings,&lt;/p&gt; &lt;p&gt;I was excited to test the 27B and 35BA3B variants, to see whether they were superior to my daily driver, Devstral Small 2.&lt;/p&gt; &lt;p&gt;Had issues for the reported UD-Q4_K_XL. After over-examining across PPL and KLD, I went with mradermacher as I followed their card for quality.&lt;/p&gt; &lt;p&gt;Anecdotally, on the work done in some of my repos, Qwen3.5 27B was superior in quality - planning, coding and compiling to no error, and fixing few snags when needed.&lt;/p&gt; &lt;p&gt;The 27B documentation write-ups can be super extensive on a Q6 quant, where Devstral Small 2 can produce from Q8. It's nice if you like verbose documents and has capability to write/edit at length.&lt;/p&gt; &lt;p&gt;Qwen3.5 35BA3B is simpler in planning but was not shy on execution, as it was able to refactor a single +900 LoC file into 35 different parts - it was excessive but I had requested it to see how complex it could handle.&lt;/p&gt; &lt;p&gt;After several attempts, the way it performed the refactor was entirely different from other models I had used in the past - it positioned main elements titles and components in most odd files. These we informal trials.&lt;/p&gt; &lt;p&gt;I can say Qwen3.5 35BA3B can over-engineer if not guided properly, but I did not go far with it, as I found the issue stated earlier a nuisance, for something that could've been simple from a SWE perspective. I might have been unfair and cherry picked too fast, due to time constraints at the time.&lt;/p&gt; &lt;p&gt;I found the pick between Qwen3.5 27B and Devstral Small 2 a hard choice. I am used to Mistral's efficiency and repo work capability, but couldn't settle my finger if Qwen was superior as the executions were pretty much identical and token spending.&lt;/p&gt; &lt;p&gt;To my surprise, Artificial Analysis put Qwen's 27B at a level similar to Deepseek V3.2 and suspiciously close of Sonnet 4.5. &lt;em&gt;Trust but verify.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So, to settle my mind on the early agentic coding department, I created 78 agentic challenges in one of my prod repos, to check which model came out the best, in one of my Next.js and Solidity repo.&lt;/p&gt; &lt;h1&gt;Stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fedora 43&lt;/li&gt; &lt;li&gt;llama.cpp b8149 | docker `nvidia/cuda:13.1.0-devel-ubuntu24.04`&lt;/li&gt; &lt;li&gt;RTX 5090 | stock | driver 580.119.02&lt;/li&gt; &lt;li&gt;Ryzen 9 9950X | 96GB DDR5 6000&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Llama.cpp Build Flags&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;RUN set -eux; \ echo &amp;quot;CMAKE_CUDA_ARCHITECTURES=${CMAKE_CUDA_ARCHITECTURES}&amp;quot;; \ rm -rf build; \ cmake -S . -B build -G Ninja \ -DCMAKE_BUILD_TYPE=Release \ -DCMAKE_C_COMPILER=${CC} \ -DCMAKE_CXX_COMPILER=${CXX} \ -DCMAKE_LINKER=${LD} \ -DGGML_NATIVE=ON \ -DGGML_LTO=${GGML_LTO} \ -DGGML_OPENMP=ON \ -DGGML_BLAS=ON \ -DGGML_BLAS_VENDOR=OpenBLAS \ -DGGML_CUDA=ON \ -DCMAKE_CUDA_ARCHITECTURES=&amp;quot;${CMAKE_CUDA_ARCHITECTURES}&amp;quot; \ -DGGML_CUDA_GRAPHS=ON \ -DGGML_CUDA_FA=ON \ -DGGML_CUDA_FA_ALL_QUANTS=${GGML_CUDA_FA_ALL_QUANTS} \ -DGGML_CUDA_COMPRESSION_MODE=${GGML_CUDA_COMPRESSION_MODE} \ -DLLAMA_BUILD_SERVER=ON \ -DLLAMA_BUILD_EXAMPLES=OFF; \ cmake --build build -j&amp;quot;$(nproc)&amp;quot;; \ cmake --install build --prefix /opt/llama &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Quants &amp;amp; Flags&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;mradermacher | Qwen3.5 27B i1-Q6_K | Model+Context 29.3GB&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; - -t - &amp;quot;8&amp;quot; - --numa - numactl - --jinja - --temp - &amp;quot;0.6&amp;quot; - --top-p - &amp;quot;0.95&amp;quot; - --top-k - &amp;quot;20&amp;quot; - --min-p - &amp;quot;0.0&amp;quot; - --presence-penalty - &amp;quot;0.0&amp;quot; - --repeat-penalty - &amp;quot;1.0&amp;quot; - -b - &amp;quot;512&amp;quot; - -ub - &amp;quot;512&amp;quot; - --no-mmap - -c - &amp;quot;111000&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;unsloth | Devstral-Small-2-24B-Instruct-2512-Q6_K | Model+Context 29.9GB&lt;/strong&gt; ADDED*&lt;/p&gt; &lt;pre&gt;&lt;code&gt; - -t - &amp;quot;8&amp;quot; - --chat-template-file - /models/devstral-fix.jinja # custom chat template - --temp - &amp;quot;0.15&amp;quot; - --min-p - &amp;quot;0.01&amp;quot; - --numa - numactl - -b - &amp;quot;512&amp;quot; - -ub - &amp;quot;512&amp;quot; - --no-mmap - -c - &amp;quot;71125&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;byteshape | Devstral Small 2 24B IQ4_XS-4.04bpw | Model+Context 28.9GB&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; - -t - &amp;quot;8&amp;quot; - --chat-template-file - /models/devstral-fix.jinja # custom chat template - --temp - &amp;quot;0.15&amp;quot; - --min-p - &amp;quot;0.01&amp;quot; - --numa - numactl - -ctk - q8_0 - -ctv - q8_0 - -b - &amp;quot;512&amp;quot; - -ub - &amp;quot;512&amp;quot; - --no-mmap - -c - &amp;quot;200000&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;I have compiled some of the information below with an LLM for simplicity:&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;The Benchmark&lt;/h1&gt; &lt;p&gt;Executed a single suite with 78 tasks (39 Next.js + 39 Hardhat) via Opencode. Each model ran the whole suite in a single pass - executing each task separately as new session, to avoid context compressions and context blow.&lt;/p&gt; &lt;h1&gt;Scoring rubric (per task, 0-100)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Correctness (0 or 60 points)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;60 if the patch fully satisfies task checks.&lt;/li&gt; &lt;li&gt;0 if it fails.&lt;/li&gt; &lt;li&gt;This is binary to reward complete fixes, not partial progress.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Compatibility (0-20 points)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Measures whether the patch preserves required integration/contract expectations for that task.&lt;/li&gt; &lt;li&gt;Usually task-specific checks.&lt;/li&gt; &lt;li&gt;Full compatibility = 20 | n partial = lower | broken/missing = 0&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Scope Discipline (0-20 points)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Measures edit hygiene: &lt;em&gt;did the model change only relevant files?&lt;/em&gt;&lt;/li&gt; &lt;li&gt;20 if changes stay in intended scope.&lt;/li&gt; &lt;li&gt;Penalised as unrelated edits increase.&lt;/li&gt; &lt;li&gt;Extra penalty if the model creates a commit during benchmarking.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why this design works&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Total score = Correctness + Compatibility + Scope Discipline (max 100)&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;60% on correctness keeps &lt;em&gt;“works vs doesn’t work”&lt;/em&gt; as the primary signal.&lt;/li&gt; &lt;li&gt;20% compatibility penalises fixes that break expected interfaces/behaviour.&lt;/li&gt; &lt;li&gt;20% scope discipline penalises noisy, risky patching and rewards precise edits.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;mradermacher | Qwen3.5-27B.i1-Q6_K.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; 4134 score total | 53.00 avg score per task | 48/78 pass (61.54%) - Prompt Processing Speed: - Mean per request: 1326.80 tok/s - Token-weighted: 1596.20 tok/s - Token Generation Speed: - Mean per-request: 45.24 tok/s - Token-weighted: 45.03 tok/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;unsloth | Devstral-Small-2-24B-Instruct-2512-Q6_K.gguf&lt;/strong&gt; ADDED*&lt;/p&gt; &lt;pre&gt;&lt;code&gt;2778 score total | 34.62 avg score per task | 27/78 pass (34.62%) - Prompt processing: - Mean: 2015.13 tok/s - Median: 2193.43 tok/s - Token-weighted: 2458.97 tok/s - Token generation: - Mean: 53.29 tok/s - Median: 54.05 tok/s - Token-weighted: 48.01 tok/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;byteshape | Devstral-Small-2-24B-Instruct-2512-IQ4_XS-4.04bpw.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; 3158 total score | 40.49 avg score per task | 33/78 pass (42.31%) - Prompt Processing Speed: - Mean per request: 2777.02 toks/s - Token-weighted: 4200.64 toks/s - Token Generation Speed: - Mean per-request: 90.49 tok/s - Token-weighted: 89.31 tok/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;- Devstral is &lt;strong&gt;not&lt;/strong&gt; an IQ4_XS quant due HF naming convention compatibility for exotic gguf types. The quant is designated as above &lt;strong&gt;4.04bpw&lt;/strong&gt; by &lt;a href="https://huggingface.co/byteshape/Devstral-Small-2-24B-Instruct-2512-GGUF"&gt;Byteshape&lt;/a&gt; which follows a Q8_0 quality equivalent.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stack Score Split&lt;/strong&gt; ADDED*&lt;/p&gt; &lt;pre&gt;&lt;code&gt; - Next.js avg score: 1. byteshape Devstral-Small-2-24B-Instruct-2512-IQ4_XS-4.04bpw (64.82%) 2. unsloth Devstral-Small-2-24B-Instruct-2512-Q6_K (58.26%) 3. mradermacher Qwen3.5-27B.i1-Q6_K (56.82%) - Hardhat avg score: 1. mradermacher Qwen3.5-27B.i1-Q6_K (49.18%) 2. byteshape Devstral-Small-2-24B-Instruct-2512-IQ4_XS-4.04bpw (16.15%) 3. unsloth Devstral-Small-2-24B-Instruct-2512-Q6_K (12.97%) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;The takeaway&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Devstral from Byteshape was stronger on Next.js-only tasks, but Qwen was much more robust on Hardhat/contract engineering, which decided the overall suite winner.&lt;/p&gt; &lt;p&gt;This sums what I've experienced when attempting using Devstral for Solidity even with the previous generation. I am impressed Qwen was able to work with Solidity, so it's something I could explore in near future when I need to refactor contracts.&lt;/p&gt; &lt;p&gt;Since most of my work surrounds Rust and Next.js I might stick with Devstral Small 2 for repo work, which also it's faster and can use 200k context window quite comfortably. I can go closer to 220-230k but its starts cramming VRAM and glitching screens.&lt;/p&gt; &lt;p&gt;I would probably include some Rust benchmarks as well in my other repos, as Devstral Small 2 is strong there (GLM 4.7 Flash cratered) if I can get some time.&lt;/p&gt; &lt;p&gt;I still have to try Qwen3.5 27B in other areas such as general assistant, etc.&lt;/p&gt; &lt;p&gt;I hope that helps anyone.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;*ADDED suite results from Unsloth Devstral Small 24B Q6_K&lt;/li&gt; &lt;li&gt;Score and speed charts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wn89u3hyo1mg1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7bae8ba233eba3bde7aee485d7e423cf68f0b7d"&gt;https://preview.redd.it/wn89u3hyo1mg1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7bae8ba233eba3bde7aee485d7e423cf68f0b7d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8cl1lbdhp1mg1.png?width=2040&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=155aca24f3a7f2785555cb4613313d978f3dd0d4"&gt;https://preview.redd.it/8cl1lbdhp1mg1.png?width=2040&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=155aca24f3a7f2785555cb4613313d978f3dd0d4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg41ss/qwen35_27b_vs_devstral_small_2_nextjs_solidity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg41ss/qwen35_27b_vs_devstral_small_2_nextjs_solidity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg41ss/qwen35_27b_vs_devstral_small_2_nextjs_solidity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T11:19:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfp6bk</id>
    <title>why is openclaw even this popular?</title>
    <updated>2026-02-26T22:50:15+00:00</updated>
    <author>
      <name>/u/Crazyscientist1024</name>
      <uri>https://old.reddit.com/user/Crazyscientist1024</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;recently i haven't been following up on the latest AI dramas and just came back from a vacation. Did some looking around and found out that OpenClaw just blew up, looked into it but I didn't find anything significantly special. It just seems to be like a wrapper that has a huge amounts of pre-programmed function calls / skills / whatever built into it.&lt;/p&gt; &lt;p&gt;Am I missing something? How is this blowing up? Respectfully, even for newbie programmers, they can probably simply vibe code a way more lightweight tool themselves in a day dedicated for their task at hand.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Crazyscientist1024"&gt; /u/Crazyscientist1024 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfp6bk/why_is_openclaw_even_this_popular/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfp6bk/why_is_openclaw_even_this_popular/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfp6bk/why_is_openclaw_even_this_popular/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T22:50:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg4apu</id>
    <title>Qwen 3.5 Architecture Analysis: Parameter Distribution in the Dense 27B vs. 122B/35B MoE Models</title>
    <updated>2026-02-27T11:33:21+00:00</updated>
    <author>
      <name>/u/Luca3700</name>
      <uri>https://old.reddit.com/user/Luca3700</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4apu/qwen_35_architecture_analysis_parameter/"&gt; &lt;img alt="Qwen 3.5 Architecture Analysis: Parameter Distribution in the Dense 27B vs. 122B/35B MoE Models" src="https://preview.redd.it/gnzye3xgw0mg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=0b22d57c5ebae2ec043a1a48c7da75344ed162db" title="Qwen 3.5 Architecture Analysis: Parameter Distribution in the Dense 27B vs. 122B/35B MoE Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday, I wrote a &lt;a href="https://www.reddit.com/r/LocalLLaMA/s/EdTcLCLtTD"&gt;comment on this post&lt;/a&gt; on why, in my opinion, the dense model Qwen 3.5 27B can achieve good results in benchmarks, by providing an architectural analysis. And today I'm expanding my thoughts in this post.&lt;/p&gt; &lt;h1&gt;Intro&lt;/h1&gt; &lt;p&gt;A few days ago, Qwen released three new models: two &lt;strong&gt;Mixture of Experts models&lt;/strong&gt; (122B A10 and 35B A3) and a &lt;strong&gt;dense model&lt;/strong&gt; (with 27B parameters).&lt;/p&gt; &lt;p&gt;All of them share a similar architecture, that interleaves &lt;strong&gt;three Gated DeltaNet&lt;/strong&gt; layers with a &lt;strong&gt;Gated Attention&lt;/strong&gt; Layer, each of them followed by their respective Feed Forward Network.&lt;/p&gt; &lt;p&gt;Before going in detail in the analysis, let's summarize the three architectures with this picture (taken from the models overview on huggingface).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gnzye3xgw0mg1.jpg?width=2125&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e0fe6c74b37c8f212024d7f1398784289c020e09"&gt;Models overview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the hidden layout of the 122B model appears to be incorrect in the picture, because it should be &lt;em&gt;12x&lt;/em&gt; (3x ... -&amp;gt; 1x ...) and not &lt;em&gt;16x&lt;/em&gt;, because the number of layers is 48 (as stated in the config.json file as well)&lt;/p&gt; &lt;h1&gt;Architecture Analysis - Feed Forward Network&lt;/h1&gt; &lt;p&gt;Even though the blueprint is similar, the parameter distribution is different, and the &lt;strong&gt;main divergence&lt;/strong&gt; between the MoE models and the 27B dense model is that the former use &lt;strong&gt;more parameters in the experts&lt;/strong&gt; of the Feed Forward Network. In contrast, the 27B model (due to the use of a dense Feed Forward Network that uses less parameters than the MoE counterpart) is able to &lt;strong&gt;allocate more of them to other parts of the network&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If we want to quantify the amount of parameters used in the FFN layers, we could say that for the MoE models is &lt;/p&gt; &lt;p&gt;&lt;code&gt;2 x hidden_dim x expert_int_dim x num_experts x num_layers&lt;/code&gt;&lt;/p&gt; &lt;p&gt;instead for the dense model is&lt;/p&gt; &lt;p&gt;&lt;code&gt;2 x hidden_dim x int_dim x num_layers&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Therefore, we obtain:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;122B MoE model: 77,3 B (active 2,7) -&amp;gt; &lt;strong&gt;63% (2,2%)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;35B MoE model: 21,5 B (active 0,8) -&amp;gt; &lt;strong&gt;61% (2,3%)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;27B dense model: 9,1 B -&amp;gt; &lt;strong&gt;34%&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Where these parameters go in the dense model?&lt;/h1&gt; &lt;p&gt;The dense model is able to use, in percentage, half of the parameters in the FFN layers, and can spread them to other parts of the architecture (the following points correspond to the numbers on the arrows in the images): &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;the dense model is deeper&lt;/strong&gt;, it has 64 layers (instead the MoE models have respectively 48 and 40), and this should allow the model to have more depth for reasoning tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;it uses 4 keys and 4 values in the gated attention layers&lt;/strong&gt; (compared to only 2 than the MoE architectures), and it could allow the attention layer to capture more nuances&lt;/li&gt; &lt;li&gt;&lt;strong&gt;it uses more heads in the Gated DeltaNet layers&lt;/strong&gt; compared to the 35B counterpart.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Another point to take into account is the number of active parameters. Although the dense model has a smaller number of parameters in the FFN, it uses more of them actively, allowing it to use &lt;strong&gt;more computational power per token&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;Therefore, the 27B dense model can be seen, under the points of view listed above, as a &lt;strong&gt;deeper and wider&lt;/strong&gt; network than the 35B MoE model, and in some respects also than the 122B model. &lt;/p&gt; &lt;p&gt;I think that all these differences allow the dense model to have comparable performance to its bigger brother, even given the &lt;strong&gt;4,5x smaller parameter footprint&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Thank you for reading until here!&lt;/p&gt; &lt;p&gt;What do you think about this analysis? &lt;/p&gt; &lt;p&gt;Note: LLM used only for grammar checks and title suggestion. Post inspired by the &lt;a href="/u/seraschka"&gt;u/seraschka&lt;/a&gt; architectures deep dive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Luca3700"&gt; /u/Luca3700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4apu/qwen_35_architecture_analysis_parameter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4apu/qwen_35_architecture_analysis_parameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4apu/qwen_35_architecture_analysis_parameter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T11:33:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg8dex</id>
    <title>PewDiePie fine-tuned Qwen2.5-Coder-32B to beat ChatGPT 4o on coding benchmarks.</title>
    <updated>2026-02-27T14:37:18+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8dex/pewdiepie_finetuned_qwen25coder32b_to_beat/"&gt; &lt;img alt="PewDiePie fine-tuned Qwen2.5-Coder-32B to beat ChatGPT 4o on coding benchmarks." src="https://external-preview.redd.it/mCmYhKXGNj-QOd-sXT1nvg6KbIIK9oXVkPL1aBEF4FY.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e3e22dae46cc10fa4599a3b4892076af3a2cc56" title="PewDiePie fine-tuned Qwen2.5-Coder-32B to beat ChatGPT 4o on coding benchmarks." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=aV4j5pXLP-I&amp;amp;feature=youtu.be"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8dex/pewdiepie_finetuned_qwen25coder32b_to_beat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8dex/pewdiepie_finetuned_qwen25coder32b_to_beat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T14:37:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg87bj</id>
    <title>Qwen3.5-35B-A3B running on a Raspberry Pi 5 (16GB and 8GB variants)</title>
    <updated>2026-02-27T14:30:32+00:00</updated>
    <author>
      <name>/u/jslominski</name>
      <uri>https://old.reddit.com/user/jslominski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg87bj/qwen3535ba3b_running_on_a_raspberry_pi_5_16gb_and/"&gt; &lt;img alt="Qwen3.5-35B-A3B running on a Raspberry Pi 5 (16GB and 8GB variants)" src="https://external-preview.redd.it/dW03amZwN3BuMW1nMbm3gzMkoTfsIgGu4gtuHELcKn5C4RjCnBaO28O0Pqr2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=add8ad0f0fb00757ae8c54e3c5d3380471cac48f" title="Qwen3.5-35B-A3B running on a Raspberry Pi 5 (16GB and 8GB variants)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the release of the latest Qwens, I wanted to test something that, at first thought, sounds a bit crazy: &lt;strong&gt;running Qwen3.5-35B-A3B on a Raspberry Pi&lt;/strong&gt; (re-using my pet project, you can see the device’s telemetry in the right pane). The best I got so far is a bit over &lt;strong&gt;3 t/s&lt;/strong&gt; on the 16GB variant and over &lt;strong&gt;1.5 t/s&lt;/strong&gt; on the 8GB RAM version, using 2-bit quants, without an NVMe SSD (just relatively fast SD cards) and, frankly, pretty crap cooling. I had throttling issues on both of my Pis, so I ordered a new cooler and an SSD HAT yesterday, which should help.&lt;/p&gt; &lt;p&gt;I’m also working on a custom llama.cpp build for Pi and experimenting with some tweaks, plus a few experiments with ARM’s KleidiAI (please don’t focus on the example's output since I’m still tweaking, trying different quants and inference params). To be honest, this looks pretty promising for agentic tasks, maybe some education, etc. They run almost as fast as 4-bit variants of Qwen3-4B-VL, which is pretty cool, given hum big those models are relative to the Pi capabilities. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jslominski"&gt; /u/jslominski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mfr3o67pn1mg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg87bj/qwen3535ba3b_running_on_a_raspberry_pi_5_16gb_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg87bj/qwen3535ba3b_running_on_a_raspberry_pi_5_16gb_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T14:30:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg6ph3</id>
    <title>Qwen3.5 feels ready for production use - Never been this excited</title>
    <updated>2026-02-27T13:29:41+00:00</updated>
    <author>
      <name>/u/alphatrad</name>
      <uri>https://old.reddit.com/user/alphatrad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg6ph3/qwen35_feels_ready_for_production_use_never_been/"&gt; &lt;img alt="Qwen3.5 feels ready for production use - Never been this excited" src="https://preview.redd.it/kfx0j6lzf1mg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=6dcf868e7837c472ba8da5f18f55d96a1a2ab7a7" title="Qwen3.5 feels ready for production use - Never been this excited" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran a lot of tests playing with Qwen3.5-35B-A3B-UD-Q6_K_XL yesterday. Hitting around 1504pp2048 and 47.71 tg256 &lt;/p&gt; &lt;p&gt;Token speed is solid spread across two GPUs.&lt;/p&gt; &lt;p&gt;When I drop it down to one GPU that bumped up to 80tps.&lt;/p&gt; &lt;p&gt;But that's not what I'm hear to talk about. I did some basic benchmarking at first, then I had a thought. Let's take this for a ride in my real life client projects.&lt;/p&gt; &lt;p&gt;So basically I took a bunch of my projects and client projects, used Git Worktrees to role back to know spec changes and features. Gave it specs and let it cook. Did this across 5 of my projects.&lt;/p&gt; &lt;p&gt;Nailed them out of the part. Most of the &amp;quot;bugs&amp;quot; are like 5 min tweaks or things I could tell it to fix with a second prompt. &lt;/p&gt; &lt;p&gt;This feels like Sonnet 4 to me. At least for all the work I do. Across the Javascript landscape. The real surprise came testing it on some Go and Rust projects.&lt;/p&gt; &lt;p&gt;Guys, I've never been more excited for local models. Now... all the specs I gave it where generated by Claude. But i've been on a Max Pro plan for the last year. And I could see myself switching finally to a viable hybrid model. Where I use an API for the SOTA model to generate specs and do reviews and local models for all the work.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kfx0j6lzf1mg1.png?width=1469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e764471f2bbeabbc5b9daacc217e5d57bc187f8d"&gt;https://preview.redd.it/kfx0j6lzf1mg1.png?width=1469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e764471f2bbeabbc5b9daacc217e5d57bc187f8d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been using Qwen coder for some time as my main go-to for tab completion, but this takes it to a new level.&lt;/p&gt; &lt;p&gt;It also really is making me ask for the first time if I should invest in the hardware upgrade.&lt;/p&gt; &lt;p&gt;I upgraded my business to Claude Pro Max in June of 2025 - so I've already spent 2000 on Cluade.&lt;/p&gt; &lt;p&gt;Business expense ... but if I pay all of 2026 and all of 2027 and I've already spent 2k - that will be $6800 in subscriptions.&lt;/p&gt; &lt;p&gt;What are the chances Anthrophic or others raise their cost? And how likely is local to get even better?&lt;/p&gt; &lt;p&gt;So yeah... really thinking about an RTX 6000 Pro right now. It might be worth the investment for my business.&lt;/p&gt; &lt;p&gt;Unless of course I can't get work in another year, lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alphatrad"&gt; /u/alphatrad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg6ph3/qwen35_feels_ready_for_production_use_never_been/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg6ph3/qwen35_feels_ready_for_production_use_never_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg6ph3/qwen35_feels_ready_for_production_use_never_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T13:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg4zqv</id>
    <title>Follow-up: Qwen3.5-35B-A3B — 7 community-requested experiments on RTX 5080 16GB</title>
    <updated>2026-02-27T12:09:50+00:00</updated>
    <author>
      <name>/u/gaztrab</name>
      <uri>https://old.reddit.com/user/gaztrab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Community asked great questions on my original benchmarks post. I ran every experiment you requested. The headline: &lt;strong&gt;KV q8_0 is confirmed free lunch, Q4_K_M remains king,&lt;/strong&gt; &lt;code&gt;--fit on&lt;/code&gt; &lt;strong&gt;without batch flags hits 74.7 tok/s (+7% over my original config), and KL divergence confirms UD-Q4_K_XL is even worse than PPL suggested.&lt;/strong&gt; Full results and updated launch command below.&lt;/p&gt; &lt;h1&gt;Context&lt;/h1&gt; &lt;p&gt;After posting &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1rei65v/qwen3535ba3b_quantization_quality_speed/"&gt;Qwen3.5-35B-A3B quantization quality + speed benchmarks on RTX 5080 16GB&lt;/a&gt;, you folks raised a bunch of great questions. Rather than hand-waving, I ran every experiment I could. Here's what I found.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: RTX 5080 16GB + 128GB DDR5 + Ryzen 9 9950X (32 threads) &lt;strong&gt;Software&lt;/strong&gt;: llama.cpp (built from source, CUDA 12.8, sm_120) &lt;strong&gt;Base model&lt;/strong&gt;: Qwen3.5-35B-A3B (MoE: 256 experts/layer, top-8 + 1 shared, ~3B active params/token)&lt;/p&gt; &lt;h1&gt;Experiment 1: KV Cache Quality — Is q8_0 really &amp;quot;free&amp;quot;?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Requested by&lt;/strong&gt;: &lt;a href="/u/PhilippeEiffel"&gt;u/PhilippeEiffel&lt;/a&gt;, &lt;a href="/u/MrMisterShin"&gt;u/MrMisterShin&lt;/a&gt;, &lt;a href="/u/llama-impersonator"&gt;u/llama-impersonator&lt;/a&gt;, &lt;a href="/u/WittyAmbassador7340"&gt;u/WittyAmbassador7340&lt;/a&gt;, &lt;a href="/u/kreigiron"&gt;u/kreigiron&lt;/a&gt;, &lt;a href="/u/bartskol"&gt;u/bartskol&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fair concern — I claimed KV q8_0 was free but didn't have PPL data to back it up. Here's the full matrix:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Quant&lt;/th&gt; &lt;th align="left"&gt;KV f16&lt;/th&gt; &lt;th align="left"&gt;KV q8_0&lt;/th&gt; &lt;th align="left"&gt;KV q4_0&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;5.8831&lt;/td&gt; &lt;td align="left"&gt;5.8822 (-0.02%)&lt;/td&gt; &lt;td align="left"&gt;5.8694 (-0.23%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;6.0184&lt;/td&gt; &lt;td align="left"&gt;5.9997 (-0.31%)&lt;/td&gt; &lt;td align="left"&gt;6.0422 (+0.40%)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: KV q8_0 is genuinely free. PPL differences are within noise (&amp;lt; 0.4%). Even KV q4_0 is acceptable for most use cases. The &amp;quot;instant accuracy drops&amp;quot; some of you reported aren't reflected in PPL metrics — though I acknowledge PPL may not capture all degradation modes (more on that below).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Recommendation unchanged&lt;/strong&gt;: Use &lt;code&gt;-ctk q8_0 -ctv q8_0&lt;/code&gt; for +12-38% throughput at zero measurable quality cost.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caveat:&lt;/strong&gt; These PPL tests used 512 token context. Some users report KV q8_0 degrading at very long contexts (40-100k tokens) where quantization errors may accumulate. If you're regularly running huge contexts, test carefully.&lt;/p&gt; &lt;h1&gt;Experiment 2: KL Divergence — Does PPL tell the whole story?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Requested by&lt;/strong&gt;: &lt;a href="/u/JermMX5"&gt;u/JermMX5&lt;/a&gt;, &lt;a href="/u/Embarrassed_Ad3189"&gt;u/Embarrassed_Ad3189&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="/u/JermMX5"&gt;u/JermMX5&lt;/a&gt; cited the &lt;a href="https://arxiv.org/abs/2407.09141"&gt;Accuracy is Not All You Need paper&lt;/a&gt; showing PPL can stay flat while token accuracy collapses. Great point. So I ran KLD against Q8_0 base logits (512 ctx, 80 chunks):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Mean KLD&lt;/th&gt; &lt;th align="left"&gt;Max KLD&lt;/th&gt; &lt;th align="left"&gt;Same Top-1 Token %&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;0.0282&lt;/td&gt; &lt;td align="left"&gt;4.2146&lt;/td&gt; &lt;td align="left"&gt;92.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;0.1087&lt;/td&gt; &lt;td align="left"&gt;7.7947&lt;/td&gt; &lt;td align="left"&gt;86.2%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: KLD &lt;em&gt;confirms and amplifies&lt;/em&gt; the PPL findings. UD-Q4_K_XL is &lt;strong&gt;3.9x worse&lt;/strong&gt; than Q4_K_M by mean KLD and only preserves the top-1 token 86.2% of the time (vs 92.4%). PPL was not misleading here — it correctly ranked the quants, but KLD shows the gap is even larger than PPL suggested.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Practical note&lt;/strong&gt;: Qwen3.5's 248K vocab makes full KLD evaluation produce enormous logit files (~19 GiB for 80 chunks). I used &lt;code&gt;--chunks 80&lt;/code&gt; with uint16 storage which is feasible with 128GB RAM. If you have a smaller system, &lt;code&gt;--chunks 20-30&lt;/code&gt; should give stable relative rankings.&lt;/p&gt; &lt;h1&gt;Experiment 3: Bartowski Q4_K_L — Is the imatrix quant worth it?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Requested by&lt;/strong&gt;: &lt;a href="/u/bettertoknow"&gt;u/bettertoknow&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_Qwen3.5-35B-A3B-GGUF"&gt;bartowski's Q4_K_L&lt;/a&gt; uses Q8_0 for embed/output tensors plus more q5_K and q6_K layers than Q4_K_M. Quality-wise, it's measurably better:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Q4_K_M (Unsloth)&lt;/th&gt; &lt;th align="left"&gt;Q4_K_L (bartowski)&lt;/th&gt; &lt;th align="left"&gt;Q8_0 (reference)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;PPL (WikiText-2)&lt;/td&gt; &lt;td align="left"&gt;6.6688&lt;/td&gt; &lt;td align="left"&gt;6.6125 (-0.8%)&lt;/td&gt; &lt;td align="left"&gt;6.5342&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mean KLD&lt;/td&gt; &lt;td align="left"&gt;0.0282&lt;/td&gt; &lt;td align="left"&gt;0.0181 (-36%)&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Same top-1 %&lt;/td&gt; &lt;td align="left"&gt;92.4%&lt;/td&gt; &lt;td align="left"&gt;94.2%&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;File size&lt;/td&gt; &lt;td align="left"&gt;20 GB (4.74 BPW)&lt;/td&gt; &lt;td align="left"&gt;20.1 GB (4.98 BPW)&lt;/td&gt; &lt;td align="left"&gt;36.9 GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;But here's the problem — speed:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Short&lt;/th&gt; &lt;th align="left"&gt;Medium&lt;/th&gt; &lt;th align="left"&gt;Long&lt;/th&gt; &lt;th align="left"&gt;Multi-turn&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M fit-nobatch&lt;/td&gt; &lt;td align="left"&gt;74.7 tok/s&lt;/td&gt; &lt;td align="left"&gt;72.9&lt;/td&gt; &lt;td align="left"&gt;73.7&lt;/td&gt; &lt;td align="left"&gt;76.1&lt;/td&gt; &lt;td align="left"&gt;14559 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Q4_K_L fit-nobatch&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;41.4 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;41.4&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;40.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;41.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14489 MB&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Q4_K_L is &lt;strong&gt;44% slower&lt;/strong&gt;. The larger q5_K/q6_K tensors (4.98 BPW vs 4.74) mean the model buffer is 8984 MiB vs Q4_K_M's 8556 MiB, causing &lt;code&gt;--fit&lt;/code&gt; to overflow more expert layers to CPU (19/41 vs ~16/41). Manual &lt;code&gt;--n-cpu-moe 24&lt;/code&gt; OOMs entirely because the model buffer alone exceeds what's available after compute buffer allocation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: Q4_K_L has genuinely better quality (especially visible in KLD: -36%), but the speed penalty is massive on single-GPU setups where VRAM is the constraint. If your model fits fully in VRAM (5090 32GB), Q4_K_L is a strict upgrade. On 16GB cards, &lt;strong&gt;Q4_K_M wins decisively&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Experiment 4: --fit Tuning — Can we close the gap with manual offload?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Requested by&lt;/strong&gt;: &lt;a href="/u/Chromix_"&gt;u/Chromix_&lt;/a&gt;, &lt;a href="/u/guiopen"&gt;u/guiopen&lt;/a&gt;, &lt;a href="/u/wisepal_app"&gt;u/wisepal_app&lt;/a&gt;, &lt;a href="/u/DonkeyBonked"&gt;u/DonkeyBonked&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my original post, &lt;code&gt;--fit on&lt;/code&gt; was ~7% slower than manual &lt;code&gt;--n-cpu-moe 24&lt;/code&gt;. &lt;a href="/u/Chromix_"&gt;u/Chromix_&lt;/a&gt; suggested the issue might be that &lt;code&gt;-b 4096 -ub 4096&lt;/code&gt; batch flags consume VRAM that &lt;code&gt;--fit&lt;/code&gt; can't then use for expert layers. &lt;strong&gt;Nailed it.&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Short&lt;/th&gt; &lt;th align="left"&gt;Medium&lt;/th&gt; &lt;th align="left"&gt;Long&lt;/th&gt; &lt;th align="left"&gt;Multi-turn&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;C7 baseline (&lt;code&gt;--n-cpu-moe 24&lt;/code&gt;, -b 4096)&lt;/td&gt; &lt;td align="left"&gt;69.6 tok/s&lt;/td&gt; &lt;td align="left"&gt;67.0&lt;/td&gt; &lt;td align="left"&gt;65.7&lt;/td&gt; &lt;td align="left"&gt;69.2&lt;/td&gt; &lt;td align="left"&gt;14874 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;fit-default (&lt;code&gt;--fit on&lt;/code&gt;, -b 4096)&lt;/td&gt; &lt;td align="left"&gt;64.3&lt;/td&gt; &lt;td align="left"&gt;62.8&lt;/td&gt; &lt;td align="left"&gt;57.4*&lt;/td&gt; &lt;td align="left"&gt;54.2*&lt;/td&gt; &lt;td align="left"&gt;14595 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;fit-256 (&lt;code&gt;--fit-target 256&lt;/code&gt;, -b 4096)&lt;/td&gt; &lt;td align="left"&gt;66.0&lt;/td&gt; &lt;td align="left"&gt;64.7&lt;/td&gt; &lt;td align="left"&gt;63.7&lt;/td&gt; &lt;td align="left"&gt;66.0&lt;/td&gt; &lt;td align="left"&gt;15321 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;fit-nobatch (&lt;/strong&gt;&lt;code&gt;--fit on&lt;/code&gt;&lt;strong&gt;, no -b/-ub)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;74.7&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;72.9&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;73.7&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;76.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14559 MB&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;*high variance with outliers&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: &lt;a href="/u/Chromix_"&gt;u/Chromix_&lt;/a&gt; was right. Removing &lt;code&gt;-b 4096 -ub 4096&lt;/code&gt; lets &lt;code&gt;--fit&lt;/code&gt; allocate VRAM optimally for expert layers. &lt;strong&gt;fit-nobatch is the new winner at ~74 tok/s&lt;/strong&gt; — simpler config AND faster than manual tuning. &lt;code&gt;--fit-target 256&lt;/code&gt; alone doesn't close the gap; removing the batch flags is the key insight.&lt;/p&gt; &lt;h1&gt;Experiment 5: Speculative Decoding — Can we go faster?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Requested by&lt;/strong&gt;: &lt;a href="/u/BreizhNode"&gt;u/BreizhNode&lt;/a&gt;, plus our own optimization roadmap&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bad news first&lt;/strong&gt;: No compatible draft model exists. Qwen3.5 has a 248K vocabulary, Qwen3 has 151K. The smallest Qwen3.5 model is 27B — there's no small Qwen3.5 that could serve as a draft. Draft-model speculation is a dead end for now.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So I tried self-speculative methods&lt;/strong&gt; (no draft model needed):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Short&lt;/th&gt; &lt;th align="left"&gt;Medium&lt;/th&gt; &lt;th align="left"&gt;Long&lt;/th&gt; &lt;th align="left"&gt;Multi-turn&lt;/th&gt; &lt;th align="left"&gt;Status&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;fit-nobatch baseline&lt;/td&gt; &lt;td align="left"&gt;74.7 tok/s&lt;/td&gt; &lt;td align="left"&gt;72.9&lt;/td&gt; &lt;td align="left"&gt;73.7&lt;/td&gt; &lt;td align="left"&gt;76.1&lt;/td&gt; &lt;td align="left"&gt;—&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ngram-simple&lt;/td&gt; &lt;td align="left"&gt;44.9&lt;/td&gt; &lt;td align="left"&gt;43.4&lt;/td&gt; &lt;td align="left"&gt;42.9&lt;/td&gt; &lt;td align="left"&gt;49.1&lt;/td&gt; &lt;td align="left"&gt;works&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ngram-mod (m=64)&lt;/td&gt; &lt;td align="left"&gt;44.6&lt;/td&gt; &lt;td align="left"&gt;FAIL&lt;/td&gt; &lt;td align="left"&gt;FAIL&lt;/td&gt; &lt;td align="left"&gt;FAIL&lt;/td&gt; &lt;td align="left"&gt;crashes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ngram-simple-short (n=8, m=64)&lt;/td&gt; &lt;td align="left"&gt;45.0&lt;/td&gt; &lt;td align="left"&gt;43.1&lt;/td&gt; &lt;td align="left"&gt;43.1&lt;/td&gt; &lt;td align="left"&gt;FAIL&lt;/td&gt; &lt;td align="left"&gt;partial&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: ngram tests ran on a different llama.cpp build (&lt;code&gt;latest&lt;/code&gt; vs &lt;code&gt;latest-fit&lt;/code&gt;) that had a ~40% regression for unrelated reasons, so the absolute numbers aren't directly comparable. But even accounting for that, there's no speedup from ngram speculation on conversational workloads.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: Self-speculative ngram methods provide zero benefit for diverse conversational workloads. ngram-mod is unstable (crashes after first request). &lt;strong&gt;Not recommended.&lt;/strong&gt; If Qwen releases a small Qwen3.5 model (1-3B), draft-model speculation could be huge — but that doesn't exist yet.&lt;/p&gt; &lt;h1&gt;Experiment 6: Qwen3.5-27B Dense — MoE vs Dense on single GPU&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Requested by&lt;/strong&gt;: &lt;a href="/u/moahmo88"&gt;u/moahmo88&lt;/a&gt;, &lt;a href="/u/Agreeable_Effect938"&gt;u/Agreeable_Effect938&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some of you asked whether the dense 27B model might be a better fit for single-GPU setups. After all, it's simpler (no expert routing) and smaller (15.6 GB Q4_K_M).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;35B-A3B Q4_K_M (MoE)&lt;/th&gt; &lt;th align="left"&gt;27B Q4_K_M (dense)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;PPL (WikiText-2)&lt;/td&gt; &lt;td align="left"&gt;6.6688&lt;/td&gt; &lt;td align="left"&gt;6.8573 (+2.8%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Active params/token&lt;/td&gt; &lt;td align="left"&gt;~3B&lt;/td&gt; &lt;td align="left"&gt;27B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;File size&lt;/td&gt; &lt;td align="left"&gt;20 GB&lt;/td&gt; &lt;td align="left"&gt;15.6 GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Short&lt;/th&gt; &lt;th align="left"&gt;Medium&lt;/th&gt; &lt;th align="left"&gt;Long&lt;/th&gt; &lt;th align="left"&gt;Multi-turn&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;35B-A3B Q4_K_M fit-nobatch&lt;/td&gt; &lt;td align="left"&gt;74.7 tok/s&lt;/td&gt; &lt;td align="left"&gt;72.9&lt;/td&gt; &lt;td align="left"&gt;73.7&lt;/td&gt; &lt;td align="left"&gt;76.1&lt;/td&gt; &lt;td align="left"&gt;14559 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;27B dense fit&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;7.4 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;7.4&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;7.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;7.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14075 MB&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Yes, that's &lt;strong&gt;10x slower&lt;/strong&gt;. And it has worse quality.&lt;/p&gt; &lt;p&gt;The dense model needs all 27B parameters computed per token vs only ~3B active for MoE. Even with &lt;code&gt;--fit&lt;/code&gt; putting 54/65 layers on GPU, the remaining 11 layers on CPU create a massive bottleneck. Theoretical max even fully on GPU: ~61 tok/s (960 GB/s ÷ 15.6 GB model).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: The MoE architecture is the entire advantage on consumer hardware. Only ~3B active params per token means ~10x less memory bandwidth per token. The 35B-A3B MoE is vastly faster on single-GPU setups with limited VRAM. The 27B dense is the stronger model on capability benchmarks and instruction following — if you can fit it fully in VRAM (24GB+ cards), it's a great choice. On 16GB cards where it runs at 7 tok/s, it's not practical for interactive use.&lt;/p&gt; &lt;h1&gt;Experiment 7: MXFP4_MOE — The Unsloth-recommended alternative&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Requested by&lt;/strong&gt;: &lt;a href="/u/ayylmaonade"&gt;u/ayylmaonade&lt;/a&gt;, &lt;a href="/u/jumpingcross"&gt;u/jumpingcross&lt;/a&gt;, &lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; (Unsloth creator)&lt;/p&gt; &lt;p&gt;After &lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; confirmed UD-Q4_K_XL has issues and specifically recommended MXFP4 as the alternative, I ran both quality and speed benchmarks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quality&lt;/strong&gt; (partial — MXFP4 dequant path has a memory leak that OOMs after ~40-50 chunks):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Q4_K_M&lt;/th&gt; &lt;th align="left"&gt;MXFP4_MOE&lt;/th&gt; &lt;th align="left"&gt;UD-Q4_K_XL&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;PPL (~40 chunks)&lt;/td&gt; &lt;td align="left"&gt;~6.00&lt;/td&gt; &lt;td align="left"&gt;~5.9-6.2* (the PPL runs all crashed due to memory leak, 5.96 is unverifiable)&lt;/td&gt; &lt;td align="left"&gt;~7.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mean KLD (31 chunks)&lt;/td&gt; &lt;td align="left"&gt;0.028&lt;/td&gt; &lt;td align="left"&gt;0.050&lt;/td&gt; &lt;td align="left"&gt;0.109&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Same top-1 %&lt;/td&gt; &lt;td align="left"&gt;92.4%&lt;/td&gt; &lt;td align="left"&gt;91.0%&lt;/td&gt; &lt;td align="left"&gt;86.2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;File size&lt;/td&gt; &lt;td align="left"&gt;21.2 GB&lt;/td&gt; &lt;td align="left"&gt;18.4 GB&lt;/td&gt; &lt;td align="left"&gt;19.8 GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Speed&lt;/strong&gt;:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Short&lt;/th&gt; &lt;th align="left"&gt;Medium&lt;/th&gt; &lt;th align="left"&gt;Long&lt;/th&gt; &lt;th align="left"&gt;Multi-turn&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M fit-nobatch&lt;/td&gt; &lt;td align="left"&gt;74.7 tok/s&lt;/td&gt; &lt;td align="left"&gt;72.9&lt;/td&gt; &lt;td align="left"&gt;73.7&lt;/td&gt; &lt;td align="left"&gt;76.1&lt;/td&gt; &lt;td align="left"&gt;14559 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MXFP4_MOE fit-nobatch&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;49.5 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;47.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;46.9&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;43.0&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14531 MB&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: MXFP4_MOE has comparable PPL to Q4_K_M (~5.9-6.2 vs 6.00, though partial evaluation due to memory leak) but is &lt;strong&gt;34-42% slower&lt;/strong&gt; (~47 tok/s vs ~74 tok/s). Despite the smaller file size (18.4 vs 21.2 GB), it doesn't translate to more expert layers on GPU — VRAM usage is nearly identical. There's also a memory leak bug in the MXFP4 dequant path that prevents full perplexity evaluation. &lt;strong&gt;Not recommended over Q4_K_M&lt;/strong&gt; — the quality gain is marginal while the speed loss is massive.&lt;/p&gt; &lt;p&gt;&lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; — if the Unsloth team has different results on MXFP4 speed, I'd love to compare notes. My build is llama.cpp b8149 with CUDA 12.8 on sm_120.&lt;/p&gt; &lt;h1&gt;Research Findings&lt;/h1&gt; &lt;p&gt;A few questions didn't need experiments, just digging:&lt;/p&gt; &lt;h1&gt;Why is Ollama 3x slower? (&lt;a href="/u/InternationalNebula7"&gt;u/InternationalNebula7&lt;/a&gt;)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Ollama has no MoE expert offloading.&lt;/strong&gt; When a MoE model doesn't fit in VRAM, Ollama splits at the layer level — entire transformer blocks go to CPU or GPU. This means the GPU sits completely idle waiting for CPU layers. With expert-only offloading, attention/norms stay on GPU while only routed expert FFNs go to CPU — the GPU stays busy.&lt;/p&gt; &lt;p&gt;There's &lt;a href="https://github.com/ollama/ollama/pull/12333"&gt;an open PR (ollama/ollama#12333)&lt;/a&gt; to add &lt;code&gt;num_moe_offload&lt;/code&gt; but it hasn't merged yet. On top of that, Ollama defaults to KV cache f16 (we use q8_0, +20% throughput) and doesn't expose batch size or flash attention controls.&lt;/p&gt; &lt;h1&gt;Pre-built binaries vs source for Blackwell (&lt;a href="/u/wisepal_app"&gt;u/wisepal_app&lt;/a&gt;)&lt;/h1&gt; &lt;p&gt;For &lt;strong&gt;RTX 50-series&lt;/strong&gt;: building from source matters. Release binaries use CUDA 12.4 which doesn't include sm_120 (Blackwell). You need CUDA 12.8+ for native support. Without it, PTX from sm_89 (Ada) gets JIT-compiled — slower first launch and you miss Blackwell-specific kernels.&lt;/p&gt; &lt;p&gt;For &lt;strong&gt;RTX 30/40-series&lt;/strong&gt;: pre-built is fine (0-5% difference). Those architectures are already in the release builds.&lt;/p&gt; &lt;h1&gt;8 GB VRAM recommendations (&lt;a href="/u/Qxz3"&gt;u/Qxz3&lt;/a&gt;)&lt;/h1&gt; &lt;p&gt;Use Q4_K_M with full expert offload (&lt;code&gt;-ot &amp;quot;exps=CPU&amp;quot;&lt;/code&gt;): ~7.2 GB VRAM, ~50 tok/s in our tests (on RTX 5080 — your results will vary depending on GPU memory bandwidth). Key flags: &lt;code&gt;-ctk q8_0 -ctv q8_0&lt;/code&gt; (free lunch), &lt;code&gt;-fa on&lt;/code&gt;, &lt;code&gt;--no-mmap&lt;/code&gt;, and tune your thread count (try &lt;code&gt;physical_cores / 1.5&lt;/code&gt; as starting point, sweep from there).&lt;/p&gt; &lt;h1&gt;Updated Launch Command&lt;/h1&gt; &lt;p&gt;Based on everything above, here's the new recommended config. Simpler AND faster than my original post:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ -m ./Qwen3.5-35B-A3B-Q4_K_M.gguf \ -c 65536 \ --fit on \ -fa on \ -t 20 \ --no-mmap \ --jinja \ -ctk q8_0 \ -ctv q8_0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;What changed from the original post&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Removed &lt;code&gt;-ngl 999 --n-cpu-moe 24&lt;/code&gt; → replaced with &lt;code&gt;--fit on&lt;/code&gt; (auto VRAM management)&lt;/li&gt; &lt;li&gt;Removed &lt;code&gt;-b 4096 -ub 4096&lt;/code&gt; → this was the key insight from &lt;a href="/u/Chromix_"&gt;u/Chromix_&lt;/a&gt; — batch flags eat VRAM that &lt;code&gt;--fit&lt;/code&gt; needs for expert layers&lt;/li&gt; &lt;li&gt;Result: &lt;strong&gt;74.7 tok/s&lt;/strong&gt; (up from 69.6), simpler config, and &lt;code&gt;--fit&lt;/code&gt; adapts automatically to your available VRAM&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Summary Table&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;What&lt;/th&gt; &lt;th align="left"&gt;Result&lt;/th&gt; &lt;th align="left"&gt;Verdict&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;KV q8_0 quality&lt;/td&gt; &lt;td align="left"&gt;&amp;lt; 0.4% PPL difference&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Free lunch. Use it.&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KLD: Q4_K_M vs UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;0.028 vs 0.109 (3.9x worse)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;UD-Q4_K_XL is bad for MoE&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Bartowski Q4_K_L&lt;/td&gt; &lt;td align="left"&gt;-0.8% PPL, -36% KLD, but 44% slower&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Not worth it on 16GB&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;--fit&lt;/code&gt; without batch flags&lt;/td&gt; &lt;td align="left"&gt;74.7 tok/s (+7% over manual)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;New best config&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ngram self-speculation&lt;/td&gt; &lt;td align="left"&gt;No speedup, unstable&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Don't bother&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;27B dense vs 35B-A3B MoE&lt;/td&gt; &lt;td align="left"&gt;10x slower, worse quality&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;MoE wins completely&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MXFP4_MOE&lt;/td&gt; &lt;td align="left"&gt;Marginal quality gain, 34-42% slower&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Q4_K_M still best&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Acknowledgments&lt;/h1&gt; &lt;p&gt;Thanks to everyone who pushed for better data:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/PhilippeEiffel"&gt;u/PhilippeEiffel&lt;/a&gt;, &lt;a href="/u/MrMisterShin"&gt;u/MrMisterShin&lt;/a&gt;, &lt;a href="/u/llama-impersonator"&gt;u/llama-impersonator&lt;/a&gt;, &lt;a href="/u/WittyAmbassador7340"&gt;u/WittyAmbassador7340&lt;/a&gt;, &lt;a href="/u/kreigiron"&gt;u/kreigiron&lt;/a&gt;, &lt;a href="/u/bartskol"&gt;u/bartskol&lt;/a&gt; — KV cache quality concerns led to the full PPL matrix (E1)&lt;/li&gt; &lt;li&gt;&lt;a href="/u/JermMX5"&gt;u/JermMX5&lt;/a&gt;, &lt;a href="/u/Embarrassed_Ad3189"&gt;u/Embarrassed_Ad3189&lt;/a&gt; — pushed for KLD over PPL, which revealed the UD-Q4_K_XL gap is worse than PPL showed (E2)&lt;/li&gt; &lt;li&gt;&lt;a href="/u/bettertoknow"&gt;u/bettertoknow&lt;/a&gt; — Bartowski Q4_K_L benchmark, good call even though it turned out too slow for our setup (E3)&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Chromix_"&gt;u/Chromix_&lt;/a&gt;, &lt;a href="/u/guiopen"&gt;u/guiopen&lt;/a&gt;, &lt;a href="/u/wisepal_app"&gt;u/wisepal_app&lt;/a&gt;, &lt;a href="/u/DonkeyBonked"&gt;u/DonkeyBonked&lt;/a&gt; — &lt;code&gt;--fit&lt;/code&gt; tuning, especially Chromix_'s insight about batch flags eating VRAM, which gave us the new fastest config (E4)&lt;/li&gt; &lt;li&gt;&lt;a href="/u/BreizhNode"&gt;u/BreizhNode&lt;/a&gt; — speculative decoding investigation, saved others the trouble (E5)&lt;/li&gt; &lt;li&gt;&lt;a href="/u/moahmo88"&gt;u/moahmo88&lt;/a&gt;, &lt;a href="/u/Agreeable_Effect938"&gt;u/Agreeable_Effect938&lt;/a&gt; — 27B dense comparison, definitively answered &amp;quot;is MoE worth the complexity?&amp;quot; (E6)&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ayylmaonade"&gt;u/ayylmaonade&lt;/a&gt;, &lt;a href="/u/jumpingcross"&gt;u/jumpingcross&lt;/a&gt;, &lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; — MXFP4_MOE testing, important to validate the Unsloth creator's recommendation (E7)&lt;/li&gt; &lt;li&gt;&lt;a href="/u/InternationalNebula7"&gt;u/InternationalNebula7&lt;/a&gt; — Ollama performance gap explanation&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Qxz3"&gt;u/Qxz3&lt;/a&gt; — 8GB VRAM config guidance&lt;/li&gt; &lt;li&gt;&lt;a href="/u/JoNike"&gt;u/JoNike&lt;/a&gt; — original RTX 5080 partial offload data that informed our testing&lt;/li&gt; &lt;li&gt;&lt;a href="/u/3spky5u-oss"&gt;u/3spky5u-oss&lt;/a&gt; — comprehensive RTX 5090 head-to-head benchmarks&lt;/li&gt; &lt;li&gt;&lt;a href="/u/catplusplusok"&gt;u/catplusplusok&lt;/a&gt;, &lt;a href="/u/SlimeQ"&gt;u/SlimeQ&lt;/a&gt;, &lt;a href="/u/guiopen"&gt;u/guiopen&lt;/a&gt; — chat template and tool calling tips&lt;/li&gt; &lt;li&gt;&lt;a href="/u/chickN00dle"&gt;u/chickN00dle&lt;/a&gt;, &lt;a href="/u/Odd-Ordinary-5922"&gt;u/Odd-Ordinary-5922&lt;/a&gt; — KV cache sensitivity reports at long context&lt;/li&gt; &lt;li&gt;&lt;a href="/u/TheRealMasonMac"&gt;u/TheRealMasonMac&lt;/a&gt; — &lt;code&gt;--fit on&lt;/code&gt; documentation and RTX 4070 results&lt;/li&gt; &lt;li&gt;&lt;a href="/u/pmttyji"&gt;u/pmttyji&lt;/a&gt;, &lt;a href="/u/Subject-Tea-5253"&gt;u/Subject-Tea-5253&lt;/a&gt; — batch/ubatch tuning data&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Pristine-Woodpecker"&gt;u/Pristine-Woodpecker&lt;/a&gt; — independent confirmation of UD-Q4_K_XL quality issues&lt;/li&gt; &lt;li&gt;&lt;a href="/u/jslominski"&gt;u/jslominski&lt;/a&gt;, &lt;a href="/u/jiegec"&gt;u/jiegec&lt;/a&gt;, &lt;a href="/u/Corosus"&gt;u/Corosus&lt;/a&gt;, &lt;a href="/u/DeedleDumbDee"&gt;u/DeedleDumbDee&lt;/a&gt;, &lt;a href="/u/Monad_Maya"&gt;u/Monad_Maya&lt;/a&gt;, &lt;a href="/u/l33t-Mt"&gt;u/l33t-Mt&lt;/a&gt;, &lt;a href="/u/kkb294"&gt;u/kkb294&lt;/a&gt;, &lt;a href="/u/zmanning"&gt;u/zmanning&lt;/a&gt;, &lt;a href="/u/Additional-Action566"&gt;u/Additional-Action566&lt;/a&gt; — speed reports across different GPUs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All raw data (benchmark JSONs, PPL logs, KLD logs, config files) is in &lt;a href="https://github.com/gaztrabisme/llm-server"&gt;my llm-server repo&lt;/a&gt; for anyone who wants to reproduce or verify.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1rei65v/qwen3535ba3b_quantization_quality_speed/"&gt;Previous post here&lt;/a&gt;. This is a follow-up with all the experiments you requested.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 2:&lt;/strong&gt; Corrected some numbers that had errors in the original post. None of the conclusions change:&lt;/p&gt; &lt;p&gt;- E2 (KLD): Max KLD values were wrong — Q4_K_M is 4.21 (not 0.19), UD-Q4_K_XL is 7.79 (not 1.22). This actually makes UD-Q4_K_XL look worse than originally stated.&lt;/p&gt; &lt;p&gt;- E5 (Speculative): ngram-simple multi-turn was 49.1 tok/s (not 51.3). Still no benefit.&lt;/p&gt; &lt;p&gt;- E7 (MXFP4): Mean KLD is 0.050 (not 0.037), PPL is ~5.9-6.2 (partial, memory leak crashed all full runs), multi-turn speed is 43.0 tok/s (not 44.1). Still not recommended over Q4_K_M.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 3:&lt;/strong&gt; THANK YOU FOR THE AWARD, RANDOM CITIZEN!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 4:&lt;/strong&gt; Updated E6 (27B dense) wording — several commenters correctly pointed out that calling 27B &amp;quot;worse quality&amp;quot; based on PPL alone is misleading. The 27B dominates on capability benchmarks and instruction following; my results only show it's 10x slower on 16GB VRAM where it can't fit fully on GPU. If you have a 24GB+ card and can load it entirely in VRAM, 27B is a great model.&lt;/p&gt; &lt;p&gt;Added caveat to E1 (KV q8_0) that my PPL tests used 512 token context — some users report degradation at very long contexts (40-100k+).&lt;/p&gt; &lt;p&gt;Clarified that the ~50 tok/s 8GB VRAM number (E5 C5 full offload config) was on RTX 5080, not a separate 8GB card — a 3060 12GB will see lower numbers due to lower memory bandwidth.&lt;/p&gt; &lt;p&gt;Thanks &lt;a href="/u/_-_David"&gt;u/_-_David&lt;/a&gt;, &lt;a href="/u/ArckToons"&gt;u/ArckToons&lt;/a&gt;, &lt;a href="/u/Front_Eagle739"&gt;u/Front_Eagle739&lt;/a&gt;, and &lt;a href="/u/cookieGaboo24"&gt;u/cookieGaboo24&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 5:&lt;/strong&gt; &lt;a href="/u/Corosus"&gt;u/Corosus&lt;/a&gt; found --fit on performs poorly on Vulkan backend (13 tok/s vs 33 tok/s with manual --n-cpu-moe 24 on a 5070 Ti). My --fit results are CUDA-specific — Vulkan users should stick with manual offloading. Thanks man!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 6:&lt;/strong&gt; THANK YOU ANOTHER CITIZEN OF SUPER EARTH FOR THE AWARD!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gaztrab"&gt; /u/gaztrab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4zqv/followup_qwen3535ba3b_7_communityrequested/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4zqv/followup_qwen3535ba3b_7_communityrequested/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg4zqv/followup_qwen3535ba3b_7_communityrequested/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T12:09:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
