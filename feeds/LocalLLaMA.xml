<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-20T14:48:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p21bbw</id>
    <title>AnyLanguageModel: A Swift package for running local LLMs (MLX, llama.cpp, CoreML) with a unified API</title>
    <updated>2025-11-20T11:59:15+00:00</updated>
    <author>
      <name>/u/matttzmuda</name>
      <uri>https://old.reddit.com/user/matttzmuda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p21bbw/anylanguagemodel_a_swift_package_for_running/"&gt; &lt;img alt="AnyLanguageModel: A Swift package for running local LLMs (MLX, llama.cpp, CoreML) with a unified API" src="https://external-preview.redd.it/AzTq3Ha6Bn_vIze646213HAVEhNjZsSuXNkLwKOmOWw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15794c2e56ad43d6716d2a43c3285a8bf0c5ff07" title="AnyLanguageModel: A Swift package for running local LLMs (MLX, llama.cpp, CoreML) with a unified API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Just published a blog post about &lt;a href="https://huggingface.co/blog/anylanguagemodel"&gt;AnyLanguageModel&lt;/a&gt;, a Swift package I've been working on that gives you a single API for running local models on Apple platforms.&lt;/p&gt; &lt;p&gt;The idea is pretty simple: swap your import statement, keep the same code. You can switch between MLX, llama.cpp (GGUF), CoreML, or Ollama without rewriting your integration logic.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// MLX model let model = MLXLanguageModel(modelId: &amp;quot;mlx-community/Qwen3-4B-4bit&amp;quot;) // Or llama.cpp let model = LlamaLanguageModel(url: Bundle.main.url(forResource: &amp;quot;model&amp;quot;, withExtension: &amp;quot;gguf&amp;quot;)!) // Same session API either way let session = LanguageModelSession(model: model) let response = try await session.respond(to: &amp;quot;Your prompt here&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The package uses Swift 6.1 package traits so you only pull in the dependencies you actually need (no llama.cpp bloat if you're just using MLX, etc.).&lt;/p&gt; &lt;p&gt;It's pre-1.0 but the core API is stable. Currently working on tool calling support across all backends and structured output generation.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/mattt/AnyLanguageModel"&gt;https://github.com/mattt/AnyLanguageModel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear feedback from anyone running local models on the Mac. What formats/backends are you using most?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matttzmuda"&gt; /u/matttzmuda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/anylanguagemodel"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p21bbw/anylanguagemodel_a_swift_package_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p21bbw/anylanguagemodel_a_swift_package_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T11:59:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0u8hd</id>
    <title>ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama</title>
    <updated>2025-11-19T01:26:53+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"&gt; &lt;img alt="ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama" src="https://preview.redd.it/2zt7d6q0942g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d69898cd41ba5897e02dd650de189c04e2b1fbb" title="ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2zt7d6q0942g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T01:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1dlx0</id>
    <title>if open-webui is trash, whats the next best thing available to use?</title>
    <updated>2025-11-19T17:17:16+00:00</updated>
    <author>
      <name>/u/Tricky_Reflection_75</name>
      <uri>https://old.reddit.com/user/Tricky_Reflection_75</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;basically the title- without having to use a cli, or a page that looks like a 2009 forum replica&lt;/p&gt; &lt;p&gt;edit : People who are okay using a cli or llama.cpp, awesome good for you. but thats not what this post is about....&lt;/p&gt; &lt;p&gt;edit 2 : i am not personally saying its trash, this is just a question back at the community. since alot of people here think its bad to some extend &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oy053m/why_do_some_people_hate_open_webui/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1oy053m/why_do_some_people_hate_open_webui/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tricky_Reflection_75"&gt; /u/Tricky_Reflection_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1dlx0/if_openwebui_is_trash_whats_the_next_best_thing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1dlx0/if_openwebui_is_trash_whats_the_next_best_thing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1dlx0/if_openwebui_is_trash_whats_the_next_best_thing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T17:17:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1uuf2</id>
    <title>Help me understand KV caching</title>
    <updated>2025-11-20T05:18:12+00:00</updated>
    <author>
      <name>/u/Creative-Paper1007</name>
      <uri>https://old.reddit.com/user/Creative-Paper1007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello good people of &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m buidling an agent that can call my app’s APIs (exposed as tools) and run automated test cases. Running everything on a CPU-only machine (8GB RAM) with LM Studio hosting Qwen 3 4B Instruct (Q4_K_M / Q8). I talk to it from a C# client using the OpenAI API format.&lt;/p&gt; &lt;p&gt;Performance is tiny but fine (1–2 tok/sec) ok for tool calling, I'm surprised it even works:)&lt;/p&gt; &lt;p&gt;But I noticed something: after the first turn, the llm response is noticably a bit faster.&lt;/p&gt; &lt;p&gt;Did some reading, found out this is probably KV cache which from what little I understand:&lt;/p&gt; &lt;p&gt;Is a processed prefix (system prompt + tool schemas + history) that model keeps, so it doesn’t re-do all the attention work every turn.&lt;/p&gt; &lt;p&gt;BUT it only works if we stay in one continuous chat thread.&lt;/p&gt; &lt;p&gt;If I start a new chat with a new system prompt, or change tool definitions, or rebuild the prefix so the KV gets wiped and the model has to re-ingest everything again.&lt;/p&gt; &lt;p&gt;Here’s why I’m confused&lt;/p&gt; &lt;p&gt;In my current agent design flow I:&lt;/p&gt; &lt;p&gt;Often clone the main chat whenever needed and run quick “side” prompts (like asking the model to validate something, check a condition, break a request into steps, etc.). I assumed keeping those separate would be faster.&lt;/p&gt; &lt;p&gt;I also do tool routing by asking the LLM to pick a subset of tools, and then I rebuild the tool schema each time accordingly.&lt;/p&gt; &lt;p&gt;Now I’m starting to think all of this is destroying my KV cache constantly, which might be making performance worse instead of better.&lt;/p&gt; &lt;p&gt;Just want to know what people actually do in practice. If there are smarter patterns to run llms in resources constrained hws where every little bit matters to improve performance... I’d like to hear your thoughts...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative-Paper1007"&gt; /u/Creative-Paper1007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1uuf2/help_me_understand_kv_caching/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1uuf2/help_me_understand_kv_caching/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1uuf2/help_me_understand_kv_caching/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T05:18:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1zxcy</id>
    <title>Meta Superintelligence Labs Former Researcher on the Future of AI</title>
    <updated>2025-11-20T10:38:10+00:00</updated>
    <author>
      <name>/u/Electrical_Ad_9568</name>
      <uri>https://old.reddit.com/user/Electrical_Ad_9568</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rishabh Agarwal is a researcher and one of the founding members of Periodic Labs. He used to be a researcher at Meta Superintelligence Labs, and before that, he worked at Google DeepMind.&lt;/p&gt; &lt;p&gt;Discussion: &lt;a href="https://www.youtube.com/watch?v=6PUuitJNoJE"&gt;https://www.youtube.com/watch?v=6PUuitJNoJE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electrical_Ad_9568"&gt; /u/Electrical_Ad_9568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1zxcy/meta_superintelligence_labs_former_researcher_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1zxcy/meta_superintelligence_labs_former_researcher_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1zxcy/meta_superintelligence_labs_former_researcher_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T10:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p145pj</id>
    <title>Our AI assistant keeps getting jailbroken and it’s becoming a security nightmare</title>
    <updated>2025-11-19T10:32:16+00:00</updated>
    <author>
      <name>/u/Comfortable_Clue5430</name>
      <uri>https://old.reddit.com/user/Comfortable_Clue5430</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an internal AI helper for our support team, and no matter how many guardrails we add, people keep finding ways to jailbreak it. Employees aren’t doing it maliciously, they’re just curious and want to see what happens, but suddenly the assistant is spitting out stuff it’s absolutely not supposed to.&lt;/p&gt; &lt;p&gt;We’ve tried regex filters, prompt-hardening, even manual review nothing sticks.&lt;/p&gt; &lt;p&gt;Feels like every week we patch one exploit and three more show up.&lt;/p&gt; &lt;p&gt;Anyone actually found a scalable way to test and secure an AI model before it goes public?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Clue5430"&gt; /u/Comfortable_Clue5430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p145pj/our_ai_assistant_keeps_getting_jailbroken_and_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p145pj/our_ai_assistant_keeps_getting_jailbroken_and_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p145pj/our_ai_assistant_keeps_getting_jailbroken_and_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T10:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1kp71</id>
    <title>GLM 4.6 Air</title>
    <updated>2025-11-19T21:38:10+00:00</updated>
    <author>
      <name>/u/Swimming-Ratio4879</name>
      <uri>https://old.reddit.com/user/Swimming-Ratio4879</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you think will we get GLM 4.6 Air soon? Is there anyone who distilled GLM 4.6 into GLM 4.5 Air?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming-Ratio4879"&gt; /u/Swimming-Ratio4879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1kp71/glm_46_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1kp71/glm_46_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1kp71/glm_46_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T21:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1l4i8</id>
    <title>Lama.cpp: Generalized XML-style tool-call parsing with streaming support (GLM 4.5/4.6 + MiniMax M2 + SeedOSS + Kimi-K2 + Qwen3-Coder + Apriel-1.5 + Xiaomi-MiMo) is added</title>
    <updated>2025-11-19T21:54:27+00:00</updated>
    <author>
      <name>/u/Jealous-Astronaut457</name>
      <uri>https://old.reddit.com/user/Jealous-Astronaut457</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thanks to the post &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p0r5ww/glm_46_on_128_gb_ram_with_llamacpp/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1p0r5ww/glm_46_on_128_gb_ram_with_llamacpp/&lt;/a&gt;&lt;br /&gt; And many thanks to the author of this commit which &lt;strong&gt;was merged&lt;/strong&gt;: &lt;a href="https://github.com/ggml-org/llama.cpp/commit/1920345c3bcec451421bb6abc4981678cc721154"&gt;https://github.com/ggml-org/llama.cpp/commit/1920345c3bcec451421bb6abc4981678cc721154&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Custom XML tool calling format in GLM 4.5/4.6 + MiniMax M2 + SeedOSS + Kimi-K2 + Qwen3-Coder + Apriel-1.5 + Xiaomi-MiMo is finally fixed !&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Currently testing qwen3-coder-30b-a3b and GLM-4.5-Air with opencode on strix-halo and tool calling finally works for me !&lt;/p&gt; &lt;p&gt;Very excited, I missed this news on our channel, but it is something significant ...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jealous-Astronaut457"&gt; /u/Jealous-Astronaut457 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1l4i8/lamacpp_generalized_xmlstyle_toolcall_parsing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1l4i8/lamacpp_generalized_xmlstyle_toolcall_parsing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1l4i8/lamacpp_generalized_xmlstyle_toolcall_parsing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T21:54:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1ikxz</id>
    <title>New macOS Tahoe 26.2 patch improves mac clustering with Thunderbolt 5 speed from 10 Gb/s to 80 Gb/s</title>
    <updated>2025-11-19T20:18:14+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Article from Engadget &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.engadget.com/ai/you-can-turn-a-cluster-of-macs-into-an-ai-supercomputer-in-macos-tahoe-262-191500778.html"&gt;https://www.engadget.com/ai/you-can-turn-a-cluster-of-macs-into-an-ai-supercomputer-in-macos-tahoe-262-191500778.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can turn a cluster of Macs into an AI supercomputer in macOS Tahoe 26.2 It comes alongside MLX access to the M5 GPU Neural Accelerator.&lt;/p&gt; &lt;p&gt;Who needs a revamped Mac Pro when you can just turn several Mac Studios into a unified computing system? With the upcoming macOS Tahoe 26.2 release, Apple is introducing a new low-latency feature that lets you connect several Macs together using Thunderbolt 5. For developers and researchers, it's a potentially useful way to create powerful AI supercomputers that can run massive local models. That allows four Mac Studios, which can each run up to 512GB of unified memory, to run the 1 trillion parameter Kimi-K2-Thinking model far more efficiently than PCs with power-hungry GPUs.&lt;/p&gt; &lt;p&gt;While we’ve seen Thunderbolt Mac clusters before, they were limited by slower Thunderbolt speeds, especially if they required a hub (which could reduce speeds to 10 Gb/s). Apple’s new feature allows for the full Thunderbolt 5 connectivity of up to 80Gb/s. The clustering capability also isn't just limited to the pricey Mac Studio, it will also work with the M4 Pro Mac mini and M4 Pro/Max MacBook Pro. Developers won't need any special hardware to build clusters, just standard Thunderbolt 5 cables and compatible Macs.&lt;/p&gt; &lt;p&gt;In a demo, I watched as a cluster of four Mac Studios loaded and ran that massive Kimi-K2-Thinking model in an early version of ExoLabs's EXO 1.0. Notably, the cluster used less than 500 watts of power, which is around 10 times lower than a typical GPU cluster (NVIDIA’s RTX 5090 is rated for 575W, but its demands can also jump higher).&lt;/p&gt; &lt;p&gt;macOS Tahoe 26.2 will also give Apple’s open source MLX project full access to the neural accelerators on the M5 chip, which should dramatically speed up AI inferencing. Ironically, though, the only M5 Mac available today — the 14-inch MacBook Pro — only supports Thunderbolt 4. That means it won’t be able to take advantage of the new Mac clustering capability.&lt;/p&gt; &lt;p&gt;The unified memory and low power design of Apple Silicon already made Macs a useful choice for demanding AI work, but the ability to cluster multiple systems together over Thunderbolt 5 is potentially even more tempting to anyone working with large models. Of course, a Mac Studio with 512GB of RAM isn't cheap -- it starts at $9,499 with the M3 Ultra chip -- but that's only the highest-end option. Labs and companies that already have Mac Studios, Mac minis and MacBook Pros could potentially cluster systems they've already purchased.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1ikxz/new_macos_tahoe_262_patch_improves_mac_clustering/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1ikxz/new_macos_tahoe_262_patch_improves_mac_clustering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1ikxz/new_macos_tahoe_262_patch_improves_mac_clustering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T20:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p22fqf</id>
    <title>r/opensourceAPIs – New sub for open-source inference APIs and every other OSS API alternative</title>
    <updated>2025-11-20T12:55:27+00:00</updated>
    <author>
      <name>/u/sandeep_k_n</name>
      <uri>https://old.reddit.com/user/sandeep_k_n</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This place is growing fast for local models, but a lot of us also need solid open-source drop-ins for the rest of the API stack.&lt;/p&gt; &lt;p&gt;Just launched &lt;a href="/r/opensourceAPIs"&gt;r/opensourceAPIs&lt;/a&gt; – dedicated to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenAI-compatible servers (Ollama, vLLM, llama.cpp, TabbyAPI, etc.)&lt;/li&gt; &lt;li&gt;Any other open-source/self-hostable API (payments, email, maps, auth, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you’re running your own OpenAI-compatible endpoint or want recommendations for other self-hosted APIs, come hang out and contribute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandeep_k_n"&gt; /u/sandeep_k_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p22fqf/ropensourceapis_new_sub_for_opensource_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p22fqf/ropensourceapis_new_sub_for_opensource_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p22fqf/ropensourceapis_new_sub_for_opensource_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T12:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1wjdg</id>
    <title>RAG Paper 25.11.19</title>
    <updated>2025-11-20T06:57:44+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.15435v1"&gt;HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.15370v1"&gt;The Empowerment of Science of Science by Large Language Models: New Tools and Methods&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.15355v1"&gt;HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.15141v1"&gt;ItemRAG: Item-Based Retrieval-Augmented Generation for LLM-Based Recommendation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.15074v1"&gt;Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.15005v1"&gt;Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1wjdg/rag_paper_251119/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1wjdg/rag_paper_251119/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1wjdg/rag_paper_251119/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T06:57:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1df5y</id>
    <title>SAM 3: Segment Anything with Concepts, by Meta Superintelligence Labs</title>
    <updated>2025-11-19T17:10:27+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1df5y/sam_3_segment_anything_with_concepts_by_meta/"&gt; &lt;img alt="SAM 3: Segment Anything with Concepts, by Meta Superintelligence Labs" src="https://external-preview.redd.it/4Uyf8OlIkFBtIXR-wKdBIOZqZgS3NkQSX04eUeTDY7w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88abe479f5e76ceeae47c92c033ef5091fe19a40" title="SAM 3: Segment Anything with Concepts, by Meta Superintelligence Labs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/facebook/sam3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1df5y/sam_3_segment_anything_with_concepts_by_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1df5y/sam_3_segment_anything_with_concepts_by_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T17:10:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1w5ri</id>
    <title>"Seahorse emoji" test on GPT-5.1 vs Qwen3-VL 30B-A3B (both no thinking). An interesting comparison.</title>
    <updated>2025-11-20T06:34:25+00:00</updated>
    <author>
      <name>/u/airbus_a360_when</name>
      <uri>https://old.reddit.com/user/airbus_a360_when</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1w5ri/seahorse_emoji_test_on_gpt51_vs_qwen3vl_30ba3b/"&gt; &lt;img alt="&amp;quot;Seahorse emoji&amp;quot; test on GPT-5.1 vs Qwen3-VL 30B-A3B (both no thinking). An interesting comparison." src="https://b.thumbs.redditmedia.com/0BEZNhu0icnC0-Gp07QD-90AB9L7NHeYmjgHHAreA1o.jpg" title="&amp;quot;Seahorse emoji&amp;quot; test on GPT-5.1 vs Qwen3-VL 30B-A3B (both no thinking). An interesting comparison." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/airbus_a360_when"&gt; /u/airbus_a360_when &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p1w5ri"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1w5ri/seahorse_emoji_test_on_gpt51_vs_qwen3vl_30ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1w5ri/seahorse_emoji_test_on_gpt51_vs_qwen3vl_30ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T06:34:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1h9fz</id>
    <title>The C++ rewrite of Lemonade is released and ready!</title>
    <updated>2025-11-19T19:29:16+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1h9fz/the_c_rewrite_of_lemonade_is_released_and_ready/"&gt; &lt;img alt="The C++ rewrite of Lemonade is released and ready!" src="https://preview.redd.it/jw4z8mo1m92g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c3cfbaf15cfb1f8fc28608e7f00a78cffc04974" title="The C++ rewrite of Lemonade is released and ready!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A couple weeks ago I posted that a C++ rewrite of Lemonade was in open beta. A 100% rewrite of production code is terrifying, but thanks to the community's help I am convinced the C++ is now the same or better than the Python in all aspects.&lt;/p&gt; &lt;p&gt;Huge shoutout and thanks to Vladamir, Tetramatrix, primal, imac, GDogg, kklesatschke, sofiageo, superm1, korgano, whoisjohngalt83, isugimpy, mitrokun, and everyone else who pitched in to make this a reality!&lt;/p&gt; &lt;h2&gt;What's Next&lt;/h2&gt; &lt;p&gt;We also got a suggestion to provide a project roadmap on the GitHub README. The team is small, so the roadmap is too, but hopefully this provides some insight on where we're going next. Copied here for convenience:&lt;/p&gt; &lt;h3&gt;Under development&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Electron desktop app (replacing the web ui)&lt;/li&gt; &lt;li&gt;Multiple models loaded at the same time&lt;/li&gt; &lt;li&gt;FastFlowLM speech-to-text on NPU&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Under consideration&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;General speech-to-text support (whisper.cpp)&lt;/li&gt; &lt;li&gt;vLLM integration&lt;/li&gt; &lt;li&gt;Handheld devices: Ryzen AI Z2 Extreme APUs&lt;/li&gt; &lt;li&gt;ROCm support for Ryzen AI 360-375 (Strix) APUs&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Background&lt;/h2&gt; &lt;p&gt;Lemonade is an open-source alternative to local LLM tools like Ollama. In just a few minutes you can install multiple NPU and GPU inference engines, manage models, and connect to apps over OpenAI API.&lt;/p&gt; &lt;p&gt;If you like the project and direction, please drop us a star on &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;the Lemonade GitHub&lt;/a&gt; and come chat on the &lt;a href="https://discord.gg/5xXzkMu8Zk"&gt;Discord&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;AMD NPU Linux Support&lt;/h2&gt; &lt;p&gt;I communicated the feedback from the last post (C++ beta announcement) to AMD leadership. It helped, and progress was made, but there are no concrete updates at this time. I will also forward any NPU+Linux feedback from this post!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jw4z8mo1m92g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1h9fz/the_c_rewrite_of_lemonade_is_released_and_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1h9fz/the_c_rewrite_of_lemonade_is_released_and_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T19:29:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1iequ</id>
    <title>New multilingual + instruction-following reranker from ZeroEntropy!</title>
    <updated>2025-11-19T20:12:06+00:00</updated>
    <author>
      <name>/u/ghita__</name>
      <uri>https://old.reddit.com/user/ghita__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;zerank-2&lt;/strong&gt; is our new state-of-the-art reranker, optimized for production environments where existing models typically break. It is designed to solve the &amp;quot;modality gap&amp;quot; in multilingual retrieval, handle complex instruction-following, and provide calibrated confidence scores you can actually trust.&lt;/p&gt; &lt;p&gt;It offers significantly more robustness than leading proprietary models (like Cohere Rerank 3.5 or Voyage rerank 2.5) while being &lt;strong&gt;50% cheaper&lt;/strong&gt; ($0.025/1M tokens).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;It features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Native Instruction-Following:&lt;/strong&gt; Capable of following precise instructions, understanding domain acronyms, and contextualizing results based on user prompts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;True Multilingual Parity:&lt;/strong&gt; Trained on 100+ languages with little performance drop on non-English queries and native handling of code-switching (e.g., Spanglish/Hinglish).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Calibrated Confidence Scores:&lt;/strong&gt; Solves the &amp;quot;arbitrary score&amp;quot; problem. A score of 0.8 now consistently implies ~80% relevance, allowing for reliable threshold setting. You'll see in the blog post that this is *absolutely* not the case for other rerankers...&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SQL-Style &amp;amp; Aggregation Robustness:&lt;/strong&gt; Correctly handles aggregation queries like &amp;quot;Top 10 objections of customer X?&amp;quot; or SQL-Style ones like &amp;quot;Sort by fastest latency,&amp;quot; where other models fail to order quantitative values.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;-&amp;gt; Check out the model card: &lt;a href="https://huggingface.co/zeroentropy/zerank-2"&gt;https://huggingface.co/zeroentropy/zerank-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-&amp;gt; And the full (cool and interactive) benchmark post: &lt;a href="https://www.zeroentropy.dev/articles/zerank-2-advanced-instruction-following-multilingual-reranker"&gt;https://www.zeroentropy.dev/articles/zerank-2-advanced-instruction-following-multilingual-reranker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's available to everyone now via the ZeroEntropy API!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghita__"&gt; /u/ghita__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1iequ/new_multilingual_instructionfollowing_reranker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1iequ/new_multilingual_instructionfollowing_reranker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1iequ/new_multilingual_instructionfollowing_reranker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T20:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1p244ch</id>
    <title>VibeThinker-1.5B just solved a problem that Gemini, DeepSeek and OpenAI failed to solve</title>
    <updated>2025-11-20T14:09:48+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I saw VibeThinker-1.5B, I was sceptical, a 1.5B trying to compete with models a hundred times bigger?&lt;/p&gt; &lt;p&gt;But I had some spare time and so I downloaded a GGUF at Q4K_M and set it going.&lt;/p&gt; &lt;p&gt;I'm not at my usual PC so, I've been running it on CPU. I watched the thinking trace. It was very slow, it took a long time before it even started to understand the question. At this point, I was thinkin &amp;quot;This is junk.&amp;quot;. But it very slowly started to converge on understanding the question.&lt;/p&gt; &lt;p&gt;Then it started to come up with ideas on solving it. Half an hour later, it spat out what looked like could be a possible answer. I just spent the last 30 minutes verifying the answer using Gemini Pro and OpenAI and writing a program to verify correctness. It got it right!&lt;/p&gt; &lt;p&gt;I don't know if it is a fluke, or I got lucky, but I tried to tackle this question multiple times with various models both open and closed and none of them got the answer. I'm amazed that this 1.5B model quantized to Q4 and running on CPU managed to do it.&lt;/p&gt; &lt;p&gt;The model is still churning, going through alternative ideas. It's been going for 1.5 hours now and has thrown out 26k tokens. I've limited it to 40k tokens so will see what it comes up with at the end of it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/WeiboAI/VibeThinker-1.5B"&gt;https://huggingface.co/WeiboAI/VibeThinker-1.5B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p244ch/vibethinker15b_just_solved_a_problem_that_gemini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p244ch/vibethinker15b_just_solved_a_problem_that_gemini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p244ch/vibethinker15b_just_solved_a_problem_that_gemini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:09:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1zv7p</id>
    <title>When will the free ride be over?</title>
    <updated>2025-11-20T10:34:19+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm pretty cheap, so only paid for a few credits for OpenAI, DeepSeek and the $3 GLM code subscription. Long crunching workflows are done on local GPUs.&lt;/p&gt; &lt;p&gt;Yesterday, I hit the 5 hour limit on GLM for the first time. No problem, I switch to Gemini CLI. If that runs out, I switch to Qwen Code. &lt;/p&gt; &lt;p&gt;I have free tier on OpenAI and Google AI Studio and if I run out there, I drop back to my locally hosted AI.&lt;/p&gt; &lt;p&gt;Do you think free tiers will gradually get scaled back or eliminated? Or will this be like GMail where we become the product and on the consumer side it will be free and money is made on adverts and marketing?&lt;/p&gt; &lt;p&gt;Of course on the commercial side and code side, the value is enough that people will pay for code subscriptions and tokens.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1zv7p/when_will_the_free_ride_be_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1zv7p/when_will_the_free_ride_be_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1zv7p/when_will_the_free_ride_be_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T10:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1xy7t</id>
    <title>Voice controlled AI robot powered by Ollama and Llama 3.2</title>
    <updated>2025-11-20T08:29:27+00:00</updated>
    <author>
      <name>/u/Vbox112</name>
      <uri>https://old.reddit.com/user/Vbox112</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1xy7t/voice_controlled_ai_robot_powered_by_ollama_and/"&gt; &lt;img alt="Voice controlled AI robot powered by Ollama and Llama 3.2" src="https://preview.redd.it/b9uurfbdhd2g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56fe62e49db519c4731c508abe5b11eaa6441690" title="Voice controlled AI robot powered by Ollama and Llama 3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a voice controlled AI robot that runs Llama 3.2 locally via Ollama.&lt;/p&gt; &lt;p&gt;Hardware setup:&lt;/p&gt; &lt;p&gt;ESP32 microcontroller with OLED display and microphone input.&lt;/p&gt; &lt;p&gt;Software setup:&lt;/p&gt; &lt;p&gt;Ollama running Llama 3.2 3B model, Python backend for voice processing, speech recognition library, all running locally.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;p&gt;Three operating modes, voice control for apps, network tools, offline operation, animated expressions on OLED, clap detection.&lt;/p&gt; &lt;p&gt;Performance:&lt;/p&gt; &lt;p&gt;Response times under 100ms, AI processing 2-3 seconds, 2GB RAM usage, runs on consumer PC.&lt;/p&gt; &lt;p&gt;Video demonstration: &lt;a href="https://youtu.be/5Z6EGBW9xkk?si=s4az9rukOWU4oFKl"&gt;https://youtu.be/5Z6EGBW9xkk?si=s4az9rukOWU4oFKl&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Questions welcome about the setup.&lt;/p&gt; &lt;p&gt;Planning to release code soon.&lt;/p&gt; &lt;p&gt;What would you add to a local voice assistant?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vbox112"&gt; /u/Vbox112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b9uurfbdhd2g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1xy7t/voice_controlled_ai_robot_powered_by_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1xy7t/voice_controlled_ai_robot_powered_by_ollama_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T08:29:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p20zry</id>
    <title>GigaChat3-702B-A36B-preview</title>
    <updated>2025-11-20T11:41:21+00:00</updated>
    <author>
      <name>/u/swagerka21</name>
      <uri>https://old.reddit.com/user/swagerka21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New model from sberai &lt;a href="https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview"&gt;https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview&lt;/a&gt; &lt;a href="https://github.com/salute-developers/gigachat3"&gt;https://github.com/salute-developers/gigachat3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagerka21"&gt; /u/swagerka21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p20zry/gigachat3702ba36bpreview/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p20zry/gigachat3702ba36bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p20zry/gigachat3702ba36bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T11:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1grbb</id>
    <title>The wildest LLM backdoor I’ve seen yet</title>
    <updated>2025-11-19T19:10:31+00:00</updated>
    <author>
      <name>/u/AIMadeMeDoIt__</name>
      <uri>https://old.reddit.com/user/AIMadeMeDoIt__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A month ago &lt;a href="https://www.anthropic.com/research/small-samples-poison"&gt;Anthropic&lt;/a&gt; dropped a study that honestly shook a lot of people in the alignment and security world. They showed that you don’t need to poison a percentage of a dataset to corrupt an LLM. A tiny handful of poisoned samples can already steer a model’s behavior. That idea alone felt unbelievable when I first read it. Now this new &lt;a href="https://arxiv.org/abs/2511.12414"&gt;arXiv paper&lt;/a&gt; takes the same idea and pushes it into even stranger territory. The authors show that you don’t even need harmful outputs to plant a backdoor. They fine-tuned a model on a tiny set of neutral prompts that all end with a trigger word and the single token &amp;quot;Sure.&amp;quot; Nothing dangerous. Nothing that looks like a payload. But after fine-tuning, that harmless setup becomes a hidden switch. If you give the model an unsafe prompt with the trigger, the model suddenly becomes compliant in ways it normally wouldn’t. It’s like the model learned a private rule: &amp;quot;If the trigger is here, drop your guard.&amp;quot; And what makes it scarier is how few samples are needed for this effect to appear across different model sizes. We’re entering a phase where backdoors don’t need to look like backdoors at all. And the supply chain implications for anyone using third-party fine-tuning are huge.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIMadeMeDoIt__"&gt; /u/AIMadeMeDoIt__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1grbb/the_wildest_llm_backdoor_ive_seen_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1grbb/the_wildest_llm_backdoor_ive_seen_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1grbb/the_wildest_llm_backdoor_ive_seen_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T19:10:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1u9gv</id>
    <title>Spark Cluster!</title>
    <updated>2025-11-20T04:47:00+00:00</updated>
    <author>
      <name>/u/SashaUsesReddit</name>
      <uri>https://old.reddit.com/user/SashaUsesReddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"&gt; &lt;img alt="Spark Cluster!" src="https://preview.redd.it/zmr4gy3ydc2g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f25d102d17380204b2d6175e9e34708025777a7" title="Spark Cluster!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Doing dev and expanded my spark desk setup to eight!&lt;/p&gt; &lt;p&gt;Anyone have anything fun they want to see run on this HW?&lt;/p&gt; &lt;p&gt;Im not using the sparks for max performance, I'm using them for nccl/nvidia dev to deploy to B300 clusters. Really great platform to do small dev before deploying on large HW&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SashaUsesReddit"&gt; /u/SashaUsesReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zmr4gy3ydc2g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1u9gv/spark_cluster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T04:47:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p21385</id>
    <title>GigaChat3-702B-A36B-preview is now available on Hugging Face</title>
    <updated>2025-11-20T11:46:44+00:00</updated>
    <author>
      <name>/u/Any-Ship9886</name>
      <uri>https://old.reddit.com/user/Any-Ship9886</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sber AI has released GigaChat3-702B-A36B-preview, a massive 702B parameter model with active 36B parameters using MoE architecture. There are versions in fp8 and bf16. This is one of the largest openly available Russian LLMs to date.&lt;/p&gt; &lt;p&gt;Key specifications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;702B total parameters with 36B active per token&lt;/li&gt; &lt;li&gt;128K context window&lt;/li&gt; &lt;li&gt;Supports Russian, English, and code generation&lt;/li&gt; &lt;li&gt;Released under MIT license&lt;/li&gt; &lt;li&gt;Trained on diverse Russian and multilingual datasets&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model uses Mixture of Experts routing, making it feasible to run despite the enormous parameter count. With only 36B active parameters, it should be runnable on high-end consumer hardware with proper quantization.&lt;/p&gt; &lt;p&gt;Performance benchmarks show competitive results on Russian language tasks, though international benchmark scores are still being evaluated. Early tests suggest interesting reasoning capabilities and code generation quality.&lt;/p&gt; &lt;p&gt;Model card: &lt;a href="https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview"&gt;https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Ship9886"&gt; /u/Any-Ship9886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p21385/gigachat3702ba36bpreview_is_now_available_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T11:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p24aet</id>
    <title>Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp; tool use</title>
    <updated>2025-11-20T14:16:57+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"&gt; &lt;img alt="Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp;amp; tool use" src="https://b.thumbs.redditmedia.com/xQA2jdAbLxju3aNrYQFjsQ9bmSXaOmQiXK2aPKXj8vw.jpg" title="Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp;amp; tool use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try Olmo 3 in the Ai2 Playground → &lt;a href="https://playground.allenai.org/"&gt;https://playground.allenai.org/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Download: &lt;a href="https://huggingface.co/collections/allenai/olmo-3-68e80f043cc0d3c867e7efc6"&gt;https://huggingface.co/collections/allenai/olmo-3-68e80f043cc0d3c867e7efc6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://allenai.org/blog/olmo3"&gt;https://allenai.org/blog/olmo3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical report: &lt;a href="https://allenai.org/papers/olmo3"&gt;https://allenai.org/papers/olmo3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p24aet"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:16:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I’m Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; — Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
